usage: train.py [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]
                [--log-format {json,none,simple,tqdm}] [--log-file LOG_FILE]
                [--aim-repo AIM_REPO] [--aim-run-hash AIM_RUN_HASH]
                [--tensorboard-logdir TENSORBOARD_LOGDIR]
                [--wandb-project WANDB_PROJECT] [--azureml-logging]
                [--seed SEED] [--cpu] [--tpu] [--bf16]
                [--memory-efficient-bf16] [--fp16] [--memory-efficient-fp16]
                [--fp16-no-flatten-grads] [--fp16-init-scale FP16_INIT_SCALE]
                [--fp16-scale-window FP16_SCALE_WINDOW]
                [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]
                [--on-cpu-convert-precision] [--min-loss-scale MIN_LOSS_SCALE]
                [--threshold-loss-scale THRESHOLD_LOSS_SCALE] [--amp]
                [--amp-batch-retries AMP_BATCH_RETRIES]
                [--amp-init-scale AMP_INIT_SCALE]
                [--amp-scale-window AMP_SCALE_WINDOW] [--user-dir USER_DIR]
                [--empty-cache-freq EMPTY_CACHE_FREQ]
                [--all-gather-list-size ALL_GATHER_LIST_SIZE]
                [--model-parallel-size MODEL_PARALLEL_SIZE]
                [--quantization-config-path QUANTIZATION_CONFIG_PATH]
                [--profile] [--reset-logging] [--suppress-crashes]
                [--use-plasma-view] [--plasma-path PLASMA_PATH]
                [--criterion {adaptive_loss,cmcl_contrastive_loss,cmcl_kd_contrastive_loss,composite_loss,cross_entropy,ctc,ctc_contrastive_beta_kd_loss,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,label_smoothed_cross_entropy_with_double_ctc,label_smoothed_cross_entropy_with_rdrop,label_smoothed_cross_entropy_with_w2v_ctc,label_smoothed_cross_entropy_with_w2v_ctc_joint,label_smoothed_cross_entropy_with_w2v_ctc_joint_tune_on_st,label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint,label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_align,label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_cluster,label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_cr,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_unit_2pass,speech_to_spectrogram,speech_to_spectrogram_2pass,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]
                [--tokenizer {moses,nltk,space}]
                [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]
                [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]
                [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]
                [--simul-type {hard_aligned,infinite_lookback,waitk,chunkwise,waitk_fixed_pre_decision,hard_aligned_fixed_pre_decision,infinite_lookback_fixed_pre_decision}]
                [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}]
                [--task TASK] [--num-workers NUM_WORKERS]
                [--skip-invalid-size-inputs-valid-test]
                [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]
                [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]
                [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]
                [--dataset-impl {raw,lazy,cached,mmap,fasta,huffman}]
                [--data-buffer-size DATA_BUFFER_SIZE]
                [--train-subset TRAIN_SUBSET] [--valid-subset VALID_SUBSET]
                [--combine-valid-subsets] [--ignore-unused-valid-subsets]
                [--validate-interval VALIDATE_INTERVAL]
                [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]
                [--validate-after-updates VALIDATE_AFTER_UPDATES]
                [--fixed-validation-seed FIXED_VALIDATION_SEED]
                [--disable-validation] [--max-tokens-valid MAX_TOKENS_VALID]
                [--batch-size-valid BATCH_SIZE_VALID]
                [--max-valid-steps MAX_VALID_STEPS] [--curriculum CURRICULUM]
                [--gen-subset GEN_SUBSET] [--num-shards NUM_SHARDS]
                [--shard-id SHARD_ID] [--grouped-shuffling]
                [--update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR]
                [--update-ordered-indices-seed]
                [--distributed-world-size DISTRIBUTED_WORLD_SIZE]
                [--distributed-num-procs DISTRIBUTED_NUM_PROCS]
                [--distributed-rank DISTRIBUTED_RANK]
                [--distributed-backend DISTRIBUTED_BACKEND]
                [--distributed-init-method DISTRIBUTED_INIT_METHOD]
                [--distributed-port DISTRIBUTED_PORT] [--device-id DEVICE_ID]
                [--distributed-no-spawn]
                [--ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}]
                [--ddp-comm-hook {none,fp16}] [--bucket-cap-mb BUCKET_CAP_MB]
                [--fix-batches-to-gpus] [--find-unused-parameters]
                [--gradient-as-bucket-view] [--fast-stat-sync]
                [--heartbeat-timeout HEARTBEAT_TIMEOUT] [--broadcast-buffers]
                [--slowmo-momentum SLOWMO_MOMENTUM]
                [--slowmo-base-algorithm SLOWMO_BASE_ALGORITHM]
                [--localsgd-frequency LOCALSGD_FREQUENCY]
                [--nprocs-per-node NPROCS_PER_NODE]
                [--pipeline-model-parallel]
                [--pipeline-balance PIPELINE_BALANCE]
                [--pipeline-devices PIPELINE_DEVICES]
                [--pipeline-chunks PIPELINE_CHUNKS]
                [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]
                [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]
                [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]
                [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]
                [--pipeline-checkpoint {always,never,except_last}]
                [--zero-sharding {none,os}] [--no-reshard-after-forward]
                [--fp32-reduce-scatter] [--cpu-offload] [--use-sharded-state]
                [--not-fsdp-flatten-parameters] [--arch ARCH]
                [--max-epoch MAX_EPOCH] [--max-update MAX_UPDATE]
                [--stop-time-hours STOP_TIME_HOURS] [--clip-norm CLIP_NORM]
                [--sentence-avg] [--update-freq UPDATE_FREQ] [--lr LR]
                [--stop-min-lr STOP_MIN_LR] [--use-bmuf]
                [--skip-remainder-batch] [--save-dir SAVE_DIR]
                [--restore-file RESTORE_FILE] [--continue-once CONTINUE_ONCE]
                [--finetune-from-model FINETUNE_FROM_MODEL]
                [--reset-dataloader] [--reset-lr-scheduler] [--reset-meters]
                [--reset-optimizer]
                [--optimizer-overrides OPTIMIZER_OVERRIDES]
                [--save-interval SAVE_INTERVAL]
                [--save-interval-updates SAVE_INTERVAL_UPDATES]
                [--keep-interval-updates KEEP_INTERVAL_UPDATES]
                [--keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN]
                [--keep-last-epochs KEEP_LAST_EPOCHS]
                [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS] [--no-save]
                [--no-epoch-checkpoints] [--no-last-checkpoints]
                [--no-save-optimizer-state]
                [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]
                [--maximize-best-checkpoint-metric] [--patience PATIENCE]
                [--checkpoint-suffix CHECKPOINT_SUFFIX]
                [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]
                [--load-checkpoint-on-all-dp-ranks]
                [--write-checkpoints-asynchronously] [--store-ema]
                [--ema-decay EMA_DECAY] [--ema-start-update EMA_START_UPDATE]
                [--ema-seed-model EMA_SEED_MODEL]
                [--ema-update-freq EMA_UPDATE_FREQ] [--ema-fp32]
                [--train-config TRAIN_CONFIG]

optional arguments:
  -h, --help            show this help message and exit
  --no-progress-bar     disable progress bar
  --log-interval LOG_INTERVAL
                        log progress every N batches (when progress bar is
                        disabled)
  --log-format {json,none,simple,tqdm}
                        log format to use
  --log-file LOG_FILE   log file to copy metrics to.
  --aim-repo AIM_REPO   path to Aim repository
  --aim-run-hash AIM_RUN_HASH
                        Aim run hash. If skipped, creates or continues run
                        based on save_dir
  --tensorboard-logdir TENSORBOARD_LOGDIR
                        path to save logs for tensorboard, should match
                        --logdir of running tensorboard (default: no
                        tensorboard logging)
  --wandb-project WANDB_PROJECT
                        Weights and Biases project name to use for logging
  --azureml-logging     Log scalars to AzureML context
  --seed SEED           pseudo random number generator seed
  --cpu                 use CPU instead of CUDA
  --tpu                 use TPU instead of CUDA
  --bf16                use bfloat16; implies --tpu
  --memory-efficient-bf16
                        use a memory-efficient version of BF16 training;
                        implies --bf16
  --fp16                use FP16
  --memory-efficient-fp16
                        use a memory-efficient version of FP16 training;
                        implies --fp16
  --fp16-no-flatten-grads
                        don't flatten FP16 grads tensor
  --fp16-init-scale FP16_INIT_SCALE
                        default FP16 loss scale
  --fp16-scale-window FP16_SCALE_WINDOW
                        number of updates before increasing loss scale
  --fp16-scale-tolerance FP16_SCALE_TOLERANCE
                        pct of updates that can overflow before decreasing the
                        loss scale
  --on-cpu-convert-precision
                        if set, the floating point conversion to fp16/bf16
                        runs on CPU. This reduces bus transfer time and GPU
                        memory usage.
  --min-loss-scale MIN_LOSS_SCALE
                        minimum FP16/AMP loss scale, after which training is
                        stopped
  --threshold-loss-scale THRESHOLD_LOSS_SCALE
                        threshold FP16 loss scale from below
  --amp                 use automatic mixed precision
  --amp-batch-retries AMP_BATCH_RETRIES
                        number of retries of same batch after reducing loss
                        scale with AMP
  --amp-init-scale AMP_INIT_SCALE
                        default AMP loss scale
  --amp-scale-window AMP_SCALE_WINDOW
                        number of updates before increasing AMP loss scale
  --user-dir USER_DIR   path to a python module containing custom extensions
                        (tasks and/or architectures)
  --empty-cache-freq EMPTY_CACHE_FREQ
                        how often to clear the PyTorch CUDA cache (0 to
                        disable)
  --all-gather-list-size ALL_GATHER_LIST_SIZE
                        number of bytes reserved for gathering stats from
                        workers
  --model-parallel-size MODEL_PARALLEL_SIZE
                        total number of GPUs to parallelize model over
  --quantization-config-path QUANTIZATION_CONFIG_PATH
                        path to quantization config file
  --profile             enable autograd profiler emit_nvtx
  --reset-logging       when using Hydra, reset the logging at the beginning
                        of training
  --suppress-crashes    suppress crashes when training with the hydra_train
                        entry point so that the main method can return a value
                        (useful for sweeps)
  --use-plasma-view     Store indices and sizes in shared memory
  --plasma-path PLASMA_PATH
                        path to run plasma_store, defaults to /tmp/plasma.
                        Paths outside /tmp tend to fail.
  --criterion {adaptive_loss,cmcl_contrastive_loss,cmcl_kd_contrastive_loss,composite_loss,cross_entropy,ctc,ctc_contrastive_beta_kd_loss,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,label_smoothed_cross_entropy_with_double_ctc,label_smoothed_cross_entropy_with_rdrop,label_smoothed_cross_entropy_with_w2v_ctc,label_smoothed_cross_entropy_with_w2v_ctc_joint,label_smoothed_cross_entropy_with_w2v_ctc_joint_tune_on_st,label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint,label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_align,label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_cluster,label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_cr,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_unit_2pass,speech_to_spectrogram,speech_to_spectrogram_2pass,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}
  --tokenizer {moses,nltk,space}
  --bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}
  --optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}
  --lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}
  --simul-type {hard_aligned,infinite_lookback,waitk,chunkwise,waitk_fixed_pre_decision,hard_aligned_fixed_pre_decision,infinite_lookback_fixed_pre_decision}
  --scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}
  --task TASK           task
  --train-config TRAIN_CONFIG
                        Configuration YAML filename (for training)

dataset_data_loading:
  --num-workers NUM_WORKERS
                        how many subprocesses to use for data loading
  --skip-invalid-size-inputs-valid-test
                        ignore too long or too short lines in valid and test
                        set
  --max-tokens MAX_TOKENS
                        maximum number of tokens in a batch
  --batch-size BATCH_SIZE, --max-sentences BATCH_SIZE
                        number of examples in a batch
  --required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE
                        batch size will be a multiplier of this value
  --required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE
                        maximum sequence length in batch will be a multiplier
                        of this value
  --dataset-impl {raw,lazy,cached,mmap,fasta,huffman}
                        output dataset implementation
  --data-buffer-size DATA_BUFFER_SIZE
                        Number of batches to preload
  --train-subset TRAIN_SUBSET
                        data subset to use for training (e.g. train, valid,
                        test)
  --valid-subset VALID_SUBSET
                        comma separated list of data subsets to use for
                        validation (e.g. train, valid, test)
  --combine-valid-subsets, --combine-val
                        comma separated list of data subsets to use for
                        validation (e.g. train, valid, test)
  --ignore-unused-valid-subsets
                        do not raise error if valid subsets are ignored
  --validate-interval VALIDATE_INTERVAL
                        validate every N epochs
  --validate-interval-updates VALIDATE_INTERVAL_UPDATES
                        validate every N updates
  --validate-after-updates VALIDATE_AFTER_UPDATES
                        dont validate until reaching this many updates
  --fixed-validation-seed FIXED_VALIDATION_SEED
                        specified random seed for validation
  --disable-validation  disable validation
  --max-tokens-valid MAX_TOKENS_VALID
                        maximum number of tokens in a validation batch
                        (defaults to --max-tokens)
  --batch-size-valid BATCH_SIZE_VALID, --max-sentences-valid BATCH_SIZE_VALID
                        batch size of the validation batch (defaults to
                        --batch-size)
  --max-valid-steps MAX_VALID_STEPS, --nval MAX_VALID_STEPS
                        How many batches to evaluate
  --curriculum CURRICULUM
                        don't shuffle batches for first N epochs
  --gen-subset GEN_SUBSET
                        data subset to generate (train, valid, test)
  --num-shards NUM_SHARDS
                        shard generation over N shards
  --shard-id SHARD_ID   id of the shard to generate (id < num_shards)
  --grouped-shuffling   shuffle batches in groups of num_shards to enable
                        similar sequence lengths on each GPU worker when
                        batches are sorted by length
  --update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR
                        if true then prevents the reuse the epoch batch
                        iterator by setting can_reuse_epoch_itr to false,
                        defaults to --grouped-shuffling )
  --update-ordered-indices-seed
                        if true then increment seed with epoch for getting
                        batch iterators, defautls to False.

distributed_training:
  --distributed-world-size DISTRIBUTED_WORLD_SIZE
                        total number of GPUs across all nodes (default: all
                        visible GPUs)
  --distributed-num-procs DISTRIBUTED_NUM_PROCS
                        total number of processes to fork (default: all
                        visible GPUs)
  --distributed-rank DISTRIBUTED_RANK
                        rank of the current worker
  --distributed-backend DISTRIBUTED_BACKEND
                        distributed backend
  --distributed-init-method DISTRIBUTED_INIT_METHOD
                        typically tcp://hostname:port that will be used to
                        establish initial connetion
  --distributed-port DISTRIBUTED_PORT
                        port number (not required if using --distributed-init-
                        method)
  --device-id DEVICE_ID, --local_rank DEVICE_ID
                        which GPU to use (by default looks for $LOCAL_RANK,
                        usually configured automatically)
  --distributed-no-spawn
                        do not spawn multiple processes even if multiple GPUs
                        are visible
  --ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}
                        DistributedDataParallel backend
  --ddp-comm-hook {none,fp16}
                        communication hook
  --bucket-cap-mb BUCKET_CAP_MB
                        bucket size for reduction
  --fix-batches-to-gpus
                        don't shuffle batches between GPUs; this reduces
                        overall randomness and may affect precision but avoids
                        the cost of re-reading the data
  --find-unused-parameters
                        disable unused parameter detection (not applicable to
                        --ddp-backend=legacy_ddp)
  --gradient-as-bucket-view
                        when set to True, gradients will be views pointing to
                        different offsets of allreduce communication buckets.
                        This can reduce peak memory usage, where the saved
                        memory size will be equal to the total gradients size.
                        --gradient-as-bucket-view=gradient_as_bucket_view)
  --fast-stat-sync      [deprecated] this is now defined per Criterion
  --heartbeat-timeout HEARTBEAT_TIMEOUT
                        kill the job if no progress is made in N seconds; set
                        to -1 to disable
  --broadcast-buffers   Copy non-trainable parameters between GPUs, such as
                        batchnorm population statistics
  --slowmo-momentum SLOWMO_MOMENTUM
                        SlowMo momentum term; by default use 0.0 for 16 GPUs,
                        0.2 for 32 GPUs; 0.5 for 64 GPUs, 0.6 for > 64 GPUs
  --slowmo-base-algorithm SLOWMO_BASE_ALGORITHM
                        Base algorithm. Either 'localsgd' or 'sgp'. Please
                        refer to the documentation of 'slowmo_base_algorithm'
                        parameter in https://fairscale.readthedocs.io/en/lates
                        t/api/experimental/nn/slowmo_ddp.html for more details
  --localsgd-frequency LOCALSGD_FREQUENCY
                        Local SGD allreduce frequency
  --nprocs-per-node NPROCS_PER_NODE
                        number of GPUs in each node. An allreduce operation
                        across GPUs in a node is very fast. Hence, we do
                        allreduce across GPUs in a node, and gossip across
                        different nodes
  --pipeline-model-parallel
                        if set, use pipeline model parallelism across GPUs
  --pipeline-balance PIPELINE_BALANCE
                        partition the model into N_K pieces, where each piece
                        contains N_i layers. The sum(args.pipeline_balance)
                        should equal the total number of layers in the model
  --pipeline-devices PIPELINE_DEVICES
                        a list of device indices indicating which device to
                        place each of the N_K partitions. The length of this
                        list should equal the length of the --pipeline-balance
                        argument
  --pipeline-chunks PIPELINE_CHUNKS
                        microbatch count for pipeline model parallelism
  --pipeline-encoder-balance PIPELINE_ENCODER_BALANCE
                        partition the pipeline parallel encoder into N_K
                        pieces, where each piece contains N_i layers. The
                        sum(args.pipeline_encoder_balance) should equal the
                        total number of encoder layers in the model
  --pipeline-encoder-devices PIPELINE_ENCODER_DEVICES
                        a list of device indices indicating which device to
                        place each of the N_K partitions. The length of this
                        list should equal the length of the --pipeline-
                        encoder-balance argument
  --pipeline-decoder-balance PIPELINE_DECODER_BALANCE
                        partition the pipeline parallel decoder into N_K
                        pieces, where each piece contains N_i layers. The
                        sum(args.pipeline_decoder_balance) should equal the
                        total number of decoder layers in the model
  --pipeline-decoder-devices PIPELINE_DECODER_DEVICES
                        a list of device indices indicating which device to
                        place each of the N_K partitions. The length of this
                        list should equal the length of the --pipeline-
                        decoder-balance argument
  --pipeline-checkpoint {always,never,except_last}
                        checkpointing mode for pipeline model parallelism
  --zero-sharding {none,os}
                        ZeRO sharding
  --no-reshard-after-forward
                        don't reshard parameters after forward pass
  --fp32-reduce-scatter
                        reduce-scatter grads in FP32
  --cpu-offload         offload FP32 params to CPU
  --use-sharded-state   use sharded checkpoint files
  --not-fsdp-flatten-parameters
                        not flatten parameter param for fsdp

Model configuration:
  --arch ARCH, -a ARCH  model architecture

optimization:
  --max-epoch MAX_EPOCH
                        force stop training at specified epoch
  --max-update MAX_UPDATE
                        force stop training at specified update
  --stop-time-hours STOP_TIME_HOURS
                        force stop training after specified cumulative time
                        (if >0)
  --clip-norm CLIP_NORM
                        clip threshold of gradients
  --sentence-avg        normalize gradients by the number of sentences in a
                        batch (default is to normalize by number of tokens)
  --update-freq UPDATE_FREQ
                        update parameters every N_i batches, when in epoch i
  --lr LR               learning rate for the first N epochs; all epochs >N
                        using LR_N (note: this may be interpreted differently
                        depending on --lr-scheduler)
  --stop-min-lr STOP_MIN_LR
                        stop training when the learning rate reaches this
                        minimum
  --use-bmuf            specify global optimizer for syncing models on
                        different GPUs/shards
  --skip-remainder-batch
                        if set, include the last (partial) batch of each epoch
                        in training (default is to skip it).

checkpoint:
  --save-dir SAVE_DIR   path to save checkpoints
  --restore-file RESTORE_FILE
                        filename from which to load checkpoint (default:
                        <save-dir>/checkpoint_last.pt
  --continue-once CONTINUE_ONCE
                        continues from this checkpoint, unless a checkpoint
                        indicated in 'restore_file' option is present
  --finetune-from-model FINETUNE_FROM_MODEL
                        finetune from a pretrained model; note that meters and
                        lr scheduler will be reset
  --reset-dataloader    if set, does not reload dataloader state from the
                        checkpoint
  --reset-lr-scheduler  if set, does not load lr scheduler state from the
                        checkpoint
  --reset-meters        if set, does not load meters from the checkpoint
  --reset-optimizer     if set, does not load optimizer state from the
                        checkpoint
  --optimizer-overrides OPTIMIZER_OVERRIDES
                        a dictionary used to override optimizer args when
                        loading a checkpoint
  --save-interval SAVE_INTERVAL
                        save a checkpoint every N epochs
  --save-interval-updates SAVE_INTERVAL_UPDATES
                        save a checkpoint (and validate) every N updates
  --keep-interval-updates KEEP_INTERVAL_UPDATES
                        keep the last N checkpoints saved with --save-
                        interval-updates
  --keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN
                        when used with --keep-interval-updates, skips deleting
                        any checkpoints with update X where X %
                        keep_interval_updates_pattern == 0
  --keep-last-epochs KEEP_LAST_EPOCHS
                        keep last N epoch checkpoints
  --keep-best-checkpoints KEEP_BEST_CHECKPOINTS
                        keep best N checkpoints based on scores
  --no-save             don't save models or checkpoints
  --no-epoch-checkpoints
                        only store last and best checkpoints
  --no-last-checkpoints
                        don't store last checkpoints
  --no-save-optimizer-state
                        don't save optimizer-state as part of checkpoint
  --best-checkpoint-metric BEST_CHECKPOINT_METRIC
                        metric to use for saving "best" checkpoints
  --maximize-best-checkpoint-metric
                        select the largest metric value for saving "best"
                        checkpoints
  --patience PATIENCE   early stop training if valid performance doesn't
                        improve for N consecutive validation runs; note that
                        this is influenced by --validate-interval
  --checkpoint-suffix CHECKPOINT_SUFFIX
                        suffix to add to the checkpoint file name
  --checkpoint-shard-count CHECKPOINT_SHARD_COUNT
                        Number of shards containing the checkpoint - if the
                        checkpoint is over 300GB, it is preferable to split it
                        into shards to prevent OOM on CPU while loading the
                        checkpoint
  --load-checkpoint-on-all-dp-ranks
                        load checkpoints on all data parallel devices
                        (default: only load on rank 0 and broadcast to other
                        devices)
  --write-checkpoints-asynchronously, --save-async
                        Write checkpoints asynchronously in a separate thread.
                        NOTE: This feature is currently being tested.

EMA configuration:
  --store-ema
  --ema-decay EMA_DECAY
                        decay for exponential moving average model
  --ema-start-update EMA_START_UPDATE
                        start EMA update after this many model updates
  --ema-seed-model EMA_SEED_MODEL
                        Seed to load EMA model from. Used to load EMA model
                        separately from the actual model.
  --ema-update-freq EMA_UPDATE_FREQ
                        Do EMA update every this many model updates
  --ema-fp32            If true, store EMA model in fp32 even if model is in
                        fp16

Args that start with '--' (eg. --no-progress-bar) can also be set in a config
file (specified via --train-config). Config file syntax allows: key=value,
flag=true, stuff=[a,b,c] (for details, see syntax at https://goo.gl/R74nmi).
If an arg is specified in more than one place, then commandline values
override config file values which override defaults.
