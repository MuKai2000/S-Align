train-subset: train_st,train_asr
valid-subset: dev_st

adversarial-training: True    # do AT task
at-level: sentence            # the level of at task, support: sentence / token / window  
at-scale: 4.0                 # set the weight of at loss
mix-tag: 0.5
keep-mt-task: True            # keep update mt task and do not update weight
merge-mt-st: True             # merge (at,) mt, st task --> st
at-nopad: True
mixup-rate: -0.1             # mixup rate, mixup-rate < 0 means random [0.1, 0.9]
mixup-change-id: True
# mixup-for-whole-model: True


max-epoch: 100
max-update: 50000

num-workers: 2
#patience: 12
no-progress-bar: True
log-interval: 100
seed: 1
report-accuracy: True

#load-pretrained-encoder-from:
#load-pretrained-decoder-from:

arch: s2t_joint
w2v-path: 

mt-model-path: 
decoder-embed-path: 

share-decoder-input-output-embed: True
share-two-encoders: True
optimizer: adam
clip-norm: 10.0
lr-scheduler: inverse_sqrt
#lr-scheduler: polynomial_decay
#lr-scheduler: tri_stage
#phase-ratio: 0.1,0.4,0.5
#final-lr-scale: 0.05
warmup-init-lr: 1e-7
warmup-updates: 5000
lr: 2e-4
#weight-decay: 0.0001
adam-betas: (0.9,0.98)
adapter-dim: 4096
adapter-dropout: 0.0

ctc-weight: 0.3                                                                                  
criterion: label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge                         
weight-steps: 5000
share-ctc-embed: true
use-ctc-shrink: true                                                                             
#train-st-without-ctc: true
#max_position_ctc: 100
label-smoothing: 0.2

#use-two-contrastive: true   # double level contrastive learning           
#use-token-contrastive: true                                
contrastive-alpha: 0  #1.5    # contrastive learning                       
contrastive-beta: 1.0
contrastive-temperature: 0.1
zero-infinity: true 
decrease-step: 0
post-process: sentencepiece
is-shrink: uniq


#wav2vec configuration
#macaron-style: False
use-cnn-module: False

cnn-module-kernel: 31
apply-mask: True
mask-prob: 0.5
mask-channel-prob: 0.25
mask-channel-length: 6
use-ctc-loss: True
add-position-embed: true
add-position-embed-after-ctc: true
adapter-layers: 0
sead-layers: 6
final-dropout: 0.1
freeze-finetune-updates: 3000

conv-kernel-sizes: 5,5
conv-channels: 512
dropout: 0.1
activation-fn: relu
encoder-embed-dim: 512
encoder-ffn-embed-dim: 2048
encoder-layers: 6
decoder-layers: 6
encoder-attention-heads: 8
#
decoder-embed-dim: 512
decoder-ffn-embed-dim: 2048
decoder-attention-heads: 8
attention-dropout: 0.1
activation-dropout: 0.1
