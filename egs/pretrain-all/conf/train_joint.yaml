train-subset: train_st,train_asr,train_mt
valid-subset: dev_st

max-epoch: 100
max-update: 120000

num-workers: 4
#patience: 12
no-progress-bar: True
log-interval: 100
seed: 1
report-accuracy: True

#load-pretrained-encoder-from:
#load-pretrained-decoder-from:

arch: s2t_joint
#arch: s2t_w2v2_ode_sead_s
#w2v2-model-path: /apdcephfs/share_1157259/users/adrienxu/st/wav2vec/wav2vec2.0/wav2vec_small.pt
#w2v2-model-path: /apdcephfs/share_1157259/users/adrienxu/st/wav2vec/wav2vec2.0/wav2vec_small_960h.pt
#w2v2-model-path: /apdcephfs/share_1157259/users/adrienxu/st/wav2vec/wav2vec2.0/wav2vec_vox_960h_pl.pt
#adapter-model-path: /apdcephfs/share_1157259/users/adrienxu/st/adapter-pretrain/sead_joint_checkpoint.pt
#adapter-model-path: /apdcephfs/share_1157259/users/adrienxu/st/adapter-pretrain/sead_kd_joint_checkpoint.pt
#adapter-model-path: /apdcephfs/share_1157259/users/adrienxu/st/adapter-pretrain/sead_filter_joint_checkpoint.pt
#adapter-model-path: /apdcephfs/share_1157259/users/adrienxu/st/adapter-pretrain/sead_mustc_kd_checkpoint_merge_joint.pt
#adapter-model-path: /mnt/zhangyuhao/MSP-ST/fairseq/egs/asr-pretrain/checkpoints/deep_transformer_2conv/avg_5_checkpoint.pt
#adapter-model-path: /mnt/zhangyuhao/MSP-ST/fairseq/egs/asr-pretrain/checkpoints/deep_transformer/avg_3_checkpoint.pt
w2v-path: /mnt/zhangyuhao/pretrain/wav2vec_small.pt
#mt-model-path: /mnt/zhangyuhao/MSP-ST/fairseq/egs/machine_translation/checkpoints/wmt-en2de/merge-lcrm/last5.ensemble.pt
mt-model-path: /mnt/zhangyuhao/MSP-ST/fairseq/egs/machine_translation/checkpoints/wmt-en2de/mustc-ende-iwslt-prenorm/last8.ensemble.pt
decoder-embed-path: /mnt/zhangyuhao/MSP-ST/fairseq/egs/machine_translation/pretrain_embeddings_mustc_ende_iwlst_prenorm

#w2v2-model-path: /apdcephfs/share_1157259/users/adrienxu/st/wav2vec/wav2vec-tune/checkpoint_last.pt
share-decoder-input-output-embed: True
optimizer: adam
clip-norm: 10.0
#lr-scheduler: inverse_sqrt
#lr-scheduler: polynomial_decay
lr-scheduler: tri_stage
phase-ratio: 0.1,0.4,0.5
final-lr-scale: 0.05
#warmup-init-lr: 1e-7
#warmup-updates: 5000
lr: 1e-4
#weight-decay: 0.0001
adam-betas: (0.9,0.98)
adapter-dim: 4096
adapter-dropout: 0.0

ctc-weight: 0.3
#criterion: label_smoothed_cross_entropy
#criterion: label_smoothed_cross_entropy_with_ctc
criterion: label_smoothed_cross_entropy_with_w2v_ctc_joint
#train-st-without-ctc
label-smoothing: 0.2
contrastive-alpha: 0.3
contrastive-beta: 1.0
contrastive-temperature: 0.1
zero-infinity: true
decrease-step: 0
post-process: sentencepiece
is-shrink: uniq


#wav2vec configuration
#macaron-style: False
use-cnn-module: False
cnn-module-kernel: 31
apply-mask: True
mask-prob: 0.5
mask-channel-prob: 0.25
mask-channel-length: 6
use-ctc-loss: True
add-position-embed: true
add-position-embed-after-ctc: true
adapter-layers: 1
sead-layers: 1
final-dropout: 0.1
freeze-finetune-updates: 3000

conv-kernel-sizes: 5,5
conv-channels: 512
dropout: 0.1
activation-fn: relu
#encoder-embed-dim: 512
#encoder-ffn-embed-dim: 2048
#encoder-layers: 12
#decoder-layers: 6
#encoder-attention-heads: 8
#
#decoder-embed-dim: 512
#decoder-ffn-embed-dim: 1024
#decoder-attention-heads: 4
attention-dropout: 0.1
activation-dropout: 0.1
