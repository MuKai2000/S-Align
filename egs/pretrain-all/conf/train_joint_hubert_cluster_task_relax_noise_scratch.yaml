train-subset: train_st,train_mt,train_asr
#train-subset: train_asr
valid-subset: dev_st

max-epoch: 100
max-update: 120000

num-workers: 4
#patience: 12
no-progress-bar: True
log-interval: 100
seed: 1
report-accuracy: True
best-checkpoint-metric: ppl

#load-pretrained-encoder-from:
#load-pretrained-decoder-from:

arch: s2t_joint
#arch: s2t_w2v2_ode_sead_s
#w2v2-model-path: /apdcephfs/share_1157259/users/adrienxu/st/wav2vec/wav2vec2.0/wav2vec_small.pt
#w2v2-model-path: /apdcephfs/share_1157259/users/adrienxu/st/wav2vec/wav2vec2.0/wav2vec_small_960h.pt
#w2v2-model-path: /apdcephfs/share_1157259/users/adrienxu/st/wav2vec/wav2vec2.0/wav2vec_vox_960h_pl.pt
#adapter-model-path: /apdcephfs/share_1157259/users/adrienxu/st/adapter-pretrain/sead_joint_checkpoint.pt
#adapter-model-path: /apdcephfs/share_1157259/users/adrienxu/st/adapter-pretrain/sead_kd_joint_checkpoint.pt
#adapter-model-path: /apdcephfs/share_1157259/users/adrienxu/st/adapter-pretrain/sead_filter_joint_checkpoint.pt
#adapter-model-path: /apdcephfs/share_1157259/users/adrienxu/st/adapter-pretrain/sead_mustc_kd_checkpoint_merge_joint.pt
#adapter-model-path: /mnt/zhangyuhao/MSP-ST/fairseq/egs/asr-pretrain/checkpoints/deep_transformer_2conv/avg_5_checkpoint.pt
#adapter-model-path: /mnt/zhangyuhao/MSP-ST/fairseq/egs/asr-pretrain/checkpoints/deep_transformer/avg_3_checkpoint.pt
#w2v-path: /mnt/zhangyuhao/pretrain/wav2vec_small.pt
w2v-path: /mnt/zhangyuhao/pretrain/hubert_base_ls960.pt
#mt-model-path: /mnt/zhangyuhao/MSP-ST/fairseq/egs/machine_translation/checkpoints/wmt-en2de/merge-lcrm/last5.ensemble.pt
mt-model-path: /mnt/zhangyuhao/MSP-ST/fairseq/egs/machine_translation/checkpoints/wmt-en2de/mustc-ende-iwslt-prenorm-conv2-silent/last8.ensemble.pt
decoder-embed-path: /mnt/zhangyuhao/MSP-ST/fairseq/egs/machine_translation/pretrain_embeddings_mustc_ende_iwlst_prenorm_conv2_silent
#cluster-embed-path: /mnt/zhangyuhao/MSP-ST/fairseq/egs/machine_translation/pretrain_embeddings_mustc_ende_iwlst_prenorm_cluster

#w2v2-model-path: /apdcephfs/share_1157259/users/adrienxu/st/wav2vec/wav2vec-tune/checkpoint_last.pt
share-decoder-input-output-embed: True
optimizer: adam
clip-norm: 10.0
lr-scheduler: inverse_sqrt
#lr-scheduler: polynomial_decay
#lr-scheduler: tri_stage
#phase-ratio: 0.1,0.4,0.5
#final-lr-scale: 0.05
warmup-init-lr: 1e-7
warmup-updates: 5000
lr: 2e-4
#weight-decay: 0.0001
adam-betas: (0.9,0.98)
adapter-dim: 4096
adapter-dropout: 0.0

ctc-weight: 0.3
share-ctc-embed: true
share-two-encoders: true
#use-ctc-cluster: true
#max-position-ctc: 200
#position-unit-size: 256
use-ctc-shrink: true 
lookback: true
#avg-shrink: true
use-token-contrastive: true
use-two-contrastive: true
text-conv-kernel: 5
#word-align: true
#use-double-ctc: true
#
#criterion: label_smoothed_cross_entropy
#criterion: label_smoothed_cross_entropy_with_ctc
criterion: label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_cluster
#train-st-without-ctc
label-smoothing: 0.2
contrastive-alpha: 1.0
contrastive-beta: 1.0
contrastive-temperature: 0.1
zero-infinity: true
decrease-step: 3000
post-process: sentencepiece
is-shrink: uniq
ban-cl-step: 50000
#add-proj-norm: true


#wav2vec configuration
#macaron-style: False
use-cnn-module: False
cnn-module-kernel: 31
apply-mask: True
mask-prob: 0.5
mask-channel-prob: 0.25
mask-channel-length: 6
use-ctc-loss: True
add-position-embed: true
add-position-embed-after-ctc: true
adapter-layers: 0
sead-layers: 6
final-dropout: 0.1
freeze-finetune-updates: 3000


conv-kernel-sizes: 5,5
conv-channels: 512
dropout: 0.1
activation-fn: relu
#
attention-dropout: 0.1
activation-dropout: 0.1
