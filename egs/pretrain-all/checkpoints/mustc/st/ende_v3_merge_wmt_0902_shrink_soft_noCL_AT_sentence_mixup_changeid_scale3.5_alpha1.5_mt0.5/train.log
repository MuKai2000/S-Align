2023-09-02 09:51:30 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:13602
2023-09-02 09:51:30 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:13602
2023-09-02 09:51:30 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:13602
2023-09-02 09:51:30 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:13602
2023-09-02 09:51:30 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-09-02 09:51:30 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:13602
2023-09-02 09:51:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-09-02 09:51:31 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:13602
2023-09-02 09:51:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-09-02 09:51:31 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:13602
2023-09-02 09:51:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-09-02 09:51:31 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:13602
2023-09-02 09:51:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-09-02 09:51:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-09-02 09:51:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-09-02 09:51:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-09-02 09:51:31 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 09:51:31 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 0
2023-09-02 09:51:31 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 09:51:31 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 3
2023-09-02 09:51:31 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 09:51:31 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 2
2023-09-02 09:51:31 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 09:51:31 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 09:51:31 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 5
2023-09-02 09:51:31 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 6
2023-09-02 09:51:31 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 09:51:31 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 1
2023-09-02 09:51:31 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 09:51:31 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 4
2023-09-02 09:51:31 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 09:51:31 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 7
2023-09-02 09:51:35 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13602', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-09-02 09:51:35 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-09-02 09:51:35 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-09-02 09:51:35 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-09-02 09:51:35 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-09-02 09:51:35 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-09-02 09:51:39 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-09-02 09:51:39 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-09-02 09:51:39 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-09-02 09:51:41 | INFO | root | load pretrained hubert
2023-09-02 09:51:48 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-09-02 09:51:52 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-09-02 09:51:59 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-09-02 09:51:59 | INFO | root | share the sematic adapter and textual encoder
2023-09-02 09:51:59 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-09-02 09:51:59 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-09-02 09:51:59 | INFO | fairseq_cli.train | model: S2TJoint
2023-09-02 09:51:59 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-09-02 09:51:59 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-09-02 09:51:59 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-09-02 09:51:59 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-02 09:51:59 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 09:51:59 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 09:51:59 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-02 09:52:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-09-02 09:52:14 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-09-02 09:52:14 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-09-02 09:52:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-02 09:52:15 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 09:52:15 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 09:52:15 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 09:52:15 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 09:52:15 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 09:52:15 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 09:52:15 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 09:52:15 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 09:52:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-02 09:52:15 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-09-02 09:52:15 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-09-02 09:52:15 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_last.pt
2023-09-02 09:52:15 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_last.pt
2023-09-02 09:52:15 | INFO | fairseq.trainer | loading train data for epoch 1
2023-09-02 09:52:15 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-02 09:52:15 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 09:52:15 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 09:52:16 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-02 09:52:18 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-02 09:52:58 | INFO | fairseq.optim.adam | using FusedAdam
2023-09-02 09:52:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 09:52:58 | INFO | fairseq.trainer | begin training epoch 1
2023-09-02 09:52:58 | INFO | fairseq_cli.train | Start iterating over samples
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
2023-09-02 09:53:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
2023-09-02 09:54:11 | INFO | train_inner | epoch 001:    101 / 1474 loss=17.39, trans_loss=5.881, nll_loss=4.692, w2v_ctc_loss=22.309, task_loss=4.958, task_loss_gen=4.891, contrastive_loss=0, total=4212.33, n_correct=124.18, ppl=25.84, accuracy=2.948, wps=20753.9, ups=1.66, wpb=12566.1, bsz=472.9, num_updates=100, lr=4.098e-06, gnorm=2.695, clip=0, loss_scale=64, train_wall=66, gb_free=18.8, wall=116
2023-09-02 09:54:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-02 09:55:12 | INFO | train_inner | epoch 001:    202 / 1474 loss=13.544, trans_loss=5.957, nll_loss=4.817, w2v_ctc_loss=16.303, task_loss=5.922, task_loss_gen=5.031, contrastive_loss=0, total=4127.88, n_correct=122.11, ppl=28.19, accuracy=2.958, wps=20377.2, ups=1.65, wpb=12326, bsz=463, num_updates=200, lr=8.096e-06, gnorm=7.325, clip=15, loss_scale=32, train_wall=60, gb_free=18.7, wall=177
2023-09-02 09:56:13 | INFO | train_inner | epoch 001:    302 / 1474 loss=7.368, trans_loss=5.994, nll_loss=4.899, w2v_ctc_loss=6.747, task_loss=7.297, task_loss_gen=5.565, contrastive_loss=0, total=4077.62, n_correct=134.16, ppl=29.83, accuracy=3.29, wps=19999.6, ups=1.64, wpb=12179.5, bsz=437.4, num_updates=300, lr=1.2094e-05, gnorm=1.413, clip=0, loss_scale=32, train_wall=60, gb_free=19.3, wall=238
2023-09-02 09:57:13 | INFO | train_inner | epoch 001:    402 / 1474 loss=6.832, trans_loss=5.91, nll_loss=4.821, w2v_ctc_loss=6.011, task_loss=5.365, task_loss_gen=4.986, contrastive_loss=0, total=4177.45, n_correct=106.36, ppl=28.26, accuracy=2.546, wps=20811.7, ups=1.67, wpb=12474.2, bsz=462.8, num_updates=400, lr=1.6092e-05, gnorm=0.588, clip=0, loss_scale=32, train_wall=59, gb_free=18.7, wall=298
2023-09-02 09:58:13 | INFO | train_inner | epoch 001:    502 / 1474 loss=6.637, trans_loss=5.927, nll_loss=4.85, w2v_ctc_loss=5.69, task_loss=3.572, task_loss_gen=5.099, contrastive_loss=0, total=4202.06, n_correct=66.89, ppl=28.84, accuracy=1.592, wps=20968.6, ups=1.67, wpb=12556.3, bsz=490.7, num_updates=500, lr=2.009e-05, gnorm=0.411, clip=0, loss_scale=32, train_wall=59, gb_free=14.6, wall=358
2023-09-02 09:59:12 | INFO | train_inner | epoch 001:    602 / 1474 loss=6.748, trans_loss=6.306, nll_loss=5.329, w2v_ctc_loss=5.466, task_loss=2.524, task_loss_gen=5.783, contrastive_loss=0, total=4124.52, n_correct=16.89, ppl=40.18, accuracy=0.41, wps=20808.1, ups=1.69, wpb=12301.1, bsz=471, num_updates=600, lr=2.4088e-05, gnorm=0.379, clip=0, loss_scale=32, train_wall=58, gb_free=18.8, wall=417
2023-09-02 10:00:11 | INFO | train_inner | epoch 001:    702 / 1474 loss=6.504, trans_loss=6.276, nll_loss=5.287, w2v_ctc_loss=5.117, task_loss=2.324, task_loss_gen=6.122, contrastive_loss=0, total=4147.01, n_correct=11.6, ppl=39.05, accuracy=0.28, wps=20947.7, ups=1.69, wpb=12381.3, bsz=455.2, num_updates=700, lr=2.8086e-05, gnorm=0.45, clip=0, loss_scale=32, train_wall=58, gb_free=19.1, wall=476
2023-09-02 10:01:10 | INFO | train_inner | epoch 001:    802 / 1474 loss=6.143, trans_loss=6.173, nll_loss=5.151, w2v_ctc_loss=4.67, task_loss=3.297, task_loss_gen=5.431, contrastive_loss=0, total=4121.11, n_correct=6.92, ppl=35.54, accuracy=0.168, wps=20665.8, ups=1.68, wpb=12298.3, bsz=463.4, num_updates=800, lr=3.2084e-05, gnorm=0.745, clip=0, loss_scale=32, train_wall=59, gb_free=19.1, wall=535
2023-09-02 10:02:11 | INFO | train_inner | epoch 001:    902 / 1474 loss=5.918, trans_loss=6.139, nll_loss=5.126, w2v_ctc_loss=4.36, task_loss=2.62, task_loss_gen=6.091, contrastive_loss=0, total=4167.98, n_correct=5.75, ppl=34.92, accuracy=0.138, wps=20480.5, ups=1.65, wpb=12446.6, bsz=457.5, num_updates=900, lr=3.6082e-05, gnorm=0.766, clip=0, loss_scale=32, train_wall=60, gb_free=19, wall=596
2023-09-02 10:03:11 | INFO | train_inner | epoch 001:   1002 / 1474 loss=5.73, trans_loss=6.104, nll_loss=5.08, w2v_ctc_loss=4.104, task_loss=2.668, task_loss_gen=6.134, contrastive_loss=0, total=4136.38, n_correct=11.73, ppl=33.82, accuracy=0.284, wps=20698.7, ups=1.68, wpb=12354.6, bsz=458.8, num_updates=1000, lr=4.008e-05, gnorm=0.955, clip=0, loss_scale=32, train_wall=59, gb_free=19.2, wall=656
2023-09-02 10:04:10 | INFO | train_inner | epoch 001:   1102 / 1474 loss=5.655, trans_loss=6.163, nll_loss=5.154, w2v_ctc_loss=3.93, task_loss=3.87, task_loss_gen=5.453, contrastive_loss=0, total=4148.31, n_correct=12.04, ppl=35.6, accuracy=0.29, wps=20801.9, ups=1.68, wpb=12371.7, bsz=453.4, num_updates=1100, lr=4.4078e-05, gnorm=0.959, clip=0, loss_scale=32, train_wall=59, gb_free=18.6, wall=715
2023-09-02 10:04:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-02 10:05:10 | INFO | train_inner | epoch 001:   1203 / 1474 loss=5.541, trans_loss=6.111, nll_loss=5.087, w2v_ctc_loss=3.802, task_loss=4.634, task_loss_gen=5.893, contrastive_loss=0, total=4116.39, n_correct=8.71, ppl=34, accuracy=0.212, wps=20398.2, ups=1.66, wpb=12295.1, bsz=430, num_updates=1200, lr=4.8076e-05, gnorm=1.028, clip=0, loss_scale=16, train_wall=60, gb_free=18.8, wall=776
2023-09-02 10:06:10 | INFO | train_inner | epoch 001:   1303 / 1474 loss=5.357, trans_loss=5.984, nll_loss=4.953, w2v_ctc_loss=3.656, task_loss=3.814, task_loss_gen=5.715, contrastive_loss=0, total=4055.88, n_correct=17.39, ppl=30.96, accuracy=0.429, wps=20407.9, ups=1.69, wpb=12109.1, bsz=443.9, num_updates=1300, lr=5.2074e-05, gnorm=1.019, clip=0, loss_scale=16, train_wall=59, gb_free=19.2, wall=835
2023-09-02 10:07:11 | INFO | train_inner | epoch 001:   1403 / 1474 loss=5.366, trans_loss=6.118, nll_loss=5.109, w2v_ctc_loss=3.531, task_loss=3.204, task_loss_gen=5.946, contrastive_loss=0, total=4127.47, n_correct=31.34, ppl=34.51, accuracy=0.759, wps=20318.2, ups=1.65, wpb=12332.8, bsz=452, num_updates=1400, lr=5.6072e-05, gnorm=1.037, clip=0, loss_scale=16, train_wall=60, gb_free=19.1, wall=896
2023-09-02 10:07:53 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2023-09-02 10:08:40 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 10.369 | trans_loss 13.351 | nll_loss 13.16 | w2v_ctc_loss 4.478 | task_loss 10.097 | task_loss_gen 32.857 | contrastive_loss 0 | total 4003.4 | n_correct 0.7 | ppl 9153.17 | accuracy 0.017 | uer 60.083 | wer 58.704 | raw_wer 58.704 | bleu 0 | wps 1039.8 | wpb 4003.4 | bsz 141.8 | num_updates 1471
2023-09-02 10:08:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1471 updates
2023-09-02 10:08:40 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 10:08:42 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 10:08:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 1 @ 1471 updates, score 0.0) (writing took 4.204932773980545 seconds)
2023-09-02 10:08:44 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-09-02 10:08:44 | INFO | train | epoch 001 | loss 7.388 | trans_loss 6.074 | nll_loss 5.027 | w2v_ctc_loss 6.693 | task_loss 3.935 | task_loss_gen 5.587 | contrastive_loss 0 | total 4138.13 | n_correct 46.5479 | ppl 32.6 | accuracy 1.125 | wps 19476.4 | ups 1.58 | wpb 12354.2 | bsz 458.2 | num_updates 1471 | lr 5.89106e-05 | gnorm 1.406 | clip 1 | loss_scale 16 | train_wall 878 | gb_free 18.9 | wall 989
2023-09-02 10:08:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 10:08:44 | INFO | fairseq.trainer | begin training epoch 2
2023-09-02 10:08:44 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 10:09:08 | INFO | train_inner | epoch 002:     29 / 1474 loss=5.242, trans_loss=6.043, nll_loss=5.022, w2v_ctc_loss=3.419, task_loss=2.537, task_loss_gen=6, contrastive_loss=0, total=4165.52, n_correct=14.93, ppl=32.5, accuracy=0.358, wps=10580.1, ups=0.85, wpb=12425.3, bsz=471.4, num_updates=1500, lr=6.007e-05, gnorm=1.263, clip=0, loss_scale=16, train_wall=59, gb_free=18.7, wall=1013
2023-09-02 10:10:07 | INFO | train_inner | epoch 002:    129 / 1474 loss=5.177, trans_loss=6.014, nll_loss=4.97, w2v_ctc_loss=3.346, task_loss=2.267, task_loss_gen=6.504, contrastive_loss=0, total=4149.27, n_correct=28.12, ppl=31.35, accuracy=0.678, wps=20910.6, ups=1.69, wpb=12375.1, bsz=451.7, num_updates=1600, lr=6.4068e-05, gnorm=1.027, clip=0, loss_scale=16, train_wall=58, gb_free=18.7, wall=1072
2023-09-02 10:11:06 | INFO | train_inner | epoch 002:    229 / 1474 loss=5.166, trans_loss=6.114, nll_loss=5.114, w2v_ctc_loss=3.228, task_loss=1.893, task_loss_gen=5.585, contrastive_loss=0, total=4199.2, n_correct=16.01, ppl=34.64, accuracy=0.381, wps=21192, ups=1.69, wpb=12541.6, bsz=494.4, num_updates=1700, lr=6.8066e-05, gnorm=1.048, clip=0, loss_scale=16, train_wall=58, gb_free=18.8, wall=1132
2023-09-02 10:12:06 | INFO | train_inner | epoch 002:    329 / 1474 loss=5.196, trans_loss=6.183, nll_loss=5.192, w2v_ctc_loss=3.191, task_loss=3.114, task_loss_gen=6.326, contrastive_loss=0, total=4130.92, n_correct=11.49, ppl=36.55, accuracy=0.278, wps=20813.3, ups=1.69, wpb=12331.6, bsz=442.3, num_updates=1800, lr=7.2064e-05, gnorm=1.058, clip=0, loss_scale=16, train_wall=59, gb_free=18.6, wall=1191
2023-09-02 10:13:05 | INFO | train_inner | epoch 002:    429 / 1474 loss=5.103, trans_loss=6.094, nll_loss=5.084, w2v_ctc_loss=3.141, task_loss=2.208, task_loss_gen=7.56, contrastive_loss=0, total=4036.18, n_correct=10.66, ppl=33.92, accuracy=0.264, wps=20227.8, ups=1.68, wpb=12064.4, bsz=416.3, num_updates=1900, lr=7.6062e-05, gnorm=0.922, clip=0, loss_scale=16, train_wall=59, gb_free=18.9, wall=1250
2023-09-02 10:14:05 | INFO | train_inner | epoch 002:    529 / 1474 loss=5.011, trans_loss=6.065, nll_loss=5.041, w2v_ctc_loss=3.036, task_loss=1.743, task_loss_gen=6.617, contrastive_loss=0, total=4185.63, n_correct=21.64, ppl=32.93, accuracy=0.517, wps=20795.4, ups=1.67, wpb=12487.7, bsz=470.3, num_updates=2000, lr=8.006e-05, gnorm=0.929, clip=0, loss_scale=16, train_wall=59, gb_free=18.7, wall=1310
2023-09-02 10:14:05 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 10:14:52 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 10.632 | trans_loss 13.993 | nll_loss 13.89 | w2v_ctc_loss 3.907 | task_loss 24.843 | task_loss_gen 26.502 | contrastive_loss 0 | total 4003.4 | n_correct 2.5 | ppl 15183.6 | accuracy 0.062 | uer 54.317 | wer 53.626 | raw_wer 53.626 | bleu 0 | wps 1040.5 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_bleu 0
2023-09-02 10:14:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-09-02 10:14:52 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_2_2000.pt
2023-09-02 10:14:54 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_2_2000.pt
2023-09-02 10:15:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.0) (writing took 11.982410583004821 seconds)
--------------------
Before gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0542, -0.3477, -0.1787,  ...,  0.0038, -0.0464, -0.0202],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0082, -0.1965, -0.0265,  ..., -0.0425, -0.0249, -0.0222]],
       device='cuda:0', dtype=torch.float16)
task_net layer_norm.weight True tensor([-6.5352, -5.7031, -0.5132, -0.4238,  4.0898], device='cuda:0',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.2205e-02, -4.7046e-01, -2.2095e-01,  ...,  1.4534e-03,
         -6.2744e-02, -2.6611e-02],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.9291e-04, -2.0471e-01, -6.1462e-02,  ..., -5.3009e-02,
         -4.0833e-02, -3.6163e-02]], device='cuda:0', dtype=torch.float16)
task_net layer_norm.weight True tensor([-6.5352, -5.7031, -0.5132, -0.4238,  4.0898], device='cuda:0',
       dtype=torch.float16)
--------------------
2023-09-02 10:16:03 | INFO | train_inner | epoch 002:    629 / 1474 loss=5.024, trans_loss=6.143, nll_loss=5.138, w2v_ctc_loss=2.972, task_loss=1.397, task_loss_gen=7.813, contrastive_loss=0, total=4116.05, n_correct=22.07, ppl=35.21, accuracy=0.536, wps=10429, ups=0.85, wpb=12285, bsz=443.4, num_updates=2100, lr=8.4058e-05, gnorm=0.973, clip=0, loss_scale=16, train_wall=58, gb_free=19.5, wall=1428
2023-09-02 10:17:02 | INFO | train_inner | epoch 002:    729 / 1474 loss=4.971, trans_loss=6.108, nll_loss=5.091, w2v_ctc_loss=2.934, task_loss=1.23, task_loss_gen=7.85, contrastive_loss=0, total=4152.4, n_correct=12.9, ppl=34.09, accuracy=0.311, wps=20965.9, ups=1.69, wpb=12393.8, bsz=463.6, num_updates=2200, lr=8.8056e-05, gnorm=0.888, clip=0, loss_scale=16, train_wall=58, gb_free=18.8, wall=1487
2023-09-02 10:18:02 | INFO | train_inner | epoch 002:    829 / 1474 loss=4.942, trans_loss=6.097, nll_loss=5.09, w2v_ctc_loss=2.898, task_loss=0.911, task_loss_gen=8.947, contrastive_loss=0, total=4168.87, n_correct=16.74, ppl=34.05, accuracy=0.402, wps=20884.1, ups=1.68, wpb=12453.5, bsz=461.2, num_updates=2300, lr=9.2054e-05, gnorm=0.814, clip=0, loss_scale=16, train_wall=59, gb_free=18.6, wall=1547
2023-09-02 10:19:01 | INFO | train_inner | epoch 002:    929 / 1474 loss=4.958, trans_loss=6.18, nll_loss=5.185, w2v_ctc_loss=2.834, task_loss=0.734, task_loss_gen=9.703, contrastive_loss=0, total=4104.79, n_correct=7.64, ppl=36.38, accuracy=0.186, wps=20691.5, ups=1.69, wpb=12254.8, bsz=445.6, num_updates=2400, lr=9.6052e-05, gnorm=0.815, clip=0, loss_scale=16, train_wall=59, gb_free=18.8, wall=1606
2023-09-02 10:20:01 | INFO | train_inner | epoch 002:   1029 / 1474 loss=4.828, trans_loss=6.025, nll_loss=4.993, w2v_ctc_loss=2.795, task_loss=0.617, task_loss_gen=10.215, contrastive_loss=0, total=4100.85, n_correct=8.29, ppl=31.83, accuracy=0.202, wps=20589.3, ups=1.68, wpb=12245.2, bsz=455.2, num_updates=2500, lr=0.00010005, gnorm=0.755, clip=0, loss_scale=16, train_wall=59, gb_free=19.2, wall=1666
2023-09-02 10:21:00 | INFO | train_inner | epoch 002:   1129 / 1474 loss=4.872, trans_loss=6.161, nll_loss=5.165, w2v_ctc_loss=2.729, task_loss=0.388, task_loss_gen=10.059, contrastive_loss=0, total=4195.47, n_correct=6.59, ppl=35.88, accuracy=0.157, wps=20997.2, ups=1.68, wpb=12522.7, bsz=489.6, num_updates=2600, lr=0.000104048, gnorm=0.712, clip=0, loss_scale=16, train_wall=59, gb_free=18.8, wall=1725
2023-09-02 10:22:00 | INFO | train_inner | epoch 002:   1229 / 1474 loss=4.847, trans_loss=6.144, nll_loss=5.133, w2v_ctc_loss=2.708, task_loss=0.317, task_loss_gen=10.882, contrastive_loss=0, total=4220.45, n_correct=11.4, ppl=35.09, accuracy=0.27, wps=21159.2, ups=1.68, wpb=12591.7, bsz=492, num_updates=2700, lr=0.000108046, gnorm=0.654, clip=0, loss_scale=16, train_wall=59, gb_free=19.6, wall=1785
2023-09-02 10:22:59 | INFO | train_inner | epoch 002:   1329 / 1474 loss=4.899, trans_loss=6.235, nll_loss=5.259, w2v_ctc_loss=2.683, task_loss=0.178, task_loss_gen=13.601, contrastive_loss=0, total=4159.97, n_correct=11.97, ppl=38.29, accuracy=0.288, wps=21114.5, ups=1.7, wpb=12433.6, bsz=462.1, num_updates=2800, lr=0.000112044, gnorm=0.62, clip=0, loss_scale=16, train_wall=58, gb_free=19.4, wall=1844
2023-09-02 10:23:58 | INFO | train_inner | epoch 002:   1429 / 1474 loss=4.799, trans_loss=6.112, nll_loss=5.093, w2v_ctc_loss=2.662, task_loss=0.154, task_loss_gen=16.343, contrastive_loss=0, total=4050.6, n_correct=28.57, ppl=34.13, accuracy=0.705, wps=20443.3, ups=1.69, wpb=12095, bsz=438.3, num_updates=2900, lr=0.000116042, gnorm=0.701, clip=0, loss_scale=16, train_wall=58, gb_free=19.5, wall=1903
2023-09-02 10:24:24 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--------------------
Before gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0060, -0.0616, -0.0977,  ...,  0.0084, -0.0845,  0.0063],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0526, -0.1096, -1.0527,  ..., -0.0022,  0.0019, -0.0623]],
       device='cuda:4', dtype=torch.float16)
task_net layer_norm.weight True tensor([-7.5664, -6.5547, -0.6260, -0.5454,  4.0352], device='cuda:4',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0053, -0.0701, -0.1223,  ...,  0.0100, -0.1046,  0.0124],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0599, -0.1478, -1.1465,  ..., -0.0116, -0.0069, -0.0741]],
       device='cuda:4', dtype=torch.float16)
task_net layer_norm.weight True tensor([-7.5664, -6.5547, -0.6260, -0.5454,  4.0352], device='cuda:4',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0394, -0.1912, -0.0797,  ...,  0.0017, -0.0762, -0.0146],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1180, -0.4666, -0.9478,  ..., -0.0654, -0.0544, -0.1050]],
       device='cuda:1', dtype=torch.float16)
task_net layer_norm.weight True tensor([-10.9844,  -9.1641,  -0.6855,  -0.7856,   5.7539], device='cuda:1',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0475, -0.2219, -0.1118,  ...,  0.0034, -0.0876, -0.0136],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1473, -0.5957, -1.0791,  ..., -0.0736, -0.0640, -0.1261]],
       device='cuda:1', dtype=torch.float16)
task_net layer_norm.weight True tensor([-10.9844,  -9.1641,  -0.6855,  -0.7856,   5.7539], device='cuda:1',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0292, -0.2028,  0.2881,  ..., -0.0087, -0.0425, -0.0404],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0097,  0.3740,  0.2537,  ...,  0.0036, -0.0426, -0.0202]],
       device='cuda:5', dtype=torch.float16)
task_net layer_norm.weight True tensor([-5.4023, -4.6680, -0.4622, -0.4395,  3.1738], device='cuda:5',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0370, -0.2490,  0.3569,  ..., -0.0108, -0.0535, -0.0512],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0088,  0.4912,  0.3423,  ...,  0.0017, -0.0538, -0.0233]],
       device='cuda:5', dtype=torch.float16)
task_net layer_norm.weight True tensor([-5.4023, -4.6680, -0.4622, -0.4395,  3.1738], device='cuda:5',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0069, -0.1576, -0.0402,  ...,  0.0062, -0.0414, -0.0279],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0388, -0.0328, -0.0881,  ..., -0.0350, -0.0151, -0.0134]],
       device='cuda:7', dtype=torch.float16)
task_net layer_norm.weight True tensor([-2.1309, -2.2812, -0.2083, -0.1624,  1.6328], device='cuda:7',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0098, -0.2147, -0.0213,  ...,  0.0105, -0.0587, -0.0411],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0637, -0.0417, -0.1191,  ..., -0.0530, -0.0218, -0.0162]],
       device='cuda:7', dtype=torch.float16)
task_net layer_norm.weight True tensor([-2.1309, -2.2812, -0.2083, -0.1624,  1.6328], device='cuda:7',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0256, -0.1433,  0.0760,  ...,  0.0189, -0.0282, -0.0197],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0046, -0.2666, -0.2128,  ..., -0.0245, -0.0183, -0.0283]],
       device='cuda:3', dtype=torch.float16)
task_net layer_norm.weight True tensor([-2.5371, -2.8594, -0.2443, -0.2412,  1.7354], device='cuda:3',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0338, -0.1882,  0.0915,  ...,  0.0261, -0.0364, -0.0300],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0088, -0.3577, -0.2739,  ..., -0.0308, -0.0232, -0.0353]],
       device='cuda:3', dtype=torch.float16)
task_net layer_norm.weight True tensor([-2.5371, -2.8594, -0.2443, -0.2412,  1.7354], device='cuda:3',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0058, -0.1059, -0.0169,  ...,  0.0097, -0.0464, -0.0131],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0483,  0.0386,  0.0148,  ..., -0.0134, -0.0157, -0.0063]],
       device='cuda:6', dtype=torch.float16)
task_net layer_norm.weight True tensor([-1.9736, -2.0996, -0.1998, -0.1860,  1.3418], device='cuda:6',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0038, -0.1364, -0.0245,  ...,  0.0133, -0.0611, -0.0153],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0652,  0.0408,  0.0160,  ..., -0.0181, -0.0239, -0.0053]],
       device='cuda:6', dtype=torch.float16)
task_net layer_norm.weight True tensor([-1.9736, -2.0996, -0.1998, -0.1860,  1.3418], device='cuda:6',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0253, -0.1763,  0.1232,  ...,  0.0014, -0.0634, -0.0195],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0330, -0.3936, -0.2166,  ..., -0.0673, -0.0080, -0.0508]],
       device='cuda:2', dtype=torch.float16)
task_net layer_norm.weight True tensor([-3.3574, -3.2383, -0.3018, -0.2307,  2.0332], device='cuda:2',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0300, -0.2335,  0.1689,  ...,  0.0032, -0.0843, -0.0240],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0367, -0.5132, -0.2766,  ..., -0.0884, -0.0123, -0.0686]],
       device='cuda:2', dtype=torch.float16)
task_net layer_norm.weight True tensor([-3.3574, -3.2383, -0.3018, -0.2307,  2.0332], device='cuda:2',
       dtype=torch.float16)
--------------------
2023-09-02 10:25:11 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 10.21 | trans_loss 13.595 | nll_loss 13.407 | w2v_ctc_loss 3.397 | task_loss 1.232 | task_loss_gen 66.695 | contrastive_loss 0 | total 4003.4 | n_correct 32.4 | ppl 10862.6 | accuracy 0.809 | uer 48.494 | wer 47.321 | raw_wer 47.321 | bleu 0 | wps 1040.8 | wpb 4003.4 | bsz 141.8 | num_updates 2945 | best_bleu 0
2023-09-02 10:25:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2945 updates
2023-09-02 10:25:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 10:25:18 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 10:25:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 2 @ 2945 updates, score 0.0) (writing took 13.36681328600389 seconds)
2023-09-02 10:25:25 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-09-02 10:25:25 | INFO | train | epoch 002 | loss 4.983 | trans_loss 6.117 | nll_loss 5.108 | w2v_ctc_loss 2.939 | task_loss 1.196 | task_loss_gen 9.273 | contrastive_loss 0 | total 4138.65 | n_correct 15.384 | ppl 34.48 | accuracy 0.372 | wps 18194 | ups 1.47 | wpb 12355.8 | bsz 458.5 | num_updates 2945 | lr 0.000117841 | gnorm 0.852 | clip 0 | loss_scale 16 | train_wall 864 | gb_free 19 | wall 1990
2023-09-02 10:25:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 10:25:25 | INFO | fairseq.trainer | begin training epoch 3
2023-09-02 10:25:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 10:26:05 | INFO | train_inner | epoch 003:     55 / 1474 loss=4.785, trans_loss=6.13, nll_loss=5.126, w2v_ctc_loss=2.613, task_loss=0.102, task_loss_gen=16.797, contrastive_loss=0, total=4066.57, n_correct=15, ppl=34.91, accuracy=0.369, wps=9554.1, ups=0.79, wpb=12139.2, bsz=441.1, num_updates=3000, lr=0.00012004, gnorm=0.65, clip=0, loss_scale=16, train_wall=59, gb_free=18.7, wall=2030
2023-09-02 10:26:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-09-02 10:26:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-09-02 10:27:42 | INFO | train_inner | epoch 003:    157 / 1474 loss=3.836, trans_loss=5.067, nll_loss=3.758, w2v_ctc_loss=2.279, task_loss=2.784, task_loss_gen=8.447, contrastive_loss=0, total=4137.54, n_correct=470.63, ppl=13.53, accuracy=11.375, wps=12710.5, ups=1.03, wpb=12351.9, bsz=456.2, num_updates=3100, lr=0.000124038, gnorm=1.411, clip=0, loss_scale=4, train_wall=96, gb_free=16.7, wall=2127
2023-09-02 10:29:19 | INFO | train_inner | epoch 003:    257 / 1474 loss=3.142, trans_loss=4.291, nll_loss=2.73, w2v_ctc_loss=2.031, task_loss=1.656, task_loss_gen=7.568, contrastive_loss=0, total=4146.68, n_correct=1147.69, ppl=6.63, accuracy=27.677, wps=12773.5, ups=1.03, wpb=12391.1, bsz=461.7, num_updates=3200, lr=0.000128036, gnorm=1.073, clip=0, loss_scale=4, train_wall=96, gb_free=14.7, wall=2224
2023-09-02 10:30:55 | INFO | train_inner | epoch 003:    357 / 1474 loss=3.06, trans_loss=4.269, nll_loss=2.7, w2v_ctc_loss=1.94, task_loss=1.708, task_loss_gen=8.815, contrastive_loss=0, total=4171.72, n_correct=1201.14, ppl=6.5, accuracy=28.792, wps=13027.3, ups=1.05, wpb=12449, bsz=468.1, num_updates=3300, lr=0.000132034, gnorm=1.118, clip=0, loss_scale=4, train_wall=95, gb_free=17.2, wall=2320
2023-09-02 10:32:32 | INFO | train_inner | epoch 003:    457 / 1474 loss=2.993, trans_loss=4.242, nll_loss=2.664, w2v_ctc_loss=1.868, task_loss=1.847, task_loss_gen=7.351, contrastive_loss=0, total=4200.59, n_correct=1258.41, ppl=6.34, accuracy=29.958, wps=12926.8, ups=1.03, wpb=12537.4, bsz=475.4, num_updates=3400, lr=0.000136032, gnorm=1.001, clip=0, loss_scale=4, train_wall=96, gb_free=11.9, wall=2417
2023-09-02 10:34:08 | INFO | train_inner | epoch 003:    557 / 1474 loss=2.951, trans_loss=4.249, nll_loss=2.675, w2v_ctc_loss=1.802, task_loss=2.048, task_loss_gen=8.928, contrastive_loss=0, total=4093.13, n_correct=1222.29, ppl=6.39, accuracy=29.862, wps=12715.1, ups=1.04, wpb=12228.7, bsz=440.1, num_updates=3500, lr=0.00014003, gnorm=0.977, clip=0, loss_scale=4, train_wall=95, gb_free=17.2, wall=2513
2023-09-02 10:35:45 | INFO | train_inner | epoch 003:    657 / 1474 loss=2.891, trans_loss=4.232, nll_loss=2.648, w2v_ctc_loss=1.733, task_loss=2.538, task_loss_gen=7.124, contrastive_loss=0, total=4222.97, n_correct=1295.87, ppl=6.27, accuracy=30.686, wps=12895.3, ups=1.02, wpb=12591.9, bsz=483.6, num_updates=3600, lr=0.000144028, gnorm=0.945, clip=0, loss_scale=4, train_wall=97, gb_free=16.6, wall=2611
2023-09-02 10:37:21 | INFO | train_inner | epoch 003:    757 / 1474 loss=2.86, trans_loss=4.207, nll_loss=2.621, w2v_ctc_loss=1.713, task_loss=2.318, task_loss_gen=7.079, contrastive_loss=0, total=4164.5, n_correct=1310.63, ppl=6.15, accuracy=31.471, wps=12974.3, ups=1.04, wpb=12438.4, bsz=470.6, num_updates=3700, lr=0.000148026, gnorm=0.972, clip=0, loss_scale=4, train_wall=95, gb_free=16.4, wall=2706
2023-09-02 10:38:57 | INFO | train_inner | epoch 003:    857 / 1474 loss=2.836, trans_loss=4.216, nll_loss=2.63, w2v_ctc_loss=1.672, task_loss=2.854, task_loss_gen=8.295, contrastive_loss=0, total=4165.03, n_correct=1309.47, ppl=6.19, accuracy=31.44, wps=12959.6, ups=1.04, wpb=12436.6, bsz=457.7, num_updates=3800, lr=0.000152024, gnorm=0.931, clip=0, loss_scale=4, train_wall=95, gb_free=14.2, wall=2802
2023-09-02 10:40:34 | INFO | train_inner | epoch 003:    957 / 1474 loss=2.811, trans_loss=4.199, nll_loss=2.607, w2v_ctc_loss=1.651, task_loss=2.253, task_loss_gen=6.845, contrastive_loss=0, total=4159.73, n_correct=1336.27, ppl=6.09, accuracy=32.124, wps=12858.2, ups=1.04, wpb=12409.3, bsz=467.1, num_updates=3900, lr=0.000156022, gnorm=0.902, clip=0, loss_scale=4, train_wall=96, gb_free=16.7, wall=2899
2023-09-02 10:42:10 | INFO | train_inner | epoch 003:   1057 / 1474 loss=2.808, trans_loss=4.198, nll_loss=2.609, w2v_ctc_loss=1.642, task_loss=2.461, task_loss_gen=7.489, contrastive_loss=0, total=4059.97, n_correct=1303.84, ppl=6.1, accuracy=32.115, wps=12658.8, ups=1.04, wpb=12124.1, bsz=438.4, num_updates=4000, lr=0.00016002, gnorm=0.914, clip=0, loss_scale=4, train_wall=95, gb_free=17.1, wall=2995
2023-09-02 10:42:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 10:42:43 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.583 | trans_loss 7.428 | nll_loss 5.356 | w2v_ctc_loss 1.868 | task_loss 6.501 | task_loss_gen 18.378 | contrastive_loss 0 | total 4003.4 | n_correct 1355.6 | ppl 40.95 | accuracy 33.861 | uer 28.248 | wer 29.287 | raw_wer 29.287 | bleu 0.23 | wps 1611.9 | wpb 4003.4 | bsz 141.8 | num_updates 4000 | best_bleu 0.23
2023-09-02 10:42:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-09-02 10:42:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_3_4000.pt
2023-09-02 10:42:46 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_3_4000.pt
2023-09-02 10:43:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 0.23) (writing took 17.97398792702006 seconds)
2023-09-02 10:44:36 | INFO | train_inner | epoch 003:   1157 / 1474 loss=2.784, trans_loss=4.207, nll_loss=2.618, w2v_ctc_loss=1.605, task_loss=2.211, task_loss_gen=7.694, contrastive_loss=0, total=4048.71, n_correct=1296.54, ppl=6.14, accuracy=32.024, wps=8248.9, ups=0.68, wpb=12082.9, bsz=437, num_updates=4100, lr=0.000164018, gnorm=0.876, clip=0, loss_scale=4, train_wall=95, gb_free=16.3, wall=3141
2023-09-02 10:46:11 | INFO | train_inner | epoch 003:   1257 / 1474 loss=2.755, trans_loss=4.186, nll_loss=2.593, w2v_ctc_loss=1.577, task_loss=2.437, task_loss_gen=6.526, contrastive_loss=0, total=4063.12, n_correct=1320.83, ppl=6.04, accuracy=32.508, wps=12776.1, ups=1.05, wpb=12134.3, bsz=433.7, num_updates=4200, lr=0.000168016, gnorm=0.881, clip=0, loss_scale=4, train_wall=94, gb_free=13, wall=3236
2023-09-02 10:47:48 | INFO | train_inner | epoch 003:   1357 / 1474 loss=2.732, trans_loss=4.181, nll_loss=2.587, w2v_ctc_loss=1.545, task_loss=2.531, task_loss_gen=6.217, contrastive_loss=0, total=4141.08, n_correct=1356.19, ppl=6.01, accuracy=32.75, wps=12781.4, ups=1.03, wpb=12364.3, bsz=463.4, num_updates=4300, lr=0.000172014, gnorm=0.851, clip=0, loss_scale=4, train_wall=96, gb_free=16.9, wall=3333
2023-09-02 10:49:24 | INFO | train_inner | epoch 003:   1457 / 1474 loss=2.709, trans_loss=4.165, nll_loss=2.568, w2v_ctc_loss=1.527, task_loss=2.727, task_loss_gen=5.291, contrastive_loss=0, total=4212.48, n_correct=1396.9, ppl=5.93, accuracy=33.161, wps=13039.8, ups=1.04, wpb=12581.1, bsz=478.2, num_updates=4400, lr=0.000176012, gnorm=0.841, clip=0, loss_scale=4, train_wall=96, gb_free=16.4, wall=3429
2023-09-02 10:49:40 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 10:50:16 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.509 | trans_loss 7.387 | nll_loss 5.306 | w2v_ctc_loss 1.717 | task_loss 10.622 | task_loss_gen 16.826 | contrastive_loss 0 | total 4003.4 | n_correct 1383.8 | ppl 39.56 | accuracy 34.566 | uer 27.343 | wer 28.508 | raw_wer 28.508 | bleu 0.18 | wps 1401.3 | wpb 4003.4 | bsz 141.8 | num_updates 4417 | best_bleu 0.23
2023-09-02 10:50:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4417 updates
2023-09-02 10:50:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_0.1807.pt
2023-09-02 10:50:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_0.1807.pt
2023-09-02 10:50:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_0.1807.pt (epoch 3 @ 4417 updates, score 0.18) (writing took 7.162534937000601 seconds)
2023-09-02 10:50:24 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-09-02 10:50:24 | INFO | train | epoch 003 | loss 3.007 | trans_loss 4.347 | nll_loss 2.803 | w2v_ctc_loss 1.785 | task_loss 2.234 | task_loss_gen 7.742 | contrastive_loss 0 | total 4138.93 | n_correct 1185.93 | ppl 6.98 | accuracy 28.653 | wps 12132.8 | ups 0.98 | wpb 12356.6 | bsz 458.7 | num_updates 4417 | lr 0.000176692 | gnorm 0.964 | clip 0 | loss_scale 4 | train_wall 1387 | gb_free 16.1 | wall 3489
2023-09-02 10:50:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 10:50:24 | INFO | fairseq.trainer | begin training epoch 4
2023-09-02 10:50:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 10:51:50 | INFO | train_inner | epoch 004:     83 / 1474 loss=2.68, trans_loss=4.155, nll_loss=2.553, w2v_ctc_loss=1.482, task_loss=2.943, task_loss_gen=5.577, contrastive_loss=0, total=4088.42, n_correct=1359.5, ppl=5.87, accuracy=33.252, wps=8354.5, ups=0.68, wpb=12204.1, bsz=437, num_updates=4500, lr=0.00018001, gnorm=0.811, clip=0, loss_scale=4, train_wall=94, gb_free=15.9, wall=3576
2023-09-02 10:53:26 | INFO | train_inner | epoch 004:    183 / 1474 loss=2.664, trans_loss=4.137, nll_loss=2.531, w2v_ctc_loss=1.47, task_loss=4.109, task_loss_gen=4.339, contrastive_loss=0, total=4183.38, n_correct=1403.72, ppl=5.78, accuracy=33.555, wps=13103.6, ups=1.05, wpb=12489.3, bsz=469.7, num_updates=4600, lr=0.000184008, gnorm=0.869, clip=0, loss_scale=4, train_wall=95, gb_free=16.2, wall=3671
2023-09-02 10:54:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-09-02 10:54:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-09-02 10:54:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-09-02 10:55:05 | INFO | train_inner | epoch 004:    286 / 1474 loss=2.709, trans_loss=4.168, nll_loss=2.573, w2v_ctc_loss=1.512, task_loss=6.384, task_loss_gen=4.822, contrastive_loss=0, total=4146.17, n_correct=1354.92, ppl=5.95, accuracy=32.679, wps=12484.1, ups=1.01, wpb=12384.6, bsz=462.2, num_updates=4700, lr=0.000188006, gnorm=2.374, clip=0, loss_scale=0.5, train_wall=98, gb_free=15.8, wall=3770
2023-09-02 10:56:40 | INFO | train_inner | epoch 004:    386 / 1474 loss=2.708, trans_loss=4.301, nll_loss=2.737, w2v_ctc_loss=1.496, task_loss=12.73, task_loss_gen=7.975, contrastive_loss=0, total=4120.11, n_correct=1328.57, ppl=6.67, accuracy=32.246, wps=12922.7, ups=1.05, wpb=12293.6, bsz=440.8, num_updates=4800, lr=0.000192004, gnorm=3.691, clip=0, loss_scale=0.5, train_wall=94, gb_free=17.2, wall=3865
2023-09-02 10:58:16 | INFO | train_inner | epoch 004:    486 / 1474 loss=2.699, trans_loss=4.34, nll_loss=2.782, w2v_ctc_loss=1.497, task_loss=7.841, task_loss_gen=4.898, contrastive_loss=0, total=4223.31, n_correct=1371.63, ppl=6.88, accuracy=32.478, wps=13078.9, ups=1.04, wpb=12605.5, bsz=500.9, num_updates=4900, lr=0.000196002, gnorm=3.132, clip=0, loss_scale=0.5, train_wall=95, gb_free=15.7, wall=3962
2023-09-02 10:59:53 | INFO | train_inner | epoch 004:    586 / 1474 loss=2.663, trans_loss=4.153, nll_loss=2.552, w2v_ctc_loss=1.47, task_loss=4.549, task_loss_gen=4.17, contrastive_loss=0, total=4228.66, n_correct=1422.43, ppl=5.87, accuracy=33.638, wps=13066.8, ups=1.04, wpb=12623.9, bsz=488.4, num_updates=5000, lr=0.0002, gnorm=1.652, clip=0, loss_scale=0.5, train_wall=96, gb_free=15.1, wall=4058
mt_weight tensor(0.5000)
asr_weight tensor(0.4071, device='cuda:0')
2023-09-02 11:01:32 | INFO | train_inner | epoch 004:    686 / 1474 loss=2.679, trans_loss=4.155, nll_loss=2.55, w2v_ctc_loss=1.488, task_loss=4.849, task_loss_gen=4.399, contrastive_loss=0, total=4172.53, n_correct=1402.67, ppl=5.86, accuracy=33.617, wps=12607.8, ups=1.01, wpb=12437.1, bsz=453.2, num_updates=5100, lr=0.00019803, gnorm=1.859, clip=0, loss_scale=0.5, train_wall=98, gb_free=16.8, wall=4157
2023-09-02 11:03:07 | INFO | train_inner | epoch 004:    786 / 1474 loss=2.713, trans_loss=4.15, nll_loss=2.551, w2v_ctc_loss=1.538, task_loss=4.5, task_loss_gen=4.354, contrastive_loss=0, total=4019.56, n_correct=1339.69, ppl=5.86, accuracy=33.329, wps=12525.5, ups=1.04, wpb=12004.8, bsz=419.9, num_updates=5200, lr=0.000196116, gnorm=1.832, clip=1, loss_scale=0.5, train_wall=95, gb_free=14.6, wall=4253
2023-09-02 11:04:44 | INFO | train_inner | epoch 004:    886 / 1474 loss=2.681, trans_loss=4.128, nll_loss=2.522, w2v_ctc_loss=1.511, task_loss=4.009, task_loss_gen=3.906, contrastive_loss=0, total=4183.92, n_correct=1423.67, ppl=5.74, accuracy=34.027, wps=12926.9, ups=1.03, wpb=12492.9, bsz=465.6, num_updates=5300, lr=0.000194257, gnorm=1.491, clip=0, loss_scale=0.5, train_wall=96, gb_free=17.3, wall=4349
2023-09-02 11:06:21 | INFO | train_inner | epoch 004:    986 / 1474 loss=2.715, trans_loss=4.118, nll_loss=2.51, w2v_ctc_loss=1.57, task_loss=4.037, task_loss_gen=3.997, contrastive_loss=0, total=4126.25, n_correct=1420.84, ppl=5.7, accuracy=34.434, wps=12788.2, ups=1.04, wpb=12324.7, bsz=455.9, num_updates=5400, lr=0.00019245, gnorm=1.768, clip=1, loss_scale=0.5, train_wall=96, gb_free=14.3, wall=4446
2023-09-02 11:07:57 | INFO | train_inner | epoch 004:   1086 / 1474 loss=2.671, trans_loss=4.129, nll_loss=2.523, w2v_ctc_loss=1.494, task_loss=4.214, task_loss_gen=4.106, contrastive_loss=0, total=4084.57, n_correct=1395.93, ppl=5.75, accuracy=34.176, wps=12671.3, ups=1.04, wpb=12192, bsz=441.2, num_updates=5500, lr=0.000190693, gnorm=1.44, clip=0, loss_scale=0.5, train_wall=95, gb_free=16.2, wall=4542
2023-09-02 11:09:33 | INFO | train_inner | epoch 004:   1186 / 1474 loss=2.646, trans_loss=4.117, nll_loss=2.51, w2v_ctc_loss=1.473, task_loss=3.719, task_loss_gen=3.667, contrastive_loss=0, total=4162.44, n_correct=1438.57, ppl=5.69, accuracy=34.561, wps=12937.8, ups=1.04, wpb=12433, bsz=482.7, num_updates=5600, lr=0.000188982, gnorm=1.364, clip=0, loss_scale=0.5, train_wall=95, gb_free=16.6, wall=4638
2023-09-02 11:11:09 | INFO | train_inner | epoch 004:   1286 / 1474 loss=2.628, trans_loss=4.11, nll_loss=2.5, w2v_ctc_loss=1.448, task_loss=3.805, task_loss_gen=3.721, contrastive_loss=0, total=4144.75, n_correct=1444.01, ppl=5.66, accuracy=34.839, wps=12917.2, ups=1.04, wpb=12378.7, bsz=470, num_updates=5700, lr=0.000187317, gnorm=1.277, clip=0, loss_scale=0.5, train_wall=95, gb_free=16.3, wall=4734
2023-09-02 11:12:43 | INFO | train_inner | epoch 004:   1386 / 1474 loss=2.629, trans_loss=4.115, nll_loss=2.506, w2v_ctc_loss=1.442, task_loss=4.017, task_loss_gen=3.972, contrastive_loss=0, total=4107.86, n_correct=1420.16, ppl=5.68, accuracy=34.572, wps=13034.9, ups=1.06, wpb=12268.5, bsz=439.2, num_updates=5800, lr=0.000185695, gnorm=1.202, clip=0, loss_scale=0.5, train_wall=93, gb_free=16.9, wall=4828
2023-09-02 11:14:06 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.4071, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.4071, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.4071, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.4071, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.4071, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.4071, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.4071, device='cuda:2')
2023-09-02 11:14:43 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 5.388 | trans_loss 7.256 | nll_loss 5.131 | w2v_ctc_loss 1.608 | task_loss 15.53 | task_loss_gen 15.759 | contrastive_loss 0 | total 4003.4 | n_correct 1454.2 | ppl 35.05 | accuracy 36.324 | uer 24.275 | wer 25.823 | raw_wer 25.823 | bleu 0.29 | wps 1340.1 | wpb 4003.4 | bsz 141.8 | num_updates 5888 | best_bleu 0.29
2023-09-02 11:14:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5888 updates
2023-09-02 11:14:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 11:14:49 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 11:14:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 4 @ 5888 updates, score 0.29) (writing took 12.73254061298212 seconds)
2023-09-02 11:14:56 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-09-02 11:14:56 | INFO | train | epoch 004 | loss 2.674 | trans_loss 4.16 | nll_loss 2.561 | w2v_ctc_loss 1.489 | task_loss 5.084 | task_loss_gen 4.512 | contrastive_loss 0 | total 4139.18 | n_correct 1396.47 | ppl 5.9 | accuracy 33.738 | wps 12347.4 | ups 1 | wpb 12357.4 | bsz 458.6 | num_updates 5888 | lr 0.000184302 | gnorm 1.75 | clip 0.1 | loss_scale 0.5 | train_wall 1402 | gb_free 14.5 | wall 4961
2023-09-02 11:14:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 11:14:56 | INFO | fairseq.trainer | begin training epoch 5
2023-09-02 11:14:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 11:15:15 | INFO | train_inner | epoch 005:     12 / 1474 loss=2.62, trans_loss=4.114, nll_loss=2.504, w2v_ctc_loss=1.428, task_loss=4.075, task_loss_gen=4.057, contrastive_loss=0, total=4036.09, n_correct=1402.74, ppl=5.67, accuracy=34.755, wps=7929.3, ups=0.66, wpb=12049.9, bsz=436.9, num_updates=5900, lr=0.000184115, gnorm=1.279, clip=0, loss_scale=0.5, train_wall=93, gb_free=11.3, wall=4980
2023-09-02 11:16:51 | INFO | train_inner | epoch 005:    112 / 1474 loss=2.543, trans_loss=4.078, nll_loss=2.458, w2v_ctc_loss=1.339, task_loss=3.382, task_loss_gen=3.438, contrastive_loss=0, total=4256.69, n_correct=1521.13, ppl=5.5, accuracy=35.735, wps=13237.9, ups=1.04, wpb=12710.6, bsz=500.9, num_updates=6000, lr=0.000182574, gnorm=1.116, clip=0, loss_scale=0.5, train_wall=95, gb_free=17.1, wall=5076
2023-09-02 11:16:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 11:17:25 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 5.369 | trans_loss 7.245 | nll_loss 5.127 | w2v_ctc_loss 1.571 | task_loss 15.826 | task_loss_gen 15.729 | contrastive_loss 0 | total 4003.4 | n_correct 1455.4 | ppl 34.94 | accuracy 36.354 | uer 24.028 | wer 25.469 | raw_wer 25.469 | bleu 0.45 | wps 1565.5 | wpb 4003.4 | bsz 141.8 | num_updates 6000 | best_bleu 0.45
2023-09-02 11:17:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6000 updates
2023-09-02 11:17:25 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_5_6000.pt
2023-09-02 11:17:27 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_5_6000.pt
2023-09-02 11:17:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_5_6000.pt (epoch 5 @ 6000 updates, score 0.45) (writing took 14.918373574008001 seconds)
2023-09-02 11:18:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-09-02 11:19:15 | INFO | train_inner | epoch 005:    213 / 1474 loss=2.742, trans_loss=4.117, nll_loss=2.506, w2v_ctc_loss=1.617, task_loss=3.542, task_loss_gen=3.586, contrastive_loss=0, total=4189.76, n_correct=1452.34, ppl=5.68, accuracy=34.664, wps=8685.6, ups=0.69, wpb=12502.6, bsz=486.2, num_updates=6100, lr=0.000181071, gnorm=3.79, clip=7, loss_scale=0.25, train_wall=95, gb_free=12.3, wall=5220
2023-09-02 11:20:50 | INFO | train_inner | epoch 005:    313 / 1474 loss=2.689, trans_loss=4.102, nll_loss=2.491, w2v_ctc_loss=1.55, task_loss=4.152, task_loss_gen=3.958, contrastive_loss=0, total=4096.44, n_correct=1432.11, ppl=5.62, accuracy=34.96, wps=12844, ups=1.05, wpb=12249.1, bsz=448.2, num_updates=6200, lr=0.000179605, gnorm=2.916, clip=1, loss_scale=0.25, train_wall=95, gb_free=16.4, wall=5315
2023-09-02 11:22:26 | INFO | train_inner | epoch 005:    413 / 1474 loss=2.568, trans_loss=4.09, nll_loss=2.474, w2v_ctc_loss=1.374, task_loss=4.258, task_loss_gen=3.853, contrastive_loss=0, total=4133.67, n_correct=1467.41, ppl=5.56, accuracy=35.499, wps=12817.6, ups=1.04, wpb=12352.7, bsz=463.8, num_updates=6300, lr=0.000178174, gnorm=2.649, clip=0, loss_scale=0.25, train_wall=95, gb_free=16.3, wall=5412
2023-09-02 11:24:02 | INFO | train_inner | epoch 005:    513 / 1474 loss=2.562, trans_loss=4.098, nll_loss=2.483, w2v_ctc_loss=1.359, task_loss=4.466, task_loss_gen=4.148, contrastive_loss=0, total=4036.61, n_correct=1426.15, ppl=5.59, accuracy=35.33, wps=12645.1, ups=1.05, wpb=12057.3, bsz=428.7, num_updates=6400, lr=0.000176777, gnorm=2.527, clip=0, loss_scale=0.25, train_wall=95, gb_free=17.5, wall=5507
2023-09-02 11:25:38 | INFO | train_inner | epoch 005:    613 / 1474 loss=2.572, trans_loss=4.105, nll_loss=2.489, w2v_ctc_loss=1.367, task_loss=4.296, task_loss_gen=4.042, contrastive_loss=0, total=4112.09, n_correct=1452.97, ppl=5.62, accuracy=35.334, wps=12783.4, ups=1.04, wpb=12268.1, bsz=443.6, num_updates=6500, lr=0.000175412, gnorm=2.365, clip=0, loss_scale=0.25, train_wall=95, gb_free=16.5, wall=5603
2023-09-02 11:27:14 | INFO | train_inner | epoch 005:    713 / 1474 loss=2.592, trans_loss=4.109, nll_loss=2.496, w2v_ctc_loss=1.412, task_loss=4.124, task_loss_gen=3.687, contrastive_loss=0, total=4160.15, n_correct=1481.67, ppl=5.64, accuracy=35.616, wps=12920.6, ups=1.04, wpb=12418.1, bsz=477.6, num_updates=6600, lr=0.000174078, gnorm=3.195, clip=2, loss_scale=0.25, train_wall=95, gb_free=15.2, wall=5699
2023-09-02 11:28:51 | INFO | train_inner | epoch 005:    813 / 1474 loss=2.573, trans_loss=4.091, nll_loss=2.473, w2v_ctc_loss=1.381, task_loss=3.994, task_loss_gen=3.888, contrastive_loss=0, total=4129.67, n_correct=1469.98, ppl=5.55, accuracy=35.596, wps=12717.7, ups=1.03, wpb=12327.3, bsz=452.6, num_updates=6700, lr=0.000172774, gnorm=2.048, clip=0, loss_scale=0.25, train_wall=96, gb_free=15.6, wall=5796
2023-09-02 11:30:27 | INFO | train_inner | epoch 005:    913 / 1474 loss=2.548, trans_loss=4.078, nll_loss=2.458, w2v_ctc_loss=1.353, task_loss=4.074, task_loss_gen=3.91, contrastive_loss=0, total=4107.27, n_correct=1474.66, ppl=5.49, accuracy=35.904, wps=12782.9, ups=1.04, wpb=12262.1, bsz=446.2, num_updates=6800, lr=0.000171499, gnorm=2.265, clip=0, loss_scale=0.25, train_wall=95, gb_free=12.6, wall=5892
2023-09-02 11:32:02 | INFO | train_inner | epoch 005:   1013 / 1474 loss=2.564, trans_loss=4.076, nll_loss=2.456, w2v_ctc_loss=1.38, task_loss=3.855, task_loss_gen=3.773, contrastive_loss=0, total=4154.85, n_correct=1494.15, ppl=5.49, accuracy=35.962, wps=13080.6, ups=1.05, wpb=12403.9, bsz=458.6, num_updates=6900, lr=0.000170251, gnorm=2.118, clip=0, loss_scale=0.25, train_wall=94, gb_free=15.2, wall=5987
2023-09-02 11:33:39 | INFO | train_inner | epoch 005:   1113 / 1474 loss=2.541, trans_loss=4.065, nll_loss=2.439, w2v_ctc_loss=1.351, task_loss=3.721, task_loss_gen=3.737, contrastive_loss=0, total=4178.83, n_correct=1516.13, ppl=5.42, accuracy=36.281, wps=12826, ups=1.03, wpb=12465.4, bsz=468.4, num_updates=7000, lr=0.000169031, gnorm=1.934, clip=0, loss_scale=0.25, train_wall=96, gb_free=12.8, wall=6084
2023-09-02 11:35:15 | INFO | train_inner | epoch 005:   1213 / 1474 loss=2.549, trans_loss=4.081, nll_loss=2.459, w2v_ctc_loss=1.352, task_loss=4.061, task_loss_gen=3.888, contrastive_loss=0, total=4163.71, n_correct=1495.28, ppl=5.5, accuracy=35.912, wps=12928.8, ups=1.04, wpb=12420.2, bsz=454, num_updates=7100, lr=0.000167836, gnorm=2.195, clip=0, loss_scale=0.25, train_wall=95, gb_free=17, wall=6180
2023-09-02 11:36:51 | INFO | train_inner | epoch 005:   1313 / 1474 loss=2.532, trans_loss=4.067, nll_loss=2.444, w2v_ctc_loss=1.334, task_loss=3.896, task_loss_gen=3.894, contrastive_loss=0, total=4125.59, n_correct=1497.76, ppl=5.44, accuracy=36.304, wps=12781.9, ups=1.04, wpb=12312.6, bsz=442.8, num_updates=7200, lr=0.000166667, gnorm=1.969, clip=0, loss_scale=0.25, train_wall=96, gb_free=14.6, wall=6276
2023-09-02 11:38:27 | INFO | train_inner | epoch 005:   1413 / 1474 loss=2.525, trans_loss=4.063, nll_loss=2.44, w2v_ctc_loss=1.328, task_loss=3.802, task_loss_gen=3.764, contrastive_loss=0, total=4145.41, n_correct=1508.95, ppl=5.43, accuracy=36.401, wps=12961.7, ups=1.05, wpb=12381.3, bsz=460.5, num_updates=7300, lr=0.000165521, gnorm=2.088, clip=0, loss_scale=0.25, train_wall=95, gb_free=16.2, wall=6372
2023-09-02 11:39:25 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 11:40:00 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 5.298 | trans_loss 7.173 | nll_loss 5.032 | w2v_ctc_loss 1.496 | task_loss 13.549 | task_loss_gen 15.698 | contrastive_loss 0 | total 4003.4 | n_correct 1494 | ppl 32.72 | accuracy 37.318 | uer 23.444 | wer 24.936 | raw_wer 24.936 | bleu 0.69 | wps 1476.2 | wpb 4003.4 | bsz 141.8 | num_updates 7361 | best_bleu 0.69
2023-09-02 11:40:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7361 updates
2023-09-02 11:40:00 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 11:40:07 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 11:40:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 5 @ 7361 updates, score 0.69) (writing took 13.3005631310225 seconds)
2023-09-02 11:40:14 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-09-02 11:40:14 | INFO | train | epoch 005 | loss 2.576 | trans_loss 4.086 | nll_loss 2.467 | w2v_ctc_loss 1.39 | task_loss 3.961 | task_loss_gen 3.826 | contrastive_loss 0 | total 4138.7 | n_correct 1477.69 | ppl 5.53 | accuracy 35.704 | wps 11993 | ups 0.97 | wpb 12356.1 | bsz 458.5 | num_updates 7361 | lr 0.000164834 | gnorm 2.335 | clip 0.7 | loss_scale 0.25 | train_wall 1401 | gb_free 15.9 | wall 6479
2023-09-02 11:40:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 11:40:14 | INFO | fairseq.trainer | begin training epoch 6
2023-09-02 11:40:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 11:40:59 | INFO | train_inner | epoch 006:     39 / 1474 loss=2.514, trans_loss=4.049, nll_loss=2.419, w2v_ctc_loss=1.317, task_loss=3.854, task_loss_gen=3.955, contrastive_loss=0, total=4112.54, n_correct=1502.98, ppl=5.35, accuracy=36.546, wps=8061.8, ups=0.66, wpb=12270.9, bsz=445.9, num_updates=7400, lr=0.000164399, gnorm=1.753, clip=0, loss_scale=0.25, train_wall=95, gb_free=16.1, wall=6524
2023-09-02 11:42:34 | INFO | train_inner | epoch 006:    139 / 1474 loss=2.485, trans_loss=4.028, nll_loss=2.395, w2v_ctc_loss=1.284, task_loss=3.761, task_loss_gen=3.738, contrastive_loss=0, total=4157.02, n_correct=1532.84, ppl=5.26, accuracy=36.874, wps=13013.2, ups=1.05, wpb=12417.6, bsz=457.6, num_updates=7500, lr=0.000163299, gnorm=1.967, clip=0, loss_scale=0.25, train_wall=95, gb_free=14.1, wall=6620
2023-09-02 11:44:10 | INFO | train_inner | epoch 006:    239 / 1474 loss=2.506, trans_loss=4.036, nll_loss=2.405, w2v_ctc_loss=1.315, task_loss=3.967, task_loss_gen=3.951, contrastive_loss=0, total=4120.34, n_correct=1515.1, ppl=5.3, accuracy=36.771, wps=12927.8, ups=1.05, wpb=12308.9, bsz=444.8, num_updates=7600, lr=0.000162221, gnorm=2.132, clip=0, loss_scale=0.25, train_wall=94, gb_free=17.3, wall=6715
2023-09-02 11:45:48 | INFO | train_inner | epoch 006:    339 / 1474 loss=2.447, trans_loss=4.02, nll_loss=2.383, w2v_ctc_loss=1.241, task_loss=3.635, task_loss_gen=3.625, contrastive_loss=0, total=4160.74, n_correct=1548.91, ppl=5.22, accuracy=37.227, wps=12691.3, ups=1.02, wpb=12422.4, bsz=482.1, num_updates=7700, lr=0.000161165, gnorm=1.9, clip=0, loss_scale=0.25, train_wall=97, gb_free=15.9, wall=6813
2023-09-02 11:47:22 | INFO | train_inner | epoch 006:    439 / 1474 loss=2.459, trans_loss=4.023, nll_loss=2.389, w2v_ctc_loss=1.253, task_loss=3.567, task_loss_gen=3.647, contrastive_loss=0, total=4157.21, n_correct=1548.04, ppl=5.24, accuracy=37.237, wps=13098.8, ups=1.06, wpb=12413.9, bsz=470.2, num_updates=7800, lr=0.000160128, gnorm=1.758, clip=0, loss_scale=0.25, train_wall=94, gb_free=15.2, wall=6907
2023-09-02 11:48:58 | INFO | train_inner | epoch 006:    539 / 1474 loss=2.462, trans_loss=4.027, nll_loss=2.392, w2v_ctc_loss=1.258, task_loss=3.797, task_loss_gen=3.813, contrastive_loss=0, total=4168.09, n_correct=1551.92, ppl=5.25, accuracy=37.233, wps=12989.4, ups=1.04, wpb=12441.1, bsz=456, num_updates=7900, lr=0.000159111, gnorm=1.771, clip=0, loss_scale=0.25, train_wall=95, gb_free=15.9, wall=7003
2023-09-02 11:50:33 | INFO | train_inner | epoch 006:    639 / 1474 loss=2.449, trans_loss=4.023, nll_loss=2.388, w2v_ctc_loss=1.242, task_loss=3.528, task_loss_gen=3.588, contrastive_loss=0, total=4152.97, n_correct=1552.56, ppl=5.23, accuracy=37.384, wps=13065.4, ups=1.05, wpb=12396.5, bsz=473.1, num_updates=8000, lr=0.000158114, gnorm=1.73, clip=0, loss_scale=0.25, train_wall=94, gb_free=15.1, wall=7098
2023-09-02 11:50:33 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 11:51:08 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 5.259 | trans_loss 7.126 | nll_loss 4.969 | w2v_ctc_loss 1.472 | task_loss 15.353 | task_loss_gen 15.899 | contrastive_loss 0 | total 4003.4 | n_correct 1519.3 | ppl 31.33 | accuracy 37.95 | uer 22.539 | wer 24.153 | raw_wer 24.153 | bleu 0.77 | wps 1514.9 | wpb 4003.4 | bsz 141.8 | num_updates 8000 | best_bleu 0.77
2023-09-02 11:51:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8000 updates
2023-09-02 11:51:08 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_6_8000.pt
2023-09-02 11:51:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_6_8000.pt
2023-09-02 11:51:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_6_8000.pt (epoch 6 @ 8000 updates, score 0.77) (writing took 12.192186152009526 seconds)
2023-09-02 11:52:55 | INFO | train_inner | epoch 006:    739 / 1474 loss=2.46, trans_loss=4.023, nll_loss=2.386, w2v_ctc_loss=1.258, task_loss=3.789, task_loss_gen=3.918, contrastive_loss=0, total=4136.55, n_correct=1542.27, ppl=5.23, accuracy=37.284, wps=8682.6, ups=0.7, wpb=12349.7, bsz=450.8, num_updates=8100, lr=0.000157135, gnorm=1.471, clip=0, loss_scale=0.5, train_wall=95, gb_free=16.4, wall=7240
2023-09-02 11:54:32 | INFO | train_inner | epoch 006:    839 / 1474 loss=2.455, trans_loss=4.021, nll_loss=2.385, w2v_ctc_loss=1.248, task_loss=3.763, task_loss_gen=3.93, contrastive_loss=0, total=4134.7, n_correct=1546.3, ppl=5.22, accuracy=37.398, wps=12703.7, ups=1.03, wpb=12344.1, bsz=447.8, num_updates=8200, lr=0.000156174, gnorm=0.899, clip=0, loss_scale=0.5, train_wall=96, gb_free=14.4, wall=7338
2023-09-02 11:56:09 | INFO | train_inner | epoch 006:    939 / 1474 loss=2.459, trans_loss=4.023, nll_loss=2.387, w2v_ctc_loss=1.253, task_loss=4.165, task_loss_gen=4.016, contrastive_loss=0, total=4074.92, n_correct=1518.08, ppl=5.23, accuracy=37.254, wps=12651.7, ups=1.04, wpb=12163, bsz=439.9, num_updates=8300, lr=0.00015523, gnorm=1.169, clip=0, loss_scale=0.5, train_wall=95, gb_free=16.2, wall=7434
2023-09-02 11:57:44 | INFO | train_inner | epoch 006:   1039 / 1474 loss=2.431, trans_loss=4.008, nll_loss=2.368, w2v_ctc_loss=1.226, task_loss=3.531, task_loss_gen=3.553, contrastive_loss=0, total=4167.38, n_correct=1574.03, ppl=5.16, accuracy=37.77, wps=13049.6, ups=1.05, wpb=12438.7, bsz=478.3, num_updates=8400, lr=0.000154303, gnorm=0.998, clip=0, loss_scale=0.5, train_wall=95, gb_free=16, wall=7529
2023-09-02 11:59:20 | INFO | train_inner | epoch 006:   1139 / 1474 loss=2.451, trans_loss=4.015, nll_loss=2.377, w2v_ctc_loss=1.241, task_loss=4.071, task_loss_gen=4.194, contrastive_loss=0, total=4066.48, n_correct=1517.38, ppl=5.19, accuracy=37.314, wps=12657.9, ups=1.04, wpb=12140, bsz=428.3, num_updates=8500, lr=0.000153393, gnorm=0.98, clip=0, loss_scale=0.5, train_wall=95, gb_free=15.5, wall=7625
2023-09-02 12:00:56 | INFO | train_inner | epoch 006:   1239 / 1474 loss=2.422, trans_loss=3.994, nll_loss=2.352, w2v_ctc_loss=1.219, task_loss=3.632, task_loss_gen=3.721, contrastive_loss=0, total=4143.59, n_correct=1574.68, ppl=5.1, accuracy=38.003, wps=12904, ups=1.04, wpb=12377.1, bsz=471.5, num_updates=8600, lr=0.000152499, gnorm=0.954, clip=0, loss_scale=0.5, train_wall=95, gb_free=16.9, wall=7721
2023-09-02 12:02:31 | INFO | train_inner | epoch 006:   1339 / 1474 loss=2.424, trans_loss=4.006, nll_loss=2.363, w2v_ctc_loss=1.215, task_loss=3.642, task_loss_gen=3.757, contrastive_loss=0, total=4125.75, n_correct=1559.85, ppl=5.15, accuracy=37.808, wps=12959, ups=1.05, wpb=12308.4, bsz=454.4, num_updates=8700, lr=0.00015162, gnorm=0.887, clip=0, loss_scale=0.5, train_wall=94, gb_free=15.8, wall=7816
2023-09-02 12:04:07 | INFO | train_inner | epoch 006:   1439 / 1474 loss=2.438, trans_loss=4.005, nll_loss=2.364, w2v_ctc_loss=1.233, task_loss=3.693, task_loss_gen=3.847, contrastive_loss=0, total=4194.06, n_correct=1588.07, ppl=5.15, accuracy=37.865, wps=12940.2, ups=1.03, wpb=12518.6, bsz=462, num_updates=8800, lr=0.000150756, gnorm=0.985, clip=0, loss_scale=0.5, train_wall=96, gb_free=15.9, wall=7913
2023-09-02 12:04:40 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 12:05:14 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 5.23 | trans_loss 7.086 | nll_loss 4.919 | w2v_ctc_loss 1.464 | task_loss 10.254 | task_loss_gen 16.536 | contrastive_loss 0 | total 4003.4 | n_correct 1533 | ppl 30.25 | accuracy 38.292 | uer 22.897 | wer 24.787 | raw_wer 24.787 | bleu 0.79 | wps 1566.8 | wpb 4003.4 | bsz 141.8 | num_updates 8835 | best_bleu 0.79
2023-09-02 12:05:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8835 updates
2023-09-02 12:05:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 12:05:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 12:05:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 6 @ 8835 updates, score 0.79) (writing took 14.712126321013784 seconds)
2023-09-02 12:05:28 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-09-02 12:05:28 | INFO | train | epoch 006 | loss 2.454 | trans_loss 4.018 | nll_loss 2.381 | w2v_ctc_loss 1.249 | task_loss 3.741 | task_loss_gen 3.799 | contrastive_loss 0 | total 4138.65 | n_correct 1547.2 | ppl 5.21 | accuracy 37.384 | wps 12023.7 | ups 0.97 | wpb 12355.8 | bsz 458.5 | num_updates 8835 | lr 0.000150457 | gnorm 1.403 | clip 0.1 | loss_scale 0.5 | train_wall 1401 | gb_free 14.7 | wall 7994
2023-09-02 12:05:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 12:05:29 | INFO | fairseq.trainer | begin training epoch 7
2023-09-02 12:05:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 12:06:39 | INFO | train_inner | epoch 007:     65 / 1474 loss=2.411, trans_loss=3.992, nll_loss=2.346, w2v_ctc_loss=1.207, task_loss=3.646, task_loss_gen=3.728, contrastive_loss=0, total=4103.49, n_correct=1560.79, ppl=5.08, accuracy=38.036, wps=8094.1, ups=0.66, wpb=12251.2, bsz=461.5, num_updates=8900, lr=0.000149906, gnorm=1.007, clip=1, loss_scale=0.5, train_wall=95, gb_free=16.7, wall=8064
2023-09-02 12:08:14 | INFO | train_inner | epoch 007:    165 / 1474 loss=2.385, trans_loss=3.985, nll_loss=2.337, w2v_ctc_loss=1.166, task_loss=3.713, task_loss_gen=3.865, contrastive_loss=0, total=4110.42, n_correct=1564.07, ppl=5.05, accuracy=38.051, wps=12928.2, ups=1.05, wpb=12272.5, bsz=455.4, num_updates=9000, lr=0.000149071, gnorm=0.847, clip=0, loss_scale=0.5, train_wall=94, gb_free=16.1, wall=8159
2023-09-02 12:09:49 | INFO | train_inner | epoch 007:    265 / 1474 loss=2.381, trans_loss=3.979, nll_loss=2.328, w2v_ctc_loss=1.169, task_loss=3.775, task_loss_gen=3.792, contrastive_loss=0, total=4130.95, n_correct=1585.89, ppl=5.02, accuracy=38.39, wps=12926.4, ups=1.05, wpb=12328.4, bsz=454.4, num_updates=9100, lr=0.00014825, gnorm=1.011, clip=0, loss_scale=0.5, train_wall=94, gb_free=17.3, wall=8254
2023-09-02 12:11:25 | INFO | train_inner | epoch 007:    365 / 1474 loss=2.373, trans_loss=3.975, nll_loss=2.323, w2v_ctc_loss=1.162, task_loss=3.523, task_loss_gen=3.635, contrastive_loss=0, total=4204.16, n_correct=1622.62, ppl=5.01, accuracy=38.596, wps=13069.7, ups=1.04, wpb=12547.1, bsz=481.5, num_updates=9200, lr=0.000147442, gnorm=0.903, clip=0, loss_scale=0.5, train_wall=95, gb_free=16.8, wall=8350
2023-09-02 12:13:01 | INFO | train_inner | epoch 007:    465 / 1474 loss=2.388, trans_loss=3.988, nll_loss=2.341, w2v_ctc_loss=1.172, task_loss=3.805, task_loss_gen=3.773, contrastive_loss=0, total=4147.32, n_correct=1578.38, ppl=5.07, accuracy=38.058, wps=12855.9, ups=1.04, wpb=12384.6, bsz=458.5, num_updates=9300, lr=0.000146647, gnorm=1.097, clip=0, loss_scale=0.5, train_wall=95, gb_free=17.5, wall=8447
2023-09-02 12:14:37 | INFO | train_inner | epoch 007:    565 / 1474 loss=2.378, trans_loss=3.981, nll_loss=2.331, w2v_ctc_loss=1.163, task_loss=3.628, task_loss_gen=3.703, contrastive_loss=0, total=4171.72, n_correct=1606.23, ppl=5.03, accuracy=38.503, wps=13070.7, ups=1.05, wpb=12445.8, bsz=460.5, num_updates=9400, lr=0.000145865, gnorm=0.932, clip=0, loss_scale=0.5, train_wall=95, gb_free=15.4, wall=8542
2023-09-02 12:16:13 | INFO | train_inner | epoch 007:    665 / 1474 loss=2.376, trans_loss=3.982, nll_loss=2.332, w2v_ctc_loss=1.159, task_loss=3.712, task_loss_gen=3.802, contrastive_loss=0, total=4150.49, n_correct=1596.19, ppl=5.03, accuracy=38.458, wps=12896.7, ups=1.04, wpb=12385.7, bsz=454.7, num_updates=9500, lr=0.000145095, gnorm=0.898, clip=0, loss_scale=0.5, train_wall=95, gb_free=16.1, wall=8638
2023-09-02 12:17:49 | INFO | train_inner | epoch 007:    765 / 1474 loss=2.378, trans_loss=3.975, nll_loss=2.324, w2v_ctc_loss=1.161, task_loss=3.86, task_loss_gen=3.906, contrastive_loss=0, total=4132.17, n_correct=1586.13, ppl=5.01, accuracy=38.385, wps=12836, ups=1.04, wpb=12338.5, bsz=450.8, num_updates=9600, lr=0.000144338, gnorm=1.007, clip=0, loss_scale=0.5, train_wall=95, gb_free=15.9, wall=8734
2023-09-02 12:19:25 | INFO | train_inner | epoch 007:    865 / 1474 loss=2.37, trans_loss=3.983, nll_loss=2.332, w2v_ctc_loss=1.153, task_loss=3.775, task_loss_gen=3.86, contrastive_loss=0, total=4140.18, n_correct=1592.28, ppl=5.04, accuracy=38.459, wps=12846.6, ups=1.04, wpb=12352.6, bsz=457.8, num_updates=9700, lr=0.000143592, gnorm=0.901, clip=0, loss_scale=0.5, train_wall=95, gb_free=15.3, wall=8830
2023-09-02 12:21:02 | INFO | train_inner | epoch 007:    965 / 1474 loss=2.357, trans_loss=3.97, nll_loss=2.318, w2v_ctc_loss=1.142, task_loss=3.497, task_loss_gen=3.658, contrastive_loss=0, total=4144.65, n_correct=1605.53, ppl=4.99, accuracy=38.737, wps=12809.1, ups=1.04, wpb=12374.9, bsz=475.4, num_updates=9800, lr=0.000142857, gnorm=0.813, clip=0, loss_scale=0.5, train_wall=96, gb_free=14.7, wall=8927
2023-09-02 12:22:38 | INFO | train_inner | epoch 007:   1065 / 1474 loss=2.379, trans_loss=3.985, nll_loss=2.337, w2v_ctc_loss=1.162, task_loss=3.897, task_loss_gen=3.979, contrastive_loss=0, total=4097.24, n_correct=1571.63, ppl=5.05, accuracy=38.358, wps=12722.5, ups=1.04, wpb=12232.2, bsz=435.9, num_updates=9900, lr=0.000142134, gnorm=0.944, clip=0, loss_scale=0.5, train_wall=95, gb_free=15.7, wall=9023
2023-09-02 12:24:13 | INFO | train_inner | epoch 007:   1165 / 1474 loss=2.369, trans_loss=3.972, nll_loss=2.323, w2v_ctc_loss=1.157, task_loss=3.622, task_loss_gen=3.653, contrastive_loss=0, total=4142.16, n_correct=1597.77, ppl=5, accuracy=38.573, wps=12948.3, ups=1.05, wpb=12377.5, bsz=471.9, num_updates=10000, lr=0.000141421, gnorm=1.063, clip=0, loss_scale=0.5, train_wall=95, gb_free=15, wall=9119
2023-09-02 12:24:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 12:24:48 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 5.171 | trans_loss 7.038 | nll_loss 4.856 | w2v_ctc_loss 1.376 | task_loss 12.887 | task_loss_gen 16.448 | contrastive_loss 0 | total 4003.4 | n_correct 1565.3 | ppl 28.95 | accuracy 39.099 | uer 20.686 | wer 22.538 | raw_wer 22.538 | bleu 1.28 | wps 1536.3 | wpb 4003.4 | bsz 141.8 | num_updates 10000 | best_bleu 1.28
2023-09-02 12:24:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10000 updates
2023-09-02 12:24:48 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_7_10000.pt
2023-09-02 12:24:50 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_7_10000.pt
2023-09-02 12:25:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_7_10000.pt (epoch 7 @ 10000 updates, score 1.28) (writing took 12.13976396099315 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:0')
2023-09-02 12:26:35 | INFO | train_inner | epoch 007:   1265 / 1474 loss=2.36, trans_loss=3.978, nll_loss=2.328, w2v_ctc_loss=1.139, task_loss=3.937, task_loss_gen=3.92, contrastive_loss=0, total=4119.52, n_correct=1583.81, ppl=5.02, accuracy=38.446, wps=8668.7, ups=0.7, wpb=12302.1, bsz=446.6, num_updates=10100, lr=0.00014072, gnorm=0.869, clip=0, loss_scale=0.5, train_wall=94, gb_free=13.5, wall=9260
2023-09-02 12:28:10 | INFO | train_inner | epoch 007:   1365 / 1474 loss=2.349, trans_loss=3.96, nll_loss=2.305, w2v_ctc_loss=1.14, task_loss=3.456, task_loss_gen=3.509, contrastive_loss=0, total=4185.65, n_correct=1637.58, ppl=4.94, accuracy=39.124, wps=13160.6, ups=1.05, wpb=12496.2, bsz=480.7, num_updates=10200, lr=0.000140028, gnorm=0.575, clip=0, loss_scale=1, train_wall=94, gb_free=16.5, wall=9355
2023-09-02 12:29:48 | INFO | train_inner | epoch 007:   1465 / 1474 loss=2.356, trans_loss=3.964, nll_loss=2.311, w2v_ctc_loss=1.141, task_loss=3.975, task_loss_gen=4.006, contrastive_loss=0, total=4113.9, n_correct=1596.48, ppl=4.96, accuracy=38.807, wps=12524.3, ups=1.02, wpb=12290.7, bsz=446.9, num_updates=10300, lr=0.000139347, gnorm=0.53, clip=0, loss_scale=1, train_wall=97, gb_free=16.2, wall=9454
2023-09-02 12:29:57 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:2')
2023-09-02 12:30:31 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 5.134 | trans_loss 6.993 | nll_loss 4.796 | w2v_ctc_loss 1.355 | task_loss 11.416 | task_loss_gen 16.599 | contrastive_loss 0 | total 4003.4 | n_correct 1582.1 | ppl 27.78 | accuracy 39.519 | uer 20.24 | wer 21.856 | raw_wer 21.856 | bleu 1.25 | wps 1540.1 | wpb 4003.4 | bsz 141.8 | num_updates 10309 | best_bleu 1.28
2023-09-02 12:30:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10309 updates
2023-09-02 12:30:31 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_1.2501.pt
2023-09-02 12:30:34 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_1.2501.pt
2023-09-02 12:30:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_1.2501.pt (epoch 7 @ 10309 updates, score 1.25) (writing took 7.771236583997961 seconds)
2023-09-02 12:30:39 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-09-02 12:30:39 | INFO | train | epoch 007 | loss 2.373 | trans_loss 3.977 | nll_loss 2.327 | w2v_ctc_loss 1.158 | task_loss 3.73 | task_loss_gen 3.794 | contrastive_loss 0 | total 4138.65 | n_correct 1592.4 | ppl 5.02 | accuracy 38.476 | wps 12060.5 | ups 0.98 | wpb 12355.8 | bsz 458.5 | num_updates 10309 | lr 0.000139286 | gnorm 0.885 | clip 0 | loss_scale 1 | train_wall 1402 | gb_free 12.8 | wall 9504
2023-09-02 12:30:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 12:30:39 | INFO | fairseq.trainer | begin training epoch 8
2023-09-02 12:30:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 12:32:14 | INFO | train_inner | epoch 008:     91 / 1474 loss=2.334, trans_loss=3.962, nll_loss=2.303, w2v_ctc_loss=1.111, task_loss=3.86, task_loss_gen=4.13, contrastive_loss=0, total=4095.89, n_correct=1601.43, ppl=4.94, accuracy=39.098, wps=8411.8, ups=0.69, wpb=12213.1, bsz=435.6, num_updates=10400, lr=0.000138675, gnorm=0.467, clip=0, loss_scale=1, train_wall=95, gb_free=17.5, wall=9599
2023-09-02 12:33:49 | INFO | train_inner | epoch 008:    191 / 1474 loss=2.332, trans_loss=3.956, nll_loss=2.296, w2v_ctc_loss=1.109, task_loss=4.073, task_loss_gen=4.104, contrastive_loss=0, total=4040.13, n_correct=1580.6, ppl=4.91, accuracy=39.123, wps=12630.4, ups=1.05, wpb=12049.1, bsz=429, num_updates=10500, lr=0.000138013, gnorm=0.545, clip=0, loss_scale=1, train_wall=95, gb_free=15.7, wall=9694
2023-09-02 12:35:24 | INFO | train_inner | epoch 008:    291 / 1474 loss=2.32, trans_loss=3.94, nll_loss=2.277, w2v_ctc_loss=1.111, task_loss=3.473, task_loss_gen=3.506, contrastive_loss=0, total=4216.54, n_correct=1671.32, ppl=4.85, accuracy=39.637, wps=13219.4, ups=1.05, wpb=12583.3, bsz=489.6, num_updates=10600, lr=0.000137361, gnorm=0.537, clip=0, loss_scale=1, train_wall=95, gb_free=15.7, wall=9789
2023-09-02 12:37:01 | INFO | train_inner | epoch 008:    391 / 1474 loss=2.332, trans_loss=3.951, nll_loss=2.289, w2v_ctc_loss=1.115, task_loss=3.882, task_loss_gen=3.963, contrastive_loss=0, total=4134.8, n_correct=1622.44, ppl=4.89, accuracy=39.239, wps=12711.9, ups=1.03, wpb=12337.2, bsz=446.1, num_updates=10700, lr=0.000136717, gnorm=0.501, clip=0, loss_scale=1, train_wall=96, gb_free=16, wall=9886
2023-09-02 12:38:38 | INFO | train_inner | epoch 008:    491 / 1474 loss=2.306, trans_loss=3.934, nll_loss=2.271, w2v_ctc_loss=1.096, task_loss=3.363, task_loss_gen=3.432, contrastive_loss=0, total=4193.98, n_correct=1671.74, ppl=4.83, accuracy=39.86, wps=12929.4, ups=1.03, wpb=12521.7, bsz=500.7, num_updates=10800, lr=0.000136083, gnorm=0.484, clip=0, loss_scale=1, train_wall=96, gb_free=17.4, wall=9983
2023-09-02 12:40:14 | INFO | train_inner | epoch 008:    591 / 1474 loss=2.336, trans_loss=3.945, nll_loss=2.288, w2v_ctc_loss=1.123, task_loss=3.964, task_loss_gen=4.171, contrastive_loss=0, total=4063.58, n_correct=1594.31, ppl=4.88, accuracy=39.234, wps=12646, ups=1.04, wpb=12147.8, bsz=426.8, num_updates=10900, lr=0.000135457, gnorm=0.488, clip=0, loss_scale=1, train_wall=95, gb_free=12.1, wall=10079
2023-09-02 12:41:50 | INFO | train_inner | epoch 008:    691 / 1474 loss=2.324, trans_loss=3.938, nll_loss=2.275, w2v_ctc_loss=1.115, task_loss=3.697, task_loss_gen=3.892, contrastive_loss=0, total=4138.77, n_correct=1642.81, ppl=4.84, accuracy=39.693, wps=12848.5, ups=1.04, wpb=12354.5, bsz=450.4, num_updates=11000, lr=0.00013484, gnorm=0.48, clip=0, loss_scale=1, train_wall=95, gb_free=16.1, wall=10175
2023-09-02 12:43:26 | INFO | train_inner | epoch 008:    791 / 1474 loss=2.315, trans_loss=3.933, nll_loss=2.271, w2v_ctc_loss=1.106, task_loss=3.765, task_loss_gen=3.882, contrastive_loss=0, total=4122.32, n_correct=1636.33, ppl=4.83, accuracy=39.694, wps=12867.5, ups=1.04, wpb=12319.9, bsz=449, num_updates=11100, lr=0.000134231, gnorm=0.481, clip=0, loss_scale=1, train_wall=95, gb_free=17.1, wall=10271
2023-09-02 12:45:01 | INFO | train_inner | epoch 008:    891 / 1474 loss=2.301, trans_loss=3.932, nll_loss=2.268, w2v_ctc_loss=1.093, task_loss=3.514, task_loss_gen=3.637, contrastive_loss=0, total=4180.85, n_correct=1676.44, ppl=4.82, accuracy=40.098, wps=13085.4, ups=1.05, wpb=12485.3, bsz=477, num_updates=11200, lr=0.000133631, gnorm=0.493, clip=0, loss_scale=1, train_wall=95, gb_free=15.3, wall=10367
2023-09-02 12:46:37 | INFO | train_inner | epoch 008:    991 / 1474 loss=2.309, trans_loss=3.933, nll_loss=2.268, w2v_ctc_loss=1.102, task_loss=3.458, task_loss_gen=3.679, contrastive_loss=0, total=4145.35, n_correct=1656.58, ppl=4.82, accuracy=39.962, wps=12891.3, ups=1.04, wpb=12377.4, bsz=460.4, num_updates=11300, lr=0.000133038, gnorm=0.483, clip=0, loss_scale=1, train_wall=95, gb_free=16.1, wall=10463
2023-09-02 12:48:14 | INFO | train_inner | epoch 008:   1091 / 1474 loss=2.303, trans_loss=3.931, nll_loss=2.266, w2v_ctc_loss=1.094, task_loss=3.725, task_loss_gen=3.782, contrastive_loss=0, total=4191.42, n_correct=1684.84, ppl=4.81, accuracy=40.197, wps=12915, ups=1.03, wpb=12511.3, bsz=464.9, num_updates=11400, lr=0.000132453, gnorm=0.515, clip=0, loss_scale=1, train_wall=96, gb_free=17.3, wall=10560
2023-09-02 12:49:50 | INFO | train_inner | epoch 008:   1191 / 1474 loss=2.304, trans_loss=3.924, nll_loss=2.257, w2v_ctc_loss=1.102, task_loss=3.483, task_loss_gen=3.579, contrastive_loss=0, total=4187.13, n_correct=1684.51, ppl=4.78, accuracy=40.231, wps=13111.2, ups=1.05, wpb=12506.2, bsz=474, num_updates=11500, lr=0.000131876, gnorm=0.485, clip=0, loss_scale=1, train_wall=95, gb_free=12.7, wall=10655
2023-09-02 12:51:25 | INFO | train_inner | epoch 008:   1291 / 1474 loss=2.321, trans_loss=3.937, nll_loss=2.274, w2v_ctc_loss=1.116, task_loss=4.757, task_loss_gen=4.06, contrastive_loss=0, total=4055.06, n_correct=1609.62, ppl=4.84, accuracy=39.694, wps=12721.7, ups=1.05, wpb=12112.2, bsz=434.6, num_updates=11600, lr=0.000131306, gnorm=0.764, clip=0, loss_scale=1, train_wall=95, gb_free=16.2, wall=10750
2023-09-02 12:53:00 | INFO | train_inner | epoch 008:   1391 / 1474 loss=2.317, trans_loss=3.931, nll_loss=2.267, w2v_ctc_loss=1.118, task_loss=3.628, task_loss_gen=3.633, contrastive_loss=0, total=4166, n_correct=1674.25, ppl=4.81, accuracy=40.188, wps=13147.2, ups=1.06, wpb=12439.6, bsz=471.7, num_updates=11700, lr=0.000130744, gnorm=0.55, clip=0, loss_scale=1, train_wall=94, gb_free=16.4, wall=10845
2023-09-02 12:54:18 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 12:54:54 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 5.061 | trans_loss 6.887 | nll_loss 4.655 | w2v_ctc_loss 1.349 | task_loss 10.426 | task_loss_gen 17.207 | contrastive_loss 0 | total 4003.4 | n_correct 1644 | ppl 25.2 | accuracy 41.065 | uer 19.566 | wer 21.334 | raw_wer 21.334 | bleu 2.2 | wps 1446.2 | wpb 4003.4 | bsz 141.8 | num_updates 11783 | best_bleu 2.2
2023-09-02 12:54:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11783 updates
2023-09-02 12:54:54 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 12:55:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 12:55:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 8 @ 11783 updates, score 2.2) (writing took 12.195119202020578 seconds)
2023-09-02 12:55:07 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-09-02 12:55:07 | INFO | train | epoch 008 | loss 2.316 | trans_loss 3.938 | nll_loss 2.275 | w2v_ctc_loss 1.107 | task_loss 3.739 | task_loss_gen 3.801 | contrastive_loss 0 | total 4138.65 | n_correct 1645.8 | ppl 4.84 | accuracy 39.767 | wps 12406.9 | ups 1 | wpb 12355.8 | bsz 458.5 | num_updates 11783 | lr 0.000130283 | gnorm 0.516 | clip 0 | loss_scale 1 | train_wall 1401 | gb_free 16.5 | wall 10972
2023-09-02 12:55:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 12:55:07 | INFO | fairseq.trainer | begin training epoch 9
2023-09-02 12:55:07 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 12:55:31 | INFO | train_inner | epoch 009:     17 / 1474 loss=2.286, trans_loss=3.918, nll_loss=2.247, w2v_ctc_loss=1.082, task_loss=3.613, task_loss_gen=3.777, contrastive_loss=0, total=4113.19, n_correct=1668.3, ppl=4.75, accuracy=40.56, wps=8113.5, ups=0.66, wpb=12275.6, bsz=463.4, num_updates=11800, lr=0.000130189, gnorm=0.458, clip=0, loss_scale=1, train_wall=95, gb_free=17.4, wall=10996
2023-09-02 12:57:06 | INFO | train_inner | epoch 009:    117 / 1474 loss=2.259, trans_loss=3.895, nll_loss=2.219, w2v_ctc_loss=1.056, task_loss=3.384, task_loss_gen=3.576, contrastive_loss=0, total=4192.68, n_correct=1725.59, ppl=4.66, accuracy=41.157, wps=13127, ups=1.05, wpb=12520.8, bsz=481.1, num_updates=11900, lr=0.000129641, gnorm=0.448, clip=0, loss_scale=1, train_wall=95, gb_free=15.5, wall=11091
2023-09-02 12:58:42 | INFO | train_inner | epoch 009:    217 / 1474 loss=2.27, trans_loss=3.908, nll_loss=2.234, w2v_ctc_loss=1.056, task_loss=3.945, task_loss_gen=4.148, contrastive_loss=0, total=4065.4, n_correct=1656.24, ppl=4.71, accuracy=40.74, wps=12628.3, ups=1.04, wpb=12138.7, bsz=429.9, num_updates=12000, lr=0.000129099, gnorm=0.46, clip=0, loss_scale=1, train_wall=95, gb_free=15.5, wall=11188
2023-09-02 12:58:42 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 12:59:17 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 5.052 | trans_loss 6.873 | nll_loss 4.63 | w2v_ctc_loss 1.351 | task_loss 11.02 | task_loss_gen 16.867 | contrastive_loss 0 | total 4003.4 | n_correct 1649.7 | ppl 24.76 | accuracy 41.207 | uer 20.046 | wer 22.154 | raw_wer 22.154 | bleu 2.26 | wps 1510.3 | wpb 4003.4 | bsz 141.8 | num_updates 12000 | best_bleu 2.26
2023-09-02 12:59:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12000 updates
2023-09-02 12:59:17 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_9_12000.pt
2023-09-02 12:59:20 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_9_12000.pt
2023-09-02 12:59:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_9_12000.pt (epoch 9 @ 12000 updates, score 2.26) (writing took 13.430089542991482 seconds)
2023-09-02 13:01:06 | INFO | train_inner | epoch 009:    317 / 1474 loss=2.249, trans_loss=3.887, nll_loss=2.21, w2v_ctc_loss=1.049, task_loss=3.397, task_loss_gen=3.571, contrastive_loss=0, total=4152.87, n_correct=1716.86, ppl=4.63, accuracy=41.342, wps=8648.7, ups=0.7, wpb=12408.5, bsz=479.2, num_updates=12100, lr=0.000128565, gnorm=0.46, clip=0, loss_scale=1, train_wall=94, gb_free=16.4, wall=11331
2023-09-02 13:02:42 | INFO | train_inner | epoch 009:    417 / 1474 loss=2.262, trans_loss=3.899, nll_loss=2.223, w2v_ctc_loss=1.06, task_loss=3.524, task_loss_gen=3.783, contrastive_loss=0, total=4191.41, n_correct=1718.9, ppl=4.67, accuracy=41.01, wps=12966.6, ups=1.04, wpb=12514.6, bsz=464.1, num_updates=12200, lr=0.000128037, gnorm=0.429, clip=0, loss_scale=2, train_wall=96, gb_free=13.7, wall=11428
2023-09-02 13:04:18 | INFO | train_inner | epoch 009:    517 / 1474 loss=2.28, trans_loss=3.898, nll_loss=2.221, w2v_ctc_loss=1.082, task_loss=3.461, task_loss_gen=4.013, contrastive_loss=0, total=4119.12, n_correct=1689.37, ppl=4.66, accuracy=41.013, wps=12843.2, ups=1.04, wpb=12295.1, bsz=438.8, num_updates=12300, lr=0.000127515, gnorm=0.336, clip=0, loss_scale=2, train_wall=95, gb_free=16.2, wall=11523
2023-09-02 13:05:55 | INFO | train_inner | epoch 009:    617 / 1474 loss=2.247, trans_loss=3.884, nll_loss=2.207, w2v_ctc_loss=1.051, task_loss=3.495, task_loss_gen=3.879, contrastive_loss=0, total=4140.76, n_correct=1720.02, ppl=4.62, accuracy=41.539, wps=12832.3, ups=1.04, wpb=12374.8, bsz=463.2, num_updates=12400, lr=0.000127, gnorm=0.334, clip=0, loss_scale=2, train_wall=96, gb_free=15.2, wall=11620
2023-09-02 13:07:29 | INFO | train_inner | epoch 009:    717 / 1474 loss=2.268, trans_loss=3.89, nll_loss=2.213, w2v_ctc_loss=1.077, task_loss=3.864, task_loss_gen=3.936, contrastive_loss=0, total=4075.27, n_correct=1686.41, ppl=4.64, accuracy=41.382, wps=12859.8, ups=1.06, wpb=12176.3, bsz=443.7, num_updates=12500, lr=0.000126491, gnorm=0.361, clip=0, loss_scale=2, train_wall=94, gb_free=16.7, wall=11714
2023-09-02 13:09:06 | INFO | train_inner | epoch 009:    817 / 1474 loss=2.248, trans_loss=3.87, nll_loss=2.187, w2v_ctc_loss=1.068, task_loss=3.204, task_loss_gen=3.446, contrastive_loss=0, total=4215.48, n_correct=1771.77, ppl=4.55, accuracy=42.03, wps=13077.3, ups=1.04, wpb=12596.2, bsz=499.7, num_updates=12600, lr=0.000125988, gnorm=0.339, clip=0, loss_scale=2, train_wall=96, gb_free=16.5, wall=11811
2023-09-02 13:10:43 | INFO | train_inner | epoch 009:    917 / 1474 loss=2.255, trans_loss=3.88, nll_loss=2.195, w2v_ctc_loss=1.068, task_loss=3.671, task_loss_gen=3.981, contrastive_loss=0, total=4152.4, n_correct=1736.8, ppl=4.58, accuracy=41.826, wps=12751.2, ups=1.03, wpb=12388.2, bsz=450.8, num_updates=12700, lr=0.000125491, gnorm=0.34, clip=0, loss_scale=2, train_wall=97, gb_free=11.4, wall=11908
2023-09-02 13:12:18 | INFO | train_inner | epoch 009:   1017 / 1474 loss=2.266, trans_loss=3.883, nll_loss=2.2, w2v_ctc_loss=1.081, task_loss=3.822, task_loss_gen=4.334, contrastive_loss=0, total=4101.32, n_correct=1715.57, ppl=4.6, accuracy=41.83, wps=12813.5, ups=1.05, wpb=12242.4, bsz=424.9, num_updates=12800, lr=0.000125, gnorm=0.357, clip=0, loss_scale=2, train_wall=95, gb_free=15.5, wall=12003
2023-09-02 13:13:55 | INFO | train_inner | epoch 009:   1117 / 1474 loss=2.238, trans_loss=3.857, nll_loss=2.163, w2v_ctc_loss=1.07, task_loss=3.424, task_loss_gen=3.624, contrastive_loss=0, total=4172.83, n_correct=1799.47, ppl=4.48, accuracy=43.123, wps=12898, ups=1.04, wpb=12437.9, bsz=471.9, num_updates=12900, lr=0.000124515, gnorm=0.347, clip=0, loss_scale=2, train_wall=96, gb_free=16, wall=12100
2023-09-02 13:15:31 | INFO | train_inner | epoch 009:   1217 / 1474 loss=2.236, trans_loss=3.827, nll_loss=2.128, w2v_ctc_loss=1.096, task_loss=3.683, task_loss_gen=4.089, contrastive_loss=0, total=4138.15, n_correct=1824.68, ppl=4.37, accuracy=44.094, wps=12808.5, ups=1.04, wpb=12357.2, bsz=448.8, num_updates=13000, lr=0.000124035, gnorm=0.365, clip=0, loss_scale=2, train_wall=96, gb_free=15.8, wall=12196
2023-09-02 13:17:06 | INFO | train_inner | epoch 009:   1317 / 1474 loss=2.184, trans_loss=3.768, nll_loss=2.049, w2v_ctc_loss=1.084, task_loss=3.162, task_loss_gen=3.525, contrastive_loss=0, total=4205.27, n_correct=1957.66, ppl=4.14, accuracy=46.553, wps=13224.3, ups=1.05, wpb=12548.1, bsz=491.2, num_updates=13100, lr=0.00012356, gnorm=0.373, clip=0, loss_scale=2, train_wall=94, gb_free=16.2, wall=12291
2023-09-02 13:18:41 | INFO | train_inner | epoch 009:   1417 / 1474 loss=2.184, trans_loss=3.745, nll_loss=2.019, w2v_ctc_loss=1.11, task_loss=3.974, task_loss_gen=4.2, contrastive_loss=0, total=4071.37, n_correct=1951.24, ppl=4.05, accuracy=47.926, wps=12792.1, ups=1.05, wpb=12147.5, bsz=429, num_updates=13200, lr=0.000123091, gnorm=0.423, clip=0, loss_scale=2, train_wall=94, gb_free=14.5, wall=12386
2023-09-02 13:19:34 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 13:20:12 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.387 | trans_loss 5.883 | nll_loss 3.349 | w2v_ctc_loss 1.366 | task_loss 12.842 | task_loss_gen 16.492 | contrastive_loss 0 | total 4003.4 | n_correct 2208.8 | ppl 10.19 | accuracy 55.173 | uer 19.292 | wer 21.226 | raw_wer 21.226 | bleu 11.76 | wps 1311 | wpb 4003.4 | bsz 141.8 | num_updates 13257 | best_bleu 11.76
2023-09-02 13:20:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 13257 updates
2023-09-02 13:20:12 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 13:20:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 13:20:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 9 @ 13257 updates, score 11.76) (writing took 12.299936124007218 seconds)
2023-09-02 13:20:25 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-09-02 13:20:25 | INFO | train | epoch 009 | loss 2.242 | trans_loss 3.857 | nll_loss 2.168 | w2v_ctc_loss 1.073 | task_loss 3.588 | task_loss_gen 3.856 | contrastive_loss 0 | total 4138.65 | n_correct 1772.4 | ppl 4.49 | accuracy 42.826 | wps 11992.2 | ups 0.97 | wpb 12355.8 | bsz 458.5 | num_updates 13257 | lr 0.000122827 | gnorm 0.388 | clip 0 | loss_scale 2 | train_wall 1402 | gb_free 11.1 | wall 12490
2023-09-02 13:20:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 13:20:25 | INFO | fairseq.trainer | begin training epoch 10
2023-09-02 13:20:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 13:21:13 | INFO | train_inner | epoch 010:     43 / 1474 loss=2.114, trans_loss=3.665, nll_loss=1.917, w2v_ctc_loss=1.084, task_loss=3.93, task_loss_gen=3.774, contrastive_loss=0, total=4113.02, n_correct=2106.97, ppl=3.78, accuracy=51.227, wps=8052.7, ups=0.66, wpb=12276.4, bsz=475.9, num_updates=13300, lr=0.000122628, gnorm=0.437, clip=0, loss_scale=2, train_wall=93, gb_free=16, wall=12539
2023-09-02 13:22:50 | INFO | train_inner | epoch 010:    143 / 1474 loss=2.073, trans_loss=3.619, nll_loss=1.86, w2v_ctc_loss=1.063, task_loss=3.79, task_loss_gen=3.718, contrastive_loss=0, total=4234.99, n_correct=2236.75, ppl=3.63, accuracy=52.816, wps=13124.6, ups=1.04, wpb=12647.2, bsz=473.2, num_updates=13400, lr=0.000122169, gnorm=0.416, clip=0, loss_scale=2, train_wall=96, gb_free=16.1, wall=12635
2023-09-02 13:24:25 | INFO | train_inner | epoch 010:    243 / 1474 loss=2.062, trans_loss=3.591, nll_loss=1.823, w2v_ctc_loss=1.071, task_loss=3.728, task_loss_gen=3.726, contrastive_loss=0, total=4131.11, n_correct=2228.74, ppl=3.54, accuracy=53.95, wps=12965.4, ups=1.05, wpb=12328.7, bsz=463.9, num_updates=13500, lr=0.000121716, gnorm=0.412, clip=0, loss_scale=2, train_wall=94, gb_free=15.8, wall=12730
2023-09-02 13:26:01 | INFO | train_inner | epoch 010:    343 / 1474 loss=2.045, trans_loss=3.572, nll_loss=1.803, w2v_ctc_loss=1.064, task_loss=3.57, task_loss_gen=3.868, contrastive_loss=0, total=4135.65, n_correct=2260.62, ppl=3.49, accuracy=54.662, wps=12855.2, ups=1.04, wpb=12362.4, bsz=454, num_updates=13600, lr=0.000121268, gnorm=0.4, clip=0, loss_scale=2, train_wall=96, gb_free=15.3, wall=12826
2023-09-02 13:27:38 | INFO | train_inner | epoch 010:    443 / 1474 loss=2.036, trans_loss=3.568, nll_loss=1.794, w2v_ctc_loss=1.059, task_loss=3.379, task_loss_gen=3.69, contrastive_loss=0, total=4199.14, n_correct=2319.27, ppl=3.47, accuracy=55.232, wps=12968.7, ups=1.03, wpb=12535.9, bsz=482.3, num_updates=13700, lr=0.000120824, gnorm=0.395, clip=0, loss_scale=2, train_wall=96, gb_free=16.5, wall=12923
2023-09-02 13:29:14 | INFO | train_inner | epoch 010:    543 / 1474 loss=2.057, trans_loss=3.57, nll_loss=1.794, w2v_ctc_loss=1.09, task_loss=3.953, task_loss_gen=4.206, contrastive_loss=0, total=4094.23, n_correct=2264.05, ppl=3.47, accuracy=55.299, wps=12751.7, ups=1.04, wpb=12209.6, bsz=433.2, num_updates=13800, lr=0.000120386, gnorm=0.43, clip=0, loss_scale=2, train_wall=95, gb_free=14, wall=13019
2023-09-02 13:30:50 | INFO | train_inner | epoch 010:    643 / 1474 loss=2.036, trans_loss=3.552, nll_loss=1.772, w2v_ctc_loss=1.078, task_loss=3.485, task_loss_gen=3.633, contrastive_loss=0, total=4182.84, n_correct=2342.81, ppl=3.42, accuracy=56.01, wps=12944.4, ups=1.04, wpb=12481.2, bsz=481.3, num_updates=13900, lr=0.000119952, gnorm=0.409, clip=0, loss_scale=2, train_wall=96, gb_free=16.3, wall=13115
2023-09-02 13:32:25 | INFO | train_inner | epoch 010:    743 / 1474 loss=2.043, trans_loss=3.543, nll_loss=1.762, w2v_ctc_loss=1.094, task_loss=3.701, task_loss_gen=3.914, contrastive_loss=0, total=4120.62, n_correct=2319.62, ppl=3.39, accuracy=56.293, wps=12963.9, ups=1.05, wpb=12301.2, bsz=451.7, num_updates=14000, lr=0.000119523, gnorm=0.402, clip=0, loss_scale=2, train_wall=94, gb_free=16.4, wall=13210
2023-09-02 13:32:25 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 13:32:58 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.177 | trans_loss 5.547 | nll_loss 2.903 | w2v_ctc_loss 1.422 | task_loss 11.071 | task_loss_gen 16.914 | contrastive_loss 0 | total 4003.4 | n_correct 2421.5 | ppl 7.48 | accuracy 60.486 | uer 19.967 | wer 21.796 | raw_wer 21.796 | bleu 16.75 | wps 1619.6 | wpb 4003.4 | bsz 141.8 | num_updates 14000 | best_bleu 16.75
2023-09-02 13:32:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14000 updates
2023-09-02 13:32:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_10_14000.pt
2023-09-02 13:33:01 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_10_14000.pt
2023-09-02 13:33:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_10_14000.pt (epoch 10 @ 14000 updates, score 16.75) (writing took 12.756866273004562 seconds)
2023-09-02 13:34:46 | INFO | train_inner | epoch 010:    843 / 1474 loss=2.015, trans_loss=3.53, nll_loss=1.747, w2v_ctc_loss=1.066, task_loss=3.67, task_loss_gen=3.769, contrastive_loss=0, total=4132.62, n_correct=2351.75, ppl=3.36, accuracy=56.907, wps=8719.9, ups=0.71, wpb=12339.2, bsz=457.4, num_updates=14100, lr=0.000119098, gnorm=0.41, clip=0, loss_scale=2, train_wall=95, gb_free=16.3, wall=13352
2023-09-02 13:36:21 | INFO | train_inner | epoch 010:    943 / 1474 loss=2.017, trans_loss=3.522, nll_loss=1.734, w2v_ctc_loss=1.076, task_loss=3.535, task_loss_gen=3.697, contrastive_loss=0, total=4160.84, n_correct=2381.23, ppl=3.33, accuracy=57.23, wps=13076.5, ups=1.05, wpb=12411.3, bsz=467.9, num_updates=14200, lr=0.000118678, gnorm=0.411, clip=0, loss_scale=2, train_wall=94, gb_free=15.4, wall=13446
2023-09-02 13:37:57 | INFO | train_inner | epoch 010:   1043 / 1474 loss=2.021, trans_loss=3.518, nll_loss=1.731, w2v_ctc_loss=1.082, task_loss=3.751, task_loss_gen=4.254, contrastive_loss=0, total=4059.22, n_correct=2322.47, ppl=3.32, accuracy=57.215, wps=12652.6, ups=1.04, wpb=12120, bsz=431.2, num_updates=14300, lr=0.000118262, gnorm=0.383, clip=0, loss_scale=4, train_wall=95, gb_free=9, wall=13542
2023-09-02 13:39:32 | INFO | train_inner | epoch 010:   1143 / 1474 loss=2.023, trans_loss=3.518, nll_loss=1.731, w2v_ctc_loss=1.09, task_loss=3.601, task_loss_gen=4.299, contrastive_loss=0, total=4045.82, n_correct=2322.52, ppl=3.32, accuracy=57.405, wps=12710, ups=1.05, wpb=12079.3, bsz=422.8, num_updates=14400, lr=0.000117851, gnorm=0.377, clip=0, loss_scale=4, train_wall=94, gb_free=12.4, wall=13637
2023-09-02 13:41:08 | INFO | train_inner | epoch 010:   1243 / 1474 loss=2.007, trans_loss=3.497, nll_loss=1.708, w2v_ctc_loss=1.082, task_loss=3.221, task_loss_gen=4.022, contrastive_loss=0, total=4107.6, n_correct=2380.47, ppl=3.27, accuracy=57.953, wps=12832.9, ups=1.04, wpb=12284.6, bsz=446.5, num_updates=14500, lr=0.000117444, gnorm=0.371, clip=0, loss_scale=4, train_wall=95, gb_free=16.3, wall=13733
2023-09-02 13:42:44 | INFO | train_inner | epoch 010:   1343 / 1474 loss=2.005, trans_loss=3.502, nll_loss=1.712, w2v_ctc_loss=1.078, task_loss=3.214, task_loss_gen=4.028, contrastive_loss=0, total=4127.69, n_correct=2398.2, ppl=3.28, accuracy=58.1, wps=12860.2, ups=1.04, wpb=12326.4, bsz=452.2, num_updates=14600, lr=0.000117041, gnorm=0.372, clip=0, loss_scale=4, train_wall=95, gb_free=15.7, wall=13829
2023-09-02 13:44:20 | INFO | train_inner | epoch 010:   1443 / 1474 loss=1.991, trans_loss=3.508, nll_loss=1.717, w2v_ctc_loss=1.058, task_loss=3.318, task_loss_gen=3.827, contrastive_loss=0, total=4195.02, n_correct=2437.4, ppl=3.29, accuracy=58.102, wps=12955.5, ups=1.04, wpb=12514.1, bsz=483, num_updates=14700, lr=0.000116642, gnorm=0.373, clip=0, loss_scale=4, train_wall=96, gb_free=16, wall=13926
2023-09-02 13:44:50 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 13:45:24 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.092 | trans_loss 5.429 | nll_loss 2.764 | w2v_ctc_loss 1.408 | task_loss 13.496 | task_loss_gen 16.449 | contrastive_loss 0 | total 4003.4 | n_correct 2491.7 | ppl 6.79 | accuracy 62.24 | uer 19.149 | wer 20.883 | raw_wer 20.883 | bleu 18.09 | wps 1541.8 | wpb 4003.4 | bsz 141.8 | num_updates 14731 | best_bleu 18.09
2023-09-02 13:45:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14731 updates
2023-09-02 13:45:24 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 13:45:30 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 13:45:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 10 @ 14731 updates, score 18.09) (writing took 13.31383579300018 seconds)
2023-09-02 13:45:38 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-09-02 13:45:38 | INFO | train | epoch 010 | loss 2.031 | trans_loss 3.546 | nll_loss 1.766 | w2v_ctc_loss 1.073 | task_loss 3.584 | task_loss_gen 3.896 | contrastive_loss 0 | total 4138.65 | n_correct 2323.66 | ppl 3.4 | accuracy 56.145 | wps 12043.1 | ups 0.97 | wpb 12355.8 | bsz 458.5 | num_updates 14731 | lr 0.00011652 | gnorm 0.398 | clip 0 | loss_scale 4 | train_wall 1401 | gb_free 17 | wall 14003
2023-09-02 13:45:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 13:45:38 | INFO | fairseq.trainer | begin training epoch 11
2023-09-02 13:45:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 13:46:50 | INFO | train_inner | epoch 011:     69 / 1474 loss=1.968, trans_loss=3.474, nll_loss=1.674, w2v_ctc_loss=1.044, task_loss=4.015, task_loss_gen=3.907, contrastive_loss=0, total=4166, n_correct=2459.9, ppl=3.19, accuracy=59.047, wps=8314.8, ups=0.67, wpb=12436.1, bsz=475.7, num_updates=14800, lr=0.000116248, gnorm=0.392, clip=0, loss_scale=4, train_wall=94, gb_free=17.6, wall=14075
2023-09-02 13:47:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-09-02 13:47:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-09-02 13:48:28 | INFO | train_inner | epoch 011:    171 / 1474 loss=1.996, trans_loss=3.488, nll_loss=1.694, w2v_ctc_loss=1.075, task_loss=4.085, task_loss_gen=4.051, contrastive_loss=0, total=4108.41, n_correct=2404.62, ppl=3.24, accuracy=58.529, wps=12537.3, ups=1.02, wpb=12272, bsz=451.9, num_updates=14900, lr=0.000115857, gnorm=0.882, clip=0, loss_scale=1, train_wall=97, gb_free=15.9, wall=14173
2023-09-02 13:50:03 | INFO | train_inner | epoch 011:    271 / 1474 loss=2.093, trans_loss=3.527, nll_loss=1.743, w2v_ctc_loss=1.187, task_loss=4.499, task_loss_gen=4.208, contrastive_loss=0, total=4109.58, n_correct=2337.32, ppl=3.35, accuracy=56.875, wps=12852.9, ups=1.05, wpb=12274.9, bsz=441.4, num_updates=15000, lr=0.00011547, gnorm=2.006, clip=2, loss_scale=1, train_wall=95, gb_free=16.6, wall=14268
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:0')
2023-09-02 13:51:15 | INFO | train_inner | epoch 011:    371 / 1474 loss=2.192, trans_loss=5.21, nll_loss=2.553, w2v_ctc_loss=0.866, task_loss=6.156, task_loss_gen=5.909, contrastive_loss=0, total=4103.77, n_correct=2364.41, ppl=5.87, accuracy=57.616, wps=11437.1, ups=1.39, wpb=8253.5, bsz=299, num_updates=15100, lr=0.000115087, gnorm=2.858, clip=4, loss_scale=1, train_wall=71, gb_free=13.7, wall=14341
2023-09-02 13:52:28 | INFO | train_inner | epoch 011:    471 / 1474 loss=2.197, trans_loss=5.236, nll_loss=2.565, w2v_ctc_loss=0.879, task_loss=5.932, task_loss_gen=5.924, contrastive_loss=0, total=4117.47, n_correct=2373.6, ppl=5.92, accuracy=57.647, wps=11289.4, ups=1.37, wpb=8234.9, bsz=303.7, num_updates=15200, lr=0.000114708, gnorm=2.02, clip=3, loss_scale=1, train_wall=72, gb_free=15.9, wall=14414
2023-09-02 13:53:41 | INFO | train_inner | epoch 011:    571 / 1474 loss=2.195, trans_loss=5.226, nll_loss=2.552, w2v_ctc_loss=0.888, task_loss=5.982, task_loss_gen=6.132, contrastive_loss=0, total=4070.21, n_correct=2357.64, ppl=5.86, accuracy=57.924, wps=11168.2, ups=1.37, wpb=8140.4, bsz=292.9, num_updates=15300, lr=0.000114332, gnorm=2.073, clip=3, loss_scale=1, train_wall=72, gb_free=12.9, wall=14486
2023-09-02 13:54:54 | INFO | train_inner | epoch 011:    671 / 1474 loss=2.157, trans_loss=5.195, nll_loss=2.512, w2v_ctc_loss=0.825, task_loss=5.277, task_loss_gen=5.718, contrastive_loss=0, total=4161.7, n_correct=2444.69, ppl=5.7, accuracy=58.743, wps=11388, ups=1.37, wpb=8323.4, bsz=311.8, num_updates=15400, lr=0.000113961, gnorm=0.73, clip=0, loss_scale=1, train_wall=72, gb_free=16.9, wall=14560
2023-09-02 13:56:07 | INFO | train_inner | epoch 011:    771 / 1474 loss=2.147, trans_loss=5.189, nll_loss=2.504, w2v_ctc_loss=0.812, task_loss=5.656, task_loss_gen=5.888, contrastive_loss=0, total=4151.24, n_correct=2457.42, ppl=5.67, accuracy=59.197, wps=11377.2, ups=1.37, wpb=8302.5, bsz=301, num_updates=15500, lr=0.000113592, gnorm=0.748, clip=0, loss_scale=1, train_wall=72, gb_free=15.5, wall=14633
2023-09-02 13:57:20 | INFO | train_inner | epoch 011:    871 / 1474 loss=2.138, trans_loss=5.181, nll_loss=2.493, w2v_ctc_loss=0.796, task_loss=5.785, task_loss_gen=6.001, contrastive_loss=0, total=4128.97, n_correct=2447.59, ppl=5.63, accuracy=59.278, wps=11424.8, ups=1.38, wpb=8257.9, bsz=294.6, num_updates=15600, lr=0.000113228, gnorm=0.749, clip=0, loss_scale=1, train_wall=72, gb_free=16.3, wall=14705
2023-09-02 13:58:32 | INFO | train_inner | epoch 011:    971 / 1474 loss=2.132, trans_loss=5.178, nll_loss=2.49, w2v_ctc_loss=0.795, task_loss=5.709, task_loss_gen=5.782, contrastive_loss=0, total=4144.6, n_correct=2463.26, ppl=5.62, accuracy=59.433, wps=11480.6, ups=1.39, wpb=8289.2, bsz=304.5, num_updates=15700, lr=0.000112867, gnorm=0.745, clip=0, loss_scale=1, train_wall=71, gb_free=17.6, wall=14777
2023-09-02 13:59:44 | INFO | train_inner | epoch 011:   1071 / 1474 loss=2.12, trans_loss=5.16, nll_loss=2.468, w2v_ctc_loss=0.789, task_loss=5.49, task_loss_gen=5.547, contrastive_loss=0, total=4153.33, n_correct=2488.98, ppl=5.53, accuracy=59.927, wps=11512.1, ups=1.39, wpb=8306.7, bsz=310.5, num_updates=15800, lr=0.000112509, gnorm=0.81, clip=0, loss_scale=1, train_wall=71, gb_free=15.6, wall=14849
2023-09-02 14:00:57 | INFO | train_inner | epoch 011:   1171 / 1474 loss=2.127, trans_loss=5.171, nll_loss=2.482, w2v_ctc_loss=0.797, task_loss=5.42, task_loss_gen=5.623, contrastive_loss=0, total=4177.49, n_correct=2491.07, ppl=5.59, accuracy=59.631, wps=11475.7, ups=1.37, wpb=8355, bsz=311.7, num_updates=15900, lr=0.000112154, gnorm=0.733, clip=0, loss_scale=1, train_wall=72, gb_free=16.6, wall=14922
2023-09-02 14:02:11 | INFO | train_inner | epoch 011:   1271 / 1474 loss=2.125, trans_loss=5.159, nll_loss=2.468, w2v_ctc_loss=0.802, task_loss=5.583, task_loss_gen=5.684, contrastive_loss=0, total=4150.45, n_correct=2482.46, ppl=5.53, accuracy=59.812, wps=11255.9, ups=1.36, wpb=8300.9, bsz=307.4, num_updates=16000, lr=0.000111803, gnorm=0.789, clip=0, loss_scale=1, train_wall=73, gb_free=16.3, wall=14996
2023-09-02 14:02:11 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:3')
2023-09-02 14:02:43 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.067 | trans_loss 5.37 | nll_loss 2.687 | w2v_ctc_loss 1.454 | task_loss 13.259 | task_loss_gen 16.415 | contrastive_loss 0 | total 4003.4 | n_correct 2523.9 | ppl 6.44 | accuracy 63.044 | uer 19.372 | wer 21.237 | raw_wer 21.237 | bleu 18.32 | wps 1652.1 | wpb 4003.4 | bsz 141.8 | num_updates 16000 | best_bleu 18.32
2023-09-02 14:02:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16000 updates
2023-09-02 14:02:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_11_16000.pt
2023-09-02 14:02:46 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_11_16000.pt
2023-09-02 14:02:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_11_16000.pt (epoch 11 @ 16000 updates, score 18.32) (writing took 13.012180033983896 seconds)
2023-09-02 14:04:10 | INFO | train_inner | epoch 011:   1371 / 1474 loss=2.11, trans_loss=5.15, nll_loss=2.456, w2v_ctc_loss=0.779, task_loss=5.091, task_loss_gen=5.267, contrastive_loss=0, total=4191.95, n_correct=2516.35, ppl=5.49, accuracy=60.028, wps=7025.3, ups=0.84, wpb=8383.9, bsz=326.8, num_updates=16100, lr=0.000111456, gnorm=0.735, clip=0, loss_scale=1, train_wall=72, gb_free=15.4, wall=15115
2023-09-02 14:05:22 | INFO | train_inner | epoch 011:   1471 / 1474 loss=2.113, trans_loss=5.155, nll_loss=2.461, w2v_ctc_loss=0.787, task_loss=5.092, task_loss_gen=5.471, contrastive_loss=0, total=4172.29, n_correct=2508.65, ppl=5.51, accuracy=60.126, wps=11578, ups=1.39, wpb=8344.6, bsz=315.6, num_updates=16200, lr=0.000111111, gnorm=0.682, clip=0, loss_scale=1, train_wall=71, gb_free=16.1, wall=15187
2023-09-02 14:05:24 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 14:05:57 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.046 | trans_loss 5.364 | nll_loss 2.676 | w2v_ctc_loss 1.4 | task_loss 12.337 | task_loss_gen 16.461 | contrastive_loss 0 | total 4003.4 | n_correct 2531.2 | ppl 6.39 | accuracy 63.226 | uer 19.178 | wer 20.935 | raw_wer 20.935 | bleu 18.68 | wps 1639.1 | wpb 4003.4 | bsz 141.8 | num_updates 16203 | best_bleu 18.68
2023-09-02 14:05:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16203 updates
2023-09-02 14:05:57 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 14:06:04 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 14:06:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 11 @ 16203 updates, score 18.68) (writing took 14.259932174987625 seconds)
2023-09-02 14:06:11 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-09-02 14:06:11 | INFO | train | epoch 011 | loss 2.116 | trans_loss 4.763 | nll_loss 2.302 | w2v_ctc_loss 0.89 | task_loss 5.23 | task_loss_gen 5.306 | contrastive_loss 0 | total 4139.53 | n_correct 2439.57 | ppl 4.93 | accuracy 58.934 | wps 10770.6 | ups 1.19 | wpb 9027 | bsz 333.7 | num_updates 16203 | lr 0.000111101 | gnorm 1.144 | clip 0.8 | loss_scale 1 | train_wall 1121 | gb_free 16.9 | wall 15236
2023-09-02 14:06:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 14:06:11 | INFO | fairseq.trainer | begin training epoch 12
2023-09-02 14:06:11 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 14:07:28 | INFO | train_inner | epoch 012:     97 / 1474 loss=2.094, trans_loss=5.118, nll_loss=2.414, w2v_ctc_loss=0.77, task_loss=5.323, task_loss_gen=5.575, contrastive_loss=0, total=4138.25, n_correct=2513.57, ppl=5.33, accuracy=60.74, wps=6547.3, ups=0.79, wpb=8276.5, bsz=312.6, num_updates=16300, lr=0.00011077, gnorm=0.738, clip=0, loss_scale=1, train_wall=71, gb_free=16.6, wall=15314
2023-09-02 14:08:41 | INFO | train_inner | epoch 012:    197 / 1474 loss=2.099, trans_loss=5.12, nll_loss=2.416, w2v_ctc_loss=0.776, task_loss=5.732, task_loss_gen=5.871, contrastive_loss=0, total=4126.75, n_correct=2503.97, ppl=5.34, accuracy=60.677, wps=11362.6, ups=1.38, wpb=8253.5, bsz=296.6, num_updates=16400, lr=0.000110432, gnorm=0.736, clip=0, loss_scale=1, train_wall=72, gb_free=16.3, wall=15386
2023-09-02 14:09:55 | INFO | train_inner | epoch 012:    297 / 1474 loss=2.087, trans_loss=5.117, nll_loss=2.413, w2v_ctc_loss=0.763, task_loss=5.122, task_loss_gen=5.354, contrastive_loss=0, total=4221.98, n_correct=2569.51, ppl=5.32, accuracy=60.86, wps=11497.1, ups=1.36, wpb=8444, bsz=326, num_updates=16500, lr=0.000110096, gnorm=0.656, clip=0, loss_scale=1, train_wall=73, gb_free=16.1, wall=15460
2023-09-02 14:11:07 | INFO | train_inner | epoch 012:    397 / 1474 loss=2.094, trans_loss=5.114, nll_loss=2.41, w2v_ctc_loss=0.777, task_loss=5.567, task_loss_gen=5.721, contrastive_loss=0, total=4136.43, n_correct=2519.44, ppl=5.32, accuracy=60.909, wps=11424.7, ups=1.38, wpb=8272.9, bsz=301.9, num_updates=16600, lr=0.000109764, gnorm=0.751, clip=0, loss_scale=1, train_wall=72, gb_free=15.5, wall=15532
2023-09-02 14:12:19 | INFO | train_inner | epoch 012:    497 / 1474 loss=2.101, trans_loss=5.125, nll_loss=2.425, w2v_ctc_loss=0.785, task_loss=5.474, task_loss_gen=5.976, contrastive_loss=0, total=4082.65, n_correct=2485.57, ppl=5.37, accuracy=60.881, wps=11360.3, ups=1.39, wpb=8165.3, bsz=297.7, num_updates=16700, lr=0.000109435, gnorm=0.674, clip=0, loss_scale=1, train_wall=71, gb_free=17, wall=15604
2023-09-02 14:13:32 | INFO | train_inner | epoch 012:    597 / 1474 loss=2.09, trans_loss=5.114, nll_loss=2.41, w2v_ctc_loss=0.775, task_loss=5.129, task_loss_gen=5.325, contrastive_loss=0, total=4217.4, n_correct=2570.25, ppl=5.31, accuracy=60.944, wps=11477, ups=1.36, wpb=8434.8, bsz=320.8, num_updates=16800, lr=0.000109109, gnorm=0.745, clip=0, loss_scale=1, train_wall=73, gb_free=15.7, wall=15678
2023-09-02 14:14:44 | INFO | train_inner | epoch 012:    697 / 1474 loss=2.078, trans_loss=5.104, nll_loss=2.397, w2v_ctc_loss=0.76, task_loss=5.094, task_loss_gen=5.268, contrastive_loss=0, total=4197.24, n_correct=2576.13, ppl=5.27, accuracy=61.377, wps=11666.2, ups=1.39, wpb=8394.5, bsz=323.6, num_updates=16900, lr=0.000108786, gnorm=0.732, clip=0, loss_scale=1, train_wall=71, gb_free=14.9, wall=15749
2023-09-02 14:15:57 | INFO | train_inner | epoch 012:    797 / 1474 loss=2.084, trans_loss=5.101, nll_loss=2.393, w2v_ctc_loss=0.771, task_loss=5.771, task_loss_gen=5.889, contrastive_loss=0, total=4083.9, n_correct=2501.83, ppl=5.25, accuracy=61.261, wps=11163.2, ups=1.37, wpb=8167.8, bsz=296.3, num_updates=17000, lr=0.000108465, gnorm=0.604, clip=0, loss_scale=2, train_wall=72, gb_free=12.5, wall=15823
2023-09-02 14:17:11 | INFO | train_inner | epoch 012:    897 / 1474 loss=2.085, trans_loss=5.102, nll_loss=2.395, w2v_ctc_loss=0.773, task_loss=5.642, task_loss_gen=5.878, contrastive_loss=0, total=4168.41, n_correct=2556.92, ppl=5.26, accuracy=61.34, wps=11415.8, ups=1.37, wpb=8336.8, bsz=306.2, num_updates=17100, lr=0.000108148, gnorm=0.575, clip=0, loss_scale=2, train_wall=72, gb_free=15.1, wall=15896
2023-09-02 14:18:23 | INFO | train_inner | epoch 012:    997 / 1474 loss=2.09, trans_loss=5.107, nll_loss=2.402, w2v_ctc_loss=0.779, task_loss=5.537, task_loss_gen=5.848, contrastive_loss=0, total=4121.91, n_correct=2521.84, ppl=5.29, accuracy=61.181, wps=11398.5, ups=1.38, wpb=8243.8, bsz=301.5, num_updates=17200, lr=0.000107833, gnorm=0.575, clip=0, loss_scale=2, train_wall=72, gb_free=15.9, wall=15968
2023-09-02 14:19:35 | INFO | train_inner | epoch 012:   1097 / 1474 loss=2.091, trans_loss=5.108, nll_loss=2.404, w2v_ctc_loss=0.779, task_loss=5.726, task_loss_gen=5.937, contrastive_loss=0, total=4055.97, n_correct=2482.32, ppl=5.29, accuracy=61.202, wps=11174.1, ups=1.38, wpb=8111.9, bsz=291.2, num_updates=17300, lr=0.000107521, gnorm=0.586, clip=0, loss_scale=2, train_wall=72, gb_free=11.7, wall=16041
2023-09-02 14:20:48 | INFO | train_inner | epoch 012:   1197 / 1474 loss=2.1, trans_loss=5.127, nll_loss=2.427, w2v_ctc_loss=0.799, task_loss=5.204, task_loss_gen=5.66, contrastive_loss=0, total=4187.55, n_correct=2550.9, ppl=5.38, accuracy=60.916, wps=11536.9, ups=1.38, wpb=8375.1, bsz=317.5, num_updates=17400, lr=0.000107211, gnorm=0.571, clip=0, loss_scale=2, train_wall=72, gb_free=15.6, wall=16113
2023-09-02 14:22:01 | INFO | train_inner | epoch 012:   1297 / 1474 loss=2.092, trans_loss=5.105, nll_loss=2.399, w2v_ctc_loss=0.786, task_loss=6.292, task_loss_gen=6.361, contrastive_loss=0, total=4075.32, n_correct=2494.37, ppl=5.27, accuracy=61.207, wps=11128.9, ups=1.37, wpb=8150.6, bsz=287, num_updates=17500, lr=0.000106904, gnorm=0.603, clip=0, loss_scale=2, train_wall=72, gb_free=16.8, wall=16186
2023-09-02 14:23:14 | INFO | train_inner | epoch 012:   1397 / 1474 loss=2.086, trans_loss=5.112, nll_loss=2.408, w2v_ctc_loss=0.776, task_loss=5.546, task_loss_gen=5.866, contrastive_loss=0, total=4137.07, n_correct=2531.63, ppl=5.31, accuracy=61.194, wps=11332.2, ups=1.37, wpb=8274.1, bsz=305.6, num_updates=17600, lr=0.0001066, gnorm=0.566, clip=0, loss_scale=2, train_wall=72, gb_free=16.7, wall=16259
2023-09-02 14:24:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 14:24:42 | INFO | dev_st | epoch 012 | valid on 'dev_st' subset | loss 4.002 | trans_loss 5.311 | nll_loss 2.616 | w2v_ctc_loss 1.372 | task_loss 13.086 | task_loss_gen 16.495 | contrastive_loss 0 | total 4003.4 | n_correct 2565.2 | ppl 6.13 | accuracy 64.076 | uer 18.865 | wer 20.629 | raw_wer 20.629 | bleu 19.9 | wps 1650.2 | wpb 4003.4 | bsz 141.8 | num_updates 17677 | best_bleu 19.9
2023-09-02 14:24:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 17677 updates
2023-09-02 14:24:42 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 14:24:49 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 14:24:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 12 @ 17677 updates, score 19.9) (writing took 13.264171102986438 seconds)
2023-09-02 14:24:56 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-09-02 14:24:56 | INFO | train | epoch 012 | loss 2.09 | trans_loss 5.112 | nll_loss 2.408 | w2v_ctc_loss 0.777 | task_loss 5.507 | task_loss_gen 5.754 | contrastive_loss 0 | total 4138.65 | n_correct 2527.38 | ppl 5.31 | accuracy 61.068 | wps 10844.7 | ups 1.31 | wpb 8277.3 | bsz 305.7 | num_updates 17677 | lr 0.000106368 | gnorm 0.646 | clip 0 | loss_scale 2 | train_wall 1059 | gb_free 12.4 | wall 16361
2023-09-02 14:24:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 14:24:56 | INFO | fairseq.trainer | begin training epoch 13
2023-09-02 14:24:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 14:25:20 | INFO | train_inner | epoch 013:     23 / 1474 loss=2.086, trans_loss=5.104, nll_loss=2.397, w2v_ctc_loss=0.782, task_loss=5.619, task_loss_gen=5.989, contrastive_loss=0, total=4092.72, n_correct=2514, ppl=5.27, accuracy=61.426, wps=6500.4, ups=0.79, wpb=8185.4, bsz=296.2, num_updates=17700, lr=0.000106299, gnorm=0.567, clip=0, loss_scale=2, train_wall=71, gb_free=16, wall=16385
2023-09-02 14:26:33 | INFO | train_inner | epoch 013:    123 / 1474 loss=2.063, trans_loss=5.07, nll_loss=2.354, w2v_ctc_loss=0.756, task_loss=5.662, task_loss_gen=5.669, contrastive_loss=0, total=4178.31, n_correct=2593.22, ppl=5.11, accuracy=62.064, wps=11527.3, ups=1.38, wpb=8356.6, bsz=304, num_updates=17800, lr=0.000106, gnorm=0.585, clip=0, loss_scale=2, train_wall=72, gb_free=16.5, wall=16458
2023-09-02 14:27:46 | INFO | train_inner | epoch 013:    223 / 1474 loss=2.064, trans_loss=5.078, nll_loss=2.365, w2v_ctc_loss=0.759, task_loss=5.167, task_loss_gen=5.358, contrastive_loss=0, total=4188.38, n_correct=2592.73, ppl=5.15, accuracy=61.903, wps=11407.3, ups=1.36, wpb=8376.8, bsz=326.5, num_updates=17900, lr=0.000105703, gnorm=0.566, clip=0, loss_scale=2, train_wall=73, gb_free=14.2, wall=16531
2023-09-02 14:28:59 | INFO | train_inner | epoch 013:    323 / 1474 loss=2.06, trans_loss=5.062, nll_loss=2.343, w2v_ctc_loss=0.754, task_loss=5.621, task_loss_gen=6.019, contrastive_loss=0, total=4101.73, n_correct=2556.4, ppl=5.07, accuracy=62.325, wps=11222.3, ups=1.37, wpb=8203.5, bsz=293.4, num_updates=18000, lr=0.000105409, gnorm=0.564, clip=0, loss_scale=2, train_wall=72, gb_free=17.4, wall=16604
2023-09-02 14:28:59 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 14:29:32 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 3.99 | trans_loss 5.297 | nll_loss 2.596 | w2v_ctc_loss 1.362 | task_loss 13.404 | task_loss_gen 16.481 | contrastive_loss 0 | total 4003.4 | n_correct 2568.2 | ppl 6.05 | accuracy 64.15 | uer 18.894 | wer 20.607 | raw_wer 20.607 | bleu 19.59 | wps 1640.1 | wpb 4003.4 | bsz 141.8 | num_updates 18000 | best_bleu 19.9
2023-09-02 14:29:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 18000 updates
2023-09-02 14:29:32 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_13_18000.pt
2023-09-02 14:29:35 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_13_18000.pt
2023-09-02 14:29:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_13_18000.pt (epoch 13 @ 18000 updates, score 19.59) (writing took 8.263249126001028 seconds)
2023-09-02 14:30:53 | INFO | train_inner | epoch 013:    423 / 1474 loss=2.058, trans_loss=5.067, nll_loss=2.351, w2v_ctc_loss=0.759, task_loss=5.075, task_loss_gen=5.241, contrastive_loss=0, total=4205.54, n_correct=2616.89, ppl=5.1, accuracy=62.225, wps=7383, ups=0.88, wpb=8411.1, bsz=323.5, num_updates=18100, lr=0.000105118, gnorm=0.572, clip=0, loss_scale=2, train_wall=71, gb_free=16.5, wall=16718
2023-09-02 14:32:06 | INFO | train_inner | epoch 013:    523 / 1474 loss=2.063, trans_loss=5.074, nll_loss=2.359, w2v_ctc_loss=0.759, task_loss=5.087, task_loss_gen=5.603, contrastive_loss=0, total=4185.31, n_correct=2594.97, ppl=5.13, accuracy=62.002, wps=11450.3, ups=1.37, wpb=8370.6, bsz=316.7, num_updates=18200, lr=0.000104828, gnorm=0.562, clip=0, loss_scale=2, train_wall=72, gb_free=16.7, wall=16791
2023-09-02 14:33:19 | INFO | train_inner | epoch 013:    623 / 1474 loss=2.064, trans_loss=5.073, nll_loss=2.358, w2v_ctc_loss=0.765, task_loss=5.401, task_loss_gen=5.709, contrastive_loss=0, total=4157.86, n_correct=2582.69, ppl=5.13, accuracy=62.116, wps=11475.4, ups=1.38, wpb=8315.7, bsz=306.4, num_updates=18300, lr=0.000104542, gnorm=0.577, clip=0, loss_scale=2, train_wall=72, gb_free=16.9, wall=16864
2023-09-02 14:34:31 | INFO | train_inner | epoch 013:    723 / 1474 loss=2.074, trans_loss=5.076, nll_loss=2.361, w2v_ctc_loss=0.776, task_loss=6.255, task_loss_gen=6.333, contrastive_loss=0, total=4099, n_correct=2537.01, ppl=5.14, accuracy=61.893, wps=11319.6, ups=1.38, wpb=8198, bsz=286.3, num_updates=18400, lr=0.000104257, gnorm=0.592, clip=0, loss_scale=2, train_wall=72, gb_free=15.8, wall=16936
2023-09-02 14:35:44 | INFO | train_inner | epoch 013:    823 / 1474 loss=2.063, trans_loss=5.073, nll_loss=2.359, w2v_ctc_loss=0.759, task_loss=5.961, task_loss_gen=5.873, contrastive_loss=0, total=4122.84, n_correct=2556.09, ppl=5.13, accuracy=61.998, wps=11250.7, ups=1.36, wpb=8245.7, bsz=305.7, num_updates=18500, lr=0.000103975, gnorm=0.605, clip=0, loss_scale=2, train_wall=73, gb_free=16.8, wall=17010
2023-09-02 14:36:57 | INFO | train_inner | epoch 013:    923 / 1474 loss=2.062, trans_loss=5.067, nll_loss=2.352, w2v_ctc_loss=0.764, task_loss=5.572, task_loss_gen=5.819, contrastive_loss=0, total=4100.74, n_correct=2551.65, ppl=5.1, accuracy=62.224, wps=11354, ups=1.38, wpb=8201.5, bsz=296.8, num_updates=18600, lr=0.000103695, gnorm=0.575, clip=0, loss_scale=2, train_wall=71, gb_free=15.7, wall=17082
2023-09-02 14:38:09 | INFO | train_inner | epoch 013:   1023 / 1474 loss=2.071, trans_loss=5.074, nll_loss=2.361, w2v_ctc_loss=0.775, task_loss=5.845, task_loss_gen=6.068, contrastive_loss=0, total=4080.72, n_correct=2528.48, ppl=5.14, accuracy=61.962, wps=11299.3, ups=1.38, wpb=8161.4, bsz=292.3, num_updates=18700, lr=0.000103418, gnorm=0.584, clip=0, loss_scale=2, train_wall=72, gb_free=17.1, wall=17154
2023-09-02 14:39:20 | INFO | train_inner | epoch 013:   1123 / 1474 loss=2.053, trans_loss=5.057, nll_loss=2.338, w2v_ctc_loss=0.756, task_loss=5.444, task_loss_gen=5.586, contrastive_loss=0, total=4103.17, n_correct=2564, ppl=5.06, accuracy=62.488, wps=11486.9, ups=1.4, wpb=8206.3, bsz=305.6, num_updates=18800, lr=0.000103142, gnorm=0.587, clip=0, loss_scale=2, train_wall=71, gb_free=17.3, wall=17226
2023-09-02 14:40:33 | INFO | train_inner | epoch 013:   1223 / 1474 loss=2.065, trans_loss=5.069, nll_loss=2.354, w2v_ctc_loss=0.77, task_loss=5.767, task_loss_gen=6.079, contrastive_loss=0, total=4124.88, n_correct=2563.64, ppl=5.11, accuracy=62.151, wps=11345.7, ups=1.38, wpb=8249.8, bsz=296.7, num_updates=18900, lr=0.000102869, gnorm=0.585, clip=0, loss_scale=2, train_wall=72, gb_free=17.4, wall=17298
2023-09-02 14:41:46 | INFO | train_inner | epoch 013:   1323 / 1474 loss=2.049, trans_loss=5.053, nll_loss=2.333, w2v_ctc_loss=0.751, task_loss=5.418, task_loss_gen=5.672, contrastive_loss=0, total=4108.18, n_correct=2572.23, ppl=5.04, accuracy=62.612, wps=11290.8, ups=1.37, wpb=8216.4, bsz=308.4, num_updates=19000, lr=0.000102598, gnorm=0.556, clip=0, loss_scale=4, train_wall=72, gb_free=16.1, wall=17371
2023-09-02 14:42:59 | INFO | train_inner | epoch 013:   1423 / 1474 loss=2.055, trans_loss=5.062, nll_loss=2.346, w2v_ctc_loss=0.756, task_loss=5.276, task_loss_gen=5.684, contrastive_loss=0, total=4171.47, n_correct=2600.67, ppl=5.08, accuracy=62.344, wps=11363.3, ups=1.36, wpb=8342.9, bsz=310.5, num_updates=19100, lr=0.000102329, gnorm=0.523, clip=0, loss_scale=4, train_wall=73, gb_free=14.8, wall=17445
2023-09-02 14:43:36 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 14:44:09 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 3.972 | trans_loss 5.272 | nll_loss 2.565 | w2v_ctc_loss 1.36 | task_loss 11.521 | task_loss_gen 16.953 | contrastive_loss 0 | total 4003.4 | n_correct 2583.4 | ppl 5.92 | accuracy 64.53 | uer 18.546 | wer 20.264 | raw_wer 20.264 | bleu 20.08 | wps 1650 | wpb 4003.4 | bsz 141.8 | num_updates 19151 | best_bleu 20.08
2023-09-02 14:44:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 19151 updates
2023-09-02 14:44:09 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 14:44:16 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 14:44:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 13 @ 19151 updates, score 20.08) (writing took 13.660846620972734 seconds)
2023-09-02 14:44:23 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-09-02 14:44:23 | INFO | train | epoch 013 | loss 2.061 | trans_loss 5.067 | nll_loss 2.351 | w2v_ctc_loss 0.761 | task_loss 5.498 | task_loss_gen 5.738 | contrastive_loss 0 | total 4138.65 | n_correct 2573.44 | ppl 5.1 | accuracy 62.181 | wps 10458.5 | ups 1.26 | wpb 8277.3 | bsz 305.7 | num_updates 19151 | lr 0.000102193 | gnorm 0.571 | clip 0 | loss_scale 4 | train_wall 1059 | gb_free 17.3 | wall 17528
2023-09-02 14:44:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 14:44:23 | INFO | fairseq.trainer | begin training epoch 14
2023-09-02 14:44:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 14:45:06 | INFO | train_inner | epoch 014:     49 / 1474 loss=2.038, trans_loss=5.03, nll_loss=2.304, w2v_ctc_loss=0.752, task_loss=4.808, task_loss_gen=5.255, contrastive_loss=0, total=4182.69, n_correct=2634.82, ppl=4.94, accuracy=62.993, wps=6607.6, ups=0.79, wpb=8365.4, bsz=322.3, num_updates=19200, lr=0.000102062, gnorm=0.514, clip=0, loss_scale=4, train_wall=71, gb_free=15.6, wall=17571
2023-09-02 14:46:18 | INFO | train_inner | epoch 014:    149 / 1474 loss=2.036, trans_loss=5.023, nll_loss=2.294, w2v_ctc_loss=0.747, task_loss=5.307, task_loss_gen=5.692, contrastive_loss=0, total=4086.4, n_correct=2583, ppl=4.9, accuracy=63.21, wps=11326.7, ups=1.39, wpb=8172.8, bsz=301.5, num_updates=19300, lr=0.000101797, gnorm=0.526, clip=0, loss_scale=4, train_wall=71, gb_free=16.7, wall=17643
2023-09-02 14:47:30 | INFO | train_inner | epoch 014:    249 / 1474 loss=2.046, trans_loss=5.04, nll_loss=2.316, w2v_ctc_loss=0.75, task_loss=5.866, task_loss_gen=6.175, contrastive_loss=0, total=4103.37, n_correct=2577.52, ppl=4.98, accuracy=62.815, wps=11406.7, ups=1.39, wpb=8206.7, bsz=294, num_updates=19400, lr=0.000101535, gnorm=0.534, clip=0, loss_scale=4, train_wall=71, gb_free=16.9, wall=17715
2023-09-02 14:48:43 | INFO | train_inner | epoch 014:    349 / 1474 loss=2.037, trans_loss=5.037, nll_loss=2.313, w2v_ctc_loss=0.743, task_loss=5.148, task_loss_gen=5.515, contrastive_loss=0, total=4168.35, n_correct=2625.35, ppl=4.97, accuracy=62.983, wps=11483.9, ups=1.38, wpb=8336.7, bsz=318.7, num_updates=19500, lr=0.000101274, gnorm=0.519, clip=0, loss_scale=4, train_wall=72, gb_free=15.9, wall=17788
2023-09-02 14:49:55 | INFO | train_inner | epoch 014:    449 / 1474 loss=2.037, trans_loss=5.036, nll_loss=2.311, w2v_ctc_loss=0.74, task_loss=5.374, task_loss_gen=5.664, contrastive_loss=0, total=4155.83, n_correct=2619.19, ppl=4.96, accuracy=63.024, wps=11425.2, ups=1.37, wpb=8311.7, bsz=306.7, num_updates=19600, lr=0.000101015, gnorm=0.523, clip=0, loss_scale=4, train_wall=72, gb_free=15.5, wall=17861
2023-09-02 14:51:08 | INFO | train_inner | epoch 014:    549 / 1474 loss=2.048, trans_loss=5.037, nll_loss=2.312, w2v_ctc_loss=0.759, task_loss=5.908, task_loss_gen=6.227, contrastive_loss=0, total=4064.87, n_correct=2551.61, ppl=4.97, accuracy=62.772, wps=11123.8, ups=1.37, wpb=8129.7, bsz=288.5, num_updates=19700, lr=0.000100759, gnorm=0.526, clip=0, loss_scale=4, train_wall=72, gb_free=17.5, wall=17934
2023-09-02 14:52:21 | INFO | train_inner | epoch 014:    649 / 1474 loss=2.043, trans_loss=5.039, nll_loss=2.315, w2v_ctc_loss=0.752, task_loss=5.258, task_loss_gen=5.716, contrastive_loss=0, total=4167.34, n_correct=2622.51, ppl=4.98, accuracy=62.93, wps=11454.8, ups=1.37, wpb=8334.7, bsz=307.8, num_updates=19800, lr=0.000100504, gnorm=0.512, clip=0, loss_scale=4, train_wall=72, gb_free=16.8, wall=18006
2023-09-02 14:53:34 | INFO | train_inner | epoch 014:    749 / 1474 loss=2.036, trans_loss=5.027, nll_loss=2.3, w2v_ctc_loss=0.747, task_loss=5.081, task_loss_gen=5.738, contrastive_loss=0, total=4142.94, n_correct=2614.27, ppl=4.92, accuracy=63.102, wps=11466.6, ups=1.38, wpb=8285.9, bsz=308.6, num_updates=19900, lr=0.000100251, gnorm=0.514, clip=0, loss_scale=4, train_wall=71, gb_free=16.1, wall=18079
2023-09-02 14:54:46 | INFO | train_inner | epoch 014:    849 / 1474 loss=2.034, trans_loss=5.028, nll_loss=2.302, w2v_ctc_loss=0.745, task_loss=5.036, task_loss_gen=5.495, contrastive_loss=0, total=4173.06, n_correct=2629.71, ppl=4.93, accuracy=63.016, wps=11538.2, ups=1.38, wpb=8346.1, bsz=319.1, num_updates=20000, lr=0.0001, gnorm=0.519, clip=0, loss_scale=4, train_wall=72, gb_free=12.4, wall=18151
2023-09-02 14:54:46 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 14:55:20 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 3.965 | trans_loss 5.256 | nll_loss 2.541 | w2v_ctc_loss 1.372 | task_loss 11.668 | task_loss_gen 16.865 | contrastive_loss 0 | total 4003.4 | n_correct 2597.9 | ppl 5.82 | accuracy 64.892 | uer 18.602 | wer 20.6 | raw_wer 20.6 | bleu 20.4 | wps 1567.9 | wpb 4003.4 | bsz 141.8 | num_updates 20000 | best_bleu 20.4
2023-09-02 14:55:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20000 updates
2023-09-02 14:55:20 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_14_20000.pt
2023-09-02 14:55:22 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_14_20000.pt
2023-09-02 14:55:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_14_20000.pt (epoch 14 @ 20000 updates, score 20.4) (writing took 12.804669092001859 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:0')
2023-09-02 14:56:47 | INFO | train_inner | epoch 014:    949 / 1474 loss=2.04, trans_loss=5.036, nll_loss=2.312, w2v_ctc_loss=0.746, task_loss=5.234, task_loss_gen=5.739, contrastive_loss=0, total=4166.71, n_correct=2620.02, ppl=4.96, accuracy=62.88, wps=6882.8, ups=0.83, wpb=8333.4, bsz=310.6, num_updates=20100, lr=9.97509e-05, gnorm=0.517, clip=0, loss_scale=4, train_wall=73, gb_free=16.2, wall=18272
2023-09-02 14:58:00 | INFO | train_inner | epoch 014:   1049 / 1474 loss=2.035, trans_loss=5.031, nll_loss=2.306, w2v_ctc_loss=0.738, task_loss=5.224, task_loss_gen=5.944, contrastive_loss=0, total=4145.57, n_correct=2616.15, ppl=4.95, accuracy=63.107, wps=11288, ups=1.36, wpb=8291.1, bsz=301.1, num_updates=20200, lr=9.95037e-05, gnorm=0.517, clip=0, loss_scale=4, train_wall=73, gb_free=17, wall=18346
2023-09-02 14:59:14 | INFO | train_inner | epoch 014:   1149 / 1474 loss=2.041, trans_loss=5.036, nll_loss=2.313, w2v_ctc_loss=0.752, task_loss=5.08, task_loss_gen=5.542, contrastive_loss=0, total=4219.9, n_correct=2652.44, ppl=4.97, accuracy=62.856, wps=11532.9, ups=1.37, wpb=8439.8, bsz=325.2, num_updates=20300, lr=9.92583e-05, gnorm=0.518, clip=0, loss_scale=4, train_wall=72, gb_free=16, wall=18419
2023-09-02 15:00:26 | INFO | train_inner | epoch 014:   1249 / 1474 loss=2.052, trans_loss=5.042, nll_loss=2.319, w2v_ctc_loss=0.762, task_loss=6.504, task_loss_gen=6.846, contrastive_loss=0, total=4032.06, n_correct=2535.53, ppl=4.99, accuracy=62.884, wps=11093.1, ups=1.38, wpb=8064.1, bsz=274.4, num_updates=20400, lr=9.90148e-05, gnorm=0.543, clip=0, loss_scale=4, train_wall=72, gb_free=17.3, wall=18491
2023-09-02 15:01:39 | INFO | train_inner | epoch 014:   1349 / 1474 loss=2.032, trans_loss=5.032, nll_loss=2.307, w2v_ctc_loss=0.74, task_loss=5.694, task_loss_gen=5.604, contrastive_loss=0, total=4205.07, n_correct=2656.3, ppl=4.95, accuracy=63.169, wps=11639.2, ups=1.38, wpb=8410.1, bsz=317.3, num_updates=20500, lr=9.8773e-05, gnorm=0.553, clip=0, loss_scale=4, train_wall=71, gb_free=16.3, wall=18564
2023-09-02 15:02:51 | INFO | train_inner | epoch 014:   1449 / 1474 loss=2.038, trans_loss=5.034, nll_loss=2.31, w2v_ctc_loss=0.745, task_loss=5.632, task_loss_gen=5.826, contrastive_loss=0, total=4126.44, n_correct=2601.86, ppl=4.96, accuracy=63.053, wps=11398.8, ups=1.38, wpb=8252.9, bsz=303.9, num_updates=20600, lr=9.85329e-05, gnorm=0.523, clip=0, loss_scale=4, train_wall=72, gb_free=15.8, wall=18636
2023-09-02 15:03:09 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:3')
2023-09-02 15:03:42 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 3.968 | trans_loss 5.246 | nll_loss 2.534 | w2v_ctc_loss 1.403 | task_loss 11.415 | task_loss_gen 16.911 | contrastive_loss 0 | total 4003.4 | n_correct 2602.1 | ppl 5.79 | accuracy 64.997 | uer 18.69 | wer 20.458 | raw_wer 20.458 | bleu 20.6 | wps 1631 | wpb 4003.4 | bsz 141.8 | num_updates 20625 | best_bleu 20.6
2023-09-02 15:03:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20625 updates
2023-09-02 15:03:42 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 15:03:49 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 15:03:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 14 @ 20625 updates, score 20.6) (writing took 13.459946185990702 seconds)
2023-09-02 15:03:56 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-09-02 15:03:56 | INFO | train | epoch 014 | loss 2.04 | trans_loss 5.034 | nll_loss 2.309 | w2v_ctc_loss 0.748 | task_loss 5.434 | task_loss_gen 5.815 | contrastive_loss 0 | total 4138.65 | n_correct 2606.68 | ppl 4.96 | accuracy 62.984 | wps 10395.7 | ups 1.26 | wpb 8277.3 | bsz 305.7 | num_updates 20625 | lr 9.84732e-05 | gnorm 0.525 | clip 0 | loss_scale 4 | train_wall 1058 | gb_free 16 | wall 18702
2023-09-02 15:03:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 15:03:57 | INFO | fairseq.trainer | begin training epoch 15
2023-09-02 15:03:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 15:04:57 | INFO | train_inner | epoch 015:     75 / 1474 loss=2.03, trans_loss=5.019, nll_loss=2.29, w2v_ctc_loss=0.742, task_loss=5.471, task_loss_gen=5.8, contrastive_loss=0, total=4090.99, n_correct=2591.77, ppl=4.89, accuracy=63.353, wps=6483, ups=0.79, wpb=8182, bsz=300.8, num_updates=20700, lr=9.82946e-05, gnorm=0.519, clip=0, loss_scale=4, train_wall=71, gb_free=16.6, wall=18762
2023-09-02 15:06:10 | INFO | train_inner | epoch 015:    175 / 1474 loss=2.026, trans_loss=5.01, nll_loss=2.278, w2v_ctc_loss=0.739, task_loss=5.227, task_loss_gen=6.072, contrastive_loss=0, total=4115.56, n_correct=2615.87, ppl=4.85, accuracy=63.56, wps=11335, ups=1.38, wpb=8231.1, bsz=298.5, num_updates=20800, lr=9.80581e-05, gnorm=0.516, clip=0, loss_scale=4, train_wall=72, gb_free=16.5, wall=18835
2023-09-02 15:07:22 | INFO | train_inner | epoch 015:    275 / 1474 loss=2.019, trans_loss=5.01, nll_loss=2.277, w2v_ctc_loss=0.727, task_loss=4.993, task_loss_gen=5.588, contrastive_loss=0, total=4182.19, n_correct=2661.97, ppl=4.85, accuracy=63.65, wps=11557.1, ups=1.38, wpb=8364.4, bsz=310.5, num_updates=20900, lr=9.78232e-05, gnorm=0.511, clip=0, loss_scale=4, train_wall=72, gb_free=16.1, wall=18907
2023-09-02 15:08:34 | INFO | train_inner | epoch 015:    375 / 1474 loss=2.022, trans_loss=5.005, nll_loss=2.272, w2v_ctc_loss=0.733, task_loss=5.116, task_loss_gen=5.774, contrastive_loss=0, total=4172.52, n_correct=2653.06, ppl=4.83, accuracy=63.584, wps=11541.5, ups=1.38, wpb=8345, bsz=307.7, num_updates=21000, lr=9.759e-05, gnorm=0.527, clip=0, loss_scale=4, train_wall=72, gb_free=16.3, wall=18980
2023-09-02 15:09:48 | INFO | train_inner | epoch 015:    475 / 1474 loss=2.023, trans_loss=5.009, nll_loss=2.276, w2v_ctc_loss=0.728, task_loss=4.817, task_loss_gen=6.28, contrastive_loss=0, total=4076.84, n_correct=2588.56, ppl=4.84, accuracy=63.494, wps=11116.7, ups=1.36, wpb=8153.7, bsz=293.4, num_updates=21100, lr=9.73585e-05, gnorm=0.513, clip=0, loss_scale=8, train_wall=73, gb_free=16.4, wall=19053
2023-09-02 15:11:00 | INFO | train_inner | epoch 015:    575 / 1474 loss=2.025, trans_loss=5.008, nll_loss=2.276, w2v_ctc_loss=0.74, task_loss=4.411, task_loss_gen=6.106, contrastive_loss=0, total=4156.05, n_correct=2642.77, ppl=4.84, accuracy=63.589, wps=11456.8, ups=1.38, wpb=8312.1, bsz=302.8, num_updates=21200, lr=9.71286e-05, gnorm=0.506, clip=0, loss_scale=8, train_wall=72, gb_free=11.2, wall=19126
2023-09-02 15:12:13 | INFO | train_inner | epoch 015:    675 / 1474 loss=2.026, trans_loss=5.006, nll_loss=2.273, w2v_ctc_loss=0.739, task_loss=4.754, task_loss_gen=6.053, contrastive_loss=0, total=4118.87, n_correct=2616.7, ppl=4.83, accuracy=63.53, wps=11397.9, ups=1.38, wpb=8237.7, bsz=303, num_updates=21300, lr=9.69003e-05, gnorm=0.512, clip=0, loss_scale=8, train_wall=71, gb_free=16.6, wall=19198
2023-09-02 15:13:26 | INFO | train_inner | epoch 015:    775 / 1474 loss=2.028, trans_loss=5.015, nll_loss=2.284, w2v_ctc_loss=0.739, task_loss=4.895, task_loss_gen=6.042, contrastive_loss=0, total=4176.64, n_correct=2649.4, ppl=4.87, accuracy=63.434, wps=11420.5, ups=1.37, wpb=8353.3, bsz=305.3, num_updates=21400, lr=9.66736e-05, gnorm=0.51, clip=0, loss_scale=8, train_wall=72, gb_free=13.6, wall=19271
2023-09-02 15:14:38 | INFO | train_inner | epoch 015:    875 / 1474 loss=2.033, trans_loss=5.018, nll_loss=2.29, w2v_ctc_loss=0.744, task_loss=7.639, task_loss_gen=7.257, contrastive_loss=0, total=4056.99, n_correct=2568.1, ppl=4.89, accuracy=63.301, wps=11280, ups=1.39, wpb=8114, bsz=288, num_updates=21500, lr=9.64486e-05, gnorm=0.555, clip=0, loss_scale=8, train_wall=71, gb_free=17, wall=19343
2023-09-02 15:15:50 | INFO | train_inner | epoch 015:    975 / 1474 loss=2.022, trans_loss=5.01, nll_loss=2.278, w2v_ctc_loss=0.735, task_loss=6.511, task_loss_gen=5.982, contrastive_loss=0, total=4134.44, n_correct=2631.25, ppl=4.85, accuracy=63.642, wps=11451.6, ups=1.38, wpb=8268.9, bsz=304.8, num_updates=21600, lr=9.6225e-05, gnorm=0.526, clip=0, loss_scale=8, train_wall=72, gb_free=15.8, wall=19415
2023-09-02 15:17:04 | INFO | train_inner | epoch 015:   1075 / 1474 loss=2.017, trans_loss=5.008, nll_loss=2.278, w2v_ctc_loss=0.728, task_loss=5.554, task_loss_gen=5.511, contrastive_loss=0, total=4185.02, n_correct=2661.77, ppl=4.85, accuracy=63.602, wps=11333.2, ups=1.35, wpb=8370, bsz=324.2, num_updates=21700, lr=9.60031e-05, gnorm=0.506, clip=0, loss_scale=8, train_wall=73, gb_free=16.4, wall=19489
2023-09-02 15:18:16 | INFO | train_inner | epoch 015:   1175 / 1474 loss=2.011, trans_loss=5.003, nll_loss=2.272, w2v_ctc_loss=0.726, task_loss=4.756, task_loss_gen=5.181, contrastive_loss=0, total=4187.68, n_correct=2673.38, ppl=4.83, accuracy=63.839, wps=11594.6, ups=1.38, wpb=8375.4, bsz=329.7, num_updates=21800, lr=9.57826e-05, gnorm=0.504, clip=0, loss_scale=8, train_wall=71, gb_free=16.4, wall=19561
2023-09-02 15:19:29 | INFO | train_inner | epoch 015:   1275 / 1474 loss=2.022, trans_loss=5.001, nll_loss=2.267, w2v_ctc_loss=0.741, task_loss=5.361, task_loss_gen=6.055, contrastive_loss=0, total=4141.6, n_correct=2638.01, ppl=4.81, accuracy=63.695, wps=11390.2, ups=1.38, wpb=8283.2, bsz=301.1, num_updates=21900, lr=9.55637e-05, gnorm=0.506, clip=0, loss_scale=8, train_wall=72, gb_free=13, wall=19634
2023-09-02 15:20:41 | INFO | train_inner | epoch 015:   1375 / 1474 loss=2.023, trans_loss=5.004, nll_loss=2.271, w2v_ctc_loss=0.739, task_loss=5.086, task_loss_gen=6.156, contrastive_loss=0, total=4099.6, n_correct=2612.32, ppl=4.83, accuracy=63.721, wps=11277.1, ups=1.38, wpb=8199.2, bsz=293.2, num_updates=22000, lr=9.53463e-05, gnorm=0.517, clip=0, loss_scale=8, train_wall=72, gb_free=13.9, wall=19707
2023-09-02 15:20:41 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 15:21:14 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 3.927 | trans_loss 5.234 | nll_loss 2.511 | w2v_ctc_loss 1.293 | task_loss 9.78 | task_loss_gen 17.876 | contrastive_loss 0 | total 4003.4 | n_correct 2615.4 | ppl 5.7 | accuracy 65.329 | uer 17.739 | wer 19.384 | raw_wer 19.384 | bleu 21.05 | wps 1653 | wpb 4003.4 | bsz 141.8 | num_updates 22000 | best_bleu 21.05
2023-09-02 15:21:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22000 updates
2023-09-02 15:21:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_15_22000.pt
2023-09-02 15:21:16 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_15_22000.pt
2023-09-02 15:21:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_15_22000.pt (epoch 15 @ 22000 updates, score 21.05) (writing took 13.532768105040304 seconds)
2023-09-02 15:22:41 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 15:23:15 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 3.939 | trans_loss 5.238 | nll_loss 2.519 | w2v_ctc_loss 1.327 | task_loss 9.809 | task_loss_gen 17.879 | contrastive_loss 0 | total 4003.4 | n_correct 2612.4 | ppl 5.73 | accuracy 65.255 | uer 18.138 | wer 19.839 | raw_wer 19.839 | bleu 20.91 | wps 1581.5 | wpb 4003.4 | bsz 141.8 | num_updates 22099 | best_bleu 21.05
2023-09-02 15:23:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22099 updates
2023-09-02 15:23:15 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_20.9100.pt
2023-09-02 15:23:17 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_20.9100.pt
2023-09-02 15:23:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_20.9100.pt (epoch 15 @ 22099 updates, score 20.91) (writing took 7.992931035987567 seconds)
2023-09-02 15:23:23 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-09-02 15:23:23 | INFO | train | epoch 015 | loss 2.023 | trans_loss 5.008 | nll_loss 2.277 | w2v_ctc_loss 0.735 | task_loss 5.259 | task_loss_gen 5.946 | contrastive_loss 0 | total 4138.65 | n_correct 2631.79 | ppl 4.85 | accuracy 63.59 | wps 10455.7 | ups 1.26 | wpb 8277.3 | bsz 305.7 | num_updates 22099 | lr 9.51325e-05 | gnorm 0.515 | clip 0 | loss_scale 8 | train_wall 1059 | gb_free 16.6 | wall 19869
2023-09-02 15:23:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 15:23:24 | INFO | fairseq.trainer | begin training epoch 16
2023-09-02 15:23:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 15:23:31 | INFO | train_inner | epoch 016:      1 / 1474 loss=2.023, trans_loss=5.013, nll_loss=2.284, w2v_ctc_loss=0.74, task_loss=4.647, task_loss_gen=5.635, contrastive_loss=0, total=4149.9, n_correct=2639.38, ppl=4.87, accuracy=63.601, wps=4891.1, ups=0.59, wpb=8299.8, bsz=316.3, num_updates=22100, lr=9.51303e-05, gnorm=0.503, clip=0, loss_scale=8, train_wall=72, gb_free=17.1, wall=19876
2023-09-02 15:24:43 | INFO | train_inner | epoch 016:    101 / 1474 loss=2.001, trans_loss=4.983, nll_loss=2.244, w2v_ctc_loss=0.715, task_loss=4.317, task_loss_gen=5.73, contrastive_loss=0, total=4118.73, n_correct=2643.48, ppl=4.74, accuracy=64.182, wps=11450.4, ups=1.39, wpb=8237.5, bsz=314, num_updates=22200, lr=9.49158e-05, gnorm=0.505, clip=0, loss_scale=8, train_wall=71, gb_free=15.9, wall=19948
2023-09-02 15:25:56 | INFO | train_inner | epoch 016:    201 / 1474 loss=2.001, trans_loss=4.977, nll_loss=2.235, w2v_ctc_loss=0.71, task_loss=4.479, task_loss_gen=6.14, contrastive_loss=0, total=4106.45, n_correct=2639.24, ppl=4.71, accuracy=64.271, wps=11276.7, ups=1.37, wpb=8212.9, bsz=297.4, num_updates=22300, lr=9.47027e-05, gnorm=0.511, clip=0, loss_scale=8, train_wall=72, gb_free=16.1, wall=20021
2023-09-02 15:27:08 | INFO | train_inner | epoch 016:    301 / 1474 loss=2.01, trans_loss=4.988, nll_loss=2.25, w2v_ctc_loss=0.727, task_loss=4.62, task_loss_gen=5.881, contrastive_loss=0, total=4169.65, n_correct=2669.51, ppl=4.76, accuracy=64.022, wps=11554.3, ups=1.39, wpb=8339.3, bsz=309.9, num_updates=22400, lr=9.44911e-05, gnorm=0.512, clip=0, loss_scale=8, train_wall=71, gb_free=10.3, wall=20093
2023-09-02 15:28:20 | INFO | train_inner | epoch 016:    401 / 1474 loss=2.015, trans_loss=4.988, nll_loss=2.249, w2v_ctc_loss=0.73, task_loss=4.619, task_loss_gen=6.571, contrastive_loss=0, total=4063.79, n_correct=2601.74, ppl=4.75, accuracy=64.023, wps=11319.9, ups=1.39, wpb=8127.6, bsz=286.5, num_updates=22500, lr=9.42809e-05, gnorm=0.517, clip=0, loss_scale=8, train_wall=71, gb_free=12.2, wall=20165
2023-09-02 15:29:33 | INFO | train_inner | epoch 016:    501 / 1474 loss=2.008, trans_loss=4.99, nll_loss=2.253, w2v_ctc_loss=0.725, task_loss=4.442, task_loss_gen=5.698, contrastive_loss=0, total=4179.53, n_correct=2684.14, ppl=4.77, accuracy=64.221, wps=11432.6, ups=1.37, wpb=8359.1, bsz=319.5, num_updates=22600, lr=9.40721e-05, gnorm=0.507, clip=0, loss_scale=8, train_wall=72, gb_free=17.5, wall=20238
2023-09-02 15:30:45 | INFO | train_inner | epoch 016:    601 / 1474 loss=2.013, trans_loss=4.993, nll_loss=2.257, w2v_ctc_loss=0.725, task_loss=5.701, task_loss_gen=6.476, contrastive_loss=0, total=4121.37, n_correct=2637.76, ppl=4.78, accuracy=64.002, wps=11441.7, ups=1.39, wpb=8242.7, bsz=297.5, num_updates=22700, lr=9.38647e-05, gnorm=0.542, clip=0, loss_scale=8, train_wall=71, gb_free=17.5, wall=20310
2023-09-02 15:31:57 | INFO | train_inner | epoch 016:    701 / 1474 loss=2.009, trans_loss=4.987, nll_loss=2.249, w2v_ctc_loss=0.725, task_loss=9.887, task_loss_gen=8.189, contrastive_loss=0, total=4099.17, n_correct=2623.91, ppl=4.75, accuracy=64.011, wps=11389.8, ups=1.39, wpb=8198.3, bsz=297.5, num_updates=22800, lr=9.36586e-05, gnorm=0.562, clip=0, loss_scale=8, train_wall=71, gb_free=16.5, wall=20382
2023-09-02 15:33:09 | INFO | train_inner | epoch 016:    801 / 1474 loss=2, trans_loss=4.983, nll_loss=2.243, w2v_ctc_loss=0.709, task_loss=7.566, task_loss_gen=6.512, contrastive_loss=0, total=4184.53, n_correct=2686.29, ppl=4.73, accuracy=64.196, wps=11581.3, ups=1.38, wpb=8369.1, bsz=313, num_updates=22900, lr=9.34539e-05, gnorm=0.522, clip=0, loss_scale=8, train_wall=72, gb_free=13, wall=20455
2023-09-02 15:34:22 | INFO | train_inner | epoch 016:    901 / 1474 loss=2.004, trans_loss=4.984, nll_loss=2.246, w2v_ctc_loss=0.716, task_loss=6.844, task_loss_gen=6.254, contrastive_loss=0, total=4151.84, n_correct=2665.25, ppl=4.74, accuracy=64.194, wps=11505.4, ups=1.39, wpb=8303.7, bsz=307, num_updates=23000, lr=9.32505e-05, gnorm=0.513, clip=0, loss_scale=8, train_wall=71, gb_free=16.5, wall=20527
2023-09-02 15:35:34 | INFO | train_inner | epoch 016:   1001 / 1474 loss=2.012, trans_loss=4.988, nll_loss=2.251, w2v_ctc_loss=0.731, task_loss=6.092, task_loss_gen=6.172, contrastive_loss=0, total=4112.79, n_correct=2632.77, ppl=4.76, accuracy=64.014, wps=11288.7, ups=1.37, wpb=8225.6, bsz=299.5, num_updates=23100, lr=9.30484e-05, gnorm=0.514, clip=0, loss_scale=16, train_wall=72, gb_free=14.5, wall=20600
2023-09-02 15:36:48 | INFO | train_inner | epoch 016:   1101 / 1474 loss=2.015, trans_loss=4.992, nll_loss=2.257, w2v_ctc_loss=0.735, task_loss=5.648, task_loss_gen=6.189, contrastive_loss=0, total=4111.6, n_correct=2629.58, ppl=4.78, accuracy=63.955, wps=11216.6, ups=1.36, wpb=8223.2, bsz=295.6, num_updates=23200, lr=9.28477e-05, gnorm=0.508, clip=0, loss_scale=16, train_wall=72, gb_free=14.3, wall=20673
2023-09-02 15:38:01 | INFO | train_inner | epoch 016:   1201 / 1474 loss=2.01, trans_loss=4.992, nll_loss=2.257, w2v_ctc_loss=0.722, task_loss=4.714, task_loss_gen=6.005, contrastive_loss=0, total=4157.51, n_correct=2656.27, ppl=4.78, accuracy=63.891, wps=11377.9, ups=1.37, wpb=8315, bsz=306.6, num_updates=23300, lr=9.26482e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=72, gb_free=14.5, wall=20746
2023-09-02 15:39:14 | INFO | train_inner | epoch 016:   1301 / 1474 loss=2.009, trans_loss=4.983, nll_loss=2.246, w2v_ctc_loss=0.735, task_loss=4.028, task_loss_gen=5.894, contrastive_loss=0, total=4151.03, n_correct=2662.88, ppl=4.74, accuracy=64.15, wps=11381.9, ups=1.37, wpb=8302.1, bsz=314.5, num_updates=23400, lr=9.245e-05, gnorm=0.502, clip=0, loss_scale=16, train_wall=72, gb_free=15.7, wall=20819
2023-09-02 15:40:27 | INFO | train_inner | epoch 016:   1401 / 1474 loss=2.001, trans_loss=4.979, nll_loss=2.24, w2v_ctc_loss=0.723, task_loss=3.843, task_loss_gen=5.93, contrastive_loss=0, total=4201.47, n_correct=2701.06, ppl=4.72, accuracy=64.288, wps=11503.8, ups=1.37, wpb=8402.9, bsz=320.7, num_updates=23500, lr=9.22531e-05, gnorm=0.499, clip=0, loss_scale=16, train_wall=72, gb_free=15.6, wall=20892
2023-09-02 15:41:20 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 15:41:53 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 3.917 | trans_loss 5.218 | nll_loss 2.498 | w2v_ctc_loss 1.298 | task_loss 11.743 | task_loss_gen 17.677 | contrastive_loss 0 | total 4003.4 | n_correct 2624.6 | ppl 5.65 | accuracy 65.559 | uer 18.013 | wer 19.649 | raw_wer 19.649 | bleu 20.89 | wps 1591.1 | wpb 4003.4 | bsz 141.8 | num_updates 23573 | best_bleu 21.05
2023-09-02 15:41:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 23573 updates
2023-09-02 15:41:53 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_20.8904.pt
2023-09-02 15:41:56 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_20.8904.pt
2023-09-02 15:42:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_20.8904.pt (epoch 16 @ 23573 updates, score 20.89) (writing took 8.770385109004565 seconds)
2023-09-02 15:42:03 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-09-02 15:42:03 | INFO | train | epoch 016 | loss 2.008 | trans_loss 4.986 | nll_loss 2.248 | w2v_ctc_loss 0.723 | task_loss 5.389 | task_loss_gen 6.254 | contrastive_loss 0 | total 4138.65 | n_correct 2653.04 | ppl 4.75 | accuracy 64.104 | wps 10900.2 | ups 1.32 | wpb 8277.3 | bsz 305.7 | num_updates 23573 | lr 9.21102e-05 | gnorm 0.516 | clip 0 | loss_scale 16 | train_wall 1058 | gb_free 15.1 | wall 20988
2023-09-02 15:42:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 15:42:03 | INFO | fairseq.trainer | begin training epoch 17
2023-09-02 15:42:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 15:42:30 | INFO | train_inner | epoch 017:     27 / 1474 loss=2, trans_loss=4.97, nll_loss=2.228, w2v_ctc_loss=0.715, task_loss=3.973, task_loss_gen=6.464, contrastive_loss=0, total=4145.04, n_correct=2667.71, ppl=4.68, accuracy=64.359, wps=6717.7, ups=0.81, wpb=8290.1, bsz=302.4, num_updates=23600, lr=9.20575e-05, gnorm=0.506, clip=0, loss_scale=16, train_wall=72, gb_free=15.3, wall=21015
2023-09-02 15:43:43 | INFO | train_inner | epoch 017:    127 / 1474 loss=1.997, trans_loss=4.96, nll_loss=2.214, w2v_ctc_loss=0.718, task_loss=3.552, task_loss_gen=6.85, contrastive_loss=0, total=4117.27, n_correct=2661.11, ppl=4.64, accuracy=64.633, wps=11347.3, ups=1.38, wpb=8234.5, bsz=296.2, num_updates=23700, lr=9.1863e-05, gnorm=0.506, clip=0, loss_scale=16, train_wall=72, gb_free=17.5, wall=21088
2023-09-02 15:44:55 | INFO | train_inner | epoch 017:    227 / 1474 loss=1.987, trans_loss=4.958, nll_loss=2.212, w2v_ctc_loss=0.701, task_loss=3.331, task_loss_gen=6.253, contrastive_loss=0, total=4159.6, n_correct=2687.93, ppl=4.63, accuracy=64.62, wps=11496.4, ups=1.38, wpb=8319.2, bsz=317.6, num_updates=23800, lr=9.16698e-05, gnorm=0.501, clip=0, loss_scale=16, train_wall=72, gb_free=16.5, wall=21160
2023-09-02 15:46:07 | INFO | train_inner | epoch 017:    327 / 1474 loss=1.995, trans_loss=4.965, nll_loss=2.221, w2v_ctc_loss=0.712, task_loss=3.656, task_loss_gen=6.552, contrastive_loss=0, total=4156.91, n_correct=2684.17, ppl=4.66, accuracy=64.571, wps=11570.9, ups=1.39, wpb=8313.8, bsz=305.7, num_updates=23900, lr=9.14779e-05, gnorm=0.509, clip=0, loss_scale=16, train_wall=71, gb_free=17.1, wall=21232
2023-09-02 15:47:19 | INFO | train_inner | epoch 017:    427 / 1474 loss=1.991, trans_loss=4.964, nll_loss=2.22, w2v_ctc_loss=0.707, task_loss=3.784, task_loss_gen=6.609, contrastive_loss=0, total=4146.43, n_correct=2684.24, ppl=4.66, accuracy=64.736, wps=11462.4, ups=1.38, wpb=8292.9, bsz=308, num_updates=24000, lr=9.12871e-05, gnorm=0.51, clip=0, loss_scale=16, train_wall=72, gb_free=15.9, wall=21305
2023-09-02 15:47:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 15:47:53 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 3.946 | trans_loss 5.225 | nll_loss 2.5 | w2v_ctc_loss 1.378 | task_loss 16.426 | task_loss_gen 18.362 | contrastive_loss 0 | total 4003.4 | n_correct 2618.1 | ppl 5.65 | accuracy 65.397 | uer 17.978 | wer 19.611 | raw_wer 19.611 | bleu 20.72 | wps 1592.2 | wpb 4003.4 | bsz 141.8 | num_updates 24000 | best_bleu 21.05
2023-09-02 15:47:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 24000 updates
2023-09-02 15:47:53 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_17_24000.pt
2023-09-02 15:47:55 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_17_24000.pt
2023-09-02 15:48:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_17_24000.pt (epoch 17 @ 24000 updates, score 20.72) (writing took 9.128656724991743 seconds)
2023-09-02 15:49:16 | INFO | train_inner | epoch 017:    527 / 1474 loss=2.002, trans_loss=4.973, nll_loss=2.232, w2v_ctc_loss=0.723, task_loss=6.035, task_loss_gen=7.258, contrastive_loss=0, total=4182.1, n_correct=2691.42, ppl=4.7, accuracy=64.356, wps=7159.9, ups=0.86, wpb=8364.2, bsz=307.9, num_updates=24100, lr=9.10975e-05, gnorm=0.522, clip=0, loss_scale=16, train_wall=73, gb_free=16.5, wall=21421
2023-09-02 15:50:29 | INFO | train_inner | epoch 017:    627 / 1474 loss=1.997, trans_loss=4.971, nll_loss=2.229, w2v_ctc_loss=0.711, task_loss=12.328, task_loss_gen=9.745, contrastive_loss=0, total=4167.27, n_correct=2686.69, ppl=4.69, accuracy=64.471, wps=11468.9, ups=1.38, wpb=8334.5, bsz=302.2, num_updates=24200, lr=9.09091e-05, gnorm=0.533, clip=0, loss_scale=16, train_wall=72, gb_free=10.3, wall=21494
2023-09-02 15:51:42 | INFO | train_inner | epoch 017:    727 / 1474 loss=2.002, trans_loss=4.971, nll_loss=2.229, w2v_ctc_loss=0.728, task_loss=7.743, task_loss_gen=7.037, contrastive_loss=0, total=4166.12, n_correct=2684.03, ppl=4.69, accuracy=64.425, wps=11362.9, ups=1.36, wpb=8332.2, bsz=308.1, num_updates=24300, lr=9.07218e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=73, gb_free=16, wall=21567
2023-09-02 15:52:55 | INFO | train_inner | epoch 017:    827 / 1474 loss=1.999, trans_loss=4.969, nll_loss=2.226, w2v_ctc_loss=0.719, task_loss=11.152, task_loss_gen=9.767, contrastive_loss=0, total=4091.64, n_correct=2641.6, ppl=4.68, accuracy=64.561, wps=11320.7, ups=1.38, wpb=8183.3, bsz=295.3, num_updates=24400, lr=9.05357e-05, gnorm=0.537, clip=0, loss_scale=16, train_wall=72, gb_free=17, wall=21640
2023-09-02 15:54:05 | INFO | train_inner | epoch 017:    927 / 1474 loss=1.991, trans_loss=4.963, nll_loss=2.219, w2v_ctc_loss=0.709, task_loss=7.374, task_loss_gen=7.019, contrastive_loss=0, total=4106.83, n_correct=2655.26, ppl=4.66, accuracy=64.655, wps=11593.9, ups=1.41, wpb=8213.7, bsz=304.6, num_updates=24500, lr=9.03508e-05, gnorm=0.522, clip=0, loss_scale=16, train_wall=70, gb_free=15.5, wall=21711
2023-09-02 15:55:18 | INFO | train_inner | epoch 017:   1027 / 1474 loss=1.993, trans_loss=4.964, nll_loss=2.221, w2v_ctc_loss=0.714, task_loss=6.704, task_loss_gen=6.233, contrastive_loss=0, total=4115.49, n_correct=2658.55, ppl=4.66, accuracy=64.599, wps=11404, ups=1.39, wpb=8231, bsz=305.8, num_updates=24600, lr=9.0167e-05, gnorm=0.518, clip=0, loss_scale=16, train_wall=71, gb_free=16.3, wall=21783
2023-09-02 15:56:30 | INFO | train_inner | epoch 017:   1127 / 1474 loss=1.992, trans_loss=4.96, nll_loss=2.215, w2v_ctc_loss=0.708, task_loss=6.722, task_loss_gen=6.332, contrastive_loss=0, total=4078.39, n_correct=2639.88, ppl=4.64, accuracy=64.728, wps=11332.7, ups=1.39, wpb=8156.8, bsz=293.7, num_updates=24700, lr=8.99843e-05, gnorm=0.508, clip=0, loss_scale=16, train_wall=71, gb_free=15.1, wall=21855
2023-09-02 15:57:43 | INFO | train_inner | epoch 017:   1227 / 1474 loss=1.992, trans_loss=4.968, nll_loss=2.227, w2v_ctc_loss=0.707, task_loss=5.497, task_loss_gen=5.774, contrastive_loss=0, total=4173.49, n_correct=2687.72, ppl=4.68, accuracy=64.4, wps=11299.9, ups=1.35, wpb=8347, bsz=323.7, num_updates=24800, lr=8.98027e-05, gnorm=0.499, clip=0, loss_scale=16, train_wall=73, gb_free=15.7, wall=21929
2023-09-02 15:58:56 | INFO | train_inner | epoch 017:   1327 / 1474 loss=1.988, trans_loss=4.961, nll_loss=2.218, w2v_ctc_loss=0.701, task_loss=4.986, task_loss_gen=5.938, contrastive_loss=0, total=4156.28, n_correct=2688.9, ppl=4.65, accuracy=64.695, wps=11400.6, ups=1.37, wpb=8312.6, bsz=308, num_updates=24900, lr=8.96221e-05, gnorm=0.501, clip=0, loss_scale=16, train_wall=72, gb_free=17.5, wall=22002
2023-09-02 16:00:09 | INFO | train_inner | epoch 017:   1427 / 1474 loss=1.989, trans_loss=4.962, nll_loss=2.218, w2v_ctc_loss=0.706, task_loss=5.247, task_loss_gen=6, contrastive_loss=0, total=4112.95, n_correct=2664.04, ppl=4.65, accuracy=64.772, wps=11331.1, ups=1.38, wpb=8225.9, bsz=303.2, num_updates=25000, lr=8.94427e-05, gnorm=0.506, clip=0, loss_scale=16, train_wall=72, gb_free=16.5, wall=22074
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:0')
2023-09-02 16:00:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:6')
2023-09-02 16:01:17 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 3.909 | trans_loss 5.208 | nll_loss 2.479 | w2v_ctc_loss 1.294 | task_loss 16.148 | task_loss_gen 16.829 | contrastive_loss 0 | total 4003.4 | n_correct 2636.7 | ppl 5.57 | accuracy 65.862 | uer 17.633 | wer 19.358 | raw_wer 19.358 | bleu 21.25 | wps 1575.6 | wpb 4003.4 | bsz 141.8 | num_updates 25047 | best_bleu 21.25
2023-09-02 16:01:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 25047 updates
2023-09-02 16:01:17 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 16:01:24 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 16:01:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 17 @ 25047 updates, score 21.25) (writing took 14.239823173033074 seconds)
2023-09-02 16:01:31 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-09-02 16:01:31 | INFO | train | epoch 017 | loss 1.994 | trans_loss 4.965 | nll_loss 2.221 | w2v_ctc_loss 0.712 | task_loss 6.224 | task_loss_gen 6.902 | contrastive_loss 0 | total 4138.65 | n_correct 2673.31 | ppl 4.66 | accuracy 64.594 | wps 10441.9 | ups 1.26 | wpb 8277.3 | bsz 305.7 | num_updates 25047 | lr 8.93588e-05 | gnorm 0.514 | clip 0 | loss_scale 16 | train_wall 1058 | gb_free 16.1 | wall 22156
2023-09-02 16:01:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 16:01:31 | INFO | fairseq.trainer | begin training epoch 18
2023-09-02 16:01:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 16:02:18 | INFO | train_inner | epoch 018:     53 / 1474 loss=1.993, trans_loss=4.958, nll_loss=2.213, w2v_ctc_loss=0.72, task_loss=5.96, task_loss_gen=6.04, contrastive_loss=0, total=4139.04, n_correct=2681.21, ppl=4.64, accuracy=64.779, wps=6417.7, ups=0.78, wpb=8278.1, bsz=303.3, num_updates=25100, lr=8.92644e-05, gnorm=0.511, clip=0, loss_scale=16, train_wall=72, gb_free=16.8, wall=22203
2023-09-02 16:02:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-02 16:03:31 | INFO | train_inner | epoch 018:    154 / 1474 loss=1.971, trans_loss=4.932, nll_loss=2.18, w2v_ctc_loss=0.687, task_loss=5.569, task_loss_gen=5.81, contrastive_loss=0, total=4151.9, n_correct=2709.21, ppl=4.53, accuracy=65.252, wps=11337.6, ups=1.37, wpb=8303.8, bsz=313.6, num_updates=25200, lr=8.90871e-05, gnorm=0.499, clip=0, loss_scale=16, train_wall=72, gb_free=16.5, wall=22276
2023-09-02 16:04:44 | INFO | train_inner | epoch 018:    254 / 1474 loss=1.976, trans_loss=4.938, nll_loss=2.186, w2v_ctc_loss=0.699, task_loss=5.449, task_loss_gen=6.037, contrastive_loss=0, total=4164.11, n_correct=2717.36, ppl=4.55, accuracy=65.257, wps=11454.4, ups=1.38, wpb=8328.2, bsz=312.5, num_updates=25300, lr=8.89108e-05, gnorm=0.509, clip=0, loss_scale=16, train_wall=72, gb_free=14.5, wall=22349
2023-09-02 16:05:57 | INFO | train_inner | epoch 018:    354 / 1474 loss=1.979, trans_loss=4.943, nll_loss=2.192, w2v_ctc_loss=0.697, task_loss=5.855, task_loss_gen=6.226, contrastive_loss=0, total=4163.13, n_correct=2709.13, ppl=4.57, accuracy=65.074, wps=11395.3, ups=1.37, wpb=8326.3, bsz=301.5, num_updates=25400, lr=8.87357e-05, gnorm=0.509, clip=0, loss_scale=16, train_wall=72, gb_free=17.3, wall=22422
2023-09-02 16:07:10 | INFO | train_inner | epoch 018:    454 / 1474 loss=1.987, trans_loss=4.949, nll_loss=2.2, w2v_ctc_loss=0.709, task_loss=5.618, task_loss_gen=6.324, contrastive_loss=0, total=4087.83, n_correct=2654.09, ppl=4.59, accuracy=64.927, wps=11136.6, ups=1.36, wpb=8175.7, bsz=295.3, num_updates=25500, lr=8.85615e-05, gnorm=0.511, clip=0, loss_scale=16, train_wall=73, gb_free=16, wall=22496
2023-09-02 16:08:23 | INFO | train_inner | epoch 018:    554 / 1474 loss=1.967, trans_loss=4.93, nll_loss=2.178, w2v_ctc_loss=0.688, task_loss=4.452, task_loss_gen=5.428, contrastive_loss=0, total=4204.41, n_correct=2747.93, ppl=4.52, accuracy=65.358, wps=11619.3, ups=1.38, wpb=8408.8, bsz=328, num_updates=25600, lr=8.83883e-05, gnorm=0.498, clip=0, loss_scale=16, train_wall=72, gb_free=17.1, wall=22568
2023-09-02 16:09:35 | INFO | train_inner | epoch 018:    654 / 1474 loss=1.986, trans_loss=4.95, nll_loss=2.203, w2v_ctc_loss=0.705, task_loss=5.239, task_loss_gen=6.162, contrastive_loss=0, total=4096.81, n_correct=2658.63, ppl=4.6, accuracy=64.895, wps=11307, ups=1.38, wpb=8193.6, bsz=298.9, num_updates=25700, lr=8.82162e-05, gnorm=0.502, clip=0, loss_scale=16, train_wall=72, gb_free=16.3, wall=22640
2023-09-02 16:10:49 | INFO | train_inner | epoch 018:    754 / 1474 loss=1.984, trans_loss=4.951, nll_loss=2.205, w2v_ctc_loss=0.708, task_loss=4.847, task_loss_gen=5.768, contrastive_loss=0, total=4208.29, n_correct=2730.25, ppl=4.61, accuracy=64.878, wps=11495.6, ups=1.37, wpb=8416.6, bsz=322.8, num_updates=25800, lr=8.80451e-05, gnorm=0.506, clip=0, loss_scale=16, train_wall=73, gb_free=17.5, wall=22714
2023-09-02 16:12:01 | INFO | train_inner | epoch 018:    854 / 1474 loss=1.981, trans_loss=4.945, nll_loss=2.195, w2v_ctc_loss=0.702, task_loss=4.676, task_loss_gen=6.186, contrastive_loss=0, total=4166.81, n_correct=2713.96, ppl=4.58, accuracy=65.133, wps=11439.9, ups=1.37, wpb=8333.6, bsz=301.9, num_updates=25900, lr=8.7875e-05, gnorm=0.508, clip=0, loss_scale=16, train_wall=72, gb_free=12.2, wall=22787
2023-09-02 16:13:13 | INFO | train_inner | epoch 018:    954 / 1474 loss=1.973, trans_loss=4.938, nll_loss=2.188, w2v_ctc_loss=0.695, task_loss=3.526, task_loss_gen=5.828, contrastive_loss=0, total=4142.65, n_correct=2705.14, ppl=4.56, accuracy=65.3, wps=11636.3, ups=1.4, wpb=8285.3, bsz=316.1, num_updates=26000, lr=8.77058e-05, gnorm=0.504, clip=0, loss_scale=16, train_wall=70, gb_free=14.5, wall=22858
2023-09-02 16:13:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 16:13:45 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 3.912 | trans_loss 5.198 | nll_loss 2.469 | w2v_ctc_loss 1.328 | task_loss 8.719 | task_loss_gen 18.836 | contrastive_loss 0 | total 4003.4 | n_correct 2636.9 | ppl 5.54 | accuracy 65.867 | uer 17.931 | wer 19.746 | raw_wer 19.746 | bleu 21.59 | wps 1668.7 | wpb 4003.4 | bsz 141.8 | num_updates 26000 | best_bleu 21.59
2023-09-02 16:13:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26000 updates
2023-09-02 16:13:45 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_18_26000.pt
2023-09-02 16:13:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_18_26000.pt
2023-09-02 16:13:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_18_26000.pt (epoch 18 @ 26000 updates, score 21.59) (writing took 13.528316655952949 seconds)
2023-09-02 16:15:12 | INFO | train_inner | epoch 018:   1054 / 1474 loss=1.981, trans_loss=4.944, nll_loss=2.196, w2v_ctc_loss=0.699, task_loss=3.601, task_loss_gen=6.822, contrastive_loss=0, total=4137.77, n_correct=2694.98, ppl=4.58, accuracy=65.131, wps=6902.6, ups=0.83, wpb=8275.5, bsz=300.5, num_updates=26100, lr=8.75376e-05, gnorm=0.519, clip=0, loss_scale=16, train_wall=72, gb_free=17, wall=22978
2023-09-02 16:16:25 | INFO | train_inner | epoch 018:   1154 / 1474 loss=1.983, trans_loss=4.942, nll_loss=2.193, w2v_ctc_loss=0.711, task_loss=3.218, task_loss_gen=6.345, contrastive_loss=0, total=4153.69, n_correct=2700.93, ppl=4.57, accuracy=65.025, wps=11487, ups=1.38, wpb=8307.4, bsz=314.9, num_updates=26200, lr=8.73704e-05, gnorm=0.512, clip=0, loss_scale=16, train_wall=72, gb_free=14.8, wall=23050
2023-09-02 16:17:38 | INFO | train_inner | epoch 018:   1254 / 1474 loss=1.988, trans_loss=4.956, nll_loss=2.21, w2v_ctc_loss=0.702, task_loss=4.286, task_loss_gen=7.119, contrastive_loss=0, total=4087.62, n_correct=2649.85, ppl=4.63, accuracy=64.826, wps=11211.9, ups=1.37, wpb=8175.2, bsz=287.1, num_updates=26300, lr=8.72041e-05, gnorm=0.513, clip=0, loss_scale=16, train_wall=72, gb_free=16.3, wall=23123
2023-09-02 16:18:50 | INFO | train_inner | epoch 018:   1354 / 1474 loss=1.999, trans_loss=4.961, nll_loss=2.217, w2v_ctc_loss=0.728, task_loss=4.52, task_loss_gen=6.915, contrastive_loss=0, total=4070.69, n_correct=2633.91, ppl=4.65, accuracy=64.704, wps=11191.3, ups=1.37, wpb=8141.4, bsz=291.7, num_updates=26400, lr=8.70388e-05, gnorm=0.517, clip=0, loss_scale=16, train_wall=72, gb_free=17.1, wall=23196
2023-09-02 16:20:03 | INFO | train_inner | epoch 018:   1454 / 1474 loss=1.989, trans_loss=4.953, nll_loss=2.206, w2v_ctc_loss=0.712, task_loss=4.295, task_loss_gen=6.731, contrastive_loss=0, total=4113.2, n_correct=2668.93, ppl=4.61, accuracy=64.887, wps=11336.9, ups=1.38, wpb=8226.4, bsz=297.5, num_updates=26500, lr=8.68744e-05, gnorm=0.52, clip=0, loss_scale=16, train_wall=72, gb_free=16.1, wall=23268
2023-09-02 16:20:18 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 16:20:50 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 3.928 | trans_loss 5.208 | nll_loss 2.485 | w2v_ctc_loss 1.355 | task_loss 15.347 | task_loss_gen 18.258 | contrastive_loss 0 | total 4003.4 | n_correct 2631.7 | ppl 5.6 | accuracy 65.737 | uer 18.087 | wer 19.846 | raw_wer 19.846 | bleu 21.45 | wps 1636.2 | wpb 4003.4 | bsz 141.8 | num_updates 26520 | best_bleu 21.59
2023-09-02 16:20:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26520 updates
2023-09-02 16:20:50 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.4508.pt
2023-09-02 16:20:54 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.4508.pt
2023-09-02 16:20:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.4508.pt (epoch 18 @ 26520 updates, score 21.45) (writing took 8.182704514008947 seconds)
2023-09-02 16:20:59 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-09-02 16:20:59 | INFO | train | epoch 018 | loss 1.982 | trans_loss 4.945 | nll_loss 2.197 | w2v_ctc_loss 0.703 | task_loss 4.682 | task_loss_gen 6.243 | contrastive_loss 0 | total 4138.43 | n_correct 2691.62 | ppl 4.58 | accuracy 65.04 | wps 10438.3 | ups 1.26 | wpb 8276.9 | bsz 305.7 | num_updates 26520 | lr 8.68417e-05 | gnorm 0.509 | clip 0 | loss_scale 16 | train_wall 1061 | gb_free 15.5 | wall 23324
2023-09-02 16:20:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 16:20:59 | INFO | fairseq.trainer | begin training epoch 19
2023-09-02 16:20:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 16:22:04 | INFO | train_inner | epoch 019:     80 / 1474 loss=1.974, trans_loss=4.931, nll_loss=2.178, w2v_ctc_loss=0.692, task_loss=4.669, task_loss_gen=6.499, contrastive_loss=0, total=4102.06, n_correct=2679.22, ppl=4.53, accuracy=65.314, wps=6776.2, ups=0.83, wpb=8204.1, bsz=296.9, num_updates=26600, lr=8.6711e-05, gnorm=0.518, clip=0, loss_scale=16, train_wall=71, gb_free=17.1, wall=23389
2023-09-02 16:23:17 | INFO | train_inner | epoch 019:    180 / 1474 loss=1.973, trans_loss=4.932, nll_loss=2.179, w2v_ctc_loss=0.7, task_loss=4.523, task_loss_gen=5.955, contrastive_loss=0, total=4227.7, n_correct=2763.02, ppl=4.53, accuracy=65.355, wps=11613, ups=1.37, wpb=8455.4, bsz=324.8, num_updates=26700, lr=8.65485e-05, gnorm=0.508, clip=0, loss_scale=16, train_wall=72, gb_free=16.7, wall=23462
2023-09-02 16:24:30 | INFO | train_inner | epoch 019:    280 / 1474 loss=1.972, trans_loss=4.925, nll_loss=2.169, w2v_ctc_loss=0.696, task_loss=11.123, task_loss_gen=9.065, contrastive_loss=0, total=4187.34, n_correct=2738.21, ppl=4.5, accuracy=65.393, wps=11500.1, ups=1.37, wpb=8374.7, bsz=306.4, num_updates=26800, lr=8.63868e-05, gnorm=0.549, clip=0, loss_scale=16, train_wall=72, gb_free=15.5, wall=23535
2023-09-02 16:25:43 | INFO | train_inner | epoch 019:    380 / 1474 loss=1.969, trans_loss=4.926, nll_loss=2.171, w2v_ctc_loss=0.689, task_loss=13.687, task_loss_gen=10.045, contrastive_loss=0, total=4170.52, n_correct=2726.44, ppl=4.5, accuracy=65.374, wps=11453.9, ups=1.37, wpb=8341, bsz=311, num_updates=26900, lr=8.62261e-05, gnorm=0.524, clip=0, loss_scale=16, train_wall=72, gb_free=16, wall=23608
2023-09-02 16:26:54 | INFO | train_inner | epoch 019:    480 / 1474 loss=1.969, trans_loss=4.927, nll_loss=2.172, w2v_ctc_loss=0.691, task_loss=12.141, task_loss_gen=9.082, contrastive_loss=0, total=4113.89, n_correct=2692.95, ppl=4.51, accuracy=65.46, wps=11485, ups=1.4, wpb=8227.8, bsz=301.5, num_updates=27000, lr=8.60663e-05, gnorm=0.514, clip=0, loss_scale=16, train_wall=71, gb_free=16.9, wall=23679
2023-09-02 16:28:06 | INFO | train_inner | epoch 019:    580 / 1474 loss=1.968, trans_loss=4.925, nll_loss=2.17, w2v_ctc_loss=0.689, task_loss=8.444, task_loss_gen=7.089, contrastive_loss=0, total=4128.58, n_correct=2701.62, ppl=4.5, accuracy=65.437, wps=11508.4, ups=1.39, wpb=8257.2, bsz=306.2, num_updates=27100, lr=8.59074e-05, gnorm=0.511, clip=0, loss_scale=16, train_wall=71, gb_free=16.3, wall=23751
2023-09-02 16:29:18 | INFO | train_inner | epoch 019:    680 / 1474 loss=1.962, trans_loss=4.929, nll_loss=2.175, w2v_ctc_loss=0.679, task_loss=5.488, task_loss_gen=5.55, contrastive_loss=0, total=4201.56, n_correct=2751.18, ppl=4.52, accuracy=65.48, wps=11662.5, ups=1.39, wpb=8403.1, bsz=321.5, num_updates=27200, lr=8.57493e-05, gnorm=0.498, clip=0, loss_scale=32, train_wall=71, gb_free=17.5, wall=23823
2023-09-02 16:30:31 | INFO | train_inner | epoch 019:    780 / 1474 loss=1.972, trans_loss=4.925, nll_loss=2.17, w2v_ctc_loss=0.701, task_loss=4.508, task_loss_gen=6.307, contrastive_loss=0, total=4124.03, n_correct=2699.77, ppl=4.5, accuracy=65.464, wps=11337.5, ups=1.37, wpb=8248.1, bsz=299, num_updates=27300, lr=8.55921e-05, gnorm=0.505, clip=0, loss_scale=32, train_wall=72, gb_free=17.1, wall=23896
2023-09-02 16:31:44 | INFO | train_inner | epoch 019:    880 / 1474 loss=1.973, trans_loss=4.931, nll_loss=2.178, w2v_ctc_loss=0.697, task_loss=3.626, task_loss_gen=6.342, contrastive_loss=0, total=4177.8, n_correct=2729.94, ppl=4.53, accuracy=65.344, wps=11402.8, ups=1.36, wpb=8355.6, bsz=309.6, num_updates=27400, lr=8.54358e-05, gnorm=0.501, clip=0, loss_scale=32, train_wall=73, gb_free=14.5, wall=23969
2023-09-02 16:32:58 | INFO | train_inner | epoch 019:    980 / 1474 loss=1.97, trans_loss=4.934, nll_loss=2.182, w2v_ctc_loss=0.688, task_loss=3.072, task_loss_gen=6.968, contrastive_loss=0, total=4084.26, n_correct=2669.59, ppl=4.54, accuracy=65.363, wps=11108.6, ups=1.36, wpb=8168.5, bsz=305.8, num_updates=27500, lr=8.52803e-05, gnorm=0.503, clip=0, loss_scale=32, train_wall=73, gb_free=15.8, wall=24043
2023-09-02 16:34:10 | INFO | train_inner | epoch 019:   1080 / 1474 loss=1.972, trans_loss=4.935, nll_loss=2.184, w2v_ctc_loss=0.69, task_loss=3.592, task_loss_gen=7.119, contrastive_loss=0, total=4042.73, n_correct=2639.73, ppl=4.54, accuracy=65.296, wps=11236.6, ups=1.39, wpb=8085.5, bsz=294, num_updates=27600, lr=8.51257e-05, gnorm=0.51, clip=0, loss_scale=32, train_wall=71, gb_free=17, wall=24115
2023-09-02 16:35:23 | INFO | train_inner | epoch 019:   1180 / 1474 loss=1.971, trans_loss=4.932, nll_loss=2.18, w2v_ctc_loss=0.689, task_loss=3.34, task_loss_gen=7.197, contrastive_loss=0, total=4140.95, n_correct=2703.78, ppl=4.53, accuracy=65.294, wps=11260.2, ups=1.36, wpb=8281.9, bsz=307.9, num_updates=27700, lr=8.49719e-05, gnorm=0.507, clip=0, loss_scale=32, train_wall=73, gb_free=12.3, wall=24188
2023-09-02 16:36:35 | INFO | train_inner | epoch 019:   1280 / 1474 loss=1.971, trans_loss=4.934, nll_loss=2.183, w2v_ctc_loss=0.688, task_loss=3.164, task_loss_gen=7.423, contrastive_loss=0, total=4135.79, n_correct=2701.91, ppl=4.54, accuracy=65.33, wps=11538.9, ups=1.4, wpb=8271.6, bsz=299.5, num_updates=27800, lr=8.48189e-05, gnorm=0.507, clip=0, loss_scale=32, train_wall=71, gb_free=17.6, wall=24260
2023-09-02 16:37:48 | INFO | train_inner | epoch 019:   1380 / 1474 loss=1.973, trans_loss=4.931, nll_loss=2.179, w2v_ctc_loss=0.696, task_loss=2.61, task_loss_gen=8.071, contrastive_loss=0, total=4138.67, n_correct=2703.31, ppl=4.53, accuracy=65.318, wps=11325.7, ups=1.37, wpb=8277.3, bsz=301.6, num_updates=27900, lr=8.46668e-05, gnorm=0.509, clip=0, loss_scale=32, train_wall=72, gb_free=16.2, wall=24333
2023-09-02 16:38:56 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 16:39:31 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 3.904 | trans_loss 5.196 | nll_loss 2.467 | w2v_ctc_loss 1.304 | task_loss 9.414 | task_loss_gen 21.234 | contrastive_loss 0 | total 4003.4 | n_correct 2643.9 | ppl 5.53 | accuracy 66.041 | uer 17.641 | wer 19.444 | raw_wer 19.444 | bleu 21.08 | wps 1520.4 | wpb 4003.4 | bsz 141.8 | num_updates 27994 | best_bleu 21.59
2023-09-02 16:39:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 27994 updates
2023-09-02 16:39:31 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.0800.pt
2023-09-02 16:39:33 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.0800.pt
2023-09-02 16:39:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.0800.pt (epoch 19 @ 27994 updates, score 21.08) (writing took 7.934059067978524 seconds)
2023-09-02 16:39:39 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-09-02 16:39:39 | INFO | train | epoch 019 | loss 1.971 | trans_loss 4.929 | nll_loss 2.176 | w2v_ctc_loss 0.692 | task_loss 5.835 | task_loss_gen 7.409 | contrastive_loss 0 | total 4138.65 | n_correct 2705.87 | ppl 4.52 | accuracy 65.38 | wps 10896.5 | ups 1.32 | wpb 8277.3 | bsz 305.7 | num_updates 27994 | lr 8.45245e-05 | gnorm 0.512 | clip 0 | loss_scale 32 | train_wall 1059 | gb_free 17 | wall 24444
2023-09-02 16:39:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 16:39:39 | INFO | fairseq.trainer | begin training epoch 20
2023-09-02 16:39:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 16:39:51 | INFO | train_inner | epoch 020:      6 / 1474 loss=1.97, trans_loss=4.925, nll_loss=2.172, w2v_ctc_loss=0.693, task_loss=2.632, task_loss_gen=8.271, contrastive_loss=0, total=4117.61, n_correct=2692.84, ppl=4.51, accuracy=65.398, wps=6677, ups=0.81, wpb=8235.2, bsz=303, num_updates=28000, lr=8.45154e-05, gnorm=0.51, clip=0, loss_scale=32, train_wall=72, gb_free=16.1, wall=24456
2023-09-02 16:39:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 16:40:26 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 3.901 | trans_loss 5.196 | nll_loss 2.463 | w2v_ctc_loss 1.294 | task_loss 8.092 | task_loss_gen 22.248 | contrastive_loss 0 | total 4003.4 | n_correct 2642.6 | ppl 5.51 | accuracy 66.009 | uer 17.57 | wer 19.41 | raw_wer 19.41 | bleu 21.14 | wps 1532.6 | wpb 4003.4 | bsz 141.8 | num_updates 28000 | best_bleu 21.59
2023-09-02 16:40:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 28000 updates
2023-09-02 16:40:26 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_20_28000.pt
2023-09-02 16:40:28 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_20_28000.pt
2023-09-02 16:40:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_20_28000.pt (epoch 20 @ 28000 updates, score 21.14) (writing took 8.445639707962982 seconds)
2023-09-02 16:41:47 | INFO | train_inner | epoch 020:    106 / 1474 loss=1.952, trans_loss=4.904, nll_loss=2.143, w2v_ctc_loss=0.672, task_loss=2.951, task_loss_gen=7.601, contrastive_loss=0, total=4192.82, n_correct=2764.14, ppl=4.42, accuracy=65.926, wps=7257.7, ups=0.87, wpb=8385.6, bsz=312.8, num_updates=28100, lr=8.43649e-05, gnorm=0.505, clip=0, loss_scale=32, train_wall=72, gb_free=15.9, wall=24572
2023-09-02 16:43:00 | INFO | train_inner | epoch 020:    206 / 1474 loss=1.958, trans_loss=4.91, nll_loss=2.151, w2v_ctc_loss=0.678, task_loss=3.506, task_loss_gen=7.836, contrastive_loss=0, total=4155.9, n_correct=2732.8, ppl=4.44, accuracy=65.757, wps=11401.8, ups=1.37, wpb=8311.8, bsz=302.3, num_updates=28200, lr=8.42152e-05, gnorm=0.508, clip=0, loss_scale=32, train_wall=72, gb_free=11.3, wall=24645
2023-09-02 16:44:12 | INFO | train_inner | epoch 020:    306 / 1474 loss=1.948, trans_loss=4.902, nll_loss=2.141, w2v_ctc_loss=0.674, task_loss=2.317, task_loss_gen=7.607, contrastive_loss=0, total=4192.69, n_correct=2769.81, ppl=4.41, accuracy=66.063, wps=11595.3, ups=1.38, wpb=8385.4, bsz=327.6, num_updates=28300, lr=8.40663e-05, gnorm=0.494, clip=0, loss_scale=32, train_wall=72, gb_free=16.7, wall=24717
2023-09-02 16:45:25 | INFO | train_inner | epoch 020:    406 / 1474 loss=1.956, trans_loss=4.904, nll_loss=2.143, w2v_ctc_loss=0.678, task_loss=2.703, task_loss_gen=8.485, contrastive_loss=0, total=4116.96, n_correct=2717.53, ppl=4.42, accuracy=66.008, wps=11320.7, ups=1.37, wpb=8233.9, bsz=296.8, num_updates=28400, lr=8.39181e-05, gnorm=0.511, clip=0, loss_scale=32, train_wall=72, gb_free=12.3, wall=24790
2023-09-02 16:46:37 | INFO | train_inner | epoch 020:    506 / 1474 loss=1.962, trans_loss=4.921, nll_loss=2.165, w2v_ctc_loss=0.677, task_loss=3.266, task_loss_gen=8.525, contrastive_loss=0, total=4100.73, n_correct=2689.72, ppl=4.48, accuracy=65.591, wps=11315.6, ups=1.38, wpb=8201.5, bsz=298.4, num_updates=28500, lr=8.37708e-05, gnorm=0.515, clip=0, loss_scale=32, train_wall=72, gb_free=16, wall=24862
2023-09-02 16:47:49 | INFO | train_inner | epoch 020:    606 / 1474 loss=1.965, trans_loss=4.919, nll_loss=2.162, w2v_ctc_loss=0.684, task_loss=5.202, task_loss_gen=7.931, contrastive_loss=0, total=4101.99, n_correct=2690.18, ppl=4.47, accuracy=65.582, wps=11430, ups=1.39, wpb=8204, bsz=298.3, num_updates=28600, lr=8.36242e-05, gnorm=0.515, clip=0, loss_scale=32, train_wall=71, gb_free=13.1, wall=24934
2023-09-02 16:49:01 | INFO | train_inner | epoch 020:    706 / 1474 loss=1.967, trans_loss=4.921, nll_loss=2.165, w2v_ctc_loss=0.691, task_loss=17.531, task_loss_gen=14.396, contrastive_loss=0, total=4124.25, n_correct=2701.54, ppl=4.48, accuracy=65.504, wps=11482.3, ups=1.39, wpb=8248.5, bsz=297.2, num_updates=28700, lr=8.34784e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=71, gb_free=16.4, wall=25006
2023-09-02 16:50:13 | INFO | train_inner | epoch 020:    806 / 1474 loss=1.964, trans_loss=4.919, nll_loss=2.162, w2v_ctc_loss=0.691, task_loss=13.147, task_loss_gen=10.673, contrastive_loss=0, total=4153.23, n_correct=2727.18, ppl=4.48, accuracy=65.664, wps=11448.1, ups=1.38, wpb=8306.5, bsz=308.5, num_updates=28800, lr=8.33333e-05, gnorm=0.517, clip=0, loss_scale=32, train_wall=72, gb_free=17.4, wall=25079
2023-09-02 16:51:27 | INFO | train_inner | epoch 020:    906 / 1474 loss=1.964, trans_loss=4.922, nll_loss=2.168, w2v_ctc_loss=0.685, task_loss=16.852, task_loss_gen=12.325, contrastive_loss=0, total=4153.72, n_correct=2719.47, ppl=4.49, accuracy=65.471, wps=11299.4, ups=1.36, wpb=8307.4, bsz=320.7, num_updates=28900, lr=8.3189e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=73, gb_free=15.8, wall=25152
2023-09-02 16:52:40 | INFO | train_inner | epoch 020:   1006 / 1474 loss=1.958, trans_loss=4.915, nll_loss=2.157, w2v_ctc_loss=0.678, task_loss=15.324, task_loss_gen=11.918, contrastive_loss=0, total=4156.05, n_correct=2733, ppl=4.46, accuracy=65.76, wps=11368.8, ups=1.37, wpb=8312.1, bsz=305.3, num_updates=29000, lr=8.30455e-05, gnorm=0.524, clip=0, loss_scale=32, train_wall=72, gb_free=14.6, wall=25225
2023-09-02 16:53:52 | INFO | train_inner | epoch 020:   1106 / 1474 loss=1.96, trans_loss=4.915, nll_loss=2.159, w2v_ctc_loss=0.683, task_loss=6.633, task_loss_gen=6.39, contrastive_loss=0, total=4181.53, n_correct=2747.79, ppl=4.47, accuracy=65.713, wps=11565, ups=1.38, wpb=8363.1, bsz=320.2, num_updates=29100, lr=8.29027e-05, gnorm=0.511, clip=0, loss_scale=32, train_wall=72, gb_free=16.8, wall=25298
2023-09-02 16:55:05 | INFO | train_inner | epoch 020:   1206 / 1474 loss=1.967, trans_loss=4.909, nll_loss=2.149, w2v_ctc_loss=0.698, task_loss=7.736, task_loss_gen=7.289, contrastive_loss=0, total=4029.26, n_correct=2647.69, ppl=4.44, accuracy=65.712, wps=11119.7, ups=1.38, wpb=8058.5, bsz=282.4, num_updates=29200, lr=8.27606e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=72, gb_free=16.3, wall=25370
2023-09-02 16:56:18 | INFO | train_inner | epoch 020:   1306 / 1474 loss=1.961, trans_loss=4.915, nll_loss=2.159, w2v_ctc_loss=0.684, task_loss=5.947, task_loss_gen=6.639, contrastive_loss=0, total=4127.21, n_correct=2715, ppl=4.46, accuracy=65.783, wps=11296.9, ups=1.37, wpb=8254.4, bsz=299.9, num_updates=29300, lr=8.26192e-05, gnorm=0.51, clip=0, loss_scale=64, train_wall=72, gb_free=14.3, wall=25443
2023-09-02 16:56:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-02 16:57:32 | INFO | train_inner | epoch 020:   1407 / 1474 loss=1.966, trans_loss=4.917, nll_loss=2.16, w2v_ctc_loss=0.691, task_loss=4.424, task_loss_gen=6.974, contrastive_loss=0, total=4110.87, n_correct=2698.94, ppl=4.47, accuracy=65.654, wps=11123.4, ups=1.35, wpb=8221.7, bsz=292.2, num_updates=29400, lr=8.24786e-05, gnorm=0.512, clip=0, loss_scale=32, train_wall=73, gb_free=14, wall=25517
2023-09-02 16:58:20 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 16:58:53 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 3.891 | trans_loss 5.182 | nll_loss 2.446 | w2v_ctc_loss 1.292 | task_loss 10.586 | task_loss_gen 20.201 | contrastive_loss 0 | total 4003.4 | n_correct 2647 | ppl 5.45 | accuracy 66.119 | uer 17.514 | wer 19.123 | raw_wer 19.123 | bleu 21.35 | wps 1652.4 | wpb 4003.4 | bsz 141.8 | num_updates 29467 | best_bleu 21.59
2023-09-02 16:58:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 29467 updates
2023-09-02 16:58:53 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.3501.pt
2023-09-02 16:58:55 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.3501.pt
2023-09-02 16:59:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.3501.pt (epoch 20 @ 29467 updates, score 21.35) (writing took 7.761226521979552 seconds)
2023-09-02 16:59:01 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-09-02 16:59:01 | INFO | train | epoch 020 | loss 1.96 | trans_loss 4.914 | nll_loss 2.156 | w2v_ctc_loss 0.683 | task_loss 7.464 | task_loss_gen 8.754 | contrastive_loss 0 | total 4138.42 | n_correct 2720.19 | ppl 4.46 | accuracy 65.73 | wps 10494 | ups 1.27 | wpb 8276.8 | bsz 305.6 | num_updates 29467 | lr 8.23848e-05 | gnorm 0.513 | clip 0 | loss_scale 32 | train_wall 1059 | gb_free 15.9 | wall 25606
2023-09-02 16:59:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 16:59:01 | INFO | fairseq.trainer | begin training epoch 21
2023-09-02 16:59:01 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 16:59:32 | INFO | train_inner | epoch 021:     33 / 1474 loss=1.958, trans_loss=4.912, nll_loss=2.155, w2v_ctc_loss=0.683, task_loss=3.756, task_loss_gen=6.291, contrastive_loss=0, total=4155.01, n_correct=2732.72, ppl=4.45, accuracy=65.769, wps=6913.6, ups=0.83, wpb=8310, bsz=317.6, num_updates=29500, lr=8.23387e-05, gnorm=0.502, clip=0, loss_scale=32, train_wall=71, gb_free=16, wall=25637
2023-09-02 17:00:44 | INFO | train_inner | epoch 021:    133 / 1474 loss=1.944, trans_loss=4.89, nll_loss=2.125, w2v_ctc_loss=0.668, task_loss=3.803, task_loss_gen=6.388, contrastive_loss=0, total=4186.67, n_correct=2771.34, ppl=4.36, accuracy=66.194, wps=11561.4, ups=1.38, wpb=8373.3, bsz=317.4, num_updates=29600, lr=8.21995e-05, gnorm=0.503, clip=0, loss_scale=32, train_wall=72, gb_free=12.5, wall=25710
2023-09-02 17:01:56 | INFO | train_inner | epoch 021:    233 / 1474 loss=1.943, trans_loss=4.895, nll_loss=2.132, w2v_ctc_loss=0.662, task_loss=5.294, task_loss_gen=6.624, contrastive_loss=0, total=4166.37, n_correct=2756.48, ppl=4.38, accuracy=66.16, wps=11592.5, ups=1.39, wpb=8332.7, bsz=315.2, num_updates=29700, lr=8.2061e-05, gnorm=0.511, clip=0, loss_scale=32, train_wall=71, gb_free=13.7, wall=25782
2023-09-02 17:03:10 | INFO | train_inner | epoch 021:    333 / 1474 loss=1.953, trans_loss=4.9, nll_loss=2.137, w2v_ctc_loss=0.68, task_loss=10.752, task_loss_gen=8.591, contrastive_loss=0, total=4132.25, n_correct=2725.72, ppl=4.4, accuracy=65.962, wps=11296.6, ups=1.37, wpb=8264.5, bsz=305, num_updates=29800, lr=8.19232e-05, gnorm=0.508, clip=0, loss_scale=32, train_wall=72, gb_free=16.3, wall=25855
2023-09-02 17:04:22 | INFO | train_inner | epoch 021:    433 / 1474 loss=1.945, trans_loss=4.893, nll_loss=2.128, w2v_ctc_loss=0.669, task_loss=7.663, task_loss_gen=7.348, contrastive_loss=0, total=4195.53, n_correct=2777.68, ppl=4.37, accuracy=66.206, wps=11635.1, ups=1.39, wpb=8391.1, bsz=311.6, num_updates=29900, lr=8.17861e-05, gnorm=0.499, clip=0, loss_scale=32, train_wall=71, gb_free=16, wall=25927
2023-09-02 17:05:34 | INFO | train_inner | epoch 021:    533 / 1474 loss=1.948, trans_loss=4.888, nll_loss=2.122, w2v_ctc_loss=0.679, task_loss=5.877, task_loss_gen=6.559, contrastive_loss=0, total=4085.05, n_correct=2709.04, ppl=4.35, accuracy=66.316, wps=11313.4, ups=1.38, wpb=8170.1, bsz=296.1, num_updates=30000, lr=8.16497e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=71, gb_free=15.9, wall=25999
2023-09-02 17:05:34 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 17:06:07 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 3.921 | trans_loss 5.192 | nll_loss 2.456 | w2v_ctc_loss 1.372 | task_loss 12.22 | task_loss_gen 17.802 | contrastive_loss 0 | total 4003.4 | n_correct 2644.4 | ppl 5.49 | accuracy 66.054 | uer 17.586 | wer 19.347 | raw_wer 19.347 | bleu 21.38 | wps 1623.5 | wpb 4003.4 | bsz 141.8 | num_updates 30000 | best_bleu 21.59
2023-09-02 17:06:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30000 updates
2023-09-02 17:06:07 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_21_30000.pt
2023-09-02 17:06:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_21_30000.pt
2023-09-02 17:06:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_21_30000.pt (epoch 21 @ 30000 updates, score 21.38) (writing took 8.49964331497904 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:0')
2023-09-02 17:07:29 | INFO | train_inner | epoch 021:    633 / 1474 loss=1.948, trans_loss=4.894, nll_loss=2.131, w2v_ctc_loss=0.671, task_loss=5.137, task_loss_gen=6.164, contrastive_loss=0, total=4220.3, n_correct=2792.93, ppl=4.38, accuracy=66.178, wps=7346.2, ups=0.87, wpb=8440.6, bsz=315.8, num_updates=30100, lr=8.15139e-05, gnorm=0.506, clip=0, loss_scale=32, train_wall=72, gb_free=15.3, wall=26114
2023-09-02 17:08:41 | INFO | train_inner | epoch 021:    733 / 1474 loss=1.95, trans_loss=4.901, nll_loss=2.141, w2v_ctc_loss=0.671, task_loss=5.557, task_loss_gen=6.477, contrastive_loss=0, total=4148.18, n_correct=2741.95, ppl=4.41, accuracy=66.1, wps=11449.3, ups=1.38, wpb=8296.4, bsz=308.3, num_updates=30200, lr=8.13788e-05, gnorm=0.507, clip=0, loss_scale=32, train_wall=72, gb_free=11.5, wall=26186
2023-09-02 17:09:54 | INFO | train_inner | epoch 021:    833 / 1474 loss=1.96, trans_loss=4.91, nll_loss=2.151, w2v_ctc_loss=0.682, task_loss=5.18, task_loss_gen=6.941, contrastive_loss=0, total=4062.56, n_correct=2675.58, ppl=4.44, accuracy=65.859, wps=11098, ups=1.37, wpb=8125.1, bsz=293, num_updates=30300, lr=8.12444e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=72, gb_free=15.9, wall=26260
2023-09-02 17:11:07 | INFO | train_inner | epoch 021:    933 / 1474 loss=1.95, trans_loss=4.895, nll_loss=2.133, w2v_ctc_loss=0.68, task_loss=5.945, task_loss_gen=6.441, contrastive_loss=0, total=4103.66, n_correct=2715.03, ppl=4.39, accuracy=66.161, wps=11371.8, ups=1.39, wpb=8207.3, bsz=301.5, num_updates=30400, lr=8.11107e-05, gnorm=0.515, clip=0, loss_scale=32, train_wall=71, gb_free=16.9, wall=26332
2023-09-02 17:12:19 | INFO | train_inner | epoch 021:   1033 / 1474 loss=1.959, trans_loss=4.909, nll_loss=2.15, w2v_ctc_loss=0.685, task_loss=5.035, task_loss_gen=6.328, contrastive_loss=0, total=4100.54, n_correct=2701.36, ppl=4.44, accuracy=65.878, wps=11405.3, ups=1.39, wpb=8201.1, bsz=298.2, num_updates=30500, lr=8.09776e-05, gnorm=0.509, clip=0, loss_scale=32, train_wall=71, gb_free=17.6, wall=26404
2023-09-02 17:13:31 | INFO | train_inner | epoch 021:   1133 / 1474 loss=1.952, trans_loss=4.895, nll_loss=2.131, w2v_ctc_loss=0.676, task_loss=5.209, task_loss_gen=6.861, contrastive_loss=0, total=4119.98, n_correct=2725.02, ppl=4.38, accuracy=66.142, wps=11414.5, ups=1.39, wpb=8240, bsz=294, num_updates=30600, lr=8.08452e-05, gnorm=0.505, clip=0, loss_scale=32, train_wall=72, gb_free=17.5, wall=26476
2023-09-02 17:14:43 | INFO | train_inner | epoch 021:   1233 / 1474 loss=1.95, trans_loss=4.9, nll_loss=2.139, w2v_ctc_loss=0.676, task_loss=3.64, task_loss_gen=6.221, contrastive_loss=0, total=4161.49, n_correct=2750.98, ppl=4.4, accuracy=66.106, wps=11590.8, ups=1.39, wpb=8323, bsz=313, num_updates=30700, lr=8.07134e-05, gnorm=0.516, clip=0, loss_scale=32, train_wall=71, gb_free=16.5, wall=26548
2023-09-02 17:15:55 | INFO | train_inner | epoch 021:   1333 / 1474 loss=1.947, trans_loss=4.898, nll_loss=2.137, w2v_ctc_loss=0.67, task_loss=4.099, task_loss_gen=6.355, contrastive_loss=0, total=4141.76, n_correct=2740.04, ppl=4.4, accuracy=66.156, wps=11380.8, ups=1.37, wpb=8283.5, bsz=311.7, num_updates=30800, lr=8.05823e-05, gnorm=0.505, clip=0, loss_scale=32, train_wall=72, gb_free=17.1, wall=26621
2023-09-02 17:17:09 | INFO | train_inner | epoch 021:   1433 / 1474 loss=1.963, trans_loss=4.907, nll_loss=2.148, w2v_ctc_loss=0.694, task_loss=4.574, task_loss_gen=6.885, contrastive_loss=0, total=4127.02, n_correct=2717.39, ppl=4.43, accuracy=65.844, wps=11210.2, ups=1.36, wpb=8254, bsz=302.1, num_updates=30900, lr=8.04518e-05, gnorm=0.512, clip=0, loss_scale=32, train_wall=73, gb_free=16.1, wall=26694
2023-09-02 17:17:39 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:2')
2023-09-02 17:18:12 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 3.908 | trans_loss 5.187 | nll_loss 2.452 | w2v_ctc_loss 1.338 | task_loss 9.744 | task_loss_gen 18.695 | contrastive_loss 0 | total 4003.4 | n_correct 2652.6 | ppl 5.47 | accuracy 66.259 | uer 17.649 | wer 19.477 | raw_wer 19.477 | bleu 21.63 | wps 1649.7 | wpb 4003.4 | bsz 141.8 | num_updates 30941 | best_bleu 21.63
2023-09-02 17:18:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30941 updates
2023-09-02 17:18:12 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 17:18:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 17:18:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 21 @ 30941 updates, score 21.63) (writing took 13.826262298971415 seconds)
2023-09-02 17:18:26 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-09-02 17:18:26 | INFO | train | epoch 021 | loss 1.951 | trans_loss 4.898 | nll_loss 2.136 | w2v_ctc_loss 0.676 | task_loss 5.494 | task_loss_gen 6.725 | contrastive_loss 0 | total 4138.65 | n_correct 2735.09 | ppl 4.4 | accuracy 66.086 | wps 10469.2 | ups 1.26 | wpb 8277.3 | bsz 305.7 | num_updates 30941 | lr 8.03985e-05 | gnorm 0.51 | clip 0 | loss_scale 32 | train_wall 1058 | gb_free 15.1 | wall 26771
2023-09-02 17:18:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 17:18:26 | INFO | fairseq.trainer | begin training epoch 22
2023-09-02 17:18:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 17:19:17 | INFO | train_inner | epoch 022:     59 / 1474 loss=1.944, trans_loss=4.883, nll_loss=2.117, w2v_ctc_loss=0.672, task_loss=4.477, task_loss_gen=6.511, contrastive_loss=0, total=4140.16, n_correct=2752.04, ppl=4.34, accuracy=66.472, wps=6478.8, ups=0.78, wpb=8280.3, bsz=300.1, num_updates=31000, lr=8.03219e-05, gnorm=0.503, clip=0, loss_scale=32, train_wall=72, gb_free=17.4, wall=26822
2023-09-02 17:20:29 | INFO | train_inner | epoch 022:    159 / 1474 loss=1.938, trans_loss=4.878, nll_loss=2.11, w2v_ctc_loss=0.664, task_loss=4.024, task_loss_gen=6.551, contrastive_loss=0, total=4115.86, n_correct=2740.14, ppl=4.32, accuracy=66.575, wps=11333.6, ups=1.38, wpb=8231.7, bsz=309.4, num_updates=31100, lr=8.01927e-05, gnorm=0.507, clip=0, loss_scale=32, train_wall=72, gb_free=17, wall=26895
2023-09-02 17:21:42 | INFO | train_inner | epoch 022:    259 / 1474 loss=1.932, trans_loss=4.876, nll_loss=2.108, w2v_ctc_loss=0.657, task_loss=3.056, task_loss_gen=6.27, contrastive_loss=0, total=4247.73, n_correct=2829.66, ppl=4.31, accuracy=66.616, wps=11752.4, ups=1.38, wpb=8495.5, bsz=323.2, num_updates=31200, lr=8.00641e-05, gnorm=0.493, clip=0, loss_scale=32, train_wall=72, gb_free=13.5, wall=26967
2023-09-02 17:22:56 | INFO | train_inner | epoch 022:    359 / 1474 loss=1.945, trans_loss=4.886, nll_loss=2.121, w2v_ctc_loss=0.671, task_loss=3.056, task_loss_gen=7.045, contrastive_loss=0, total=4212.22, n_correct=2791.72, ppl=4.35, accuracy=66.277, wps=11348.5, ups=1.35, wpb=8424.4, bsz=317.9, num_updates=31300, lr=7.99361e-05, gnorm=0.502, clip=0, loss_scale=32, train_wall=74, gb_free=15.2, wall=27041
2023-09-02 17:24:09 | INFO | train_inner | epoch 022:    459 / 1474 loss=1.947, trans_loss=4.886, nll_loss=2.12, w2v_ctc_loss=0.675, task_loss=3.479, task_loss_gen=7.483, contrastive_loss=0, total=4131.12, n_correct=2743.36, ppl=4.35, accuracy=66.407, wps=11320, ups=1.37, wpb=8262.2, bsz=297.3, num_updates=31400, lr=7.98087e-05, gnorm=0.508, clip=0, loss_scale=64, train_wall=72, gb_free=16.1, wall=27114
2023-09-02 17:25:22 | INFO | train_inner | epoch 022:    559 / 1474 loss=1.94, trans_loss=4.879, nll_loss=2.112, w2v_ctc_loss=0.67, task_loss=2.788, task_loss_gen=8.108, contrastive_loss=0, total=4153.54, n_correct=2760.52, ppl=4.32, accuracy=66.462, wps=11323.7, ups=1.36, wpb=8307.1, bsz=307.1, num_updates=31500, lr=7.96819e-05, gnorm=0.502, clip=0, loss_scale=64, train_wall=73, gb_free=14.5, wall=27188
2023-09-02 17:26:34 | INFO | train_inner | epoch 022:    659 / 1474 loss=1.929, trans_loss=4.872, nll_loss=2.103, w2v_ctc_loss=0.653, task_loss=2.082, task_loss_gen=8.29, contrastive_loss=0, total=4143.91, n_correct=2764.53, ppl=4.3, accuracy=66.713, wps=11618.1, ups=1.4, wpb=8287.8, bsz=313.1, num_updates=31600, lr=7.95557e-05, gnorm=0.504, clip=0, loss_scale=64, train_wall=71, gb_free=15.5, wall=27259
2023-09-02 17:27:46 | INFO | train_inner | epoch 022:    759 / 1474 loss=1.943, trans_loss=4.88, nll_loss=2.113, w2v_ctc_loss=0.674, task_loss=1.982, task_loss_gen=9.835, contrastive_loss=0, total=4168.91, n_correct=2771.09, ppl=4.33, accuracy=66.47, wps=11460.9, ups=1.37, wpb=8337.8, bsz=303.5, num_updates=31700, lr=7.94301e-05, gnorm=0.509, clip=0, loss_scale=64, train_wall=72, gb_free=17.2, wall=27332
2023-09-02 17:29:00 | INFO | train_inner | epoch 022:    859 / 1474 loss=1.948, trans_loss=4.89, nll_loss=2.126, w2v_ctc_loss=0.673, task_loss=1.958, task_loss_gen=11.189, contrastive_loss=0, total=4079.59, n_correct=2700.24, ppl=4.37, accuracy=66.189, wps=11078.4, ups=1.36, wpb=8159.2, bsz=288.7, num_updates=31800, lr=7.93052e-05, gnorm=0.513, clip=0, loss_scale=64, train_wall=73, gb_free=16.9, wall=27405
2023-09-02 17:30:13 | INFO | train_inner | epoch 022:    959 / 1474 loss=1.937, trans_loss=4.878, nll_loss=2.11, w2v_ctc_loss=0.66, task_loss=2.036, task_loss_gen=10.409, contrastive_loss=0, total=4129.75, n_correct=2749.83, ppl=4.32, accuracy=66.586, wps=11374.7, ups=1.38, wpb=8259.5, bsz=303.9, num_updates=31900, lr=7.91808e-05, gnorm=0.506, clip=0, loss_scale=64, train_wall=72, gb_free=17.4, wall=27478
2023-09-02 17:31:25 | INFO | train_inner | epoch 022:   1059 / 1474 loss=1.931, trans_loss=4.874, nll_loss=2.105, w2v_ctc_loss=0.652, task_loss=1.772, task_loss_gen=10.357, contrastive_loss=0, total=4155.56, n_correct=2770.59, ppl=4.3, accuracy=66.672, wps=11460.7, ups=1.38, wpb=8311.1, bsz=315.3, num_updates=32000, lr=7.90569e-05, gnorm=0.502, clip=0, loss_scale=64, train_wall=72, gb_free=16.7, wall=27550
2023-09-02 17:31:25 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 17:31:58 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 3.905 | trans_loss 5.186 | nll_loss 2.452 | w2v_ctc_loss 1.331 | task_loss 7.771 | task_loss_gen 24.939 | contrastive_loss 0 | total 4003.4 | n_correct 2653.4 | ppl 5.47 | accuracy 66.279 | uer 18.053 | wer 19.977 | raw_wer 19.977 | bleu 21.44 | wps 1680.9 | wpb 4003.4 | bsz 141.8 | num_updates 32000 | best_bleu 21.63
2023-09-02 17:31:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32000 updates
2023-09-02 17:31:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_22_32000.pt
2023-09-02 17:32:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_22_32000.pt
2023-09-02 17:32:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_22_32000.pt (epoch 22 @ 32000 updates, score 21.44) (writing took 8.305033376964275 seconds)
2023-09-02 17:33:18 | INFO | train_inner | epoch 022:   1159 / 1474 loss=1.953, trans_loss=4.899, nll_loss=2.138, w2v_ctc_loss=0.678, task_loss=1.759, task_loss_gen=12.199, contrastive_loss=0, total=4089.92, n_correct=2701.96, ppl=4.4, accuracy=66.064, wps=7225.1, ups=0.88, wpb=8179.8, bsz=292.2, num_updates=32100, lr=7.89337e-05, gnorm=0.509, clip=0, loss_scale=64, train_wall=71, gb_free=15.8, wall=27664
2023-09-02 17:34:31 | INFO | train_inner | epoch 022:   1259 / 1474 loss=1.942, trans_loss=4.894, nll_loss=2.132, w2v_ctc_loss=0.666, task_loss=1.757, task_loss_gen=10.833, contrastive_loss=0, total=4179.82, n_correct=2768.23, ppl=4.38, accuracy=66.228, wps=11539.9, ups=1.38, wpb=8359.6, bsz=322.5, num_updates=32200, lr=7.8811e-05, gnorm=0.5, clip=0, loss_scale=64, train_wall=72, gb_free=15.5, wall=27736
2023-09-02 17:35:42 | INFO | train_inner | epoch 022:   1359 / 1474 loss=1.936, trans_loss=4.878, nll_loss=2.111, w2v_ctc_loss=0.662, task_loss=1.895, task_loss_gen=11.185, contrastive_loss=0, total=4076.98, n_correct=2713.95, ppl=4.32, accuracy=66.568, wps=11401.1, ups=1.4, wpb=8154, bsz=303.1, num_updates=32300, lr=7.86889e-05, gnorm=0.514, clip=0, loss_scale=64, train_wall=71, gb_free=10.1, wall=27808
2023-09-02 17:36:55 | INFO | train_inner | epoch 022:   1459 / 1474 loss=1.952, trans_loss=4.897, nll_loss=2.135, w2v_ctc_loss=0.679, task_loss=1.724, task_loss_gen=13.162, contrastive_loss=0, total=4070.93, n_correct=2694.23, ppl=4.39, accuracy=66.182, wps=11240.2, ups=1.38, wpb=8141.9, bsz=286.5, num_updates=32400, lr=7.85674e-05, gnorm=0.518, clip=0, loss_scale=64, train_wall=72, gb_free=16.4, wall=27880
2023-09-02 17:37:06 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 17:37:40 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 3.882 | trans_loss 5.175 | nll_loss 2.438 | w2v_ctc_loss 1.278 | task_loss 2.958 | task_loss_gen 34.714 | contrastive_loss 0 | total 4003.4 | n_correct 2658.6 | ppl 5.42 | accuracy 66.409 | uer 17.612 | wer 19.436 | raw_wer 19.436 | bleu 21.86 | wps 1563.9 | wpb 4003.4 | bsz 141.8 | num_updates 32415 | best_bleu 21.86
2023-09-02 17:37:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32415 updates
2023-09-02 17:37:40 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 17:37:46 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 17:37:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 22 @ 32415 updates, score 21.86) (writing took 12.483123150013853 seconds)
2023-09-02 17:37:53 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-09-02 17:37:53 | INFO | train | epoch 022 | loss 1.941 | trans_loss 4.883 | nll_loss 2.117 | w2v_ctc_loss 0.667 | task_loss 2.461 | task_loss_gen 9.374 | contrastive_loss 0 | total 4138.65 | n_correct 2749.79 | ppl 4.34 | accuracy 66.442 | wps 10456.4 | ups 1.26 | wpb 8277.3 | bsz 305.7 | num_updates 32415 | lr 7.85492e-05 | gnorm 0.506 | clip 0 | loss_scale 64 | train_wall 1059 | gb_free 11.3 | wall 27938
2023-09-02 17:37:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 17:37:53 | INFO | fairseq.trainer | begin training epoch 23
2023-09-02 17:37:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 17:38:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-02 17:39:03 | INFO | train_inner | epoch 023:     86 / 1474 loss=1.937, trans_loss=4.87, nll_loss=2.1, w2v_ctc_loss=0.67, task_loss=2.163, task_loss_gen=11.548, contrastive_loss=0, total=4097.38, n_correct=2733.53, ppl=4.29, accuracy=66.714, wps=6407.8, ups=0.78, wpb=8194.8, bsz=301.7, num_updates=32500, lr=7.84465e-05, gnorm=0.514, clip=0, loss_scale=32, train_wall=73, gb_free=16.3, wall=28008
2023-09-02 17:40:15 | INFO | train_inner | epoch 023:    186 / 1474 loss=1.933, trans_loss=4.865, nll_loss=2.093, w2v_ctc_loss=0.656, task_loss=3.172, task_loss_gen=9.537, contrastive_loss=0, total=4117.76, n_correct=2748.76, ppl=4.27, accuracy=66.754, wps=11338.5, ups=1.38, wpb=8235.5, bsz=296, num_updates=32600, lr=7.8326e-05, gnorm=0.52, clip=0, loss_scale=32, train_wall=72, gb_free=15.2, wall=28081
2023-09-02 17:41:29 | INFO | train_inner | epoch 023:    286 / 1474 loss=1.936, trans_loss=4.879, nll_loss=2.111, w2v_ctc_loss=0.653, task_loss=9.095, task_loss_gen=10.563, contrastive_loss=0, total=4144.73, n_correct=2757.22, ppl=4.32, accuracy=66.524, wps=11278.1, ups=1.36, wpb=8289.5, bsz=304, num_updates=32700, lr=7.82062e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=73, gb_free=17.2, wall=28154
2023-09-02 17:42:41 | INFO | train_inner | epoch 023:    386 / 1474 loss=1.932, trans_loss=4.868, nll_loss=2.097, w2v_ctc_loss=0.651, task_loss=22.265, task_loss_gen=15.286, contrastive_loss=0, total=4126.79, n_correct=2755.19, ppl=4.28, accuracy=66.764, wps=11438.8, ups=1.39, wpb=8253.6, bsz=296.4, num_updates=32800, lr=7.80869e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=71, gb_free=16.8, wall=28226
2023-09-02 17:43:53 | INFO | train_inner | epoch 023:    486 / 1474 loss=1.935, trans_loss=4.877, nll_loss=2.108, w2v_ctc_loss=0.658, task_loss=19.167, task_loss_gen=14.221, contrastive_loss=0, total=4150.15, n_correct=2762.87, ppl=4.31, accuracy=66.573, wps=11505.4, ups=1.39, wpb=8300.3, bsz=312.1, num_updates=32900, lr=7.79681e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=71, gb_free=15.9, wall=28298
2023-09-02 17:45:05 | INFO | train_inner | epoch 023:    586 / 1474 loss=1.927, trans_loss=4.866, nll_loss=2.094, w2v_ctc_loss=0.653, task_loss=20.554, task_loss_gen=15.32, contrastive_loss=0, total=4174.6, n_correct=2788.14, ppl=4.27, accuracy=66.788, wps=11569.2, ups=1.39, wpb=8349.2, bsz=316.3, num_updates=33000, lr=7.78499e-05, gnorm=0.517, clip=0, loss_scale=32, train_wall=71, gb_free=16, wall=28371
2023-09-02 17:46:18 | INFO | train_inner | epoch 023:    686 / 1474 loss=1.939, trans_loss=4.879, nll_loss=2.111, w2v_ctc_loss=0.662, task_loss=15.83, task_loss_gen=12.038, contrastive_loss=0, total=4136.6, n_correct=2753.61, ppl=4.32, accuracy=66.567, wps=11401.6, ups=1.38, wpb=8273.2, bsz=301.2, num_updates=33100, lr=7.77322e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=72, gb_free=17.4, wall=28443
2023-09-02 17:47:30 | INFO | train_inner | epoch 023:    786 / 1474 loss=1.939, trans_loss=4.879, nll_loss=2.111, w2v_ctc_loss=0.666, task_loss=14.961, task_loss_gen=11.625, contrastive_loss=0, total=4147.22, n_correct=2761.05, ppl=4.32, accuracy=66.576, wps=11484.9, ups=1.38, wpb=8294.4, bsz=305.1, num_updates=33200, lr=7.76151e-05, gnorm=0.562, clip=0, loss_scale=32, train_wall=71, gb_free=17, wall=28515
2023-09-02 17:48:42 | INFO | train_inner | epoch 023:    886 / 1474 loss=1.929, trans_loss=4.87, nll_loss=2.1, w2v_ctc_loss=0.657, task_loss=10.682, task_loss_gen=8.503, contrastive_loss=0, total=4193.16, n_correct=2799.23, ppl=4.29, accuracy=66.757, wps=11638.9, ups=1.39, wpb=8386.3, bsz=327.3, num_updates=33300, lr=7.74984e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=71, gb_free=15.6, wall=28587
2023-09-02 17:49:55 | INFO | train_inner | epoch 023:    986 / 1474 loss=1.934, trans_loss=4.874, nll_loss=2.105, w2v_ctc_loss=0.655, task_loss=13.536, task_loss_gen=10.245, contrastive_loss=0, total=4164.33, n_correct=2771.67, ppl=4.3, accuracy=66.557, wps=11463.7, ups=1.38, wpb=8328.7, bsz=310.1, num_updates=33400, lr=7.73823e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=72, gb_free=17.4, wall=28660
2023-09-02 17:51:08 | INFO | train_inner | epoch 023:   1086 / 1474 loss=1.941, trans_loss=4.877, nll_loss=2.109, w2v_ctc_loss=0.669, task_loss=9.421, task_loss_gen=7.946, contrastive_loss=0, total=4088.37, n_correct=2723.31, ppl=4.31, accuracy=66.611, wps=11142.3, ups=1.36, wpb=8176.7, bsz=289.6, num_updates=33500, lr=7.72667e-05, gnorm=0.517, clip=0, loss_scale=32, train_wall=73, gb_free=15.1, wall=28733
2023-09-02 17:52:22 | INFO | train_inner | epoch 023:   1186 / 1474 loss=1.932, trans_loss=4.875, nll_loss=2.107, w2v_ctc_loss=0.659, task_loss=7.897, task_loss_gen=6.928, contrastive_loss=0, total=4162.3, n_correct=2772.51, ppl=4.31, accuracy=66.61, wps=11327.3, ups=1.36, wpb=8324.6, bsz=309, num_updates=33600, lr=7.71517e-05, gnorm=0.511, clip=0, loss_scale=32, train_wall=73, gb_free=15.3, wall=28807
2023-09-02 17:53:34 | INFO | train_inner | epoch 023:   1286 / 1474 loss=1.927, trans_loss=4.87, nll_loss=2.099, w2v_ctc_loss=0.652, task_loss=6.478, task_loss_gen=6.264, contrastive_loss=0, total=4131.74, n_correct=2761.92, ppl=4.28, accuracy=66.846, wps=11487.5, ups=1.39, wpb=8263.5, bsz=308.7, num_updates=33700, lr=7.70371e-05, gnorm=0.503, clip=0, loss_scale=32, train_wall=71, gb_free=16.7, wall=28879
2023-09-02 17:54:47 | INFO | train_inner | epoch 023:   1386 / 1474 loss=1.941, trans_loss=4.885, nll_loss=2.12, w2v_ctc_loss=0.667, task_loss=6.31, task_loss_gen=6.628, contrastive_loss=0, total=4141.25, n_correct=2752.06, ppl=4.35, accuracy=66.455, wps=11349.6, ups=1.37, wpb=8282.5, bsz=304.7, num_updates=33800, lr=7.69231e-05, gnorm=0.52, clip=0, loss_scale=32, train_wall=72, gb_free=16.4, wall=28952
2023-09-02 17:55:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 17:56:23 | INFO | dev_st | epoch 023 | valid on 'dev_st' subset | loss 3.902 | trans_loss 5.181 | nll_loss 2.444 | w2v_ctc_loss 1.334 | task_loss 16.058 | task_loss_gen 17.792 | contrastive_loss 0 | total 4003.4 | n_correct 2661.1 | ppl 5.44 | accuracy 66.471 | uer 17.509 | wer 19.291 | raw_wer 19.291 | bleu 21.47 | wps 1660.7 | wpb 4003.4 | bsz 141.8 | num_updates 33888 | best_bleu 21.86
2023-09-02 17:56:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 33888 updates
2023-09-02 17:56:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.4700.pt
2023-09-02 17:56:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.4700.pt
2023-09-02 17:56:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.4700.pt (epoch 23 @ 33888 updates, score 21.47) (writing took 8.343522779003251 seconds)
2023-09-02 17:56:32 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-09-02 17:56:32 | INFO | train | epoch 023 | loss 1.935 | trans_loss 4.874 | nll_loss 2.105 | w2v_ctc_loss 0.659 | task_loss 11.379 | task_loss_gen 10.227 | contrastive_loss 0 | total 4138.69 | n_correct 2757.99 | ppl 4.3 | accuracy 66.639 | wps 10893.7 | ups 1.32 | wpb 8277.4 | bsz 305.7 | num_updates 33888 | lr 7.68231e-05 | gnorm 0.524 | clip 0 | loss_scale 32 | train_wall 1059 | gb_free 13.3 | wall 29057
2023-09-02 17:56:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 17:56:32 | INFO | fairseq.trainer | begin training epoch 24
2023-09-02 17:56:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 17:56:48 | INFO | train_inner | epoch 024:     12 / 1474 loss=1.935, trans_loss=4.881, nll_loss=2.115, w2v_ctc_loss=0.655, task_loss=6.532, task_loss_gen=6.663, contrastive_loss=0, total=4095.53, n_correct=2723.61, ppl=4.33, accuracy=66.502, wps=6731.2, ups=0.82, wpb=8191.1, bsz=306.3, num_updates=33900, lr=7.68095e-05, gnorm=0.514, clip=0, loss_scale=32, train_wall=72, gb_free=17, wall=29074
2023-09-02 17:58:01 | INFO | train_inner | epoch 024:    112 / 1474 loss=1.917, trans_loss=4.85, nll_loss=2.073, w2v_ctc_loss=0.643, task_loss=5.191, task_loss_gen=5.625, contrastive_loss=0, total=4167.42, n_correct=2794.73, ppl=4.21, accuracy=67.061, wps=11449.4, ups=1.37, wpb=8334.8, bsz=323, num_updates=34000, lr=7.66965e-05, gnorm=0.507, clip=0, loss_scale=32, train_wall=72, gb_free=17.4, wall=29146
2023-09-02 17:58:01 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 17:58:34 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 3.883 | trans_loss 5.179 | nll_loss 2.437 | w2v_ctc_loss 1.271 | task_loss 15.011 | task_loss_gen 17.303 | contrastive_loss 0 | total 4003.4 | n_correct 2661.1 | ppl 5.42 | accuracy 66.471 | uer 17.113 | wer 19.112 | raw_wer 19.112 | bleu 21.2 | wps 1643.6 | wpb 4003.4 | bsz 141.8 | num_updates 34000 | best_bleu 21.86
2023-09-02 17:58:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 34000 updates
2023-09-02 17:58:34 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_24_34000.pt
2023-09-02 17:58:36 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_24_34000.pt
2023-09-02 17:58:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_24_34000.pt (epoch 24 @ 34000 updates, score 21.2) (writing took 9.569590234023053 seconds)
2023-09-02 17:59:57 | INFO | train_inner | epoch 024:    212 / 1474 loss=1.915, trans_loss=4.856, nll_loss=2.082, w2v_ctc_loss=0.637, task_loss=4.568, task_loss_gen=5.282, contrastive_loss=0, total=4247.08, n_correct=2850.3, ppl=4.24, accuracy=67.112, wps=7334.4, ups=0.86, wpb=8494.2, bsz=339.5, num_updates=34100, lr=7.6584e-05, gnorm=0.497, clip=0, loss_scale=32, train_wall=72, gb_free=16.2, wall=29262
2023-09-02 18:01:09 | INFO | train_inner | epoch 024:    312 / 1474 loss=1.917, trans_loss=4.851, nll_loss=2.075, w2v_ctc_loss=0.64, task_loss=4.295, task_loss_gen=6.161, contrastive_loss=0, total=4139.31, n_correct=2782.26, ppl=4.21, accuracy=67.216, wps=11470, ups=1.39, wpb=8278.6, bsz=308.4, num_updates=34200, lr=7.64719e-05, gnorm=0.498, clip=0, loss_scale=32, train_wall=71, gb_free=16.1, wall=29334
2023-09-02 18:02:22 | INFO | train_inner | epoch 024:    412 / 1474 loss=1.935, trans_loss=4.859, nll_loss=2.086, w2v_ctc_loss=0.666, task_loss=3.294, task_loss_gen=7.301, contrastive_loss=0, total=4157.07, n_correct=2775.5, ppl=4.25, accuracy=66.766, wps=11457.3, ups=1.38, wpb=8314.1, bsz=299.9, num_updates=34300, lr=7.63604e-05, gnorm=0.512, clip=0, loss_scale=32, train_wall=72, gb_free=15.5, wall=29407
2023-09-02 18:03:35 | INFO | train_inner | epoch 024:    512 / 1474 loss=1.924, trans_loss=4.854, nll_loss=2.079, w2v_ctc_loss=0.651, task_loss=5, task_loss_gen=7.048, contrastive_loss=0, total=4138.82, n_correct=2775.41, ppl=4.23, accuracy=67.058, wps=11325, ups=1.37, wpb=8277.6, bsz=301.6, num_updates=34400, lr=7.62493e-05, gnorm=0.51, clip=0, loss_scale=32, train_wall=72, gb_free=16.7, wall=29480
2023-09-02 18:04:47 | INFO | train_inner | epoch 024:    612 / 1474 loss=1.917, trans_loss=4.852, nll_loss=2.076, w2v_ctc_loss=0.639, task_loss=6.401, task_loss_gen=7.138, contrastive_loss=0, total=4164.75, n_correct=2794.36, ppl=4.22, accuracy=67.096, wps=11470.8, ups=1.38, wpb=8329.5, bsz=309.1, num_updates=34500, lr=7.61387e-05, gnorm=0.513, clip=0, loss_scale=64, train_wall=72, gb_free=15.2, wall=29553
2023-09-02 18:06:00 | INFO | train_inner | epoch 024:    712 / 1474 loss=1.931, trans_loss=4.867, nll_loss=2.096, w2v_ctc_loss=0.654, task_loss=6.06, task_loss_gen=7.273, contrastive_loss=0, total=4103.92, n_correct=2739.33, ppl=4.27, accuracy=66.749, wps=11315.6, ups=1.38, wpb=8207.8, bsz=294, num_updates=34600, lr=7.60286e-05, gnorm=0.508, clip=0, loss_scale=64, train_wall=72, gb_free=11.6, wall=29625
2023-09-02 18:07:13 | INFO | train_inner | epoch 024:    812 / 1474 loss=1.927, trans_loss=4.865, nll_loss=2.095, w2v_ctc_loss=0.653, task_loss=4.08, task_loss_gen=6.801, contrastive_loss=0, total=4109.8, n_correct=2750.58, ppl=4.27, accuracy=66.927, wps=11268.9, ups=1.37, wpb=8219.6, bsz=305.3, num_updates=34700, lr=7.5919e-05, gnorm=0.503, clip=0, loss_scale=64, train_wall=72, gb_free=16.9, wall=29698
2023-09-02 18:08:25 | INFO | train_inner | epoch 024:    912 / 1474 loss=1.936, trans_loss=4.865, nll_loss=2.093, w2v_ctc_loss=0.665, task_loss=4.134, task_loss_gen=7.586, contrastive_loss=0, total=4042.08, n_correct=2697.24, ppl=4.27, accuracy=66.729, wps=11216.5, ups=1.39, wpb=8084.2, bsz=280.6, num_updates=34800, lr=7.58098e-05, gnorm=0.522, clip=0, loss_scale=64, train_wall=71, gb_free=16.3, wall=29770
2023-09-02 18:09:38 | INFO | train_inner | epoch 024:   1012 / 1474 loss=1.927, trans_loss=4.864, nll_loss=2.093, w2v_ctc_loss=0.649, task_loss=3.431, task_loss_gen=7.394, contrastive_loss=0, total=4140.44, n_correct=2770.5, ppl=4.27, accuracy=66.913, wps=11419.8, ups=1.38, wpb=8280.9, bsz=298.7, num_updates=34900, lr=7.57011e-05, gnorm=0.507, clip=0, loss_scale=64, train_wall=72, gb_free=11.3, wall=29843
2023-09-02 18:10:50 | INFO | train_inner | epoch 024:   1112 / 1474 loss=1.921, trans_loss=4.85, nll_loss=2.075, w2v_ctc_loss=0.654, task_loss=2.954, task_loss_gen=7.377, contrastive_loss=0, total=4134.93, n_correct=2773.43, ppl=4.21, accuracy=67.073, wps=11401.9, ups=1.38, wpb=8269.9, bsz=309.4, num_updates=35000, lr=7.55929e-05, gnorm=0.504, clip=0, loss_scale=64, train_wall=72, gb_free=16, wall=29915
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:0')
2023-09-02 18:12:03 | INFO | train_inner | epoch 024:   1212 / 1474 loss=1.925, trans_loss=4.861, nll_loss=2.088, w2v_ctc_loss=0.652, task_loss=2.695, task_loss_gen=8.217, contrastive_loss=0, total=4144.49, n_correct=2774.12, ppl=4.25, accuracy=66.935, wps=11393.5, ups=1.37, wpb=8289, bsz=309.8, num_updates=35100, lr=7.54851e-05, gnorm=0.507, clip=0, loss_scale=64, train_wall=72, gb_free=16.2, wall=29988
2023-09-02 18:13:16 | INFO | train_inner | epoch 024:   1312 / 1474 loss=1.93, trans_loss=4.859, nll_loss=2.086, w2v_ctc_loss=0.662, task_loss=2.636, task_loss_gen=9.338, contrastive_loss=0, total=4110.93, n_correct=2755.11, ppl=4.25, accuracy=67.019, wps=11231.1, ups=1.37, wpb=8221.9, bsz=293, num_updates=35200, lr=7.53778e-05, gnorm=0.512, clip=0, loss_scale=64, train_wall=72, gb_free=13.1, wall=30061
2023-09-02 18:14:29 | INFO | train_inner | epoch 024:   1412 / 1474 loss=1.931, trans_loss=4.865, nll_loss=2.094, w2v_ctc_loss=0.66, task_loss=2.392, task_loss_gen=9.576, contrastive_loss=0, total=4088.73, n_correct=2734.17, ppl=4.27, accuracy=66.871, wps=11286.8, ups=1.38, wpb=8177.5, bsz=294.1, num_updates=35300, lr=7.5271e-05, gnorm=0.507, clip=0, loss_scale=64, train_wall=72, gb_free=17.4, wall=30134
2023-09-02 18:15:14 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:3')
2023-09-02 18:15:46 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 3.885 | trans_loss 5.175 | nll_loss 2.438 | w2v_ctc_loss 1.286 | task_loss 9.362 | task_loss_gen 22.324 | contrastive_loss 0 | total 4003.4 | n_correct 2662.6 | ppl 5.42 | accuracy 66.508 | uer 17.209 | wer 19.078 | raw_wer 19.078 | bleu 21.74 | wps 1633.8 | wpb 4003.4 | bsz 141.8 | num_updates 35362 | best_bleu 21.86
2023-09-02 18:15:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 35362 updates
2023-09-02 18:15:46 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.7402.pt
2023-09-02 18:15:50 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.7402.pt
2023-09-02 18:15:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.7402.pt (epoch 24 @ 35362 updates, score 21.74) (writing took 8.09689270396484 seconds)
2023-09-02 18:15:55 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-09-02 18:15:55 | INFO | train | epoch 024 | loss 1.925 | trans_loss 4.858 | nll_loss 2.085 | w2v_ctc_loss 0.651 | task_loss 4.024 | task_loss_gen 7.315 | contrastive_loss 0 | total 4138.65 | n_correct 2772.05 | ppl 4.24 | accuracy 66.98 | wps 10492.9 | ups 1.27 | wpb 8277.3 | bsz 305.7 | num_updates 35362 | lr 7.5205e-05 | gnorm 0.507 | clip 0 | loss_scale 64 | train_wall 1059 | gb_free 15.8 | wall 30220
2023-09-02 18:15:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 18:15:55 | INFO | fairseq.trainer | begin training epoch 25
2023-09-02 18:15:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 18:16:30 | INFO | train_inner | epoch 025:     38 / 1474 loss=1.917, trans_loss=4.852, nll_loss=2.078, w2v_ctc_loss=0.643, task_loss=2.323, task_loss_gen=9.179, contrastive_loss=0, total=4170.36, n_correct=2806.25, ppl=4.22, accuracy=67.29, wps=6885.8, ups=0.83, wpb=8340.7, bsz=313, num_updates=35400, lr=7.51646e-05, gnorm=0.503, clip=0, loss_scale=64, train_wall=71, gb_free=15.9, wall=30255
2023-09-02 18:17:42 | INFO | train_inner | epoch 025:    138 / 1474 loss=1.911, trans_loss=4.838, nll_loss=2.058, w2v_ctc_loss=0.639, task_loss=3.782, task_loss_gen=8.818, contrastive_loss=0, total=4133.56, n_correct=2786.48, ppl=4.16, accuracy=67.411, wps=11490.7, ups=1.39, wpb=8267.1, bsz=306.4, num_updates=35500, lr=7.50587e-05, gnorm=0.513, clip=0, loss_scale=64, train_wall=71, gb_free=16.8, wall=30327
2023-09-02 18:18:55 | INFO | train_inner | epoch 025:    238 / 1474 loss=1.914, trans_loss=4.842, nll_loss=2.064, w2v_ctc_loss=0.64, task_loss=10.322, task_loss_gen=9.994, contrastive_loss=0, total=4112.46, n_correct=2769.15, ppl=4.18, accuracy=67.336, wps=11244, ups=1.37, wpb=8224.9, bsz=303.4, num_updates=35600, lr=7.49532e-05, gnorm=0.513, clip=0, loss_scale=64, train_wall=72, gb_free=16.6, wall=30400
2023-09-02 18:20:08 | INFO | train_inner | epoch 025:    338 / 1474 loss=1.918, trans_loss=4.843, nll_loss=2.064, w2v_ctc_loss=0.642, task_loss=15.135, task_loss_gen=12.657, contrastive_loss=0, total=4139.11, n_correct=2786.03, ppl=4.18, accuracy=67.31, wps=11283.2, ups=1.36, wpb=8278.2, bsz=291.7, num_updates=35700, lr=7.48481e-05, gnorm=0.513, clip=0, loss_scale=64, train_wall=73, gb_free=14.6, wall=30473
2023-09-02 18:21:21 | INFO | train_inner | epoch 025:    438 / 1474 loss=1.922, trans_loss=4.842, nll_loss=2.064, w2v_ctc_loss=0.654, task_loss=10.815, task_loss_gen=10.456, contrastive_loss=0, total=4181.96, n_correct=2810.6, ppl=4.18, accuracy=67.208, wps=11421.5, ups=1.37, wpb=8363.9, bsz=303, num_updates=35800, lr=7.47435e-05, gnorm=0.511, clip=0, loss_scale=64, train_wall=72, gb_free=16.3, wall=30547
2023-09-02 18:22:34 | INFO | train_inner | epoch 025:    538 / 1474 loss=1.919, trans_loss=4.854, nll_loss=2.079, w2v_ctc_loss=0.646, task_loss=7.402, task_loss_gen=7.604, contrastive_loss=0, total=4162.59, n_correct=2796.11, ppl=4.23, accuracy=67.172, wps=11466.9, ups=1.38, wpb=8325.2, bsz=312.9, num_updates=35900, lr=7.46393e-05, gnorm=0.504, clip=0, loss_scale=64, train_wall=72, gb_free=11.5, wall=30619
2023-09-02 18:23:46 | INFO | train_inner | epoch 025:    638 / 1474 loss=1.914, trans_loss=4.837, nll_loss=2.058, w2v_ctc_loss=0.643, task_loss=6.005, task_loss_gen=7.119, contrastive_loss=0, total=4142.58, n_correct=2788.69, ppl=4.16, accuracy=67.318, wps=11462.9, ups=1.38, wpb=8285.2, bsz=308.3, num_updates=36000, lr=7.45356e-05, gnorm=0.507, clip=0, loss_scale=64, train_wall=72, gb_free=14.4, wall=30691
2023-09-02 18:23:46 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 18:24:19 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 3.904 | trans_loss 5.18 | nll_loss 2.441 | w2v_ctc_loss 1.34 | task_loss 13.78 | task_loss_gen 20.496 | contrastive_loss 0 | total 4003.4 | n_correct 2663.4 | ppl 5.43 | accuracy 66.528 | uer 17.302 | wer 19.078 | raw_wer 19.078 | bleu 21.53 | wps 1655.6 | wpb 4003.4 | bsz 141.8 | num_updates 36000 | best_bleu 21.86
2023-09-02 18:24:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36000 updates
2023-09-02 18:24:19 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_25_36000.pt
2023-09-02 18:24:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_25_36000.pt
2023-09-02 18:24:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_25_36000.pt (epoch 25 @ 36000 updates, score 21.53) (writing took 9.373653658025432 seconds)
2023-09-02 18:25:42 | INFO | train_inner | epoch 025:    738 / 1474 loss=1.92, trans_loss=4.843, nll_loss=2.066, w2v_ctc_loss=0.651, task_loss=3.808, task_loss_gen=7.411, contrastive_loss=0, total=4126.89, n_correct=2774.81, ppl=4.19, accuracy=67.237, wps=7158.8, ups=0.87, wpb=8253.8, bsz=300.9, num_updates=36100, lr=7.44323e-05, gnorm=0.514, clip=0, loss_scale=64, train_wall=72, gb_free=16.7, wall=30807
2023-09-02 18:26:54 | INFO | train_inner | epoch 025:    838 / 1474 loss=1.907, trans_loss=4.843, nll_loss=2.065, w2v_ctc_loss=0.634, task_loss=2.583, task_loss_gen=7.033, contrastive_loss=0, total=4199.63, n_correct=2833.75, ppl=4.18, accuracy=67.476, wps=11582.1, ups=1.38, wpb=8399.3, bsz=329, num_updates=36200, lr=7.43294e-05, gnorm=0.498, clip=0, loss_scale=64, train_wall=72, gb_free=15.7, wall=30879
2023-09-02 18:28:07 | INFO | train_inner | epoch 025:    938 / 1474 loss=1.919, trans_loss=4.849, nll_loss=2.073, w2v_ctc_loss=0.65, task_loss=2.252, task_loss_gen=8.128, contrastive_loss=0, total=4134.29, n_correct=2777.22, ppl=4.21, accuracy=67.175, wps=11373.2, ups=1.38, wpb=8268.6, bsz=312.8, num_updates=36300, lr=7.4227e-05, gnorm=0.511, clip=0, loss_scale=64, train_wall=72, gb_free=16.9, wall=30952
2023-09-02 18:29:19 | INFO | train_inner | epoch 025:   1038 / 1474 loss=1.92, trans_loss=4.857, nll_loss=2.084, w2v_ctc_loss=0.641, task_loss=2.435, task_loss_gen=8.271, contrastive_loss=0, total=4184.54, n_correct=2803, ppl=4.24, accuracy=66.985, wps=11538.1, ups=1.38, wpb=8369.1, bsz=311.2, num_updates=36400, lr=7.41249e-05, gnorm=0.504, clip=0, loss_scale=64, train_wall=72, gb_free=17.3, wall=31024
2023-09-02 18:30:32 | INFO | train_inner | epoch 025:   1138 / 1474 loss=1.92, trans_loss=4.846, nll_loss=2.07, w2v_ctc_loss=0.644, task_loss=2.527, task_loss_gen=9.425, contrastive_loss=0, total=4042.38, n_correct=2716.21, ppl=4.2, accuracy=67.193, wps=11130.2, ups=1.38, wpb=8084.8, bsz=286.2, num_updates=36500, lr=7.40233e-05, gnorm=0.511, clip=0, loss_scale=64, train_wall=72, gb_free=16.6, wall=31097
2023-09-02 18:31:44 | INFO | train_inner | epoch 025:   1238 / 1474 loss=1.924, trans_loss=4.856, nll_loss=2.082, w2v_ctc_loss=0.651, task_loss=2.333, task_loss_gen=9.487, contrastive_loss=0, total=4077.76, n_correct=2736.42, ppl=4.23, accuracy=67.106, wps=11366.6, ups=1.39, wpb=8155.5, bsz=291.8, num_updates=36600, lr=7.39221e-05, gnorm=0.51, clip=0, loss_scale=128, train_wall=71, gb_free=15.2, wall=31169
2023-09-02 18:32:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-09-02 18:32:58 | INFO | train_inner | epoch 025:   1339 / 1474 loss=1.917, trans_loss=4.847, nll_loss=2.071, w2v_ctc_loss=0.646, task_loss=1.802, task_loss_gen=10.223, contrastive_loss=0, total=4170.3, n_correct=2806.72, ppl=4.2, accuracy=67.303, wps=11286.2, ups=1.35, wpb=8340.6, bsz=311.9, num_updates=36700, lr=7.38213e-05, gnorm=0.503, clip=0, loss_scale=64, train_wall=73, gb_free=16.3, wall=31243
2023-09-02 18:33:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-02 18:33:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-02 18:34:13 | INFO | train_inner | epoch 025:   1441 / 1474 loss=1.928, trans_loss=4.865, nll_loss=2.094, w2v_ctc_loss=0.65, task_loss=2.564, task_loss_gen=10.142, contrastive_loss=0, total=4115.59, n_correct=2748.69, ppl=4.27, accuracy=66.787, wps=10959.6, ups=1.33, wpb=8231.2, bsz=304.5, num_updates=36800, lr=7.3721e-05, gnorm=0.531, clip=0, loss_scale=16, train_wall=74, gb_free=16.2, wall=31318
2023-09-02 18:34:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-09-02 18:34:37 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 18:35:10 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 3.882 | trans_loss 5.188 | nll_loss 2.46 | w2v_ctc_loss 1.248 | task_loss 39.768 | task_loss_gen 26.007 | contrastive_loss 0 | total 4003.4 | n_correct 2653.5 | ppl 5.5 | accuracy 66.281 | uer 17.015 | wer 19.004 | raw_wer 19.004 | bleu 21.21 | wps 1559.6 | wpb 4003.4 | bsz 141.8 | num_updates 36832 | best_bleu 21.86
2023-09-02 18:35:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36832 updates
2023-09-02 18:35:10 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_last.pt
2023-09-02 18:35:17 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_last.pt
2023-09-02 18:35:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_last.pt (epoch 25 @ 36832 updates, score 21.21) (writing took 7.102789781987667 seconds)
2023-09-02 18:35:18 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-09-02 18:35:18 | INFO | train | epoch 025 | loss 1.918 | trans_loss 4.848 | nll_loss 2.072 | w2v_ctc_loss 0.645 | task_loss 5.175 | task_loss_gen 8.986 | contrastive_loss 0 | total 4138.32 | n_correct 2781.3 | ppl 4.2 | accuracy 67.208 | wps 10464.3 | ups 1.26 | wpb 8276.6 | bsz 305.6 | num_updates 36832 | lr 7.36889e-05 | gnorm 0.512 | clip 0 | loss_scale 8 | train_wall 1061 | gb_free 13.9 | wall 31383
2023-09-02 18:35:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 18:35:18 | INFO | fairseq.trainer | begin training epoch 26
2023-09-02 18:35:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 18:36:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-09-02 18:36:15 | INFO | train_inner | epoch 026:     69 / 1474 loss=1.922, trans_loss=4.856, nll_loss=2.08, w2v_ctc_loss=0.641, task_loss=17.005, task_loss_gen=11.97, contrastive_loss=0, total=4172.88, n_correct=2790.45, ppl=4.23, accuracy=66.871, wps=6830.8, ups=0.82, wpb=8345.8, bsz=317.5, num_updates=36900, lr=7.3621e-05, gnorm=0.73, clip=0, loss_scale=4, train_wall=73, gb_free=11.5, wall=31440
2023-09-02 18:37:27 | INFO | train_inner | epoch 026:    169 / 1474 loss=1.93, trans_loss=4.895, nll_loss=2.123, w2v_ctc_loss=0.63, task_loss=37.595, task_loss_gen=22.157, contrastive_loss=0, total=4269.48, n_correct=2834.61, ppl=4.36, accuracy=66.392, wps=11766, ups=1.38, wpb=8539, bsz=338.4, num_updates=37000, lr=7.35215e-05, gnorm=1.126, clip=0, loss_scale=4, train_wall=72, gb_free=16.9, wall=31513
2023-09-02 18:38:40 | INFO | train_inner | epoch 026:    269 / 1474 loss=1.945, trans_loss=4.913, nll_loss=2.145, w2v_ctc_loss=0.655, task_loss=40.492, task_loss_gen=23.84, contrastive_loss=0, total=4124.11, n_correct=2723.41, ppl=4.42, accuracy=66.036, wps=11308.8, ups=1.37, wpb=8248.2, bsz=307.4, num_updates=37100, lr=7.34223e-05, gnorm=1.208, clip=0, loss_scale=4, train_wall=72, gb_free=14.6, wall=31586
2023-09-02 18:39:53 | INFO | train_inner | epoch 026:    369 / 1474 loss=1.934, trans_loss=4.901, nll_loss=2.133, w2v_ctc_loss=0.642, task_loss=32.742, task_loss_gen=20.025, contrastive_loss=0, total=4165.17, n_correct=2763.65, ppl=4.39, accuracy=66.351, wps=11481.6, ups=1.38, wpb=8330.3, bsz=314.8, num_updates=37200, lr=7.33236e-05, gnorm=0.982, clip=0, loss_scale=4, train_wall=72, gb_free=15.8, wall=31658
2023-09-02 18:41:05 | INFO | train_inner | epoch 026:    469 / 1474 loss=1.941, trans_loss=4.887, nll_loss=2.118, w2v_ctc_loss=0.655, task_loss=28.967, task_loss_gen=18.202, contrastive_loss=0, total=4159.48, n_correct=2757.93, ppl=4.34, accuracy=66.305, wps=11541.7, ups=1.39, wpb=8319, bsz=313, num_updates=37300, lr=7.32252e-05, gnorm=1.613, clip=0, loss_scale=4, train_wall=71, gb_free=12.8, wall=31730
2023-09-02 18:42:17 | INFO | train_inner | epoch 026:    569 / 1474 loss=1.94, trans_loss=4.884, nll_loss=2.115, w2v_ctc_loss=0.658, task_loss=17.728, task_loss_gen=11.635, contrastive_loss=0, total=4165.04, n_correct=2764.84, ppl=4.33, accuracy=66.382, wps=11519.1, ups=1.38, wpb=8330.1, bsz=305.4, num_updates=37400, lr=7.31272e-05, gnorm=0.969, clip=0, loss_scale=4, train_wall=72, gb_free=15.3, wall=31803
2023-09-02 18:43:30 | INFO | train_inner | epoch 026:    669 / 1474 loss=1.929, trans_loss=4.873, nll_loss=2.101, w2v_ctc_loss=0.644, task_loss=17.851, task_loss_gen=11.778, contrastive_loss=0, total=4129.35, n_correct=2756.34, ppl=4.29, accuracy=66.75, wps=11308, ups=1.37, wpb=8258.7, bsz=296.8, num_updates=37500, lr=7.30297e-05, gnorm=0.743, clip=0, loss_scale=4, train_wall=72, gb_free=17.1, wall=31876
2023-09-02 18:44:42 | INFO | train_inner | epoch 026:    769 / 1474 loss=1.928, trans_loss=4.871, nll_loss=2.099, w2v_ctc_loss=0.644, task_loss=14.089, task_loss_gen=9.62, contrastive_loss=0, total=4094.41, n_correct=2730.46, ppl=4.28, accuracy=66.688, wps=11389.3, ups=1.39, wpb=8188.8, bsz=299.5, num_updates=37600, lr=7.29325e-05, gnorm=0.628, clip=0, loss_scale=4, train_wall=71, gb_free=16, wall=31948
2023-09-02 18:45:55 | INFO | train_inner | epoch 026:    869 / 1474 loss=1.927, trans_loss=4.861, nll_loss=2.087, w2v_ctc_loss=0.652, task_loss=11.063, task_loss_gen=8.027, contrastive_loss=0, total=4174.8, n_correct=2791.09, ppl=4.25, accuracy=66.856, wps=11489.2, ups=1.38, wpb=8349.6, bsz=306.1, num_updates=37700, lr=7.28357e-05, gnorm=0.75, clip=0, loss_scale=4, train_wall=72, gb_free=16.9, wall=32020
2023-09-02 18:47:07 | INFO | train_inner | epoch 026:    969 / 1474 loss=1.919, trans_loss=4.86, nll_loss=2.086, w2v_ctc_loss=0.633, task_loss=10.662, task_loss_gen=7.801, contrastive_loss=0, total=4143.83, n_correct=2776.84, ppl=4.25, accuracy=67.011, wps=11433.7, ups=1.38, wpb=8287.7, bsz=302.3, num_updates=37800, lr=7.27393e-05, gnorm=0.747, clip=0, loss_scale=4, train_wall=72, gb_free=17.1, wall=32093
2023-09-02 18:48:20 | INFO | train_inner | epoch 026:   1069 / 1474 loss=1.924, trans_loss=4.857, nll_loss=2.083, w2v_ctc_loss=0.647, task_loss=8.781, task_loss_gen=6.966, contrastive_loss=0, total=4115.98, n_correct=2763.33, ppl=4.24, accuracy=67.137, wps=11286.8, ups=1.37, wpb=8232, bsz=293.3, num_updates=37900, lr=7.26433e-05, gnorm=0.609, clip=0, loss_scale=4, train_wall=72, gb_free=14.9, wall=32166
2023-09-02 18:49:33 | INFO | train_inner | epoch 026:   1169 / 1474 loss=1.927, trans_loss=4.865, nll_loss=2.093, w2v_ctc_loss=0.65, task_loss=7.771, task_loss_gen=6.576, contrastive_loss=0, total=4120.78, n_correct=2756.95, ppl=4.26, accuracy=66.904, wps=11281.9, ups=1.37, wpb=8241.6, bsz=299.5, num_updates=38000, lr=7.25476e-05, gnorm=0.563, clip=0, loss_scale=4, train_wall=72, gb_free=16.1, wall=32239
2023-09-02 18:49:33 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 18:50:07 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 3.911 | trans_loss 5.19 | nll_loss 2.455 | w2v_ctc_loss 1.342 | task_loss 16.187 | task_loss_gen 16.482 | contrastive_loss 0 | total 4003.4 | n_correct 2656.2 | ppl 5.48 | accuracy 66.349 | uer 17.448 | wer 19.324 | raw_wer 19.324 | bleu 21.62 | wps 1573 | wpb 4003.4 | bsz 141.8 | num_updates 38000 | best_bleu 21.86
2023-09-02 18:50:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38000 updates
2023-09-02 18:50:07 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_26_38000.pt
2023-09-02 18:50:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_26_38000.pt
2023-09-02 18:50:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_26_38000.pt (epoch 26 @ 38000 updates, score 21.62) (writing took 9.151570949004963 seconds)
2023-09-02 18:51:29 | INFO | train_inner | epoch 026:   1269 / 1474 loss=1.934, trans_loss=4.869, nll_loss=2.098, w2v_ctc_loss=0.658, task_loss=7.759, task_loss_gen=6.82, contrastive_loss=0, total=4002.48, n_correct=2674.86, ppl=4.28, accuracy=66.83, wps=6925.4, ups=0.87, wpb=8005, bsz=280, num_updates=38100, lr=7.24524e-05, gnorm=0.553, clip=0, loss_scale=4, train_wall=71, gb_free=15.8, wall=32354
2023-09-02 18:52:43 | INFO | train_inner | epoch 026:   1369 / 1474 loss=1.917, trans_loss=4.858, nll_loss=2.084, w2v_ctc_loss=0.636, task_loss=6.938, task_loss_gen=6.052, contrastive_loss=0, total=4156.47, n_correct=2789.26, ppl=4.24, accuracy=67.106, wps=11270.7, ups=1.36, wpb=8312.9, bsz=312.4, num_updates=38200, lr=7.23575e-05, gnorm=0.551, clip=0, loss_scale=4, train_wall=73, gb_free=16.9, wall=32428
2023-09-02 18:53:55 | INFO | train_inner | epoch 026:   1469 / 1474 loss=1.911, trans_loss=4.85, nll_loss=2.074, w2v_ctc_loss=0.631, task_loss=6.548, task_loss_gen=5.852, contrastive_loss=0, total=4144.2, n_correct=2789.57, ppl=4.21, accuracy=67.313, wps=11454.5, ups=1.38, wpb=8288.4, bsz=312.6, num_updates=38300, lr=7.22629e-05, gnorm=0.544, clip=0, loss_scale=4, train_wall=72, gb_free=16, wall=32500
2023-09-02 18:53:59 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 18:54:31 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 3.907 | trans_loss 5.182 | nll_loss 2.444 | w2v_ctc_loss 1.344 | task_loss 15.628 | task_loss_gen 16.463 | contrastive_loss 0 | total 4003.4 | n_correct 2658.7 | ppl 5.44 | accuracy 66.411 | uer 17.652 | wer 19.451 | raw_wer 19.451 | bleu 21.25 | wps 1649.8 | wpb 4003.4 | bsz 141.8 | num_updates 38305 | best_bleu 21.86
2023-09-02 18:54:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38305 updates
2023-09-02 18:54:31 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_last.pt
2023-09-02 18:54:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_last.pt
2023-09-02 18:54:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_last.pt (epoch 26 @ 38305 updates, score 21.25) (writing took 7.501189271977637 seconds)
2023-09-02 18:54:39 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-09-02 18:54:39 | INFO | train | epoch 026 | loss 1.929 | trans_loss 4.873 | nll_loss 2.101 | w2v_ctc_loss 0.645 | task_loss 18.274 | task_loss_gen 12.084 | contrastive_loss 0 | total 4139.01 | n_correct 2761.84 | ppl 4.29 | accuracy 66.727 | wps 10500.1 | ups 1.27 | wpb 8278 | bsz 305.7 | num_updates 38305 | lr 7.22582e-05 | gnorm 0.825 | clip 0 | loss_scale 4 | train_wall 1058 | gb_free 15.6 | wall 32544
2023-09-02 18:54:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 18:54:39 | INFO | fairseq.trainer | begin training epoch 27
2023-09-02 18:54:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 18:55:55 | INFO | train_inner | epoch 027:     95 / 1474 loss=1.901, trans_loss=4.815, nll_loss=2.027, w2v_ctc_loss=0.625, task_loss=6.864, task_loss_gen=6.291, contrastive_loss=0, total=4072.98, n_correct=2765.57, ppl=4.07, accuracy=67.9, wps=6806.8, ups=0.84, wpb=8146, bsz=284.4, num_updates=38400, lr=7.21688e-05, gnorm=0.55, clip=0, loss_scale=4, train_wall=71, gb_free=16.6, wall=32620
2023-09-02 18:57:08 | INFO | train_inner | epoch 027:    195 / 1474 loss=1.901, trans_loss=4.826, nll_loss=2.042, w2v_ctc_loss=0.633, task_loss=6.017, task_loss_gen=5.543, contrastive_loss=0, total=4189.84, n_correct=2839.88, ppl=4.12, accuracy=67.78, wps=11520.4, ups=1.37, wpb=8379.7, bsz=323.5, num_updates=38500, lr=7.2075e-05, gnorm=0.538, clip=0, loss_scale=4, train_wall=72, gb_free=16.5, wall=32693
2023-09-02 18:58:21 | INFO | train_inner | epoch 027:    295 / 1474 loss=1.913, trans_loss=4.835, nll_loss=2.054, w2v_ctc_loss=0.645, task_loss=6.142, task_loss_gen=5.778, contrastive_loss=0, total=4168.47, n_correct=2818.01, ppl=4.15, accuracy=67.603, wps=11417.2, ups=1.37, wpb=8336.9, bsz=306.2, num_updates=38600, lr=7.19816e-05, gnorm=0.537, clip=0, loss_scale=4, train_wall=72, gb_free=17, wall=32766
2023-09-02 18:59:35 | INFO | train_inner | epoch 027:    395 / 1474 loss=1.912, trans_loss=4.838, nll_loss=2.058, w2v_ctc_loss=0.636, task_loss=6.627, task_loss_gen=6.167, contrastive_loss=0, total=4082.79, n_correct=2753.34, ppl=4.17, accuracy=67.438, wps=11027.4, ups=1.35, wpb=8165.6, bsz=299.1, num_updates=38700, lr=7.18885e-05, gnorm=0.57, clip=0, loss_scale=4, train_wall=73, gb_free=16.4, wall=32840
2023-09-02 19:00:48 | INFO | train_inner | epoch 027:    495 / 1474 loss=1.909, trans_loss=4.842, nll_loss=2.064, w2v_ctc_loss=0.636, task_loss=5.659, task_loss_gen=5.401, contrastive_loss=0, total=4236.86, n_correct=2854.63, ppl=4.18, accuracy=67.376, wps=11631.6, ups=1.37, wpb=8473.7, bsz=329, num_updates=38800, lr=7.17958e-05, gnorm=0.539, clip=0, loss_scale=4, train_wall=72, gb_free=16.7, wall=32913
2023-09-02 19:02:00 | INFO | train_inner | epoch 027:    595 / 1474 loss=1.907, trans_loss=4.83, nll_loss=2.049, w2v_ctc_loss=0.639, task_loss=5.938, task_loss_gen=5.698, contrastive_loss=0, total=4135.74, n_correct=2793.96, ppl=4.14, accuracy=67.556, wps=11463.9, ups=1.39, wpb=8271.5, bsz=312, num_updates=38900, lr=7.17035e-05, gnorm=0.533, clip=0, loss_scale=4, train_wall=71, gb_free=14.6, wall=32985
2023-09-02 19:03:12 | INFO | train_inner | epoch 027:    695 / 1474 loss=1.91, trans_loss=4.835, nll_loss=2.054, w2v_ctc_loss=0.64, task_loss=5.841, task_loss_gen=5.778, contrastive_loss=0, total=4170.03, n_correct=2814.23, ppl=4.15, accuracy=67.487, wps=11493, ups=1.38, wpb=8340.1, bsz=306.8, num_updates=39000, lr=7.16115e-05, gnorm=0.518, clip=0, loss_scale=8, train_wall=72, gb_free=17.5, wall=33057
2023-09-02 19:04:24 | INFO | train_inner | epoch 027:    795 / 1474 loss=1.913, trans_loss=4.834, nll_loss=2.053, w2v_ctc_loss=0.644, task_loss=5.836, task_loss_gen=6.053, contrastive_loss=0, total=4105.12, n_correct=2772.33, ppl=4.15, accuracy=67.533, wps=11420, ups=1.39, wpb=8210.2, bsz=293.7, num_updates=39100, lr=7.15199e-05, gnorm=0.522, clip=0, loss_scale=8, train_wall=71, gb_free=17.2, wall=33129
2023-09-02 19:05:37 | INFO | train_inner | epoch 027:    895 / 1474 loss=1.909, trans_loss=4.836, nll_loss=2.056, w2v_ctc_loss=0.632, task_loss=5.439, task_loss_gen=5.996, contrastive_loss=0, total=4105.85, n_correct=2773.65, ppl=4.16, accuracy=67.554, wps=11342.6, ups=1.38, wpb=8211.7, bsz=293.4, num_updates=39200, lr=7.14286e-05, gnorm=0.51, clip=0, loss_scale=8, train_wall=72, gb_free=16.3, wall=33202
2023-09-02 19:06:50 | INFO | train_inner | epoch 027:    995 / 1474 loss=1.907, trans_loss=4.833, nll_loss=2.052, w2v_ctc_loss=0.633, task_loss=5.188, task_loss_gen=5.629, contrastive_loss=0, total=4181.76, n_correct=2825.48, ppl=4.15, accuracy=67.567, wps=11363, ups=1.36, wpb=8363.5, bsz=313.7, num_updates=39300, lr=7.13376e-05, gnorm=0.512, clip=0, loss_scale=8, train_wall=73, gb_free=16.5, wall=33275
2023-09-02 19:08:03 | INFO | train_inner | epoch 027:   1095 / 1474 loss=1.906, trans_loss=4.83, nll_loss=2.049, w2v_ctc_loss=0.633, task_loss=5.311, task_loss_gen=5.753, contrastive_loss=0, total=4159.93, n_correct=2814.33, ppl=4.14, accuracy=67.653, wps=11484.6, ups=1.38, wpb=8319.9, bsz=307.8, num_updates=39400, lr=7.1247e-05, gnorm=0.519, clip=0, loss_scale=8, train_wall=72, gb_free=15.4, wall=33348
2023-09-02 19:09:15 | INFO | train_inner | epoch 027:   1195 / 1474 loss=1.912, trans_loss=4.835, nll_loss=2.054, w2v_ctc_loss=0.642, task_loss=6.57, task_loss_gen=6.363, contrastive_loss=0, total=4096.3, n_correct=2761.66, ppl=4.15, accuracy=67.418, wps=11256.5, ups=1.37, wpb=8192.6, bsz=294.1, num_updates=39500, lr=7.11568e-05, gnorm=0.541, clip=0, loss_scale=8, train_wall=72, gb_free=12, wall=33421
2023-09-02 19:10:27 | INFO | train_inner | epoch 027:   1295 / 1474 loss=1.912, trans_loss=4.837, nll_loss=2.058, w2v_ctc_loss=0.637, task_loss=6.437, task_loss_gen=6.256, contrastive_loss=0, total=4065.06, n_correct=2742.03, ppl=4.16, accuracy=67.454, wps=11319.2, ups=1.39, wpb=8130.1, bsz=294.3, num_updates=39600, lr=7.10669e-05, gnorm=0.534, clip=0, loss_scale=8, train_wall=71, gb_free=16.1, wall=33492
2023-09-02 19:11:39 | INFO | train_inner | epoch 027:   1395 / 1474 loss=1.905, trans_loss=4.834, nll_loss=2.054, w2v_ctc_loss=0.633, task_loss=5.581, task_loss_gen=5.547, contrastive_loss=0, total=4151.71, n_correct=2807.04, ppl=4.15, accuracy=67.612, wps=11582.5, ups=1.39, wpb=8303.4, bsz=312.9, num_updates=39700, lr=7.09773e-05, gnorm=0.513, clip=0, loss_scale=8, train_wall=71, gb_free=17.5, wall=33564
2023-09-02 19:12:36 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 19:13:09 | INFO | dev_st | epoch 027 | valid on 'dev_st' subset | loss 3.898 | trans_loss 5.175 | nll_loss 2.438 | w2v_ctc_loss 1.33 | task_loss 12.586 | task_loss_gen 16.8 | contrastive_loss 0 | total 4003.4 | n_correct 2666.6 | ppl 5.42 | accuracy 66.608 | uer 17.625 | wer 19.529 | raw_wer 19.529 | bleu 22 | wps 1659.2 | wpb 4003.4 | bsz 141.8 | num_updates 39779 | best_bleu 22
2023-09-02 19:13:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 39779 updates
2023-09-02 19:13:09 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 19:13:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 19:13:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 27 @ 39779 updates, score 22.0) (writing took 13.247781436017249 seconds)
2023-09-02 19:13:22 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-09-02 19:13:22 | INFO | train | epoch 027 | loss 1.908 | trans_loss 4.833 | nll_loss 2.052 | w2v_ctc_loss 0.636 | task_loss 5.912 | task_loss_gen 5.84 | contrastive_loss 0 | total 4138.65 | n_correct 2796.61 | ppl 4.15 | accuracy 67.573 | wps 10858.8 | ups 1.31 | wpb 8277.3 | bsz 305.7 | num_updates 39779 | lr 7.09068e-05 | gnorm 0.53 | clip 0 | loss_scale 8 | train_wall 1058 | gb_free 17.5 | wall 33668
2023-09-02 19:13:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 19:13:23 | INFO | fairseq.trainer | begin training epoch 28
2023-09-02 19:13:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 19:13:45 | INFO | train_inner | epoch 028:     21 / 1474 loss=1.899, trans_loss=4.827, nll_loss=2.045, w2v_ctc_loss=0.624, task_loss=5.439, task_loss_gen=5.56, contrastive_loss=0, total=4120.02, n_correct=2792.47, ppl=4.13, accuracy=67.778, wps=6550.9, ups=0.8, wpb=8240, bsz=307.6, num_updates=39800, lr=7.08881e-05, gnorm=0.52, clip=0, loss_scale=8, train_wall=71, gb_free=15.2, wall=33690
2023-09-02 19:14:57 | INFO | train_inner | epoch 028:    121 / 1474 loss=1.896, trans_loss=4.804, nll_loss=2.014, w2v_ctc_loss=0.627, task_loss=6.285, task_loss_gen=6.134, contrastive_loss=0, total=4107.51, n_correct=2799.04, ppl=4.04, accuracy=68.144, wps=11397.7, ups=1.39, wpb=8215, bsz=291.1, num_updates=39900, lr=7.07992e-05, gnorm=0.513, clip=0, loss_scale=8, train_wall=71, gb_free=16.2, wall=33762
2023-09-02 19:16:09 | INFO | train_inner | epoch 028:    221 / 1474 loss=1.892, trans_loss=4.813, nll_loss=2.027, w2v_ctc_loss=0.619, task_loss=4.893, task_loss_gen=5.58, contrastive_loss=0, total=4193.44, n_correct=2854.06, ppl=4.07, accuracy=68.06, wps=11631.3, ups=1.39, wpb=8386.9, bsz=316.2, num_updates=40000, lr=7.07107e-05, gnorm=0.508, clip=0, loss_scale=8, train_wall=71, gb_free=16.7, wall=33834
2023-09-02 19:16:09 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 19:16:42 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 3.892 | trans_loss 5.178 | nll_loss 2.439 | w2v_ctc_loss 1.305 | task_loss 8.795 | task_loss_gen 18.179 | contrastive_loss 0 | total 4003.4 | n_correct 2669.1 | ppl 5.42 | accuracy 66.671 | uer 17.28 | wer 19.25 | raw_wer 19.25 | bleu 22.14 | wps 1642.6 | wpb 4003.4 | bsz 141.8 | num_updates 40000 | best_bleu 22.14
2023-09-02 19:16:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 40000 updates
2023-09-02 19:16:42 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_28_40000.pt
2023-09-02 19:16:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_28_40000.pt
2023-09-02 19:16:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_28_40000.pt (epoch 28 @ 40000 updates, score 22.14) (writing took 14.623945067985915 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:0')
2023-09-02 19:18:10 | INFO | train_inner | epoch 028:    321 / 1474 loss=1.899, trans_loss=4.821, nll_loss=2.037, w2v_ctc_loss=0.62, task_loss=4.838, task_loss_gen=6.037, contrastive_loss=0, total=4131, n_correct=2795.31, ppl=4.1, accuracy=67.667, wps=6845.3, ups=0.83, wpb=8262, bsz=313.5, num_updates=40100, lr=7.06225e-05, gnorm=0.533, clip=0, loss_scale=8, train_wall=72, gb_free=16.4, wall=33955
2023-09-02 19:19:22 | INFO | train_inner | epoch 028:    421 / 1474 loss=1.901, trans_loss=4.814, nll_loss=2.027, w2v_ctc_loss=0.634, task_loss=5.577, task_loss_gen=6.001, contrastive_loss=0, total=4096.12, n_correct=2783.25, ppl=4.08, accuracy=67.948, wps=11303.8, ups=1.38, wpb=8192.2, bsz=296.6, num_updates=40200, lr=7.05346e-05, gnorm=0.533, clip=0, loss_scale=8, train_wall=72, gb_free=17.3, wall=34027
2023-09-02 19:20:35 | INFO | train_inner | epoch 028:    521 / 1474 loss=1.899, trans_loss=4.817, nll_loss=2.032, w2v_ctc_loss=0.626, task_loss=5.137, task_loss_gen=6.123, contrastive_loss=0, total=4109.15, n_correct=2788.11, ppl=4.09, accuracy=67.851, wps=11307.9, ups=1.38, wpb=8218.3, bsz=298.6, num_updates=40300, lr=7.0447e-05, gnorm=0.515, clip=0, loss_scale=8, train_wall=72, gb_free=14.7, wall=34100
2023-09-02 19:21:47 | INFO | train_inner | epoch 028:    621 / 1474 loss=1.9, trans_loss=4.824, nll_loss=2.04, w2v_ctc_loss=0.627, task_loss=4.612, task_loss_gen=6.135, contrastive_loss=0, total=4178.56, n_correct=2832.84, ppl=4.11, accuracy=67.795, wps=11628.6, ups=1.39, wpb=8357.1, bsz=304.4, num_updates=40400, lr=7.03598e-05, gnorm=0.512, clip=0, loss_scale=8, train_wall=71, gb_free=15.8, wall=34172
2023-09-02 19:22:59 | INFO | train_inner | epoch 028:    721 / 1474 loss=1.897, trans_loss=4.825, nll_loss=2.043, w2v_ctc_loss=0.624, task_loss=3.9, task_loss_gen=5.462, contrastive_loss=0, total=4184.74, n_correct=2839.5, ppl=4.12, accuracy=67.854, wps=11521.8, ups=1.38, wpb=8369.5, bsz=327.2, num_updates=40500, lr=7.02728e-05, gnorm=0.516, clip=0, loss_scale=8, train_wall=72, gb_free=16.3, wall=34244
2023-09-02 19:24:11 | INFO | train_inner | epoch 028:    821 / 1474 loss=1.895, trans_loss=4.816, nll_loss=2.03, w2v_ctc_loss=0.623, task_loss=4.241, task_loss_gen=5.92, contrastive_loss=0, total=4087.21, n_correct=2780.24, ppl=4.08, accuracy=68.023, wps=11415.1, ups=1.4, wpb=8174.4, bsz=304.8, num_updates=40600, lr=7.01862e-05, gnorm=0.517, clip=0, loss_scale=8, train_wall=71, gb_free=16.7, wall=34316
2023-09-02 19:25:25 | INFO | train_inner | epoch 028:    921 / 1474 loss=1.905, trans_loss=4.825, nll_loss=2.042, w2v_ctc_loss=0.634, task_loss=4.548, task_loss_gen=6.281, contrastive_loss=0, total=4119.18, n_correct=2787.9, ppl=4.12, accuracy=67.681, wps=11174.2, ups=1.36, wpb=8238.4, bsz=299.7, num_updates=40700, lr=7.01e-05, gnorm=0.516, clip=0, loss_scale=8, train_wall=73, gb_free=16.6, wall=34390
2023-09-02 19:26:37 | INFO | train_inner | epoch 028:   1021 / 1474 loss=1.904, trans_loss=4.823, nll_loss=2.04, w2v_ctc_loss=0.636, task_loss=4.416, task_loss_gen=5.893, contrastive_loss=0, total=4174.98, n_correct=2828.74, ppl=4.11, accuracy=67.755, wps=11463, ups=1.37, wpb=8350, bsz=310.9, num_updates=40800, lr=7.0014e-05, gnorm=0.512, clip=0, loss_scale=8, train_wall=72, gb_free=16.7, wall=34463
2023-09-02 19:27:50 | INFO | train_inner | epoch 028:   1121 / 1474 loss=1.896, trans_loss=4.817, nll_loss=2.032, w2v_ctc_loss=0.627, task_loss=4.295, task_loss_gen=5.739, contrastive_loss=0, total=4222.19, n_correct=2864.14, ppl=4.09, accuracy=67.835, wps=11613.5, ups=1.38, wpb=8444.4, bsz=321.5, num_updates=40900, lr=6.99284e-05, gnorm=0.504, clip=0, loss_scale=8, train_wall=72, gb_free=17.5, wall=34535
2023-09-02 19:29:02 | INFO | train_inner | epoch 028:   1221 / 1474 loss=1.895, trans_loss=4.82, nll_loss=2.036, w2v_ctc_loss=0.616, task_loss=4.167, task_loss_gen=5.941, contrastive_loss=0, total=4104.72, n_correct=2790.13, ppl=4.1, accuracy=67.974, wps=11356.9, ups=1.38, wpb=8209.4, bsz=305.5, num_updates=41000, lr=6.9843e-05, gnorm=0.516, clip=0, loss_scale=16, train_wall=72, gb_free=15.3, wall=34608
2023-09-02 19:30:16 | INFO | train_inner | epoch 028:   1321 / 1474 loss=1.91, trans_loss=4.825, nll_loss=2.042, w2v_ctc_loss=0.638, task_loss=5.477, task_loss_gen=7.015, contrastive_loss=0, total=4074.43, n_correct=2756.34, ppl=4.12, accuracy=67.65, wps=11149.2, ups=1.37, wpb=8148.9, bsz=282.9, num_updates=41100, lr=6.9758e-05, gnorm=0.52, clip=0, loss_scale=16, train_wall=72, gb_free=17.5, wall=34681
2023-09-02 19:31:29 | INFO | train_inner | epoch 028:   1421 / 1474 loss=1.901, trans_loss=4.82, nll_loss=2.035, w2v_ctc_loss=0.627, task_loss=6.849, task_loss_gen=6.922, contrastive_loss=0, total=4156.52, n_correct=2816.55, ppl=4.1, accuracy=67.762, wps=11397.2, ups=1.37, wpb=8313, bsz=300.8, num_updates=41200, lr=6.96733e-05, gnorm=0.533, clip=0, loss_scale=16, train_wall=72, gb_free=17.4, wall=34754
2023-09-02 19:32:07 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:2')
2023-09-02 19:32:40 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 3.884 | trans_loss 5.17 | nll_loss 2.431 | w2v_ctc_loss 1.295 | task_loss 10.677 | task_loss_gen 17.407 | contrastive_loss 0 | total 4003.4 | n_correct 2671.1 | ppl 5.39 | accuracy 66.721 | uer 17.339 | wer 19.351 | raw_wer 19.351 | bleu 22.21 | wps 1629.7 | wpb 4003.4 | bsz 141.8 | num_updates 41253 | best_bleu 22.21
2023-09-02 19:32:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 41253 updates
2023-09-02 19:32:40 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 19:32:47 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 19:32:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 28 @ 41253 updates, score 22.21) (writing took 14.313177420001011 seconds)
2023-09-02 19:32:54 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-09-02 19:32:54 | INFO | train | epoch 028 | loss 1.899 | trans_loss 4.819 | nll_loss 2.034 | w2v_ctc_loss 0.627 | task_loss 4.959 | task_loss_gen 6.054 | contrastive_loss 0 | total 4138.65 | n_correct 2808.89 | ppl 4.09 | accuracy 67.87 | wps 10410.6 | ups 1.26 | wpb 8277.3 | bsz 305.7 | num_updates 41253 | lr 6.96285e-05 | gnorm 0.518 | clip 0 | loss_scale 16 | train_wall 1058 | gb_free 16 | wall 34840
2023-09-02 19:32:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 19:32:55 | INFO | fairseq.trainer | begin training epoch 29
2023-09-02 19:32:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 19:33:36 | INFO | train_inner | epoch 029:     47 / 1474 loss=1.894, trans_loss=4.809, nll_loss=2.022, w2v_ctc_loss=0.628, task_loss=5.706, task_loss_gen=5.868, contrastive_loss=0, total=4169.02, n_correct=2839.3, ppl=4.06, accuracy=68.105, wps=6560.8, ups=0.79, wpb=8338, bsz=315.1, num_updates=41300, lr=6.95889e-05, gnorm=0.51, clip=0, loss_scale=16, train_wall=72, gb_free=15.6, wall=34881
2023-09-02 19:34:48 | INFO | train_inner | epoch 029:    147 / 1474 loss=1.893, trans_loss=4.807, nll_loss=2.019, w2v_ctc_loss=0.624, task_loss=5.376, task_loss_gen=5.852, contrastive_loss=0, total=4110.03, n_correct=2799.65, ppl=4.05, accuracy=68.118, wps=11326.9, ups=1.38, wpb=8220.1, bsz=305.5, num_updates=41400, lr=6.95048e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=72, gb_free=16.2, wall=34953
2023-09-02 19:36:01 | INFO | train_inner | epoch 029:    247 / 1474 loss=1.881, trans_loss=4.799, nll_loss=2.009, w2v_ctc_loss=0.607, task_loss=4.663, task_loss_gen=5.269, contrastive_loss=0, total=4197.89, n_correct=2867.13, ppl=4.03, accuracy=68.299, wps=11493.7, ups=1.37, wpb=8395.8, bsz=329.5, num_updates=41500, lr=6.9421e-05, gnorm=0.501, clip=0, loss_scale=16, train_wall=72, gb_free=17.2, wall=35026
2023-09-02 19:37:14 | INFO | train_inner | epoch 029:    347 / 1474 loss=1.905, trans_loss=4.818, nll_loss=2.033, w2v_ctc_loss=0.638, task_loss=5.046, task_loss_gen=6.388, contrastive_loss=0, total=4094.4, n_correct=2780.51, ppl=4.09, accuracy=67.91, wps=11248.2, ups=1.37, wpb=8188.8, bsz=291.2, num_updates=41600, lr=6.93375e-05, gnorm=0.517, clip=0, loss_scale=16, train_wall=72, gb_free=16.1, wall=35099
2023-09-02 19:38:27 | INFO | train_inner | epoch 029:    447 / 1474 loss=1.884, trans_loss=4.79, nll_loss=1.996, w2v_ctc_loss=0.616, task_loss=4.692, task_loss_gen=5.867, contrastive_loss=0, total=4157.41, n_correct=2844.28, ppl=3.99, accuracy=68.415, wps=11466.2, ups=1.38, wpb=8314.8, bsz=308.5, num_updates=41700, lr=6.92543e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=72, gb_free=14.3, wall=35172
2023-09-02 19:39:40 | INFO | train_inner | epoch 029:    547 / 1474 loss=1.904, trans_loss=4.818, nll_loss=2.032, w2v_ctc_loss=0.631, task_loss=5.546, task_loss_gen=6.318, contrastive_loss=0, total=4149.27, n_correct=2812.53, ppl=4.09, accuracy=67.784, wps=11323.2, ups=1.36, wpb=8298.5, bsz=293.3, num_updates=41800, lr=6.91714e-05, gnorm=0.519, clip=0, loss_scale=16, train_wall=73, gb_free=15.1, wall=35245
2023-09-02 19:40:52 | INFO | train_inner | epoch 029:    647 / 1474 loss=1.888, trans_loss=4.804, nll_loss=2.015, w2v_ctc_loss=0.618, task_loss=5.583, task_loss_gen=6.009, contrastive_loss=0, total=4145.39, n_correct=2829.69, ppl=4.04, accuracy=68.261, wps=11425.4, ups=1.38, wpb=8290.8, bsz=319.3, num_updates=41900, lr=6.90889e-05, gnorm=0.541, clip=0, loss_scale=16, train_wall=72, gb_free=17.1, wall=35318
2023-09-02 19:42:06 | INFO | train_inner | epoch 029:    747 / 1474 loss=1.887, trans_loss=4.805, nll_loss=2.016, w2v_ctc_loss=0.618, task_loss=5.606, task_loss_gen=5.848, contrastive_loss=0, total=4242.46, n_correct=2891.86, ppl=4.05, accuracy=68.165, wps=11613.9, ups=1.37, wpb=8484.9, bsz=329.9, num_updates=42000, lr=6.90066e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=72, gb_free=16.2, wall=35391
2023-09-02 19:42:06 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 19:42:38 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 3.881 | trans_loss 5.168 | nll_loss 2.427 | w2v_ctc_loss 1.29 | task_loss 10.644 | task_loss_gen 17.429 | contrastive_loss 0 | total 4003.4 | n_correct 2663.7 | ppl 5.38 | accuracy 66.536 | uer 17.094 | wer 18.851 | raw_wer 18.851 | bleu 21.63 | wps 1649.8 | wpb 4003.4 | bsz 141.8 | num_updates 42000 | best_bleu 22.21
2023-09-02 19:42:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42000 updates
2023-09-02 19:42:38 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_29_42000.pt
2023-09-02 19:42:41 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_29_42000.pt
2023-09-02 19:42:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_29_42000.pt (epoch 29 @ 42000 updates, score 21.63) (writing took 10.27106872398872 seconds)
2023-09-02 19:44:01 | INFO | train_inner | epoch 029:    847 / 1474 loss=1.898, trans_loss=4.817, nll_loss=2.031, w2v_ctc_loss=0.619, task_loss=6.549, task_loss_gen=6.813, contrastive_loss=0, total=4027.03, n_correct=2733.46, ppl=4.09, accuracy=67.878, wps=6985.9, ups=0.87, wpb=8054.1, bsz=280.3, num_updates=42100, lr=6.89246e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=71, gb_free=17, wall=35506
2023-09-02 19:45:13 | INFO | train_inner | epoch 029:    947 / 1474 loss=1.898, trans_loss=4.814, nll_loss=2.027, w2v_ctc_loss=0.628, task_loss=5.472, task_loss_gen=6, contrastive_loss=0, total=4086.72, n_correct=2780.96, ppl=4.08, accuracy=68.049, wps=11334.5, ups=1.39, wpb=8173.4, bsz=296.3, num_updates=42200, lr=6.88428e-05, gnorm=0.515, clip=0, loss_scale=16, train_wall=71, gb_free=15, wall=35578
2023-09-02 19:46:26 | INFO | train_inner | epoch 029:   1047 / 1474 loss=1.89, trans_loss=4.805, nll_loss=2.017, w2v_ctc_loss=0.617, task_loss=4.805, task_loss_gen=5.992, contrastive_loss=0, total=4139.4, n_correct=2822.85, ppl=4.05, accuracy=68.195, wps=11389.8, ups=1.38, wpb=8278.8, bsz=307.4, num_updates=42300, lr=6.87614e-05, gnorm=0.511, clip=0, loss_scale=16, train_wall=72, gb_free=15.2, wall=35651
2023-09-02 19:47:38 | INFO | train_inner | epoch 029:   1147 / 1474 loss=1.904, trans_loss=4.821, nll_loss=2.037, w2v_ctc_loss=0.633, task_loss=5.337, task_loss_gen=6.597, contrastive_loss=0, total=4072.33, n_correct=2763.34, ppl=4.1, accuracy=67.856, wps=11266.3, ups=1.38, wpb=8144.7, bsz=284.1, num_updates=42400, lr=6.86803e-05, gnorm=0.537, clip=0, loss_scale=16, train_wall=71, gb_free=16.5, wall=35723
2023-09-02 19:48:51 | INFO | train_inner | epoch 029:   1247 / 1474 loss=1.901, trans_loss=4.819, nll_loss=2.034, w2v_ctc_loss=0.631, task_loss=4.607, task_loss_gen=6.304, contrastive_loss=0, total=4160.52, n_correct=2825.63, ppl=4.1, accuracy=67.915, wps=11451.1, ups=1.38, wpb=8321, bsz=301.5, num_updates=42500, lr=6.85994e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=72, gb_free=15.2, wall=35796
2023-09-02 19:50:03 | INFO | train_inner | epoch 029:   1347 / 1474 loss=1.892, trans_loss=4.806, nll_loss=2.018, w2v_ctc_loss=0.62, task_loss=4.715, task_loss_gen=6.238, contrastive_loss=0, total=4168.02, n_correct=2836.55, ppl=4.05, accuracy=68.055, wps=11453.8, ups=1.37, wpb=8336, bsz=310.2, num_updates=42600, lr=6.85189e-05, gnorm=0.522, clip=0, loss_scale=16, train_wall=72, gb_free=16.3, wall=35869
2023-09-02 19:51:15 | INFO | train_inner | epoch 029:   1447 / 1474 loss=1.89, trans_loss=4.805, nll_loss=2.017, w2v_ctc_loss=0.619, task_loss=4.529, task_loss_gen=6.085, contrastive_loss=0, total=4166.06, n_correct=2837.39, ppl=4.05, accuracy=68.107, wps=11553.1, ups=1.39, wpb=8332.1, bsz=313.1, num_updates=42700, lr=6.84386e-05, gnorm=0.519, clip=0, loss_scale=16, train_wall=71, gb_free=16.5, wall=35941
2023-09-02 19:51:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 19:52:08 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 3.888 | trans_loss 5.166 | nll_loss 2.426 | w2v_ctc_loss 1.317 | task_loss 10.369 | task_loss_gen 17.611 | contrastive_loss 0 | total 4003.4 | n_correct 2672.6 | ppl 5.37 | accuracy 66.758 | uer 16.951 | wer 18.825 | raw_wer 18.825 | bleu 21.92 | wps 1612.4 | wpb 4003.4 | bsz 141.8 | num_updates 42727 | best_bleu 22.21
2023-09-02 19:52:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42727 updates
2023-09-02 19:52:08 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.9202.pt
2023-09-02 19:52:11 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.9202.pt
2023-09-02 19:52:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.9202.pt (epoch 29 @ 42727 updates, score 21.92) (writing took 7.8167354019824415 seconds)
2023-09-02 19:52:17 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-09-02 19:52:17 | INFO | train | epoch 029 | loss 1.894 | trans_loss 4.809 | nll_loss 2.021 | w2v_ctc_loss 0.623 | task_loss 5.168 | task_loss_gen 6.08 | contrastive_loss 0 | total 4138.65 | n_correct 2817.45 | ppl 4.06 | accuracy 68.077 | wps 10497.5 | ups 1.27 | wpb 8277.3 | bsz 305.7 | num_updates 42727 | lr 6.8417e-05 | gnorm 0.521 | clip 0 | loss_scale 16 | train_wall 1059 | gb_free 15.8 | wall 36002
2023-09-02 19:52:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 19:52:17 | INFO | fairseq.trainer | begin training epoch 30
2023-09-02 19:52:17 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 19:53:18 | INFO | train_inner | epoch 030:     73 / 1474 loss=1.883, trans_loss=4.798, nll_loss=2.007, w2v_ctc_loss=0.608, task_loss=3.968, task_loss_gen=5.845, contrastive_loss=0, total=4175.11, n_correct=2852.7, ppl=4.02, accuracy=68.326, wps=6801.2, ups=0.81, wpb=8350.2, bsz=318.6, num_updates=42800, lr=6.83586e-05, gnorm=0.516, clip=0, loss_scale=16, train_wall=72, gb_free=16.8, wall=36063
2023-09-02 19:54:31 | INFO | train_inner | epoch 030:    173 / 1474 loss=1.881, trans_loss=4.784, nll_loss=1.989, w2v_ctc_loss=0.616, task_loss=3.984, task_loss_gen=5.698, contrastive_loss=0, total=4202.64, n_correct=2881.03, ppl=3.97, accuracy=68.553, wps=11601.3, ups=1.38, wpb=8405.3, bsz=318.3, num_updates=42900, lr=6.82789e-05, gnorm=0.511, clip=0, loss_scale=16, train_wall=72, gb_free=16.4, wall=36136
2023-09-02 19:55:43 | INFO | train_inner | epoch 030:    273 / 1474 loss=1.892, trans_loss=4.801, nll_loss=2.011, w2v_ctc_loss=0.624, task_loss=4.506, task_loss_gen=6.318, contrastive_loss=0, total=4120.21, n_correct=2811.7, ppl=4.03, accuracy=68.242, wps=11391.8, ups=1.38, wpb=8240.4, bsz=294.9, num_updates=43000, lr=6.81994e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=72, gb_free=14.9, wall=36208
2023-09-02 19:56:56 | INFO | train_inner | epoch 030:    373 / 1474 loss=1.881, trans_loss=4.789, nll_loss=1.995, w2v_ctc_loss=0.611, task_loss=4.515, task_loss_gen=6.144, contrastive_loss=0, total=4178.23, n_correct=2862.81, ppl=3.99, accuracy=68.517, wps=11462.5, ups=1.37, wpb=8356.5, bsz=307.5, num_updates=43100, lr=6.81203e-05, gnorm=0.507, clip=0, loss_scale=32, train_wall=72, gb_free=9.6, wall=36281
2023-09-02 19:58:07 | INFO | train_inner | epoch 030:    473 / 1474 loss=1.883, trans_loss=4.798, nll_loss=2.007, w2v_ctc_loss=0.61, task_loss=4.427, task_loss_gen=5.942, contrastive_loss=0, total=4124.47, n_correct=2818.77, ppl=4.02, accuracy=68.343, wps=11540.7, ups=1.4, wpb=8248.9, bsz=312.6, num_updates=43200, lr=6.80414e-05, gnorm=0.513, clip=0, loss_scale=32, train_wall=71, gb_free=17.2, wall=36353
2023-09-02 19:59:20 | INFO | train_inner | epoch 030:    573 / 1474 loss=1.886, trans_loss=4.798, nll_loss=2.007, w2v_ctc_loss=0.617, task_loss=4.19, task_loss_gen=6.088, contrastive_loss=0, total=4168.41, n_correct=2849.23, ppl=4.02, accuracy=68.353, wps=11471.1, ups=1.38, wpb=8336.8, bsz=312.4, num_updates=43300, lr=6.79628e-05, gnorm=0.509, clip=0, loss_scale=32, train_wall=72, gb_free=16.9, wall=36425
2023-09-02 20:00:33 | INFO | train_inner | epoch 030:    673 / 1474 loss=1.888, trans_loss=4.798, nll_loss=2.007, w2v_ctc_loss=0.621, task_loss=3.892, task_loss_gen=6.205, contrastive_loss=0, total=4187.95, n_correct=2856.26, ppl=4.02, accuracy=68.202, wps=11428, ups=1.36, wpb=8375.9, bsz=315, num_updates=43400, lr=6.78844e-05, gnorm=0.51, clip=0, loss_scale=32, train_wall=73, gb_free=15.4, wall=36499
2023-09-02 20:01:46 | INFO | train_inner | epoch 030:    773 / 1474 loss=1.896, trans_loss=4.806, nll_loss=2.018, w2v_ctc_loss=0.63, task_loss=3.782, task_loss_gen=6.587, contrastive_loss=0, total=4105.32, n_correct=2793.08, ppl=4.05, accuracy=68.036, wps=11299, ups=1.38, wpb=8210.6, bsz=302.6, num_updates=43500, lr=6.78064e-05, gnorm=0.515, clip=0, loss_scale=32, train_wall=72, gb_free=12.6, wall=36571
2023-09-02 20:02:59 | INFO | train_inner | epoch 030:    873 / 1474 loss=1.894, trans_loss=4.805, nll_loss=2.016, w2v_ctc_loss=0.623, task_loss=3.573, task_loss_gen=6.836, contrastive_loss=0, total=4102.11, n_correct=2794.72, ppl=4.04, accuracy=68.129, wps=11254.9, ups=1.37, wpb=8204.2, bsz=295.6, num_updates=43600, lr=6.77285e-05, gnorm=0.519, clip=0, loss_scale=32, train_wall=72, gb_free=17, wall=36644
2023-09-02 20:04:11 | INFO | train_inner | epoch 030:    973 / 1474 loss=1.894, trans_loss=4.805, nll_loss=2.016, w2v_ctc_loss=0.625, task_loss=3.969, task_loss_gen=6.602, contrastive_loss=0, total=4129.98, n_correct=2815.58, ppl=4.04, accuracy=68.174, wps=11432.8, ups=1.38, wpb=8260, bsz=300.4, num_updates=43700, lr=6.7651e-05, gnorm=0.508, clip=0, loss_scale=32, train_wall=72, gb_free=16.1, wall=36716
2023-09-02 20:05:25 | INFO | train_inner | epoch 030:   1073 / 1474 loss=1.895, trans_loss=4.805, nll_loss=2.016, w2v_ctc_loss=0.62, task_loss=4.497, task_loss_gen=7.333, contrastive_loss=0, total=4101.17, n_correct=2793.77, ppl=4.04, accuracy=68.121, wps=11115.6, ups=1.36, wpb=8202.3, bsz=282.3, num_updates=43800, lr=6.75737e-05, gnorm=0.519, clip=0, loss_scale=32, train_wall=73, gb_free=15.4, wall=36790
2023-09-02 20:06:38 | INFO | train_inner | epoch 030:   1173 / 1474 loss=1.881, trans_loss=4.797, nll_loss=2.006, w2v_ctc_loss=0.606, task_loss=3.825, task_loss_gen=6.168, contrastive_loss=0, total=4168.36, n_correct=2851.01, ppl=4.02, accuracy=68.396, wps=11472.7, ups=1.38, wpb=8336.7, bsz=314, num_updates=43900, lr=6.74967e-05, gnorm=0.5, clip=0, loss_scale=32, train_wall=72, gb_free=15.7, wall=36863
2023-09-02 20:07:51 | INFO | train_inner | epoch 030:   1273 / 1474 loss=1.897, trans_loss=4.806, nll_loss=2.018, w2v_ctc_loss=0.627, task_loss=4.433, task_loss_gen=7.141, contrastive_loss=0, total=4036.17, n_correct=2748.83, ppl=4.05, accuracy=68.105, wps=11041.7, ups=1.37, wpb=8072.3, bsz=284.3, num_updates=44000, lr=6.742e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=72, gb_free=15.4, wall=36936
2023-09-02 20:07:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 20:08:24 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 3.887 | trans_loss 5.159 | nll_loss 2.414 | w2v_ctc_loss 1.331 | task_loss 12.331 | task_loss_gen 18.793 | contrastive_loss 0 | total 4003.4 | n_correct 2680.9 | ppl 5.33 | accuracy 66.966 | uer 16.821 | wer 18.661 | raw_wer 18.661 | bleu 22.2 | wps 1650.8 | wpb 4003.4 | bsz 141.8 | num_updates 44000 | best_bleu 22.21
2023-09-02 20:08:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44000 updates
2023-09-02 20:08:24 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_30_44000.pt
2023-09-02 20:08:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_30_44000.pt
2023-09-02 20:08:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_30_44000.pt (epoch 30 @ 44000 updates, score 22.2) (writing took 8.077189682982862 seconds)
2023-09-02 20:09:45 | INFO | train_inner | epoch 030:   1373 / 1474 loss=1.881, trans_loss=4.799, nll_loss=2.01, w2v_ctc_loss=0.61, task_loss=3.428, task_loss_gen=6.205, contrastive_loss=0, total=4165.07, n_correct=2845.86, ppl=4.03, accuracy=68.327, wps=7348.8, ups=0.88, wpb=8330.1, bsz=321.6, num_updates=44100, lr=6.73435e-05, gnorm=0.501, clip=0, loss_scale=32, train_wall=71, gb_free=16.4, wall=37050
2023-09-02 20:10:57 | INFO | train_inner | epoch 030:   1473 / 1474 loss=1.886, trans_loss=4.804, nll_loss=2.016, w2v_ctc_loss=0.609, task_loss=3.965, task_loss_gen=6.225, contrastive_loss=0, total=4141.76, n_correct=2825.05, ppl=4.05, accuracy=68.209, wps=11517.4, ups=1.39, wpb=8283.5, bsz=314.3, num_updates=44200, lr=6.72673e-05, gnorm=0.509, clip=0, loss_scale=32, train_wall=71, gb_free=15.9, wall=37122
2023-09-02 20:10:57 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 20:11:30 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 3.888 | trans_loss 5.169 | nll_loss 2.43 | w2v_ctc_loss 1.31 | task_loss 12.008 | task_loss_gen 19.131 | contrastive_loss 0 | total 4003.4 | n_correct 2666.7 | ppl 5.39 | accuracy 66.611 | uer 17.527 | wer 19.399 | raw_wer 19.399 | bleu 22.24 | wps 1655.2 | wpb 4003.4 | bsz 141.8 | num_updates 44201 | best_bleu 22.24
2023-09-02 20:11:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44201 updates
2023-09-02 20:11:30 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 20:11:37 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 20:11:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 30 @ 44201 updates, score 22.24) (writing took 12.696375900995918 seconds)
2023-09-02 20:11:43 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-09-02 20:11:43 | INFO | train | epoch 030 | loss 1.888 | trans_loss 4.799 | nll_loss 2.009 | w2v_ctc_loss 0.617 | task_loss 4.062 | task_loss_gen 6.362 | contrastive_loss 0 | total 4138.65 | n_correct 2825.56 | ppl 4.02 | accuracy 68.272 | wps 10456.4 | ups 1.26 | wpb 8277.3 | bsz 305.7 | num_updates 44201 | lr 6.72665e-05 | gnorm 0.512 | clip 0 | loss_scale 32 | train_wall 1059 | gb_free 16.8 | wall 37169
2023-09-02 20:11:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 20:11:44 | INFO | fairseq.trainer | begin training epoch 31
2023-09-02 20:11:44 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 20:13:03 | INFO | train_inner | epoch 031:     99 / 1474 loss=1.884, trans_loss=4.786, nll_loss=1.99, w2v_ctc_loss=0.615, task_loss=4.137, task_loss_gen=7.016, contrastive_loss=0, total=4054.44, n_correct=2780.48, ppl=3.97, accuracy=68.579, wps=6414, ups=0.79, wpb=8108.9, bsz=288.2, num_updates=44300, lr=6.71913e-05, gnorm=0.514, clip=0, loss_scale=32, train_wall=72, gb_free=16.1, wall=37248
2023-09-02 20:14:16 | INFO | train_inner | epoch 031:    199 / 1474 loss=1.883, trans_loss=4.791, nll_loss=1.998, w2v_ctc_loss=0.612, task_loss=3.782, task_loss_gen=6.938, contrastive_loss=0, total=4147.4, n_correct=2840.31, ppl=3.99, accuracy=68.484, wps=11429.2, ups=1.38, wpb=8294.8, bsz=302.2, num_updates=44400, lr=6.71156e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=72, gb_free=16.4, wall=37321
2023-09-02 20:15:29 | INFO | train_inner | epoch 031:    299 / 1474 loss=1.882, trans_loss=4.785, nll_loss=1.989, w2v_ctc_loss=0.613, task_loss=3.684, task_loss_gen=6.834, contrastive_loss=0, total=4149.21, n_correct=2846.29, ppl=3.97, accuracy=68.598, wps=11357.2, ups=1.37, wpb=8298.4, bsz=301.6, num_updates=44500, lr=6.70402e-05, gnorm=0.511, clip=0, loss_scale=32, train_wall=72, gb_free=15.7, wall=37394
2023-09-02 20:16:42 | INFO | train_inner | epoch 031:    399 / 1474 loss=1.884, trans_loss=4.793, nll_loss=1.999, w2v_ctc_loss=0.608, task_loss=4.497, task_loss_gen=7.184, contrastive_loss=0, total=4092.62, n_correct=2801.63, ppl=4, accuracy=68.456, wps=11233.6, ups=1.37, wpb=8185.2, bsz=285.6, num_updates=44600, lr=6.6965e-05, gnorm=0.513, clip=0, loss_scale=32, train_wall=72, gb_free=16.9, wall=37467
2023-09-02 20:17:55 | INFO | train_inner | epoch 031:    499 / 1474 loss=1.886, trans_loss=4.789, nll_loss=1.995, w2v_ctc_loss=0.621, task_loss=4.795, task_loss_gen=6.58, contrastive_loss=0, total=4111.85, n_correct=2815.74, ppl=3.99, accuracy=68.479, wps=11271.4, ups=1.37, wpb=8223.7, bsz=300.3, num_updates=44700, lr=6.689e-05, gnorm=0.515, clip=0, loss_scale=32, train_wall=72, gb_free=10.6, wall=37540
2023-09-02 20:19:07 | INFO | train_inner | epoch 031:    599 / 1474 loss=1.881, trans_loss=4.787, nll_loss=1.992, w2v_ctc_loss=0.608, task_loss=4.662, task_loss_gen=6.688, contrastive_loss=0, total=4083.44, n_correct=2798.15, ppl=3.98, accuracy=68.524, wps=11242, ups=1.38, wpb=8166.9, bsz=294.5, num_updates=44800, lr=6.68153e-05, gnorm=0.511, clip=0, loss_scale=32, train_wall=72, gb_free=16.5, wall=37612
2023-09-02 20:20:20 | INFO | train_inner | epoch 031:    699 / 1474 loss=1.875, trans_loss=4.786, nll_loss=1.992, w2v_ctc_loss=0.601, task_loss=5.081, task_loss_gen=6.482, contrastive_loss=0, total=4213.98, n_correct=2890.97, ppl=3.98, accuracy=68.604, wps=11600.7, ups=1.38, wpb=8428, bsz=315.7, num_updates=44900, lr=6.67409e-05, gnorm=0.512, clip=0, loss_scale=32, train_wall=72, gb_free=15.9, wall=37685
2023-09-02 20:21:33 | INFO | train_inner | epoch 031:    799 / 1474 loss=1.887, trans_loss=4.797, nll_loss=2.006, w2v_ctc_loss=0.613, task_loss=6.84, task_loss_gen=6.975, contrastive_loss=0, total=4097.37, n_correct=2794.55, ppl=4.02, accuracy=68.204, wps=11244.7, ups=1.37, wpb=8194.7, bsz=295.8, num_updates=45000, lr=6.66667e-05, gnorm=0.526, clip=0, loss_scale=32, train_wall=72, gb_free=12.5, wall=37758
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:0')
2023-09-02 20:22:46 | INFO | train_inner | epoch 031:    899 / 1474 loss=1.882, trans_loss=4.783, nll_loss=1.987, w2v_ctc_loss=0.614, task_loss=6.432, task_loss_gen=7.007, contrastive_loss=0, total=4096.72, n_correct=2804.87, ppl=3.96, accuracy=68.466, wps=11267.9, ups=1.38, wpb=8193.4, bsz=296.1, num_updates=45100, lr=6.65927e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=72, gb_free=16.9, wall=37831
2023-09-02 20:23:57 | INFO | train_inner | epoch 031:    999 / 1474 loss=1.881, trans_loss=4.797, nll_loss=2.006, w2v_ctc_loss=0.609, task_loss=4.832, task_loss_gen=6.45, contrastive_loss=0, total=4187.84, n_correct=2864.86, ppl=4.02, accuracy=68.409, wps=11654, ups=1.39, wpb=8375.7, bsz=319.5, num_updates=45200, lr=6.6519e-05, gnorm=0.509, clip=0, loss_scale=64, train_wall=71, gb_free=16.9, wall=37903
2023-09-02 20:25:10 | INFO | train_inner | epoch 031:   1099 / 1474 loss=1.88, trans_loss=4.791, nll_loss=1.999, w2v_ctc_loss=0.607, task_loss=3.582, task_loss_gen=6.628, contrastive_loss=0, total=4149.44, n_correct=2840.84, ppl=4, accuracy=68.463, wps=11374.2, ups=1.37, wpb=8298.9, bsz=315, num_updates=45300, lr=6.64455e-05, gnorm=0.515, clip=0, loss_scale=64, train_wall=72, gb_free=17.3, wall=37976
2023-09-02 20:26:22 | INFO | train_inner | epoch 031:   1199 / 1474 loss=1.879, trans_loss=4.792, nll_loss=2, w2v_ctc_loss=0.608, task_loss=2.561, task_loss_gen=6.999, contrastive_loss=0, total=4189.76, n_correct=2868.85, ppl=4, accuracy=68.473, wps=11679.2, ups=1.39, wpb=8379.5, bsz=321.3, num_updates=45400, lr=6.63723e-05, gnorm=0.508, clip=0, loss_scale=64, train_wall=71, gb_free=12.9, wall=38047
2023-09-02 20:27:35 | INFO | train_inner | epoch 031:   1299 / 1474 loss=1.88, trans_loss=4.796, nll_loss=2.004, w2v_ctc_loss=0.611, task_loss=2.113, task_loss_gen=7.321, contrastive_loss=0, total=4227.44, n_correct=2893.54, ppl=4.01, accuracy=68.447, wps=11684.1, ups=1.38, wpb=8454.9, bsz=326.3, num_updates=45500, lr=6.62994e-05, gnorm=0.508, clip=0, loss_scale=64, train_wall=71, gb_free=16.1, wall=38120
2023-09-02 20:28:47 | INFO | train_inner | epoch 031:   1399 / 1474 loss=1.883, trans_loss=4.795, nll_loss=2.005, w2v_ctc_loss=0.612, task_loss=2.05, task_loss_gen=7.842, contrastive_loss=0, total=4186.05, n_correct=2860.88, ppl=4.01, accuracy=68.343, wps=11513.8, ups=1.38, wpb=8372.1, bsz=326.6, num_updates=45600, lr=6.62266e-05, gnorm=0.508, clip=0, loss_scale=64, train_wall=72, gb_free=17.2, wall=38192
2023-09-02 20:29:41 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.1445, device='cuda:5')
2023-09-02 20:30:14 | INFO | dev_st | epoch 031 | valid on 'dev_st' subset | loss 3.892 | trans_loss 5.164 | nll_loss 2.419 | w2v_ctc_loss 1.337 | task_loss 6.552 | task_loss_gen 23.186 | contrastive_loss 0 | total 4003.4 | n_correct 2668.5 | ppl 5.35 | accuracy 66.656 | uer 17.156 | wer 19.011 | raw_wer 19.011 | bleu 21.74 | wps 1654.7 | wpb 4003.4 | bsz 141.8 | num_updates 45675 | best_bleu 22.24
2023-09-02 20:30:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 45675 updates
2023-09-02 20:30:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.7404.pt
2023-09-02 20:30:16 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.7404.pt
2023-09-02 20:30:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_21.7404.pt (epoch 31 @ 45675 updates, score 21.74) (writing took 7.037618060014211 seconds)
2023-09-02 20:30:21 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-09-02 20:30:21 | INFO | train | epoch 031 | loss 1.882 | trans_loss 4.791 | nll_loss 1.997 | w2v_ctc_loss 0.611 | task_loss 4.08 | task_loss_gen 7.037 | contrastive_loss 0 | total 4138.65 | n_correct 2833.44 | ppl 3.99 | accuracy 68.463 | wps 10915.7 | ups 1.32 | wpb 8277.3 | bsz 305.7 | num_updates 45675 | lr 6.61722e-05 | gnorm 0.515 | clip 0 | loss_scale 64 | train_wall 1058 | gb_free 11.7 | wall 38286
2023-09-02 20:30:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 20:30:21 | INFO | fairseq.trainer | begin training epoch 32
2023-09-02 20:30:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 20:30:47 | INFO | train_inner | epoch 032:     25 / 1474 loss=1.882, trans_loss=4.786, nll_loss=1.991, w2v_ctc_loss=0.613, task_loss=2.231, task_loss_gen=9.141, contrastive_loss=0, total=4042.6, n_correct=2770.19, ppl=3.98, accuracy=68.525, wps=6758.7, ups=0.84, wpb=8085.2, bsz=288.7, num_updates=45700, lr=6.61541e-05, gnorm=0.518, clip=0, loss_scale=64, train_wall=71, gb_free=16, wall=38312
2023-09-02 20:32:00 | INFO | train_inner | epoch 032:    125 / 1474 loss=1.862, trans_loss=4.765, nll_loss=1.964, w2v_ctc_loss=0.591, task_loss=2.918, task_loss_gen=7.161, contrastive_loss=0, total=4227.68, n_correct=2919.45, ppl=3.9, accuracy=69.056, wps=11580.6, ups=1.37, wpb=8455.4, bsz=323.3, num_updates=45800, lr=6.60819e-05, gnorm=0.494, clip=0, loss_scale=64, train_wall=72, gb_free=15.9, wall=38385
2023-09-02 20:33:13 | INFO | train_inner | epoch 032:    225 / 1474 loss=1.875, trans_loss=4.783, nll_loss=1.988, w2v_ctc_loss=0.609, task_loss=2.902, task_loss_gen=7.408, contrastive_loss=0, total=4157.32, n_correct=2853.85, ppl=3.97, accuracy=68.646, wps=11388.6, ups=1.37, wpb=8314.6, bsz=320.6, num_updates=45900, lr=6.60098e-05, gnorm=0.511, clip=0, loss_scale=64, train_wall=72, gb_free=16.9, wall=38458
2023-09-02 20:34:25 | INFO | train_inner | epoch 032:    325 / 1474 loss=1.862, trans_loss=4.765, nll_loss=1.964, w2v_ctc_loss=0.59, task_loss=2.349, task_loss_gen=8.139, contrastive_loss=0, total=4183.45, n_correct=2894.72, ppl=3.9, accuracy=69.195, wps=11668.5, ups=1.39, wpb=8366.9, bsz=314.4, num_updates=46000, lr=6.5938e-05, gnorm=0.501, clip=0, loss_scale=64, train_wall=71, gb_free=17.2, wall=38530
2023-09-02 20:34:25 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 20:34:58 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 3.91 | trans_loss 5.18 | nll_loss 2.44 | w2v_ctc_loss 1.361 | task_loss 7.39 | task_loss_gen 23.889 | contrastive_loss 0 | total 4003.4 | n_correct 2665.6 | ppl 5.43 | accuracy 66.583 | uer 17.408 | wer 19.332 | raw_wer 19.332 | bleu 21.96 | wps 1645.8 | wpb 4003.4 | bsz 141.8 | num_updates 46000 | best_bleu 22.24
2023-09-02 20:34:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 46000 updates
2023-09-02 20:34:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_32_46000.pt
2023-09-02 20:35:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_32_46000.pt
2023-09-02 20:35:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_32_46000.pt (epoch 32 @ 46000 updates, score 21.96) (writing took 8.176755937980488 seconds)
2023-09-02 20:36:19 | INFO | train_inner | epoch 032:    425 / 1474 loss=1.872, trans_loss=4.773, nll_loss=1.974, w2v_ctc_loss=0.602, task_loss=2.123, task_loss_gen=9.077, contrastive_loss=0, total=4157.28, n_correct=2861.38, ppl=3.93, accuracy=68.828, wps=7270.1, ups=0.87, wpb=8314.6, bsz=305.9, num_updates=46100, lr=6.58665e-05, gnorm=0.512, clip=0, loss_scale=64, train_wall=72, gb_free=14.5, wall=38644
2023-09-02 20:37:32 | INFO | train_inner | epoch 032:    525 / 1474 loss=1.882, trans_loss=4.787, nll_loss=1.993, w2v_ctc_loss=0.619, task_loss=2.18, task_loss_gen=8.871, contrastive_loss=0, total=4198.93, n_correct=2877.85, ppl=3.98, accuracy=68.538, wps=11451.9, ups=1.36, wpb=8397.9, bsz=317.6, num_updates=46200, lr=6.57952e-05, gnorm=0.512, clip=0, loss_scale=64, train_wall=73, gb_free=17, wall=38718
2023-09-02 20:38:45 | INFO | train_inner | epoch 032:    625 / 1474 loss=1.879, trans_loss=4.787, nll_loss=1.992, w2v_ctc_loss=0.608, task_loss=2.014, task_loss_gen=10.123, contrastive_loss=0, total=4142.69, n_correct=2839.6, ppl=3.98, accuracy=68.545, wps=11362.4, ups=1.37, wpb=8285.4, bsz=301.6, num_updates=46300, lr=6.57241e-05, gnorm=0.509, clip=0, loss_scale=64, train_wall=72, gb_free=17, wall=38790
2023-09-02 20:39:58 | INFO | train_inner | epoch 032:    725 / 1474 loss=1.879, trans_loss=4.785, nll_loss=1.99, w2v_ctc_loss=0.611, task_loss=1.934, task_loss_gen=10.556, contrastive_loss=0, total=4154.59, n_correct=2853.3, ppl=3.97, accuracy=68.678, wps=11391.9, ups=1.37, wpb=8309.2, bsz=301.8, num_updates=46400, lr=6.56532e-05, gnorm=0.519, clip=0, loss_scale=64, train_wall=72, gb_free=15.4, wall=38863
2023-09-02 20:41:10 | INFO | train_inner | epoch 032:    825 / 1474 loss=1.874, trans_loss=4.779, nll_loss=1.983, w2v_ctc_loss=0.598, task_loss=1.993, task_loss_gen=10.708, contrastive_loss=0, total=4114.54, n_correct=2828.22, ppl=3.95, accuracy=68.737, wps=11410.1, ups=1.39, wpb=8229.1, bsz=294.9, num_updates=46500, lr=6.55826e-05, gnorm=0.51, clip=0, loss_scale=64, train_wall=71, gb_free=16.8, wall=38936
2023-09-02 20:42:23 | INFO | train_inner | epoch 032:    925 / 1474 loss=1.879, trans_loss=4.784, nll_loss=1.989, w2v_ctc_loss=0.605, task_loss=2.264, task_loss_gen=10.473, contrastive_loss=0, total=4139.67, n_correct=2842.73, ppl=3.97, accuracy=68.67, wps=11376.3, ups=1.37, wpb=8279.3, bsz=298.3, num_updates=46600, lr=6.55122e-05, gnorm=0.52, clip=0, loss_scale=64, train_wall=72, gb_free=16.5, wall=39008
2023-09-02 20:42:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-02 20:43:36 | INFO | train_inner | epoch 032:   1026 / 1474 loss=1.885, trans_loss=4.793, nll_loss=2.001, w2v_ctc_loss=0.613, task_loss=3.842, task_loss_gen=8.3, contrastive_loss=0, total=4119.63, n_correct=2815.07, ppl=4, accuracy=68.333, wps=11276.6, ups=1.37, wpb=8239.3, bsz=305.7, num_updates=46700, lr=6.5442e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=72, gb_free=16.5, wall=39081
2023-09-02 20:44:49 | INFO | train_inner | epoch 032:   1126 / 1474 loss=1.892, trans_loss=4.795, nll_loss=2.002, w2v_ctc_loss=0.616, task_loss=7.641, task_loss_gen=9.837, contrastive_loss=0, total=4015.59, n_correct=2740.81, ppl=4, accuracy=68.254, wps=10982.8, ups=1.37, wpb=8031.2, bsz=270.1, num_updates=46800, lr=6.5372e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=72, gb_free=16.9, wall=39155
2023-09-02 20:46:02 | INFO | train_inner | epoch 032:   1226 / 1474 loss=1.888, trans_loss=4.802, nll_loss=2.011, w2v_ctc_loss=0.612, task_loss=13.193, task_loss_gen=12.466, contrastive_loss=0, total=4153.44, n_correct=2832.08, ppl=4.03, accuracy=68.186, wps=11365.9, ups=1.37, wpb=8306.9, bsz=310.8, num_updates=46900, lr=6.53023e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=72, gb_free=16, wall=39228
2023-09-02 20:47:15 | INFO | train_inner | epoch 032:   1326 / 1474 loss=1.887, trans_loss=4.792, nll_loss=1.997, w2v_ctc_loss=0.618, task_loss=30.548, task_loss_gen=22.582, contrastive_loss=0, total=4075.86, n_correct=2786.87, ppl=3.99, accuracy=68.375, wps=11298, ups=1.39, wpb=8151.7, bsz=295.4, num_updates=47000, lr=6.52328e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=71, gb_free=16.5, wall=39300
2023-09-02 20:48:27 | INFO | train_inner | epoch 032:   1426 / 1474 loss=1.885, trans_loss=4.791, nll_loss=1.997, w2v_ctc_loss=0.615, task_loss=20.708, task_loss_gen=15.836, contrastive_loss=0, total=4116.4, n_correct=2820, ppl=3.99, accuracy=68.506, wps=11380.9, ups=1.38, wpb=8232.8, bsz=307.6, num_updates=47100, lr=6.51635e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=71, gb_free=16.5, wall=39372
2023-09-02 20:49:01 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 20:49:34 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 3.879 | trans_loss 5.162 | nll_loss 2.419 | w2v_ctc_loss 1.295 | task_loss 28.757 | task_loss_gen 25.003 | contrastive_loss 0 | total 4003.4 | n_correct 2669 | ppl 5.35 | accuracy 66.668 | uer 17.044 | wer 18.858 | raw_wer 18.858 | bleu 22.01 | wps 1649.2 | wpb 4003.4 | bsz 141.8 | num_updates 47148 | best_bleu 22.24
2023-09-02 20:49:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 47148 updates
2023-09-02 20:49:34 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_22.0100.pt
2023-09-02 20:49:37 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_22.0100.pt
2023-09-02 20:49:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint.best_bleu_22.0100.pt (epoch 32 @ 47148 updates, score 22.01) (writing took 7.706322916026693 seconds)
2023-09-02 20:49:42 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-09-02 20:49:42 | INFO | train | epoch 032 | loss 1.878 | trans_loss 4.784 | nll_loss 1.989 | w2v_ctc_loss 0.607 | task_loss 7.174 | task_loss_gen 10.841 | contrastive_loss 0 | total 4138.66 | n_correct 2839.71 | ppl 3.97 | accuracy 68.614 | wps 10500.7 | ups 1.27 | wpb 8277.3 | bsz 305.7 | num_updates 47148 | lr 6.51303e-05 | gnorm 0.519 | clip 0 | loss_scale 32 | train_wall 1058 | gb_free 16 | wall 39448
2023-09-02 20:49:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 20:49:43 | INFO | fairseq.trainer | begin training epoch 33
2023-09-02 20:49:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 20:50:28 | INFO | train_inner | epoch 033:     52 / 1474 loss=1.873, trans_loss=4.784, nll_loss=1.988, w2v_ctc_loss=0.601, task_loss=23.736, task_loss_gen=16.619, contrastive_loss=0, total=4149.21, n_correct=2849.13, ppl=3.97, accuracy=68.667, wps=6848.1, ups=0.83, wpb=8298.4, bsz=320.5, num_updates=47200, lr=6.50945e-05, gnorm=0.563, clip=0, loss_scale=32, train_wall=71, gb_free=16.7, wall=39493
2023-09-02 20:51:41 | INFO | train_inner | epoch 033:    152 / 1474 loss=1.867, trans_loss=4.767, nll_loss=1.965, w2v_ctc_loss=0.588, task_loss=31.148, task_loss_gen=20.282, contrastive_loss=0, total=4073.9, n_correct=2810.39, ppl=3.9, accuracy=68.985, wps=11219.6, ups=1.38, wpb=8147.8, bsz=284.8, num_updates=47300, lr=6.50256e-05, gnorm=0.605, clip=0, loss_scale=32, train_wall=72, gb_free=14.9, wall=39566
2023-09-02 20:52:54 | INFO | train_inner | epoch 033:    252 / 1474 loss=1.863, trans_loss=4.77, nll_loss=1.971, w2v_ctc_loss=0.592, task_loss=14.131, task_loss_gen=9.658, contrastive_loss=0, total=4280.14, n_correct=2952.96, ppl=3.92, accuracy=68.992, wps=11698.9, ups=1.37, wpb=8560.3, bsz=346.5, num_updates=47400, lr=6.4957e-05, gnorm=0.512, clip=0, loss_scale=32, train_wall=72, gb_free=16.1, wall=39639
2023-09-02 20:54:06 | INFO | train_inner | epoch 033:    352 / 1474 loss=1.874, trans_loss=4.775, nll_loss=1.977, w2v_ctc_loss=0.604, task_loss=12.196, task_loss_gen=9.038, contrastive_loss=0, total=4120.27, n_correct=2835.51, ppl=3.94, accuracy=68.819, wps=11386.2, ups=1.38, wpb=8240.5, bsz=300.7, num_updates=47500, lr=6.48886e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=72, gb_free=17, wall=39711
2023-09-02 20:55:18 | INFO | train_inner | epoch 033:    452 / 1474 loss=1.86, trans_loss=4.762, nll_loss=1.961, w2v_ctc_loss=0.589, task_loss=6.795, task_loss_gen=6.826, contrastive_loss=0, total=4141.22, n_correct=2865.44, ppl=3.89, accuracy=69.193, wps=11567.8, ups=1.4, wpb=8282.4, bsz=310.8, num_updates=47600, lr=6.48204e-05, gnorm=0.51, clip=0, loss_scale=32, train_wall=71, gb_free=16, wall=39783
2023-09-02 20:56:31 | INFO | train_inner | epoch 033:    552 / 1474 loss=1.876, trans_loss=4.776, nll_loss=1.977, w2v_ctc_loss=0.606, task_loss=8.06, task_loss_gen=7.586, contrastive_loss=0, total=4133.59, n_correct=2843.47, ppl=3.94, accuracy=68.789, wps=11322.3, ups=1.37, wpb=8267.2, bsz=294, num_updates=47700, lr=6.47524e-05, gnorm=0.537, clip=0, loss_scale=32, train_wall=72, gb_free=14.8, wall=39856
2023-09-02 20:57:44 | INFO | train_inner | epoch 033:    652 / 1474 loss=1.879, trans_loss=4.789, nll_loss=1.994, w2v_ctc_loss=0.603, task_loss=7.171, task_loss_gen=6.581, contrastive_loss=0, total=4157.63, n_correct=2847.36, ppl=3.98, accuracy=68.485, wps=11386.2, ups=1.37, wpb=8315.3, bsz=301.6, num_updates=47800, lr=6.46846e-05, gnorm=0.519, clip=0, loss_scale=32, train_wall=72, gb_free=17.4, wall=39929
2023-09-02 20:58:57 | INFO | train_inner | epoch 033:    752 / 1474 loss=1.887, trans_loss=4.784, nll_loss=1.989, w2v_ctc_loss=0.622, task_loss=6.218, task_loss_gen=6.753, contrastive_loss=0, total=4070.75, n_correct=2789.9, ppl=3.97, accuracy=68.535, wps=11208.3, ups=1.38, wpb=8141.5, bsz=287.2, num_updates=47900, lr=6.46171e-05, gnorm=0.524, clip=0, loss_scale=32, train_wall=72, gb_free=16, wall=40002
2023-09-02 21:00:08 | INFO | train_inner | epoch 033:    852 / 1474 loss=1.867, trans_loss=4.773, nll_loss=1.975, w2v_ctc_loss=0.595, task_loss=4.649, task_loss_gen=5.945, contrastive_loss=0, total=4130.24, n_correct=2849.06, ppl=3.93, accuracy=68.98, wps=11513.1, ups=1.39, wpb=8260.5, bsz=316.3, num_updates=48000, lr=6.45497e-05, gnorm=0.515, clip=0, loss_scale=32, train_wall=71, gb_free=16.3, wall=40074
2023-09-02 21:00:08 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 21:00:41 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 3.885 | trans_loss 5.166 | nll_loss 2.421 | w2v_ctc_loss 1.31 | task_loss 13.067 | task_loss_gen 17.151 | contrastive_loss 0 | total 4003.4 | n_correct 2673.7 | ppl 5.36 | accuracy 66.786 | uer 17.02 | wer 18.843 | raw_wer 18.843 | bleu 22.25 | wps 1662.1 | wpb 4003.4 | bsz 141.8 | num_updates 48000 | best_bleu 22.25
2023-09-02 21:00:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48000 updates
2023-09-02 21:00:41 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_33_48000.pt
2023-09-02 21:00:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_33_48000.pt
2023-09-02 21:00:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_33_48000.pt (epoch 33 @ 48000 updates, score 22.25) (writing took 14.154196188959759 seconds)
2023-09-02 21:02:08 | INFO | train_inner | epoch 033:    952 / 1474 loss=1.876, trans_loss=4.778, nll_loss=1.981, w2v_ctc_loss=0.61, task_loss=6.442, task_loss_gen=6.674, contrastive_loss=0, total=4151.18, n_correct=2854.43, ppl=3.95, accuracy=68.762, wps=6942.2, ups=0.84, wpb=8302.4, bsz=308.3, num_updates=48100, lr=6.44826e-05, gnorm=0.522, clip=0, loss_scale=32, train_wall=71, gb_free=10.9, wall=40193
2023-09-02 21:03:22 | INFO | train_inner | epoch 033:   1052 / 1474 loss=1.875, trans_loss=4.776, nll_loss=1.979, w2v_ctc_loss=0.604, task_loss=6.529, task_loss_gen=6.786, contrastive_loss=0, total=4140.1, n_correct=2841.03, ppl=3.94, accuracy=68.622, wps=11243.5, ups=1.36, wpb=8280.2, bsz=307.6, num_updates=48200, lr=6.44157e-05, gnorm=0.526, clip=0, loss_scale=32, train_wall=73, gb_free=11.2, wall=40267
2023-09-02 21:04:35 | INFO | train_inner | epoch 033:   1152 / 1474 loss=1.877, trans_loss=4.785, nll_loss=1.991, w2v_ctc_loss=0.6, task_loss=6.144, task_loss_gen=6.379, contrastive_loss=0, total=4182.67, n_correct=2870.56, ppl=3.97, accuracy=68.63, wps=11393.2, ups=1.36, wpb=8365.3, bsz=309.5, num_updates=48300, lr=6.43489e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=73, gb_free=17.2, wall=40340
2023-09-02 21:05:48 | INFO | train_inner | epoch 033:   1252 / 1474 loss=1.878, trans_loss=4.777, nll_loss=1.98, w2v_ctc_loss=0.608, task_loss=5.604, task_loss_gen=6.402, contrastive_loss=0, total=4110.02, n_correct=2825.29, ppl=3.94, accuracy=68.742, wps=11266.2, ups=1.37, wpb=8220, bsz=294.4, num_updates=48400, lr=6.42824e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=72, gb_free=16.4, wall=40413
2023-09-02 21:07:01 | INFO | train_inner | epoch 033:   1352 / 1474 loss=1.872, trans_loss=4.777, nll_loss=1.981, w2v_ctc_loss=0.606, task_loss=4.995, task_loss_gen=5.895, contrastive_loss=0, total=4128.82, n_correct=2842.11, ppl=3.95, accuracy=68.836, wps=11327.8, ups=1.37, wpb=8257.6, bsz=312.4, num_updates=48500, lr=6.42161e-05, gnorm=0.51, clip=0, loss_scale=32, train_wall=72, gb_free=15.7, wall=40486
2023-09-02 21:08:14 | INFO | train_inner | epoch 033:   1452 / 1474 loss=1.869, trans_loss=4.775, nll_loss=1.979, w2v_ctc_loss=0.594, task_loss=5.315, task_loss_gen=6.409, contrastive_loss=0, total=4123.47, n_correct=2835.27, ppl=3.94, accuracy=68.759, wps=11315.8, ups=1.37, wpb=8246.9, bsz=308.9, num_updates=48600, lr=6.415e-05, gnorm=0.515, clip=0, loss_scale=32, train_wall=72, gb_free=16.2, wall=40559
2023-09-02 21:08:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 21:09:02 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 3.892 | trans_loss 5.168 | nll_loss 2.425 | w2v_ctc_loss 1.328 | task_loss 23.707 | task_loss_gen 20.726 | contrastive_loss 0 | total 4003.4 | n_correct 2670.9 | ppl 5.37 | accuracy 66.716 | uer 17.087 | wer 18.892 | raw_wer 18.892 | bleu 22.25 | wps 1625 | wpb 4003.4 | bsz 141.8 | num_updates 48622 | best_bleu 22.25
2023-09-02 21:09:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48622 updates
2023-09-02 21:09:02 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 21:09:09 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt
2023-09-02 21:09:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_best.pt (epoch 33 @ 48622 updates, score 22.25) (writing took 12.797717934998218 seconds)
2023-09-02 21:09:16 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-09-02 21:09:16 | INFO | train | epoch 033 | loss 1.873 | trans_loss 4.776 | nll_loss 1.978 | w2v_ctc_loss 0.602 | task_loss 9.575 | task_loss_gen 8.295 | contrastive_loss 0 | total 4138.65 | n_correct 2847.29 | ppl 3.94 | accuracy 68.797 | wps 10398.1 | ups 1.26 | wpb 8277.3 | bsz 305.7 | num_updates 48622 | lr 6.41355e-05 | gnorm 0.529 | clip 0 | loss_scale 32 | train_wall 1061 | gb_free 17.5 | wall 40621
2023-09-02 21:09:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 21:09:16 | INFO | fairseq.trainer | begin training epoch 34
2023-09-02 21:09:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-02 21:10:20 | INFO | train_inner | epoch 034:     78 / 1474 loss=1.866, trans_loss=4.763, nll_loss=1.961, w2v_ctc_loss=0.6, task_loss=7.445, task_loss_gen=7.102, contrastive_loss=0, total=4128.94, n_correct=2852.82, ppl=3.89, accuracy=69.093, wps=6532.4, ups=0.79, wpb=8257.9, bsz=302.1, num_updates=48700, lr=6.40841e-05, gnorm=0.524, clip=0, loss_scale=64, train_wall=71, gb_free=14.9, wall=40685
2023-09-02 21:11:33 | INFO | train_inner | epoch 034:    178 / 1474 loss=1.863, trans_loss=4.754, nll_loss=1.949, w2v_ctc_loss=0.595, task_loss=5.203, task_loss_gen=6.732, contrastive_loss=0, total=4071.22, n_correct=2821.85, ppl=3.86, accuracy=69.312, wps=11187.7, ups=1.37, wpb=8142.4, bsz=295.1, num_updates=48800, lr=6.40184e-05, gnorm=0.514, clip=0, loss_scale=64, train_wall=72, gb_free=15.2, wall=40758
2023-09-02 21:12:46 | INFO | train_inner | epoch 034:    278 / 1474 loss=1.869, trans_loss=4.774, nll_loss=1.976, w2v_ctc_loss=0.595, task_loss=4.219, task_loss_gen=6.465, contrastive_loss=0, total=4237.89, n_correct=2915.5, ppl=3.93, accuracy=68.796, wps=11595.4, ups=1.37, wpb=8475.8, bsz=326.9, num_updates=48900, lr=6.39529e-05, gnorm=0.502, clip=0, loss_scale=64, train_wall=72, gb_free=10, wall=40831
2023-09-02 21:13:59 | INFO | train_inner | epoch 034:    378 / 1474 loss=1.852, trans_loss=4.75, nll_loss=1.945, w2v_ctc_loss=0.58, task_loss=3.446, task_loss_gen=6.394, contrastive_loss=0, total=4167, n_correct=2891.16, ppl=3.85, accuracy=69.382, wps=11472.5, ups=1.38, wpb=8334, bsz=319, num_updates=49000, lr=6.38877e-05, gnorm=0.508, clip=0, loss_scale=64, train_wall=72, gb_free=17, wall=40904
2023-09-02 21:15:11 | INFO | train_inner | epoch 034:    478 / 1474 loss=1.871, trans_loss=4.765, nll_loss=1.963, w2v_ctc_loss=0.603, task_loss=3.184, task_loss_gen=7.872, contrastive_loss=0, total=4071.65, n_correct=2807.72, ppl=3.9, accuracy=68.958, wps=11232.7, ups=1.38, wpb=8143.3, bsz=284.8, num_updates=49100, lr=6.38226e-05, gnorm=0.517, clip=0, loss_scale=64, train_wall=72, gb_free=11.2, wall=40976
2023-09-02 21:16:23 | INFO | train_inner | epoch 034:    578 / 1474 loss=1.861, trans_loss=4.753, nll_loss=1.949, w2v_ctc_loss=0.593, task_loss=2.616, task_loss_gen=7.783, contrastive_loss=0, total=4110.13, n_correct=2845.52, ppl=3.86, accuracy=69.232, wps=11412.5, ups=1.39, wpb=8220.3, bsz=299, num_updates=49200, lr=6.37577e-05, gnorm=0.514, clip=0, loss_scale=64, train_wall=71, gb_free=16.3, wall=41048
2023-09-02 21:17:36 | INFO | train_inner | epoch 034:    678 / 1474 loss=1.865, trans_loss=4.764, nll_loss=1.964, w2v_ctc_loss=0.593, task_loss=2.449, task_loss_gen=8.098, contrastive_loss=0, total=4128.65, n_correct=2849.64, ppl=3.9, accuracy=69.021, wps=11345.4, ups=1.37, wpb=8257.3, bsz=300.7, num_updates=49300, lr=6.3693e-05, gnorm=0.514, clip=0, loss_scale=64, train_wall=72, gb_free=17.5, wall=41121
2023-09-02 21:18:49 | INFO | train_inner | epoch 034:    778 / 1474 loss=1.872, trans_loss=4.782, nll_loss=1.986, w2v_ctc_loss=0.595, task_loss=2.322, task_loss_gen=8.923, contrastive_loss=0, total=4075.69, n_correct=2801.79, ppl=3.96, accuracy=68.744, wps=11240.5, ups=1.38, wpb=8151.4, bsz=294.5, num_updates=49400, lr=6.36285e-05, gnorm=0.514, clip=0, loss_scale=64, train_wall=72, gb_free=17, wall=41194
2023-09-02 21:20:02 | INFO | train_inner | epoch 034:    878 / 1474 loss=1.874, trans_loss=4.774, nll_loss=1.976, w2v_ctc_loss=0.604, task_loss=2.158, task_loss_gen=9.332, contrastive_loss=0, total=4104.97, n_correct=2828.57, ppl=3.93, accuracy=68.906, wps=11233.5, ups=1.37, wpb=8209.9, bsz=296.3, num_updates=49500, lr=6.35642e-05, gnorm=0.523, clip=0, loss_scale=64, train_wall=72, gb_free=17.5, wall=41267
2023-09-02 21:21:14 | INFO | train_inner | epoch 034:    978 / 1474 loss=1.869, trans_loss=4.77, nll_loss=1.972, w2v_ctc_loss=0.601, task_loss=2.113, task_loss_gen=8.611, contrastive_loss=0, total=4168.94, n_correct=2873.41, ppl=3.92, accuracy=68.924, wps=11479.8, ups=1.38, wpb=8337.9, bsz=312.8, num_updates=49600, lr=6.35001e-05, gnorm=0.518, clip=0, loss_scale=64, train_wall=72, gb_free=14.6, wall=41340
2023-09-02 21:22:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-02 21:22:27 | INFO | train_inner | epoch 034:   1079 / 1474 loss=1.869, trans_loss=4.767, nll_loss=1.967, w2v_ctc_loss=0.605, task_loss=2.058, task_loss_gen=8.736, contrastive_loss=0, total=4148.51, n_correct=2862.95, ppl=3.91, accuracy=69.012, wps=11412.1, ups=1.38, wpb=8297, bsz=307.1, num_updates=49700, lr=6.34361e-05, gnorm=0.52, clip=0, loss_scale=32, train_wall=72, gb_free=16.5, wall=41412
2023-09-02 21:23:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-02 21:23:40 | INFO | train_inner | epoch 034:   1180 / 1474 loss=1.872, trans_loss=4.774, nll_loss=1.976, w2v_ctc_loss=0.6, task_loss=4.06, task_loss_gen=7.627, contrastive_loss=0, total=4101.5, n_correct=2826.15, ppl=3.93, accuracy=68.905, wps=11172.8, ups=1.36, wpb=8203, bsz=298.5, num_updates=49800, lr=6.33724e-05, gnorm=0.519, clip=0, loss_scale=16, train_wall=73, gb_free=15.9, wall=41486
2023-09-02 21:24:53 | INFO | train_inner | epoch 034:   1280 / 1474 loss=1.875, trans_loss=4.774, nll_loss=1.975, w2v_ctc_loss=0.604, task_loss=5.91, task_loss_gen=6.584, contrastive_loss=0, total=4146.01, n_correct=2853.4, ppl=3.93, accuracy=68.823, wps=11503.4, ups=1.39, wpb=8292, bsz=300.6, num_updates=49900, lr=6.33089e-05, gnorm=0.541, clip=0, loss_scale=16, train_wall=71, gb_free=16.9, wall=41558
2023-09-02 21:26:06 | INFO | train_inner | epoch 034:   1380 / 1474 loss=1.878, trans_loss=4.784, nll_loss=1.989, w2v_ctc_loss=0.609, task_loss=5.477, task_loss_gen=6.214, contrastive_loss=0, total=4197.99, n_correct=2876.27, ppl=3.97, accuracy=68.515, wps=11464.7, ups=1.37, wpb=8396, bsz=321.1, num_updates=50000, lr=6.32456e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=72, gb_free=16.9, wall=41631
2023-09-02 21:26:06 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2023-09-02 21:26:06 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-02 21:26:39 | INFO | dev_st | epoch 034 | valid on 'dev_st' subset | loss 3.895 | trans_loss 5.177 | nll_loss 2.44 | w2v_ctc_loss 1.316 | task_loss 16.111 | task_loss_gen 18.483 | contrastive_loss 0 | total 4003.4 | n_correct 2664.1 | ppl 5.43 | accuracy 66.546 | uer 16.845 | wer 18.81 | raw_wer 18.81 | bleu 21.95 | wps 1644.2 | wpb 4003.4 | bsz 141.8 | num_updates 50000 | best_bleu 22.25
2023-09-02 21:26:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 50000 updates
2023-09-02 21:26:39 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_34_50000.pt
2023-09-02 21:26:41 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_34_50000.pt
2023-09-02 21:26:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_34_50000.pt (epoch 34 @ 50000 updates, score 21.95) (writing took 9.907662609010004 seconds)
2023-09-02 21:26:49 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2023-09-02 21:26:49 | INFO | train | epoch 034 | loss 1.868 | trans_loss 4.768 | nll_loss 1.968 | w2v_ctc_loss 0.598 | task_loss 3.725 | task_loss_gen 7.604 | contrastive_loss 0 | total 4132.83 | n_correct 2850.58 | ppl 3.91 | accuracy 68.974 | wps 10813.8 | ups 1.31 | wpb 8265.7 | bsz 304.2 | num_updates 50000 | lr 6.32456e-05 | gnorm 0.518 | clip 0 | loss_scale 16 | train_wall 992 | gb_free 16.9 | wall 41674
2023-09-02 21:26:49 | INFO | fairseq_cli.train | done training in 41631.3 seconds
Exception in thread Thread-6:
Traceback (most recent call last):
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-7:
Traceback (most recent call last):
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    res = self._recv_bytes()
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    self.run()
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    buf = self._recv_bytes(maxlength)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
    data = self._queue.get(True, queue_wait_duration)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    self.run()
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    res = self._recv_bytes()
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    data = self._queue.get(True, queue_wait_duration)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    buf = self._recv_bytes(maxlength)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    res = self._recv_bytes()
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv(4)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    buf = self._recv_bytes(maxlength)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    raise EOFError
EOFError
    buf = self._recv(4)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1472 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-09-02 21:47:40 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:13472
2023-09-02 21:47:41 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:13472
2023-09-02 21:47:41 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:13472
2023-09-02 21:47:41 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:13472
2023-09-02 21:47:41 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:13472
2023-09-02 21:47:41 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:13472
2023-09-02 21:47:41 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:13472
2023-09-02 21:47:41 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:13472
2023-09-02 21:47:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-09-02 21:47:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-09-02 21:47:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-09-02 21:47:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-09-02 21:47:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-09-02 21:47:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-09-02 21:47:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-09-02 21:47:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-09-02 21:47:42 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:47:42 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 0
2023-09-02 21:47:42 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:47:42 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 2
2023-09-02 21:47:42 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:47:42 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 4
2023-09-02 21:47:42 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:47:42 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 5
2023-09-02 21:47:42 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:47:42 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:47:42 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 1
2023-09-02 21:47:42 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:47:42 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:47:42 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 3
2023-09-02 21:47:42 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 6
2023-09-02 21:47:42 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 7
2023-09-02 21:47:45 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13472', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=False, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=False, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=False, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-09-02 21:47:45 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-09-02 21:47:45 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-09-02 21:47:45 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-09-02 21:47:45 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-09-02 21:47:45 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-09-02 21:47:49 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-09-02 21:47:49 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-09-02 21:47:49 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-09-02 21:47:51 | INFO | root | load pretrained hubert
2023-09-02 21:47:59 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-09-02 21:48:02 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-09-02 21:48:09 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-09-02 21:48:09 | INFO | root | share the sematic adapter and textual encoder
2023-09-02 21:48:09 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-09-02 21:48:09 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-09-02 21:48:09 | INFO | fairseq_cli.train | model: S2TJoint
2023-09-02 21:48:09 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-09-02 21:48:09 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-09-02 21:48:09 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-09-02 21:48:09 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-02 21:48:09 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 21:48:09 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 21:48:09 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-02 21:48:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-09-02 21:48:25 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-09-02 21:48:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-09-02 21:48:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-02 21:48:26 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:48:26 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:48:26 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:48:26 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:48:26 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:48:26 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:48:26 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:48:26 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:48:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-02 21:48:26 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-09-02 21:48:26 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-09-02 21:48:26 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_last.pt
2023-09-02 21:48:28 | INFO | fairseq.trainer | load the task parameters
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
2023-09-02 21:48:43 | INFO | fairseq.optim.adam | using FusedAdam
2023-09-02 21:48:45 | INFO | fairseq.trainer | Loaded checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_last.pt (epoch 34 @ 50000 updates)
2023-09-02 21:48:45 | INFO | fairseq.trainer | loading train data for epoch 34
2023-09-02 21:48:45 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-02 21:48:45 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 21:48:45 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 21:48:48 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-02 21:48:50 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-02 21:49:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 21:49:31 | INFO | fairseq.trainer | begin training epoch 34
2023-09-02 21:49:31 | INFO | fairseq_cli.train | Start iterating over samples
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
task_net module.module.task_net.layer_norm.weight True tensor([-7.4805e-01, -7.9297e-01, -1.3672e-01,  1.0986e-01, -6.8848e-02,
        -1.3916e-02, -2.2412e-01,  4.3555e-01, -3.6914e-01, -1.5967e-01,
        -8.2617e-01,  1.2158e-01,  9.9365e-02, -3.8086e-01, -7.8125e-02,
         5.0293e-02, -1.4209e-01,  1.2354e-01, -7.0312e-01, -9.7168e-02,
        -5.4395e-01,  1.0742e-01,  5.0879e-01,  7.0898e-01,  1.9385e-01,
         5.4810e-02,  4.3555e-01, -5.7910e-01, -3.4473e-01, -4.8438e-01,
         1.6523e+00, -2.0947e-01, -1.1758e+00,  1.4551e+00, -4.0039e-02,
        -1.7676e-01,  3.1030e-01,  1.1543e+00, -3.4570e-01, -6.1914e-01,
        -1.5273e+00,  2.0142e-01,  9.0332e-02, -1.1743e-01, -2.8613e-01,
         5.7178e-01, -7.8125e-01, -7.5195e-01, -6.4648e-01,  2.8076e-01,
        -4.9805e-02,  5.0781e-02,  1.6235e-01, -7.1484e-01,  1.0879e+00,
        -1.1230e+00, -2.9688e-01, -5.8203e-01, -3.6987e-02, -5.4297e-01,
         2.3242e-01, -8.4961e-01,  3.5059e-01, -6.6992e-01, -2.9395e-01,
        -4.3359e-01, -1.7548e-03, -1.7285e-01,  5.9204e-03, -3.4766e-01,
        -3.3008e-01, -9.9609e-01,  1.6357e-01, -5.1562e-01,  7.0312e-01,
         2.1545e-02, -2.0459e-01, -7.1045e-02,  1.9531e-02, -5.5566e-01,
        -2.6093e-02, -1.0176e+00, -1.7041e-01, -1.5472e-02,  3.1738e-02,
         1.5781e+00, -7.5781e-01,  1.7285e+00, -1.9287e-01, -4.2969e-02,
        -4.5703e-01, -7.0312e-01, -2.2461e-01,  1.8281e+00, -5.9961e-01,
        -1.2695e+00, -3.3203e-01, -1.0586e+00,  1.0381e+00, -1.8883e-03,
        -7.5195e-01, -1.0547e-01,  3.0176e-01,  1.2256e-01, -4.6387e-03,
         1.1615e-01, -2.6660e-01,  1.3086e+00, -7.3828e-01,  4.2920e-01,
        -2.3242e-01, -7.3633e-01,  9.3945e-01, -5.3833e-02, -5.1074e-01,
        -1.4062e-01, -8.7695e-01,  1.7305e+00, -2.0166e-01,  4.2920e-01,
         1.0889e+00, -2.2656e-01,  2.0898e-01, -1.3135e-01, -2.5177e-04,
        -3.3057e-01,  1.6465e+00,  2.0447e-02, -4.7266e-01, -2.8125e-01,
        -4.5898e-01,  6.1084e-01, -7.5195e-01, -1.0996e+00,  3.5522e-02,
        -2.9297e-01, -5.8984e-01,  1.2805e-01, -2.2852e-01, -7.3633e-01,
        -2.9834e-01, -3.0670e-02, -2.5342e-01,  3.5858e-03, -4.1504e-01,
        -6.9727e-01,  2.4731e-01, -4.5020e-01,  7.8247e-02, -5.4492e-01,
        -7.5781e-01,  3.2104e-01, -1.3794e-02,  5.7812e-01, -5.2979e-02,
         4.4287e-01,  6.2012e-01,  1.8301e+00,  6.6016e-01, -2.8125e-01,
        -6.2305e-01,  2.8625e-02, -2.2412e-01,  2.8809e-02, -9.7559e-01,
         1.7578e-02, -4.5166e-02, -3.1982e-01,  2.3779e-01, -5.3711e-02,
        -5.1172e-01, -1.3398e+00, -5.9375e-01, -3.3057e-01, -2.5000e-01,
         1.2036e-01,  6.1816e-01, -4.7266e-01,  1.6963e+00,  7.5977e-01,
        -1.6748e-01, -5.4297e-01, -5.7129e-02,  4.3457e-01,  2.7695e+00,
         1.3733e-02, -1.0742e-01,  4.7363e-02, -9.2578e-01,  2.7515e-01,
        -1.1426e-01,  8.3740e-02,  2.8906e-01, -2.8564e-02,  2.9736e-01,
         4.1016e-02,  3.7598e-01,  1.5273e+00,  9.4727e-02,  2.4756e-01,
         1.5078e+00, -5.9180e-01, -3.4082e-01,  6.1230e-01,  2.9834e-01,
         1.2329e-02,  2.6782e-01, -1.4014e-01,  3.6865e-02,  3.8770e-01,
         7.8125e-03,  5.7812e-01, -6.3867e-01, -9.8633e-01,  7.3364e-02,
        -4.4336e-01, -8.0078e-01, -8.0469e-01,  2.3120e-01, -2.4512e-01,
        -1.2549e-01,  4.3823e-02, -2.4231e-02,  3.3887e-01,  9.2383e-01,
         1.3809e+00, -5.5859e-01, -9.2969e-01,  2.7246e-01,  4.8401e-02,
        -5.9766e-01, -6.4648e-01, -6.0181e-02,  1.2695e-02, -4.5410e-01,
        -5.8789e-01, -9.6924e-02,  5.5664e-02, -5.4883e-01,  2.6672e-02,
         3.1348e-01, -2.9297e-01, -4.6753e-02, -5.6836e-01,  2.5684e-01,
        -7.9492e-01, -7.2070e-01, -4.8828e-01,  1.2695e-01, -2.0264e-01,
        -4.6484e-01,  1.9385e-01,  1.7041e-01,  2.1460e-01, -6.6211e-01,
        -3.8574e-01, -5.7520e-01,  8.1641e-01,  3.1250e-01,  9.2676e-01,
         8.5938e-01, -5.9814e-02, -5.9668e-01, -1.0430e+00,  8.6719e-01,
        -4.0137e-01,  1.7285e+00,  1.7891e-03, -4.6094e-01,  3.0334e-02,
        -1.0293e+00, -7.0117e-01,  5.6348e-01,  1.4648e+00,  3.9600e-01,
         1.9189e-01, -9.3750e-01, -1.7236e-01,  4.3091e-02, -8.5156e-01,
         2.1562e+00, -1.6875e+00, -1.5625e-02, -1.1973e+00, -7.8857e-02,
        -9.2773e-01, -3.6963e-01, -3.8086e-01,  6.5039e-01,  7.7344e-01,
        -9.1406e-01, -4.9414e-01, -8.5742e-01, -3.6426e-01, -6.0059e-01,
        -6.4062e-01, -6.4844e-01, -2.4805e-01,  6.8359e-02, -4.5117e-01,
         1.8066e-02,  2.3633e+00, -6.6211e-01, -5.3809e-01, -9.8438e-01,
         2.2754e-01,  3.6011e-02, -6.2891e-01,  4.2297e-02,  5.0171e-02,
        -1.4648e-01,  2.3145e-01, -4.0918e-01, -7.5195e-01, -6.0742e-01,
         1.4727e+00,  1.9092e-01, -1.9727e-01,  9.1797e-01, -1.1602e+00,
        -1.5381e-02,  7.5928e-02, -3.3496e-01,  4.7422e+00, -4.0625e-01,
         4.0161e-02, -5.5859e-01,  1.2598e-01, -6.3477e-01, -2.5830e-01,
        -3.1543e-01, -1.3926e+00,  2.4658e-02, -4.6191e-01,  3.3035e-03,
         7.1289e-01,  3.0957e-01, -7.4951e-02,  4.7607e-03, -4.5898e-01,
        -1.1562e+00,  1.3855e-02,  1.3223e+00, -1.0156e-01, -6.6016e-01,
        -4.4727e-01, -7.5586e-01, -3.1738e-01,  1.0391e+00,  1.4771e-01,
        -1.9043e-02, -6.2500e-02, -9.0820e-02,  8.5693e-02, -2.0508e-01,
        -5.1367e-01,  3.0615e-01,  1.3984e+00, -6.9531e-01,  3.1055e-01,
        -4.5020e-01,  6.2695e-01, -3.6426e-01, -6.5820e-01, -1.0693e-01,
         7.4951e-02, -8.5938e-01,  2.3633e-01, -9.7266e-01, -1.5723e-01,
        -1.3398e+00,  1.1963e-01,  1.6260e-01, -2.3193e-01, -7.2461e-01,
         2.7954e-01, -9.0234e-01, -5.6396e-02,  1.4014e-01,  1.6270e+00,
        -1.2970e-02, -3.3789e-01, -5.6250e-01, -4.5703e-01,  3.6914e-01,
        -2.2852e-01,  4.6539e-03, -3.5645e-02,  1.6621e+00, -5.9766e-01,
        -4.8828e-01,  1.1987e-01, -2.2754e-01,  1.2207e-01, -3.3398e-01,
        -1.0864e-02,  5.8203e-01, -4.8633e-01, -6.9580e-02, -4.2188e-01,
        -4.9219e-01, -4.4824e-01, -8.8867e-02, -9.4482e-02,  9.1797e-02,
        -2.2461e-01, -1.4258e-01,  2.1289e+00, -5.0000e-01, -2.7344e-01,
        -1.8994e-01,  1.4277e+00,  2.7783e-01, -7.3047e-01, -2.8418e-01,
         3.7812e+00,  5.5957e-01,  1.2734e+00,  9.9121e-02, -2.5146e-02,
        -4.4629e-01, -2.6562e-01, -6.9336e-01, -7.5586e-01, -3.7305e-01,
         6.1035e-02, -3.4180e-01, -5.2051e-01, -7.8516e-01, -5.4688e-02,
        -1.4990e-01, -3.8184e-01, -2.7441e-01, -3.2422e-01, -6.0742e-01,
        -1.6406e-01,  1.7334e-01, -5.2539e-01, -4.0649e-02,  1.3887e+00,
        -8.6914e-02, -3.2593e-02,  4.6631e-02,  6.9214e-02, -4.6289e-01,
        -4.0625e-01, -5.3711e-02,  9.7656e-02,  1.3184e-02,  1.2852e+00,
         1.5820e+00, -5.8301e-01, -5.3320e-01,  4.4922e-01,  5.4346e-01,
         9.0283e-01, -7.3242e-01, -6.5820e-01,  8.5547e-01, -1.1680e+00,
         6.2988e-02,  6.8750e-01,  6.9336e-02, -4.8438e-01,  1.0986e+00,
         7.3828e-01,  1.7051e+00,  3.1250e-02, -4.9805e-01,  6.6016e-01,
        -1.0120e-01,  3.2715e-01,  3.2764e-01,  2.6025e-01, -8.7402e-02,
        -6.3867e-01, -7.1289e-01, -8.3496e-02, -8.3789e-01, -1.2939e-02,
        -1.1113e+00, -5.3418e-01, -2.9004e-01,  3.7305e-01, -1.2134e-01,
         7.5391e-01,  1.1719e-02,  3.9551e-01,  1.7737e-01, -2.3804e-02,
         1.3281e+00, -4.8047e-01,  1.6943e-01, -6.8359e-01, -1.6357e-01,
        -2.1777e-01,  2.4854e-01, -4.4141e-01,  1.2832e+00,  1.0801e+00,
         3.4414e+00, -6.1328e-01,  1.3623e-01, -4.2480e-02,  6.0059e-01,
         4.1992e-01, -8.6719e-01, -4.7363e-01, -6.5820e-01, -6.5234e-01,
        -1.1113e+00,  1.6094e+00], device='cuda:0', dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([-9.1016e-01, -5.1953e-01,  6.2109e-01,  9.7900e-02,  1.0986e-01,
        -1.0590e-02, -2.5293e-01, -4.3359e-01,  1.8750e-01, -2.4121e-01,
         6.3867e-01, -4.5508e-01, -1.2793e-01,  1.9580e-01,  2.6172e-01,
         3.5767e-02,  1.1426e-01,  6.2305e-01,  2.8027e-01,  4.8828e-01,
        -2.9297e-01, -1.4375e+00,  7.7148e-01,  1.1055e+00,  8.1250e-01,
        -2.6172e-01, -7.4219e-01,  2.5488e-01, -1.4111e-01, -2.6562e-01,
        -2.2031e+00, -4.1602e-01,  4.7070e-01,  2.4141e+00,  6.3281e-01,
        -2.5195e-01,  7.6953e-01, -1.0039e+00,  2.6074e-01, -6.0547e-01,
         8.5938e-01, -5.1562e-01, -8.6328e-01, -4.5703e-01, -4.9805e-01,
        -6.0547e-01, -6.0938e-01, -3.6914e-01,  4.3555e-01,  7.6953e-01,
        -7.3242e-01, -5.4102e-01,  2.8613e-01, -8.2617e-01,  1.3594e+00,
        -7.4609e-01, -1.8848e-01, -4.6484e-01, -2.1912e-02, -3.9453e-01,
        -2.0469e+00,  6.0547e-01,  9.2578e-01,  3.9648e-01,  3.0273e-01,
        -5.0391e-01,  2.0117e-01, -1.0059e-01,  8.5144e-03,  3.0078e-01,
        -1.7041e-01,  4.7070e-01, -2.3438e-01, -4.9023e-01,  1.2773e+00,
         3.2471e-02, -1.1475e-01, -1.1914e-01, -2.2705e-02, -1.2109e+00,
         4.0405e-02,  4.9023e-01,  8.6719e-01, -4.4678e-02,  5.0977e-01,
        -1.7109e+00,  5.0195e-01, -2.2344e+00, -2.1387e-01, -6.5625e-01,
         3.0469e-01,  4.7461e-01, -1.3184e-01,  4.8125e+00,  5.1172e-01,
         1.2773e+00,  1.9043e-01,  5.7617e-01, -1.6797e+00, -1.7929e-03,
         4.0332e-01,  7.3828e-01, -7.5000e-01, -8.8281e-01, -1.3031e-02,
        -1.2061e-01,  2.0557e-01, -2.3203e+00, -3.6719e-01, -9.1797e-01,
         7.2656e-01,  3.4570e-01, -1.1484e+00, -6.5918e-02,  2.4414e-01,
         4.4141e-01, -1.0859e+00,  2.2578e+00, -2.0020e-01,  8.3789e-01,
        -1.1641e+00,  3.0078e-01,  7.5586e-01, -1.4941e-01, -4.6539e-03,
         2.8711e-01,  2.3359e+00, -1.2451e-01,  5.4688e-01, -6.9141e-01,
        -2.2363e-01,  9.1016e-01,  6.0742e-01,  1.3633e+00, -7.8125e-02,
         3.2910e-01,  2.7930e-01, -3.3887e-01, -1.3721e-01,  5.1953e-01,
        -2.2363e-01,  8.5693e-02, -1.1914e-01, -4.6875e-01, -2.5977e-01,
         3.3594e-01,  1.0352e+00, -1.1172e+00, -4.0234e-01,  4.1309e-01,
         3.9941e-01, -5.4492e-01, -5.7129e-02,  2.8594e+00,  3.4863e-01,
        -4.9023e-01, -8.9844e-01, -1.7656e+00,  9.4141e-01, -2.1191e-01,
        -4.2969e-01,  1.9897e-02, -1.4795e-01,  4.3555e-01, -2.9219e+00,
        -5.2539e-01, -6.1279e-02, -4.2773e-01, -3.0957e-01,  1.2109e-01,
         4.8438e-01, -1.0000e+00, -3.9453e-01,  4.0723e-01,  7.0117e-01,
        -2.3633e-01, -1.0117e+00, -2.4707e-01,  9.1562e+00,  2.1953e+00,
        -1.8701e-01,  2.8809e-01, -3.9160e-01,  6.3672e-01,  4.7500e+00,
         2.6367e-02, -9.8047e-01, -1.7871e-01,  3.4668e-01,  5.6836e-01,
         6.8359e-01, -8.7646e-02, -7.1484e-01, -2.2363e-01, -6.3477e-01,
        -8.6426e-02, -6.7383e-01,  1.8594e+00, -8.0469e-01,  3.2422e-01,
         3.1406e+00,  3.1152e-01,  4.1211e-01,  1.3438e+00,  4.5312e-01,
        -3.7354e-02, -1.3594e+00, -9.3262e-02,  3.2104e-02,  4.2969e-01,
         5.7812e-01, -1.2617e+00,  3.3203e-01,  4.8242e-01, -1.4600e-01,
        -6.9922e-01,  3.5547e-01, -1.0547e+00, -5.6445e-01, -1.6992e-01,
        -3.4082e-01, -2.4609e-01, -3.6499e-02, -3.0566e-01, -2.4297e+00,
        -3.1250e+00, -2.6855e-01, -7.5586e-01,  3.3203e-01,  8.0078e-02,
         4.1211e-01,  3.9355e-01,  2.2949e-01, -5.4102e-01,  3.6328e-01,
        -4.2773e-01,  3.5059e-01,  2.2852e-01, -1.1797e+00,  4.8096e-02,
         1.0508e+00,  3.7305e-01,  1.5918e-01, -2.2559e-01,  8.7109e-01,
        -6.9336e-01,  3.5449e-01, -3.9160e-01, -7.1680e-01, -1.6602e-01,
         4.4922e-01,  7.4219e-01,  3.0176e-01,  5.6445e-01, -3.9746e-01,
        -6.6797e-01,  3.5059e-01, -1.5312e+00, -2.0938e+00, -1.0586e+00,
         8.4766e-01,  1.4111e-01,  1.0117e+00,  1.2891e+00,  3.4531e+00,
        -3.1055e-01,  2.5000e+00,  7.6904e-03, -3.5059e-01,  1.0449e-01,
         5.1562e-01,  5.2656e+00,  1.9844e+00, -1.9922e+00,  7.9883e-01,
         2.7051e-01,  5.8008e-01,  1.8262e-01, -4.4067e-02, -8.7500e-01,
         2.6094e+00,  1.4258e+00, -2.4707e-01,  1.5391e+00, -6.9336e-02,
        -5.4102e-01, -5.1562e-01,  1.4355e-01, -8.1641e-01,  1.9688e+00,
         3.7012e-01,  5.5078e-01, -5.7617e-01,  4.5703e-01,  2.8809e-01,
         6.1914e-01,  4.0625e-01,  7.5781e-01, -5.3223e-02,  2.8809e-01,
        -4.1602e-01, -1.7812e+00,  9.4141e-01,  2.8906e-01,  4.8242e-01,
        -5.8008e-01, -2.3535e-01,  3.7891e-01,  5.2979e-02,  1.4014e-01,
        -9.5312e-01, -1.3984e+00, -2.8418e-01,  5.0977e-01,  4.5703e-01,
         1.1641e+00,  4.0430e-01,  2.9785e-01, -6.9336e-01, -1.1016e+00,
         2.6172e-01,  5.1758e-01,  2.9492e-01,  3.7812e+00, -2.0166e-01,
        -4.8828e-01, -2.9492e-01, -8.6328e-01, -5.1562e-01, -2.8125e-01,
        -2.4316e-01, -6.5820e-01,  2.1667e-02, -2.4219e-01,  6.5674e-02,
        -9.0625e-01, -1.0742e+00,  6.6650e-02,  7.7148e-02, -3.9062e-01,
         4.9414e-01, -1.2891e-01,  8.8672e-01, -1.2305e-01, -1.5664e+00,
         1.9678e-01,  9.5312e-01,  1.8311e-01,  1.2969e+00,  4.5508e-01,
        -1.0000e+00, -6.1094e+00, -3.8477e-01, -3.0371e-01,  3.8867e-01,
        -2.7344e-01, -7.6172e-01, -5.3125e+00, -5.4492e-01,  4.1113e-01,
        -2.0996e-01, -6.5625e-01,  2.8516e-01,  5.3125e-01,  4.7070e-01,
         1.9873e-01,  5.0000e-01, -1.0312e+00,  4.3945e-01, -2.7246e-01,
        -1.0703e+00,  2.4707e-01,  9.4531e-01, -9.5459e-02,  5.1562e-01,
        -8.0664e-01, -5.2148e-01,  1.8506e-01, -1.5820e-01, -3.6406e+00,
        -1.4038e-02,  2.3730e-01, -3.2422e-01, -2.6172e-01,  3.2812e-01,
         1.3135e-01, -6.5186e-02, -5.1562e-01,  1.5195e+00, -2.7734e-01,
        -1.2266e+00,  5.4443e-02, -2.3242e-01, -1.3721e-01, -4.0234e-01,
         9.0332e-02, -4.2188e+00,  3.7207e-01, -5.8594e-02,  2.8223e-01,
         4.9219e-01, -2.5781e-01,  1.6846e-01,  1.4697e-01, -3.1445e-01,
         1.1572e-01,  4.1797e-01, -4.7188e+00,  7.8320e-01, -1.9375e+00,
         2.2949e-01, -1.7812e+00, -7.0117e-01, -2.7051e-01,  1.6113e-01,
         6.7500e+00, -1.2852e+00,  2.0547e+00, -1.7480e-01, -1.3281e-01,
        -3.5059e-01, -4.5898e-01,  4.3945e-01, -6.0547e-01, -7.6367e-01,
         5.3223e-02, -3.2422e-01,  5.9570e-01,  4.5703e-01, -6.3281e-01,
        -7.8857e-02,  4.8828e-01, -1.7041e-01,  1.7773e-01, -4.1797e-01,
         2.2422e+00, -5.9180e-01, -3.1738e-01, -2.4121e-01,  1.6797e+00,
        -4.3164e-01, -9.1553e-02,  4.0527e-01, -5.3711e-02, -4.4922e-01,
         3.0957e-01, -9.9854e-02,  6.7871e-02, -3.3496e-01,  1.4531e+00,
         6.5625e-01,  2.0947e-01,  4.3555e-01, -8.3594e-01,  6.8555e-01,
        -1.6172e+00,  3.6426e-01,  4.2578e-01, -3.5938e-01, -6.0938e-01,
        -3.0859e-01, -1.2969e+00,  7.5684e-02,  3.3203e-01, -2.1328e+00,
        -1.5000e+00,  2.4688e+00,  3.5645e-02, -2.3633e-01,  1.2461e+00,
        -1.0449e-01,  5.3516e-01,  2.9199e-01,  2.7148e-01,  7.0312e-02,
         5.5859e-01, -3.7012e-01,  1.0742e-01, -3.5645e-01, -6.1340e-03,
         6.6992e-01, -2.4414e-01,  2.4609e-01,  9.5312e-01, -1.6602e-01,
        -1.2070e+00, -1.1523e-01,  4.9023e-01,  3.1934e-01, -4.8340e-02,
         9.2578e-01, -5.2734e-01,  5.7422e-01, -3.6523e-01,  1.7236e-01,
         4.0234e-01, -1.0859e+00, -2.6855e-01, -1.2188e+00,  1.3516e+00,
         6.6562e+00,  2.9688e-01, -2.6562e-01, -3.1543e-01,  7.4219e-01,
        -2.2031e+00, -8.2227e-01,  5.1758e-01, -2.5098e-01,  3.4961e-01,
         5.4062e+00, -2.5078e+00], device='cuda:0', dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.1724,  0.1626,  0.0784,  ..., -0.4067, -0.7427, -0.2292],
        [ 0.9331, -0.2539,  2.1719,  ..., -2.3105, -2.1367, -1.0488],
        [-0.3271, -0.3894,  0.2124,  ..., -0.7710,  0.8638, -0.9336],
        ...,
        [-0.0806, -1.0508,  1.7256,  ...,  0.6597, -1.6279,  2.0215],
        [ 0.1162,  0.0239,  0.0125,  ...,  0.7617, -0.0906,  0.6045],
        [ 0.0029, -0.1875,  0.8623,  ..., -1.9473, -0.8315, -0.5186]],
       device='cuda:0', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([  3.4883,   1.5938,  -2.8359,   0.5898,  -7.3516,   1.8828,  -2.2656,
         -0.6484,  -0.4531,   1.2500,   4.6094,  -5.7031,  -5.8516,   7.2344,
          3.3750,   7.4062,  -1.6797,   4.5469,  -4.0312,  -3.2812,   2.0117,
         -1.5312,   4.7500,   1.1250,   3.6562,  -1.0352,  -3.1250,  -2.1719,
         -2.6797,  -2.1562,  -0.6211,   3.5156,   6.5469,   0.1875,  -1.9453,
         -2.6562,   1.7344,  -0.3281,  18.6094,  -0.7344,   1.1797,   1.4609,
         -4.4688,  15.4062,  -2.7266,   3.2500,   1.8906,   1.0469,  -3.1641,
          4.8594,  -0.6484,   0.7344,  -6.4375,   0.4219,   3.2344, -11.6719,
          2.8438,   1.3906,   2.0234,   0.7656,  -1.2188,   0.3750,   0.2969,
         -5.1562,   4.2031,  -0.4766,   2.5938,   8.7891,   4.1094,  -3.8750,
         -6.0938,  -6.2031,  -1.6777,   4.5391,  -4.9531,  -3.1016,   1.8438,
          4.9688, -12.9844,   1.2891,  -4.2031,  -2.1719,   0.5547,   1.4688,
         -2.3438,  -1.1055,  -0.7891,   1.3516,   4.1953,   4.1719,   2.3281,
         -4.1250,   0.6094,  -6.5625,  -2.3594,  -0.0938,  -7.5469,  -2.3125,
         -8.0156,   2.8750,  -1.1250,  -1.1719,   0.3037,   1.3438,   0.2188,
         -1.0586,   0.4844,   2.1406,   1.6875,   0.9609,  -1.4219,   4.2188,
         -0.6562,   3.0938,  -0.9961,   0.5625,  -0.4688,   1.0625,   2.0156,
          0.2500,  -0.9766,   0.1953,   2.0156,  -0.3672,  -5.9375,   6.7031,
         -4.9688, -13.9688,  -2.2539,  -2.9688,  -3.2812,  -5.8750,  -0.6094,
         -5.6719,   2.9297,   2.5625,   0.2188,   4.2031,  -0.3242,  -4.5625,
          2.9844,  -1.3438,  -2.8281,   3.0156,   0.5000,   0.2969,  -0.1836,
          3.1875,   0.6719,   2.2891,  -6.3438,   3.2734,  -4.1875,  -0.5625,
          6.9258,   2.0938,   4.0859,   0.8125,   2.6562,   0.6250,   0.8633,
          3.2109,  -5.0625,  -0.8125,   2.3672,  -2.4688,   0.9375,  -2.2500,
         -3.6406,  -2.8125,  -5.9219,   1.5469,  -0.7578,  -2.9531,   2.5312,
          4.5938,  -1.4062,  -2.3047,  -5.0938,   1.4844,   2.9219, -12.0625,
          1.4375,   0.0938,   8.9297,  -2.5156,   1.3125,   1.6875,   2.7109,
        -10.5312,  -0.3027,  -3.4688, -10.7500,  -0.7031,   2.7812,  -3.8398,
        -10.8438,  -5.0859,   2.8281,   1.2266,  -0.1250,   4.4531,  -8.7031,
         -1.9766,  -4.7031,   8.3906,  -2.2500,   2.8750, -14.4375,  -2.5312,
        -11.7344,  -8.8125,  -0.2969,   6.1562,  -8.6875,   0.3594,  -3.6406,
          0.1016,  -1.1250,  -6.4688,  -4.9844,   3.6816,   4.2812,   0.0811,
          3.0625,   0.6719,   7.9531,  -4.9297, -11.8281,   2.1875,  -2.7188,
          4.8750,   4.9062,  -0.5312,  -0.2969,   1.3672,  -7.0312,   1.5469,
          1.9219,   3.0898,  -3.6094,  -4.7344,  -3.0625, -19.7500,  -7.3125,
          3.3125,  -5.9688,   0.3750,  -0.5000, -16.3906, -12.8125,  -0.2695,
         -7.5312,  -6.4219,   0.4336,  -2.7969], device='cuda:0',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 0.0355,  0.0613, -0.0469,  ..., -0.0413,  0.0117, -0.0029],
        [ 0.0126,  0.0050, -0.0272,  ..., -0.0621, -0.1294, -0.0121],
        [-0.0214, -0.0120,  0.0361,  ...,  0.0190,  0.0986,  0.0135],
        ...,
        [-0.0011, -0.0056,  0.0041,  ...,  0.0677,  0.0931, -0.0056],
        [-0.2817, -0.4180,  0.3613,  ...,  0.1309,  0.0703,  0.0776],
        [ 0.1434,  0.1377, -0.1675,  ...,  0.0254, -0.0859, -0.0334]],
       device='cuda:0', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([-1.6016e-01, -1.5796e-01,  1.5137e-01, -5.9937e-02,  5.8289e-02,
        -7.5684e-02, -1.2305e-01, -7.9102e-02,  8.1299e-02, -8.9722e-02,
         1.4551e-01, -5.3711e-02, -2.1240e-02,  9.9243e-02,  2.1484e-02,
        -1.7090e-02,  4.4067e-02,  8.2031e-02,  1.0156e-01,  6.3232e-02,
        -6.7505e-02, -2.5879e-01,  9.9609e-02,  1.8164e-01,  1.9580e-01,
        -2.1240e-02, -1.2402e-01,  6.7688e-02, -7.2571e-02, -5.5664e-02,
        -3.3008e-01, -2.8809e-02,  8.3496e-02,  4.5703e-01,  1.3867e-01,
        -6.0547e-02,  1.7090e-01, -2.4316e-01,  4.4189e-02, -1.6553e-01,
         1.6797e-01, -1.1475e-01, -1.6309e-01, -8.4961e-02, -9.3750e-02,
        -1.6113e-01, -1.4307e-01, -1.1377e-01,  6.0303e-02,  1.2695e-01,
        -1.9043e-01, -1.0889e-01,  1.7090e-03, -1.3477e-01,  1.8555e-01,
        -1.6895e-01, -5.1514e-02, -1.8701e-01, -9.4299e-03, -1.5039e-01,
        -3.3984e-01,  1.5283e-01,  1.0059e-01,  7.7393e-02,  5.6641e-02,
        -1.2256e-01,  8.7341e-02, -8.5083e-02, -1.3580e-02,  5.8350e-02,
        -1.2170e-01,  1.0583e-01, -2.7100e-02, -1.6772e-01,  1.6016e-01,
        -2.3865e-02, -6.3171e-02, -6.6406e-02,  6.0791e-02, -2.2363e-01,
         1.6968e-02,  1.0669e-01,  1.3867e-01, -5.2063e-02,  7.8613e-02,
        -3.4961e-01,  1.1548e-01, -3.9648e-01, -5.5908e-02, -1.0303e-01,
         7.5684e-02,  1.4111e-01, -3.8147e-02,  9.3750e-01,  1.2817e-01,
         1.9531e-01,  1.0211e-01,  1.3745e-01, -1.2695e-01, -2.8229e-02,
         9.0698e-02,  2.1289e-01, -8.8867e-02, -2.0312e-01, -9.1553e-05,
        -3.7842e-02,  6.2073e-02, -4.1797e-01, -1.0034e-01, -1.8066e-01,
         1.0352e-01,  1.0284e-01, -1.8848e-01,  8.6670e-03,  2.2644e-02,
         5.6641e-02, -2.3047e-01,  3.8086e-01, -5.7861e-02,  1.6016e-01,
        -2.5684e-01,  7.4219e-02,  2.0117e-01, -7.1533e-02, -4.6570e-02,
         1.8433e-02,  4.3555e-01,  7.3242e-04,  1.0449e-01, -1.1523e-01,
        -5.2734e-02,  1.9531e-01,  1.4648e-01,  3.0566e-01,  2.5269e-02,
         7.5684e-02,  5.6763e-02, -3.8574e-02, -1.1450e-01,  6.8115e-02,
        -7.9956e-02, -2.6093e-02, -7.2693e-02, -1.3818e-01, -1.2195e-01,
         8.7097e-02,  1.7578e-01, -1.3965e-01, -9.4238e-02,  1.1169e-01,
         9.3506e-02, -8.8379e-02, -6.8604e-02,  6.0156e-01, -4.8828e-04,
        -1.2012e-01, -1.5039e-01, -2.6953e-01,  1.1719e-01, -5.2490e-02,
        -1.1011e-01, -5.8411e-02, -8.1665e-02,  2.4902e-02, -5.7812e-01,
        -1.4648e-01, -8.4229e-02, -1.1670e-01, -5.2734e-02,  3.5217e-02,
         1.1426e-01, -2.4121e-01, -1.3672e-01,  4.7363e-02,  5.9570e-02,
        -8.3496e-02, -2.0312e-01, -9.3262e-02,  2.2969e+00,  4.3555e-01,
        -6.8970e-02,  5.9113e-02, -1.0889e-01,  1.5381e-01,  1.0234e+00,
        -3.0579e-02, -1.9629e-01, -6.3232e-02,  7.5562e-02,  1.8555e-01,
         1.3867e-01,  9.5215e-03, -1.8164e-01, -8.8379e-02, -1.5186e-01,
         5.3711e-03, -1.4551e-01,  3.4375e-01, -1.3281e-01,  5.9082e-02,
         5.7422e-01,  5.4382e-02,  1.1401e-01,  2.0703e-01,  6.9336e-02,
         4.9194e-02, -3.3008e-01, -4.1779e-02, -2.4658e-02,  1.1768e-01,
         5.8105e-02, -2.1973e-01,  9.8145e-02,  1.0547e-01,  4.8828e-04,
        -1.4795e-01,  6.0059e-02, -1.8262e-01, -1.4697e-01, -8.3374e-02,
        -3.9062e-02, -2.2217e-02, -6.1066e-02, -3.6621e-02, -4.5898e-01,
        -4.9609e-01, -5.9082e-02, -1.8115e-01,  8.3252e-02, -8.1329e-03,
         1.0339e-01,  1.0913e-01,  7.5684e-03, -1.2939e-01,  1.0730e-01,
        -9.3750e-02,  2.4414e-02,  4.5898e-02, -2.5391e-01, -4.6814e-02,
         1.5430e-01,  8.5449e-02,  5.5664e-02, -1.1371e-01,  1.6992e-01,
        -1.7920e-01,  4.2358e-02, -1.2378e-01, -1.9824e-01, -6.8787e-02,
         9.1309e-02,  1.0156e-01,  2.4414e-04,  9.1797e-02, -1.7993e-01,
        -1.4697e-01,  9.7534e-02, -2.2656e-01, -3.6914e-01, -1.3477e-01,
         1.3477e-01,  8.3984e-02,  1.2891e-01,  2.4805e-01,  7.8516e-01,
        -1.3623e-01,  2.3438e-01, -1.1658e-02, -1.1230e-01,  2.9846e-02,
         1.3818e-01,  1.1953e+00,  2.5781e-01, -2.7148e-01,  1.4990e-01,
         7.5562e-02,  1.1084e-01,  7.8247e-02,  2.9053e-02, -1.9922e-01,
         4.4141e-01,  2.9980e-01, -2.4414e-02,  2.5977e-01, -1.0443e-01,
        -1.1377e-01, -9.2773e-02,  6.8970e-02, -8.3008e-02,  2.9688e-01,
         1.0645e-01,  1.3721e-01, -1.2988e-01,  1.0278e-01,  5.7587e-02,
         1.2012e-01,  1.1072e-01,  1.3623e-01, -5.3833e-02, -2.1973e-03,
        -7.9102e-02, -4.2188e-01,  1.5625e-01,  9.9487e-02,  1.1694e-01,
        -3.3691e-02, -1.1841e-01,  8.2642e-02, -2.0111e-02, -2.6978e-02,
        -1.7578e-01, -1.7383e-01, -1.1816e-01,  1.2378e-01,  1.1377e-01,
         2.1289e-01,  4.3213e-02,  6.9214e-02, -2.5391e-02, -2.4023e-01,
         5.2002e-02,  3.7109e-02,  3.7842e-03,  7.5781e-01, -7.5684e-02,
        -6.7871e-02, -1.0352e-01, -2.0801e-01, -1.5479e-01, -1.2134e-01,
        -5.4443e-02, -2.0605e-01,  1.2848e-02, -1.0864e-01,  2.5879e-02,
        -1.5039e-01, -1.6797e-01,  6.8970e-02,  4.5807e-02, -9.6191e-02,
         9.8145e-02, -1.0754e-01,  1.4648e-01,  5.2490e-03, -3.1055e-01,
         1.0791e-01,  1.3281e-01,  9.2896e-02,  1.8066e-01,  2.9785e-02,
        -1.0156e-01, -1.5391e+00, -1.3940e-01, -3.7598e-02,  9.5947e-02,
        -1.1401e-01, -1.6699e-01, -1.2969e+00, -1.5088e-01,  1.0254e-02,
        -5.2612e-02, -1.0742e-01,  7.2876e-02,  8.9844e-02,  1.2671e-01,
         1.2207e-04,  1.0938e-01, -1.5625e-01,  1.0840e-01, -1.3989e-01,
        -2.2949e-01, -2.0508e-02,  1.4258e-01, -7.2021e-02,  8.2275e-02,
        -1.7188e-01, -1.0840e-01,  4.3335e-02, -1.0010e-02, -8.2031e-01,
        -5.9509e-04,  9.7534e-02, -8.0078e-02, -4.3945e-02,  9.2529e-02,
         1.3574e-01, -8.1787e-03, -1.1816e-01,  2.0898e-01, -8.0688e-02,
        -1.3672e-01, -7.2998e-02, -1.0901e-01,  2.1973e-02, -1.7456e-01,
         5.8350e-02, -1.2422e+00,  1.2146e-01, -6.2347e-02,  1.0980e-01,
         1.1133e-01, -6.4819e-02,  2.7100e-02,  3.9490e-02, -7.8369e-02,
         6.3721e-02,  3.3691e-02, -1.0156e+00,  1.4111e-01, -4.8047e-01,
         9.5215e-03, -3.8281e-01, -1.8848e-01, -8.9478e-02,  8.5083e-02,
         1.6406e+00, -2.5879e-01,  3.6133e-01, -1.2939e-02, -5.6152e-03,
        -1.0938e-01, -1.7627e-01,  1.0107e-01, -1.9043e-01, -1.8945e-01,
        -5.5054e-02, -6.7383e-02,  6.2500e-02,  1.1157e-01, -1.7383e-01,
        -4.9530e-02,  1.1182e-01, -1.1023e-01,  1.2866e-01, -1.4575e-01,
         4.1211e-01, -7.3730e-02, -9.2529e-02, -9.6436e-02,  2.8711e-01,
        -7.8125e-02, -9.7046e-02,  9.3994e-02,  4.8828e-04, -1.1719e-01,
         2.9297e-02, -4.0894e-02, -4.5044e-02, -6.5918e-02,  1.9531e-01,
        -4.3945e-02,  5.4077e-02,  1.0474e-01, -1.5234e-01,  1.1475e-01,
        -3.7500e-01,  9.8816e-02,  9.9609e-02, -3.2715e-02, -1.7676e-01,
        -3.9551e-02, -2.4219e-01, -7.0068e-02,  1.0168e-01, -3.4375e-01,
        -3.1641e-01,  3.9844e-01, -3.5889e-02, -3.9185e-02,  1.5625e-01,
         1.0986e-02,  5.2246e-02,  6.9824e-02,  1.4648e-02,  9.4116e-02,
         1.1987e-01, -1.0767e-01,  5.3253e-02, -6.6162e-02, -6.4575e-02,
         1.4404e-01, -6.3843e-02,  1.0132e-01,  1.1914e-01, -1.7822e-02,
        -1.9434e-01,  5.8594e-03,  5.4688e-02,  6.2500e-02, -6.1981e-02,
         2.4609e-01, -1.7041e-01,  7.0312e-02, -8.2764e-02,  6.7139e-02,
         1.0474e-01, -2.7051e-01, -7.1777e-02, -2.6074e-01,  2.2461e-01,
         1.7109e+00,  8.4839e-02, -1.4648e-02, -6.9092e-02,  9.9121e-02,
        -4.4727e-01, -1.7773e-01,  6.7383e-02, -7.6416e-02,  7.7026e-02,
         1.3438e+00, -5.5664e-01], device='cuda:0', dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[  8.7656,  17.2500,  -3.1094,  12.4375, -10.5312,  15.3125,  14.3438,
         -14.9062, -21.8750,   6.8125, -15.0000,  -5.9219,  -7.5156, -21.4375,
          -1.7734,  14.6250, -14.1250,   5.1406, -29.8750,  -0.0508,  19.3750,
          -3.5742,   7.9844,  11.7188,   1.0000,  -1.3828, -10.2812, -28.6875,
          28.3125,  19.0625, -13.5312,   4.9844, -35.6250,  11.4062,  -2.7969,
           6.6094,   3.1172, -19.8438, -15.6562,  16.3750, -21.2500,  -6.9922,
          -3.0469,   5.1094,   4.2344, -10.0156,  13.8125,  23.2500, -17.2812,
           6.5938,   2.2656,  -2.8984,  10.0312,  11.3047,  14.2812,  18.0000,
          18.0625,  19.4375,  19.9375,  18.3125,  -0.2812, -16.9062,   7.9375,
         -19.8438, -10.6562,  13.0625,  -1.6250,  20.0000,   8.3438, -15.0312,
          21.5625, -23.9375, -12.9062,  11.6250,   9.9688,  10.9375,  20.1250,
           5.7812, -12.5938,   4.2188, -10.6562, -24.3125,  -1.2188,   6.8438,
           0.3281, -13.2578, -17.8750, -13.8438,  10.1562,  -1.0039, -22.1875,
         -17.7500,  20.0312,   4.1094, -13.4062, -12.1562, -22.0000, -21.6250,
          -8.0000,  15.7188, -21.8750,  -0.1719,  -7.7500,  -0.4062,   4.4531,
         -12.8906, -14.3125, -11.8125,  22.5000,  -4.5625,  -1.1289, -25.6250,
          -9.3125,   8.2812, -24.1250,  -1.7344,   9.0156,  14.5938,  15.0312,
           5.3672, -12.7188,  -7.4375,   1.7812,  14.2812,   3.0938, -12.0625,
          13.4062,  -4.1875,  -9.3750,   3.2031,  22.3125,   7.6523, -14.8125,
         -10.0156,  -8.1250,  -9.3438, -24.5625,  -7.5391,  18.2500, -16.0625,
          16.1875,  -1.9219,  23.8125,   2.2344,  20.1875, -25.3125,   1.3027,
           3.2988,  -4.7344, -15.1562, -21.3438,  -9.9922,   1.6094,   1.4102,
           0.2358, -11.7500, -11.5625, -16.9844,  11.3750,  14.0938,  16.9375,
          16.8125,  16.1875,   3.1250,   3.2656,   1.8125,  11.6562,   9.3906,
         -12.0312,  -7.8125, -11.9375,  15.0312,  15.8750,  -8.2969,  -2.1152,
          -4.6719, -10.3750,  21.5312,   5.6406,   7.2500,  12.6875, -21.5000,
           4.2344,   7.1094,  13.6875,   4.6406,   1.0156,  -1.5156, -32.1250,
           4.5625,  -3.3438,  -9.9688,  -3.8125,   3.5938,  -4.2109,  -8.1250,
          -4.7891,  15.5312,  -3.3125,   7.3438,  10.1562, -20.7500, -12.7500,
           6.2344,   9.7969,  -6.5625,  -0.5391,  17.0938,  12.6875,  10.6406,
           1.8398,  -8.5938, -22.8125, -24.5625,  -9.1875,   5.6875, -25.0000,
           7.6875,  -5.1484,  16.7812,   2.8125,  -4.3438,  10.9375, -11.9375,
          -4.4219,  -9.1562,  22.8750,  14.5625,   9.2266,  10.1406, -16.9375,
         -18.8750,  -2.9844,   1.2500, -17.2500,  13.2500,  -5.6719,   4.3438,
           3.9844,   9.6562,   3.3750, -11.4062,  -6.0469,  27.3125,   2.7969,
          17.1250, -23.1250,  13.8750,  -0.1875,  13.4375, -13.4688,   6.0156,
          10.0625,   3.0469,  27.8750,   9.7500, -22.7500, -10.2500,  -4.3906,
         -11.4766,  16.7812,  -7.7812,  -5.4492,  -9.4375,   6.5781,  18.3125,
          12.7656,   5.0469,  18.0625,   5.6094, -23.6875,  -0.0938,   4.6875,
          -7.6719,   4.1523,   7.4844, -16.8125, -10.9062, -10.8125,  10.7969,
          15.8750, -14.9688,  -0.7969,  -8.8594,  16.5625,  19.0625,   6.7344,
         -33.8750, -12.4688,   3.9922, -27.3750, -13.7500,  14.9375, -11.9375,
         -25.3125, -11.3750, -21.9375,  -2.1016, -13.5781, -16.7500,  -2.1289,
         -18.8125,  -7.6562, -27.1250, -25.8125,  -7.5312,  -0.6328, -18.4062,
           8.6875,   6.2812,   3.7656,  -4.3438,  19.3750, -17.8125, -18.2500,
          18.5000,   9.2812, -10.8750, -20.6562,  11.5312,  -2.2656,   4.3281,
         -11.7188,  20.4531,  23.5312,   0.5000,  21.0000,   0.0781,  18.4375,
          14.6562,  14.2500,  26.9375,  12.2188,  23.1562,  -1.4766, -12.8438,
          -5.5469, -12.0625,  -1.0742,  13.1875, -26.1875,   1.0469,  19.4375,
           8.5000,   3.4141, -27.2500,  -8.2188, -20.3750,  11.8750,   6.7031,
          -2.1016,   1.8594,   5.4688,  -3.7500,  -6.1875,  20.3125,  -4.3477,
          -7.1250,  15.8438,   8.8750,  26.2500, -11.7969, -14.6250, -14.6875,
          -1.5781,   7.4688, -21.5625,  -5.2344, -24.8125,  10.2500,  13.1875,
           8.6250,   1.5625,  29.4375, -15.9062,  -5.1484,  19.9375,  -5.0156,
          -9.5000,  -9.4688,   9.3750, -16.3750,  20.1875,  20.8750,  12.9844,
         -20.4375,  -2.9375,  -0.6484,  15.0781,  24.3125,   2.9648,  25.5000,
          15.8750, -13.4062,  14.1250,  -4.0469,  -1.0469, -17.6875,  13.2188,
         -22.9375, -15.2500,  20.1875,  -5.0938,  -5.7344,  -1.5078, -21.4375,
          -1.5078, -10.5469,  -6.5156,   0.2695,  -8.2656, -12.9531,  -3.3125,
          30.9375, -19.7500,  12.2969,  -8.2656,  12.2812,  -9.7500,   4.4844,
          16.6250,  10.2500, -18.1250,  17.7500,   9.1562,  12.5000,  10.5312,
          -9.7734, -20.0625,   3.5000,  21.9375, -11.7812,  17.8125, -21.8125,
          16.9375,   1.2891,  -6.1406,  17.9375,   4.7656,  13.7188,   0.3984,
           6.7812,   0.4531, -19.3438,  14.5000, -15.0938,   4.7188,  15.3125,
          -2.2656,  14.4062,  32.2500, -32.6875, -13.5625,  -9.7500,   8.1094,
          -6.3047, -23.6250, -18.7500, -29.4375,  21.3125,  -5.0000,  -9.8438,
          14.1875, -19.1875, -10.3438,  -5.9844,   8.9297,  11.0625,  23.3750,
          10.2188,  10.5000,  10.0625,  12.5000,  15.6875, -13.6250, -12.4375,
          21.3125, -12.7812,  26.6250,  24.3750, -20.4375,  25.5000, -13.0000,
           8.0625,   7.7969,  -8.9219,  -0.3984,  13.3125,   5.3672,   9.0000,
          19.6875,  10.6875,   6.8125,  19.8750, -10.0625,  -8.1562,  -0.9219,
          17.8125, -14.2969,  13.2188,  12.2812, -24.0625,  -9.2500,   2.9531,
          13.7812,  -5.1562,  12.0625, -10.0625,  30.1875, -21.6250,  -1.3594,
         -12.6562]], device='cuda:0', dtype=torch.float16)
2023-09-02 21:49:41 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/crash.pt
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
task_net module.module.task_net.layer_norm.weight True tensor([-6.6895e-01, -6.9043e-01, -1.4917e-01,  1.2646e-01, -7.1899e-02,
        -2.2217e-02, -2.1582e-01,  2.6929e-01, -4.5557e-01, -1.5552e-01,
        -6.9238e-01,  1.4697e-01,  8.9966e-02, -4.8486e-01, -7.0923e-02,
         6.8481e-02, -1.7944e-01, -3.3325e-02, -6.4062e-01, -1.5332e-01,
        -6.0254e-01, -4.5898e-02,  4.9121e-01,  5.1221e-01,  3.3936e-02,
         2.5452e-02,  3.0664e-01, -4.7021e-01, -3.5400e-01, -4.7314e-01,
         1.0352e+00, -3.1958e-01, -8.5742e-01,  1.2227e+00, -1.2964e-01,
        -1.4380e-01,  2.1094e-01,  9.7461e-01, -2.8320e-01, -5.9570e-01,
        -1.0547e+00,  1.2646e-01,  1.3696e-01, -8.6792e-02, -2.6025e-01,
         3.8037e-01, -7.2070e-01, -8.1348e-01, -5.3027e-01,  1.9629e-01,
        -8.9355e-02,  4.3701e-02,  1.2256e-01, -5.5029e-01,  9.2578e-01,
        -1.0605e+00, -3.0859e-01, -6.0449e-01, -3.3630e-02, -5.1172e-01,
         1.9629e-01, -7.7734e-01,  3.3936e-01, -6.1621e-01, -3.2861e-01,
        -5.3027e-01, -9.2545e-03, -1.7358e-01,  8.7280e-03, -3.5889e-01,
        -3.4375e-01, -8.8672e-01,  1.6602e-01, -6.4551e-01,  6.9434e-01,
         2.3346e-02, -2.1191e-01, -7.3730e-02,  1.9196e-02, -4.0747e-01,
        -2.0325e-02, -9.9219e-01, -3.6548e-01, -1.2192e-02, -1.3477e-01,
         1.1885e+00, -8.3008e-01,  1.1172e+00, -2.2070e-01, -1.1365e-01,
        -3.8770e-01, -7.1094e-01, -1.8164e-01,  1.1055e+00, -7.4512e-01,
        -1.1406e+00, -3.1104e-01, -9.7070e-01,  7.0898e-01, -1.8139e-03,
        -7.4512e-01, -2.5146e-01,  2.9443e-01,  1.0156e-01, -6.5918e-03,
         6.1462e-02, -2.9004e-01,  1.3730e+00, -7.8711e-01,  2.3535e-01,
        -2.9883e-01, -6.2109e-01,  9.7266e-01, -5.2124e-02, -5.0928e-01,
        -1.6016e-01, -7.5098e-01,  1.4639e+00, -1.9312e-01,  1.2988e-01,
         6.6797e-01, -2.4316e-01,  6.8359e-03, -1.2183e-01, -1.5297e-03,
        -3.0273e-01,  1.1689e+00,  2.1179e-02, -4.9316e-01, -2.2461e-01,
        -4.4922e-01,  3.3667e-01, -8.0176e-01, -1.0254e+00,  3.8879e-02,
        -2.8467e-01, -6.2988e-01,  5.4626e-02, -2.4121e-01, -7.1484e-01,
        -2.2363e-01, -1.0513e-02, -2.5586e-01,  2.6001e-02, -4.4678e-01,
        -7.1191e-01,  2.3999e-01, -2.9248e-01,  1.0938e-01, -4.8047e-01,
        -7.2852e-01,  1.6699e-01, -2.7893e-02,  7.3438e-01, -1.1633e-01,
         3.3228e-01,  6.1182e-01,  1.2549e+00,  6.5137e-01, -2.8467e-01,
        -5.8594e-01,  2.8320e-02, -2.6660e-01,  4.0039e-02, -6.7822e-01,
        -8.1787e-02, -3.3813e-02, -2.6416e-01,  2.6587e-01, -6.2622e-02,
        -5.4492e-01, -1.1914e+00, -5.5469e-01, -2.5244e-01, -2.5122e-01,
         7.2754e-02,  4.6094e-01, -4.2578e-01, -1.7480e-01,  5.2832e-01,
        -1.9995e-01, -4.6777e-01, -4.4800e-02,  3.2227e-01,  2.1289e+00,
         1.5991e-02, -1.8359e-01,  3.7720e-02, -8.3496e-01,  5.4199e-02,
        -2.3633e-01,  8.8623e-02,  1.0010e-02, -7.3730e-02,  2.7612e-01,
         5.2612e-02,  2.7466e-01,  1.1641e+00,  5.4443e-02,  2.0532e-01,
         7.6562e-01, -5.7715e-01, -4.7949e-01,  5.3516e-01,  1.4343e-01,
         1.5167e-02,  1.0559e-01, -1.4355e-01,  4.3213e-02,  3.8232e-01,
        -1.0791e-01,  7.1582e-01, -6.1621e-01, -1.1094e+00,  4.1870e-02,
        -6.0156e-01, -8.4180e-01, -6.3086e-01,  1.2598e-01, -2.5146e-01,
        -1.7432e-01,  2.1149e-02, -1.8829e-02,  3.9551e-01,  6.9336e-01,
         1.0898e+00, -5.0781e-01, -7.3730e-01,  2.0923e-01,  3.3630e-02,
        -6.7773e-01, -5.8398e-01, -7.4219e-02, -1.0352e-01, -4.7314e-01,
        -6.8164e-01, -1.8433e-01,  8.5205e-02, -4.2773e-01,  2.1515e-02,
        -1.7773e-01, -3.7549e-01, -6.5918e-02, -5.8203e-01,  1.1694e-01,
        -8.5254e-01, -6.8848e-01, -5.0488e-01, -1.1719e-02, -1.6919e-01,
        -4.6094e-01, -7.6660e-02,  1.3330e-01,  8.8196e-02, -5.0293e-01,
        -3.8525e-01, -4.6582e-01,  7.2656e-01,  2.3486e-01,  6.4941e-01,
         6.2744e-01, -8.7891e-02, -5.0635e-01, -1.1074e+00,  4.1016e-01,
        -3.9551e-01,  9.4727e-01,  1.4763e-03, -4.8242e-01,  4.9988e-02,
        -8.9355e-01, -1.0635e+00,  3.9062e-01,  1.1289e+00,  1.7346e-01,
         1.6455e-01, -9.3945e-01, -1.7700e-01,  4.4434e-02, -7.0996e-01,
         1.7871e+00, -1.4102e+00, -7.7148e-02, -9.6680e-01, -7.1655e-02,
        -9.4336e-01, -2.6807e-01, -3.3740e-01,  6.5283e-01,  6.5723e-01,
        -7.6855e-01, -4.8926e-01, -8.1250e-01, -4.2041e-01, -5.6641e-01,
        -6.1230e-01, -6.0840e-01, -1.9092e-01,  5.8960e-02, -4.8535e-01,
        -6.1829e-02,  1.6738e+00, -6.6992e-01, -4.6094e-01, -8.9453e-01,
         2.9932e-01,  7.6904e-03, -6.4453e-01,  3.4058e-02,  3.6499e-02,
        -1.4990e-01,  4.3555e-01, -3.8770e-01, -7.2363e-01, -5.8691e-01,
         1.1855e+00,  1.5210e-01, -2.8223e-01,  8.1836e-01, -9.0039e-01,
        -5.4932e-02,  4.4556e-02, -3.5645e-01,  3.2188e+00, -3.4424e-01,
        -7.8430e-03, -5.8984e-01, -3.7598e-02, -7.0703e-01, -2.3462e-01,
        -3.3154e-01, -1.1875e+00,  2.8015e-02, -3.4180e-01,  7.7209e-03,
         5.0537e-01,  3.1445e-01, -8.9600e-02,  1.1749e-02, -3.8184e-01,
        -8.9062e-01,  3.5095e-03,  1.0293e+00, -1.1292e-01, -6.8262e-01,
        -4.2432e-01, -5.4004e-01, -3.3545e-01,  8.4570e-01,  9.8145e-02,
        -9.1064e-02, -7.0312e-01, -1.4453e-01, -5.4932e-04, -2.4463e-01,
        -5.0293e-01,  2.3291e-01,  1.1699e+00, -7.2266e-01,  3.3789e-01,
        -4.1943e-01,  4.7705e-01, -3.3838e-01, -4.4043e-01, -1.2671e-01,
         5.6152e-02, -7.7441e-01,  2.3340e-01, -1.0049e+00, -1.8604e-01,
        -1.2305e+00,  9.4482e-02, -1.2988e-01, -2.3340e-01, -6.1914e-01,
         1.0547e-01, -7.1533e-01, -8.5083e-02,  1.6431e-01,  6.3965e-01,
        -1.2177e-02, -3.3887e-01, -4.7754e-01, -4.0527e-01,  3.1494e-01,
        -3.0176e-01,  3.9520e-03, -1.3110e-01,  1.0713e+00, -5.2441e-01,
        -4.1113e-01,  1.1572e-01, -2.3047e-01,  1.2378e-01, -3.9941e-01,
        -1.0803e-02,  1.8164e-01, -4.2285e-01, -7.0679e-02, -4.3262e-01,
        -4.9219e-01, -4.3604e-01, -1.4380e-01, -8.9478e-02,  7.5928e-02,
        -2.2607e-01, -2.2314e-01,  4.6484e-01, -4.0674e-01, -4.9609e-01,
        -1.8042e-01,  1.0371e+00,  6.5186e-02, -5.7227e-01, -2.9785e-01,
         1.9961e+00,  4.6533e-01,  1.0166e+00,  6.9092e-02,  2.0874e-02,
        -4.8438e-01, -3.3008e-01, -7.3828e-01, -8.9453e-01, -3.9111e-01,
         7.4097e-02, -3.2373e-01, -4.0527e-01, -8.2031e-01, -2.6904e-01,
        -1.6406e-01, -3.5742e-01, -3.1055e-01, -3.0664e-01, -5.7715e-01,
        -1.0938e-01,  2.1362e-01, -5.6641e-01, -4.1504e-02,  9.3652e-01,
        -1.3232e-01, -3.1250e-02, -5.5664e-02,  5.3894e-02, -5.2637e-01,
        -3.5303e-01, -4.6326e-02,  1.2915e-01,  3.4546e-02,  1.1543e+00,
         1.7695e+00, -5.6445e-01, -5.3418e-01,  3.7012e-01,  4.0771e-01,
         3.1567e-01, -7.0703e-01, -5.8398e-01,  8.2227e-01, -9.6289e-01,
         9.9243e-02,  4.7461e-01,  6.6040e-02, -5.0684e-01,  6.4258e-01,
         7.9297e-01,  1.1914e+00,  4.1199e-02, -5.1514e-01,  4.9121e-01,
        -9.1003e-02,  3.4082e-01,  2.6392e-01,  2.5220e-01, -1.2720e-01,
        -7.0215e-01, -7.0117e-01, -7.6294e-02, -8.8672e-01, -1.1887e-02,
        -1.0703e+00, -4.8242e-01, -3.3594e-01,  6.9214e-02, -1.2195e-01,
         5.9863e-01, -5.5695e-03,  2.4341e-01,  9.0332e-02, -3.0518e-02,
         8.4277e-01, -5.2734e-01,  1.8652e-01, -5.7324e-01, -1.9458e-01,
        -2.7246e-01,  1.0754e-01, -4.1602e-01,  9.8535e-01,  8.0078e-01,
         1.9141e+00, -5.1758e-01,  1.6357e-01, -8.1787e-02,  4.5459e-01,
         1.4551e-01, -6.2402e-01, -3.9502e-01, -5.9668e-01, -5.9180e-01,
        -7.7100e-01,  1.2637e+00], device='cuda:2', dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([-9.7070e-01, -5.5566e-01,  6.6211e-01,  1.0425e-01,  1.1743e-01,
        -1.1292e-02, -2.7051e-01, -4.6289e-01,  1.9971e-01, -2.5684e-01,
         6.8359e-01, -4.8633e-01, -1.3647e-01,  2.0947e-01,  2.7979e-01,
         3.8025e-02,  1.2231e-01,  6.6504e-01,  2.9932e-01,  5.2051e-01,
        -3.1201e-01, -1.5332e+00,  8.2422e-01,  1.1816e+00,  8.6719e-01,
        -2.7979e-01, -7.9492e-01,  2.7197e-01, -1.5063e-01, -2.8369e-01,
        -2.3477e+00, -4.4434e-01,  5.0391e-01,  2.5703e+00,  6.7383e-01,
        -2.6807e-01,  8.2031e-01, -1.0742e+00,  2.7832e-01, -6.4648e-01,
         9.1797e-01, -5.4980e-01, -9.2188e-01, -4.8828e-01, -5.3125e-01,
        -6.4746e-01, -6.5039e-01, -3.9355e-01,  4.6484e-01,  8.2422e-01,
        -7.8320e-01, -5.7812e-01,  3.0420e-01, -8.8086e-01,  1.4473e+00,
        -7.9492e-01, -2.0068e-01, -4.9609e-01, -2.3346e-02, -4.2090e-01,
        -2.1836e+00,  6.4844e-01,  9.8828e-01,  4.2285e-01,  3.2422e-01,
        -5.3809e-01,  2.1387e-01, -1.0693e-01,  9.1248e-03,  3.2031e-01,
        -1.8188e-01,  5.0195e-01, -2.4951e-01, -5.2246e-01,  1.3672e+00,
         3.4668e-02, -1.2183e-01, -1.2695e-01, -2.4323e-02, -1.2871e+00,
         4.3091e-02,  5.2344e-01,  9.2578e-01, -4.7668e-02,  5.4395e-01,
        -1.8203e+00,  5.3613e-01, -2.3789e+00, -2.2852e-01, -7.0020e-01,
         3.2471e-01,  5.0586e-01, -1.4038e-01,  5.1328e+00,  5.4785e-01,
         1.3633e+00,  2.0361e-01,  6.1426e-01, -1.7969e+00, -1.9035e-03,
         4.3066e-01,  7.9102e-01, -7.9883e-01, -9.3750e-01, -1.3885e-02,
        -1.2842e-01,  2.1924e-01, -2.4805e+00, -3.9160e-01, -9.8047e-01,
         7.7539e-01,  3.6865e-01, -1.2227e+00, -7.0190e-02,  2.6074e-01,
         4.6875e-01, -1.1641e+00,  2.4102e+00, -2.1338e-01,  8.8867e-01,
        -1.2422e+00,  3.2080e-01,  8.0469e-01, -1.5918e-01, -4.9667e-03,
         3.0566e-01,  2.5000e+00, -1.3281e-01,  5.8301e-01, -7.3730e-01,
        -2.3877e-01,  9.7266e-01,  6.4844e-01,  1.4492e+00, -8.3252e-02,
         3.5107e-01,  2.9785e-01, -3.6084e-01, -1.4624e-01,  5.5664e-01,
        -2.3877e-01,  9.1431e-02, -1.2695e-01, -5.0000e-01, -2.7832e-01,
         3.5840e-01,  1.1055e+00, -1.1895e+00, -4.2871e-01,  4.4043e-01,
         4.2676e-01, -5.8203e-01, -6.0913e-02,  3.0547e+00,  3.7207e-01,
        -5.2344e-01, -9.5898e-01, -1.8750e+00,  1.0039e+00, -2.2607e-01,
        -4.5801e-01,  2.1271e-02, -1.5796e-01,  4.6582e-01, -3.1055e+00,
        -5.6055e-01, -6.5430e-02, -4.5508e-01, -3.3203e-01,  1.2891e-01,
         5.1562e-01, -1.0664e+00, -4.2090e-01,  4.3457e-01,  7.5098e-01,
        -2.5195e-01, -1.0801e+00, -2.6318e-01,  9.7656e+00,  2.3438e+00,
        -1.9922e-01,  3.0664e-01, -4.1797e-01,  6.8066e-01,  5.0547e+00,
         2.8198e-02, -1.0469e+00, -1.9092e-01,  3.6963e-01,  6.0742e-01,
         7.2852e-01, -9.3628e-02, -7.6367e-01, -2.3926e-01, -6.7969e-01,
        -9.2407e-02, -7.1875e-01,  1.9766e+00, -8.5742e-01,  3.4619e-01,
         3.3516e+00,  3.3301e-01,  4.4043e-01,  1.4336e+00,  4.8242e-01,
        -3.9795e-02, -1.4492e+00, -9.9365e-02,  3.4119e-02,  4.5898e-01,
         6.1523e-01, -1.3457e+00,  3.5352e-01,  5.1562e-01, -1.5576e-01,
        -7.4512e-01,  3.8037e-01, -1.1211e+00, -6.0059e-01, -1.8115e-01,
        -3.6426e-01, -2.6318e-01, -3.8940e-02, -3.2617e-01, -2.5820e+00,
        -3.3359e+00, -2.8613e-01, -8.0664e-01,  3.5400e-01,  8.5571e-02,
         4.3945e-01,  4.1992e-01,  2.4365e-01, -5.7812e-01,  3.8916e-01,
        -4.5605e-01,  3.7451e-01,  2.4463e-01, -1.2598e+00,  5.1270e-02,
         1.1230e+00,  3.9941e-01,  1.6992e-01, -2.4023e-01,  9.2773e-01,
        -7.3926e-01,  3.7842e-01, -4.1699e-01, -7.6270e-01, -1.7700e-01,
         4.8047e-01,  7.9492e-01,  3.2129e-01,  6.0254e-01, -4.2480e-01,
        -7.1191e-01,  3.7354e-01, -1.6367e+00, -2.2344e+00, -1.1250e+00,
         9.0625e-01,  1.5039e-01,  1.0781e+00,  1.3770e+00,  3.6875e+00,
        -3.3105e-01,  2.6719e+00,  8.2092e-03, -3.7549e-01,  1.1108e-01,
         5.4980e-01,  5.6094e+00,  2.1211e+00, -2.1250e+00,  8.5156e-01,
         2.8906e-01,  6.2012e-01,  1.9507e-01, -4.7119e-02, -9.3359e-01,
         2.7852e+00,  1.5254e+00, -2.6416e-01,  1.6406e+00, -7.3853e-02,
        -5.7617e-01, -5.4883e-01,  1.5332e-01, -8.6914e-01,  2.1016e+00,
         3.9453e-01,  5.8398e-01, -6.1719e-01,  4.8926e-01,  3.0811e-01,
         6.6016e-01,  4.3262e-01,  8.0859e-01, -5.6641e-02,  3.0811e-01,
        -4.4336e-01, -1.9023e+00,  1.0039e+00,  3.0859e-01,  5.1367e-01,
        -6.1816e-01, -2.5195e-01,  4.0527e-01,  5.6519e-02,  1.4917e-01,
        -1.0176e+00, -1.4941e+00, -3.0371e-01,  5.4395e-01,  4.8926e-01,
         1.2402e+00,  4.2969e-01,  3.1787e-01, -7.3828e-01, -1.1738e+00,
         2.7930e-01,  5.5078e-01,  3.1494e-01,  4.0547e+00, -2.1533e-01,
        -5.2148e-01, -3.1396e-01, -9.2188e-01, -5.4980e-01, -2.9980e-01,
        -2.6025e-01, -7.0410e-01,  2.3102e-02, -2.5830e-01,  7.0068e-02,
        -9.6484e-01, -1.1445e+00,  7.1167e-02,  8.2275e-02, -4.1602e-01,
         5.2637e-01, -1.3745e-01,  9.4922e-01, -1.3135e-01, -1.6680e+00,
         2.0947e-01,  1.0195e+00,  1.9531e-01,  1.3848e+00,  4.8633e-01,
        -1.0703e+00, -6.5156e+00, -4.1016e-01, -3.2275e-01,  4.1406e-01,
        -2.9199e-01, -8.1055e-01, -5.6797e+00, -5.8008e-01,  4.3750e-01,
        -2.2461e-01, -7.0020e-01,  3.0371e-01,  5.6543e-01,  5.0098e-01,
         2.1240e-01,  5.3320e-01, -1.0977e+00,  4.6875e-01, -2.9004e-01,
        -1.1406e+00,  2.6367e-01,  1.0078e+00, -1.0181e-01,  5.4980e-01,
        -8.5742e-01, -5.5664e-01,  1.9775e-01, -1.6870e-01, -3.8750e+00,
        -1.4984e-02,  2.5244e-01, -3.4521e-01, -2.7930e-01,  3.4912e-01,
         1.4014e-01, -6.9702e-02, -5.4883e-01,  1.6289e+00, -2.9492e-01,
        -1.3086e+00,  5.7983e-02, -2.4805e-01, -1.4624e-01, -4.2969e-01,
         9.5947e-02, -4.4922e+00,  3.9648e-01, -6.2378e-02,  3.0029e-01,
         5.2637e-01, -2.7441e-01,  1.7944e-01,  1.5674e-01, -3.3545e-01,
         1.2354e-01,  4.4824e-01, -5.0469e+00,  8.3398e-01, -2.0625e+00,
         2.4414e-01, -1.9062e+00, -7.4609e-01, -2.8857e-01,  1.7188e-01,
         7.2188e+00, -1.3711e+00,  2.1836e+00, -1.8652e-01, -1.4136e-01,
        -3.7549e-01, -4.9121e-01,  4.6875e-01, -6.4648e-01, -8.1641e-01,
         5.6763e-02, -3.4619e-01,  6.3672e-01,  4.8730e-01, -6.7480e-01,
        -8.4229e-02,  5.2051e-01, -1.8164e-01,  1.8921e-01, -4.4434e-01,
         2.3945e+00, -6.3184e-01, -3.3984e-01, -2.5684e-01,  1.7891e+00,
        -4.5996e-01, -9.7656e-02,  4.3066e-01, -5.7373e-02, -4.8047e-01,
         3.3105e-01, -1.0645e-01,  7.2388e-02, -3.5742e-01,  1.5508e+00,
         6.9824e-01,  2.2266e-01,  4.6582e-01, -8.9062e-01,  7.3340e-01,
        -1.7227e+00,  3.8672e-01,  4.5312e-01, -3.8281e-01, -6.5137e-01,
        -3.2910e-01, -1.3848e+00,  8.0566e-02,  3.5400e-01, -2.2773e+00,
        -1.5977e+00,  2.6328e+00,  3.8025e-02, -2.5195e-01,  1.3320e+00,
        -1.1108e-01,  5.7031e-01,  3.1104e-01,  2.9004e-01,  7.4951e-02,
         5.9570e-01, -3.9453e-01,  1.1475e-01, -3.7988e-01, -6.5460e-03,
         7.1289e-01, -2.6074e-01,  2.6318e-01,  1.0156e+00, -1.7700e-01,
        -1.2852e+00, -1.2280e-01,  5.2344e-01,  3.4082e-01, -5.1514e-02,
         9.8828e-01, -5.6152e-01,  6.1230e-01, -3.9014e-01,  1.8335e-01,
         4.3066e-01, -1.1641e+00, -2.8662e-01, -1.3047e+00,  1.4414e+00,
         7.0938e+00,  3.1738e-01, -2.8369e-01, -3.3643e-01,  7.9297e-01,
        -2.3477e+00, -8.7695e-01,  5.5371e-01, -2.6807e-01,  3.7305e-01,
         5.7656e+00, -2.6758e+00], device='cuda:2', dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.3506, -0.0823, -0.0337,  ..., -0.3767, -0.5132,  0.0778],
        [ 1.0420, -0.6113,  0.4297,  ..., -2.5137, -1.0215, -0.7114],
        [ 0.0646, -0.0857,  0.1099,  ..., -0.6338, -0.3428, -0.1383],
        ...,
        [ 0.3857, -1.3965,  1.1562,  ...,  1.0186, -2.0938,  1.2646],
        [ 0.0185, -0.1270,  0.0590,  ...,  0.4429, -0.2881,  0.2954],
        [ 0.7051, -0.7485,  0.4753,  ..., -1.1943, -0.6143, -0.6108]],
       device='cuda:2', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([  2.7344,  -1.4219,  -0.9805,   0.5264,  -6.8828,   1.9453,  -4.2656,
          0.7695,   0.1406,   3.0508,   0.7578,  -6.2891,  -1.4219,   4.7031,
         -1.3906,   0.0703,  -1.8809,   4.0234,  -7.0469,  -5.4219,   2.1582,
          2.4766,   6.3984,   2.0508,   0.4219,   0.6406,  -5.6094,   0.1836,
          0.3711,  -0.6250,   1.3594,   4.8047,   7.1094,   1.9453,  -0.8359,
         -1.8359,  -3.4375,   0.8359,   6.1562,   0.5625,   1.7539,  -1.6641,
         -7.5000,   5.1562,   3.7070,  -2.2891,   0.6641,  -2.6875,  -0.5977,
          6.0078,   0.6484,  -1.1328,  -9.4531,   4.3750,   3.3750, -10.0000,
          4.4883,   2.3984,   2.2969,  -1.3594,  -4.9219,  -5.3750,  -0.0859,
         -7.9531,   5.2891,  -2.0625,   3.1016,   3.5039,   4.1562,  -1.2773,
         -8.9531,  -6.4062,  -1.2549,   1.3867,   0.2266,  -2.3750,   3.2031,
         -0.9297,  -7.7031,   1.6875,  -2.6562,  -0.4375,   0.7891,   2.7852,
         -3.4922,  -0.0918,   0.1836,   3.3398,   0.9688,   4.7500,   4.4141,
         -6.0391,   3.2188,  -5.2031,   1.0156,  -0.2422,  -1.3359,  -9.0469,
         -8.4531,   4.8438,  -2.2500,  -0.0400,   0.1836,  -0.4219,   1.7109,
          0.7148,   1.8516,   1.4453,  -0.4922,   2.9805,   0.1797,   4.0469,
          0.8047,   5.8359,   0.6641,   4.2109,   3.6094,  -1.5703,   2.1094,
         -0.0508,  -0.5664,  -0.4629,  -1.6016,  -0.3887,  -5.6406,   1.4375,
         -8.3438, -13.3047,   0.4746,  -4.6875,  -5.1406,  -7.4609,   2.8984,
         -6.1094,   3.5742,   4.0547,   0.6953,   3.5938,  -0.7461, -12.2969,
          3.1328,  -2.7891,  -5.8281,  -1.7969,  -1.5391,   2.1094,   0.4297,
          1.9648,   2.7695,   1.4961, -10.0312,   0.2500,  -6.4688,   2.2812,
          4.3359,   4.5312,   4.9727,  -0.3086,   3.5391,  -5.3281,   1.4844,
          3.1523,  -6.9062,  -2.5781,   1.3809,  -0.7148,   3.9219,  -8.4609,
         -2.4375,  -4.5117,  -5.8594,   2.1875,  -1.1836,   1.0234,   1.4258,
          4.3828,   2.7734,  -1.6406,  -3.0547,   0.2422,  -1.6797,  -6.1094,
         -0.6367,   3.2109,  -2.5703,   0.8281,   1.9219,  -3.3633,   3.4609,
         -7.1250,   0.6836,  -6.9375, -10.6328,  -0.6016,  -0.7031,  -3.6133,
        -12.8906,  -2.9062,   5.6094,   1.9336,  -4.3516,  -0.8672,  -9.7031,
         -2.7539,  -5.5391,   9.0078,  -0.5039,  -0.1562, -14.5625,  -2.6328,
        -10.8281,  -9.9375,   2.3906,   8.0781,  -6.2656,  -1.5859,  -0.5000,
         -1.4062,  -3.6016, -11.2266,   0.7344,   1.4717,   3.1211,   0.2285,
          4.7266,   2.7148,   1.5352,  -2.5938,  -8.5703,   2.5508,  -0.0547,
          4.6367,   0.9883,   3.0391,  -3.4609,   1.4824,  -9.6250,   3.9531,
          1.4805,   2.7266,  -4.8125,  -3.2500,  -2.5469, -11.8672,  -9.8359,
          3.1328,  -0.3047,   1.7070,   1.0195, -15.2188, -16.0781,  -1.3438,
         -4.9609,  -1.1641,   0.7754,  -2.3906], device='cuda:2',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 8.6060e-03,  9.2773e-03, -6.0120e-03,  ..., -4.2236e-02,
         -2.1680e-01, -1.6815e-02],
        [ 6.4087e-04, -1.2085e-02,  1.7548e-03,  ..., -5.5054e-02,
         -1.9653e-01, -2.1423e-02],
        [-5.0507e-03,  1.1963e-02,  5.9814e-03,  ...,  4.5654e-02,
          1.5820e-01,  1.7715e-02],
        ...,
        [ 1.0109e-03,  4.0894e-03, -3.6030e-03,  ...,  3.9001e-02,
          8.2336e-02,  4.1199e-04],
        [-9.6924e-02,  1.1719e-02,  4.6875e-02,  ...,  5.3711e-01,
          1.5430e+00,  1.8237e-01],
        [ 1.7822e-02,  4.8828e-04,  6.8359e-03,  ..., -1.0693e-01,
         -4.3164e-01, -4.0894e-02]], device='cuda:2', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([-2.3096e-01, -1.6406e-01,  1.6333e-01, -2.3560e-02,  3.3508e-02,
        -2.6367e-02, -1.1023e-01, -1.1206e-01,  3.9185e-02, -8.3618e-02,
         1.2842e-01, -1.0938e-01, -3.0762e-02,  4.3396e-02,  3.6377e-02,
         4.7028e-02,  1.5747e-02,  1.2158e-01,  6.2378e-02,  7.4219e-02,
        -5.7251e-02, -3.4473e-01,  2.0312e-01,  2.7100e-01,  2.4219e-01,
        -5.4443e-02, -1.6748e-01,  2.9846e-02, -6.7261e-02, -3.4058e-02,
        -6.8359e-01, -7.1533e-02,  5.8838e-02,  5.9863e-01,  1.4307e-01,
        -6.8970e-02,  2.1680e-01, -3.2715e-01,  4.1565e-02, -1.6406e-01,
         1.6846e-01, -1.2695e-01, -1.9873e-01, -8.8379e-02, -1.5771e-01,
        -1.9897e-01, -1.6113e-01, -1.0034e-01,  5.1758e-02,  1.6309e-01,
        -2.3828e-01, -1.4233e-01,  4.5532e-02, -2.4170e-01,  4.3555e-01,
        -1.9092e-01, -3.2959e-03, -1.7603e-01,  2.0859e-02, -1.5088e-01,
        -6.3672e-01,  1.5088e-01,  2.3877e-01,  6.7139e-02,  4.2664e-02,
        -1.7554e-01,  8.2031e-02, -4.4403e-02, -2.4414e-04,  7.8735e-02,
        -6.3232e-02,  1.1359e-01, -7.9590e-02, -1.5356e-01,  2.4854e-01,
         6.1607e-04, -4.9194e-02, -5.3711e-02,  2.0264e-02, -3.1836e-01,
         5.1727e-03,  9.8633e-02,  1.7969e-01, -5.3040e-02,  1.0547e-01,
        -5.0977e-01,  1.1157e-01, -6.1426e-01, -4.6631e-02, -1.6089e-01,
         7.0801e-02,  1.2146e-01, -1.7883e-02,  1.4180e+00,  1.5063e-01,
         3.0664e-01,  7.8064e-02,  1.2366e-01, -5.1855e-01, -2.6199e-02,
         8.5327e-02,  1.9043e-01, -1.5967e-01, -2.2900e-01, -3.0975e-03,
        -4.0161e-02,  5.8533e-02, -6.8945e-01, -9.5520e-02, -2.6318e-01,
         1.3989e-01,  8.1665e-02, -2.9590e-01,  2.6855e-02, -9.4604e-04,
         6.2988e-02, -2.4365e-01,  6.2695e-01, -4.3945e-02,  2.6172e-01,
        -3.5156e-01,  7.4768e-02,  2.2266e-01, -7.0496e-02, -3.5461e-02,
         1.8555e-02,  5.5859e-01, -4.1992e-02,  1.1890e-01, -1.8555e-01,
        -3.2349e-02,  2.4268e-01,  1.6650e-01,  3.7158e-01, -6.7139e-03,
         7.6965e-02,  2.8427e-02, -6.2012e-02, -6.8909e-02,  9.1797e-02,
        -6.2683e-02, -1.6907e-02, -4.0222e-02, -1.6284e-01, -1.0822e-01,
         7.0251e-02,  2.8076e-01, -2.4268e-01, -1.1865e-01,  9.8389e-02,
         1.0608e-01, -1.7773e-01, -5.6396e-02,  9.5312e-01,  1.6846e-02,
        -1.3696e-01, -2.3486e-01, -4.5801e-01,  1.7969e-01, -3.8513e-02,
        -1.2585e-01, -2.2949e-02, -4.4220e-02,  8.2520e-02, -8.5547e-01,
        -1.8115e-01, -1.1328e-01, -1.3672e-01, -8.3496e-02,  2.7618e-02,
         1.0559e-01, -2.6025e-01, -1.3672e-01,  5.1025e-02,  9.2529e-02,
        -1.0498e-01, -2.5439e-01, -6.9824e-02,  3.0547e+00,  6.0742e-01,
        -6.2317e-02,  4.2969e-02, -1.4502e-01,  1.6821e-01,  1.7891e+00,
        -1.9897e-02, -3.1445e-01, -8.8501e-02,  4.0894e-02,  1.5845e-01,
         1.6235e-01, -4.3945e-03, -2.0850e-01, -1.1096e-01, -2.0410e-01,
        -2.6978e-02, -2.3730e-01,  5.3516e-01, -1.7334e-01,  6.4941e-02,
         1.0039e+00,  4.1779e-02,  1.2817e-01,  3.3496e-01,  8.2520e-02,
         2.4780e-02, -3.3301e-01,  2.4414e-03,  2.8015e-02,  1.6150e-01,
         8.9600e-02, -3.6865e-01,  8.1543e-02,  1.0205e-01, -1.7334e-02,
        -2.1704e-01,  3.3020e-02, -2.6074e-01, -1.8237e-01, -6.6406e-02,
        -6.3965e-02, -4.5166e-02, -5.4291e-02, -1.1157e-01, -6.6797e-01,
        -8.5547e-01, -2.9663e-02, -1.9922e-01,  8.6792e-02,  6.1722e-03,
         1.0852e-01,  8.2458e-02,  4.9255e-02, -1.5845e-01,  9.6313e-02,
        -1.2036e-01,  6.4331e-02,  5.2612e-02, -3.1787e-01, -3.7689e-02,
         2.8516e-01,  8.8257e-02,  5.5328e-02, -5.0110e-02,  2.1826e-01,
        -2.2046e-01,  3.2104e-02, -1.1450e-01, -2.2998e-01, -7.0862e-02,
         8.6426e-02,  1.4014e-01,  3.3325e-02,  1.4209e-01, -1.6870e-01,
        -2.1631e-01,  7.9590e-02, -3.7402e-01, -5.1367e-01, -2.6465e-01,
         1.9043e-01,  6.3293e-02,  1.8799e-01,  3.6670e-01,  1.1074e+00,
        -1.2671e-01,  5.6055e-01, -5.2795e-03, -1.1829e-01,  3.4393e-02,
         1.0425e-01,  1.6680e+00,  4.9805e-01, -3.8184e-01,  1.7529e-01,
         1.0474e-01,  1.0217e-01,  5.5115e-02, -1.2573e-02, -2.2949e-01,
         9.2188e-01,  3.8965e-01, -5.0415e-02,  3.1836e-01, -7.2327e-02,
        -1.3477e-01, -1.2207e-01,  2.6489e-02, -1.8896e-01,  4.9609e-01,
         7.4219e-02,  1.4868e-01, -1.2549e-01,  1.3831e-01,  1.0071e-01,
         1.3354e-01,  1.1230e-01,  1.6724e-01, -4.3457e-02, -1.6357e-02,
        -5.0049e-02, -4.9707e-01,  1.9434e-01,  7.1289e-02,  1.0754e-01,
        -1.3452e-01, -1.0242e-01,  5.9814e-02, -6.6528e-03, -1.9958e-02,
        -2.9150e-01, -3.7109e-01, -1.2244e-01,  1.2769e-01,  1.0486e-01,
         2.8857e-01,  7.3975e-02,  6.3354e-02, -1.4697e-01, -2.3975e-01,
         5.8716e-02,  1.0718e-01,  5.9814e-03,  1.1543e+00, -5.5603e-02,
        -9.1309e-02, -8.6731e-02, -2.5146e-01, -1.7822e-01, -1.1389e-01,
        -3.1250e-02, -2.2485e-01,  5.3101e-02, -7.4158e-02,  4.6082e-02,
        -2.3242e-01, -2.7246e-01,  6.6650e-02,  5.9723e-02, -8.8135e-02,
         8.6426e-02, -1.1859e-01,  2.3926e-01,  1.0742e-02, -3.9746e-01,
         7.2327e-02,  2.0947e-01,  3.7537e-02,  3.6719e-01,  5.6885e-02,
        -2.2900e-01, -1.8359e+00, -1.5845e-01, -7.4707e-02,  9.8389e-02,
        -8.2275e-02, -2.4170e-01, -2.2695e+00, -1.4111e-01,  7.7393e-02,
        -2.7283e-02, -1.6992e-01,  5.4749e-02,  1.1401e-01,  1.3843e-01,
         1.1658e-02,  1.1511e-01, -2.9736e-01,  8.7402e-02, -1.0681e-01,
        -3.1982e-01, -1.8311e-03,  3.0225e-01, -8.6548e-02,  7.5195e-02,
        -2.2510e-01, -1.2183e-01,  4.9011e-02, -4.3701e-02, -1.0645e+00,
        -2.2430e-03,  7.7637e-02, -5.4077e-02, -3.7231e-02,  1.2317e-01,
         5.3467e-02, -2.7161e-02, -1.0669e-01,  3.6816e-01, -6.6406e-02,
        -3.6816e-01, -2.9053e-02, -8.4961e-02, -1.2573e-02, -1.6870e-01,
         4.3152e-02, -1.1758e+00,  9.9182e-02, -2.4826e-02,  1.0406e-01,
         1.2866e-01, -4.4678e-02,  3.8452e-03,  1.8555e-02, -1.0889e-01,
         9.7656e-03,  6.5308e-02, -1.6094e+00,  1.7578e-01, -5.3516e-01,
         6.1035e-04, -5.5957e-01, -2.1875e-01, -6.7505e-02,  5.7983e-02,
         2.5625e+00, -3.4082e-01,  5.9180e-01, -3.8452e-02, -4.1748e-02,
        -8.5205e-02, -1.7798e-01,  1.0291e-01, -2.0117e-01, -1.9434e-01,
        -2.2095e-02, -7.1411e-02,  1.1084e-01,  9.9609e-02, -2.0630e-01,
         2.1362e-03,  1.0876e-01, -6.2866e-02,  6.0944e-02, -1.1755e-01,
         5.7422e-01, -1.7993e-01, -9.6802e-02, -1.2048e-01,  3.8770e-01,
        -9.6924e-02, -8.8501e-02,  9.1919e-02, -1.6968e-02, -1.4355e-01,
         2.4536e-02, -4.9408e-02,  4.5166e-03, -7.0312e-02,  3.7891e-01,
         1.3428e-01,  2.4414e-02,  1.0120e-01, -1.7041e-01,  1.6357e-01,
        -4.5703e-01,  7.7271e-02,  1.3696e-01, -1.1304e-01, -1.5625e-01,
        -8.1055e-02, -2.7441e-01, -6.2378e-02,  1.0870e-01, -6.2891e-01,
        -4.3262e-01,  6.8945e-01,  1.0559e-02, -3.2837e-02,  3.1885e-01,
        -3.4180e-03,  1.2354e-01,  8.5205e-02,  4.7852e-02,  2.8809e-02,
         1.1633e-01, -9.7046e-02,  3.3264e-02, -4.8096e-02, -1.5015e-02,
         1.5649e-01, -3.8635e-02,  4.9744e-02,  2.0459e-01, -2.0142e-02,
        -3.2324e-01, -1.6968e-02,  9.1064e-02,  7.6294e-02, -4.5624e-02,
         2.5928e-01, -1.6577e-01,  1.1328e-01, -7.3120e-02,  5.0720e-02,
         1.1536e-01, -3.5742e-01, -4.3213e-02, -2.8613e-01,  3.4766e-01,
         2.1875e+00,  7.7148e-02, -6.2256e-02, -7.9346e-02,  1.5820e-01,
        -4.9902e-01, -2.0361e-01,  8.4473e-02, -3.4851e-02,  6.1951e-02,
         1.7734e+00, -6.1523e-01], device='cuda:2', dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True task_net module.module.task_net.layer_norm.weight True tensor([ 4.1211e-01,  5.2832e-01,  7.5806e-02, -9.0942e-02,  3.4912e-02,
         1.3000e-02,  9.4482e-02, -4.1260e-02,  2.7881e-01,  1.2866e-01,
         4.9219e-01, -5.3955e-02, -6.2622e-02,  2.6611e-01,  9.1187e-02,
        -4.0649e-02,  1.1963e-01,  8.5083e-02,  3.1836e-01,  1.3940e-01,
         2.8125e-01,  5.4883e-01, -1.1572e-01, -1.5234e-01,  1.5393e-01,
         2.1820e-02, -1.6919e-01,  2.1973e-01,  1.9873e-01,  2.1436e-01,
         9.8633e-02,  1.0181e-01,  5.9570e-01, -5.4590e-01,  6.7993e-02,
         1.3477e-01,  5.3711e-03, -3.5059e-01,  1.5063e-01,  3.9990e-01,
         7.2266e-01, -3.6011e-03, -1.1487e-01,  2.4536e-02,  2.2339e-01,
         7.0923e-02,  4.4727e-01,  3.5107e-01,  2.2852e-01, -1.6052e-01,
         1.5869e-01,  9.0454e-02,  6.3477e-03,  1.5137e-02, -2.1826e-01,
         3.9844e-01,  1.7261e-01,  3.4912e-01,  1.7761e-02,  2.8467e-01,
        -6.0693e-01,  5.1660e-01, -2.7271e-01,  3.7549e-01,  2.0532e-01,
         4.0967e-01,  8.1787e-03,  9.8877e-02, -6.5842e-03,  2.2852e-01,
         1.8896e-01,  5.7715e-01, -6.9946e-02,  5.2832e-01, -3.7793e-01,
        -7.3547e-03,  1.2354e-01,  6.6895e-02, -9.3079e-03,  5.9326e-02,
         3.0365e-03,  4.8340e-01,  1.2109e-01, -1.1597e-03,  1.2476e-01,
        -1.7822e-01,  5.2539e-01, -2.3340e-01,  1.2451e-01,  6.6528e-02,
         2.1729e-01,  3.9990e-01,  8.3008e-02, -1.6973e+00,  3.6328e-01,
         5.0879e-01,  1.9751e-01,  5.6152e-01, -4.3701e-02,  3.6049e-04,
         4.0820e-01,  2.8394e-01, -2.3022e-01,  2.7393e-01,  4.3945e-03,
         6.5613e-03,  1.6016e-01, -7.2461e-01,  4.0332e-01,  1.0443e-01,
         1.5601e-01,  3.1738e-01, -4.2676e-01,  2.1484e-02,  1.9775e-01,
         1.9434e-01,  3.0762e-01, -2.4609e-01,  1.1328e-01,  7.9346e-02,
        -1.6211e-01,  2.1826e-01,  2.5684e-01,  6.2988e-02,  1.8501e-03,
         1.4233e-01, -2.3926e-01, -8.6365e-03,  2.6855e-01,  1.7676e-01,
         2.6660e-01,  4.7119e-02,  4.1309e-01,  3.8574e-01, -2.0111e-02,
         2.2192e-01,  4.1650e-01,  1.2421e-02,  1.3867e-01,  3.9160e-01,
         1.3721e-01, -2.4261e-02,  1.4185e-01, -1.2665e-02,  2.5293e-01,
         3.0469e-01, -1.5137e-02,  7.1533e-02, -1.3977e-02,  3.2324e-01,
         3.1934e-01,  8.7524e-02,  2.7008e-02, -6.2012e-01,  3.5278e-02,
        -4.2236e-02, -1.1914e-01, -1.9238e-01, -2.1631e-01,  1.7188e-01,
         3.1836e-01, -1.6266e-02,  1.2866e-01, -1.3477e-01, -5.0098e-01,
         1.1206e-01,  3.5034e-02,  1.1401e-01, -1.2964e-01,  2.6550e-02,
         3.8184e-01,  5.2246e-01,  3.5645e-01,  1.1426e-01,  1.1157e-01,
         5.1422e-02, -1.6162e-01,  2.0703e-01, -2.4766e+00, -4.1064e-01,
         1.3599e-01,  3.0664e-01,  4.5044e-02, -9.2041e-02, -8.7988e-01,
        -9.4452e-03,  3.4595e-01,  1.9836e-03,  3.6035e-01,  1.5283e-01,
         3.3301e-01, -8.0933e-02,  2.1411e-01,  4.7363e-02, -1.0986e-03,
        -1.5839e-02,  5.8228e-02, -5.3320e-01, -1.1914e-01, -1.0474e-01,
        -5.6543e-01,  3.4277e-01,  2.8174e-01, -2.0947e-01, -1.0742e-02,
        -1.5182e-02, -4.8218e-02,  7.6416e-02, -3.2288e-02,  1.2207e-02,
         1.5564e-01, -2.1143e-01,  3.5840e-01,  4.6387e-01, -2.2552e-02,
         4.7852e-01,  3.0469e-01,  2.9004e-01,  2.8625e-02,  1.2183e-01,
         9.3384e-02,  2.8519e-02,  7.5684e-03, -2.4414e-01, -5.9473e-01,
        -2.0557e-01,  2.3584e-01,  5.7910e-01, -5.9937e-02,  1.4038e-03,
         4.2188e-01,  3.6865e-01,  6.5918e-02,  3.0176e-01,  2.8467e-01,
         5.0195e-01,  5.8472e-02, -1.0315e-01,  4.7705e-01, -6.1493e-03,
         2.0972e-01,  2.4463e-01,  2.6550e-02,  2.9932e-01,  2.2510e-01,
         4.9121e-01,  3.6377e-01,  2.9199e-01,  2.6514e-01,  1.0168e-01,
         2.0703e-01,  1.9409e-01, -5.9082e-02,  6.9092e-02,  2.1240e-01,
         1.8481e-01,  2.5684e-01, -4.9902e-01, -2.4268e-01, -1.6357e-01,
        -1.0645e-01,  5.1392e-02,  1.9629e-01,  6.4258e-01, -4.1406e-01,
         2.1143e-01, -2.5195e-01, -4.9591e-05,  3.9160e-01, -1.8188e-02,
         4.5312e-01, -1.4062e+00, -2.0508e-01, -5.3027e-01, -5.8350e-02,
        -1.0254e-02,  6.6992e-01,  1.3574e-01, -1.9043e-02,  3.7402e-01,
        -9.6680e-01,  4.8535e-01,  4.0161e-02,  4.3262e-01,  5.5298e-02,
         4.9707e-01,  1.5283e-01,  1.4209e-01, -1.6406e-01, -5.0391e-01,
         4.2969e-01,  3.6768e-01,  5.0684e-01,  3.1006e-01,  2.8857e-01,
         3.6768e-01,  3.7207e-01,  2.2192e-01, -2.5024e-02,  2.3047e-01,
         2.0691e-02, -3.8477e-01,  4.5312e-01,  2.4951e-01,  4.7656e-01,
        -1.9580e-01,  1.6678e-02,  3.1641e-01, -1.1261e-02, -2.7191e-02,
         2.8296e-01, -3.3984e-01,  2.9590e-01,  4.5508e-01,  3.8135e-01,
        -2.4023e-01, -8.3252e-02,  1.8970e-01, -1.2354e-01,  4.8828e-01,
         3.5583e-02, -3.3875e-02,  1.5942e-01, -6.4844e-01,  1.3013e-01,
        -3.2776e-02,  3.4277e-01,  2.3950e-01,  4.3506e-01,  1.1694e-01,
         2.0654e-01,  5.7617e-01, -1.7578e-02,  2.0996e-01,  5.7220e-06,
        -1.6113e-01, -2.7051e-01,  7.0557e-02, -1.3046e-02,  2.1631e-01,
         5.6738e-01,  1.3992e-02, -2.0996e-01,  3.0212e-02,  4.5898e-01,
         2.3193e-01,  4.3848e-01,  2.6514e-01, -3.1055e-01, -3.7842e-02,
         1.2842e-01, -2.3691e+00,  1.3574e-01,  3.5217e-02,  1.8921e-01,
         2.8467e-01,  7.9712e-02, -1.0928e+00,  3.1689e-01, -1.6479e-01,
         1.5088e-01, -2.2607e-01,  2.0557e-01,  2.9199e-01,  1.0474e-01,
        -2.0630e-02,  3.8086e-01, -1.8164e-01,  5.1270e-01,  1.8384e-01,
         6.1133e-01, -5.4077e-02,  1.9629e-01,  9.9854e-02,  4.7168e-01,
         1.9653e-01,  1.8018e-01,  3.5645e-02, -1.2695e-01, -1.1719e-01,
         7.5073e-03,  1.7993e-01,  1.9775e-01,  2.4219e-01, -1.4355e-01,
         1.8481e-01,  1.2779e-03,  1.1914e-01, -1.6113e-01,  3.6133e-01,
         2.5391e-01, -7.9712e-02,  1.8970e-01, -7.8735e-02,  2.4561e-01,
         6.5002e-03, -1.4453e+00,  3.0762e-01,  3.6560e-02,  2.0947e-01,
         3.2764e-01,  2.5879e-01,  1.4575e-01,  4.3152e-02, -3.4912e-02,
         1.4209e-01,  1.2134e-01, -1.4062e+00,  2.2021e-01,  1.0645e-01,
         5.7983e-02, -3.4863e-01,  1.5503e-01,  2.8418e-01,  2.0312e-01,
        -1.8242e+00, -2.5537e-01, -4.2285e-01, -2.0325e-02, -5.6213e-02,
         3.2178e-01,  2.7441e-01,  4.9023e-01,  6.0742e-01,  1.3818e-01,
        -4.3884e-02,  2.2314e-01,  9.9365e-02,  3.9258e-01,  2.3779e-01,
         1.0095e-01,  3.0762e-01,  1.7993e-01,  1.4746e-01,  4.5068e-01,
         5.8228e-02, -1.5820e-01,  3.4863e-01,  2.4628e-02, -1.0693e-01,
         1.0046e-01,  3.9551e-02,  8.6426e-02, -1.7029e-02,  3.1543e-01,
         1.8604e-01,  3.1982e-02, -8.3252e-02, -4.0619e-02, -2.3047e-01,
        -8.5547e-01,  3.2520e-01,  3.4424e-01, -2.6953e-01, -1.3135e-01,
         1.5271e-01,  3.6572e-01,  3.2861e-01, -3.9355e-01,  5.0391e-01,
        -4.0344e-02, -2.6855e-01, -4.2786e-02,  2.6318e-01, -2.0605e-01,
        -4.2773e-01, -2.0117e-01, -2.3376e-02,  1.5234e-01, -2.5928e-01,
         1.2695e-02, -1.5747e-01, -8.6182e-02, -8.8135e-02,  1.0266e-01,
         4.3555e-01,  3.4717e-01,  3.7231e-02,  3.2324e-01,  8.4991e-03,
         4.5508e-01,  3.0518e-01,  2.3877e-01,  1.4062e-01,  3.5767e-02,
        -2.9785e-01,  9.7580e-03, -5.1270e-03, -2.6245e-03,  2.0477e-02,
        -7.3242e-02,  5.0195e-01, -1.2195e-01,  2.8320e-01,  1.3232e-01,
         1.7334e-01, -9.0210e-02,  2.5537e-01, -8.1543e-02, -3.3984e-01,
        -8.4570e-01,  2.9297e-01, -1.2244e-01,  7.9712e-02, -1.2402e-01,
        -4.8560e-01,  4.4922e-01,  1.8652e-01,  3.2129e-01,  3.5010e-01,
        -1.4170e+00, -7.2559e-01], device='cuda:7', dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([ 7.3633e-01,  4.2139e-01, -5.0195e-01, -7.9102e-02, -8.9111e-02,
         8.5602e-03,  2.0508e-01,  3.5156e-01, -1.5137e-01,  1.9482e-01,
        -5.1855e-01,  3.6963e-01,  1.0352e-01, -1.5869e-01, -2.1240e-01,
        -2.8900e-02, -9.2896e-02, -5.0488e-01, -2.2778e-01, -3.9404e-01,
         2.3730e-01,  1.1641e+00, -6.2598e-01, -8.9453e-01, -6.5723e-01,
         2.1216e-01,  6.0352e-01, -2.0654e-01,  1.1438e-01,  2.1533e-01,
         1.7852e+00,  3.3691e-01, -3.8232e-01, -1.9531e+00, -5.1270e-01,
         2.0312e-01, -6.2305e-01,  8.1543e-01, -2.1167e-01,  4.9219e-01,
        -6.9629e-01,  4.1699e-01,  7.0020e-01,  3.7109e-01,  4.0283e-01,
         4.9121e-01,  4.9316e-01,  2.9883e-01, -3.5254e-01, -6.2500e-01,
         5.9570e-01,  4.3799e-01, -2.3120e-01,  6.6992e-01, -1.0996e+00,
         6.0449e-01,  1.5234e-01,  3.7646e-01,  1.7700e-02,  3.1982e-01,
         1.6562e+00, -4.9219e-01, -7.5000e-01, -3.2178e-01, -2.4609e-01,
         4.0869e-01, -1.6235e-01,  8.1177e-02, -6.9122e-03, -2.4365e-01,
         1.3794e-01, -3.8135e-01,  1.8921e-01,  3.9697e-01, -1.0371e+00,
        -2.6306e-02,  9.2529e-02,  9.6313e-02,  1.8463e-02,  9.7656e-01,
        -3.2715e-02, -3.9795e-01, -7.0312e-01,  3.6255e-02, -4.1357e-01,
         1.3809e+00, -4.0576e-01,  1.8066e+00,  1.7310e-01,  5.3125e-01,
        -2.4658e-01, -3.8428e-01,  1.0657e-01, -3.8984e+00, -4.1553e-01,
        -1.0352e+00, -1.5454e-01, -4.6680e-01,  1.3672e+00,  1.4496e-03,
        -3.2666e-01, -5.9961e-01,  6.0547e-01,  7.1289e-01,  1.0544e-02,
         9.7412e-02, -1.6675e-01,  1.8828e+00,  2.9736e-01,  7.4512e-01,
        -5.8887e-01, -2.7930e-01,  9.2969e-01,  5.3284e-02, -1.9751e-01,
        -3.5645e-01,  8.8281e-01, -1.8281e+00,  1.6235e-01, -6.7578e-01,
         9.4238e-01, -2.4268e-01, -6.1230e-01,  1.2085e-01,  3.7689e-03,
        -2.3242e-01, -1.8926e+00,  1.0083e-01, -4.4189e-01,  5.5859e-01,
         1.8140e-01, -7.3926e-01, -4.9219e-01, -1.0996e+00,  6.3354e-02,
        -2.6660e-01, -2.2559e-01,  2.7441e-01,  1.1108e-01, -4.2139e-01,
         1.8140e-01, -6.9458e-02,  9.6680e-02,  3.7891e-01,  2.1069e-01,
        -2.7197e-01, -8.3887e-01,  9.0234e-01,  3.2520e-01, -3.3496e-01,
        -3.2373e-01,  4.4189e-01,  4.6265e-02, -2.3203e+00, -2.8271e-01,
         3.9746e-01,  7.2949e-01,  1.4258e+00, -7.6270e-01,  1.7163e-01,
         3.4766e-01, -1.6144e-02,  1.1987e-01, -3.5303e-01,  2.3594e+00,
         4.2529e-01,  4.9744e-02,  3.4521e-01,  2.5146e-01, -9.8022e-02,
        -3.9209e-01,  8.0957e-01,  3.1982e-01, -3.2910e-01, -5.6934e-01,
         1.9165e-01,  8.1934e-01,  1.9995e-01, -7.4062e+00, -1.7812e+00,
         1.5186e-01, -2.3291e-01,  3.1738e-01, -5.1562e-01, -3.8359e+00,
        -2.1362e-02,  7.9492e-01,  1.4502e-01, -2.8076e-01, -4.6045e-01,
        -5.5469e-01,  7.1167e-02,  5.8008e-01,  1.8140e-01,  5.1465e-01,
         7.0068e-02,  5.4590e-01, -1.4961e+00,  6.5137e-01, -2.6221e-01,
        -2.5430e+00, -2.5244e-01, -3.3398e-01, -1.0898e+00, -3.6621e-01,
         3.0243e-02,  1.1016e+00,  7.5562e-02, -2.5909e-02, -3.4766e-01,
        -4.6729e-01,  1.0215e+00, -2.6807e-01, -3.9160e-01,  1.1829e-01,
         5.6641e-01, -2.8906e-01,  8.5156e-01,  4.5605e-01,  1.3770e-01,
         2.7637e-01,  1.9946e-01,  2.9572e-02,  2.4805e-01,  1.9648e+00,
         2.5352e+00,  2.1729e-01,  6.1230e-01, -2.6855e-01, -6.4941e-02,
        -3.3398e-01, -3.1885e-01, -1.8506e-01,  4.3848e-01, -2.9541e-01,
         3.4668e-01, -2.8467e-01, -1.8579e-01,  9.5703e-01, -3.8940e-02,
        -8.5156e-01, -3.0322e-01, -1.2891e-01,  1.8262e-01, -7.0508e-01,
         5.6152e-01, -2.8711e-01,  3.1689e-01,  5.8008e-01,  1.3428e-01,
        -3.6377e-01, -6.0352e-01, -2.4414e-01, -4.5752e-01,  3.2227e-01,
         5.4004e-01, -2.8320e-01,  1.2422e+00,  1.6953e+00,  8.5449e-01,
        -6.8750e-01, -1.1426e-01, -8.1836e-01, -1.0449e+00, -2.8047e+00,
         2.5098e-01, -2.0234e+00, -6.2256e-03,  2.8467e-01, -8.4351e-02,
        -4.1699e-01, -4.2578e+00, -1.6094e+00,  1.6133e+00, -6.4844e-01,
        -2.1899e-01, -4.7070e-01, -1.4795e-01,  3.5767e-02,  7.0898e-01,
        -2.1133e+00, -1.1582e+00,  2.0020e-01, -1.2441e+00,  5.5969e-02,
         4.3701e-01,  4.1602e-01, -1.1646e-01,  6.6016e-01, -1.5938e+00,
        -2.9932e-01, -4.4434e-01,  4.6729e-01, -3.7158e-01, -2.3364e-01,
        -5.0098e-01, -3.2861e-01, -6.1328e-01,  4.3030e-02, -2.3364e-01,
         3.3643e-01,  1.4453e+00, -7.6172e-01, -2.3413e-01, -3.8965e-01,
         4.7021e-01,  1.9116e-01, -3.0762e-01, -4.2847e-02, -1.1316e-01,
         7.7051e-01,  1.1348e+00,  2.3022e-01, -4.1309e-01, -3.7158e-01,
        -9.4238e-01, -3.2617e-01, -2.4146e-01,  5.6055e-01,  8.9258e-01,
        -2.1191e-01, -4.1895e-01, -2.3853e-01, -3.0703e+00,  1.6333e-01,
         3.9502e-01,  2.3828e-01,  7.0020e-01,  4.1699e-01,  2.2754e-01,
         1.9727e-01,  5.3418e-01, -1.7548e-02,  1.9604e-01, -5.3162e-02,
         7.3242e-01,  8.7109e-01, -5.3955e-02, -6.2378e-02,  3.1494e-01,
        -3.9990e-01,  1.0425e-01, -7.1875e-01,  9.9731e-02,  1.2656e+00,
        -1.5918e-01, -7.7344e-01, -1.4819e-01, -1.0508e+00, -3.6963e-01,
         8.1152e-01,  4.9531e+00,  3.1152e-01,  2.4561e-01, -3.1494e-01,
         2.2192e-01,  6.1719e-01,  4.3203e+00,  4.3994e-01, -3.3350e-01,
         1.7065e-01,  5.3125e-01, -2.3071e-01, -4.3018e-01, -3.8135e-01,
        -1.6113e-01, -4.0430e-01,  8.3301e-01, -3.5596e-01,  2.2046e-01,
         8.6816e-01, -1.9995e-01, -7.6562e-01,  7.7393e-02, -4.1699e-01,
         6.5039e-01,  4.2285e-01, -1.4990e-01,  1.2817e-01,  2.9414e+00,
         1.1414e-02, -1.9165e-01,  2.6221e-01,  2.1191e-01, -2.6514e-01,
        -1.0632e-01,  5.2917e-02,  4.1650e-01, -1.2324e+00,  2.2388e-01,
         9.9414e-01, -4.3945e-02,  1.8823e-01,  1.1121e-01,  3.2617e-01,
        -7.2876e-02,  3.4102e+00, -3.0127e-01,  4.7363e-02, -2.2803e-01,
        -3.9893e-01,  2.0825e-01, -1.3623e-01, -1.1890e-01,  2.5439e-01,
        -9.3750e-02, -3.3936e-01,  3.8281e+00, -6.3379e-01,  1.5684e+00,
        -1.8555e-01,  1.4473e+00,  5.6738e-01,  2.1875e-01, -1.3062e-01,
        -5.4844e+00,  1.0410e+00, -1.6582e+00,  1.4160e-01,  1.0754e-01,
         2.8467e-01,  3.7256e-01, -3.5596e-01,  4.8926e-01,  6.1914e-01,
        -4.3091e-02,  2.6221e-01, -4.8291e-01, -3.7012e-01,  5.1270e-01,
         6.3965e-02, -3.9453e-01,  1.3770e-01, -1.4380e-01,  3.3740e-01,
        -1.8223e+00,  4.7949e-01,  2.5732e-01,  1.9482e-01, -1.3574e+00,
         3.4961e-01,  7.4219e-02, -3.2666e-01,  4.3518e-02,  3.6377e-01,
        -2.5146e-01,  8.0811e-02, -5.4993e-02,  2.7197e-01, -1.1758e+00,
        -5.3027e-01, -1.6870e-01, -3.5303e-01,  6.7676e-01, -5.5664e-01,
         1.3086e+00, -2.9395e-01, -3.4424e-01,  2.9053e-01,  4.9316e-01,
         2.4951e-01,  1.0508e+00, -6.1157e-02, -2.6855e-01,  1.7266e+00,
         1.2109e+00, -1.9961e+00, -2.8870e-02,  1.9165e-01, -1.0098e+00,
         8.4351e-02, -4.3408e-01, -2.3608e-01, -2.2021e-01, -5.6824e-02,
        -4.5117e-01,  2.9980e-01, -8.7280e-02,  2.8857e-01,  4.9744e-03,
        -5.4102e-01,  1.9751e-01, -1.9946e-01, -7.7051e-01,  1.3428e-01,
         9.7656e-01,  9.3262e-02, -3.9746e-01, -2.5928e-01,  3.9124e-02,
        -7.5000e-01,  4.2627e-01, -4.6533e-01,  2.9590e-01, -1.3940e-01,
        -3.2617e-01,  8.8281e-01,  2.1777e-01,  9.8828e-01, -1.0938e+00,
        -5.3906e+00, -2.4097e-01,  2.1533e-01,  2.5537e-01, -6.0156e-01,
         1.7852e+00,  6.6699e-01, -4.2041e-01,  2.0361e-01, -2.8320e-01,
        -4.3750e+00,  2.0312e+00], device='cuda:7', dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.0681,  0.1826, -0.0488,  ..., -0.2576, -0.0154,  0.2168],
        [ 0.6562,  0.1392, -0.3096,  ..., -1.1309, -0.2734, -0.4238],
        [ 0.2537,  0.0593, -0.0979,  ..., -0.2920,  0.4802, -0.2357],
        ...,
        [-0.2849, -0.7373,  0.8096,  ...,  0.8169, -1.2334,  0.5693],
        [-0.0215, -0.0661,  0.1909,  ...,  0.1799, -0.1752,  0.2034],
        [-0.0154, -0.2773, -0.2455,  ..., -0.2229, -0.1713,  0.0929]],
       device='cuda:7', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 3.6367,  7.6953,  2.4707, -0.4390,  1.3047, -2.2637,  4.5625, -3.2363,
        -3.9648, -2.2109,  6.0000,  2.0117, -5.0977, -2.6523,  6.0859,  4.6055,
         0.3652, -1.1406,  9.2188,  5.5234,  1.7119, -5.9102, -3.5234, -2.4980,
         2.9766, -1.5078,  5.4531, -3.5664, -1.6562, -7.1797, -1.7832, -4.7109,
        -5.3281, -3.3750, -3.0352,  2.5781,  3.1562, -3.3555,  9.1406, -5.1914,
        -1.7207,  2.4375,  8.3984, 12.5234, -3.4570,  6.0000,  4.2969,  3.2148,
         2.0117, -4.1484, -1.9941,  2.6133,  8.3750, -5.8086, -2.6250,  4.6172,
        -0.3848, -2.8906, -2.9258,  4.4180,  5.4766,  5.5625,  3.9453,  7.2734,
        -2.4766,  2.2168, -1.7070,  6.4766, -5.7578, -3.6055,  7.6094,  4.7266,
        -0.1880,  5.5469, -6.8828,  0.9004, -5.0078,  4.6328,  5.9453,  3.7148,
         4.9766, -2.9609,  2.6055, -2.4219,  3.2578, -2.4434, -2.8984, -3.3594,
         3.9316, -5.2422, -1.3789,  2.1094, -3.6406,  2.6953,  3.3262, -6.1016,
        -5.6680,  8.0938,  2.8438, -2.8047,  3.1953, -0.7598, -0.2944,  4.5117,
        -4.3516, -2.2949, -2.3613,  5.7227,  4.4570, -3.0078, -5.0156, -1.1797,
        -4.2227, -3.3711, -1.3848, -3.3164, -5.3516,  3.6719, -1.2949,  2.7051,
        -1.7480,  0.8242,  7.3984, -1.3486,  3.2930,  5.8984,  6.9219, -2.5391,
        -2.2168,  6.8906,  1.7773,  3.0586, -4.1992,  0.9961, -2.0781, -4.3867,
         5.0859, -4.1914,  1.4082,  7.6016, -1.8672,  1.4805,  6.5156,  4.7383,
         3.6914, -4.1367,  0.7983,  4.8633, -3.1562,  3.4160,  9.9219,  3.3496,
         3.9453, -2.9727,  1.8359, -6.2031, -1.4180,  1.4863, -4.5273,  5.0781,
        -1.7363, -1.0312,  6.9062,  2.3906,  0.4404,  0.9648, -2.6367,  3.8203,
         2.2031,  1.2344,  4.0078, -1.8555,  1.9004, -4.4766, -1.0352, -3.1914,
        -5.0391,  4.0156,  3.4883,  4.8203,  4.8359,  3.1094,  2.2754, -3.8867,
         5.7812, -7.9648, -0.7012,  3.0312, -3.4609,  3.8125, -1.8779,  2.9922,
        -1.2070,  3.7754,  5.0664,  0.2031,  5.0078,  2.0469, -2.2695, -1.1211,
         3.5586,  6.4141,  1.8906,  1.0625,  4.3438, -3.2031,  2.0000,  3.6914,
         2.8516,  4.9883,  1.2188,  7.7266, -2.3789, -5.6562,  1.3906,  3.5273,
        -5.6523,  2.7344,  1.5703,  2.7031, -5.4609,  1.5703,  0.5879, -0.4812,
        -2.8281, -2.4453,  4.4297,  2.5898, -2.2734, -1.2656, -5.4727, -1.9180,
         3.5430, -4.5977,  5.4336,  2.1250,  3.4219, -5.5156, -3.1758, -1.4805,
         3.0352,  5.3281,  4.0391, -3.1133,  3.4531, -5.2656, -6.3125, -2.7988,
         2.8496,  2.5312,  7.9219,  1.6992,  3.6250, -3.8945, -1.2686,  2.8516],
       device='cuda:7', dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 2.9800e-02,  1.0107e-01,  2.0691e-02,  ...,  6.1157e-02,
          3.3765e-01,  7.5989e-03],
        [ 9.4986e-03,  2.8168e-02,  2.8305e-03,  ...,  1.4038e-03,
          3.7231e-02, -2.2705e-02],
        [-1.9150e-02, -6.0730e-02, -1.1749e-02,  ..., -3.5583e-02,
         -1.4966e-01, -6.5613e-03],
        ...,
        [-1.2465e-03, -5.8746e-03,  1.8501e-04,  ...,  1.3000e-02,
          8.6060e-03,  8.8730e-03],
        [-2.4500e-01, -8.3398e-01, -1.7749e-01,  ..., -6.0156e-01,
         -2.9316e+00, -1.3086e-01],
        [ 9.9487e-02,  3.4473e-01,  8.5083e-02,  ...,  2.7466e-01,
          1.2783e+00,  4.5654e-02]], device='cuda:7', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([ 1.9580e-01,  9.7656e-03, -1.0754e-01, -9.6802e-02,  2.2873e-02,
        -6.7505e-02,  3.4485e-02,  1.3452e-01,  8.0750e-02,  1.1841e-02,
        -8.9844e-02,  1.3464e-01,  7.7820e-02,  7.5073e-02, -3.1921e-02,
        -2.0996e-02,  5.4016e-02, -1.2378e-01,  9.1858e-02, -7.8247e-02,
         7.6294e-03,  3.1348e-01, -2.1948e-01, -2.5977e-01, -1.5698e-01,
         7.9590e-02,  1.9336e-01,  2.5528e-02, -7.7881e-02,  3.9856e-02,
         6.2305e-01,  1.2061e-01, -5.2063e-02, -6.4844e-01, -1.3269e-01,
         3.3936e-02, -1.3232e-01,  2.9443e-01, -1.5564e-03,  1.2891e-01,
        -1.4136e-01,  1.5112e-01,  2.3706e-01,  1.2964e-01,  7.0190e-02,
         1.3159e-01,  8.5815e-02,  9.4604e-03, -6.9519e-02, -1.8042e-01,
         1.5039e-01,  1.1316e-01, -5.2429e-02,  2.1558e-01, -3.6182e-01,
         1.4258e-01,  5.1697e-02,  2.4536e-02, -3.7155e-03, -4.0283e-03,
         5.6787e-01, -7.0679e-02, -2.3120e-01, -2.1851e-02, -1.3550e-02,
         4.3335e-02,  1.3824e-02, -3.3691e-02, -2.6230e-02,  1.2177e-02,
        -7.7271e-02,  2.2064e-02,  6.5674e-02,  2.8442e-02, -3.0322e-01,
        -8.3923e-03, -5.7068e-02, -1.7090e-02,  7.5073e-02,  3.1299e-01,
         4.1199e-03, -1.7395e-02, -2.0166e-01, -1.0056e-02, -1.0278e-01,
         4.6143e-01, -3.0334e-02,  4.9414e-01,  5.8899e-02,  1.4624e-01,
        -2.0752e-03,  1.0376e-03,  4.0283e-03, -1.2812e+00, -2.0508e-02,
        -2.6953e-01,  8.9478e-02, -3.1128e-02,  4.1455e-01,  1.2421e-02,
         1.1139e-02, -1.6943e-01,  1.9800e-01,  1.9824e-01, -8.7128e-03,
         4.4495e-02,  4.4556e-02,  7.3730e-01, -1.3306e-02,  2.1167e-01,
        -1.1609e-01,  9.4147e-03,  3.2324e-01,  3.6377e-02, -1.9791e-02,
        -1.1072e-01,  2.3706e-01, -5.5859e-01,  3.3569e-02, -1.4990e-01,
         2.9102e-01,  7.8430e-03, -1.1572e-01,  3.8635e-02, -2.9266e-02,
        -4.2999e-02, -6.6016e-01,  2.3621e-02, -6.7139e-02,  1.4819e-01,
         2.5635e-02, -1.9678e-01, -5.7495e-02, -2.7881e-01,  6.6956e-02,
         1.3550e-02,  2.1301e-02,  8.9722e-02, -6.0089e-02, -6.1768e-02,
        -6.7139e-03, -2.6016e-02, -6.0303e-02,  1.0168e-01, -4.2816e-02,
         1.9897e-02, -2.4951e-01,  2.7563e-01,  8.5327e-02, -1.4771e-02,
         1.2482e-02,  1.3232e-01, -4.7180e-02, -8.3398e-01, -8.9294e-02,
         1.2964e-01,  2.1997e-01,  4.4922e-01, -2.3413e-01,  2.0325e-02,
         3.4180e-02, -7.1716e-02, -4.1016e-02, -9.0576e-02,  8.3984e-01,
         9.8511e-02, -8.3374e-02,  8.5693e-02,  1.2756e-01, -2.4414e-04,
        -4.9194e-02,  1.6943e-01, -4.4556e-03, -1.0175e-01, -1.3928e-01,
         3.9246e-02,  2.5293e-01, -4.0100e-02, -3.9531e+00, -6.6797e-01,
        -1.1078e-02,  1.7593e-02,  6.7139e-02, -1.2964e-01, -1.6836e+00,
        -6.2256e-02,  2.1582e-01,  4.5349e-02,  8.6060e-03, -1.0217e-01,
        -1.0901e-01,  1.1566e-01,  1.5063e-01,  2.8748e-02,  1.3159e-01,
         3.8971e-02,  1.4258e-01, -5.0000e-01,  2.1069e-01, -8.3130e-02,
        -9.4238e-01,  2.4017e-02, -2.4414e-02, -3.1543e-01, -1.0559e-01,
         8.1238e-02,  3.3057e-01,  1.6144e-02, -3.4424e-02, -6.5308e-02,
        -1.1255e-01,  3.2764e-01,  5.3345e-02, -1.2817e-03,  5.4382e-02,
         1.2158e-01, -9.7046e-03,  2.0947e-01,  1.0938e-01, -1.1017e-02,
         1.0181e-01,  1.0852e-01, -1.2138e-02,  1.3354e-01,  6.6309e-01,
         8.4668e-01,  4.8584e-02,  1.4697e-01, -8.2153e-02,  1.2245e-02,
         2.1973e-03,  5.3101e-03, -5.4932e-04,  1.0461e-01, -1.8494e-02,
        -1.5320e-02, -8.4351e-02, -5.7617e-02,  2.4268e-01, -4.8950e-02,
        -2.3413e-01, -4.0039e-02,  9.5367e-03, -5.2185e-02, -1.8774e-01,
         9.5947e-02, -4.0680e-02,  1.8677e-02,  1.2378e-01, -2.6443e-02,
        -7.3242e-02, -1.7871e-01, -8.5876e-02, -1.0242e-01,  5.6763e-02,
         1.5527e-01,  1.6953e-02,  4.3652e-01,  5.4346e-01,  3.1104e-01,
        -1.8823e-01,  4.7974e-02, -2.2095e-01, -2.9150e-01, -1.0059e+00,
        -1.5503e-02, -5.8691e-01,  3.3966e-02, -1.1841e-02, -1.2146e-02,
        -3.3325e-02, -1.7969e+00, -5.7520e-01,  4.4629e-01, -1.5527e-01,
        -1.8372e-02, -4.6509e-02,  5.4291e-02,  8.8379e-02,  1.9604e-01,
        -7.4414e-01, -3.1885e-01,  7.5989e-02, -3.4375e-01, -6.1157e-02,
         6.6284e-02,  1.1316e-01,  9.0759e-02,  1.9604e-01, -5.0049e-01,
         4.9042e-02, -7.9102e-02,  7.7881e-02, -2.0752e-02,  7.5500e-02,
        -8.8135e-02,  8.9111e-03, -1.4111e-01,  4.9744e-02, -5.7068e-02,
         1.2305e-01,  5.0146e-01, -2.1802e-01,  1.2405e-02,  4.8828e-04,
         1.6211e-01,  3.3752e-02, -3.5461e-02, -3.6133e-02, -6.4087e-02,
         1.9336e-01,  3.5645e-01, -2.0447e-03, -3.8086e-02, -3.9673e-03,
        -3.1250e-01, -8.6426e-02, -3.1799e-02,  2.2656e-01,  2.3242e-01,
        -6.3354e-02, -1.5308e-01, -6.4453e-02, -1.1309e+00, -3.1128e-03,
         1.7969e-01, -4.5959e-02,  2.0532e-01,  6.6895e-02,  3.1555e-02,
         4.6082e-02,  6.0669e-02,  2.0172e-02, -8.5754e-03,  3.9337e-02,
         2.3535e-01,  3.5645e-01,  1.4771e-01,  2.4689e-02,  9.1675e-02,
         3.2745e-02, -1.6785e-02, -2.1704e-01,  7.7271e-02,  3.8037e-01,
         1.0223e-01, -1.8335e-01,  8.0566e-02, -3.2764e-01, -1.0559e-01,
         2.6807e-01,  2.0664e+00,  4.1260e-02,  8.0566e-02, -5.6519e-02,
        -3.2440e-02,  1.8872e-01,  1.7129e+00,  9.3628e-02, -1.2024e-01,
         1.4191e-02,  1.9922e-01,  8.7128e-03, -5.3467e-02, -3.9429e-02,
        -4.3335e-02, -1.3672e-02,  2.8296e-01,  4.2725e-03,  1.8188e-02,
         1.9531e-01, -6.7505e-02, -2.2705e-01, -1.1658e-01, -9.4727e-02,
         1.6748e-01,  1.0461e-01, -2.3010e-02,  9.5825e-02,  1.0430e+00,
        -4.0436e-02,  6.4697e-02,  5.1636e-02,  4.9805e-02, -3.9307e-02,
         1.2250e-01,  2.2034e-02,  1.1340e-01, -3.6035e-01, -8.4381e-03,
         3.0225e-01, -1.2445e-01, -2.0050e-02,  8.8989e-02,  2.9541e-02,
         2.4887e-02,  1.2383e+00,  3.1372e-02, -3.8330e-02,  5.2887e-02,
        -4.5166e-02,  4.1809e-02, -2.5665e-02,  4.5288e-02,  5.8594e-02,
         8.0933e-02, -9.4910e-02,  1.5078e+00, -1.4990e-01,  4.8291e-01,
        -4.6204e-02,  4.5312e-01,  1.6016e-01, -2.9739e-02,  1.0193e-01,
        -2.5117e+00,  3.3936e-01, -5.0049e-01,  8.8806e-02,  6.7932e-02,
         4.7119e-02,  5.5664e-02,  1.5808e-02,  6.1768e-02,  1.6211e-01,
        -6.7383e-02,  4.9194e-02, -1.3623e-01,  8.8501e-03,  1.0669e-01,
        -4.6448e-02, -5.4932e-02, -5.2826e-02,  8.6426e-02,  2.2278e-02,
        -5.6641e-01,  1.7871e-01,  6.1035e-05,  3.1799e-02, -3.7500e-01,
         7.5073e-02, -3.4119e-02, -6.8359e-02,  3.9581e-02,  6.4209e-02,
        -5.0568e-02, -1.9684e-02, -8.7952e-02,  8.0811e-02, -3.4863e-01,
        -2.6514e-01,  9.3262e-02, -1.0132e-02,  1.9531e-01, -1.6846e-01,
         3.7891e-01,  3.1799e-02,  2.8595e-02,  2.2510e-01,  1.0437e-01,
         7.9590e-02,  3.1543e-01, -1.0236e-01,  5.3528e-02,  5.5859e-01,
         4.1602e-01, -6.5625e-01, -3.5065e-02,  2.9877e-02, -3.0078e-01,
         7.1350e-02, -1.4807e-01, -5.8044e-02, -6.8420e-02,  1.0400e-01,
        -4.1260e-02,  9.2163e-03,  1.7303e-02,  4.4556e-02, -6.9214e-02,
        -8.0200e-02, -2.8992e-02,  6.9519e-02, -2.1924e-01,  7.2083e-02,
         3.1934e-01,  5.6824e-02, -9.3384e-02, -5.8899e-02, -3.6316e-03,
        -2.1289e-01,  6.1035e-02, -1.3074e-01,  2.0020e-02,  6.5430e-02,
        -1.7212e-02,  2.6611e-01,  1.5259e-02,  3.3008e-01, -4.0137e-01,
        -2.4609e+00,  7.0374e-02,  1.0107e-01,  6.8115e-02, -1.5527e-01,
         6.0840e-01,  1.7651e-01, -9.8755e-02, -1.1726e-02,  1.5320e-02,
        -1.7578e+00,  7.8027e-01], device='cuda:7', dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True task_net module.module.task_net.layer_norm.weight True tensor([ 5.7812e-01,  7.8125e-01,  2.3047e-01, -1.2524e-01,  4.0161e-02,
         2.3193e-02,  1.8945e-01, -3.5645e-02,  3.9551e-01,  2.0557e-01,
         7.3828e-01, -8.9600e-02, -9.5459e-02,  3.4277e-01,  9.8877e-02,
        -4.6143e-02,  1.6553e-01,  1.2646e-01,  4.1797e-01,  2.4512e-01,
         2.8027e-01,  8.6133e-01, -2.5146e-01, -1.6309e-01,  3.2178e-01,
         1.7548e-03, -1.8750e-01,  2.5586e-01,  2.9199e-01,  2.8906e-01,
         4.5605e-01,  1.6650e-01,  1.1230e+00, -5.5859e-01,  1.6089e-01,
         1.6309e-01,  2.2217e-02, -4.6289e-01,  1.7041e-01,  4.9316e-01,
         9.4727e-01,  3.8818e-02, -8.2764e-02,  9.8389e-02,  2.9785e-01,
         1.6736e-01,  6.1328e-01,  4.5312e-01,  3.3008e-01, -2.6416e-01,
         3.1299e-01,  1.3342e-01, -2.2217e-02,  9.1797e-02, -9.5703e-02,
         6.0352e-01,  2.6221e-01,  4.5117e-01,  2.7893e-02,  2.7734e-01,
        -4.9414e-01,  6.6602e-01, -3.3301e-01,  4.5898e-01,  2.0801e-01,
         4.8242e-01, -1.7319e-03,  1.4795e-01, -8.9722e-03,  2.9199e-01,
         1.8848e-01,  5.9961e-01, -7.5928e-02,  6.9727e-01, -5.7422e-01,
        -9.4299e-03,  1.4307e-01,  9.1797e-02, -1.5137e-02,  6.3477e-02,
         3.2043e-03,  6.6992e-01,  2.3584e-01,  3.6774e-03,  2.3926e-01,
         2.9297e-02,  7.2266e-01, -1.5625e-01,  1.7188e-01,  1.6895e-01,
         2.7930e-01,  6.3281e-01,  1.1377e-01, -1.6367e+00,  6.3281e-01,
         6.5234e-01,  2.5244e-01,  8.5352e-01, -2.7344e-02,  5.3787e-04,
         4.7070e-01,  5.0342e-01, -3.1348e-01,  4.5508e-01,  3.5248e-03,
         1.5259e-02,  1.7676e-01, -7.6562e-01,  5.9766e-01,  3.1396e-01,
         3.0176e-01,  4.3262e-01, -5.3906e-01,  2.8931e-02,  3.0371e-01,
         3.4814e-01,  4.5020e-01, -1.0156e-01,  1.5479e-01,  3.0713e-01,
        -1.8848e-01,  3.1006e-01,  4.5459e-01,  1.1060e-01,  2.8763e-03,
         1.4795e-01, -8.3984e-02, -1.0681e-02,  3.9941e-01,  2.5439e-01,
         2.7246e-01,  1.9702e-01,  4.6875e-01,  7.6367e-01, -1.7639e-02,
         2.6416e-01,  5.5957e-01,  5.6915e-02,  1.8164e-01,  5.1758e-01,
         2.1143e-01, -3.0319e-02,  1.9922e-01, -2.1149e-02,  3.2812e-01,
         4.0234e-01, -4.7119e-02,  1.2402e-01, -1.9897e-02,  3.7500e-01,
         4.0332e-01,  1.0413e-01,  3.6316e-02, -3.6523e-01,  1.5381e-02,
        -3.2227e-02, -2.6270e-01, -1.0742e-01, -3.1055e-01,  2.3535e-01,
         5.7031e-01, -2.6489e-02,  2.3145e-01, -2.1826e-01, -6.8115e-01,
         1.8823e-01,  5.1880e-02,  1.9336e-01, -2.0459e-01,  3.7842e-02,
         5.9277e-01,  8.5352e-01,  4.8535e-01,  1.5039e-01,  8.9844e-02,
         1.0370e-01, -1.5723e-01,  2.5000e-01, -5.0635e-01, -7.8711e-01,
         1.4014e-01,  3.7695e-01,  9.7900e-02, -2.8809e-02, -3.9062e-03,
        -1.2543e-02,  5.5566e-01,  1.8860e-02,  2.7734e-01,  3.4521e-01,
         5.6543e-01, -1.1450e-01,  3.4937e-01,  7.5439e-02,  4.1260e-02,
        -9.5825e-03,  1.4429e-01, -5.7227e-01, -1.8433e-01, -1.1279e-01,
        -6.5234e-01,  3.1152e-01,  3.9258e-01, -1.4941e-01,  1.0010e-02,
        -2.4994e-02, -2.3071e-02,  1.2866e-01, -4.8096e-02,  6.3477e-02,
         2.3828e-01, -9.7656e-02,  4.5801e-01,  6.6992e-01, -3.8818e-02,
         5.7617e-01,  4.6680e-01,  4.5703e-01,  1.2207e-02,  1.6748e-01,
         1.0425e-01,  1.1276e-02,  1.1414e-02, -3.5938e-01, -7.4219e-01,
         1.3379e-01,  2.7246e-01,  6.4258e-01, -4.9805e-02, -1.4648e-03,
         4.7656e-01,  5.7324e-01,  3.8086e-02,  5.0098e-01,  3.5742e-01,
         6.6406e-01,  1.2012e-01, -1.2915e-01,  5.4395e-01,  3.0518e-04,
         4.2383e-01,  3.7402e-01,  3.3691e-02,  2.8223e-01,  3.6792e-01,
         6.3672e-01,  4.4336e-01,  4.0137e-01,  3.9453e-01,  1.0010e-01,
         2.0605e-01,  3.1543e-01, -3.1738e-02,  1.4075e-01,  3.1348e-01,
         2.4609e-01,  3.7012e-01, -5.0195e-01, -1.8604e-01, -4.1016e-02,
        -1.5527e-01,  6.3599e-02,  1.4746e-01,  9.8047e-01, -2.7734e-01,
         3.0859e-01,  2.9102e-01, -8.6212e-04,  5.4688e-01, -1.0254e-02,
         5.7812e-01, -2.3516e+00, -2.8906e-01, -4.6094e-01,  1.3354e-01,
         1.3916e-02,  8.9258e-01,  2.0654e-01, -2.4292e-02,  4.7070e-01,
        -8.9453e-01,  4.9805e-01,  1.0364e-01,  6.1719e-01,  5.8105e-02,
         6.5820e-01,  1.3867e-01,  2.0117e-01, -2.3730e-01, -6.2305e-01,
         4.4141e-01,  4.6582e-01,  8.1250e-01,  2.9785e-01,  2.6758e-01,
         4.6484e-01,  5.5664e-01,  3.9258e-01, -2.8076e-02,  3.2812e-01,
         5.4504e-02, -2.5391e-02,  7.2070e-01,  3.4961e-01,  6.6992e-01,
        -2.9053e-01,  6.8359e-02,  4.8535e-01, -1.1292e-02, -4.2114e-02,
         5.0098e-01, -4.0332e-01,  3.3887e-01,  7.1680e-01,  3.8867e-01,
        -2.4609e-01, -9.3262e-02,  3.4229e-01, -1.6699e-01,  7.5000e-01,
         6.4819e-02, -1.1108e-02,  2.4805e-01, -8.7109e-01,  1.7627e-01,
        -2.1271e-02,  4.6484e-01,  3.8867e-01,  4.4141e-01,  1.8506e-01,
         3.0957e-01,  8.4961e-01, -2.1545e-02,  2.5684e-01,  5.3062e-03,
        -1.5137e-01, -4.0234e-01,  7.5195e-02, -1.8402e-02,  2.6953e-01,
         8.0078e-01,  2.0691e-02, -3.2422e-01,  4.4312e-02,  6.4062e-01,
         2.1387e-01,  5.3320e-01,  3.7402e-01, -3.5156e-01, -3.8330e-02,
         1.3110e-01, -2.5938e+00,  2.5928e-01,  3.5614e-02,  3.7500e-01,
         3.3691e-01,  2.2107e-01, -3.0273e-01,  4.7949e-01, -2.9443e-01,
         2.3438e-01, -2.5684e-01,  2.4121e-01,  4.1211e-01,  2.0020e-01,
        -2.2217e-02,  6.3281e-01, -2.1875e-01,  6.0742e-01,  3.1299e-01,
         6.8164e-01, -6.1523e-02,  2.8711e-01,  8.3008e-02,  6.8164e-01,
         3.9380e-01,  1.8457e-01,  7.2510e-02, -1.6650e-01,  7.6172e-02,
         7.6294e-03,  2.8711e-01,  2.6758e-01,  3.0469e-01, -1.6064e-01,
         2.3877e-01,  3.7384e-04,  2.3047e-01, -1.7773e-01,  3.4082e-01,
         2.8711e-01, -1.0864e-01,  2.1875e-01, -1.1865e-01,  3.1836e-01,
         1.3641e-02, -1.7188e+00,  4.3066e-01,  5.3833e-02,  2.8418e-01,
         5.1562e-01,  3.5254e-01,  1.8652e-01,  4.4189e-02, -5.1514e-02,
         2.2705e-01,  1.8701e-01, -4.9219e-01,  3.4082e-01,  1.8604e-01,
         8.0811e-02, -4.7070e-01,  3.6865e-01,  3.4668e-01,  2.5635e-01,
        -1.4961e+00, -1.7920e-01, -6.6797e-01, -1.3550e-02, -9.6558e-02,
         4.9414e-01,  3.8184e-01,  6.3477e-01,  8.4766e-01,  2.9785e-01,
        -6.8970e-02,  3.1543e-01,  1.6455e-01,  5.3711e-01,  4.9365e-01,
         1.5625e-01,  3.6133e-01,  2.1680e-01,  2.1826e-01,  5.6055e-01,
         5.1758e-02, -2.0117e-01,  4.1699e-01,  2.4902e-02, -7.3242e-02,
         1.5063e-01,  4.9072e-02,  1.1670e-01, -2.4902e-02,  5.4297e-01,
         1.9531e-01,  3.8330e-02, -1.1157e-01, -6.6650e-02, -1.9141e-01,
        -1.5898e+00,  4.4141e-01,  4.3652e-01, -4.8242e-01, -7.4219e-02,
         5.2246e-01,  5.6055e-01,  4.4629e-01, -5.9375e-01,  5.5469e-01,
        -6.4209e-02, -3.3105e-01, -4.4312e-02,  2.9688e-01, -3.0566e-01,
        -6.9336e-01, -3.3984e-01, -3.0518e-02,  2.5293e-01, -2.8320e-01,
         5.0049e-03, -1.5430e-01, -8.1543e-02, -1.0547e-01,  1.6211e-01,
         7.1289e-01,  3.9355e-01,  6.5918e-02,  3.3594e-01,  1.0803e-02,
         7.1289e-01,  3.7598e-01,  2.7930e-01,  2.3328e-01,  4.6387e-02,
        -3.8574e-01,  9.0637e-03,  2.9785e-02,  4.8401e-02,  2.4963e-02,
         7.0312e-02,  6.3867e-01, -1.6992e-01,  4.2285e-01,  1.7383e-01,
         2.8613e-01,  8.1970e-02,  3.8281e-01, -2.3438e-02, -2.3047e-01,
        -7.4023e-01,  3.2910e-01, -1.7480e-01,  1.6284e-01, -1.5137e-01,
        -4.4238e-01,  4.9023e-01,  1.7480e-01,  4.7656e-01,  3.9062e-01,
        -1.0781e+00, -5.5664e-01], device='cuda:3', dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([ 9.6289e-01,  5.4980e-01, -6.5625e-01, -1.0327e-01, -1.1646e-01,
         1.1230e-02,  2.6807e-01,  4.5898e-01, -1.9775e-01,  2.5439e-01,
        -6.7969e-01,  4.8047e-01,  1.3550e-01, -2.0752e-01, -2.7686e-01,
        -3.7720e-02, -1.2085e-01, -6.6016e-01, -2.9785e-01, -5.1465e-01,
         3.0957e-01,  1.5195e+00, -8.1836e-01, -1.1699e+00, -8.5938e-01,
         2.7686e-01,  7.8711e-01, -2.7002e-01,  1.4893e-01,  2.8174e-01,
         2.3320e+00,  4.4043e-01, -4.9805e-01, -2.5547e+00, -6.6797e-01,
         2.6562e-01, -8.1250e-01,  1.0664e+00, -2.7686e-01,  6.4258e-01,
        -9.0820e-01,  5.4590e-01,  9.1406e-01,  4.8535e-01,  5.2539e-01,
         6.4258e-01,  6.4453e-01,  3.8965e-01, -4.6094e-01, -8.1641e-01,
         7.7734e-01,  5.7227e-01, -3.0176e-01,  8.7500e-01, -1.4375e+00,
         7.8906e-01,  1.9922e-01,  4.9219e-01,  2.3132e-02,  4.1797e-01,
         2.1680e+00, -6.4062e-01, -9.8242e-01, -4.1895e-01, -3.2031e-01,
         5.3418e-01, -2.1191e-01,  1.0596e-01, -9.0332e-03, -3.1836e-01,
         1.8018e-01, -4.9707e-01,  2.4707e-01,  5.1855e-01, -1.3594e+00,
        -3.4302e-02,  1.2085e-01,  1.2573e-01,  2.4048e-02,  1.2773e+00,
        -4.2725e-02, -5.1953e-01, -9.1797e-01,  4.7363e-02, -5.4004e-01,
         1.8086e+00, -5.3125e-01,  2.3633e+00,  2.2607e-01,  6.9531e-01,
        -3.2227e-01, -5.0195e-01,  1.3916e-01, -5.0938e+00, -5.4395e-01,
        -1.3516e+00, -2.0166e-01, -6.0938e-01,  1.7812e+00,  1.8959e-03,
        -4.2676e-01, -7.8320e-01,  7.9102e-01,  9.3164e-01,  1.3794e-02,
         1.2720e-01, -2.1777e-01,  2.4531e+00,  3.8770e-01,  9.7461e-01,
        -7.6953e-01, -3.6621e-01,  1.2148e+00,  6.9580e-02, -2.5781e-01,
        -4.6582e-01,  1.1543e+00, -2.3906e+00,  2.1191e-01, -8.8281e-01,
         1.2305e+00, -3.1738e-01, -7.9688e-01,  1.5771e-01,  4.9286e-03,
        -3.0371e-01, -2.4688e+00,  1.3184e-01, -5.7715e-01,  7.3242e-01,
         2.3682e-01, -9.6094e-01, -6.4258e-01, -1.4336e+00,  8.2520e-02,
        -3.4766e-01, -2.9492e-01,  3.5742e-01,  1.4502e-01, -5.4980e-01,
         2.3682e-01, -9.0576e-02,  1.2598e-01,  4.9414e-01,  2.7490e-01,
        -3.5547e-01, -1.0977e+00,  1.1797e+00,  4.2480e-01, -4.3750e-01,
        -4.2285e-01,  5.7812e-01,  6.0425e-02, -3.0234e+00, -3.6914e-01,
         5.1855e-01,  9.5312e-01,  1.8633e+00, -9.9414e-01,  2.2461e-01,
         4.5410e-01, -2.1118e-02,  1.5674e-01, -4.6191e-01,  3.0859e+00,
         5.5469e-01,  6.4941e-02,  4.5215e-01,  3.2715e-01, -1.2769e-01,
        -5.1074e-01,  1.0586e+00,  4.1797e-01, -4.3066e-01, -7.4414e-01,
         2.5000e-01,  1.0723e+00,  2.6123e-01, -9.6562e+00, -2.3242e+00,
         1.9775e-01, -3.0371e-01,  4.1504e-01, -6.7578e-01, -5.0156e+00,
        -2.7954e-02,  1.0371e+00,  1.8896e-01, -3.6621e-01, -5.9961e-01,
        -7.2266e-01,  9.3018e-02,  7.5586e-01,  2.3730e-01,  6.7188e-01,
         9.1553e-02,  7.1289e-01, -1.9609e+00,  8.4961e-01, -3.4277e-01,
        -3.3281e+00, -3.2910e-01, -4.3652e-01, -1.4258e+00, -4.7852e-01,
         3.9551e-02,  1.4414e+00,  9.8633e-02, -3.3875e-02, -4.5410e-01,
        -6.1133e-01,  1.3359e+00, -3.4961e-01, -5.1074e-01,  1.5430e-01,
         7.4023e-01, -3.7695e-01,  1.1113e+00,  5.9375e-01,  1.8018e-01,
         3.6133e-01,  2.6025e-01,  3.8574e-02,  3.2324e-01,  2.5625e+00,
         3.3125e+00,  2.8369e-01,  7.9883e-01, -3.5059e-01, -8.4717e-02,
        -4.3652e-01, -4.1602e-01, -2.4170e-01,  5.7227e-01, -3.8477e-01,
         4.5312e-01, -3.7207e-01, -2.4170e-01,  1.2500e+00, -5.0781e-02,
        -1.1113e+00, -3.9551e-01, -1.6846e-01,  2.3877e-01, -9.1992e-01,
         7.3242e-01, -3.7500e-01,  4.1406e-01,  7.5781e-01,  1.7578e-01,
        -4.7559e-01, -7.8711e-01, -3.1934e-01, -5.9570e-01,  4.2090e-01,
         7.0703e-01, -3.7012e-01,  1.6211e+00,  2.2109e+00,  1.1152e+00,
        -8.9844e-01, -1.4941e-01, -1.0703e+00, -1.3633e+00, -3.6641e+00,
         3.2812e-01, -2.6484e+00, -8.1329e-03,  3.7207e-01, -1.1011e-01,
        -5.4492e-01, -5.5625e+00, -2.1016e+00,  2.1094e+00, -8.4766e-01,
        -2.8613e-01, -6.1328e-01, -1.9336e-01,  4.6631e-02,  9.2578e-01,
        -2.7578e+00, -1.5117e+00,  2.6172e-01, -1.6250e+00,  7.3242e-02,
         5.7129e-01,  5.4492e-01, -1.5234e-01,  8.6328e-01, -2.0820e+00,
        -3.9062e-01, -5.8008e-01,  6.0938e-01, -4.8633e-01, -3.0469e-01,
        -6.5430e-01, -4.2969e-01, -8.0078e-01,  5.6152e-02, -3.0469e-01,
         4.4043e-01,  1.8867e+00, -9.9609e-01, -3.0566e-01, -5.0879e-01,
         6.1523e-01,  2.4902e-01, -4.0137e-01, -5.6030e-02, -1.4795e-01,
         1.0059e+00,  1.4805e+00,  2.9980e-01, -5.4004e-01, -4.8633e-01,
        -1.2266e+00, -4.2676e-01, -3.1543e-01,  7.3242e-01,  1.1660e+00,
        -2.7637e-01, -5.4785e-01, -3.1152e-01, -4.0078e+00,  2.1338e-01,
         5.1562e-01,  3.1152e-01,  9.1406e-01,  5.4492e-01,  2.9688e-01,
         2.5781e-01,  6.9727e-01, -2.2888e-02,  2.5586e-01, -6.9336e-02,
         9.5703e-01,  1.1367e+00, -7.0435e-02, -8.1543e-02,  4.1211e-01,
        -5.2246e-01,  1.3647e-01, -9.4141e-01,  1.3037e-01,  1.6523e+00,
        -2.0801e-01, -1.0078e+00, -1.9336e-01, -1.3711e+00, -4.8047e-01,
         1.0605e+00,  6.4688e+00,  4.0625e-01,  3.2031e-01, -4.1016e-01,
         2.8955e-01,  8.0469e-01,  5.6406e+00,  5.7422e-01, -4.3457e-01,
         2.2314e-01,  6.9531e-01, -3.0176e-01, -5.6152e-01, -4.9609e-01,
        -2.1045e-01, -5.2832e-01,  1.0898e+00, -4.6484e-01,  2.8760e-01,
         1.1309e+00, -2.6123e-01, -9.9805e-01,  1.0107e-01, -5.4492e-01,
         8.4961e-01,  5.5078e-01, -1.9629e-01,  1.6699e-01,  3.8516e+00,
         1.4893e-02, -2.5000e-01,  3.4277e-01,  2.7734e-01, -3.4668e-01,
        -1.3867e-01,  6.8970e-02,  5.4492e-01, -1.6094e+00,  2.9248e-01,
         1.3008e+00, -5.7373e-02,  2.4609e-01,  1.4526e-01,  4.2578e-01,
        -9.5215e-02,  4.4531e+00, -3.9258e-01,  6.1768e-02, -2.9688e-01,
        -5.2051e-01,  2.7246e-01, -1.7822e-01, -1.5527e-01,  3.3203e-01,
        -1.2280e-01, -4.4336e-01,  5.0000e+00, -8.2812e-01,  2.0430e+00,
        -2.4219e-01,  1.8906e+00,  7.4023e-01,  2.8564e-01, -1.7041e-01,
        -7.1406e+00,  1.3594e+00, -2.1680e+00,  1.8457e-01,  1.4038e-01,
         3.7207e-01,  4.8535e-01, -4.6484e-01,  6.4062e-01,  8.0859e-01,
        -5.6274e-02,  3.4277e-01, -6.2891e-01, -4.8340e-01,  6.6797e-01,
         8.3252e-02, -5.1465e-01,  1.7969e-01, -1.8750e-01,  4.4043e-01,
        -2.3828e+00,  6.2695e-01,  3.3594e-01,  2.5439e-01, -1.7773e+00,
         4.5508e-01,  9.6924e-02, -4.2676e-01,  5.6763e-02,  4.7559e-01,
        -3.2812e-01,  1.0547e-01, -7.1777e-02,  3.5547e-01, -1.5352e+00,
        -6.9336e-01, -2.2070e-01, -4.6191e-01,  8.8281e-01, -7.2852e-01,
         1.7109e+00, -3.8379e-01, -4.5020e-01,  3.7891e-01,  6.4453e-01,
         3.2617e-01,  1.3711e+00, -7.9834e-02, -3.5059e-01,  2.2539e+00,
         1.5820e+00, -2.6094e+00, -3.7720e-02,  2.5000e-01, -1.3164e+00,
         1.1011e-01, -5.6543e-01, -3.0762e-01, -2.8760e-01, -7.4219e-02,
        -5.8984e-01,  3.9258e-01, -1.1401e-01,  3.7598e-01,  6.4850e-03,
        -7.0703e-01,  2.5781e-01, -2.6025e-01, -1.0059e+00,  1.7529e-01,
         1.2773e+00,  1.2158e-01, -5.1855e-01, -3.3789e-01,  5.1147e-02,
        -9.8242e-01,  5.5664e-01, -6.0547e-01,  3.8672e-01, -1.8262e-01,
        -4.2676e-01,  1.1543e+00,  2.8418e-01,  1.2891e+00, -1.4297e+00,
        -7.0312e+00, -3.1445e-01,  2.8174e-01,  3.3301e-01, -7.8516e-01,
         2.3320e+00,  8.7109e-01, -5.4785e-01,  2.6562e-01, -3.6914e-01,
        -5.7188e+00,  2.6562e+00], device='cuda:3', dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[-0.1475,  0.2778, -0.1736,  ..., -0.5708, -0.2993,  0.2412],
        [ 0.1365,  0.3281, -1.5742,  ..., -2.1211, -0.6890, -0.4756],
        [ 0.0493,  0.3018, -0.4575,  ...,  0.2593,  0.1201, -0.2546],
        ...,
        [-0.1318, -1.1748,  2.2891,  ...,  0.2390, -0.4336,  1.5273],
        [ 0.0257, -0.0415,  0.4517,  ...,  0.1499, -0.1614,  0.3621],
        [-0.2228, -0.3730, -0.6509,  ..., -0.0621,  0.0117,  0.3896]],
       device='cuda:3', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([  4.4883,  11.8281,   1.8398,  -0.4707,   1.8438,  -3.4609,   6.2812,
         -3.7266,  -4.7266,  -4.4844,   8.7109,   4.5938,  -7.8203,  -5.3203,
         13.4375,   7.9805,  -0.3652,  -4.0391,  12.8750,   7.7969,   3.1523,
         -6.2656,  -2.4688,  -2.5234,   3.0781,  -2.4805,   8.0312,  -4.3281,
         -4.2344,  -9.2500,  -4.2344,  -4.4062,  -5.5312,  -6.5391,  -3.0352,
          5.5469,   8.2188,  -5.5703,  16.7188,  -6.1875,  -2.8086,   4.5234,
         12.6094,   9.5469,  -5.5391,   9.6250,   5.6016,  10.4297,   2.7969,
         -3.1562,  -3.6953,   2.4258,  11.5000,  -8.3750,  -3.3281,   3.0469,
         -0.4961,  -3.3359,  -5.8750,   8.8906,  11.6250,  10.2344,   4.8906,
          9.7500,  -4.2109,   3.2070,  -3.3906,   9.6875,  -6.5781,  -6.3281,
         11.2812,   1.8438,  -0.2852,   7.5195, -11.0156,   0.5586,  -7.7500,
          9.9453,   7.4375,   5.5898,   3.1094,  -5.0352,   3.3516,  -3.0312,
          1.4531,  -2.3906,  -6.4570,  -3.7812,   8.0078,  -6.7344,  -2.5938,
          3.2734,  -2.8672,   1.9844,   2.8594,  -8.2500,  -9.5625,  11.3750,
          4.0938,  -3.4609,   4.5156,  -1.1895,  -0.2695,   7.9453,  -6.9922,
         -3.9805,  -4.0820,   6.0469,   5.8984,  -4.0391,  -5.6641,  -1.0625,
         -8.0312,  -5.6484,  -1.9277,  -4.1484,  -5.1719,   8.7500,  -2.6836,
          3.4258,  -2.5645,   1.8203,   9.8906,  -2.8965,   2.6250,  11.2422,
          9.4062,  -7.7188,  -2.9785,   5.0156,   5.5703,   3.6094,  -5.7656,
          0.3281,  -2.4453,  -5.5469,   8.2188,  -3.9141,   2.5273,   7.4844,
         -2.0547,   3.1641,   5.4062,   8.9844,   5.3125,  -6.3125,   0.0391,
          4.7930,  -3.0938,   4.5039,  14.5625,   5.0703,   6.1250,  -7.3516,
          2.0840, -10.5312,  -2.4141,   2.8594,  -1.6641,  15.0781,  -3.3828,
         -2.1250,   9.2969,   3.5703,   0.9482,   2.7109,  -1.7031,  11.2500,
          4.5234,  -1.3281,   0.7656,  -2.7891,   2.3320,  -9.3906,  -3.4531,
         -5.3203,  -6.2812,   7.5156,   6.0156,   6.0391,   7.2812,   3.8750,
          5.6406,  -7.1953,  11.9609, -10.1094,  -1.1797,   6.7500,  -3.8984,
          7.2812,  -1.8027,   5.5938,   1.5312,   1.9453,   7.0469,  -0.1680,
          2.0000,   2.4609,  -5.7109,  -1.5742,   6.1562,  12.7656,   1.9688,
          4.3750,   2.9844,  -4.2656,   2.6875,   6.6875,  -2.1250,   5.2422,
          2.1875,   5.8750,  -6.0703,  -5.4375,   2.3281,   7.0859,  -8.7031,
          4.3320,   4.5469,   0.6094,  -7.7422,   3.0605,  -0.5391,  -0.6680,
         -4.8906,  -5.1641,   6.9961,   2.5234,  -1.0547,  -1.6445,  -6.9297,
         -1.0469,   6.7891,  -3.3203,   4.3438,   2.1660,   7.1250,  -5.7188,
         -3.5469,  -0.3906,   4.5781,  10.0156,   2.8984,  -5.6406,   5.2031,
         -5.4062, -11.5859,  -3.4258,   2.9219,   0.7969,  25.4062,   0.7734,
          3.5156,  -8.1875,  -1.6211,   5.3867], device='cuda:3',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 3.6041e-02,  1.5112e-01,  9.2773e-03,  ...,  1.7798e-01,
          4.4629e-01,  2.0020e-02],
        [ 1.0178e-02,  4.9866e-02,  2.4872e-03,  ...,  6.1035e-02,
          9.6191e-02, -1.0262e-02],
        [-2.4994e-02, -9.2529e-02, -1.2512e-03,  ..., -1.1548e-01,
         -3.1055e-01, -1.7181e-02],
        ...,
        [-1.2054e-03, -1.2619e-02,  2.4414e-04,  ...,  4.8218e-03,
          6.3477e-03,  1.4706e-03],
        [-2.8491e-01, -1.2480e+00, -8.7402e-02,  ..., -1.2559e+00,
         -4.2891e+00, -3.4790e-01],
        [ 1.2927e-01,  5.4541e-01,  3.1006e-02,  ...,  5.8008e-01,
          1.6641e+00,  1.0498e-01]], device='cuda:3', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([ 2.3682e-01,  3.3203e-02, -1.7090e-01, -1.4734e-01,  2.4994e-02,
        -1.0443e-01,  3.6865e-02,  1.8164e-01,  1.0712e-01,  2.8076e-02,
        -1.2793e-01,  1.7676e-01,  1.1621e-01,  1.0779e-01, -5.6519e-02,
        -2.7222e-02,  7.4829e-02, -1.9214e-01,  1.1328e-01, -1.1328e-01,
         1.6846e-02,  4.5117e-01, -2.5586e-01, -3.4473e-01, -1.9336e-01,
         1.3208e-01,  2.9736e-01,  3.1586e-02, -1.1292e-01,  4.7241e-02,
         7.3438e-01,  1.6113e-01, -6.1157e-02, -8.4375e-01, -1.6406e-01,
         4.3091e-02, -1.6846e-01,  4.2871e-01, -9.1553e-03,  1.4600e-01,
        -1.7773e-01,  2.0068e-01,  2.6221e-01,  1.8018e-01,  1.2378e-01,
         1.6309e-01,  1.2451e-01,  1.2085e-02, -8.2764e-02, -2.9395e-01,
         2.3096e-01,  1.7139e-01, -8.0444e-02,  2.8906e-01, -5.0293e-01,
         1.8896e-01,  4.4800e-02,  4.2969e-02, -1.3504e-02,  3.0029e-02,
         7.8906e-01, -1.0571e-01, -3.4814e-01, -1.9775e-02, -9.0332e-03,
         4.8096e-02,  7.9956e-03, -5.4138e-02, -4.0070e-02,  2.5635e-03,
        -1.0510e-01, -4.1504e-03,  1.0596e-01,  2.8320e-02, -4.3848e-01,
        -1.2756e-02, -8.5327e-02, -2.0569e-02,  1.3269e-01,  4.3359e-01,
         1.9135e-02, -2.4902e-02, -2.6318e-01, -1.4526e-02, -1.4966e-01,
         6.2305e-01, -3.7842e-02,  8.2617e-01,  4.5288e-02,  1.6895e-01,
         4.5166e-03, -2.3438e-02,  6.1035e-04, -1.9180e+00, -5.8838e-02,
        -4.1406e-01,  8.8379e-02, -5.7861e-02,  6.3379e-01, -1.4954e-03,
         1.2207e-02, -2.1729e-01,  3.0273e-01,  2.9736e-01,  3.7155e-03,
         6.7993e-02,  5.0751e-02,  1.0137e+00,  5.1270e-03,  3.0957e-01,
        -1.7383e-01,  5.6152e-03,  4.0723e-01,  4.9927e-02, -6.7139e-03,
        -1.3647e-01,  3.2520e-01, -7.8516e-01,  4.5410e-02, -1.8896e-01,
         4.1309e-01,  2.0752e-02, -1.9385e-01,  3.1128e-02, -4.7668e-02,
        -5.3467e-02, -8.6914e-01,  4.2480e-02, -1.0352e-01,  2.0410e-01,
         6.5308e-03, -2.4121e-01, -7.5195e-02, -4.7949e-01,  1.0193e-01,
         1.0986e-02,  3.9368e-02,  1.2573e-01, -7.0007e-02, -7.5684e-02,
        -6.1035e-04, -3.6011e-02, -7.8369e-02,  1.5186e-01, -5.3406e-02,
         1.8372e-02, -3.3789e-01,  3.3203e-01,  1.4648e-01, -1.0864e-02,
         1.2207e-03,  2.3438e-01, -4.0527e-02, -1.0410e+00, -1.1304e-01,
         1.8506e-01,  2.9053e-01,  6.3281e-01, -3.3936e-01,  1.7456e-02,
         6.7139e-02, -9.1736e-02, -5.3955e-02, -1.5649e-01,  1.1523e+00,
         1.5967e-01, -7.6843e-02,  1.2256e-01,  1.9702e-01,  1.0986e-03,
        -7.0068e-02,  2.8418e-01,  5.1270e-03, -1.2939e-01, -1.8164e-01,
         4.8828e-02,  3.4814e-01, -3.9062e-02, -4.7422e+00, -9.1016e-01,
        -2.2095e-02,  1.7700e-02,  1.0059e-01, -1.5332e-01, -2.2734e+00,
        -6.8970e-02,  3.3252e-01,  6.0913e-02,  2.7222e-02, -1.5967e-01,
        -1.4941e-01,  1.4661e-01,  2.1143e-01,  3.5034e-02,  1.9629e-01,
         5.5176e-02,  1.9287e-01, -6.4941e-01,  2.9785e-01, -1.1719e-01,
        -1.1973e+00,  4.0192e-02, -4.3457e-02, -4.5703e-01, -1.6748e-01,
         1.0071e-01,  5.1758e-01,  3.1738e-03, -5.3650e-02, -7.6172e-02,
        -1.6675e-01,  3.9355e-01,  7.3120e-02, -2.4902e-02,  1.0376e-01,
         1.5625e-01, -1.5869e-03,  2.9980e-01,  1.7969e-01, -1.8555e-02,
         1.3477e-01,  1.3232e-01, -1.4923e-02,  1.9873e-01,  9.4727e-01,
         9.6484e-01,  4.1138e-02,  1.6162e-01, -1.0693e-01, -4.7188e-03,
         1.1597e-02,  2.9297e-03, -2.4414e-02,  1.5381e-01, -1.5625e-02,
         2.9541e-02, -1.1255e-01, -7.7271e-02,  3.7598e-01, -4.7241e-02,
        -3.3691e-01, -5.2002e-02,  1.2970e-02, -5.0446e-02, -2.3096e-01,
         1.4746e-01, -3.2471e-02,  3.7354e-02,  1.6162e-01, -3.6011e-03,
        -7.9834e-02, -2.4365e-01, -1.0876e-01, -1.4478e-01,  7.9590e-02,
         1.8750e-01, -1.5991e-02,  6.2695e-01,  7.6172e-01,  4.6875e-01,
        -2.7930e-01,  4.3640e-02, -2.8467e-01, -3.6816e-01, -1.4805e+00,
         9.0332e-03, -7.5000e-01,  2.7527e-02,  1.2207e-04,  2.0142e-03,
        -3.3691e-02, -2.4609e+00, -8.2227e-01,  6.5137e-01, -2.0459e-01,
        -4.0161e-02, -7.9346e-02,  7.2998e-02,  9.7046e-02,  2.4023e-01,
        -9.3164e-01, -4.5410e-01,  1.1511e-01, -4.4629e-01, -6.0974e-02,
         9.6924e-02,  1.2573e-01,  1.2274e-01,  3.0225e-01, -7.0605e-01,
         6.9153e-02, -8.4473e-02,  1.1719e-01, -3.2471e-02,  9.0576e-02,
        -9.3994e-02,  1.3428e-02, -2.1582e-01,  7.3364e-02, -3.9795e-02,
         1.5747e-01,  6.3574e-01, -3.0029e-01,  2.2766e-02, -1.1108e-02,
         2.6514e-01,  5.8594e-02, -3.6987e-02, -4.5959e-02, -8.3923e-02,
         2.7295e-01,  5.6543e-01, -2.8320e-02, -4.7363e-02, -1.1475e-02,
        -3.9160e-01, -1.2695e-01, -3.1372e-02,  2.9639e-01,  3.0469e-01,
        -7.0435e-02, -1.7749e-01, -5.8167e-02, -1.6406e+00,  4.8828e-03,
         2.1191e-01, -1.8677e-02,  2.4707e-01,  7.4463e-02,  4.3213e-02,
         5.3467e-02,  1.5283e-01,  2.0752e-03, -1.0986e-02,  4.8981e-02,
         3.5059e-01,  5.0098e-01,  1.5125e-01,  3.3691e-02,  1.1133e-01,
         1.4404e-02, -4.8828e-04, -3.2129e-01,  6.0791e-02,  5.2051e-01,
         1.3757e-01, -2.6074e-01,  1.0950e-01, -5.0098e-01, -1.4038e-01,
         3.3447e-01,  2.6445e+00,  5.6152e-02,  1.1719e-01, -1.0278e-01,
        -1.5503e-02,  2.8760e-01,  2.5820e+00,  1.2695e-01, -1.5894e-01,
         2.0264e-02,  2.8076e-01,  5.7373e-03, -9.5459e-02, -8.6426e-02,
        -6.6040e-02, -5.6885e-02,  4.0137e-01,  1.1719e-02,  3.6133e-02,
         2.4805e-01, -1.0059e-01, -3.2812e-01, -1.1719e-01, -1.1743e-01,
         3.0176e-01,  1.5552e-01, -2.6184e-02,  1.3501e-01,  1.4609e+00,
        -3.2959e-02,  9.1492e-02,  7.2266e-02,  3.6743e-02, -6.5186e-02,
         1.8774e-01,  3.4058e-02,  1.7114e-01, -5.4785e-01, -4.2419e-02,
         4.6191e-01, -1.6492e-01, -2.4902e-02,  1.3599e-01,  5.3223e-02,
         1.9653e-02,  1.7344e+00,  2.8564e-02, -5.7831e-02,  3.8330e-02,
        -5.9570e-02,  3.1982e-02, -3.4912e-02,  6.1646e-02,  1.0400e-01,
         1.4429e-01, -9.9121e-02,  2.1484e+00, -2.2754e-01,  5.9570e-01,
        -3.6987e-02,  6.5234e-01,  1.9238e-01, -1.3855e-02,  1.2756e-01,
        -2.9844e+00,  4.6973e-01, -8.2812e-01,  1.0461e-01,  9.9609e-02,
         5.8350e-02,  7.9834e-02,  1.5747e-02,  7.3730e-02,  2.0898e-01,
        -1.0968e-01,  5.4199e-02, -1.6455e-01, -4.2725e-03,  1.8359e-01,
        -5.9174e-02, -6.8848e-02, -5.6824e-02,  1.1121e-01,  3.6621e-03,
        -8.1836e-01,  2.5000e-01, -1.4648e-03,  6.4697e-02, -6.2988e-01,
         1.2183e-01, -2.7649e-02, -8.8135e-02,  6.2500e-02,  9.2041e-02,
        -4.8706e-02, -4.3091e-02, -1.3184e-01,  1.3257e-01, -4.6191e-01,
        -4.2041e-01,  1.0284e-01, -3.0029e-02,  3.5498e-01, -2.2803e-01,
         4.5801e-01,  3.1433e-02,  1.2573e-02,  2.8955e-01,  1.3281e-01,
         1.2402e-01,  4.7559e-01, -1.1145e-01,  2.0264e-02,  7.0703e-01,
         5.7812e-01, -9.3945e-01, -3.8025e-02,  2.5391e-02, -4.7363e-01,
         8.5022e-02, -1.9141e-01, -8.3984e-02, -9.1553e-02,  1.3196e-01,
        -5.9082e-02,  2.1851e-02,  2.4734e-02,  2.5024e-02, -1.0284e-01,
        -1.1108e-01, -3.1433e-02,  8.0322e-02, -3.0811e-01,  8.8257e-02,
         4.6387e-01,  8.3862e-02, -1.3257e-01, -7.8491e-02, -1.3977e-02,
        -3.0322e-01,  7.3730e-02, -2.1387e-01,  4.8828e-02,  6.2866e-02,
        -2.1240e-02,  3.9844e-01,  3.1006e-02,  4.6680e-01, -5.0488e-01,
        -3.7031e+00,  1.0675e-01,  1.5210e-01,  9.0576e-02, -2.5195e-01,
         8.8477e-01,  2.3877e-01, -1.1646e-01, -3.2043e-02,  1.6235e-02,
        -2.3047e+00,  8.8281e-01], device='cuda:3', dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True task_net module.module.task_net.layer_norm.weight True tensor([ 8.1641e-01,  1.0391e+00,  5.7324e-01, -1.4355e-01,  8.8867e-02,
         3.1128e-02,  3.8867e-01,  2.3633e-01,  4.6289e-01,  2.8320e-01,
         6.7969e-01, -1.4746e-01, -1.1719e-01,  5.0391e-01,  8.3984e-02,
        -7.2510e-02,  2.6562e-01,  1.3892e-01,  2.4609e-01,  2.6465e-01,
         2.8320e-01,  1.8379e+00, -1.0156e-01, -8.9844e-02,  1.2266e+00,
         7.0801e-03, -2.8906e-01,  1.7188e-01,  1.7188e-01,  2.9492e-01,
         2.4473e+00,  2.8320e-01,  1.1172e+00,  3.1641e-01,  8.4375e-01,
         1.1719e-01,  6.1816e-01, -2.8516e-01,  1.5039e-01,  9.1797e-01,
         1.1250e+00,  1.3770e-01, -2.3340e-01,  2.2266e-01,  2.0703e-01,
         1.1016e+00,  7.2656e-01,  5.1953e-01,  2.3828e-01,  2.9297e-02,
         7.2754e-01,  2.3792e-01,  5.5664e-02, -2.9492e-01,  6.5234e-01,
         7.8516e-01,  4.0234e-01,  8.8672e-01,  4.1748e-02,  4.3359e-01,
         1.6602e-01,  1.0742e+00, -1.8945e-01,  3.4375e-01,  4.1602e-01,
         8.2031e-01,  5.4688e-02,  1.4746e-01, -1.0132e-02,  2.0703e-01,
         2.1875e-01,  6.7188e-01, -1.1426e-01,  9.7266e-01, -3.1250e-01,
        -2.8442e-02,  1.8457e-01,  1.0352e-01, -1.9775e-02, -8.4961e-02,
        -5.1270e-03,  4.1797e-01,  4.5312e-01,  1.1780e-02,  5.6836e-01,
         1.0508e+00,  9.4922e-01,  1.5039e+00,  3.0957e-01,  2.0752e-01,
         2.4805e-01,  8.7891e-01,  1.7285e-01,  1.6094e+00,  7.4219e-01,
        -1.4453e-01,  2.7734e-01,  7.6172e-01,  1.4453e+00,  1.1749e-03,
         4.6875e-01,  1.1875e+00, -3.5352e-01,  5.7715e-01,  3.1128e-03,
         7.3853e-02,  2.3633e-01,  2.6562e-01,  8.0859e-01,  1.0566e+00,
         3.9844e-01,  4.7656e-01, -3.2812e-01,  9.0332e-03,  1.8945e-01,
         4.6387e-01,  3.3984e-01,  1.2695e+00,  2.6465e-01,  1.4668e+00,
         5.8398e-01,  4.9023e-01,  1.3203e+00,  2.0410e-01,  5.1422e-03,
         1.4355e-01,  7.0312e-01, -2.2461e-02,  7.1094e-01,  2.6465e-01,
         2.2070e-01,  1.0293e+00,  6.7578e-01,  1.0781e+00, -2.8564e-02,
         3.5547e-01,  6.6016e-01,  2.0459e-01,  1.5723e-01,  4.6875e-01,
         2.1582e-01, -9.2712e-02,  1.1328e-01,  1.3135e-01,  3.9844e-01,
         4.1016e-01,  1.2891e-01, -2.6562e-01, -1.1230e-02,  5.8789e-01,
         4.6094e-01,  3.1299e-01,  5.8838e-02,  1.7578e-01,  9.7656e-03,
         1.8945e-01, -1.9531e-01,  1.6602e+00, -3.6719e-01,  2.0898e-01,
         5.7812e-01, -3.9429e-02,  2.9590e-01, -3.0371e-01, -2.0430e+00,
         7.1777e-01,  9.5215e-02,  1.6504e-01, -2.6953e-01,  7.2510e-02,
         8.5547e-01,  5.5469e-01,  5.2344e-01, -3.3203e-02, -1.7285e-01,
         3.6328e-01,  1.9922e-01,  2.3828e-01,  5.4375e+00,  3.5156e-02,
         1.4062e-01,  5.4297e-01,  2.6172e-01,  1.5723e-01,  1.5312e+00,
        -1.2390e-02,  8.8281e-01,  1.4551e-01,  3.0469e-01,  1.0264e+00,
         1.3828e+00, -1.1426e-01,  1.0830e+00,  1.8555e-01,  5.6104e-01,
         1.4404e-02,  6.5771e-01,  5.2344e-01, -3.0566e-01,  5.8594e-03,
         5.4688e-02,  2.7734e-01,  6.5430e-01,  2.7539e-01,  1.6602e-01,
        -5.7251e-02,  7.7100e-01,  1.9141e-01, -6.8604e-02,  5.2490e-01,
         3.3984e-01, -2.1680e-01,  5.1953e-01,  6.1719e-01, -2.5879e-02,
         8.2031e-01,  4.8047e-01,  4.2969e-02,  1.6016e-01,  2.3340e-01,
         1.7676e-01,  1.6113e-02,  3.5645e-02, -5.3516e-01, -1.2539e+00,
         1.9668e+00,  2.5586e-01,  7.7734e-01,  1.7578e-01,  8.6670e-03,
         6.4844e-01,  6.7969e-01,  2.0752e-02,  1.2012e+00,  6.2109e-01,
         6.9141e-01,  2.3926e-01, -7.1777e-02,  9.1797e-01, -4.2725e-03,
         1.3027e+00,  5.9570e-01,  9.3506e-02,  2.2461e-01,  1.2236e+00,
         9.6875e-01,  3.3984e-01,  5.8594e-01,  1.3730e+00,  1.2793e-01,
         3.8281e-01,  7.7930e-01, -4.6875e-02,  6.2207e-01,  4.6484e-01,
         6.1133e-01,  3.7500e-01, -1.8359e-01,  4.5215e-01,  5.5078e-01,
         3.4961e-01,  1.1719e-01,  1.7578e-01,  1.0312e+00,  6.8359e-01,
         3.1641e-01,  1.2422e+00, -1.8158e-03,  5.5078e-01, -8.3008e-03,
         4.8047e-01, -3.7812e+00, -1.5820e-01,  3.9062e-02,  7.9688e-01,
         2.2266e-01,  8.7500e-01,  2.5684e-01, -2.6367e-02,  5.3516e-01,
         8.5938e-02,  3.6719e-01,  2.2192e-01,  7.4609e-01,  9.0332e-02,
         8.8672e-01,  2.4023e-01,  1.1328e-01, -2.7734e-01, -4.4922e-01,
         4.6484e-01,  9.2969e-01,  5.4688e-01,  5.0391e-01,  3.1055e-01,
         9.3359e-01,  5.5078e-01,  5.1562e-01,  1.2939e-02,  3.3594e-01,
         2.3926e-02,  1.0703e+00,  1.2188e+00,  5.5664e-01,  7.9688e-01,
        -3.9648e-01,  1.8677e-01,  6.4062e-01,  2.0752e-02, -5.2002e-02,
         1.0586e+00, -7.8125e-01,  3.8867e-01,  1.0156e+00,  6.3281e-01,
         3.8281e-01, -6.8359e-02,  5.7422e-01, -4.6875e-02,  9.1016e-01,
         2.8857e-01, -5.6641e-02,  2.8711e-01,  3.6250e+00,  1.3867e-01,
         8.8501e-02,  3.5547e-01,  1.1270e+00,  8.0469e-01,  2.8516e-01,
         3.5352e-01,  5.7812e-01, -2.7954e-02,  1.7773e-01,  1.3962e-03,
         2.2461e-01, -6.4453e-01,  9.0820e-02, -3.8330e-02,  1.2500e-01,
         6.6406e-01,  8.7402e-02,  2.1094e-01,  7.8125e-03,  5.1953e-01,
         2.7344e-01,  3.1641e-01,  6.0547e-01,  3.0859e-01, -6.2500e-02,
        -1.2354e-01, -1.1641e+00,  5.5957e-01,  9.0942e-02,  5.2539e-01,
         3.2422e-01,  6.5918e-01,  3.3438e+00,  5.3906e-01, -3.8867e-01,
         1.7578e-01, -1.9531e-01,  5.0977e-01,  2.6562e-01,  5.2832e-01,
        -1.7334e-02,  3.3594e-01, -6.4453e-02,  9.1797e-01,  3.5156e-01,
         6.8750e-01, -3.2715e-02,  9.4824e-01,  8.3008e-02,  8.3203e-01,
         8.6230e-01,  3.3203e-02,  1.4893e-01, -3.1445e-01,  3.0254e+00,
         8.1177e-03,  2.6367e-01,  1.3867e-01,  3.8086e-01, -8.5938e-02,
         3.9062e-01, -2.7771e-03,  4.9219e-01,  1.2031e+00,  3.2422e-01,
         1.3867e-01, -1.3525e-01,  2.4609e-01, -1.6797e-01,  4.4727e-01,
         6.3354e-02, -7.0312e-02,  6.6797e-01,  3.6133e-02,  3.2031e-01,
         8.3594e-01,  3.5156e-01,  2.5977e-01, -1.0254e-02,  2.9785e-02,
         1.7676e-01,  2.1191e-01,  2.8789e+00,  1.6797e-01,  4.9121e-01,
         7.2266e-02,  2.2266e-01,  1.2510e+00,  2.1484e-01,  3.1250e-01,
         6.1094e+00,  1.7578e-01, -7.8125e-03,  9.0820e-02, -1.7603e-01,
         6.4648e-01,  9.6875e-01,  7.5781e-01,  1.3477e+00,  7.1875e-01,
        -9.6191e-02,  2.5000e-01,  6.4453e-02,  6.3672e-01,  1.1934e+00,
         1.7578e-01,  6.9336e-01,  3.1445e-01,  1.9727e-01,  8.7500e-01,
         6.1523e-01, -1.9141e-01,  4.4141e-01,  1.1865e-01,  5.1953e-01,
         2.0508e-01,  1.1719e-01,  3.0957e-01, -3.1494e-02,  7.9102e-01,
         1.0938e-01,  3.4668e-02, -1.7578e-01, -9.9121e-02,  3.0859e-01,
        -2.6328e+00,  5.1172e-01,  6.3672e-01, -3.1641e-01,  3.8965e-01,
         2.3789e+00,  6.2500e-01,  5.6641e-01, -6.1328e-01,  5.7812e-01,
        -1.7188e-01, -1.7578e-02, -8.2520e-02,  4.0234e-01,  8.0859e-01,
        -3.2422e-01,  9.0234e-01, -4.2725e-02,  1.0352e-01, -2.1680e-01,
        -2.0264e-02,  1.0156e-01,  9.3750e-02, -1.7773e-01,  1.8457e-01,
         6.6016e-01,  5.0781e-01,  1.0547e-01,  5.6641e-01,  1.6602e-02,
         8.6719e-01,  2.8516e-01,  4.7852e-01,  7.5195e-01,  1.8066e-02,
        -7.0312e-02,  7.5989e-03,  1.2012e-01,  2.7051e-01,  5.0781e-02,
         1.0703e+00,  1.2109e+00, -2.5781e-01,  3.6719e-01,  1.3184e-01,
         4.5117e-01,  8.5645e-01,  2.6367e-01,  9.3750e-01,  4.8438e-01,
         3.9492e+00,  4.0234e-01, -2.8418e-01,  3.8916e-01,  6.2500e-02,
         4.1016e-02,  3.5547e-01,  2.0898e-01,  4.4141e-01,  4.0625e-01,
        -1.7949e+00,  4.0625e-01], device='cuda:6', dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([ 1.2500e+00,  7.1094e-01, -8.5156e-01, -1.3379e-01, -1.5088e-01,
         1.4465e-02,  3.4766e-01,  5.9375e-01, -2.5586e-01,  3.3008e-01,
        -8.7891e-01,  6.2500e-01,  1.7480e-01, -2.6855e-01, -3.5938e-01,
        -4.9072e-02, -1.5625e-01, -8.5547e-01, -3.8477e-01, -6.6797e-01,
         4.0234e-01,  1.9766e+00, -1.0547e+00, -1.5078e+00, -1.1133e+00,
         3.5742e-01,  1.0234e+00, -3.4766e-01,  1.9336e-01,  3.6328e-01,
         3.0156e+00,  5.7031e-01, -6.4844e-01, -3.2969e+00, -8.6719e-01,
         3.4375e-01, -1.0547e+00,  1.3750e+00, -3.5742e-01,  8.3594e-01,
        -1.1797e+00,  7.0703e-01,  1.1875e+00,  6.2891e-01,  6.7969e-01,
         8.3203e-01,  8.3594e-01,  5.0391e-01, -5.9766e-01, -1.0586e+00,
         1.0078e+00,  7.4219e-01, -3.9258e-01,  1.1289e+00, -1.8594e+00,
         1.0234e+00,  2.5781e-01,  6.3672e-01,  3.0029e-02,  5.4102e-01,
         2.7969e+00, -8.3203e-01, -1.2734e+00, -5.4297e-01, -4.1602e-01,
         6.9141e-01, -2.7441e-01,  1.3721e-01, -1.1719e-02, -4.1211e-01,
         2.3340e-01, -6.4844e-01,  3.2031e-01,  6.6797e-01, -1.7578e+00,
        -4.4434e-02,  1.5625e-01,  1.6309e-01,  3.1250e-02,  1.6484e+00,
        -5.5176e-02, -6.7188e-01, -1.1875e+00,  6.1279e-02, -6.9922e-01,
         2.3359e+00, -6.8359e-01,  3.0625e+00,  2.9199e-01,  9.0234e-01,
        -4.1797e-01, -6.4844e-01,  1.8066e-01, -6.5938e+00, -7.0312e-01,
        -1.7578e+00, -2.6172e-01, -7.8906e-01,  2.3047e+00,  2.4567e-03,
        -5.5273e-01, -1.0195e+00,  1.0234e+00,  1.2070e+00,  1.7822e-02,
         1.6504e-01, -2.8223e-01,  3.1875e+00,  5.0195e-01,  1.2656e+00,
        -1.0000e+00, -4.7266e-01,  1.5703e+00,  9.0332e-02, -3.3594e-01,
        -6.0352e-01,  1.5000e+00, -3.0938e+00,  2.7441e-01, -1.1445e+00,
         1.5938e+00, -4.1211e-01, -1.0352e+00,  2.0410e-01,  6.3782e-03,
        -3.9258e-01, -3.1875e+00,  1.6992e-01, -7.5000e-01,  9.4531e-01,
         3.0762e-01, -1.2500e+00, -8.3594e-01, -1.8672e+00,  1.0693e-01,
        -4.5117e-01, -3.8281e-01,  4.6289e-01,  1.8750e-01, -7.1094e-01,
         3.0762e-01, -1.1768e-01,  1.6309e-01,  6.4062e-01,  3.5547e-01,
        -4.6094e-01, -1.4219e+00,  1.5312e+00,  5.5078e-01, -5.6641e-01,
        -5.4688e-01,  7.5000e-01,  7.8125e-02, -3.9219e+00, -4.7656e-01,
         6.7188e-01,  1.2344e+00,  2.4141e+00, -1.2969e+00,  2.9004e-01,
         5.8789e-01, -2.7344e-02,  2.0312e-01, -5.9961e-01,  3.9844e+00,
         7.1875e-01,  8.3984e-02,  5.8398e-01,  4.2578e-01, -1.6602e-01,
        -6.6406e-01,  1.3750e+00,  5.4102e-01, -5.5664e-01, -9.6484e-01,
         3.2422e-01,  1.3906e+00,  3.3789e-01, -1.2500e+01, -3.0000e+00,
         2.5586e-01, -3.9453e-01,  5.3906e-01, -8.7500e-01, -6.5000e+00,
        -3.6133e-02,  1.3438e+00,  2.4512e-01, -4.7656e-01, -7.7734e-01,
        -9.3750e-01,  1.2061e-01,  9.8047e-01,  3.0664e-01,  8.7109e-01,
         1.1865e-01,  9.2188e-01, -2.5312e+00,  1.0977e+00, -4.4531e-01,
        -4.2812e+00, -4.2773e-01, -5.6445e-01, -1.8438e+00, -6.2109e-01,
         5.1270e-02,  1.8672e+00,  1.2793e-01, -4.3701e-02, -5.8789e-01,
        -7.9297e-01,  1.7266e+00, -4.5312e-01, -6.6406e-01,  2.0117e-01,
         9.6094e-01, -4.9023e-01,  1.4375e+00,  7.6953e-01,  2.3340e-01,
         4.6680e-01,  3.3594e-01,  5.0293e-02,  4.1992e-01,  3.3125e+00,
         4.2812e+00,  3.6719e-01,  1.0352e+00, -4.5508e-01, -1.0986e-01,
        -5.6250e-01, -5.4102e-01, -3.1250e-01,  7.4219e-01, -5.0000e-01,
         5.8789e-01, -4.8242e-01, -3.1250e-01,  1.6172e+00, -6.5918e-02,
        -1.4453e+00, -5.1172e-01, -2.1875e-01,  3.0859e-01, -1.1953e+00,
         9.5312e-01, -4.8438e-01,  5.3711e-01,  9.8047e-01,  2.2754e-01,
        -6.1328e-01, -1.0234e+00, -4.1211e-01, -7.7344e-01,  5.4492e-01,
         9.1406e-01, -4.8047e-01,  2.1016e+00,  2.8750e+00,  1.4453e+00,
        -1.1602e+00, -1.9238e-01, -1.3828e+00, -1.7656e+00, -4.7344e+00,
         4.2578e-01, -3.4219e+00, -1.0559e-02,  4.8242e-01, -1.4307e-01,
        -7.0703e-01, -7.1875e+00, -2.7188e+00,  2.7188e+00, -1.0977e+00,
        -3.7109e-01, -7.9688e-01, -2.5098e-01,  6.0547e-02,  1.1992e+00,
        -3.5781e+00, -1.9609e+00,  3.3984e-01, -2.1016e+00,  9.4727e-02,
         7.3828e-01,  7.0703e-01, -1.9727e-01,  1.1172e+00, -2.7031e+00,
        -5.0586e-01, -7.5000e-01,  7.9297e-01, -6.2500e-01, -3.9648e-01,
        -8.4766e-01, -5.5469e-01, -1.0352e+00,  7.2510e-02, -3.9648e-01,
         5.7227e-01,  2.4453e+00, -1.2812e+00, -3.9648e-01, -6.6016e-01,
         7.9688e-01,  3.2422e-01, -5.2148e-01, -7.2510e-02, -1.9141e-01,
         1.3047e+00,  1.9141e+00,  3.8867e-01, -6.9531e-01, -6.2500e-01,
        -1.5938e+00, -5.5078e-01, -4.1016e-01,  9.4922e-01,  1.5078e+00,
        -3.5938e-01, -7.0703e-01, -4.0430e-01, -5.1875e+00,  2.7637e-01,
         6.7188e-01,  4.0430e-01,  1.1875e+00,  7.0703e-01,  3.8477e-01,
         3.3398e-01,  9.0234e-01, -2.9785e-02,  3.3203e-01, -8.9844e-02,
         1.2422e+00,  1.4766e+00, -9.1309e-02, -1.0547e-01,  5.3516e-01,
        -6.7578e-01,  1.7676e-01, -1.2188e+00,  1.6797e-01,  2.1406e+00,
        -2.6855e-01, -1.3047e+00, -2.5000e-01, -1.7812e+00, -6.2500e-01,
         1.3672e+00,  8.4062e+00,  5.2539e-01,  4.1406e-01, -5.3320e-01,
         3.7500e-01,  1.0391e+00,  7.3438e+00,  7.4219e-01, -5.6250e-01,
         2.8711e-01,  9.0234e-01, -3.9062e-01, -7.2656e-01, -6.4453e-01,
        -2.7246e-01, -6.8359e-01,  1.4062e+00, -5.9961e-01,  3.7305e-01,
         1.4688e+00, -3.3984e-01, -1.2891e+00,  1.3135e-01, -7.0703e-01,
         1.0977e+00,  7.1875e-01, -2.5293e-01,  2.1680e-01,  4.9688e+00,
         1.9287e-02, -3.2422e-01,  4.4531e-01,  3.5742e-01, -4.4922e-01,
        -1.7969e-01,  8.9355e-02,  7.0312e-01, -2.0938e+00,  3.8086e-01,
         1.6797e+00, -7.4219e-02,  3.1836e-01,  1.8848e-01,  5.5078e-01,
        -1.2354e-01,  5.7812e+00, -5.0977e-01,  8.0078e-02, -3.8477e-01,
        -6.7188e-01,  3.5156e-01, -2.3047e-01, -2.0117e-01,  4.3164e-01,
        -1.5918e-01, -5.7422e-01,  6.4688e+00, -1.0742e+00,  2.6562e+00,
        -3.1250e-01,  2.4531e+00,  9.5703e-01,  3.6914e-01, -2.2070e-01,
        -9.2812e+00,  1.7578e+00, -2.7969e+00,  2.3926e-01,  1.8164e-01,
         4.8242e-01,  6.2891e-01, -5.9961e-01,  8.3203e-01,  1.0508e+00,
        -7.2754e-02,  4.4531e-01, -8.1250e-01, -6.2500e-01,  8.6719e-01,
         1.0840e-01, -6.6797e-01,  2.3242e-01, -2.4316e-01,  5.7227e-01,
        -3.0781e+00,  8.1250e-01,  4.3555e-01,  3.3008e-01, -2.2969e+00,
         5.8789e-01,  1.2549e-01, -5.5078e-01,  7.3730e-02,  6.1328e-01,
        -4.2578e-01,  1.3672e-01, -9.2773e-02,  4.5898e-01, -1.9922e+00,
        -8.9844e-01, -2.8516e-01, -5.9961e-01,  1.1484e+00, -9.4141e-01,
         2.2188e+00, -4.9805e-01, -5.8008e-01,  4.9023e-01,  8.3594e-01,
         4.2188e-01,  1.7812e+00, -1.0352e-01, -4.5508e-01,  2.9219e+00,
         2.0469e+00, -3.3750e+00, -4.8828e-02,  3.2422e-01, -1.7109e+00,
         1.4307e-01, -7.3438e-01, -3.9844e-01, -3.7305e-01, -9.6191e-02,
        -7.6562e-01,  5.0586e-01, -1.4795e-01,  4.8828e-01,  8.4229e-03,
        -9.1797e-01,  3.3594e-01, -3.3594e-01, -1.3047e+00,  2.2754e-01,
         1.6484e+00,  1.5820e-01, -6.7188e-01, -4.3945e-01,  6.5918e-02,
        -1.2734e+00,  7.2266e-01, -7.8906e-01,  5.0195e-01, -2.3535e-01,
        -5.5078e-01,  1.5000e+00,  3.6914e-01,  1.6719e+00, -1.8516e+00,
        -9.0938e+00, -4.0820e-01,  3.6328e-01,  4.3164e-01, -1.0195e+00,
         3.0156e+00,  1.1289e+00, -7.1094e-01,  3.4375e-01, -4.7852e-01,
        -7.4062e+00,  3.4375e+00], device='cuda:6', dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.6440, -0.0345, -0.1127,  ..., -1.2383, -0.3826, -0.6440],
        [ 4.0430,  0.8926, -0.1094,  ..., -2.9570, -0.9844, -0.6113],
        [ 0.2500,  0.8286, -1.1602,  ..., -0.6265,  0.9111, -1.6738],
        ...,
        [-2.1719, -1.6650,  4.5820,  ...,  0.9551, -2.9648,  1.4932],
        [-0.8477, -0.4180,  0.4106,  ...,  0.2056, -0.3972,  1.0576],
        [ 0.5449,  1.5010, -0.3320,  ...,  0.2949, -1.1113, -0.2302]],
       device='cuda:6', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([  8.2031,  18.9688,   0.7969,   0.7109,  -0.1250,  -4.9688,  16.5938,
         -9.4531,  -9.3281,  -5.0469,  19.2188,   6.2188, -16.5781,  -1.0312,
         37.3125,  15.0000,  -4.9688,  -7.6875,  22.7500,  11.5000,   6.7578,
        -14.8750,   0.6562,  -5.4062,   7.1406,  -5.3906,  12.4062, -11.4688,
        -10.6562, -20.3125,  -6.3359,  -4.4688,  -2.7188, -15.8438,  -9.6328,
         10.0312,  21.1406, -15.7188,  36.4062, -16.8438,  -7.4531,   0.5469,
         22.6250,  31.5938, -10.8906,  21.4688,  14.2656,   6.8438,  -1.9844,
         -3.1562,  -8.0156,   7.1719,  20.2500,  -7.9688,  -7.0938,   2.8125,
         -2.9844,  -4.9375, -11.0625,  10.7656,  25.0000,  20.0312,  11.4219,
         16.1250,  -7.4062,   7.4219,  -6.6406,  21.1406,  -9.9688, -15.0625,
         21.1875,   3.5625,  -0.7500,   7.0625, -22.1250,   0.6406,  -8.7812,
         24.8438,   4.0625,   7.5938,   9.2500,  -7.9922,   6.0625,  -6.3438,
         -0.2500,  -6.6797, -11.1016,  -5.7031,  17.9219, -20.7812,  -6.8750,
          5.7188,  -9.8125,   3.8594,   0.9531, -10.3125, -18.6406,  20.6250,
          2.1875,  -9.7031,  13.9062,  -2.6484,   1.2910,  18.5625, -10.3281,
         -5.5078,  -5.2969,  17.1094,  14.2344,  -8.2812, -13.6094,  -1.9531,
        -17.1250,  -8.9375,  -3.4609, -12.3125, -11.2500,  12.8906,  -5.3594,
          2.7266,  -6.1875,   3.6094,  17.0625,  -6.5977,   6.4062,  24.8906,
         17.3125, -18.4844,  -5.1406,   3.7188,   4.4375,   5.5625, -15.5000,
          1.9062,  -4.8906,  -6.5938,  17.4062,  -6.3438,   1.1484,   1.6250,
         -4.6250,   4.4688,  17.1875,  16.6406,  12.7344, -11.2656,   6.6406,
          9.5938,  -7.9844,   7.0625,  25.3750,   8.0938,  16.7188, -12.0625,
          3.9414, -19.8750,  -6.4062,   7.0000,  -1.4688,  24.4375,  -4.1094,
         -3.4062,  17.6250,   3.2500,   4.4648,   3.6094,  -2.8438,   7.6875,
          3.5625,  -5.0312,  15.5000,  -5.8594,   2.9844, -16.6250,  -3.5469,
        -13.0312, -19.0625,  19.5625,   4.1562,  10.0625,  18.5312,  -3.3750,
          8.7344, -12.0469,  27.5312, -15.5000,  -7.2031,   5.2031,  -3.8125,
          4.5938,  -3.6719,   9.5938, -14.9688,   2.0703,  16.3750,   0.3359,
         17.6250,  -1.8438, -10.4062,  -3.3594,  22.4062,  18.5938,   6.7500,
         11.7344,   2.9062,  -6.7500,  -2.4922,  15.1094, -12.0938,   8.0000,
         -1.4375,  10.3125, -16.5312,  -9.1875,  -2.4375,   6.3438, -18.6250,
         10.5156,  10.8750,  -0.3438, -16.1875,   6.0234,  -1.6094,  -1.4199,
        -10.7812,  -9.8750,  11.0312,  -4.7656, -17.0000,  -2.4375, -13.9844,
         -5.7812,  20.7344, -10.7188,  10.8438,   3.6641,  12.2812, -15.3438,
         -6.6562,   5.8984,   8.5625,  16.1562,   5.1406, -19.4375,  -1.6250,
         -9.9688, -25.7500,  -4.1094,   3.9688, -13.7812,  29.2500,   2.0391,
          0.7500, -25.0312,  -1.4688,   3.9219], device='cuda:6',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 5.7739e-02,  2.4219e-01,  9.6436e-03,  ...,  6.9531e-01,
          1.2031e+00,  2.8076e-02],
        [ 1.8570e-02,  7.4951e-02, -1.8524e-02,  ...,  2.6416e-01,
          4.0918e-01, -1.8341e-02],
        [-4.2450e-02, -1.6992e-01, -1.9287e-02,  ..., -4.0869e-01,
         -7.8027e-01, -6.5308e-03],
        ...,
        [-1.0185e-03,  5.0049e-03,  1.1360e-02,  ..., -1.0864e-02,
         -6.5918e-03,  7.4310e-03],
        [-5.0781e-01, -1.9180e+00, -6.8359e-03,  ..., -5.8633e+00,
         -1.0828e+01, -2.3486e-01],
        [ 1.7859e-01,  8.5156e-01,  3.9062e-02,  ...,  2.6699e+00,
          4.6797e+00,  1.1841e-01]], device='cuda:6', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([ 3.7500e-01,  3.2227e-02, -2.6465e-01, -2.7002e-01,  7.9224e-02,
        -3.0371e-01,  5.1270e-02,  2.9883e-01,  2.9175e-01,  4.0039e-02,
        -1.5430e-01,  2.9688e-01,  2.3486e-01,  2.6904e-01, -5.5176e-02,
        -1.7090e-01,  2.3987e-01, -3.2422e-01,  2.0654e-01, -1.4600e-01,
        -3.7354e-02,  8.6328e-01, -4.6680e-01, -6.7578e-01, -3.6621e-01,
         1.6602e-01,  5.4395e-01,  9.2407e-02, -2.2546e-01, -1.2207e-02,
         1.3945e+00,  2.0215e-01,  2.2461e-02, -1.3281e+00, -2.9492e-01,
         8.0566e-02, -3.5254e-01,  8.1836e-01,  2.2949e-02,  2.4805e-01,
        -2.7051e-01,  3.4766e-01,  5.6250e-01,  2.4219e-01,  2.3340e-01,
         2.9102e-01,  2.2266e-01, -2.6855e-02, -4.8340e-02, -4.0820e-01,
         3.3301e-01,  2.8223e-01, -1.6162e-01,  4.3359e-01, -7.9297e-01,
         3.4668e-01, -8.1055e-02,  9.3750e-02, -1.4722e-01, -2.4414e-02,
         1.5234e+00, -1.3672e-01, -5.1562e-01,  2.0020e-02,  2.2949e-02,
         1.0352e-01, -2.7588e-02, -1.7090e-01, -1.0535e-01,  6.0547e-02,
        -1.7432e-01,  3.7109e-02,  2.6660e-01,  1.3672e-02, -7.1680e-01,
        -8.7524e-02, -1.6748e-01, -4.1260e-02,  2.0557e-01,  6.6992e-01,
         4.9072e-02,  6.5918e-02, -3.6523e-01, -6.9580e-03, -2.0410e-01,
         1.1406e+00, -1.8066e-02,  1.4531e+00,  7.5684e-03,  3.8867e-01,
         5.0293e-02, -2.4902e-02, -1.6357e-02, -2.9609e+00, -1.5137e-02,
        -5.9961e-01,  1.9397e-01,  2.9297e-03,  1.0508e+00, -4.0466e-02,
         6.9336e-02, -3.2617e-01,  5.4199e-01,  4.1016e-01, -3.8452e-03,
         1.4600e-01,  1.2085e-01,  1.7031e+00, -6.2500e-02,  5.3906e-01,
        -2.2070e-01,  6.3477e-02,  8.9453e-01,  2.9907e-02,  7.1350e-02,
        -1.5088e-01,  6.4258e-01, -1.5039e+00, -1.5625e-02, -3.7207e-01,
         6.7383e-01,  5.0537e-02, -2.9297e-01, -2.9541e-02, -8.0200e-02,
        -1.5869e-02, -1.4648e+00,  9.9365e-02, -9.1309e-02,  4.2578e-01,
        -6.6040e-02, -5.3320e-01, -1.6504e-01, -7.2070e-01,  1.3525e-01,
         1.9531e-02,  1.7139e-01,  2.0117e-01, -1.9678e-01, -6.1035e-02,
         1.2207e-03, -4.0558e-02, -1.8311e-01,  3.4961e-01, -8.9355e-02,
         1.1011e-01, -7.2266e-01,  6.4453e-01,  2.9980e-01,  1.0742e-02,
         4.9316e-02,  3.5547e-01, -4.8035e-02, -1.8398e+00, -1.6162e-01,
         3.8281e-01,  6.0352e-01,  9.9609e-01, -5.2734e-01, -4.1992e-02,
         6.4941e-02, -2.4695e-01, -1.7786e-01, -2.6123e-01,  1.8516e+00,
         3.4570e-01, -1.7529e-01,  1.6406e-01,  3.6035e-01,  5.7800e-02,
        -5.5664e-02,  3.8477e-01,  1.2695e-02, -1.8945e-01, -2.9688e-01,
         9.2773e-02,  7.3633e-01, -4.7363e-02, -8.5000e+00, -1.4766e+00,
        -3.3936e-02,  9.6008e-02,  1.5186e-01, -3.2129e-01, -4.1484e+00,
        -7.9346e-02,  5.0586e-01,  8.9355e-02,  1.3452e-01, -2.3047e-01,
        -2.7441e-01,  2.4023e-01,  4.0332e-01,  7.2754e-02,  3.5938e-01,
         1.0254e-01,  3.4570e-01, -9.1406e-01,  5.8984e-01, -2.1582e-01,
        -2.1797e+00,  1.0364e-01, -2.9297e-03, -1.0371e+00, -2.3730e-01,
         1.9751e-01,  9.1016e-01, -1.2878e-01, -2.0264e-01, -2.3828e-01,
        -2.6172e-01,  9.5312e-01,  1.6956e-01,  4.0039e-02,  1.7236e-01,
         2.3340e-01,  9.8999e-02,  4.5898e-01,  3.5645e-01, -1.1462e-01,
         1.8018e-01,  2.0020e-01, -5.7129e-02,  4.0430e-01,  1.7578e+00,
         1.8906e+00, -1.9287e-02,  2.5684e-01, -1.3916e-01, -3.1860e-02,
         6.7139e-02,  6.5918e-02, -4.6387e-02,  2.2852e-01,  2.0996e-02,
        -8.7891e-03, -1.5527e-01, -1.2646e-01,  6.9531e-01, -4.8737e-02,
        -5.6250e-01,  4.8828e-04,  2.3560e-02, -1.3220e-01, -4.0527e-01,
         2.0215e-01,  3.1982e-02,  5.5664e-02,  3.8184e-01, -2.3682e-02,
        -1.1084e-01, -3.4863e-01, -2.3047e-01, -2.2168e-01,  1.2598e-01,
         3.0566e-01,  8.5938e-02,  1.1719e+00,  1.4609e+00,  7.8125e-01,
        -4.5117e-01,  8.4839e-02, -5.0000e-01, -5.9961e-01, -1.7344e+00,
        -3.0762e-02, -1.2734e+00,  8.4839e-03, -3.6621e-02, -5.2002e-02,
        -2.4414e-02, -4.7812e+00, -1.5703e+00,  1.2031e+00, -3.0273e-01,
        -8.0566e-02, -6.2500e-02,  1.5125e-01,  2.2046e-01,  4.2578e-01,
        -1.8281e+00, -7.2461e-01,  1.3477e-01, -9.3555e-01, -1.5527e-01,
         1.2207e-01,  3.3105e-01,  2.6367e-01,  5.7031e-01, -9.4531e-01,
         1.3708e-01, -1.3672e-01,  5.4688e-02, -7.7637e-02,  1.3843e-01,
        -1.8164e-01,  6.9580e-02, -2.4121e-01,  1.6187e-01,  3.7476e-02,
         2.9590e-01,  1.1914e+00, -4.9805e-01,  1.3770e-01,  9.2773e-03,
         4.4727e-01,  1.2305e-01,  7.0557e-02, -4.2358e-02, -1.0156e-01,
         5.0000e-01,  9.1602e-01, -4.8096e-02, -6.3965e-02, -4.3945e-02,
        -7.3438e-01, -2.5342e-01,  3.9795e-02,  4.7461e-01,  5.7031e-01,
        -6.3721e-02, -3.2227e-01, -1.0010e-02, -2.6875e+00, -3.6865e-02,
         3.1348e-01, -7.2754e-02,  5.2539e-01,  1.3184e-01,  2.4414e-02,
         3.4424e-02,  2.0898e-01, -1.1975e-01, -7.5195e-02,  3.1677e-02,
         5.8984e-01,  9.2188e-01,  2.6880e-01, -2.8259e-02,  1.3135e-01,
         3.1738e-02,  2.8564e-02, -6.6602e-01,  5.3711e-02,  7.3047e-01,
         2.7734e-01, -4.6191e-01,  2.5000e-01, -7.1484e-01, -2.7441e-01,
         5.4297e-01,  4.0156e+00,  9.9121e-02,  1.9580e-01, -4.9805e-02,
        -1.2305e-01,  5.2734e-01,  4.4062e+00,  1.6992e-01, -3.3203e-01,
        -4.9316e-02,  4.7656e-01,  9.2163e-02, -1.6602e-01, -1.4062e-01,
        -9.3750e-02, -1.5137e-02,  6.6797e-01,  9.6436e-02, -1.3672e-02,
         5.5859e-01, -1.4600e-01, -4.7070e-01, -1.8030e-01, -1.0254e-01,
         4.6875e-01,  2.6074e-01,  1.0498e-02,  3.2227e-01,  2.3516e+00,
        -6.0516e-02,  1.4062e-01,  5.4199e-02,  9.5215e-03, -2.1045e-01,
         3.6206e-01,  7.9590e-02,  2.6465e-01, -8.0078e-01, -1.1255e-01,
         6.2500e-01, -3.4814e-01, -5.4688e-02,  2.6074e-01,  1.0156e-01,
         6.8115e-02,  2.6094e+00,  3.1250e-02, -1.1145e-01,  1.1523e-01,
        -1.1230e-01, -5.9082e-02,  6.0669e-02,  6.7871e-02,  2.3340e-01,
         2.7979e-01, -1.2549e-01,  3.5625e+00, -3.7207e-01,  1.2617e+00,
        -3.3203e-02,  1.0859e+00,  3.0664e-01, -7.9590e-02,  2.1350e-01,
        -5.3594e+00,  8.1641e-01, -1.5664e+00,  1.7139e-01,  2.1045e-01,
         3.5156e-02,  1.1523e-01,  7.1045e-02,  1.3379e-01,  3.4180e-01,
        -2.1912e-01,  7.8125e-02, -2.7344e-01,  6.9824e-02,  3.0176e-01,
        -2.2131e-01, -6.9824e-02, -1.6846e-01,  2.3022e-01,  1.4160e-02,
        -1.4336e+00,  4.2871e-01, -7.6660e-02,  1.2842e-01, -9.0234e-01,
         1.7285e-01, -3.4180e-03, -1.4014e-01,  1.8408e-01,  9.4238e-02,
        -1.1963e-02,  2.6367e-02, -2.9370e-01,  2.2168e-01, -9.8047e-01,
        -7.9688e-01,  2.7881e-01, -3.2715e-02,  5.8984e-01, -3.2715e-01,
         8.2031e-01,  1.0181e-01,  2.9297e-02,  5.6641e-01,  1.9531e-01,
         2.7588e-01,  8.8281e-01, -2.2314e-01,  1.2073e-01,  1.5000e+00,
         1.0820e+00, -1.4688e+00, -1.4807e-01, -1.7090e-02, -8.4180e-01,
         9.0088e-02, -3.3496e-01, -1.8359e-01, -2.2412e-01,  3.4595e-01,
        -2.4414e-02, -1.9043e-02,  1.0278e-01, -5.8594e-03, -2.9077e-01,
        -1.5625e-01, -5.7861e-02,  2.1521e-01, -5.6445e-01,  8.3252e-02,
         9.0234e-01,  1.4062e-01, -2.8320e-01, -1.3232e-01, -6.4270e-02,
        -5.3906e-01,  1.0547e-01, -3.4375e-01,  1.7090e-02,  1.0358e-01,
        -4.5410e-02,  7.3828e-01, -2.1729e-02,  9.1406e-01, -8.7891e-01,
        -6.3125e+00,  1.7029e-01,  2.4854e-01,  1.4893e-01, -3.8281e-01,
         1.2188e+00,  3.3008e-01, -1.9092e-01, -1.1133e-01,  8.7891e-02,
        -3.8438e+00,  1.7148e+00], device='cuda:6', dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True task_net module.module.task_net.layer_norm.weight True tensor([ 2.9980e-01,  5.8398e-01, -2.4414e-04, -7.7881e-02,  1.6724e-02,
         1.2390e-02, -3.0762e-02,  5.9814e-02,  2.6953e-01,  9.0332e-02,
         4.3652e-01,  1.1230e-02, -6.3721e-02,  2.3633e-01,  1.0376e-01,
        -3.5034e-02,  1.0767e-01,  1.3379e-01,  2.6367e-01,  2.1533e-01,
         2.2559e-01,  7.9004e-01, -1.2305e-01,  3.3203e-02,  6.1279e-02,
         7.4707e-02, -4.9316e-02,  1.8457e-01,  1.8311e-01,  1.2500e-01,
         5.2637e-01,  1.3037e-01,  9.6094e-01, -1.5625e-02,  1.8311e-02,
         1.3696e-01,  2.5024e-02, -2.8516e-01,  9.8633e-02,  2.3145e-01,
         6.5234e-01,  9.1675e-02, -3.4424e-02,  6.1035e-02,  1.7334e-01,
         1.3025e-01,  1.9727e-01,  1.4844e-01,  2.2656e-01, -1.0840e-01,
         1.5405e-01,  1.2573e-01,  7.6660e-02, -4.3457e-02,  2.0703e-01,
         3.2422e-01,  1.6748e-01,  1.2695e-01,  1.9592e-02,  1.8945e-01,
        -7.4902e-01,  2.7344e-01, -1.4453e-01,  2.9883e-01,  1.3184e-01,
         2.6367e-01, -4.8920e-02,  8.4717e-02, -6.5002e-03,  2.1826e-01,
         1.6602e-01,  5.0391e-01, -4.8096e-02,  3.5352e-01, -1.2012e-01,
         2.1362e-04,  8.3984e-02,  5.5176e-02, -2.0447e-03,  2.6855e-02,
        -5.2490e-03,  3.4180e-01,  1.7383e-01, -4.7913e-03,  1.5723e-01,
         3.2910e-01,  3.8867e-01,  4.4434e-01,  1.3477e-01,  5.7373e-02,
         1.1914e-01,  2.5098e-01,  1.0278e-01, -9.2773e-01,  2.4512e-01,
         4.4922e-01,  1.4355e-01,  6.9336e-01,  1.9189e-01,  1.3351e-05,
         2.1094e-01,  2.4268e-01, -9.7168e-02,  3.6841e-01,  4.0054e-03,
         3.7231e-02,  1.2646e-01, -4.1797e-01,  4.4727e-01,  1.3550e-02,
         1.2549e-01,  1.9531e-01, -3.9258e-01,  1.4648e-02,  1.8457e-01,
         2.5049e-01,  3.5547e-01,  7.2266e-02,  9.8633e-02, -2.1942e-02,
         1.2695e-02,  2.2998e-01,  1.5039e-01,  4.7119e-02,  2.0218e-03,
         1.8750e-01,  2.3047e-01,  2.2705e-02,  1.7578e-01,  2.5146e-01,
         1.9824e-01, -2.2461e-02,  2.4805e-01,  3.8867e-01, -6.1035e-04,
         1.5723e-01,  3.4473e-01,  9.4421e-02,  1.3428e-01,  4.0430e-01,
         8.8379e-02, -2.6138e-02,  1.4014e-01, -7.3730e-02,  1.7578e-01,
         1.8652e-01,  7.5684e-03,  1.7676e-01, -3.7109e-02,  2.9102e-01,
         2.3926e-01,  2.4133e-01,  2.6184e-02, -4.5312e-01,  7.1533e-02,
        -6.3477e-03,  9.7656e-04,  3.1836e-01,  9.7656e-04,  1.6309e-01,
         3.8477e-01, -1.5320e-02,  9.1309e-02, -1.0938e-01, -5.2832e-01,
         5.3223e-02,  4.0039e-02,  5.8105e-02, -6.6895e-02,  1.3428e-03,
         3.6035e-01,  4.2188e-01,  2.9980e-01,  8.2031e-02,  5.8350e-02,
         7.7332e-02,  9.5703e-02,  1.1572e-01, -1.7070e+00, -2.2949e-01,
         9.4727e-02,  2.9297e-01, -2.7344e-02, -1.0059e-01,  4.8438e-01,
        -8.4534e-03,  4.2529e-01,  8.4381e-03,  2.5977e-01,  1.7139e-01,
         3.4375e-01, -4.8096e-02,  1.5405e-01,  2.4414e-04, -4.1016e-02,
        -2.1973e-03, -1.2207e-02, -1.5625e-01, -9.7412e-02, -9.2285e-02,
        -3.4180e-01,  2.1582e-01,  1.3184e-01, -1.8945e-01,  1.2598e-01,
        -1.3641e-02, -3.4033e-01,  9.3994e-02, -3.0334e-02,  1.6357e-02,
         2.4731e-01,  9.3750e-02,  2.4414e-01,  3.7500e-01, -4.2725e-03,
         4.0625e-01,  3.5156e-01,  2.5000e-01,  7.3242e-03,  4.3457e-02,
         8.0078e-02,  6.9702e-02, -4.7302e-03, -2.3096e-01, -2.5977e-01,
         4.9805e-01,  2.1777e-01,  3.8672e-01, -4.2969e-02,  1.9684e-02,
         2.5781e-01,  2.4707e-01,  7.4341e-02,  3.0225e-01,  1.7090e-01,
         5.0000e-01,  4.6143e-02, -1.3843e-01,  5.0977e-01,  2.9297e-03,
         2.7832e-01,  2.4365e-01,  1.3184e-02,  1.7871e-01,  3.1421e-01,
         4.3945e-01,  3.3398e-01,  2.8906e-01,  2.3340e-01,  6.8359e-02,
         1.6211e-01,  3.0273e-01,  1.1230e-02,  2.0660e-02,  8.7891e-02,
         9.7656e-02,  1.6016e-01, -1.2012e-01,  8.7891e-02,  6.2500e-02,
         7.1289e-02,  2.4170e-02,  9.9609e-02,  7.0508e-01, -4.1113e-01,
         1.3525e-01,  3.4277e-01,  4.0054e-05,  2.4414e-01, -1.5015e-02,
         3.7109e-01, -1.7275e+00, -1.0156e-01, -3.2617e-01,  7.8125e-03,
         1.0986e-03,  4.9609e-01,  1.0010e-01, -1.8127e-02,  3.2715e-01,
        -4.8828e-01,  2.1875e-01,  8.3984e-02,  4.2578e-01,  4.4434e-02,
         5.0000e-01,  1.3965e-01,  5.4199e-02,  1.8555e-02, -5.1562e-01,
         3.6621e-01,  2.5684e-01,  4.2578e-01,  2.4023e-01,  2.8320e-01,
         2.8516e-01,  2.8125e-01,  2.7686e-01, -3.5278e-02,  2.9395e-01,
         5.7495e-02,  2.1289e-01,  5.0488e-01,  1.8848e-01,  3.9258e-01,
        -9.9121e-02, -1.0468e-02,  3.3008e-01, -3.8452e-03, -1.7761e-02,
         2.8320e-01, -3.1152e-01,  2.3438e-01,  3.2617e-01,  2.6172e-01,
         1.4258e-01, -7.5928e-02,  1.6602e-01,  1.5039e-01,  5.6445e-01,
         2.1606e-02,  8.0750e-02,  1.3867e-01,  8.5938e-02,  9.2285e-02,
        -1.7822e-02,  2.7441e-01,  2.0605e-01,  3.2812e-01,  6.4453e-02,
         1.8018e-01,  3.1445e-01, -1.7151e-02,  1.3232e-01,  3.0231e-04,
         7.1777e-02, -2.1143e-01,  6.4453e-02, -2.7222e-02,  2.2754e-01,
         4.5312e-01, -4.0863e-02,  4.2969e-02,  2.3804e-02,  2.8711e-01,
         1.8066e-01,  3.3301e-01,  3.0078e-01,  0.0000e+00,  3.9185e-02,
         2.4805e-01, -4.1992e+00,  5.6641e-02,  1.0968e-01,  2.2363e-01,
         2.4414e-01, -1.7090e-02, -5.6836e-01,  2.6953e-01, -1.2402e-01,
         1.7432e-01, -9.6680e-02,  1.9971e-01,  1.9043e-01,  1.8311e-02,
         2.1362e-02,  1.8652e-01, -1.2207e-01,  4.4727e-01,  1.1963e-01,
         4.6680e-01, -1.7090e-02,  2.7124e-01,  6.0547e-02,  3.8965e-01,
         2.8589e-01,  4.4922e-02,  3.8940e-02, -1.1963e-01,  2.7148e-01,
         6.9275e-03,  1.8115e-01,  9.7656e-02,  2.1484e-01, -9.6680e-02,
         1.8457e-01,  3.1090e-03,  1.9678e-01, -1.0742e-01,  3.3398e-01,
         2.8613e-01, -5.9082e-02,  1.3281e-01, -4.1504e-02,  1.0352e-01,
        -1.5625e-02, -1.2207e+00,  2.5195e-01,  3.3447e-02,  1.5039e-01,
         2.7246e-01,  2.0801e-01,  1.3135e-01,  3.5400e-02, -4.5898e-02,
         1.0352e-01,  1.4795e-01, -4.8438e-01,  1.6016e-01, -7.8125e-02,
         4.0771e-02, -5.0781e-02,  1.9629e-01,  2.2168e-01,  1.5381e-01,
        -2.1172e+00, -5.2734e-02, -6.8359e-02,  2.1606e-02, -7.1899e-02,
         2.2070e-01,  1.6895e-01,  3.5547e-01,  4.8047e-01,  8.0078e-02,
        -5.0293e-02,  2.5293e-01,  3.7598e-02,  2.6367e-01,  5.0293e-02,
         8.8379e-02,  2.6758e-01,  1.3281e-01,  6.6895e-02,  4.2969e-01,
         3.4375e-01, -9.7656e-02,  2.7344e-01, -1.6968e-02,  3.5840e-01,
         1.4136e-01,  2.1362e-02,  8.4961e-02, -6.0425e-03,  2.5391e-01,
         1.7432e-01,  4.6204e-02, -8.1787e-02, -2.8076e-02,  1.3086e-01,
        -8.8281e-01,  2.3730e-01,  2.0117e-01, -1.6797e-01, -1.0889e-01,
         1.8860e-02,  3.1543e-01,  3.5645e-01, -4.2090e-01,  2.6953e-01,
        -7.0801e-03, -9.4727e-02, -2.7954e-02,  2.1289e-01, -1.0059e-01,
        -3.8672e-01,  3.3203e-02, -2.0386e-02,  1.2207e-01, -2.0312e-01,
         1.2329e-02, -4.0527e-02, -1.1084e-01, -1.7090e-02,  8.7646e-02,
         4.0039e-01,  2.2559e-01,  6.2256e-03,  1.9531e-01,  7.8430e-03,
         3.9453e-01,  2.6660e-01,  2.1289e-01,  2.9980e-01,  2.4292e-02,
        -2.7637e-01,  2.6260e-02,  7.4463e-02,  2.0203e-02,  1.4099e-02,
         1.3379e-01,  3.9648e-01, -4.5410e-02,  2.6465e-01,  8.6670e-02,
         9.8145e-02, -1.7908e-01,  1.9531e-01,  2.0215e-01, -1.8555e-02,
         0.0000e+00,  3.1934e-01, -6.2500e-02,  9.2529e-02,  1.1230e-02,
        -1.5918e-01,  4.1406e-01,  1.1523e-01,  2.9492e-01,  2.6465e-01,
        -1.3164e+00, -2.9297e-02], device='cuda:4', dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([ 6.2695e-01,  3.5840e-01, -4.2676e-01, -6.7139e-02, -7.5928e-02,
         7.2937e-03,  1.7432e-01,  2.9883e-01, -1.2891e-01,  1.6553e-01,
        -4.4238e-01,  3.1348e-01,  8.7891e-02, -1.3477e-01, -1.8066e-01,
        -2.4658e-02, -7.8857e-02, -4.2871e-01, -1.9336e-01, -3.3496e-01,
         2.0215e-01,  9.8828e-01, -5.3125e-01, -7.6172e-01, -5.5859e-01,
         1.8115e-01,  5.1172e-01, -1.7529e-01,  9.6924e-02,  1.8311e-01,
         1.5195e+00,  2.8711e-01, -3.2422e-01, -1.6641e+00, -4.3555e-01,
         1.7236e-01, -5.2930e-01,  6.9141e-01, -1.7969e-01,  4.1797e-01,
        -5.8984e-01,  3.5449e-01,  5.9375e-01,  3.1543e-01,  3.4277e-01,
         4.1797e-01,  4.1992e-01,  2.5391e-01, -3.0078e-01, -5.3125e-01,
         5.0586e-01,  3.7305e-01, -1.9727e-01,  5.7031e-01, -9.3750e-01,
         5.1562e-01,  1.2939e-01,  3.2031e-01,  1.5015e-02,  2.7246e-01,
         1.4062e+00, -4.1895e-01, -6.3672e-01, -2.7246e-01, -2.0947e-01,
         3.4668e-01, -1.3818e-01,  6.9092e-02, -5.8746e-03, -2.0605e-01,
         1.1719e-01, -3.2324e-01,  1.6064e-01,  3.3691e-01, -8.8477e-01,
        -2.2400e-02,  7.8613e-02,  8.2031e-02,  1.5686e-02,  8.3008e-01,
        -2.7771e-02, -3.3887e-01, -5.9766e-01,  3.0762e-02, -3.5156e-01,
         1.1719e+00, -3.4473e-01,  1.5352e+00,  1.4697e-01,  4.5215e-01,
        -2.0996e-01, -3.2715e-01,  9.0576e-02, -3.3047e+00, -3.5352e-01,
        -8.7891e-01, -1.3135e-01, -3.9648e-01,  1.1602e+00,  1.2321e-03,
        -2.7832e-01, -5.0977e-01,  5.1562e-01,  6.0547e-01,  8.9722e-03,
         8.2764e-02, -1.4160e-01,  1.6016e+00,  2.5195e-01,  6.3281e-01,
        -5.0000e-01, -2.3828e-01,  7.9102e-01,  4.5288e-02, -1.6846e-01,
        -3.0273e-01,  7.5195e-01, -1.5547e+00,  1.3818e-01, -5.7617e-01,
         7.9883e-01, -2.0654e-01, -5.1953e-01,  1.0303e-01,  3.2043e-03,
        -1.9727e-01, -1.6094e+00,  8.5693e-02, -3.7695e-01,  4.7656e-01,
         1.5430e-01, -6.2695e-01, -4.1797e-01, -9.3359e-01,  5.3955e-02,
        -2.2656e-01, -1.9141e-01,  2.3340e-01,  9.4238e-02, -3.5938e-01,
         1.5430e-01, -5.9082e-02,  8.2031e-02,  3.2227e-01,  1.7969e-01,
        -2.3145e-01, -7.1289e-01,  7.6758e-01,  2.7637e-01, -2.8516e-01,
        -2.7539e-01,  3.7598e-01,  3.9307e-02, -1.9688e+00, -2.4023e-01,
         3.3887e-01,  6.1914e-01,  1.2109e+00, -6.4648e-01,  1.4600e-01,
         2.9590e-01, -1.3733e-02,  1.0205e-01, -2.9980e-01,  2.0078e+00,
         3.6230e-01,  4.2358e-02,  2.9297e-01,  2.1289e-01, -8.3496e-02,
        -3.3398e-01,  6.8555e-01,  2.7246e-01, -2.7930e-01, -4.8438e-01,
         1.6260e-01,  6.9727e-01,  1.7041e-01, -6.2812e+00, -1.5156e+00,
         1.2891e-01, -1.9824e-01,  2.6953e-01, -4.3945e-01, -3.2656e+00,
        -1.8188e-02,  6.7773e-01,  1.2305e-01, -2.3828e-01, -3.9062e-01,
        -4.7266e-01,  6.0303e-02,  4.9219e-01,  1.5430e-01,  4.3848e-01,
         5.9570e-02,  4.6289e-01, -1.2734e+00,  5.5273e-01, -2.2266e-01,
        -2.1641e+00, -2.1484e-01, -2.8418e-01, -9.2578e-01, -3.1152e-01,
         2.5757e-02,  9.3750e-01,  6.4453e-02, -2.1973e-02, -2.9590e-01,
        -3.9746e-01,  8.6914e-01, -2.2754e-01, -3.3203e-01,  1.0083e-01,
         4.8242e-01, -2.4609e-01,  7.2461e-01,  3.8770e-01,  1.1719e-01,
         2.3438e-01,  1.6992e-01,  2.5208e-02,  2.1045e-01,  1.6641e+00,
         2.1562e+00,  1.8457e-01,  5.2148e-01, -2.2803e-01, -5.5298e-02,
        -2.8418e-01, -2.7051e-01, -1.5723e-01,  3.7305e-01, -2.5098e-01,
         2.9492e-01, -2.4121e-01, -1.5771e-01,  8.1641e-01, -3.3081e-02,
        -7.2461e-01, -2.5684e-01, -1.0938e-01,  1.5527e-01, -5.9961e-01,
         4.7656e-01, -2.4414e-01,  2.6953e-01,  4.9414e-01,  1.1450e-01,
        -3.0859e-01, -5.1172e-01, -2.0703e-01, -3.8770e-01,  2.7344e-01,
         4.5898e-01, -2.4023e-01,  1.0547e+00,  1.4414e+00,  7.2656e-01,
        -5.8398e-01, -9.7168e-02, -6.9531e-01, -8.8672e-01, -2.3750e+00,
         2.1338e-01, -1.7266e+00, -5.2948e-03,  2.4219e-01, -7.1533e-02,
        -3.5547e-01, -3.6250e+00, -1.3672e+00,  1.3711e+00, -5.4883e-01,
        -1.8652e-01, -4.0039e-01, -1.2598e-01,  3.0396e-02,  6.0156e-01,
        -1.7969e+00, -9.8438e-01,  1.7041e-01, -1.0586e+00,  4.7485e-02,
         3.7109e-01,  3.5352e-01, -9.8877e-02,  5.6250e-01, -1.3555e+00,
        -2.5488e-01, -3.7793e-01,  3.9746e-01, -3.1543e-01, -1.9922e-01,
        -4.2578e-01, -2.7832e-01, -5.2148e-01,  3.6621e-02, -1.9922e-01,
         2.8613e-01,  1.2266e+00, -6.4648e-01, -1.9824e-01, -3.3105e-01,
         4.0039e-01,  1.6211e-01, -2.6270e-01, -3.6499e-02, -9.6436e-02,
         6.5625e-01,  9.6094e-01,  1.9531e-01, -3.5059e-01, -3.1543e-01,
        -7.9883e-01, -2.7734e-01, -2.0557e-01,  4.7656e-01,  7.5977e-01,
        -1.8018e-01, -3.5547e-01, -2.0312e-01, -2.6172e+00,  1.3916e-01,
         3.3691e-01,  2.0264e-01,  5.9375e-01,  3.5547e-01,  1.9287e-01,
         1.6797e-01,  4.5312e-01, -1.4893e-02,  1.6650e-01, -4.5288e-02,
         6.2305e-01,  7.4023e-01, -4.5898e-02, -5.3101e-02,  2.6758e-01,
        -3.4082e-01,  8.8623e-02, -6.1133e-01,  8.4717e-02,  1.0742e+00,
        -1.3525e-01, -6.5820e-01, -1.2598e-01, -8.9062e-01, -3.1348e-01,
         6.8945e-01,  4.2031e+00,  2.6367e-01,  2.0801e-01, -2.6758e-01,
         1.8896e-01,  5.2344e-01,  3.6719e+00,  3.7402e-01, -2.8320e-01,
         1.4502e-01,  4.5117e-01, -1.9580e-01, -3.6621e-01, -3.2422e-01,
        -1.3672e-01, -3.4277e-01,  7.0898e-01, -3.0273e-01,  1.8701e-01,
         7.3633e-01, -1.6992e-01, -6.5039e-01,  6.5674e-02, -3.5547e-01,
         5.5273e-01,  3.5938e-01, -1.2744e-01,  1.0864e-01,  2.5000e+00,
         9.6741e-03, -1.6309e-01,  2.2314e-01,  1.8018e-01, -2.2510e-01,
        -9.0576e-02,  4.4922e-02,  3.5449e-01, -1.0508e+00,  1.8994e-01,
         8.4570e-01, -3.7476e-02,  1.6016e-01,  9.4482e-02,  2.7832e-01,
        -6.2012e-02,  2.8984e+00, -2.5684e-01,  4.0283e-02, -1.9385e-01,
        -3.3984e-01,  1.7725e-01, -1.1572e-01, -1.0107e-01,  2.1631e-01,
        -7.9590e-02, -2.8809e-01,  3.2656e+00, -5.3711e-01,  1.3359e+00,
        -1.5771e-01,  1.2266e+00,  4.8242e-01,  1.8604e-01, -1.1084e-01,
        -4.6562e+00,  8.8477e-01, -1.4062e+00,  1.2012e-01,  9.1553e-02,
         2.4219e-01,  3.1641e-01, -3.0273e-01,  4.1699e-01,  5.2539e-01,
        -3.6621e-02,  2.2266e-01, -4.1016e-01, -3.1445e-01,  4.3457e-01,
         5.4199e-02, -3.3496e-01,  1.1719e-01, -1.2207e-01,  2.8613e-01,
        -1.5469e+00,  4.0820e-01,  2.1924e-01,  1.6553e-01, -1.1562e+00,
         2.9688e-01,  6.3232e-02, -2.7832e-01,  3.6865e-02,  3.0859e-01,
        -2.1387e-01,  6.8604e-02, -4.6753e-02,  2.3047e-01, -1.0000e+00,
        -4.5020e-01, -1.4355e-01, -2.9980e-01,  5.7422e-01, -4.7266e-01,
         1.1133e+00, -2.5000e-01, -2.9199e-01,  2.4609e-01,  4.1895e-01,
         2.1191e-01,  8.9062e-01, -5.2002e-02, -2.2803e-01,  1.4688e+00,
         1.0312e+00, -1.6992e+00, -2.4475e-02,  1.6260e-01, -8.5938e-01,
         7.1533e-02, -3.6816e-01, -2.0020e-01, -1.8701e-01, -4.8340e-02,
        -3.8281e-01,  2.5488e-01, -7.4219e-02,  2.4609e-01,  4.2267e-03,
        -4.5996e-01,  1.6846e-01, -1.6992e-01, -6.5625e-01,  1.1401e-01,
         8.2812e-01,  7.9346e-02, -3.3789e-01, -2.1973e-01,  3.3325e-02,
        -6.3672e-01,  3.6230e-01, -3.9551e-01,  2.5195e-01, -1.1816e-01,
        -2.7734e-01,  7.5195e-01,  1.8506e-01,  8.3984e-01, -9.2969e-01,
        -4.5781e+00, -2.0508e-01,  1.8311e-01,  2.1680e-01, -5.1172e-01,
         1.5195e+00,  5.6641e-01, -3.5742e-01,  1.7285e-01, -2.4023e-01,
        -3.7188e+00,  1.7266e+00], device='cuda:4', dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.0403, -0.1467, -0.6025,  ..., -0.7178,  0.1104,  0.0349],
        [-0.5601, -0.1162, -0.7363,  ..., -2.2480, -0.9121, -1.1592],
        [-0.4856,  0.0586, -0.5352,  ..., -0.8643,  0.0569, -0.4456],
        ...,
        [ 0.9385, -1.4766,  2.5352,  ...,  0.7681, -2.0234,  1.2354],
        [ 0.0977, -0.1278,  0.3262,  ...,  0.3813, -0.2793,  0.4199],
        [ 0.4966, -0.3501,  0.3127,  ..., -0.1104, -0.7920, -0.3352]],
       device='cuda:4', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 4.3008,  8.9688,  2.4961,  0.0713,  1.6406, -2.8477,  3.5312, -2.9453,
        -5.1797, -1.8672,  6.5625, -0.4922, -7.6953, -0.6250,  6.4062,  4.6484,
         0.0508, -1.2500,  7.2031,  3.2500,  3.4824, -2.8828, -1.8359, -0.7773,
         2.3594, -0.5117,  3.6094, -3.3242, -2.5625, -5.2266, -1.1074, -4.9922,
        -1.7500, -3.9062, -2.8203,  3.8281,  6.7188, -3.1875, 13.3828, -4.7422,
        -1.3281,  5.4258,  6.4844, 12.7344, -0.3672,  5.1172,  4.1406,  4.1953,
         2.5352, -0.7188, -2.7969,  1.8945,  5.7812, -3.5156, -1.7969,  0.5469,
        -0.1484, -1.4766, -2.4219,  6.0977,  6.8438,  5.2656,  5.5312,  5.2969,
        -0.0391,  1.5156, -1.4531, 10.9766, -2.3906, -4.2539,  3.2656,  4.4219,
        -0.4355,  7.4727, -6.8359,  0.7656, -5.1797,  5.6094,  0.4688,  4.7891,
         1.6562, -6.1992,  2.4297, -3.5195,  4.4531, -2.6152, -3.4258, -1.7266,
         5.8672, -0.2500, -0.1328,  4.3203, -3.0156, -0.9922,  1.8125, -5.8594,
        -6.9922,  5.3750,  0.9062,  0.0469,  3.2734, -1.0928,  0.2451,  5.5234,
        -3.8203, -2.2871, -3.7227,  8.1406,  5.9531, -1.6797, -3.9688,  0.4375,
        -3.9922, -3.7188, -1.3320, -3.1953, -4.9297,  5.2109, -0.8594,  1.8086,
        -1.8457,  0.7812,  5.8281, -2.4082, -0.1797,  5.2656,  4.8281, -3.7734,
        -2.2246,  4.5938,  1.4688,  1.6562, -4.1797, -2.7500, -2.6250, -3.2656,
         5.0547, -1.0938,  2.5488, -0.3594, -0.7500,  0.1562,  4.2500,  5.9062,
         3.1797, -2.2344, -0.1143,  4.5469, -2.5312,  3.4609,  6.6250,  4.0273,
         2.0938, -3.2266,  2.9355, -5.5781,  0.6719,  1.4824, -0.0469,  7.2188,
        -1.1719, -0.0234,  3.5312,  0.9297,  0.8862,  2.4023, -1.1016,  5.5156,
         1.0859, -1.2656,  3.5391, -1.6992,  1.0703, -5.8359, -1.6992, -1.3984,
        -3.6562,  3.0234,  2.8984,  2.9375,  4.6797,  2.2422,  2.7227, -4.0938,
         6.5078, -5.5625, -1.6680,  5.6797, -2.4609,  4.1094, -2.3945,  1.3906,
        -2.0078,  3.9805,  4.6953, -0.6016, -1.8281, -1.0078,  0.0391, -0.5625,
         3.9297,  4.6562, -3.6641,  1.8555, -1.4844, -1.6094,  0.4102,  4.0234,
        -3.0938,  4.2188, -2.4297,  0.8750, -3.6094, -1.9844,  2.9922,  3.5938,
        -7.9219,  2.1250,  1.5391,  1.5000, -3.5078,  3.3672, -0.0391, -0.5122,
        -3.5234, -2.9648,  8.1250, -0.3438, -7.3125, -0.4375, -4.6875, -0.4922,
         4.3984, -3.2656,  1.4609,  2.3477,  1.6094, -1.0938, -2.9766, -0.8555,
         1.2656,  3.8359,  3.4531, -6.6406, -0.9219, -3.6016, -6.4766, -2.4453,
         3.3789,  0.1094, 11.2500,  0.6172,  4.4766, -6.6836, -0.6016,  0.2734],
       device='cuda:4', dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 3.9978e-02,  1.4490e-01,  2.3254e-02,  ...,  8.6670e-02,
          2.1826e-01,  7.7820e-03],
        [ 1.1360e-02,  4.7974e-02,  2.0599e-03,  ...,  1.6724e-02,
          1.1963e-02, -1.3199e-02],
        [-3.0075e-02, -8.6548e-02, -6.7139e-03,  ..., -6.9702e-02,
         -1.0693e-01,  2.3499e-03],
        ...,
        [ 5.2261e-04, -1.0452e-02,  3.0422e-03,  ...,  2.7161e-02,
          3.7598e-02,  4.2915e-04],
        [-3.6865e-01, -1.0498e+00, -1.8896e-01,  ..., -7.5195e-01,
         -1.7305e+00, -5.7617e-02],
        [ 1.4575e-01,  4.3213e-01,  4.5898e-02,  ...,  2.8857e-01,
          6.7188e-01,  2.0020e-02]], device='cuda:4', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([ 1.2598e-01,  4.8828e-04, -6.8359e-02, -8.9478e-02,  2.1347e-02,
        -6.1279e-02,  2.2339e-02,  8.5205e-02,  8.9966e-02, -1.4771e-02,
        -5.3223e-02,  7.3242e-02,  6.7871e-02,  7.0679e-02, -1.5015e-02,
        -9.2773e-03,  4.7302e-02, -9.3018e-02,  9.5825e-02, -5.1270e-02,
         1.2085e-02,  2.4414e-01, -1.3232e-01, -1.8457e-01, -9.5215e-02,
         7.8125e-02,  1.7773e-01,  2.4231e-02, -8.6670e-02,  1.6357e-02,
         4.7461e-01,  1.0620e-01, -3.8208e-02, -4.0430e-01, -8.3740e-02,
         1.4771e-02, -1.1670e-01,  2.7051e-01,  1.1353e-02,  7.2754e-02,
        -6.9824e-02,  1.2134e-01,  1.6260e-01,  1.2134e-01,  4.6143e-02,
         8.3496e-02,  7.3242e-03, -1.4648e-02, -3.5889e-02, -1.6455e-01,
         7.8125e-02,  8.4717e-02, -3.5400e-02,  1.3330e-01, -2.2363e-01,
         1.0059e-01,  1.6418e-02, -5.6152e-03,  8.5602e-03, -1.0132e-02,
         4.7656e-01, -3.7354e-02, -1.7773e-01,  1.0254e-02, -3.4790e-03,
         1.9287e-02,  3.2349e-03, -2.6184e-02, -1.9196e-02,  1.3184e-02,
        -6.8115e-02,  4.8523e-02,  6.7261e-02, -1.1963e-02, -2.0215e-01,
         1.5915e-02, -4.4556e-02, -3.8177e-02,  7.1411e-02,  1.9629e-01,
         4.0283e-03,  1.5869e-02, -1.4258e-01, -1.3428e-03, -5.6641e-02,
         3.3984e-01, -1.9531e-03,  3.9453e-01,  4.0527e-02,  8.6914e-02,
         8.4839e-03,  6.7139e-03, -1.3000e-02, -1.0195e+00, -1.0132e-02,
        -2.0898e-01,  9.3018e-02, -1.0254e-02,  2.2266e-01,  1.8250e-02,
         4.5166e-02, -9.6680e-02,  1.6016e-01,  1.5039e-01,  7.3051e-03,
         4.8462e-02,  6.5918e-02,  5.2930e-01, -4.0283e-03,  1.0889e-01,
        -7.0312e-02,  3.6438e-02,  2.1387e-01,  4.0314e-02, -9.0027e-03,
        -5.5664e-02,  2.0459e-01, -4.3945e-01,  1.8677e-02, -1.3135e-01,
         2.6562e-01,  2.0752e-02, -6.6895e-02,  2.2583e-02, -3.0731e-02,
        -1.7700e-02, -4.7070e-01,  1.1597e-02, -4.0771e-02,  1.2109e-01,
         5.7983e-03, -1.4502e-01, -3.1982e-02, -2.5098e-01,  4.9438e-02,
         2.5696e-02,  2.5879e-02,  6.1035e-02, -7.2205e-02, -3.9795e-02,
        -3.0518e-03, -2.5787e-02, -5.7373e-02,  6.5186e-02, -5.9174e-02,
         2.8870e-02, -1.4111e-01,  1.8262e-01,  6.5186e-02,  4.1504e-03,
         8.1787e-03,  8.1543e-02, -4.5929e-02, -4.5312e-01, -6.6772e-02,
         1.0645e-01,  1.7627e-01,  3.2324e-01, -1.7578e-01, -2.0752e-03,
         1.7822e-02, -3.8696e-02, -4.8035e-02, -5.6641e-02,  6.9336e-01,
         8.3984e-02, -8.8013e-02,  5.9082e-02,  6.3232e-02, -1.5259e-04,
        -1.9287e-02,  1.3916e-01, -1.4893e-02, -5.6152e-02, -1.0693e-01,
         3.5645e-02,  1.7432e-01, -5.5573e-02, -2.2969e+00, -3.6719e-01,
        -9.4604e-03,  4.3396e-02,  6.3232e-02, -9.4971e-02, -9.9219e-01,
        -5.3223e-02,  1.6016e-01,  3.0396e-02,  2.8259e-02, -7.3486e-02,
        -7.9346e-02,  7.2144e-02,  1.3721e-01,  3.7231e-02,  8.5449e-02,
         3.6865e-02,  1.0059e-01, -4.1309e-01,  1.6602e-01, -7.1533e-02,
        -6.2695e-01,  2.6611e-02, -6.2256e-03, -2.0215e-01, -7.5195e-02,
         6.2012e-02,  2.5684e-01,  5.5237e-03, -1.6052e-02, -3.5889e-02,
        -7.8613e-02,  1.9824e-01,  7.4402e-02,  8.4229e-03,  4.0894e-02,
         5.7129e-02,  1.3428e-02,  1.1328e-01,  9.6680e-02, -2.9877e-02,
         8.5693e-02,  8.0811e-02, -1.0208e-02,  1.1133e-01,  4.9023e-01,
         5.6445e-01,  4.4067e-02,  6.2500e-02, -5.3223e-02,  1.6205e-02,
         2.8809e-02,  3.4058e-02,  7.5684e-03,  7.2998e-02, -1.9531e-03,
        -2.8442e-02, -8.1299e-02, -3.6011e-02,  1.7578e-01, -4.8492e-02,
        -1.8359e-01, -2.3926e-02,  8.6365e-03, -6.6162e-02, -1.1572e-01,
         8.4961e-02, -1.8738e-02,  2.6245e-02,  1.0742e-01, -1.7822e-02,
        -3.8330e-02, -1.0645e-01, -6.8359e-02, -9.3750e-02,  3.7842e-02,
         1.1328e-01,  5.0720e-02,  2.7148e-01,  4.6191e-01,  1.8896e-01,
        -1.5430e-01,  3.9703e-02, -1.2109e-01, -1.8945e-01, -5.0391e-01,
        -1.1475e-02, -4.4922e-01,  4.1931e-02, -2.9663e-02,  1.4343e-02,
        -1.7334e-02, -1.1602e+00, -3.3008e-01,  3.3301e-01, -8.5938e-02,
         5.9814e-03, -9.5215e-03,  6.3721e-02,  8.5205e-02,  1.4062e-01,
        -5.6250e-01, -2.0410e-01,  6.9458e-02, -2.4902e-01, -7.3303e-02,
         3.1494e-02,  5.6152e-02,  7.2144e-02,  1.6309e-01, -3.8672e-01,
         4.8279e-02, -4.2480e-02,  4.0039e-02, -2.5879e-02,  9.6802e-02,
        -4.1748e-02,  1.4099e-02, -1.0059e-01,  6.7383e-02, -3.3997e-02,
         9.6436e-02,  3.6523e-01, -1.3525e-01,  2.7100e-02,  1.5991e-02,
         1.0645e-01,  1.6479e-02, -1.7822e-02, -3.4088e-02, -7.3486e-02,
         1.3379e-01,  2.5781e-01, -3.7964e-02,  5.7373e-03,  2.5635e-03,
        -1.4160e-01, -7.2021e-02, -5.4932e-03,  1.8457e-01,  1.3477e-01,
        -4.2358e-02, -1.2085e-01, -4.9988e-02, -8.2617e-01, -4.2114e-03,
         1.4062e-01, -5.5420e-02,  1.4355e-01,  4.0039e-02,  1.7578e-02,
         2.7466e-02,  2.1484e-02,  1.7868e-02, -2.2644e-02,  3.3966e-02,
         1.3721e-01,  2.1484e-01,  1.2537e-01,  2.9694e-02,  6.2744e-02,
         4.9072e-02, -7.2021e-03, -1.2402e-01,  7.0740e-02,  1.9141e-01,
         8.9539e-02, -1.0645e-01,  6.1096e-02, -2.2559e-01, -8.4717e-02,
         2.0312e-01,  1.6328e+00,  2.8320e-02,  6.5430e-02, -3.0151e-02,
        -3.4119e-02,  1.0938e-01,  1.1953e+00,  4.9805e-02, -1.0327e-01,
         1.3550e-02,  1.3965e-01,  2.0020e-02, -2.7100e-02, -3.8330e-02,
        -2.5452e-02,  2.1973e-03,  2.3340e-01,  1.8738e-02,  1.3550e-02,
         1.1816e-01, -6.0547e-02, -1.4990e-01, -1.0962e-01, -5.4932e-02,
         1.0352e-01,  6.1768e-02, -5.8594e-03,  7.3730e-02,  8.1055e-01,
        -3.3264e-02,  7.4463e-02,  6.1401e-02,  3.5156e-02, -5.3711e-03,
         1.3635e-01,  1.7395e-02,  1.0034e-01, -3.3789e-01, -2.3560e-02,
         2.6855e-01, -1.3013e-01, -2.6123e-02,  7.8979e-02, -8.0566e-03,
         1.7487e-02,  9.1406e-01,  3.1860e-02, -4.6814e-02,  5.4443e-02,
        -2.4658e-02,  3.1372e-02, -5.6458e-03,  6.0059e-02,  4.5410e-02,
         6.5430e-02, -4.6997e-02,  1.1211e+00, -8.0078e-02,  3.8086e-01,
        -3.6621e-02,  4.3262e-01,  1.3330e-01, -4.0222e-02,  7.6660e-02,
        -2.1484e+00,  2.3340e-01, -2.6367e-01,  6.1646e-02,  3.6377e-02,
         1.1963e-02,  3.1494e-02,  4.2603e-02,  2.9785e-02,  1.7969e-01,
        -6.6284e-02,  1.1597e-02, -7.4707e-02,  1.4526e-02,  4.9805e-02,
        -5.4352e-02, -5.6396e-02, -4.3945e-02,  5.8899e-02, -1.7090e-03,
        -4.5020e-01,  1.4355e-01, -1.6235e-02,  7.6904e-03, -2.9980e-01,
         4.7852e-02, -1.8158e-02, -5.6641e-02,  2.7100e-02,  3.1738e-02,
        -2.2949e-02, -5.0049e-02, -8.7769e-02,  4.3213e-02, -2.8320e-01,
        -2.7295e-01,  5.3345e-02,  5.6152e-03,  2.0752e-01, -1.1816e-01,
         2.4609e-01,  3.0304e-02,  5.1636e-02,  1.7529e-01,  4.0283e-02,
         6.0547e-02,  2.6953e-01, -9.6436e-02,  5.2582e-02,  4.6875e-01,
         2.5977e-01, -5.8008e-01, -2.2461e-02,  2.8198e-02, -2.5000e-01,
         7.2693e-02, -6.7383e-02, -5.0049e-02, -3.7231e-02,  1.0962e-01,
        -6.1035e-03, -8.9111e-03,  1.4610e-02,  2.2095e-02, -6.9702e-02,
        -2.6367e-02, -9.2773e-03,  1.0815e-01, -1.5869e-01,  6.2378e-02,
         2.8418e-01,  5.1514e-02, -6.2012e-02, -3.4912e-02,  8.8806e-03,
        -1.9873e-01,  1.9043e-02, -1.0156e-01,  1.8921e-02,  5.0232e-02,
         2.3193e-03,  2.7637e-01,  7.4463e-03,  2.4023e-01, -2.6953e-01,
        -1.9844e+00,  9.7107e-02,  6.2500e-02,  6.0059e-02, -1.2793e-01,
         5.0684e-01,  1.5332e-01, -5.2246e-02, -2.5635e-02,  2.2522e-02,
        -1.0000e+00,  4.1016e-01], device='cuda:4', dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[ 7.4219e+00,  1.4375e+01, -3.3867e+00,  1.4531e+01, -1.1078e+01,
          2.5781e+01,  1.4031e+01, -9.4453e+00, -2.7594e+01,  6.4531e+00,
         -1.2078e+01, -6.7812e+00, -6.3906e+00, -2.8031e+01, -1.2695e+00,
          2.0719e+01, -1.8250e+01,  1.9980e+00, -2.6875e+01, -1.3398e+00,
          2.1562e+01, -2.3223e+00,  7.5781e+00,  9.3438e+00, -1.9062e+00,
          2.7734e-01, -8.1250e+00, -2.2844e+01,  2.8844e+01,  1.8250e+01,
         -9.5000e+00,  8.3828e+00, -2.6500e+01,  1.0141e+01, -4.8984e+00,
          4.7578e+00,  1.1738e+00, -1.7250e+01, -1.2312e+01,  1.5906e+01,
         -1.3719e+01, -5.0547e+00, -3.8750e+00,  4.2812e+00,  3.3359e+00,
         -5.6875e+00,  1.2234e+01,  2.5219e+01, -1.3562e+01,  5.0312e+00,
          3.1250e+00, -2.8301e+00,  8.3359e+00,  8.3906e+00,  1.2531e+01,
          1.6719e+01,  1.8719e+01,  2.0156e+01,  1.7688e+01,  1.7344e+01,
          8.9844e-02, -1.5156e+01,  7.9688e+00, -1.7781e+01, -1.2016e+01,
          1.5719e+01, -2.2305e+00,  1.9969e+01,  1.2688e+01, -1.5500e+01,
          2.2344e+01, -2.0750e+01, -1.3234e+01,  1.4938e+01,  1.0047e+01,
          1.1781e+01,  2.0656e+01,  5.9844e+00, -1.2562e+01,  2.3926e+00,
         -8.9297e+00, -2.3406e+01, -4.2930e+00,  6.0156e+00, -4.3594e+00,
         -9.6797e+00, -1.9531e+01, -1.0047e+01,  1.1750e+01,  1.8164e-01,
         -1.9125e+01, -1.7969e+01,  1.5562e+01,  1.6094e+00, -1.7031e+01,
         -1.0500e+01, -2.0438e+01, -1.9344e+01, -4.9844e+00,  1.5438e+01,
         -2.1375e+01, -3.0469e+00, -7.8281e+00,  3.9062e-02,  6.4766e+00,
         -6.2969e+00, -1.5625e+01, -1.2438e+01,  2.4094e+01, -1.5801e+00,
         -2.0781e+00, -2.1000e+01, -9.5781e+00,  7.8125e+00, -2.3844e+01,
         -2.1953e+00,  7.2969e+00,  1.2875e+01,  1.4594e+01,  4.5312e-01,
         -7.2578e+00, -8.0000e+00, -2.1719e+00,  1.3578e+01,  6.6719e+00,
         -1.0625e+01,  1.0438e+01, -4.4219e+00, -9.7031e+00,  1.9492e+00,
          2.1531e+01,  3.2676e+00, -1.5766e+01, -9.1562e+00, -8.8594e+00,
         -8.9062e+00, -2.6219e+01, -4.7930e+00,  1.9250e+01, -1.5375e+01,
          1.1609e+01,  1.2578e+00,  2.3906e+01,  1.7227e+00,  2.1719e+01,
         -2.5719e+01,  1.0664e+00,  1.2969e+00, -5.9922e+00, -1.2938e+01,
         -2.0203e+01, -6.3867e+00,  4.7188e+00,  2.1328e+00, -1.9287e+00,
         -8.4766e+00, -1.1641e+01, -1.2578e+01,  1.1453e+01,  1.4125e+01,
          1.5625e+01,  1.6500e+01,  1.9594e+01,  3.5938e+00,  1.5117e+00,
          4.6641e+00,  9.3438e+00,  7.5391e+00, -1.3359e+01, -8.8594e+00,
         -1.2719e+01,  1.2859e+01,  1.4375e+01, -5.5508e+00, -1.9756e+00,
         -1.8984e+00, -8.4766e+00,  1.8938e+01,  1.5391e+00,  5.5469e+00,
          1.5062e+01, -1.7969e+01,  3.9766e+00,  4.5430e+00,  1.1281e+01,
          5.6484e+00,  2.0781e+00, -6.6602e-01, -2.8625e+01, -1.0391e+00,
         -5.7500e+00, -1.0547e+01,  1.8906e+00,  6.4219e+00, -3.5859e+00,
         -1.0047e+01, -2.6484e+00,  1.2625e+01, -2.7344e+00,  5.4766e+00,
          6.4375e+00, -2.0031e+01, -1.7500e+01,  5.2969e+00,  4.9492e+00,
         -7.6875e+00,  1.3984e+00,  1.7406e+01,  1.5094e+01,  1.0359e+01,
         -6.1133e-01, -1.0203e+01, -2.1719e+01, -2.7688e+01, -6.5469e+00,
          8.4375e+00, -2.6188e+01,  5.2891e+00, -2.4863e+00,  1.7125e+01,
          4.5781e+00, -3.1914e+00,  9.1562e+00, -1.4156e+01, -2.9062e+00,
         -7.9297e+00,  2.0344e+01,  1.0891e+01,  6.4766e+00,  7.8984e+00,
         -1.9375e+01, -1.6562e+01, -3.7891e+00,  4.2031e+00, -1.7969e+01,
          1.5688e+01, -9.4844e+00,  6.1094e+00,  2.4062e+00,  8.3984e+00,
         -3.4062e+00, -1.4406e+01, -7.6719e+00,  2.7750e+01,  3.8281e-01,
          1.8375e+01, -2.1719e+01,  1.4297e+01,  2.6406e+00,  1.0625e+01,
         -1.3234e+01,  1.1152e+00,  8.5312e+00, -1.4844e-01,  2.1750e+01,
          9.8125e+00, -1.8750e+01, -9.5625e+00, -4.0156e+00, -7.5625e+00,
          1.3141e+01, -1.0625e+01, -4.0898e+00, -9.9844e+00,  4.4922e+00,
          1.8188e+01,  8.4062e+00,  4.6562e+00,  1.8875e+01,  8.3672e+00,
         -2.0094e+01, -1.1953e+00,  3.2969e+00, -5.3516e+00,  2.8516e-01,
          6.0000e+00, -1.6656e+01, -1.1172e+01, -1.1078e+01,  8.4219e+00,
          1.3750e+01, -1.2031e+01,  2.4004e+00, -6.5000e+00,  1.5375e+01,
          1.9188e+01,  3.8789e+00, -2.9688e+01, -1.2703e+01,  3.0078e+00,
         -2.2250e+01, -1.3625e+01,  1.3688e+01, -1.3719e+01, -2.3562e+01,
         -1.0609e+01, -2.0656e+01, -9.4531e-01, -1.1125e+01, -1.8062e+01,
          2.6953e-01, -1.2828e+01, -7.5938e+00, -2.3438e+01, -2.3094e+01,
         -9.2500e+00,  1.1172e+00, -1.8719e+01,  6.6094e+00,  5.0312e+00,
          3.9219e+00, -6.5703e+00,  1.8406e+01, -1.6969e+01, -1.7625e+01,
          1.4688e+01,  8.1562e+00, -1.4938e+01, -1.8844e+01,  8.1406e+00,
         -4.4453e+00,  3.7031e+00, -1.2453e+01,  1.3266e+01,  1.9281e+01,
          2.0527e+00,  2.2062e+01,  3.0000e+00,  2.0500e+01,  1.3672e+01,
          1.4906e+01,  2.2531e+01,  1.4031e+01,  1.6375e+01, -7.2656e-01,
         -9.5938e+00, -5.7031e+00, -1.4750e+01,  1.7578e-02,  1.0328e+01,
         -1.9125e+01,  2.2578e+00,  1.4703e+01,  9.5625e+00,  3.4336e+00,
         -2.5656e+01, -4.9922e+00, -2.1500e+01,  9.6562e+00,  5.4609e+00,
         -1.3281e+00,  3.8125e+00,  7.4844e+00,  2.5586e-01, -7.5469e+00,
          1.9531e+01, -2.8652e+00, -6.5000e+00,  1.6500e+01,  9.6719e+00,
          2.4250e+01, -8.5000e+00, -1.3266e+01, -8.9219e+00, -2.0625e+00,
          6.4219e+00, -1.9062e+01, -5.3594e+00, -2.5469e+01,  1.1875e+01,
          1.1609e+01,  7.2969e+00, -3.1953e+00,  2.9531e+01, -1.3000e+01,
         -2.0234e+00,  1.5031e+01, -7.3125e+00, -1.1422e+01, -5.2656e+00,
          8.4844e+00, -1.6281e+01,  1.6641e+01,  1.8125e+01,  1.0594e+01,
         -2.7500e+01, -2.9102e+00,  1.7305e+00,  9.2031e+00,  2.0750e+01,
          1.9258e+00,  2.4406e+01,  1.6188e+01, -1.3750e+01,  1.6625e+01,
         -4.2109e+00,  5.9375e-01, -1.5531e+01,  1.3281e+01, -2.3688e+01,
         -1.5375e+01,  1.9312e+01, -9.3125e+00, -5.1133e+00, -6.5430e-01,
         -2.1344e+01, -3.7383e+00, -4.5156e+00, -4.7266e+00,  1.8643e+00,
         -7.5391e+00, -9.5156e+00,  1.2422e+00,  2.3375e+01, -2.0656e+01,
          6.8750e+00, -7.3516e+00,  1.0516e+01, -7.5859e+00,  1.7188e-01,
          1.8031e+01,  1.2344e+01, -1.9188e+01,  2.0969e+01,  9.6250e+00,
          1.5500e+01,  9.6562e+00, -6.9609e+00, -2.0906e+01,  8.4844e+00,
          2.4062e+01, -1.1188e+01,  2.0281e+01, -2.0375e+01,  1.5750e+01,
          1.7969e+00, -7.1875e+00,  1.9312e+01,  4.9844e+00,  1.0164e+01,
          1.5430e+00,  6.7344e+00, -3.1797e+00, -1.5781e+01,  1.6438e+01,
         -1.2578e+01,  3.5352e+00,  2.0969e+01, -3.2031e+00,  1.3391e+01,
          3.6250e+01, -3.1344e+01, -1.3391e+01, -8.6562e+00,  5.3477e+00,
         -9.1406e-01, -2.2594e+01, -1.6188e+01, -2.8156e+01,  1.6781e+01,
         -6.6875e+00, -7.6719e+00,  1.3734e+01, -2.0094e+01, -7.2266e+00,
         -6.4453e+00,  5.6875e+00,  1.4891e+01,  2.4125e+01,  8.4531e+00,
          9.0938e+00,  1.0531e+01,  9.4375e+00,  1.5438e+01, -2.0812e+01,
         -1.3812e+01,  2.0688e+01, -1.1969e+01,  2.8281e+01,  2.2062e+01,
         -1.9469e+01,  2.2500e+01, -1.5250e+01,  3.6016e+00,  7.7344e+00,
         -6.9375e+00,  1.7773e+00,  9.2656e+00,  1.5469e+00,  1.1016e+01,
          1.1875e+01,  1.1750e+01,  7.3750e+00,  1.5969e+01, -1.2250e+01,
         -1.0016e+01,  1.1250e+00,  1.6438e+01, -1.0594e+01,  1.0266e+01,
          7.6328e+00, -1.9656e+01, -1.0719e+01,  4.6641e+00,  1.1219e+01,
         -3.5469e+00,  7.8281e+00, -7.8125e+00,  2.6844e+01, -1.9156e+01,
         -1.0938e-01, -1.0656e+01]], device='cuda:2', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
task_net module.module.task_net.layer_norm.weight True tensor([-3.9551e-02, -4.3945e-03, -1.7090e-03, -1.5869e-02, -1.0468e-02,
        -3.3569e-04, -1.9287e-02,  4.0405e-02,  1.2207e-02,  2.0752e-03,
         3.9062e-02,  1.2451e-02, -5.7983e-03, -1.0254e-02,  1.6907e-02,
         1.8311e-04,  9.8877e-03,  3.3325e-02,  1.0254e-02,  1.0132e-02,
        -2.4902e-02,  1.4648e-01,  2.8320e-02,  6.5430e-02,  6.1432e-02,
         1.3885e-02,  2.0020e-02, -2.4414e-02, -8.5449e-03, -3.5156e-02,
         2.3584e-01, -3.5645e-02,  2.3438e-02,  5.3711e-02, -1.9531e-03,
         1.5015e-02, -8.0566e-03,  6.3965e-02, -1.1719e-02, -1.7578e-02,
        -6.0547e-02,  9.2163e-03, -3.4790e-02, -4.1931e-02,  3.3203e-02,
         9.5764e-02, -8.7891e-03, -1.2207e-02, -2.3438e-02, -2.6978e-02,
        -9.7656e-03,  2.5635e-02,  4.4342e-02, -1.1450e-01,  1.0205e-01,
        -7.1289e-02, -1.6357e-02, -6.8359e-03, -4.8828e-04, -3.8574e-02,
        -1.2109e-01,  3.4180e-03, -2.7100e-02,  2.0508e-02, -8.0566e-03,
         1.7578e-02, -9.3079e-03,  9.0332e-03, -4.1199e-04,  1.3428e-02,
        -8.5449e-03,  1.9531e-02,  1.8677e-02,  1.2695e-02,  1.4648e-02,
         3.4332e-03, -2.8076e-03,  6.0425e-03,  6.1035e-05, -8.5083e-02,
        -4.7455e-03, -9.7656e-04, -5.4932e-02, -3.5782e-03,  1.8799e-02,
         1.5576e-01, -7.3242e-03,  1.3672e-01,  4.3945e-03,  1.2878e-02,
        -1.0498e-02, -1.9531e-02,  8.5449e-04, -6.4062e-01, -8.7891e-03,
        -5.1758e-02,  1.6357e-02, -9.7656e-04,  9.6680e-02, -2.6512e-04,
        -4.0527e-02,  4.9561e-02, -1.4160e-02,  7.3242e-02,  8.6212e-04,
         1.7471e-02, -3.1738e-03, -5.8594e-03,  1.9531e-03,  8.1177e-02,
         2.7832e-02, -2.4414e-02, -4.1016e-02, -1.4038e-03, -3.0518e-02,
         3.0518e-02, -9.0820e-02,  1.7383e-01,  5.9814e-03,  8.9050e-02,
         6.2012e-02,  3.1738e-02,  6.9092e-02, -2.1362e-03,  3.2043e-04,
        -1.5625e-02,  1.2354e-01,  1.4572e-03, -6.8359e-03, -3.5645e-02,
        -7.3242e-03,  7.7393e-02, -4.1504e-02, -6.8359e-02,  1.2817e-03,
         2.2461e-02,  8.7891e-03,  1.9806e-02,  3.4180e-03,  5.8594e-03,
        -8.5449e-03, -9.3002e-03, -9.5215e-03,  1.5259e-04,  1.4648e-03,
        -4.0527e-02,  2.5635e-03, -9.3018e-02,  8.4839e-03, -1.4648e-02,
        -3.8574e-02,  8.6792e-02,  2.1973e-03, -2.4756e-01, -3.4180e-03,
         5.4077e-02,  8.8379e-02,  2.4072e-01,  4.8340e-02, -2.1973e-03,
        -9.7656e-03, -8.8501e-04,  2.4414e-04, -3.1372e-02, -4.7192e-01,
        -3.6621e-04,  1.2207e-03, -3.5400e-02,  0.0000e+00, -2.4719e-03,
        -9.7656e-03, -1.0547e-01, -9.7656e-04, -2.5146e-02, -4.1138e-02,
         2.9510e-02,  4.9316e-02, -3.6377e-02, -1.6514e+00, -1.0693e-01,
         2.0752e-03,  1.9531e-03, -5.8594e-03,  2.6123e-02, -3.8086e-02,
        -5.0354e-04,  6.0303e-02,  8.3008e-03, -4.9316e-02,  4.2328e-02,
         6.7383e-02, -1.0986e-02,  5.7556e-02,  9.7656e-03,  3.1006e-02,
         4.2267e-03,  5.8167e-02,  4.9805e-02, -3.2776e-02, -2.4414e-03,
        -3.4180e-02, -2.0996e-02, -2.6123e-02,  1.5625e-02,  1.7059e-02,
        -9.4604e-04, -8.6792e-02,  1.3428e-03, -2.7161e-03,  7.1167e-02,
         3.9062e-02,  3.3203e-02,  2.6367e-02, -4.2969e-02,  4.6082e-03,
         0.0000e+00, -4.1504e-02, -5.7129e-02,  4.5837e-02, -1.3428e-02,
        -3.0518e-03,  2.2049e-02, -3.1433e-03, -2.7832e-02, -3.9551e-02,
         1.6602e-02, -1.9531e-02,  4.5898e-02,  9.7656e-04,  9.1629e-03,
        -1.7578e-02, -9.7656e-04,  1.6846e-02,  6.1279e-02, -5.3711e-03,
         1.9043e-02, -8.4229e-03, -1.8311e-02,  5.1758e-02,  3.2196e-03,
         4.0527e-02,  4.8828e-03, -3.0823e-03, -1.4160e-02,  8.2336e-02,
        -2.9297e-02, -4.3457e-02, -7.3242e-03,  5.6885e-02, -6.8359e-03,
        -5.4688e-02,  5.4321e-02,  6.9580e-03,  3.3691e-02, -3.7354e-02,
        -4.1992e-02,  8.7891e-03,  2.4414e-03, -1.6602e-02,  6.0547e-02,
         8.7646e-02,  1.4038e-03, -7.7637e-02, -3.2227e-02, -3.0859e-01,
        -1.3672e-02,  1.0107e-01,  5.8174e-05,  1.6602e-02,  3.1433e-03,
        -4.4922e-02, -1.1006e+00,  1.3184e-02,  8.2031e-02,  1.6113e-02,
         2.6123e-02,  3.7109e-02,  4.7607e-03,  2.7466e-04, -4.1992e-02,
         4.3945e-02, -1.7188e-01,  7.2021e-03, -1.1816e-01,  6.1035e-05,
         2.0508e-02, -3.4180e-03, -2.7100e-02,  7.6416e-02, -7.9102e-02,
        -1.8555e-02,  2.1484e-02, -2.9297e-03, -2.4414e-04, -2.5391e-02,
        -1.9531e-03, -2.3438e-02,  2.0508e-02, -1.7090e-03, -7.3242e-03,
         9.1553e-04,  2.2559e-01,  4.8828e-04, -3.9062e-02,  1.5625e-02,
        -1.0010e-02,  9.8495e-03,  9.2773e-03,  4.1504e-03, -2.3193e-03,
         7.6416e-02, -7.9102e-02,  1.0742e-02,  1.9531e-03, -1.6113e-02,
         1.6943e-01, -2.4414e-04, -1.4404e-02,  1.8140e-01, -4.8828e-02,
         3.6011e-03,  1.0056e-02, -2.0508e-02,  4.1602e-01, -2.0020e-02,
        -1.9531e-02, -1.2695e-02,  4.9805e-02,  6.3477e-03, -1.4771e-02,
        -2.9297e-03,  3.6133e-02, -1.0376e-03, -8.3008e-03, -1.5259e-05,
         1.2939e-02, -6.2012e-02,  9.2773e-03, -3.1128e-03, -1.2695e-02,
         1.2695e-02,  1.2512e-02,  1.1963e-01, -1.0071e-02, -4.3945e-03,
        -3.0518e-02,  7.8125e-03,  1.4648e-02,  3.8574e-02,  2.3254e-02,
         1.3428e-02, -1.4902e+00,  9.7656e-03,  2.4429e-02,  6.1035e-03,
        -6.3477e-03,  5.1941e-02, -4.2090e-01, -6.7871e-02, -4.8828e-04,
        -1.2207e-02,  2.6123e-02, -1.9775e-02,  3.4180e-03,  2.7954e-02,
         6.1951e-03, -1.7090e-02, -4.9927e-02, -5.8594e-02,  1.9287e-02,
        -3.1250e-02,  1.2817e-02,  5.6885e-02, -3.5400e-03, -1.7090e-02,
         9.3628e-02, -8.1055e-02, -3.2349e-03, -1.4282e-02, -2.0166e-01,
         2.8229e-04, -1.0742e-02, -5.1514e-02, -2.0752e-02, -2.4414e-04,
         5.1270e-03, -8.8692e-05,  1.0742e-02,  1.2891e-01, -4.8828e-03,
        -2.9785e-02, -2.9297e-03,  4.8828e-03, -3.2959e-03, -9.5215e-03,
        -4.2419e-03, -7.0312e-01, -2.4414e-03,  1.6479e-03, -2.7832e-02,
        -5.3711e-03, -3.2471e-02,  1.7456e-02, -8.5449e-04, -5.4321e-03,
         1.1230e-02, -9.3994e-03, -5.1025e-01, -4.0527e-02,  1.0742e-02,
        -1.7944e-02, -7.8125e-03,  3.6102e-02, -1.9043e-02,  9.2773e-03,
        -1.0039e+00, -8.5449e-03,  5.8594e-03,  1.2207e-03, -2.1057e-02,
         1.5137e-02, -3.4180e-03, -6.8359e-03,  5.6641e-02, -4.7363e-02,
        -3.0518e-04,  1.0254e-02, -6.6162e-02, -3.4180e-02,  2.7100e-02,
         5.7373e-03,  1.3184e-02, -9.5215e-03, -1.8555e-02,  1.0254e-02,
        -1.5906e-01, -4.6997e-02, -1.0254e-02, -7.5073e-03,  9.9121e-02,
         1.2695e-02,  1.8921e-03,  3.3630e-02,  6.7444e-03,  1.0742e-02,
        -3.6621e-03,  6.4087e-03, -3.5400e-03, -2.7313e-02,  1.5967e-01,
        -1.0059e-01,  1.8066e-02,  1.5625e-02, -2.6123e-02,  2.7100e-02,
         1.0437e-01, -5.8594e-03,  1.4648e-03, -3.4180e-03, -7.9590e-02,
         8.5449e-04,  2.4170e-02,  6.5308e-03, -3.4180e-03,  3.4668e-02,
        -7.3730e-02,  1.2793e-01, -1.9531e-03, -2.8076e-02, -3.9551e-02,
        -1.2878e-02, -3.9062e-03,  3.6621e-03,  2.5513e-02,  6.3477e-03,
        -2.4414e-03, -2.4414e-03, -6.2256e-03, -6.3965e-02,  3.7384e-04,
        -6.4453e-02, -1.1230e-02,  1.5137e-02,  4.3579e-02, -1.2817e-02,
         2.1484e-02,  2.2163e-03,  4.0771e-02,  1.6541e-02,  3.3569e-04,
         1.4038e-01,  4.1504e-02,  7.5684e-03, -1.3184e-02,  9.3994e-03,
        -9.0332e-03, -5.0293e-02,  1.0254e-02,  1.5137e-01,  5.4199e-02,
        -4.2578e-01, -1.2207e-02, -7.4463e-03,  8.7280e-03,  5.1270e-02,
         8.7891e-03, -9.7656e-03, -5.2734e-02,  3.9062e-03, -1.1230e-02,
        -1.1699e+00,  5.9570e-02], device='cuda:1', dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([ 7.7148e-02,  4.3945e-02, -5.2246e-02, -8.3008e-03, -9.2773e-03,
         8.9264e-04,  2.1484e-02,  3.7109e-02, -1.6113e-02,  2.0264e-02,
        -5.4688e-02,  3.8574e-02,  1.0864e-02, -1.6602e-02, -2.2461e-02,
        -3.0212e-03, -9.6436e-03, -5.3223e-02, -2.3682e-02, -4.1504e-02,
         2.4902e-02,  1.2207e-01, -6.5430e-02, -9.3750e-02, -6.9336e-02,
         2.2217e-02,  6.3477e-02, -2.1484e-02,  1.1963e-02,  2.2705e-02,
         1.8750e-01,  3.5645e-02, -4.0527e-02, -2.0508e-01, -5.3711e-02,
         2.1240e-02, -6.5430e-02,  8.5938e-02, -2.2217e-02,  5.1758e-02,
        -7.3242e-02,  4.3945e-02,  7.3242e-02,  3.8574e-02,  4.1992e-02,
         5.1270e-02,  5.1758e-02,  3.1250e-02, -3.6621e-02, -6.5430e-02,
         6.2500e-02,  4.6387e-02, -2.4170e-02,  7.0312e-02, -1.1523e-01,
         6.3477e-02,  1.6113e-02,  3.9551e-02,  1.8463e-03,  3.3203e-02,
         1.7383e-01, -5.1758e-02, -7.9102e-02, -3.3691e-02, -2.5879e-02,
         4.2969e-02, -1.7090e-02,  8.5449e-03, -7.2479e-04, -2.5635e-02,
         1.4526e-02, -4.0039e-02,  1.9775e-02,  4.1992e-02, -1.0938e-01,
        -2.7771e-03,  9.6436e-03,  1.0132e-02,  1.9531e-03,  1.0254e-01,
        -3.4180e-03, -4.1504e-02, -7.4219e-02,  3.8452e-03, -4.3457e-02,
         1.4648e-01, -4.2480e-02,  1.8945e-01,  1.8066e-02,  5.6152e-02,
        -2.5879e-02, -4.0039e-02,  1.1230e-02, -4.0625e-01, -4.3945e-02,
        -1.0840e-01, -1.6113e-02, -4.8828e-02,  1.4258e-01,  1.5640e-04,
        -3.4668e-02, -6.2500e-02,  6.4453e-02,  7.4219e-02,  1.0986e-03,
         1.0132e-02, -1.7578e-02,  1.9727e-01,  3.0762e-02,  7.8125e-02,
        -6.1523e-02, -2.9297e-02,  9.7656e-02,  5.5542e-03, -2.0508e-02,
        -3.7109e-02,  9.2773e-02, -1.9141e-01,  1.6846e-02, -7.1289e-02,
         9.8633e-02, -2.5391e-02, -6.4453e-02,  1.2695e-02,  3.9291e-04,
        -2.4414e-02, -1.9922e-01,  1.0498e-02, -4.6387e-02,  5.8594e-02,
         1.9043e-02, -7.7148e-02, -5.1758e-02, -1.1426e-01,  6.6528e-03,
        -2.8076e-02, -2.3438e-02,  2.8809e-02,  1.1597e-02, -4.3945e-02,
         1.9043e-02, -7.3242e-03,  1.0132e-02,  3.9551e-02,  2.1973e-02,
        -2.8564e-02, -8.6914e-02,  9.4727e-02,  3.4180e-02, -3.5156e-02,
        -3.3691e-02,  4.6387e-02,  4.8218e-03, -2.4219e-01, -2.9541e-02,
         4.1504e-02,  7.6172e-02,  1.4844e-01, -8.0078e-02,  1.8066e-02,
         3.6621e-02, -1.6785e-03,  1.2451e-02, -3.7109e-02,  2.4609e-01,
         4.4434e-02,  5.1880e-03,  3.6621e-02,  2.6367e-02, -1.0376e-02,
        -4.1504e-02,  8.3984e-02,  3.3203e-02, -3.4180e-02, -6.0059e-02,
         2.0020e-02,  8.5938e-02,  2.0996e-02, -7.7344e-01, -1.8750e-01,
         1.5869e-02, -2.4414e-02,  3.3203e-02, -5.4199e-02, -4.0234e-01,
        -2.2278e-03,  8.3008e-02,  1.5137e-02, -2.9297e-02, -4.8340e-02,
        -5.8594e-02,  7.3853e-03,  6.0547e-02,  1.9043e-02,  5.3711e-02,
         7.3242e-03,  5.7129e-02, -1.5820e-01,  6.8359e-02, -2.7344e-02,
        -2.6562e-01, -2.6611e-02, -3.5156e-02, -1.1426e-01, -3.8574e-02,
         3.1738e-03,  1.1523e-01,  7.9346e-03, -2.7161e-03, -3.6621e-02,
        -4.8828e-02,  1.0742e-01, -2.8076e-02, -4.1016e-02,  1.2451e-02,
         5.9570e-02, -3.0518e-02,  8.9844e-02,  4.7852e-02,  1.4404e-02,
         2.9053e-02,  2.0996e-02,  3.1128e-03,  2.6123e-02,  2.0703e-01,
         2.6562e-01,  2.2705e-02,  6.3477e-02, -2.8320e-02, -6.8359e-03,
        -3.5156e-02, -3.3691e-02, -1.9287e-02,  4.5898e-02, -3.1250e-02,
         3.6133e-02, -2.9541e-02, -1.9287e-02,  1.0059e-01, -4.0894e-03,
        -8.8867e-02, -3.1738e-02, -1.3550e-02,  1.9287e-02, -7.3242e-02,
         5.8594e-02, -3.0029e-02,  3.3203e-02,  6.1035e-02,  1.4038e-02,
        -3.8086e-02, -6.3477e-02, -2.5635e-02, -4.7852e-02,  3.4180e-02,
         5.6641e-02, -2.9541e-02,  1.2891e-01,  1.7773e-01,  8.9844e-02,
        -7.2266e-02, -1.1963e-02, -8.5938e-02, -1.0938e-01, -2.9688e-01,
         2.6123e-02, -2.1094e-01, -6.5613e-04,  3.0029e-02, -8.7891e-03,
        -4.3457e-02, -4.4531e-01, -1.6797e-01,  1.6797e-01, -6.7383e-02,
        -2.3193e-02, -4.9316e-02, -1.5381e-02,  3.7231e-03,  7.4219e-02,
        -2.2070e-01, -1.2207e-01,  2.0996e-02, -1.3086e-01,  5.8594e-03,
         4.5898e-02,  4.3457e-02, -1.2207e-02,  6.9336e-02, -1.6797e-01,
        -3.1250e-02, -4.6387e-02,  4.9316e-02, -3.9062e-02, -2.4414e-02,
        -5.2246e-02, -3.4668e-02, -6.4453e-02,  4.5166e-03, -2.4414e-02,
         3.5645e-02,  1.5234e-01, -7.9102e-02, -2.4414e-02, -4.0527e-02,
         4.9316e-02,  2.0264e-02, -3.2227e-02, -4.5166e-03, -1.1841e-02,
         8.0078e-02,  1.1816e-01,  2.4170e-02, -4.2969e-02, -3.9062e-02,
        -9.8633e-02, -3.4180e-02, -2.5391e-02,  5.9082e-02,  9.3750e-02,
        -2.2217e-02, -4.3945e-02, -2.5146e-02, -3.2422e-01,  1.7334e-02,
         4.1016e-02,  2.4902e-02,  7.3242e-02,  4.3457e-02,  2.3926e-02,
         2.0752e-02,  5.5664e-02, -1.8311e-03,  2.0508e-02, -5.5542e-03,
         7.7148e-02,  9.0820e-02, -5.6763e-03, -6.5308e-03,  3.3203e-02,
        -4.1992e-02,  1.0986e-02, -7.5195e-02,  1.0376e-02,  1.3281e-01,
        -1.6602e-02, -8.1055e-02, -1.5381e-02, -1.1035e-01, -3.8574e-02,
         8.4961e-02,  5.2344e-01,  3.2715e-02,  2.5879e-02, -3.3203e-02,
         2.3193e-02,  6.5430e-02,  4.4922e-01,  4.5898e-02, -3.5156e-02,
         1.7822e-02,  5.5664e-02, -2.4170e-02, -4.4922e-02, -4.0039e-02,
        -1.7090e-02, -4.2480e-02,  8.6914e-02, -3.7598e-02,  2.3193e-02,
         9.0820e-02, -2.0996e-02, -8.0078e-02,  8.1787e-03, -4.3457e-02,
         6.8359e-02,  4.4434e-02, -1.5625e-02,  1.3428e-02,  3.0859e-01,
         1.1902e-03, -2.0020e-02,  2.7588e-02,  2.2217e-02, -2.7832e-02,
        -1.1108e-02,  5.5542e-03,  4.3457e-02, -1.2891e-01,  2.3438e-02,
         1.0449e-01, -4.6387e-03,  1.9775e-02,  1.1597e-02,  3.4180e-02,
        -7.6904e-03,  3.5938e-01, -3.1250e-02,  4.9438e-03, -2.3926e-02,
        -4.1992e-02,  2.1729e-02, -1.4282e-02, -1.2451e-02,  2.6611e-02,
        -9.8877e-03, -3.5645e-02,  4.0234e-01, -6.6406e-02,  1.6602e-01,
        -1.9531e-02,  1.5039e-01,  6.0059e-02,  2.2949e-02, -1.3672e-02,
        -5.7031e-01,  1.0938e-01, -1.7383e-01,  1.4893e-02,  1.1230e-02,
         3.0029e-02,  3.9062e-02, -3.7598e-02,  5.1270e-02,  6.4453e-02,
        -4.5166e-03,  2.7344e-02, -5.0781e-02, -3.8574e-02,  5.3711e-02,
         6.6528e-03, -4.1504e-02,  1.4404e-02, -1.5015e-02,  3.5156e-02,
        -1.8945e-01,  4.9805e-02,  2.6855e-02,  2.0264e-02, -1.4258e-01,
         3.6621e-02,  7.8125e-03, -3.4180e-02,  4.5776e-03,  3.8086e-02,
        -2.6367e-02,  8.4229e-03, -5.7983e-03,  2.8564e-02, -1.2305e-01,
        -5.5664e-02, -1.7578e-02, -3.7109e-02,  7.1289e-02, -5.8594e-02,
         1.3672e-01, -3.1006e-02, -3.6133e-02,  3.0518e-02,  5.1758e-02,
         2.6367e-02,  1.1035e-01, -6.4087e-03, -2.8320e-02,  1.8164e-01,
         1.2891e-01, -2.0898e-01, -3.0212e-03,  2.0020e-02, -1.0645e-01,
         8.7891e-03, -4.5410e-02, -2.4658e-02, -2.2949e-02, -5.9814e-03,
        -4.6875e-02,  3.1250e-02, -9.2773e-03,  3.0273e-02,  5.1880e-04,
        -5.6641e-02,  2.0508e-02, -2.0996e-02, -8.1055e-02,  1.4160e-02,
         1.0254e-01,  9.7656e-03, -4.1504e-02, -2.7100e-02,  4.1504e-03,
        -7.9102e-02,  4.4922e-02, -4.8828e-02,  3.0762e-02, -1.4526e-02,
        -3.4180e-02,  9.2773e-02,  2.2949e-02,  1.0352e-01, -1.1523e-01,
        -5.6250e-01, -2.5146e-02,  2.2705e-02,  2.6855e-02, -6.2500e-02,
         1.8750e-01,  7.0312e-02, -4.3945e-02,  2.1240e-02, -2.9785e-02,
        -4.6094e-01,  2.1289e-01], device='cuda:1', dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.0312,  0.0587, -0.2335,  ..., -0.0903,  0.0142, -0.0813],
        [-0.0217, -0.0374, -0.3135,  ..., -0.4004, -0.2323, -0.2496],
        [ 0.0063,  0.1616, -0.2029,  ..., -0.1311,  0.0068, -0.1309],
        ...,
        [ 0.0674, -0.2996,  0.8174,  ..., -0.0082, -0.8667,  0.1748],
        [ 0.0364, -0.0555,  0.1262,  ...,  0.0383, -0.1490,  0.0759],
        [ 0.0015, -0.1129, -0.0348,  ..., -0.0995, -0.1880, -0.3689]],
       device='cuda:1', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 1.5234e+00,  2.9141e+00,  1.4062e+00, -5.3711e-02, -9.4141e-01,
        -4.1211e-01,  5.0391e-01, -1.1504e+00, -1.6465e+00, -6.5039e-01,
         1.5664e+00, -4.8047e-01, -2.2109e+00, -3.2031e-01,  2.6602e+00,
         1.6016e+00, -2.5439e-01,  1.5625e-02,  2.7031e+00, -1.7188e-01,
         5.0391e-01, -2.2578e+00, -7.0312e-02, -5.4688e-02,  1.1289e+00,
        -2.0508e-01,  1.3867e+00, -1.2422e+00, -4.2969e-01, -3.5977e+00,
        -5.2637e-01, -1.2852e+00, -1.0781e+00, -6.4844e-01, -1.0781e+00,
         1.4258e+00,  8.0859e-01, -1.4570e+00,  4.8164e+00, -1.7578e+00,
        -5.2539e-01,  8.1055e-01,  1.7812e+00,  3.3164e+00, -7.8516e-01,
         2.1953e+00,  1.7012e+00,  1.5664e+00,  1.2266e+00, -1.1797e+00,
        -1.1602e+00,  9.1406e-01,  1.8828e+00, -1.4375e+00, -4.3359e-01,
        -2.6953e-01,  3.3984e-01, -7.6367e-01, -2.1094e-01,  1.8984e+00,
         7.1875e-01,  1.3164e+00,  1.4062e+00,  1.6953e+00, -3.1250e-02,
         7.1289e-01, -4.2969e-02,  3.3730e+00, -1.1055e+00, -1.4453e+00,
         1.0859e+00,  6.4453e-01, -3.3301e-01,  2.3945e+00, -3.1211e+00,
        -5.2344e-01, -2.1250e+00,  1.6816e+00, -2.8906e-01,  1.6016e+00,
         1.4805e+00, -1.2148e+00,  7.8906e-01, -2.7344e-01, -2.6758e-01,
        -1.0176e+00, -1.0508e+00, -8.9844e-01,  1.5469e+00, -4.0234e-01,
         3.9062e-03,  1.1719e-02, -6.9727e-01,  1.0078e+00,  1.1582e+00,
        -2.8203e+00, -3.0762e+00,  1.4453e+00, -7.5000e-01,  4.2969e-02,
         1.1250e+00, -2.9492e-01, -5.0537e-02,  1.9668e+00, -1.4414e+00,
        -9.8340e-01, -1.0605e+00,  1.9492e+00,  2.3984e+00, -4.2773e-01,
        -1.9062e+00,  4.7656e-01, -1.3047e+00, -1.4062e-01, -2.5684e-01,
        -9.7266e-01, -1.9609e+00,  9.7656e-01, -4.4922e-02,  9.2578e-01,
        -5.6885e-01, -1.2793e-01,  2.8242e+00, -4.9121e-01, -8.9844e-02,
         3.1953e+00,  1.5000e+00, -4.4023e+00, -1.1182e+00,  1.5586e+00,
        -8.2031e-02, -2.6953e-01, -1.4102e+00, -1.0098e+00, -2.5000e-01,
        -1.4570e+00,  2.2773e+00, -8.7109e-01,  1.2988e-01, -5.4688e-01,
        -1.2500e-01, -2.8711e-01,  4.6875e-02,  1.6289e+00,  1.1602e+00,
        -1.1602e+00,  3.6035e-01,  1.6133e+00, -3.7891e-01,  2.0723e+00,
         2.7031e+00,  7.8320e-01, -5.6250e-01, -1.3828e+00,  1.6777e+00,
        -1.8203e+00,  1.0547e-01,  6.6504e-01, -1.2305e+00,  1.0859e+00,
        -1.5625e-01,  3.3984e-01,  1.4688e+00,  6.8750e-01,  3.8989e-01,
         1.1523e-01,  1.4453e-01, -1.3242e+00, -2.4609e-01,  1.9531e-02,
         1.0039e+00, -8.1641e-01,  8.7695e-01, -1.9609e+00, -3.5156e-01,
        -2.2656e-01, -1.2812e+00,  1.9043e+00,  9.3359e-01,  2.0059e+00,
         1.6992e+00,  3.5156e-02,  1.4922e+00, -1.8867e+00,  3.4570e+00,
        -2.7344e+00, -2.3438e-01,  1.7617e+00, -9.3359e-01, -1.9219e+00,
        -9.0625e-01, -6.6406e-02, -2.8164e+00,  1.5195e+00,  1.8555e+00,
        -8.4961e-01, -1.6875e+00,  5.3516e-01, -2.5000e-01, -1.9531e-02,
         3.7109e-01,  3.0977e+00, -1.2070e+00,  6.2500e-02,  4.5312e-01,
        -1.2500e-01,  4.2969e-01,  1.1934e+00, -7.3828e-01,  1.1523e+00,
        -1.2891e+00, -2.1875e-01,  1.1328e-01, -7.8125e-01, -5.4688e-02,
         8.9453e-01, -2.1758e+00,  8.1445e-01, -2.8125e-01, -1.1406e+00,
        -2.0078e+00,  6.9727e-01,  9.6484e-01, -1.7017e-01,  8.5938e-02,
        -8.5352e-01,  1.2949e+00,  7.5000e-01, -2.4746e+00, -6.6406e-02,
        -2.3594e+00,  3.5742e-01,  1.8770e+00, -9.1406e-01,  1.5156e+00,
         1.0762e+00,  4.5703e-01, -1.2148e+00, -3.4766e-01, -2.0312e-01,
         6.0547e-01,  1.8633e+00,  9.2969e-01, -3.6211e+00, -4.5703e-01,
        -1.7031e+00, -2.7500e+00, -1.3984e+00,  9.0625e-01, -1.5000e+00,
         8.4375e-01,  3.6523e-01, -9.7656e-02, -1.5820e+00, -3.8770e-01,
         1.1582e+00], device='cuda:1', dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 1.1841e-02,  3.9551e-02,  1.2100e-02,  ...,  8.5449e-04,
          9.7168e-02,  2.7924e-03],
        [ 1.1902e-03,  4.8523e-03,  2.7390e-03,  ..., -2.0660e-02,
         -4.8279e-02, -1.7624e-02],
        [-8.0490e-03, -2.3438e-02, -5.2795e-03,  ...,  3.8452e-03,
         -3.7964e-02, -4.0436e-03],
        ...,
        [ 6.7043e-04, -1.2970e-03, -9.9373e-04,  ...,  1.4923e-02,
          1.3931e-02,  4.5204e-03],
        [-1.1377e-01, -2.9150e-01, -9.1187e-02,  ...,  9.2773e-03,
         -8.3398e-01, -6.3599e-02],
        [ 4.3518e-02,  1.2817e-01,  3.8574e-02,  ..., -3.4180e-03,
          3.6621e-01,  1.7029e-02]], device='cuda:1', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([ 3.9551e-02, -4.1504e-02, -2.1484e-02, -5.7922e-02,  3.3569e-04,
        -3.1189e-02,  2.0752e-03,  2.3804e-02,  4.6539e-02, -1.3855e-02,
        -1.2939e-02,  2.4536e-02,  2.2766e-02,  4.4830e-02, -3.6316e-03,
         6.9580e-03,  2.7924e-02, -2.0752e-02,  7.1716e-02, -9.0942e-03,
        -1.5686e-02,  6.3965e-02, -5.6396e-02, -5.4443e-02, -2.4902e-02,
         1.6907e-02,  4.6387e-02,  2.3926e-02, -4.8340e-02,  1.3733e-02,
         1.0059e-01,  3.6377e-02, -1.0437e-02, -1.8066e-01, -2.4780e-02,
        -1.2207e-04, -9.0332e-03,  6.0791e-02,  1.2024e-02,  2.4780e-02,
        -1.7090e-02,  3.5278e-02,  4.6387e-02,  3.4668e-02, -5.7373e-03,
         1.7090e-02, -5.8594e-03, -1.6510e-02, -1.1475e-02, -2.9541e-02,
         1.9287e-02,  6.2256e-03, -3.4180e-03,  5.0781e-02, -6.7383e-02,
         2.0996e-02,  2.8320e-02, -2.7527e-02,  1.3763e-02, -3.4607e-02,
         1.4404e-01, -3.1738e-03, -5.7373e-02,  1.5900e-02,  5.0049e-03,
        -2.4536e-02,  9.1553e-03, -2.7557e-02, -1.3992e-02,  2.0126e-02,
        -5.3528e-02,  3.6865e-02, -4.6997e-03, -2.7954e-02, -6.9580e-02,
         7.6408e-03, -3.3142e-02, -1.6556e-02,  4.2786e-02,  6.9092e-02,
        -7.8583e-03,  2.0508e-02, -4.8584e-02, -1.8036e-02, -2.2705e-02,
         1.1914e-01,  1.1292e-02,  1.0303e-01,  1.5625e-02,  3.0884e-02,
         1.1963e-02,  2.0752e-02, -5.2490e-03, -3.2812e-01,  1.5442e-02,
        -7.4463e-02,  5.5725e-02,  1.7029e-02,  8.7891e-02,  1.0910e-02,
         3.2806e-02, -2.9175e-02,  4.4922e-02,  3.8086e-02, -1.2993e-02,
         1.0010e-02,  4.0771e-02,  1.2891e-01, -3.6530e-02,  1.9287e-02,
        -1.6846e-02,  2.6337e-02,  1.0962e-01,  1.1215e-02, -1.2558e-02,
        -3.9673e-02,  4.3945e-02, -1.1475e-01,  9.1248e-03, -2.1973e-02,
         5.0049e-02,  1.9760e-02,  9.7656e-04,  1.9653e-02, -2.1561e-02,
        -9.4604e-03, -1.4111e-01, -3.8452e-03, -1.5503e-02,  2.6367e-02,
         3.0212e-03, -5.0537e-02,  1.1597e-02, -8.3496e-02,  3.0334e-02,
         2.2430e-02,  1.3428e-02,  1.9287e-02, -5.0415e-02, -6.7749e-03,
        -1.6937e-02, -7.2250e-03, -2.7466e-02,  1.4404e-02, -4.7302e-02,
         2.3682e-02, -4.5410e-02,  7.2021e-02,  2.1606e-02,  2.5269e-02,
         2.7084e-02,  2.4780e-02, -3.1219e-02, -1.5332e-01, -3.2043e-02,
         1.7212e-02,  3.8818e-02,  9.2285e-02, -5.8838e-02, -2.1973e-03,
        -1.7212e-02, -3.4790e-02, -3.3264e-02, -1.1597e-02,  2.4121e-01,
         1.4404e-02, -4.7638e-02,  1.4526e-02,  3.7354e-02, -4.2419e-03,
         9.3994e-03,  2.6855e-02, -2.9297e-02, -3.5034e-02, -4.3701e-02,
         1.7090e-03,  4.1504e-02, -4.3396e-02, -1.1484e+00, -1.3037e-01,
        -2.6794e-02,  2.3041e-02,  6.8359e-03, -2.6367e-02, -3.5547e-01,
        -4.0894e-02,  3.2471e-02,  7.6294e-03,  2.0950e-02, -1.6602e-02,
        -2.4902e-02,  4.1260e-02,  6.3477e-03,  2.0142e-03,  1.1963e-02,
         2.7161e-03,  1.4893e-02, -1.1230e-01,  4.3945e-02, -2.7649e-02,
        -1.9824e-01,  2.7374e-02,  1.0803e-02, -7.3730e-02, -2.0508e-02,
         2.1515e-02,  9.2285e-02,  3.5706e-03, -1.5808e-02, -1.3062e-02,
        -2.5024e-02,  8.8379e-02,  5.4077e-02,  2.9236e-02,  6.4697e-03,
         7.0801e-03,  4.0436e-03,  3.0029e-02,  8.0566e-03, -9.9487e-03,
         3.3813e-02,  3.4790e-02, -8.5678e-03,  3.4790e-02,  1.5332e-01,
         1.7383e-01,  1.1688e-02,  1.9775e-02, -2.8809e-02,  1.3290e-02,
         2.6215e-02,  2.4155e-02,  1.9791e-02,  8.4229e-03,  7.2632e-03,
        -4.5532e-02, -2.5024e-02, -2.1973e-02,  3.4912e-02, -2.9373e-02,
        -4.2480e-02, -3.6621e-03,  8.6517e-03, -5.2124e-02, -3.7109e-02,
        -2.4414e-03, -6.9580e-03, -1.9287e-02,  2.9297e-03, -3.3539e-02,
        -1.1108e-02, -4.1382e-02, -3.2898e-02, -2.2705e-02,  4.5166e-03,
         3.3936e-02,  4.0924e-02,  6.9824e-02,  1.1182e-01,  7.5439e-02,
        -3.5400e-02,  2.7115e-02, -4.4678e-02, -3.7109e-02, -2.2363e-01,
        -3.0640e-02, -1.3135e-01,  1.1810e-02, -3.7720e-02, -4.3030e-03,
         1.5747e-02, -4.7461e-01, -1.3623e-01,  8.2031e-02, -2.5391e-02,
         1.5259e-02,  5.8594e-03,  3.2013e-02,  3.9734e-02,  3.5400e-02,
        -1.6797e-01, -5.8838e-02,  2.3438e-02, -6.5430e-02, -4.5868e-02,
         1.9531e-03,  1.5991e-02,  5.4688e-02,  3.7842e-02, -7.4219e-02,
         3.6896e-02, -1.5747e-02,  5.2490e-03,  1.7944e-02,  6.2286e-02,
        -1.1597e-02,  2.7054e-02, -2.9541e-02,  2.6489e-02, -2.9327e-02,
         3.8330e-02,  1.0791e-01, -6.2744e-02,  1.6953e-02,  3.1342e-02,
         3.6011e-02,  1.8311e-03, -7.1411e-03, -1.4206e-02, -2.6978e-02,
         2.8809e-02,  1.1621e-01, -3.9612e-02,  1.3794e-02,  3.3020e-02,
        -7.0557e-02, -1.5381e-02, -3.7842e-03,  5.2734e-02,  3.9062e-02,
        -2.4536e-02, -3.6987e-02, -1.8372e-02, -2.5781e-01, -1.1368e-02,
         5.4077e-02, -4.4586e-02,  4.9561e-02, -4.3945e-03,  2.4414e-04,
         1.3550e-02, -1.1353e-02,  1.7883e-02, -2.1454e-02,  2.6398e-02,
         5.1270e-02,  7.8125e-02,  8.9111e-02,  2.3254e-02,  2.0264e-02,
         4.7913e-02, -3.3142e-02, -3.9551e-02,  3.4027e-02,  1.0205e-01,
         5.6732e-02, -2.5879e-02,  3.7384e-02, -9.0088e-02, -2.5513e-02,
         6.9092e-02,  5.4688e-01, -9.7656e-03,  1.5320e-02, -1.8860e-02,
        -4.1077e-02,  4.4434e-02,  3.8867e-01,  5.4932e-03, -4.5898e-02,
        -3.3569e-03,  2.8564e-02,  1.0544e-02,  9.2163e-03,  9.2163e-03,
        -9.6130e-03,  1.6663e-02,  7.8125e-02,  2.3453e-02, -6.7139e-03,
         3.8086e-02, -1.5747e-02, -5.0781e-02, -9.1125e-02, -1.7639e-02,
         4.3213e-02,  2.3926e-02, -6.3782e-03,  3.3142e-02,  2.3438e-01,
        -2.6703e-02,  4.6539e-02,  9.2773e-03,  1.1597e-02,  1.2207e-04,
         7.2266e-02,  1.8311e-04,  2.0264e-02, -8.4473e-02, -2.3102e-02,
         5.2734e-02, -7.1045e-02, -3.5706e-02,  3.6499e-02, -2.9053e-02,
         1.2711e-02,  3.0957e-01,  4.6051e-02, -3.4485e-02,  5.2063e-02,
         4.0894e-03,  6.2256e-03, -1.5656e-02,  4.1382e-02,  1.0986e-02,
         4.3945e-02, -2.3804e-02,  3.6719e-01, -2.0264e-02,  1.4307e-01,
        -1.6571e-02,  1.0791e-01,  2.3193e-02, -3.0228e-02,  4.7668e-02,
        -4.9805e-01,  5.8594e-02, -1.1865e-01,  2.8931e-02,  2.5879e-02,
        -1.4038e-03, -1.0498e-02,  2.8885e-02, -1.8555e-02,  3.1250e-02,
        -3.3966e-02,  9.1553e-04, -4.0649e-02,  2.8198e-02,  9.1553e-03,
        -3.2776e-02, -2.9297e-03, -3.6835e-02,  6.0852e-02, -3.2288e-02,
        -1.1914e-01,  4.0894e-02, -2.8564e-02, -1.7090e-03, -8.5938e-02,
         7.4463e-03, -2.5665e-02, -4.9438e-03, -1.0986e-03,  6.7139e-03,
        -1.7639e-02, -4.0619e-02, -4.5349e-02,  1.9531e-02, -9.5215e-02,
        -1.1230e-01,  4.3579e-02,  2.0874e-02,  4.5654e-02, -4.4189e-02,
         5.9082e-02,  2.9526e-02,  3.9124e-02,  1.0010e-01,  1.9531e-03,
         7.3242e-03,  7.0068e-02, -5.1666e-02,  4.8523e-02,  1.3135e-01,
         8.7891e-02, -1.5967e-01, -1.4252e-02,  6.6833e-03, -6.6406e-02,
         2.6367e-02, -3.6133e-02, -1.3306e-02, -1.1169e-02,  6.0364e-02,
         1.4709e-02, -2.2675e-02,  5.5504e-03, -3.9673e-03, -3.1067e-02,
        -2.1973e-03, -4.0375e-02,  5.7037e-02, -3.1006e-02,  3.6072e-02,
         7.3730e-02,  1.1505e-02, -1.3306e-02, -1.0803e-02,  3.2043e-04,
        -3.7598e-02, -1.3794e-02, -3.5522e-02, -8.6060e-03,  4.4922e-02,
         2.1057e-02,  6.3477e-02, -1.0223e-02,  7.0557e-02, -8.4961e-02,
        -4.9805e-01,  5.8533e-02,  2.5024e-02,  2.0386e-02, -2.7588e-02,
         7.6660e-02,  3.7598e-02, -2.3926e-02, -2.5909e-02,  1.7426e-02,
        -4.0625e-01,  1.5039e-01], device='cuda:1', dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[-4.1953e+00, -1.1031e+01,  1.6797e+00, -1.0359e+01,  6.1328e+00,
         -1.4719e+01, -6.9375e+00,  1.7656e+00,  1.6406e+01, -5.4844e+00,
          8.4844e+00,  3.5586e+00,  4.2578e+00,  1.4594e+01,  2.8359e+00,
         -1.1766e+01,  1.1953e+01, -2.5586e-01,  1.2531e+01,  1.6074e+00,
         -9.0469e+00, -3.0859e+00, -9.8438e-01, -4.0781e+00,  4.5508e+00,
         -2.3887e+00,  5.0156e+00,  1.0109e+01, -1.5594e+01, -7.2188e+00,
          1.2109e+00, -1.8672e+00,  1.8562e+01, -5.3828e+00,  3.0469e+00,
         -4.9219e+00,  2.0781e+00,  7.2031e+00,  5.9844e+00, -1.0828e+01,
          9.1719e+00,  1.3242e+00,  3.1133e+00, -2.0000e+00, -3.2070e+00,
         -3.4941e+00, -7.1094e+00, -9.6875e+00,  4.9219e+00, -4.0391e+00,
         -4.1367e+00, -1.0020e+00, -1.6328e+00,  5.9375e-01, -3.9609e+00,
         -5.2969e+00, -9.9219e+00, -1.1938e+01, -8.7969e+00, -9.7500e+00,
          3.2344e+00,  9.8438e+00, -6.2656e+00,  1.0359e+01,  7.2031e+00,
         -1.2125e+01,  1.7754e+00, -1.0891e+01, -9.5469e+00,  9.8125e+00,
         -1.1578e+01,  1.3188e+01,  6.4844e+00, -1.2391e+01, -6.0703e+00,
         -4.6484e+00, -1.1531e+01, -5.7188e+00,  6.4531e+00,  1.0303e+00,
          2.7500e+00,  1.0719e+01,  7.1094e-01, -1.4609e+00,  3.9453e+00,
          8.6719e-01,  1.2047e+01,  3.6094e+00, -6.2812e+00,  2.2363e-01,
          1.1031e+01,  9.6406e+00, -6.2344e+00, -4.0508e+00,  7.6562e+00,
          3.8906e+00,  1.2734e+01,  1.0672e+01, -6.5527e-01, -4.6250e+00,
          1.1109e+01,  4.2227e+00,  6.0469e+00, -5.7344e+00, -4.2734e+00,
         -1.5703e+00,  8.0781e+00,  7.1406e+00, -1.1500e+01, -2.9805e+00,
          3.6133e-01,  1.0016e+01,  3.4609e+00, -2.2891e+00,  8.0938e+00,
          3.8750e+00, -2.2578e+00, -3.6875e+00, -8.7969e+00,  2.5176e+00,
          1.0977e+00,  7.5625e+00,  6.5859e+00, -7.6406e+00, -6.8984e+00,
          4.1250e+00, -3.6797e+00,  2.5703e+00,  4.7734e+00, -1.6016e+00,
         -1.2250e+01,  2.2246e+00,  7.5000e+00,  2.7344e+00,  5.1016e+00,
          6.9844e+00,  1.7062e+01,  1.5117e+00, -1.0531e+01,  7.8438e+00,
         -6.9219e+00, -5.7344e+00, -1.2562e+01, -1.5156e+00, -1.2203e+01,
          1.0422e+01,  1.4580e+00,  7.9395e-01,  1.9961e+00,  8.5000e+00,
          7.9531e+00, -3.6719e-01, -4.9062e+00, -1.9473e+00, -4.7314e-01,
          3.8281e-01,  3.8359e+00,  3.4922e+00, -4.9375e+00, -8.0938e+00,
         -7.8594e+00, -9.0312e+00, -8.6719e+00, -6.0312e+00,  4.3672e+00,
         -4.9180e+00, -9.0938e+00, -2.9453e+00,  6.9531e+00,  4.5625e+00,
          8.7812e+00, -4.6562e+00, -8.8750e+00,  1.6875e+00,  8.6914e-02,
         -4.4805e+00,  3.9922e+00, -8.4062e+00, -7.0547e+00, -4.3047e+00,
         -1.0297e+01,  1.1453e+01, -3.3867e+00, -1.7578e-01, -5.6250e+00,
         -2.9102e+00, -4.5117e+00, -1.7607e+00,  1.1500e+01,  5.6055e+00,
          7.3125e+00,  9.9688e+00, -5.8633e+00, -4.3594e+00, -1.8799e+00,
          3.8359e+00, -3.1816e+00, -6.6094e+00,  3.4297e+00, -2.1211e+00,
         -4.7969e+00,  1.1297e+01,  1.0469e+01, -1.7773e+00, -6.6602e-01,
          7.1562e+00, -1.4141e+00, -8.8125e+00, -1.1219e+01,  1.7383e+00,
          2.0918e+00,  4.2188e+00,  1.2094e+01,  1.0531e+01,  4.1328e+00,
         -6.8125e+00,  8.0000e+00, -1.5859e+00, -1.2168e+00, -7.6562e+00,
         -1.9453e+00, -9.3750e-02, -4.5469e+00,  8.3594e+00,  2.6367e+00,
          3.0391e+00, -8.4688e+00, -8.6094e+00, -8.0859e-01, -1.6250e+00,
          1.1641e+01,  1.0062e+01,  3.4570e+00, -8.7188e+00,  1.0750e+01,
         -1.1469e+01,  3.7422e+00, -6.9375e+00, -3.5977e+00, -3.5781e+00,
          3.6133e+00,  9.4844e+00,  4.0000e+00, -1.3375e+01,  4.8828e+00,
         -1.0844e+01,  1.0688e+01, -7.8594e+00, -6.9531e+00, -5.9297e+00,
          5.6250e+00,  1.6953e+00, -4.5938e+00,  3.4082e+00, -9.8750e+00,
         -5.1406e+00,  1.0641e+01,  6.7422e+00,  3.4844e+00,  1.1953e+00,
         -3.9062e+00,  6.6016e+00,  5.9375e-01,  5.3281e+00, -3.9102e+00,
         -1.0000e+01, -3.6562e+00, -1.7148e+00, -1.5266e+01, -3.6797e+00,
          9.4844e+00, -6.8594e+00, -1.7656e+00,  1.9414e+00,  1.0078e+00,
          1.0840e+00,  1.1672e+01,  8.5938e+00,  4.0078e+00, -3.8047e+00,
         -8.0000e+00,  3.1562e+00, -8.3398e-01,  2.0938e+00, -1.1844e+01,
         -9.3438e+00, -1.6211e+00,  1.1969e+01,  4.5234e+00, -2.3281e+00,
          1.1688e+01,  1.0234e+01, -8.0781e+00,  1.0125e+01,  1.1328e+01,
          5.9922e+00,  1.2781e+01,  2.0664e+00,  3.7109e+00,  7.7031e+00,
          6.1816e-01,  2.1328e+00,  4.9375e+00,  1.3000e+01,  1.1750e+01,
          6.3438e+00, -2.1680e+00,  8.4219e+00, -1.5078e+00, -3.7734e+00,
         -5.4688e+00,  5.0781e+00, -1.4047e+01,  1.0297e+01,  1.1531e+01,
         -2.5859e+00, -5.1172e+00,  1.0172e+01,  4.4844e+00, -3.7578e+00,
          3.0586e+00, -2.8047e+00,  4.7344e+00, -1.7188e+00, -6.2031e+00,
         -3.9844e-01, -1.2203e+01, -5.8594e+00, -1.2875e+01, -7.4375e+00,
         -8.9219e+00, -1.0250e+01, -8.4844e+00, -9.6719e+00,  1.7402e+00,
          3.6953e+00,  4.8047e+00,  1.1703e+01, -7.3145e-01, -5.2969e+00,
          1.1750e+01, -3.3984e+00, -2.1328e+00, -1.3906e+00, -2.0859e+00,
          1.3500e+01,  4.2031e+00,  1.7062e+01, -3.5000e+00, -3.1211e+00,
          2.3828e-01,  5.4531e+00, -6.6172e+00, -1.8047e+00,  5.8281e+00,
         -1.0422e+01, -2.6895e+00,  5.6484e+00, -6.7031e+00, -4.3750e+00,
         -7.9062e+00,  3.5859e+00,  7.6719e+00,  5.6484e+00,  1.8242e+00,
         -3.4414e+00,  8.7812e+00,  4.1250e+00,  1.2219e+01, -1.1203e+01,
         -4.7578e+00, -4.5391e+00,  3.9961e+00, -1.2031e+01,  9.9375e+00,
         -3.4473e+00, -2.1875e+00,  3.2500e+00,  8.8438e+00,  2.3711e+00,
         -4.8672e+00,  8.0781e+00, -6.0000e+00, -1.0531e+01, -4.0625e+00,
          1.6562e+01,  1.3281e+00, -1.8242e+00, -6.0938e-01, -1.4094e+01,
         -8.1055e-01, -1.6625e+01, -1.3203e+01,  9.0469e+00, -1.0500e+01,
          2.9492e+00,  4.6172e+00,  1.1328e+01, -6.2500e+00,  1.2062e+01,
          1.0359e+01, -1.1000e+01,  9.9688e+00,  1.6719e+00, -4.8096e-01,
          1.2969e+01,  1.4062e+00,  7.3516e+00,  2.0391e+00,  6.7188e-01,
          1.2109e+00,  3.3750e+00, -5.1758e+00, -1.0875e+01,  1.3859e+01,
         -6.1641e+00,  4.4766e+00, -5.3672e+00,  3.2891e+00,  3.7832e+00,
         -1.2031e+01, -1.0078e+01,  1.2547e+01, -1.4281e+01, -4.3359e+00,
         -8.7344e+00, -6.4531e+00,  4.8828e-01,  9.0938e+00, -7.1953e+00,
         -1.4438e+01,  9.5156e+00, -1.1234e+01,  9.1094e+00, -1.2359e+01,
         -1.5273e+00,  5.3672e+00, -1.1500e+01, -3.3984e+00, -2.7812e+00,
         -1.1699e+00, -7.4141e+00,  3.9414e+00,  6.1719e+00, -1.0031e+01,
          5.9844e+00, -2.2578e+00, -1.3172e+01,  2.9805e+00, -4.2656e+00,
         -1.7031e+01,  1.7469e+01,  8.2812e+00,  6.3828e+00, -6.2109e-01,
         -2.8281e+00,  1.1000e+01,  8.5938e+00,  1.2812e+01, -7.9688e+00,
          3.6016e+00,  4.8203e+00, -9.1562e+00,  1.0578e+01,  3.4609e+00,
          3.1641e+00, -1.5625e-02, -8.2500e+00, -5.9531e+00, -5.1172e+00,
          2.1875e-01, -5.1406e+00, -1.9688e+00, -6.5938e+00,  1.6938e+01,
          8.1875e+00, -9.3594e+00,  6.4688e+00, -9.0000e+00, -1.5656e+01,
          7.3594e+00, -1.3812e+01,  1.0656e+01,  2.0410e-01, -1.2500e+00,
          3.3047e+00, -2.0195e+00, -2.1094e+00,  1.6162e+00, -7.6094e+00,
         -1.5625e-02, -1.1625e+01, -5.1328e+00, -6.9844e+00,  8.1406e+00,
          6.5156e+00, -7.4023e-01, -9.6094e+00,  0.0000e+00, -4.8516e+00,
         -3.8906e+00,  1.0531e+01,  8.0469e+00, -4.2812e+00, -4.3672e+00,
          5.1406e+00, -5.5156e+00,  2.9297e+00, -1.3719e+01,  1.0891e+01,
         -6.4102e+00,  6.6562e+00]], device='cuda:7', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([[-6.0938e+00, -1.6750e+01,  5.3203e+00, -1.4375e+01,  7.3750e+00,
         -2.7125e+01, -1.2594e+01,  1.7031e+00,  2.3625e+01, -9.2344e+00,
          1.3031e+01,  5.1875e+00,  6.9531e+00,  1.8812e+01,  2.6953e+00,
         -1.2969e+01,  1.6719e+01, -1.1719e-02,  1.6438e+01,  3.7266e+00,
         -8.2188e+00, -5.3945e+00, -3.1406e+00, -4.8594e+00,  8.0312e+00,
         -1.7031e+00,  5.9531e+00,  1.1562e+01, -2.3375e+01, -9.8125e+00,
         -6.4062e-01, -3.5156e+00,  3.4188e+01, -6.0781e+00,  5.5938e+00,
         -5.7656e+00,  2.9824e+00,  9.5312e+00,  6.5000e+00, -1.3438e+01,
          1.2062e+01,  5.3906e-01,  3.0312e+00, -4.6094e+00, -4.3750e+00,
         -6.1875e+00, -9.9375e+00, -1.2469e+01,  7.4062e+00, -6.3438e+00,
         -7.4531e+00, -1.6992e+00, -3.5625e+00, -4.6875e-01, -2.9844e+00,
         -8.4062e+00, -1.5531e+01, -1.5438e+01, -1.4281e+01, -9.6250e+00,
          2.0664e+00,  1.2688e+01, -7.8125e+00,  1.2500e+01,  6.8125e+00,
         -1.4438e+01,  1.4688e+00, -1.6656e+01, -1.3094e+01,  1.2469e+01,
         -1.0812e+01,  1.3031e+01,  7.4688e+00, -1.6344e+01, -8.7500e+00,
         -6.0156e+00, -1.3062e+01, -7.9375e+00,  1.0188e+01,  1.5059e+00,
          3.3438e+00,  1.5000e+01,  2.1914e+00, -3.4688e+00,  7.2812e+00,
         -1.2344e+00,  1.6719e+01,  3.7656e+00, -8.7812e+00, -1.2178e+00,
          1.4156e+01,  1.5719e+01, -8.6562e+00, -3.3828e+00,  1.4156e+01,
          4.9531e+00,  1.6250e+01,  1.6656e+01, -1.1172e+00, -6.5312e+00,
          1.2469e+01,  8.2656e+00,  8.1250e+00, -9.0625e+00, -3.2344e+00,
         -2.8438e+00,  8.5000e+00,  8.0000e+00, -1.7438e+01, -6.5117e+00,
          2.1582e+00,  1.3750e+01,  4.2812e+00, -3.1406e+00,  1.2938e+01,
          7.9453e+00, -3.5469e+00, -3.2812e+00, -1.1938e+01,  6.5625e+00,
          1.1562e+00,  1.0969e+01,  1.0844e+01, -1.2531e+01, -1.0234e+01,
          3.7188e+00, -3.2969e+00,  3.2969e+00,  7.4688e+00, -2.5391e+00,
         -1.1812e+01,  5.0078e+00,  8.1875e+00,  6.3750e+00,  5.1562e+00,
          8.0938e+00,  2.3000e+01,  3.5938e-01, -1.3781e+01,  1.0406e+01,
         -1.0938e+01, -7.2812e+00, -1.7938e+01, -1.8438e+00, -1.5844e+01,
          1.3938e+01,  1.5381e+00,  6.7578e-01,  2.6641e+00,  9.5938e+00,
          9.9688e+00, -2.3438e-01, -6.6484e+00, -1.9531e-01, -1.7334e+00,
         -1.5625e-01,  6.5469e+00,  3.4219e+00, -6.8281e+00, -1.1188e+01,
         -1.5125e+01, -1.5312e+01, -1.6625e+01, -9.2188e+00,  5.8477e+00,
         -7.5625e+00, -1.3188e+01, -5.2500e+00,  1.0656e+01,  6.2969e+00,
          1.4062e+01, -8.3750e+00, -1.2250e+01,  2.2578e+00, -8.8379e-01,
         -7.8789e+00,  4.5156e+00, -9.9688e+00, -3.0547e+00, -7.5625e+00,
         -1.0844e+01,  1.3938e+01, -5.7031e+00,  1.7344e+00, -2.7656e+00,
         -3.8828e+00, -7.3750e+00, -3.5059e+00,  7.8750e+00,  1.0938e+01,
          1.2094e+01,  1.4312e+01, -9.0391e+00, -6.5156e+00, -3.3984e+00,
          3.2656e+00, -5.4805e+00, -7.5625e+00,  4.9844e+00, -1.8203e+00,
         -5.8125e+00,  9.1875e+00,  1.4531e+01, -9.1406e-01, -1.1719e-01,
          1.1172e+01, -2.2773e+00, -1.5375e+01, -1.7031e+01,  3.7812e+00,
          3.5078e+00,  3.6094e+00,  1.5438e+01,  1.5562e+01,  6.2344e+00,
         -8.0156e+00,  1.2812e+01, -3.0938e+00, -9.5703e-01, -1.0625e+01,
         -1.8672e+00,  1.3477e+00, -6.5000e+00,  1.2594e+01,  3.2266e+00,
          2.0312e+00, -9.4062e+00, -9.1875e+00,  1.1719e-01, -2.6719e+00,
          1.2781e+01,  1.6188e+01,  1.6172e+00, -1.4047e+01,  1.3500e+01,
         -1.5219e+01,  6.7344e+00, -8.7188e+00, -3.7656e+00, -2.3906e+00,
          6.7812e+00,  1.4312e+01,  5.1250e+00, -1.1562e+01,  7.6016e+00,
         -1.4094e+01,  1.2750e+01, -1.0906e+01, -1.0016e+01, -5.1875e+00,
          5.3438e+00,  3.3691e+00, -3.9688e+00,  5.6797e+00, -1.4281e+01,
         -6.8281e+00,  1.5219e+01,  7.3906e+00,  3.6719e+00, -8.2812e-01,
         -5.3750e+00,  8.2812e+00, -6.7578e-01,  8.5469e+00, -3.8125e+00,
         -1.4438e+01, -1.1719e+00, -3.5859e+00, -2.1281e+01, -3.0000e+00,
          1.2031e+01, -1.0773e+01, -2.4766e+00,  1.0078e+00,  4.8125e+00,
          2.7539e+00,  1.5719e+01,  1.3375e+01,  5.0938e+00, -4.7031e+00,
         -8.1562e+00,  2.7188e+00, -3.8281e+00,  3.2188e+00, -1.2750e+01,
         -1.2438e+01, -5.2344e-01,  1.7125e+01,  6.2812e+00, -2.7969e+00,
          1.1188e+01,  1.3000e+01, -1.3750e+01,  1.0000e+01,  9.6875e+00,
          7.5312e+00,  1.8938e+01,  4.5078e+00,  3.7344e+00,  1.1188e+01,
         -5.4688e-02, -1.3125e+00,  8.3750e+00,  1.8125e+01,  1.6688e+01,
          9.0312e+00, -5.5625e+00,  1.3438e+01, -1.1328e+00, -5.5781e+00,
         -9.0781e+00,  6.2344e+00, -1.6156e+01,  1.6812e+01,  1.1844e+01,
         -2.4375e+00, -6.1406e+00,  1.7656e+01,  5.9688e+00, -6.3125e+00,
          4.9531e+00, -2.8125e+00,  7.8750e+00, -2.3594e+00, -8.5000e+00,
         -1.1680e+00, -1.6750e+01, -8.9219e+00, -1.3469e+01, -1.1281e+01,
         -1.3781e+01, -1.5438e+01, -1.0250e+01, -1.1656e+01,  3.3438e+00,
          3.8906e+00,  7.0312e+00,  1.1969e+01, -1.1934e+00, -6.4531e+00,
          1.6875e+01, -4.6875e+00, -3.5625e+00, -2.3438e+00, -3.1016e+00,
          1.1875e+01,  4.9375e+00,  2.4312e+01, -3.9688e+00, -3.7812e+00,
          7.8711e-01,  5.6875e+00, -1.1500e+01, -1.8750e+00,  1.2203e+01,
         -1.2062e+01, -5.7266e+00,  3.5156e+00, -1.0375e+01, -8.2969e+00,
         -1.2656e+01,  3.8750e+00,  8.7188e+00,  8.1562e+00,  4.3281e+00,
         -4.1875e+00,  1.5125e+01,  5.1406e+00,  1.4188e+01, -1.8406e+01,
         -4.7500e+00, -5.4219e+00,  5.7188e+00, -9.3125e+00,  1.4625e+01,
         -6.9688e+00, -1.5625e+00,  6.2969e+00,  1.1625e+01,  2.0938e+00,
         -4.4062e+00,  1.3469e+01, -8.2188e+00, -1.3125e+01, -4.1875e+00,
          2.1375e+01,  2.0078e+00, -4.3359e+00, -4.8438e-01, -1.2375e+01,
         -5.5859e-01, -2.2812e+01, -1.5469e+01,  1.3281e+01, -1.3625e+01,
          4.5938e+00,  5.3828e+00,  1.5781e+01, -9.5625e+00,  1.6312e+01,
          1.6031e+01, -1.5125e+01,  1.2719e+01,  1.1016e+00, -3.6621e-01,
          2.1438e+01,  2.6797e+00,  4.5938e+00,  3.6094e+00,  5.1123e-01,
          1.8750e+00,  4.5625e+00, -1.0211e+01, -1.3125e+01,  1.7375e+01,
         -5.3125e+00,  4.1094e+00, -7.8750e+00,  3.2812e+00,  7.1875e+00,
         -1.8375e+01, -1.3844e+01,  1.6219e+01, -1.9844e+01, -7.8750e+00,
         -1.4312e+01, -9.3906e+00,  1.4297e+00,  1.2594e+01, -1.3500e+01,
         -2.2812e+01,  1.1281e+01, -1.3344e+01,  1.3781e+01, -1.5281e+01,
         -2.1484e+00,  6.9062e+00, -1.3500e+01, -4.0547e+00, -3.0938e+00,
         -2.0820e+00, -9.3125e+00,  5.2812e+00,  8.7188e+00, -1.6844e+01,
          5.7188e+00, -2.4688e+00, -1.7688e+01,  4.4141e+00, -4.5938e+00,
         -3.2438e+01,  2.3750e+01,  1.0438e+01,  1.0438e+01,  1.0547e+00,
         -6.5977e+00,  1.7312e+01,  1.1781e+01,  1.9625e+01, -8.2812e+00,
          5.1875e+00,  6.0938e+00, -9.9375e+00,  1.2000e+01,  4.7812e+00,
          5.4688e+00, -5.0000e-01, -1.0781e+01, -1.0531e+01, -6.0625e+00,
          1.7500e+00, -5.2656e+00, -1.1250e+00, -8.1250e+00,  2.7312e+01,
          1.4094e+01, -1.0125e+01,  1.0656e+01, -8.8125e+00, -1.9875e+01,
          1.2062e+01, -1.6812e+01,  1.2188e+01,  1.0215e+00, -1.6094e+00,
          4.2812e+00, -2.1934e+00, -1.7500e+00,  4.3047e+00, -9.4688e+00,
          2.6250e+00, -1.4750e+01, -6.9531e+00, -1.0844e+01,  1.0719e+01,
          1.0453e+01, -3.6406e+00, -1.4844e+01, -1.0156e+00, -4.0312e+00,
         -3.9375e+00,  1.1438e+01,  1.1250e+01, -8.0938e+00, -5.5312e+00,
          5.4688e+00, -5.5781e+00,  2.0156e+00, -2.0812e+01,  1.1688e+01,
         -5.8672e+00,  6.2500e+00]], device='cuda:3', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([[ -8.9062, -22.2500,  13.4375, -16.3125,  13.8750, -36.3750, -23.9375,
          -7.1875,  27.2500, -12.8750,  11.2500,   7.5938,   8.3750,  28.2500,
           1.2500, -21.1250,  27.6250,  -0.5469,   7.5000,   3.4531,  -7.3750,
         -13.7812,   0.0781,  -4.8125,  24.4375,  -2.4609,   8.5000,   6.5000,
         -11.6250,  -9.1875, -13.4375,  -6.6875,  34.7500,  -1.5000,  21.2500,
          -2.6875,  14.4219,   7.5625,   4.7500, -24.0000,  14.0625,  -1.7188,
           5.8438,  -8.8438,  -1.0156, -27.2812, -11.3125, -13.7500,   3.8125,
          -0.9375, -15.9062,  -3.8555,  -0.5625,   6.5625,   5.0000, -11.0000,
         -24.3750, -29.2500, -21.8750, -14.8750,  -3.1328,  21.1250,  -6.4375,
           7.6250,  15.1875, -23.9375,   5.7969, -15.7500, -14.6250,   8.5625,
         -12.0000,  14.0000,  10.6875, -23.1250,  -6.8750, -14.5000, -16.8750,
          -8.5938,  13.3125,   3.8438,   1.4375,   7.5000,   5.2344,  -6.5625,
          16.6875, -10.6875,  22.1250,  -6.2500, -16.6875,  -1.3701,  13.2500,
          22.0000, -13.8750,   7.8906,  16.2500,  -4.9062,  17.5000,  13.6250,
         -14.7344, -11.7500,  11.6250,  21.7812,   9.6250, -11.5000,  -2.6875,
         -10.1250,  11.3750,   1.2500, -23.8750, -18.0312,   2.9062,  14.5000,
           1.0312,   1.6562,   6.0000,  10.6562,  -1.5625,   5.6250, -19.8750,
          25.6719,  -9.0938,  18.1250,  27.8438, -21.9375, -17.0625,   2.6875,
           1.0000,   5.2188,  14.3750,  -2.0625,  -8.0000,  18.5156,  12.1875,
           9.1875,   7.6875,  11.0000,  26.7500,  -4.7031, -10.7500,   8.5000,
         -10.7500, -17.3438,  -7.8750,  -7.1094, -19.2500,  13.5000,   4.5664,
           5.9219,   2.9062,  15.6250,  10.9375,  -4.6562, -11.3125,   3.1914,
          -2.6094,  -6.8750,   6.3750,  -9.9375,  -8.3750,  -8.8125, -14.5000,
         -23.2500, -21.3750, -12.5312,  13.8594, -22.7188, -23.0000,  -4.0000,
          14.0000,  10.5625,  20.6875,  -3.4375, -12.4375,  -4.5312,  -6.2969,
         -22.9531,   0.6250,  -8.5625,   9.7734,  -1.5938, -11.1250,  20.6250,
         -11.8750,   6.4258,   2.5625,  -3.0312, -11.8750, -13.5156,   8.0000,
          28.5312,  28.2500,  13.6250, -24.3125, -13.7500, -15.6016,   0.0000,
         -16.2656,   0.8750,   7.6250,   3.6406,  -2.8438,   6.7500,  23.8750,
           4.1406,   4.6719,  23.3125, -11.7344, -23.2500, -24.5000,  18.9531,
           5.2266,   5.6562,  17.0000,  13.1250,   5.8750, -11.7500,  12.2500,
           3.0312,  -4.8594, -15.0000,  -4.0469,   1.6562, -15.8125,  19.3125,
           6.0000,  -6.0312,  -7.6250, -11.0000,  10.1016,  -1.7188,  17.7500,
          18.7500,   0.3750, -31.8750,  23.6250, -14.8750,  12.2812,  -5.6875,
          -7.3281,  -4.3750,  19.0938,  22.5625,  10.5312,  -7.3750,  22.1562,
         -21.0000,   8.0000, -16.2500, -29.6250,  -6.4375,  10.5000,  11.2812,
          -5.3750,  18.0000, -20.8750, -15.0625,  15.8750,   5.1250,   0.1250,
          -9.3750,   2.1250,  14.0625,  -1.0938,   8.4375,   0.1562, -15.1250,
           3.5000,  -5.8438, -21.7500,  -3.1875,   8.7500, -16.5000,  -1.4844,
          -2.7812,  16.6406,  13.6562,  14.1875,  16.5625,   5.0000,  -4.8750,
          -2.7500,   0.6875,  -9.6641,   3.6562, -19.3750, -17.0000,  -2.2656,
           8.5000,   7.6250,  -1.0000,  11.0000,  25.3750,  -7.0000,  16.5000,
          10.8750,  16.7500,  18.8750,   5.9844,  -6.9375,  10.5625,   1.3984,
         -11.2500,  15.0938,  28.3750,  19.5000,  12.1875, -12.9688,  17.8125,
           7.0312,  -7.0000, -17.8125,  10.7812, -18.6250,  24.1250,  19.3125,
           5.9375,  -6.2500,  28.9375,   4.3750,  -7.3750,  17.2188,  -4.7188,
           8.7500,  19.0312,  -5.3750,  -4.9844, -10.8750, -22.1562, -23.7500,
         -16.7500, -15.2500,  -9.0000, -13.3125,  -6.3750,   3.2188,  -1.6875,
          10.9688,  14.1875,  -4.0586,  -0.5000,  12.2500, -12.1875,   5.3750,
           2.1875,  -1.2656,  15.0000,   1.1562,  40.2500,   3.6875,  -5.2500,
           4.8281,   1.0625, -22.7500,  -4.5078,  17.2500, -10.5000, -14.4766,
          -8.2500, -11.3750, -10.9375,  -8.5000,   2.0625,  20.5000,   3.5625,
          13.8438,  -4.6562,   6.2500,   3.7500,  22.1250, -21.1250,  -3.7500,
          -4.2500,  16.6719,  -8.7500,  17.6250, -15.3828,   3.2500,  12.5312,
          23.3750, -10.2031,  -4.0625,  11.4375,  -2.2500, -16.3750,  -0.3750,
          35.7500,   3.2500, -10.6719,  13.4375, -10.7500,   1.9062, -28.2500,
         -17.7500,  18.3750, -19.0000,  12.5312,  -1.4062,  24.1875,  -4.7500,
          18.6250,  25.5625, -14.3750,  17.8750,  -4.5938,  -4.6328,  14.8750,
           2.5781,  -7.1875,  -0.1406,  -1.2715,   0.5625,  -1.5000, -29.2188,
          -6.1250,  20.8750,  17.8125,   0.7500,  -3.8125,  -4.0312,  14.2344,
         -24.0000, -32.3125,  19.1250, -31.6250, -16.3750, -20.1250,  -5.8750,
          -1.5469,  14.6250, -29.9375, -25.2500,  21.0000, -19.8750,  11.3750,
         -24.5000,   0.7656,   7.3125, -13.3750, -10.0312,   1.0000,  -2.9844,
         -19.8750,  12.3750,  11.0625, -24.4375,   1.1875,  -1.1797, -28.8750,
           6.2188,  -0.8750, -54.5000,  27.1250,  15.6250,   8.5000,  10.7109,
         -23.8438,  18.6250,  14.8750,  19.6250,  -7.6875,  10.4688,   3.4688,
         -17.2500,  16.2500,  -2.4688,   1.3750,   7.5625, -15.0625,  -2.1250,
          -6.1250,   5.6562,   1.6875,   7.5938, -12.5625,  30.5000,  12.0000,
         -13.2500,  16.3750, -16.0000, -31.0000,  14.5000, -10.8750,  22.2500,
           8.1719,   1.3438,   0.2188,  -2.3379,   0.0000,  14.3594, -17.3750,
          18.9688, -29.2500,  -9.9688,  -8.0000,   6.8125,  16.1250, -14.7500,
          -8.3125, -13.0000,   3.3125,  10.2188,  13.7500,  17.4375, -18.0312,
          -2.2500,   3.0625,  -2.6250,   2.1250, -18.0000,  11.5000,  -8.8750,
           1.0625]], device='cuda:6', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([[-2.8125e+00, -1.2750e+01, -1.0938e-01, -8.8750e+00,  3.6875e+00,
         -1.4188e+01,  6.2500e-02, -1.6406e+00,  1.6062e+01, -3.6094e+00,
          7.5938e+00,  1.4375e+00,  4.6875e+00,  1.3156e+01,  3.7305e+00,
         -1.0156e+01,  1.0844e+01,  1.0859e+00,  1.0281e+01,  3.8594e+00,
         -7.1562e+00, -5.6719e+00, -1.3047e+00, -1.3906e+00,  2.6719e+00,
         -5.0156e+00,  2.5781e+00,  8.4688e+00, -1.4562e+01, -3.5312e+00,
         -1.9688e+00, -2.9766e+00,  2.8938e+01, -1.8281e+00,  1.7266e+00,
         -5.3516e+00,  2.1309e+00,  5.9375e+00,  3.5781e+00, -6.6562e+00,
          8.4062e+00, -1.4570e+00,  1.6719e+00, -2.9219e+00, -2.2891e+00,
         -4.4883e+00, -2.0312e+00, -2.9375e+00,  5.1875e+00, -2.8906e+00,
         -3.8828e+00, -2.0977e+00,  1.9844e+00,  1.4844e+00,  1.1250e+00,
         -4.2188e+00, -9.8750e+00, -4.9375e+00, -1.0125e+01, -6.5312e+00,
          4.4297e+00,  4.6562e+00, -3.9688e+00,  8.0938e+00,  4.2500e+00,
         -8.0625e+00, -2.3125e+00, -9.3438e+00, -9.5312e+00,  9.4375e+00,
         -1.0250e+01,  1.1562e+01,  4.7812e+00, -7.9688e+00, -3.0469e+00,
         -1.2656e+00, -7.4375e+00, -4.6562e+00,  2.1250e+00,  1.1426e+00,
         -2.0312e-01,  7.2812e+00,  1.7578e+00, -1.0938e-01,  4.7891e+00,
         -3.5859e+00,  8.7188e+00, -1.1094e+00, -7.1250e+00,  1.7578e-01,
          6.3750e+00,  5.6562e+00, -8.5938e+00, -1.7578e+00,  4.8438e+00,
          3.5156e+00,  9.0625e+00,  1.4000e+01, -2.6035e+00, -1.8906e+00,
          5.0000e+00,  3.6094e+00,  3.4531e+00, -7.0625e+00, -3.9297e+00,
         -5.1133e+00,  6.2500e+00,  4.6562e+00, -1.3469e+01, -1.4219e+00,
          1.9141e-01,  5.6250e+00,  3.2969e+00, -1.2188e+00,  7.7188e+00,
          5.8750e+00, -3.0781e+00, -1.1562e+00, -7.6562e+00,  7.0312e-01,
         -9.8438e-01,  8.3750e+00,  4.3281e+00, -5.9062e+00, -7.0625e+00,
          6.5000e+00, -2.8125e-01, -9.7656e-01,  2.7656e+00, -3.3203e+00,
         -8.8750e+00,  9.1895e-01,  4.0312e+00,  2.9844e+00,  1.5000e+00,
          4.7031e+00,  1.4125e+01, -2.0312e+00, -1.0469e+01,  8.4688e+00,
         -4.2188e+00, -5.6953e+00, -1.2688e+01,  5.8203e-01, -8.5000e+00,
          6.0938e+00,  1.5156e+00, -6.9922e-01,  2.6328e+00,  7.7188e+00,
          5.7500e+00, -4.3789e+00, -4.8984e+00, -1.2656e+00,  1.1162e+00,
         -5.3125e-01,  1.7812e+00, -8.7500e-01, -1.7031e+00, -7.8750e+00,
         -1.0281e+01, -8.6562e+00, -5.8438e+00, -4.9688e+00,  4.2656e+00,
         -3.0156e+00, -9.9375e+00, -1.2344e+00,  4.0000e+00,  1.6875e+00,
          8.4375e+00, -3.6875e+00, -7.4375e+00,  9.4531e-01, -5.7227e-01,
         -5.6797e+00,  3.9062e-01, -4.0312e+00, -5.0977e+00, -2.7344e+00,
         -7.2812e+00,  1.1188e+01, -7.1875e-01, -6.3281e-01,  1.4062e-01,
         -2.6875e+00, -5.7109e+00, -1.9980e+00,  8.0000e+00,  5.7969e+00,
          7.3906e+00,  5.4375e+00, -4.4297e+00, -1.2500e+00, -7.1289e-01,
          1.4844e+00, -1.5020e+00, -3.0938e+00,  2.8438e+00, -1.9531e+00,
         -3.3594e+00,  6.5312e+00,  5.2500e+00, -1.6484e+00,  3.6836e+00,
          6.3438e+00,  2.0742e+00, -1.1438e+01, -1.0688e+01,  1.6641e+00,
          4.3203e+00,  6.7188e-01,  7.8750e+00,  8.4062e+00,  2.1719e+00,
         -5.7812e+00,  1.0125e+01, -1.3750e+00, -6.1328e-01, -1.8750e+00,
         -1.6797e+00, -2.6406e+00,  1.2500e-01,  8.1250e+00,  7.1875e-01,
         -6.6406e-01, -8.0000e+00, -5.4375e+00, -3.5938e-01,  1.6250e+00,
          6.6562e+00,  6.3750e+00,  4.0430e+00, -8.5469e+00,  6.4062e+00,
         -1.1844e+01,  3.0312e+00, -8.8594e+00, -4.2695e+00, -7.9688e-01,
          4.4531e+00,  9.3438e+00,  2.6562e+00, -7.2500e+00,  6.1797e+00,
         -9.6562e+00,  1.0000e+01, -8.0312e+00, -6.0625e+00, -3.5938e+00,
          4.3750e+00,  4.0156e+00, -1.1562e+00,  1.9727e+00, -4.7812e+00,
         -3.0781e+00,  6.9375e+00,  2.8281e+00,  1.0000e+00, -1.7500e+00,
         -6.5625e-01,  3.7500e+00, -4.0625e-01,  6.3125e+00, -3.6094e+00,
         -6.5938e+00,  1.4062e-01, -1.3125e+00, -9.6875e+00, -3.0781e+00,
          7.6875e+00, -7.7109e+00, -9.2188e-01,  8.5156e-01,  1.8105e+00,
          1.4062e+00,  8.3750e+00,  6.1875e+00,  3.9844e+00, -3.3906e+00,
         -4.7344e+00,  6.8750e-01, -3.3711e+00,  2.3438e+00, -9.5938e+00,
         -9.7812e+00, -1.6250e+00,  4.0000e+00,  1.4219e+00, -2.5938e+00,
          9.9375e+00,  7.2500e+00, -6.7500e+00,  7.9062e+00,  1.1375e+01,
          4.5469e+00,  9.6875e+00,  3.3086e+00,  6.5469e+00,  1.0750e+01,
         -7.2607e-01, -2.8125e+00,  5.9688e+00,  9.9062e+00,  9.6250e+00,
          3.9219e+00, -3.9453e-01,  9.1875e+00,  9.3750e-02, -2.6562e+00,
         -5.2812e+00,  4.5625e+00, -1.1125e+01,  7.1562e+00,  7.9688e+00,
          2.3281e+00, -4.5156e+00,  8.8750e+00, -1.2188e+00, -5.0469e+00,
          2.1406e+00,  4.1406e-01,  4.1719e+00,  1.5000e+00, -4.0625e+00,
         -6.4062e-01, -9.6562e+00, -5.0156e+00, -9.8438e+00, -4.6562e+00,
         -7.7812e+00, -5.0000e+00, -8.5625e+00, -5.6562e+00,  1.5391e+00,
         -2.0312e-01,  3.8047e+00,  1.0812e+01, -3.4258e+00, -6.0156e+00,
          9.2188e+00,  2.4453e+00,  1.7031e+00, -9.8438e-01, -8.6719e-01,
          1.0375e+01,  3.0234e+00,  2.0031e+01,  6.2500e-02, -6.5625e-01,
         -1.5986e+00,  1.0938e+01, -3.5781e+00, -5.2188e+00,  7.2188e+00,
         -8.9688e+00, -6.7578e-01,  3.5625e+00, -5.6875e+00, -3.1875e+00,
         -9.5938e+00,  1.0312e+00,  7.6875e+00,  3.3125e+00, -6.1719e-01,
         -3.9062e-01,  3.6562e+00,  3.0781e+00,  1.0688e+01, -7.6250e+00,
         -3.3906e+00, -2.2344e+00,  5.0859e+00, -7.0000e+00,  8.0938e+00,
         -5.0703e+00,  8.9062e-01,  3.4688e+00,  8.5312e+00,  3.7500e-01,
         -4.6406e+00,  8.3750e+00, -2.3125e+00, -9.3438e+00, -2.4062e+00,
          1.6812e+01,  7.1484e-01, -4.0430e+00, -2.3438e-01, -1.3188e+01,
         -1.4766e+00, -1.2125e+01, -9.4375e+00,  5.3750e+00, -5.0938e+00,
         -5.3906e-01,  3.8984e+00,  9.3125e+00, -5.8594e+00,  8.8438e+00,
          8.6250e+00, -8.7188e+00,  9.0938e+00,  1.3125e+00,  2.9492e-01,
          9.1562e+00,  2.5156e+00,  3.5938e+00,  1.2578e+00,  1.8730e+00,
          5.3906e-01,  7.0312e-01, -5.7266e+00, -8.3125e+00,  1.0281e+01,
         -6.9688e+00,  1.9688e+00, -2.3906e+00, -2.0312e-01,  5.5625e+00,
         -8.3750e+00, -6.6562e+00,  8.8438e+00, -1.1281e+01, -3.0000e+00,
         -1.0625e+01, -7.9531e+00, -6.6406e-01,  5.6875e+00, -2.7031e+00,
         -1.2750e+01,  8.2812e+00, -8.0312e+00,  3.3750e+00, -1.2062e+01,
          6.0156e-01,  3.7344e+00, -8.8438e+00, -8.0469e-01,  1.3125e+00,
         -2.5820e+00, -4.5000e+00,  3.7500e+00,  3.1875e+00, -8.0938e+00,
          5.7812e+00, -4.4922e+00, -1.3219e+01,  2.2891e+00, -6.4062e-01,
         -1.7812e+01,  1.2375e+01,  4.2812e+00,  4.4219e+00, -4.8047e-01,
         -1.4062e+00,  9.5000e+00,  9.8125e+00,  1.4062e+01, -3.4688e+00,
          1.9219e+00,  2.6562e+00, -6.2812e+00,  8.5938e+00,  2.4062e+00,
          2.9141e+00,  1.2266e+00, -7.2188e+00, -4.6875e+00, -4.1406e+00,
         -1.5625e-02, -1.7344e+00, -3.4062e+00, -2.7500e+00,  1.4406e+01,
          7.6562e+00, -5.5625e+00,  2.2812e+00, -4.8750e+00, -1.4562e+01,
          6.4375e+00, -1.2125e+01,  9.5938e+00,  2.9180e+00, -5.7031e-01,
          3.1094e+00, -3.9043e+00,  3.7500e-01,  2.3203e+00, -5.5625e+00,
          3.1094e+00, -9.0938e+00, -2.9688e+00, -6.7500e+00,  4.9531e+00,
          3.9375e+00,  7.3828e-01, -7.1250e+00, -3.2812e+00, -1.1719e+00,
         -1.0625e+00,  1.2094e+01,  4.7812e+00, -4.7031e+00, -1.5312e+00,
          2.7188e+00, -5.2031e+00,  1.3438e+00, -1.2750e+01,  8.0625e+00,
         -5.8125e+00,  2.0000e+00]], device='cuda:4', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2023-09-02 21:49:43 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/crash.pt
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([[ 0.7969,  0.3281, -0.0566, -1.9375, -1.0000,  0.6250,  0.8438, -1.3008,
          0.5781,  0.0547,  0.6250, -0.1289,  0.3672, -0.9375,  0.6641,  0.3125,
          0.9375,  0.4883,  0.2500,  0.0547,  1.3281, -1.1621,  0.6406,  0.6172,
          1.2598, -0.8672, -0.1562, -1.4531,  1.0312,  1.8906, -1.4023,  1.2812,
          0.7812,  0.1094,  0.1172, -0.5586,  0.0632, -0.7656, -0.8047,  0.2656,
         -1.1875, -0.1230,  0.6777,  1.1367, -0.5898, -2.2871,  0.4844,  0.7188,
         -0.9375, -0.6250,  0.0781, -0.5166,  1.8945,  2.0742,  1.0234,  1.5000,
          1.3594,  0.0938,  0.4688,  1.2188,  0.7539, -0.1094, -0.6328,  0.4375,
         -0.5391, -0.5781, -0.5088, -0.9688, -0.5547,  0.5625,  0.8906,  0.2812,
         -0.9766, -0.1406, -0.0703,  1.2578,  0.5469, -0.4922,  0.0781,  1.1475,
         -1.3555, -0.2188, -1.0273,  0.8828,  0.5742, -1.4727, -0.4062, -0.6719,
         -0.1328, -0.0864, -0.3594, -0.7500,  0.1250, -1.9492, -0.4375, -0.7891,
          1.0156, -0.2500, -0.9746,  1.6484, -1.5469,  0.8477,  0.4609, -1.3242,
         -0.8672, -2.1660, -0.4062,  0.2422,  0.1875, -1.3496,  0.2363, -1.1875,
          0.3203,  0.4922, -1.8750,  0.7109,  1.2969,  1.0156, -0.5156,  1.5625,
         -0.8906,  1.1875,  1.4883, -0.0078, -1.0703, -0.9375,  0.5859,  0.0195,
         -0.3516,  0.8809,  0.6406,  1.3584, -1.1562, -0.8789, -0.0469,  0.7031,
          0.1562, -0.5742, -0.0625, -0.0625,  0.6250, -1.6055,  1.2656, -0.2031,
         -0.0469, -1.6875,  0.2090,  1.2734, -0.1602, -0.6562, -1.4531, -1.9297,
         -0.3711, -1.1748, -0.3103, -1.6484, -1.0547, -1.7109,  0.4375,  0.3750,
          0.5312, -0.4062,  0.2031, -1.1797,  2.7363, -0.1836, -0.4219,  1.2148,
          0.1250, -0.0664, -0.5000,  1.6562,  0.2969, -1.0352, -0.9297, -1.8086,
         -0.4375,  2.1094, -3.8750, -0.9414, -0.2188, -0.1562, -0.0078,  0.7539,
         -0.3672, -0.0469, -0.8164, -0.7832, -2.0469,  1.2383,  1.3906,  1.4219,
         -1.3164, -0.7500, -0.8848, -0.5234, -1.3477,  0.2031,  0.7148,  0.1172,
         -0.3750, -1.1406, -0.7812,  0.2383,  0.5020,  0.5234,  0.7471, -0.0312,
         -0.9219,  2.4023,  0.7227, -0.1484,  0.8125, -1.4375, -0.1875,  0.2266,
         -1.7500,  0.9609, -1.1963,  1.2188,  0.2949, -1.0840,  0.9297,  0.9609,
          0.1289,  0.1328,  1.1719, -0.6328,  0.2148,  1.2930, -0.7812, -0.2812,
          0.9609, -1.6484, -0.2344, -0.2344, -0.2266, -1.1719, -0.3945,  0.6836,
          0.6328,  0.2500, -0.0391,  1.0781,  1.4785,  0.4531, -1.8750,  0.4219,
         -1.2930,  0.7656, -1.7891,  0.8101,  0.1094,  0.9971,  1.2969,  0.7812,
          0.4531,  0.1875,  0.3047, -0.9453,  1.2109,  0.3125, -1.2539, -0.5547,
         -1.7070,  0.4688,  0.3555, -0.0703, -0.6719,  0.2930, -1.3594, -4.0625,
          0.0898, -0.6992,  0.4766,  1.4512,  0.5156,  0.1875, -0.2500,  0.8594,
          0.0703, -1.9453, -0.2461, -1.3320, -0.1250, -0.2188,  0.3359, -2.6406,
         -0.9766, -0.4316, -0.9219,  0.6406,  0.3594,  0.0625, -1.3750, -0.2188,
         -0.7031,  0.1641,  0.1719, -0.5469,  0.1039, -2.0508, -0.1797, -1.7969,
          0.2344,  0.4297, -0.7021,  0.0781,  1.1289, -0.3438, -1.2617,  0.9844,
         -0.5469, -0.1406, -0.4219,  2.2578, -0.2578, -0.5156, -3.3672,  0.8438,
          0.3125,  0.0547, -1.0781,  2.0781,  1.5156,  0.4404,  0.7969, -1.0352,
         -0.3281,  0.5078,  0.3750, -0.5469, -0.4375,  0.6562,  0.1777, -0.0898,
          1.0156,  1.5938, -0.3799,  0.6797,  0.0312, -1.4707,  2.0156,  1.2461,
          0.2441, -2.1875, -0.0938,  0.8281,  0.4531,  0.4375,  0.0254,  4.1484,
         -0.5391, -1.1475,  0.1172,  0.5781, -1.1094,  1.6445,  1.8438,  0.0859,
          0.9062, -0.6797, -1.1172, -0.1250,  0.7090,  0.1797, -0.6406,  0.8477,
         -1.9375, -1.1875,  0.6875,  0.5859,  1.0039,  0.6250, -0.6719, -1.6758,
          2.3047, -0.2109,  1.0078,  1.0859, -0.0547, -0.7969,  2.2812,  1.2031,
          0.1875,  0.3125,  0.1855, -0.1445,  1.3594,  0.5000,  0.5488, -0.5000,
         -0.4531,  0.5000,  0.1875, -0.4023,  2.6230, -0.0156, -0.1719, -1.2344,
         -0.0469,  1.8594,  1.2266, -0.3047,  0.0259,  0.9531, -0.5127,  2.1172,
         -0.8984,  0.0729, -1.2539,  0.1016, -0.9570,  1.1094,  0.4844, -3.1328,
          0.2656, -0.1875,  0.0859,  1.8652, -0.5781, -0.0859, -0.4062, -1.3438,
          0.7031,  0.1406, -0.1719, -1.7148, -1.2500, -0.8047, -0.7188,  0.4453,
          0.9375, -1.5781, -0.0938, -1.2285,  1.2500,  0.6562,  0.1992,  0.6094,
         -0.1816, -0.4453,  1.2656, -1.4297, -0.4062, -0.3906, -0.6523, -0.4062,
          1.1914,  1.2266, -2.0312,  0.8438,  0.2422,  0.6328,  0.7188, -1.0898,
         -0.4219, -0.1562, -0.0625,  1.9375,  0.1641, -0.0625,  0.9688, -0.0781,
         -0.0430,  0.6172,  0.9121, -0.6875,  1.6562, -0.6797,  1.8203, -0.1875,
          0.3828,  1.0312,  0.9375, -0.2812,  0.3594, -0.6172,  2.5156, -0.5781,
         -1.5469,  0.8750,  0.5781,  0.4590,  1.2148, -0.3125, -0.3574,  0.9141,
          0.8799, -0.2891,  2.3477, -0.9375, -0.0547,  0.7188,  0.5234, -0.2031,
          0.4678, -0.2188, -1.9336,  0.4766, -1.4570, -0.7656,  0.5859, -0.4688,
          0.7500,  0.1484,  0.3750, -1.6133,  0.0625, -0.6562, -3.9824, -0.1484]],
       device='cuda:1', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 693, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 550, in _per_task_train_loss
    assert False
AssertionError

Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/spawn.py", line 126, in _main
    self = reduction.pickle.load(from_parent)
_pickle.UnpicklingError: pickle data was truncated
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 29 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-09-02 21:52:21 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:11361
2023-09-02 21:52:21 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:11361
2023-09-02 21:52:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-09-02 21:52:21 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:11361
2023-09-02 21:52:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-09-02 21:52:21 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:11361
2023-09-02 21:52:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-09-02 21:52:22 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:11361
2023-09-02 21:52:22 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-09-02 21:52:22 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:11361
2023-09-02 21:52:22 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-09-02 21:52:22 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:11361
2023-09-02 21:52:22 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:11361
2023-09-02 21:52:22 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-09-02 21:52:22 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-09-02 21:52:22 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-09-02 21:52:22 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:52:22 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 1
2023-09-02 21:52:22 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:52:22 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 7
2023-09-02 21:52:22 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:52:22 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 6
2023-09-02 21:52:22 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:52:22 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 2
2023-09-02 21:52:22 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:52:22 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 3
2023-09-02 21:52:22 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:52:22 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:52:22 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 4
2023-09-02 21:52:22 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 5
2023-09-02 21:52:22 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:52:22 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 0
2023-09-02 21:52:25 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11361', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=False, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=False, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=False, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-09-02 21:52:25 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-09-02 21:52:25 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-09-02 21:52:25 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-09-02 21:52:25 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-09-02 21:52:25 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-09-02 21:52:29 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-09-02 21:52:29 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-09-02 21:52:29 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-09-02 21:52:31 | INFO | root | load pretrained hubert
2023-09-02 21:52:38 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-09-02 21:52:42 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-09-02 21:52:49 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-09-02 21:52:49 | INFO | root | share the sematic adapter and textual encoder
2023-09-02 21:52:49 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-09-02 21:52:49 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-09-02 21:52:49 | INFO | fairseq_cli.train | model: S2TJoint
2023-09-02 21:52:49 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-09-02 21:52:49 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-09-02 21:52:49 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-09-02 21:52:49 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-02 21:52:49 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 21:52:49 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 21:52:49 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-02 21:53:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-09-02 21:53:04 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-09-02 21:53:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-09-02 21:53:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-02 21:53:05 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:53:05 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:53:05 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:53:05 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:53:05 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:53:05 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:53:05 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:53:05 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:53:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-02 21:53:05 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-09-02 21:53:05 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-09-02 21:53:05 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_last.pt
2023-09-02 21:53:07 | INFO | fairseq.trainer | load the task parameters
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
2023-09-02 21:53:24 | INFO | fairseq.optim.adam | using FusedAdam
2023-09-02 21:53:25 | INFO | fairseq.trainer | Loaded checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_last.pt (epoch 34 @ 50000 updates)
2023-09-02 21:53:25 | INFO | fairseq.trainer | loading train data for epoch 34
2023-09-02 21:53:25 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-02 21:53:25 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 21:53:25 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 21:53:28 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-02 21:53:30 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
2023-09-02 21:54:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 21:54:12 | INFO | fairseq.trainer | begin training epoch 34
2023-09-02 21:54:12 | INFO | fairseq_cli.train | Start iterating over samples
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
task_net module.module.task_net.layer_norm.weight True tensor(0.1748, device='cuda:3', dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor(-0.5479, device='cuda:3', dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([-3.5229e-01, -2.0850e-01, -1.5186e-01, -2.4109e-01,  3.2275e-01,
        -5.2539e-01, -2.3096e-01, -2.8760e-01,  2.6953e-01, -1.9165e-01,
         5.3125e-01, -3.5767e-01,  2.2144e-01, -6.7505e-02, -5.4492e-01,
         3.5107e-01, -1.9287e-01,  4.6631e-02, -1.1436e+00, -4.9805e-02,
        -3.2666e-01,  6.1670e-01,  1.7969e-01,  6.6162e-02, -6.3574e-01,
        -7.1655e-02,  2.1191e-01,  4.3945e-01, -3.4644e-01,  1.0187e-01,
         1.0732e+00,  1.0479e+00,  1.8984e+00,  8.7500e-01,  2.9639e-01,
        -2.1289e-01,  3.2617e-01, -3.9038e-01,  5.8301e-01, -2.5391e-01,
        -2.5781e-01, -3.1152e-01,  6.3965e-01,  3.2397e-01,  1.5479e-01,
         5.4932e-01, -8.3887e-01,  2.5659e-01, -4.6704e-01,  1.5771e-01,
         4.7803e-01,  5.8008e-01, -1.2317e-01, -1.2256e-01, -8.0811e-01,
        -2.7637e-01, -1.1536e-01, -6.1230e-01,  6.1035e-02, -1.9482e-01,
        -8.7891e-01, -1.3086e-01,  8.6426e-02,  6.1340e-02,  2.2070e-01,
         6.4160e-01, -3.0591e-01, -1.4551e-01,  1.7725e-01,  4.7510e-01,
        -3.0457e-02,  4.3823e-01,  6.9287e-01,  4.2432e-01, -1.8018e-01,
        -3.8672e-01,  7.6367e-01, -9.1797e-02,  7.8564e-01,  1.9385e-01,
         4.5312e-01,  9.4727e-02,  3.9893e-01,  4.0479e-01,  3.9746e-01,
         3.3105e-01, -7.7393e-02, -1.9788e-01,  2.6611e-01,  1.3721e-01,
         4.7290e-01, -1.1646e-01, -6.7529e-01, -4.1260e-02, -3.2202e-01,
         3.1763e-01, -2.8418e-01, -2.7148e-01, -6.1890e-02, -8.2666e-01,
        -1.2354e-01, -9.3262e-02,  7.5195e-02,  4.7021e-01, -5.9717e-01,
         7.1875e-01, -3.1592e-01, -5.7715e-01, -1.0742e-02, -5.5664e-02,
         3.8062e-01, -2.1179e-01, -6.9092e-02, -2.7832e-01,  1.6309e-01,
        -6.4697e-01, -1.7761e-02,  1.1475e-01, -2.6270e-01, -1.5186e-01,
        -1.2695e-01, -4.9341e-01, -6.6992e-01,  3.2471e-02, -1.7773e-01,
        -5.5322e-01, -3.6084e-01, -3.3838e-01, -1.0510e-01, -3.9648e-01,
         7.8125e-03, -6.1279e-02,  4.0356e-01,  2.5684e-01,  4.6387e-02,
        -6.1279e-01, -7.3340e-01, -1.4893e-02, -1.9287e-01,  1.2988e-01,
        -2.0618e-01,  5.8105e-01, -1.6125e-01, -2.0459e-01, -1.1511e-01,
         2.0020e-01,  3.0591e-01,  2.9492e-01,  6.8311e-01, -8.2227e-01,
        -1.7944e-01, -5.3711e-02, -4.1626e-02,  5.9570e-02, -1.9238e-01,
        -4.4629e-01, -3.1836e-01,  1.3647e-01,  2.3523e-01,  8.1055e-01,
        -2.8320e-02, -1.5161e-01, -6.7383e-02,  3.4668e-01, -5.6738e-01,
         1.4368e-01, -3.6035e-01,  2.1973e-02,  3.8379e-01, -1.6406e-01,
         2.8613e-01, -5.6274e-02,  2.3779e-01,  1.6724e-01, -2.8418e-01,
         2.9834e-01, -1.6016e-01, -2.1936e-01, -8.1201e-01,  5.7227e-01,
        -1.1694e-01, -6.1572e-01,  5.3125e-01, -2.8467e-01, -9.5410e-01,
        -2.4939e-01,  1.0254e+00,  3.2568e-01, -1.2207e-01, -9.6680e-02,
        -1.9434e-01,  7.7637e-02, -8.2812e-01, -1.3574e-01,  5.3809e-01,
         4.8877e-01, -5.2832e-01, -2.0020e-02, -4.1089e-01, -4.3896e-01,
         2.2559e-01, -3.1982e-01,  1.6992e-01,  1.0938e-01,  1.6357e-01,
        -2.9688e-01, -5.1514e-01,  1.5137e-02, -6.7444e-03, -3.7695e-01,
         7.7087e-02,  6.8555e-01, -8.7305e-01,  2.2510e-01, -5.1465e-01,
         5.5664e-02, -1.6431e-01,  1.1023e-01, -8.9844e-02,  7.7637e-02,
        -1.4722e-01,  1.8555e-01, -2.7319e-01, -2.4585e-01, -7.0557e-02,
         5.2783e-01, -9.8926e-01,  8.2031e-02, -3.2080e-01, -1.8445e-01,
         2.1362e-03, -7.0264e-01, -5.0439e-01,  1.5674e-01, -1.6565e-01,
        -1.6650e-01, -8.8074e-02,  1.5820e-01,  4.0381e-01, -8.2324e-01,
         1.2128e-01,  1.3245e-01,  2.5757e-01, -7.3828e-01, -4.3774e-01,
        -3.9941e-01,  2.7563e-01, -1.4062e-01, -1.2500e-01, -3.3105e-01,
         7.0264e-01, -1.2939e-01, -7.4316e-01, -7.8125e-03, -4.1406e-01,
        -4.7412e-01, -5.8936e-01, -3.3008e-01, -3.7109e-02,  4.2993e-01,
         1.8628e-01, -2.7490e-01,  4.3091e-02, -2.4878e-01,  5.3223e-02,
        -4.1016e-01, -3.0713e-01, -5.7568e-01,  5.0635e-01, -5.4443e-01,
        -3.3447e-01,  1.9910e-01,  1.3123e-01, -4.6631e-02,  2.8955e-01,
        -6.5674e-01,  9.9487e-03, -1.5161e-01,  2.1692e-01, -5.8594e-03,
         3.0615e-01,  4.9316e-01,  4.4141e-01,  6.9580e-02,  3.0981e-01,
        -4.6582e-01, -2.2607e-01,  4.8047e-01,  7.8857e-02, -1.0254e-01,
         2.5830e-01, -1.4917e-01,  4.1968e-01,  6.3330e-01,  7.3877e-01,
        -2.5391e-02,  2.0264e-01,  5.8398e-01, -5.4688e-02,  3.6914e-01,
        -6.6211e-01,  8.8184e-01, -3.7109e-01,  6.1182e-01,  1.4697e-01,
         6.8115e-01, -2.2461e-01, -3.5840e-01,  3.4668e-01,  6.8994e-01,
         5.2734e-02, -2.7246e-01,  1.9727e-01,  2.9199e-01,  6.2012e-02,
        -2.2949e-02,  2.3291e-01, -4.1650e-01,  2.8281e+00, -2.3901e-01,
        -8.8672e-01,  6.5430e-02,  3.0298e-01, -4.5996e-01, -6.1426e-01,
         4.8584e-02,  3.5718e-01,  1.1987e-01, -3.6133e-01, -1.4526e-01,
        -2.6123e-01, -4.8633e-01,  5.3564e-01,  4.3140e-01,  4.5947e-01,
         3.9746e-01, -2.0227e-01,  1.7114e-01,  6.0498e-01,  9.7656e-02,
        -8.3008e-03, -2.1484e-01,  2.8516e-01, -3.7231e-02, -2.4341e-01,
         6.6895e-01,  1.0645e-01,  5.3564e-01,  4.7290e-01,  4.8877e-01,
        -5.4639e-01, -6.0059e-01,  2.0581e-01,  1.4368e-01, -4.0625e-01,
         1.0791e-01,  2.8589e-01,  3.7842e-01,  4.9707e-01,  7.7832e-01,
        -2.9785e-02,  3.7500e-01,  5.6641e-02, -2.7881e-01,  6.9336e-02,
         1.4648e-01, -4.2969e-02, -1.6992e-01,  7.7515e-02,  4.1553e-01,
        -3.2715e-01, -3.2349e-01,  7.6758e-01,  3.8916e-01, -4.0356e-01,
        -1.7542e-01, -4.2944e-01,  1.1719e-01, -1.6016e-01, -5.6610e-02,
        -1.5381e-01, -3.0469e-01, -4.8291e-01,  1.1713e-01, -1.1650e+00,
         5.0732e-01, -1.0553e-01,  2.5488e-01,  1.6797e-01,  3.8037e-01,
        -8.6328e-01,  2.3340e-01, -2.3193e-01, -1.5076e-01, -7.6538e-02,
        -2.3901e-01, -3.4351e-01, -2.6050e-01,  3.7402e-01,  7.4805e-01,
         9.6802e-02,  4.7339e-01,  1.5430e-01,  2.9224e-01,  4.4971e-01,
         4.7412e-01, -4.2188e-01, -1.0425e-01,  2.3682e-02,  2.0898e-01,
        -5.0977e-01,  4.8755e-01,  4.4336e-01, -4.2969e-01,  4.8828e-03,
        -1.3342e-01, -4.9902e-01, -1.6687e-01, -4.1016e-02, -9.4971e-01,
         3.1494e-01,  3.9087e-01, -5.6396e-01,  2.8809e-02,  2.4377e-01,
         1.6931e-01,  5.2002e-01,  1.3721e-01,  1.1926e-01,  5.5078e-01,
         6.7871e-02,  5.9229e-01, -4.3701e-02,  9.3945e-01,  2.6660e-01,
        -2.9663e-02, -1.7480e-01,  1.2598e-01,  1.6504e-01,  1.0889e-01,
         2.1851e-01,  3.7231e-02, -4.8975e-01,  2.4805e-01,  4.1626e-01,
        -2.8442e-02,  2.5879e-01,  2.2192e-01, -1.3647e-01, -1.2207e-01,
        -1.9062e+00,  5.1953e-01, -6.5186e-02,  2.8931e-01, -3.3398e-01,
         7.4951e-02, -7.7979e-01, -1.0144e-01,  1.6777e+00, -2.8394e-01,
        -6.4014e-01, -2.4854e-01, -2.9102e-01, -1.4893e-01, -2.5391e-02,
        -3.3154e-01,  7.9199e-01, -5.6738e-01, -1.4368e-01, -4.8853e-01,
        -8.4961e-01, -1.6345e-01,  2.1777e-01, -1.4722e-01, -5.8154e-01,
         2.8809e-02, -1.0632e-01, -1.7908e-01,  3.7842e-01,  9.3359e-01,
        -3.7012e-01, -1.8262e-01, -7.9932e-01,  2.6709e-01, -6.5430e-01,
        -6.5283e-01, -4.5288e-01, -4.5483e-01,  3.3350e-01, -9.5215e-02,
        -3.5889e-01,  7.4902e-01,  1.9531e-01, -7.6416e-02,  3.9844e-01,
        -3.9429e-02,  6.0303e-02,  1.1279e-01,  4.8047e-01, -2.7393e-01,
        -9.0723e-01, -1.9727e-01, -2.7124e-01, -3.0713e-01,  7.2754e-02,
        -4.7754e-01, -4.1431e-01,  1.2109e-01,  4.5557e-01, -2.7563e-01,
        -4.8413e-01, -5.4102e-01], device='cuda:3', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor(0.7734, device='cuda:3', dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True task_net module.module.task_net.layer_norm.weight True tensor(0.0391, device='cuda:5', dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor(-0.1543, device='cuda:5', dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([ 1.1444e-02, -1.6272e-01, -1.1914e-01, -1.9531e-01,  8.9355e-02,
        -4.3945e-01, -3.4912e-01, -2.0264e-02,  9.6191e-02, -1.3171e-01,
         1.5344e-01, -1.0864e-02,  1.3574e-01, -1.1108e-01, -2.1411e-01,
         7.6416e-02,  6.9641e-02, -1.1963e-02, -5.3467e-02, -1.5894e-01,
         1.0327e-01,  3.0396e-01,  7.6416e-02,  1.3721e-01,  7.3242e-03,
         9.9487e-02,  2.8662e-01, -1.5625e-02, -4.5874e-01,  1.4441e-01,
         3.7744e-01,  2.6758e-01,  1.1816e+00,  1.4417e-01,  7.7148e-02,
         1.1719e-01,  1.2146e-01, -2.5757e-01,  3.1982e-01, -2.4146e-01,
        -2.5244e-01,  1.1023e-01,  3.3740e-01, -6.1279e-02,  1.3403e-01,
         3.2764e-01, -4.0161e-01, -1.5430e-01, -1.8726e-01, -1.3428e-02,
         1.5564e-01,  5.6787e-01, -5.2368e-02, -1.7896e-01, -2.7148e-01,
        -7.7271e-02,  4.3677e-01, -3.6523e-01, -2.0081e-01,  1.7822e-02,
        -5.8447e-01, -1.6431e-01,  1.7773e-01,  3.6011e-02,  3.6377e-02,
         2.7661e-01, -2.2812e-02, -2.6733e-01, -1.5991e-01,  1.5808e-01,
        -1.5210e-01,  1.0022e-01,  1.5674e-01,  3.8525e-01, -3.6279e-01,
        -3.4375e-01,  2.6270e-01, -1.3635e-01,  4.8950e-01,  7.9956e-02,
         2.2571e-01, -7.9102e-02, -9.7168e-02,  1.2451e-01,  5.1270e-02,
         2.4194e-01,  1.1401e-01,  1.2779e-02,  8.5449e-02,  1.4648e-01,
         1.3184e-01, -3.5669e-01, -1.0321e-01, -9.2407e-02,  1.6309e-01,
         5.2856e-02, -1.7517e-01, -1.7688e-01,  1.1218e-01, -4.6606e-01,
        -6.3110e-02,  1.4062e-01, -3.3496e-01,  1.0986e-01, -4.9390e-01,
         1.4795e-01,  9.4788e-02, -3.2104e-01, -2.0984e-01, -1.0547e-01,
        -4.2725e-02, -1.8579e-01, -3.8513e-02, -1.6431e-01,  4.0161e-02,
        -4.4189e-01, -1.1121e-01, -1.0767e-01, -5.9814e-03,  1.2817e-02,
        -1.7822e-02, -3.5254e-01, -1.9116e-01,  1.4771e-02, -2.2498e-01,
        -2.6489e-01,  5.5664e-02, -6.3843e-02, -6.8420e-02, -2.9639e-01,
        -1.4282e-01,  1.8005e-01,  1.6614e-01,  2.1887e-01, -2.9663e-01,
        -1.3892e-01, -2.2754e-01,  1.4355e-01, -6.2866e-02,  3.5474e-01,
        -2.7686e-01,  4.4629e-01, -1.4758e-01,  1.9580e-01, -1.5967e-01,
         1.3672e-02,  2.0825e-01,  4.5532e-02, -2.6093e-02, -2.1216e-01,
        -5.6519e-02,  3.9093e-02,  7.7209e-02, -1.9531e-02, -5.4565e-02,
        -5.3223e-02, -1.2476e-01,  1.0913e-01, -9.0820e-02,  2.9395e-01,
        -1.3684e-01, -1.0718e-01,  3.7842e-02, -9.7656e-04, -4.5898e-01,
         1.3025e-01, -5.0732e-01, -2.0312e-01,  1.0559e-01, -2.2827e-01,
        -6.7871e-02, -3.3105e-01,  4.9286e-02, -1.2469e-01, -2.8906e-01,
         2.9248e-01, -1.0498e-01, -2.9541e-01, -1.2012e-01,  2.3438e-01,
         6.9336e-02, -2.2571e-01,  1.0034e-01, -3.6865e-02, -7.0679e-02,
         4.0894e-02,  3.4863e-01,  4.5532e-01,  1.2744e-01,  1.9153e-01,
        -1.7615e-01, -2.1561e-02, -2.3193e-01, -4.9072e-02,  1.3940e-01,
         2.8467e-01, -1.4795e-01,  2.4268e-01, -2.3022e-01, -3.1885e-01,
         2.2949e-02, -3.1494e-01, -3.1677e-02,  2.4976e-01,  4.8828e-03,
        -1.3184e-01, -2.2778e-01, -8.7036e-02, -4.1595e-02, -4.2139e-01,
        -2.9761e-01,  2.5391e-01, -1.6174e-01, -7.7881e-02, -3.9642e-02,
         4.5166e-02,  1.4893e-02,  2.2873e-02,  8.5388e-02,  8.7769e-02,
         6.8359e-02,  3.0103e-01, -2.4219e-01, -1.4148e-01, -1.5161e-01,
         1.5796e-01, -4.2041e-01, -4.5044e-02, -1.5991e-01, -1.3342e-01,
        -3.4521e-01, -1.9336e-01, -2.3547e-01,  3.7354e-01,  1.3281e-01,
        -8.5510e-02,  6.2103e-02, -9.3506e-02,  2.9199e-01, -4.4360e-01,
         2.1948e-01,  5.1025e-02,  1.2573e-01, -2.2656e-01, -6.8115e-02,
        -1.5881e-01,  6.6895e-02, -6.0608e-02,  7.4463e-03, -3.1421e-01,
         3.0298e-01, -8.3984e-02, -3.3875e-03, -1.0828e-01, -4.3896e-01,
        -2.4536e-01, -1.7334e-02, -3.0029e-01,  1.3525e-01, -1.3562e-01,
        -2.0020e-01, -1.5430e-01,  3.2715e-01, -2.3242e-01, -9.6191e-02,
        -4.5605e-01, -2.9541e-01, -2.7771e-03, -7.0435e-02, -2.9126e-01,
         2.9980e-01,  1.0681e-01, -4.8523e-03,  1.6064e-01,  1.7371e-01,
        -4.0771e-01, -9.7351e-03, -1.1957e-01,  2.1484e-01,  4.4739e-02,
         1.8555e-01, -5.4230e-02,  1.9897e-01,  8.2520e-02,  1.7090e-02,
        -1.0522e-01, -6.7871e-02,  2.3682e-01,  2.9102e-01,  7.9224e-02,
        -6.1279e-02, -5.0049e-02,  2.0752e-01,  4.8767e-02,  1.4551e-01,
        -5.4474e-02,  7.2083e-02, -1.3330e-01, -3.6035e-01,  2.1387e-01,
        -7.5073e-02,  2.4866e-01, -6.5234e-01,  1.7883e-01, -4.1626e-02,
         5.6458e-02, -1.5234e-01, -1.0217e-01, -3.0811e-01,  3.9526e-01,
        -2.1680e-01, -1.3086e-01,  8.6914e-02,  9.7961e-02,  2.2888e-01,
        -9.4482e-02,  7.8552e-02,  1.9373e-01,  2.1992e+00, -9.8389e-02,
        -3.0151e-01, -3.4497e-01,  1.8225e-01, -1.1816e-01, -1.5820e-01,
         3.9490e-02,  1.2134e-01,  2.7271e-01,  1.3818e-01, -1.9531e-02,
        -3.8513e-02,  5.3711e-02,  5.8105e-01,  1.4697e-01,  2.7637e-01,
        -3.8208e-02,  1.2451e-02,  1.0266e-01,  2.2668e-01,  1.9702e-01,
        -2.0203e-02, -1.3184e-02, -6.5918e-03, -2.5732e-01, -8.6182e-02,
         2.3267e-01,  4.6448e-02,  6.2500e-02,  1.6406e-01, -3.5309e-02,
        -9.6802e-02, -1.5088e-01, -1.0443e-01,  1.5588e-01, -4.2017e-01,
         3.8147e-02,  1.1646e-01,  3.7079e-03,  2.6123e-01,  2.6367e-01,
        -4.6265e-02, -7.2266e-02,  3.8574e-01,  2.6099e-01, -2.8564e-02,
        -8.7219e-02, -1.8701e-01,  6.6650e-02,  1.9336e-01,  2.5366e-01,
        -3.5400e-01, -2.0728e-01,  1.4185e-01,  1.9006e-01,  5.8960e-02,
        -2.0935e-01, -4.4238e-01,  5.1697e-02, -4.1064e-01, -2.7359e-02,
        -7.7942e-02, -2.1973e-03, -2.4023e-01,  5.6274e-02, -4.7852e-01,
        -1.4783e-01,  5.2734e-02,  7.9224e-02,  8.3008e-02,  3.5449e-01,
        -4.7485e-02,  1.1987e-01, -1.2207e-01,  1.0742e-02, -9.2651e-02,
        -5.1147e-02, -1.5784e-01,  9.9121e-02,  1.0352e-01,  5.9863e-01,
         7.1472e-02,  1.8701e-01, -2.4976e-01,  1.2439e-01, -7.1289e-02,
         9.5825e-02, -6.7749e-03, -1.5002e-01,  5.1941e-02,  4.4678e-02,
        -2.0715e-01,  4.1162e-01,  3.5986e-01, -1.1493e-01, -2.8015e-02,
        -5.2032e-02, -1.3684e-01,  2.2461e-01,  1.6577e-01, -3.5986e-01,
         3.5645e-02,  2.2900e-01, -2.0337e-01, -5.0781e-02, -2.7881e-01,
         4.9866e-02,  1.0266e-01,  9.9304e-02,  2.1729e-02,  8.5327e-02,
         1.2744e-01,  3.3691e-01,  2.7588e-01,  4.1162e-01,  1.5796e-01,
        -1.5259e-01, -1.6821e-01,  2.6367e-02,  1.2207e-03, -1.3574e-01,
         2.3267e-01,  1.4429e-01, -4.0039e-02,  4.4287e-01,  1.2671e-01,
         4.2065e-01,  2.7515e-01,  2.5391e-02, -1.6528e-01, -2.2717e-01,
        -1.2422e+00, -2.4658e-02,  9.9121e-02, -5.4077e-02, -4.6509e-02,
         1.9287e-01, -1.7529e-01, -2.2144e-01,  1.0928e+00, -3.3496e-01,
        -2.6880e-01,  2.3193e-03, -1.0938e-01, -1.4087e-01,  7.1167e-02,
        -2.9297e-03,  1.1914e-01,  5.8014e-02, -2.2974e-01, -5.0049e-03,
        -1.5466e-01,  4.2358e-02, -6.3477e-02, -2.5635e-01, -3.5156e-01,
         2.0374e-01, -1.4136e-01, -3.4912e-02,  4.5654e-02,  1.1627e-01,
        -5.3558e-02, -2.8076e-02, -2.3779e-01,  1.9214e-01, -4.7852e-01,
        -3.0005e-01,  5.6152e-03, -1.4404e-02,  1.8079e-01, -1.5991e-02,
        -1.9434e-01, -1.6211e-01,  1.7847e-01, -6.9824e-02, -6.6650e-02,
         7.4585e-02,  8.9111e-02,  1.5918e-01,  3.8013e-01, -1.1353e-01,
        -2.8027e-01, -4.4434e-02, -1.2598e-01,  4.2603e-02,  1.8359e-01,
        -1.7175e-01,  3.6279e-01, -1.3428e-01, -4.6692e-02, -1.8701e-01,
         9.2163e-02, -5.9601e-02], device='cuda:5', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor(0.6250, device='cuda:5', dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True task_net module.module.task_net.layer_norm.weight True tensor(-0.3950, device='cuda:2', dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor(0.5537, device='cuda:2', dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([ 2.4817e-01, -1.3306e-01, -1.8457e-01, -2.3608e-01,  2.5659e-01,
         6.2305e-01, -2.3853e-01, -2.1606e-01, -4.4189e-02, -5.2979e-01,
         2.4438e-01, -1.6309e-01,  3.1470e-01,  7.3242e-03, -4.7925e-01,
         4.4067e-01,  3.2501e-03,  1.7822e-01, -2.7930e-01,  2.2461e-02,
         3.1885e-01,  5.6641e-01, -3.1299e-01,  1.8066e-01, -4.5215e-01,
         0.0000e+00, -1.6260e-01,  6.0059e-02, -1.9312e-01, -3.4839e-01,
         7.1729e-01,  3.8403e-01, -1.3828e+00,  2.3022e-01,  4.0479e-01,
         1.4368e-01,  5.8228e-02, -2.4146e-01,  3.7158e-01, -3.4839e-01,
         1.0217e-01, -7.2449e-02,  4.2627e-01,  6.9043e-01,  3.1177e-01,
         5.2783e-01, -5.5078e-01,  7.3608e-02, -2.0544e-01, -1.7163e-01,
         3.3838e-01,  1.1064e+00,  1.9067e-01,  4.9438e-02, -4.6313e-01,
        -2.1582e-01,  3.6182e-01, -5.1367e-01, -4.0625e-01, -1.8799e-01,
        -1.0068e+00, -2.7393e-01,  6.0059e-02, -4.4629e-01,  1.4062e-01,
         5.7617e-01, -2.2949e-01, -7.5635e-01, -3.2178e-01,  2.5488e-01,
        -1.8140e-01,  1.8347e-01, -2.9150e-01,  4.2188e-01, -3.3032e-01,
        -2.4414e-01,  2.1741e-01, -1.6992e-01,  7.9590e-01, -2.1313e-01,
         7.4414e-01, -6.1035e-02,  2.8491e-01, -1.2476e-01,  2.4341e-01,
         5.3857e-01, -2.6709e-01, -2.1790e-01, -7.5684e-02,  2.5195e-01,
         8.2129e-01, -3.2275e-01, -1.9897e-01,  2.3059e-01, -4.3896e-01,
         5.9033e-01, -3.8086e-01, -6.7529e-01,  4.4141e-01, -7.1289e-01,
         1.8774e-01,  1.6406e-01, -2.9590e-01,  3.4082e-01, -6.0254e-01,
         6.8115e-01,  3.6194e-02, -3.8794e-01,  3.3813e-01,  2.3779e-01,
         4.6997e-01,  4.4556e-01, -1.3721e-01,  1.6357e-02,  3.9331e-01,
        -8.7402e-01, -1.5137e-01, -4.9756e-01,  3.3154e-01,  2.7002e-01,
         1.1157e-01, -4.9829e-01, -1.8152e-01,  1.7249e-01, -2.1655e-01,
        -3.2715e-01,  2.7222e-01,  2.7979e-01, -2.0508e-02, -7.6416e-02,
         3.7549e-01, -6.3477e-02,  5.1807e-01,  1.0010e+00, -3.7622e-01,
        -5.4443e-01, -6.4062e-01, -4.5410e-02,  5.4590e-01,  1.2805e-01,
        -8.7891e-02,  1.2061e+00, -3.4131e-01,  2.5732e-01, -7.4609e-01,
         4.9292e-01,  1.6260e-01, -2.0752e-02,  1.2842e-01, -4.9023e-01,
        -1.9727e-01,  2.5146e-01,  2.5171e-01,  3.7500e-01,  1.2213e-01,
        -7.9224e-02, -1.3867e-01,  2.2729e-01, -1.7114e-01,  1.0254e+00,
         8.3496e-02, -5.6885e-01, -2.8198e-01,  1.1797e+00, -6.7627e-01,
         2.6855e-02,  2.1973e-01,  3.6816e-01,  5.2979e-01,  4.8975e-01,
        -2.5903e-01, -3.8745e-01,  3.4424e-01, -1.3635e-01,  3.5767e-01,
         3.2275e-01,  2.4609e-01,  3.6804e-02, -2.1924e-01,  5.7568e-01,
         2.1985e-01, -4.1162e-01,  2.3059e-01,  5.8105e-01,  3.2617e-01,
         7.7393e-02,  7.6807e-01,  6.1572e-01,  9.8584e-01,  4.3164e-01,
        -6.7236e-01, -3.4619e-01, -6.9873e-01,  1.2158e-01,  7.2998e-02,
         9.9512e-01,  1.2695e-01,  2.5879e-01,  7.5928e-02, -1.0469e+00,
         7.4463e-01, -2.8027e-01, -2.7686e-01,  2.2314e-01, -1.3342e-01,
        -2.8809e-01, -4.8389e-01, -1.6040e-01, -7.5989e-02, -9.4824e-01,
        -1.4648e-02,  4.1455e-01, -3.1689e-01,  4.7266e-01, -7.1350e-02,
        -5.9143e-02,  5.4932e-01,  2.9785e-02,  5.2856e-02,  2.7808e-01,
        -4.7412e-01,  3.4766e-01, -5.6494e-01, -1.0077e-01, -6.9580e-02,
        -2.4414e-04, -6.6992e-01,  4.7559e-01, -3.9136e-01,  2.6929e-01,
        -2.4512e-01, -6.6748e-01, -4.6387e-03,  3.8428e-01, -1.2207e-01,
         1.7334e-02,  2.0312e-01,  4.1357e-01,  4.8633e-01, -8.7891e-02,
         4.4214e-01,  3.5132e-01,  2.1558e-01, -2.5732e-01, -3.9478e-01,
        -1.1548e-01,  4.5471e-02,  1.6876e-02,  1.8463e-02,  4.8877e-01,
         7.4023e-01,  1.3184e-02, -3.5767e-01,  4.2505e-01,  1.1353e-01,
        -1.2408e-01, -1.8506e-01, -6.4111e-01, -4.5776e-01,  3.3875e-02,
         3.7109e-02, -5.1758e-02,  3.5693e-01,  3.3508e-02,  3.4277e-01,
        -6.1182e-01, -3.2861e-01, -1.7090e-02,  1.2207e-01, -5.7080e-01,
        -2.5586e-01,  6.9043e-01,  5.3613e-01,  8.7793e-01,  5.2832e-01,
        -7.7734e-01, -3.5156e-01, -4.0674e-01,  8.3496e-01, -2.1460e-01,
         4.7559e-01, -7.4463e-03, -6.4453e-02,  2.8809e-01,  5.6250e-01,
        -6.6504e-01, -4.0088e-01,  5.6738e-01,  3.4131e-01,  2.1973e-03,
        -6.3477e-03, -6.6650e-01,  2.4146e-01,  5.1270e-03, -9.9121e-02,
        -4.2383e-01, -1.0138e-01, -1.3208e-01,  1.0156e+00,  9.3457e-01,
        -3.0322e-01,  1.3574e-01, -4.0479e-01,  3.0127e-01, -6.5002e-02,
        -2.4121e-01,  6.8066e-01,  5.2612e-02, -1.5759e-01,  6.3867e-01,
         4.0332e-01, -1.0107e+00,  6.7749e-02,  4.4629e-01,  6.7017e-02,
         6.2451e-01, -2.3413e-01,  1.2207e-03, -1.0391e+00, -5.4785e-01,
        -1.0254e-01,  4.6875e-02,  1.0968e-01, -1.0723e+00, -4.7656e-01,
         4.7852e-02,  3.7134e-01,  9.6558e-02,  2.3438e-01, -6.8115e-02,
         1.5356e-01, -3.8770e-01, -3.3447e-02, -2.6221e-01,  1.0400e-01,
        -2.9028e-01, -1.3672e-02, -2.8809e-02,  2.4658e-01,  1.1670e-01,
        -1.8616e-01,  3.9258e-01, -6.3428e-01, -5.6152e-01, -1.1499e-01,
         3.6377e-01,  1.3940e-01, -7.0117e-01,  2.7661e-01,  2.1375e-01,
        -5.5762e-01, -1.2988e+00, -8.6426e-02, -1.5283e-01, -4.9756e-01,
         2.5635e-02,  6.1670e-01,  2.8076e-01, -3.4521e-01,  4.8145e-01,
        -4.9878e-01, -5.1855e-01, -2.5098e-01, -4.7461e-01, -1.7017e-01,
         8.5083e-02, -5.9912e-01, -4.4189e-01,  3.3539e-02,  5.1318e-01,
        -1.8127e-01, -5.3467e-01,  2.4628e-02,  6.8945e-01, -1.5039e-01,
        -4.5459e-01, -5.2832e-01,  3.3496e-01, -9.5215e-01,  1.2537e-01,
        -2.8979e-01, -2.8735e-01, -1.0918e+00,  3.1128e-01, -1.5942e-01,
        -1.1334e-01, -1.0730e-01,  3.3276e-01, -4.6191e-01,  4.2578e-01,
        -4.8853e-01, -1.9788e-01, -2.4561e-01,  1.7688e-01, -1.1060e-01,
        -2.2375e-01, -5.3925e-02,  2.1973e-03,  3.9795e-01, -6.7188e-01,
        -2.7710e-02,  3.4912e-01, -1.0962e-01,  5.1514e-01, -8.8135e-02,
        -6.7993e-02,  5.4871e-02, -5.4883e-01, -1.1584e-01,  3.2715e-01,
        -3.1689e-01,  6.0352e-01,  3.4131e-01,  1.1353e-01,  1.0211e-01,
         6.2744e-02, -1.3098e-01, -2.6538e-01,  4.6045e-01, -1.2021e+00,
         4.2310e-01,  6.0645e-01, -1.4111e-01, -3.1250e-02, -3.1104e-01,
         5.8167e-02, -1.6309e-01,  6.9336e-02,  1.5503e-01,  2.6184e-02,
         2.6099e-01,  1.3574e-01,  3.3081e-01,  2.6172e-01,  3.7500e-01,
         1.5991e-01, -2.6416e-01,  7.1924e-01,  3.9600e-01,  6.2927e-02,
         3.7915e-01, -2.4036e-01,  4.3152e-02,  1.0820e+00,  3.2910e-01,
         4.2114e-01,  5.0635e-01,  4.2993e-01, -2.7148e-01,  4.2456e-01,
         1.3203e+00,  2.3914e-01, -1.2268e-02,  2.3682e-02, -3.6035e-01,
        -5.2197e-01, -3.4106e-01, -1.8689e-01,  7.8125e-03, -3.6743e-02,
        -1.9946e-01, -2.5562e-01,  3.4473e-01, -1.9263e-01,  1.8066e-01,
         4.2480e-02,  1.9531e-03, -2.3291e-01, -5.5225e-01, -1.9165e-01,
        -8.2910e-01, -2.7802e-02, -1.8799e-02, -1.2646e-01, -6.2695e-01,
         2.1753e-01, -5.3589e-02, -4.0405e-01,  4.3506e-01,  4.7119e-02,
        -2.7637e-01,  4.4092e-01, -6.9629e-01,  1.1865e-01, -9.8145e-02,
        -6.5137e-01, -4.0405e-01,  2.4194e-01,  3.8989e-01, -9.0271e-02,
        -5.2881e-01,  4.9902e-01,  5.0195e-01, -2.0312e-01,  1.8433e-01,
         7.2119e-01, -2.6172e-01,  1.7017e-01,  1.0293e+00, -2.9053e-01,
        -9.2578e-01, -3.9697e-01, -2.0447e-02, -2.9053e-02,  6.2500e-01,
        -6.4795e-01,  5.1953e-01,  2.6978e-02,  1.0645e-01, -4.9536e-01,
         4.3091e-01, -4.2822e-01], device='cuda:2', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor(-1.3438, device='cuda:2', dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True task_net module.module.task_net.layer_norm.weight True tensor(-0.0527, device='cuda:1', dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor(-0.0439, device='cuda:1', dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([-1.1761e-01,  3.5614e-02, -7.5806e-02, -1.4380e-01,  7.7881e-02,
        -2.8320e-01, -2.4707e-01, -4.8828e-04,  5.8594e-02, -2.1265e-01,
         1.2610e-01,  7.8125e-03, -1.9409e-02,  4.2206e-02, -1.9360e-01,
         9.2773e-02, -2.1973e-03,  5.2917e-02, -1.8738e-01, -1.0120e-01,
         7.6233e-02,  2.4231e-01, -8.2275e-02,  7.5684e-03, -1.4868e-01,
        -5.4703e-03,  3.4424e-02,  8.0200e-02, -8.1665e-02, -1.1530e-01,
         2.8003e-01,  1.7322e-01,  8.3008e-01,  1.4294e-01,  1.1377e-01,
        -2.0386e-02,  4.7974e-02, -1.3757e-01,  1.2720e-01, -1.1511e-01,
        -1.4307e-01,  1.7807e-02,  1.0498e-01,  2.9175e-02,  8.2825e-02,
         2.2388e-01, -3.0200e-01, -5.8350e-02, -1.9324e-01,  6.9336e-02,
        -1.6846e-02,  3.8086e-01, -7.4585e-02, -9.0942e-02, -2.1289e-01,
        -2.9861e-02,  1.3969e-02, -2.5244e-01, -1.9592e-01, -7.1899e-02,
        -2.0984e-01, -7.3486e-02,  9.1431e-02,  3.2959e-02,  6.7139e-02,
         2.6221e-01,  8.6670e-02, -7.3792e-02, -1.5039e-01,  7.7515e-02,
        -1.5552e-01,  6.8604e-02,  8.9355e-02,  1.9531e-01, -2.2888e-01,
        -2.3340e-01,  2.0142e-02, -8.1421e-02,  2.1509e-01,  8.3618e-03,
         2.0996e-01,  6.4209e-02,  1.1475e-02, -1.0364e-01, -4.2969e-02,
         3.5693e-01, -7.9712e-02, -1.5576e-01, -3.1189e-02,  2.0044e-01,
         2.0667e-01, -2.1362e-02,  3.6743e-02,  5.6763e-02, -2.7405e-02,
        -2.0752e-02, -2.2083e-01, -3.6346e-02,  8.9355e-02, -4.9170e-01,
        -1.4954e-02,  5.5420e-02, -2.7856e-01,  1.1261e-01, -9.4727e-02,
         1.4062e-01,  1.9324e-01, -3.0078e-01, -7.4219e-02,  4.0771e-02,
        -6.4514e-02, -2.3914e-01,  2.2339e-02,  2.7344e-02,  1.1340e-01,
        -3.4644e-01, -1.5942e-01, -1.0449e-01,  8.8501e-03,  7.9346e-03,
         5.1758e-02, -1.4771e-01, -1.0693e-01, -2.5940e-02, -7.0435e-02,
        -2.6709e-01,  4.3701e-02, -9.8999e-02,  1.0132e-01, -2.0288e-01,
        -5.7373e-03,  1.3586e-01,  8.0261e-02,  2.5488e-01, -1.1841e-01,
        -1.7456e-01, -2.7393e-01,  4.4373e-02,  1.0767e-01,  3.8940e-02,
        -8.9233e-02,  2.4182e-01, -1.1133e-01,  1.3989e-01, -2.8882e-01,
         9.0332e-03,  6.4392e-02, -1.1267e-01,  3.6224e-02, -2.2437e-01,
        -1.2305e-01,  2.7588e-01, -4.3060e-02,  7.4707e-02, -1.1224e-01,
        -3.2654e-02,  6.1035e-04,  3.2349e-02, -3.9948e-02,  1.4539e-01,
        -1.3098e-01, -1.0358e-01,  1.4648e-02,  2.4585e-01, -2.6929e-01,
         9.1064e-02, -1.0156e-01, -1.7737e-01,  6.9092e-02,  4.3213e-02,
        -5.1147e-02, -2.5244e-01,  3.6133e-02, -2.8687e-02,  5.3223e-02,
         3.5889e-01, -5.2734e-02, -2.4463e-01, -1.7981e-01,  2.0068e-01,
         2.7124e-01, -9.7412e-02,  1.1304e-01,  1.8542e-01, -1.8347e-01,
        -6.7749e-02,  2.5635e-01,  2.7954e-01,  6.1768e-02,  9.3079e-02,
        -2.2241e-01, -1.0547e-01, -2.3608e-01, -5.8105e-02,  6.2622e-02,
         2.5708e-01, -4.3335e-02,  1.7334e-02, -5.3619e-02, -3.0078e-01,
         9.4421e-02, -1.7847e-01,  5.5969e-02,  1.6028e-01, -9.4971e-02,
        -9.8877e-03, -1.9214e-01, -1.3391e-01, -4.4922e-02, -2.3010e-01,
        -2.1655e-01,  1.7896e-01, -1.1847e-01,  1.5747e-01,  2.5848e-02,
         1.9714e-02,  1.5027e-01,  1.5747e-01,  1.7944e-02,  1.7285e-01,
        -5.4688e-02,  1.5063e-01, -1.7188e-01, -1.4709e-01, -1.3489e-01,
         1.0547e-01, -4.2041e-01,  2.8061e-02, -1.8677e-01, -4.0558e-02,
        -1.3940e-01, -1.7346e-01, -7.6599e-02,  2.2083e-01,  6.4270e-02,
         3.3386e-02,  1.4954e-01,  1.3623e-01,  1.7505e-01, -3.6816e-01,
         2.7246e-01, -4.5868e-02,  5.7617e-02, -1.0669e-01, -4.9072e-02,
        -1.0162e-01,  2.3865e-02, -4.4312e-02,  1.8005e-02, -1.0718e-01,
         2.0435e-01,  4.6143e-02, -6.1798e-02,  4.4312e-02, -2.6123e-01,
        -2.4976e-01, -4.2511e-02, -2.5220e-01, -2.2522e-02, -1.5503e-01,
        -3.3295e-02, -1.0962e-01,  2.7637e-01, -4.3701e-02, -7.9590e-02,
        -2.3755e-01, -1.0480e-01, -2.1143e-01,  9.1919e-02, -2.8735e-01,
         2.8229e-02,  1.3049e-01, -1.9531e-03,  1.5735e-01,  2.1454e-02,
        -1.8811e-01, -6.6895e-02, -2.2437e-01,  1.0895e-02, -2.0630e-02,
         2.3730e-01,  5.6671e-02,  1.2610e-01, -1.0864e-02,  1.3562e-01,
        -9.8633e-02,  2.3804e-02,  1.6919e-01,  1.5161e-01, -6.6467e-02,
         3.1494e-02, -3.0518e-02,  1.0577e-01,  1.2988e-01,  7.9102e-02,
         2.3743e-02,  2.7069e-02,  1.4038e-02, -1.5039e-01,  2.0215e-01,
        -1.5991e-02,  1.4160e-01, -2.6221e-01, -1.4145e-02, -1.0425e-01,
         5.2521e-02, -1.4307e-01, -2.2632e-01, -2.0459e-01,  1.2939e-01,
        -8.2275e-02, -1.8164e-01, -7.5195e-02,  2.2852e-01,  4.9377e-02,
        -1.1108e-01,  2.3438e-02,  5.0354e-02,  1.6465e+00, -7.6050e-02,
        -2.2876e-01, -1.7188e-01,  1.6357e-01, -2.3853e-01, -1.4990e-01,
        -1.1230e-02,  2.9114e-02, -8.1787e-03,  8.7891e-02, -5.6366e-02,
         7.3425e-02,  1.7310e-01,  3.2471e-01,  1.6504e-01,  1.8506e-01,
        -6.6772e-02,  4.0955e-02,  1.4038e-01,  6.6162e-02, -2.0798e-02,
        -2.2827e-02,  1.9678e-01,  8.2397e-03, -7.4341e-02, -8.2581e-02,
         1.8396e-01,  6.4453e-02, -5.1758e-02,  2.8149e-01,  4.1626e-02,
        -1.4038e-02, -1.0522e-01, -6.0852e-02, -1.0211e-01, -1.7407e-01,
        -1.6541e-02,  2.6465e-01,  4.7241e-02,  3.6377e-02,  1.4966e-01,
         3.8086e-02, -1.1804e-01,  2.9907e-02,  3.8086e-02, -1.6064e-01,
        -4.1504e-03, -8.8989e-02, -7.9224e-02,  2.1252e-01, -5.6396e-02,
        -1.1475e-01, -4.4800e-02,  2.8784e-01,  1.0718e-01, -4.3823e-02,
        -1.1560e-01, -1.1487e-01,  5.6763e-02, -2.4390e-01, -5.3589e-02,
        -1.1810e-01,  1.5295e-01, -2.7734e-01,  1.0413e-01, -2.6050e-01,
         1.5186e-01, -1.2573e-02, -2.1973e-03, -1.2262e-01,  2.1899e-01,
        -1.7908e-01,  7.9956e-03, -4.2725e-02, -9.1431e-02, -6.9702e-02,
        -2.2839e-01, -2.0593e-01,  1.1401e-01,  2.1008e-01,  4.0332e-01,
        -5.1208e-02,  1.1513e-02, -1.4453e-01,  2.0581e-01, -9.2041e-02,
        -1.6113e-02, -7.1899e-02, -1.7188e-01, -8.1665e-02, -1.3647e-01,
        -7.4280e-02,  3.8550e-01,  2.3999e-01, -5.7983e-03,  2.6270e-01,
        -2.0264e-02, -3.1372e-01, -7.9773e-02,  6.8481e-02, -3.1689e-01,
         1.6833e-01,  1.6846e-01, -1.5186e-01, -1.3623e-01, -1.9116e-01,
        -5.9082e-02,  2.3315e-01,  9.6924e-02,  1.5526e-02,  6.7444e-02,
         6.1981e-02,  2.5293e-01,  1.6296e-01,  3.7988e-01,  2.2339e-02,
         8.8501e-03, -1.5076e-01,  6.6040e-02,  2.0447e-01,  9.7046e-03,
         1.1438e-01,  2.4170e-02, -1.4465e-02,  4.1650e-01,  2.1765e-01,
         1.6699e-01,  3.4033e-01,  1.5771e-01, -8.4595e-02, -2.9492e-01,
        -7.9883e-01,  7.3730e-02, -9.8877e-03,  2.4399e-02, -8.0811e-02,
        -7.0435e-02, -8.6365e-02, -2.1631e-01,  7.5195e-01, -1.0352e-01,
        -6.3782e-02,  1.1664e-01, -4.0527e-02, -1.3440e-01, -4.2969e-02,
        -1.6736e-01, -3.8025e-02,  9.0332e-03, -2.0142e-01,  8.2520e-02,
        -1.6956e-01,  4.5898e-02,  2.1240e-02, -8.5815e-02, -1.9714e-01,
         1.3721e-01, -1.0852e-01, -1.4856e-01, -7.8125e-03,  9.0271e-02,
        -8.1726e-02,  1.3794e-01, -1.4551e-01,  1.3281e-01, -2.6318e-01,
        -2.3682e-01, -1.0535e-01,  2.5879e-02,  2.4805e-01, -3.2196e-03,
        -2.3755e-01,  3.6621e-03,  6.0791e-02, -1.4923e-02,  1.2207e-02,
         5.6030e-02,  3.1860e-02,  3.3130e-01,  2.1924e-01, -7.5073e-02,
        -3.8525e-01, -1.0474e-01, -1.4233e-01, -2.3682e-02,  1.6626e-01,
        -3.8818e-02,  1.0901e-01,  2.6855e-02, -3.3813e-02,  9.7656e-03,
         3.6926e-03, -8.8135e-02], device='cuda:1', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor(0.3652, device='cuda:1', dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True task_net module.module.task_net.layer_norm.weight True tensor(0.1152, device='cuda:4', dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor(-0.3574, device='cuda:4', dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([-3.6719e-01,  7.3242e-04, -1.0815e-01, -3.1299e-01,  4.3213e-01,
        -4.5312e-01, -5.6787e-01, -4.8779e-01,  1.4490e-01, -3.4912e-01,
        -1.2756e-01, -1.0260e-01,  3.9404e-01,  9.5154e-02, -1.9409e-01,
         1.0785e-01,  2.1912e-01,  7.3364e-02, -6.3232e-01, -8.7891e-03,
         3.7183e-01,  5.0391e-01, -1.9629e-01, -1.5381e-02, -1.8652e-01,
        -2.7832e-01, -1.8872e-01,  1.9531e-01,  7.7881e-02, -6.6711e-02,
         5.2051e-01,  4.5630e-01,  1.0742e+00,  3.5156e-01,  9.0332e-02,
        -5.4077e-02, -1.2720e-01, -8.3057e-01,  5.8838e-01, -7.5928e-02,
        -2.0410e-01, -4.0649e-02,  3.4180e-01,  3.0811e-01, -5.2734e-02,
         4.6777e-01, -3.1030e-01,  8.4961e-02, -2.4304e-01, -1.6846e-01,
         4.8584e-01,  8.0664e-01,  2.3804e-03, -7.4316e-01, -6.0596e-01,
        -1.2231e-01,  4.1772e-01, -5.0049e-01, -2.3669e-01, -1.5039e-01,
        -8.3545e-01, -3.6523e-01,  2.2705e-01, -9.5215e-02,  2.0874e-01,
         4.3799e-01, -1.7029e-02, -8.3740e-02,  4.3762e-02,  3.5400e-02,
        -1.8286e-01,  2.8516e-01, -7.1289e-02,  5.6738e-01, -4.7876e-01,
        -7.7539e-01,  1.2329e-01, -2.9736e-01,  9.8730e-01,  1.6052e-01,
         2.7075e-01,  6.4270e-02,  1.4648e-03, -3.8086e-02,  1.7944e-01,
         4.8682e-01, -2.0178e-01,  1.2219e-01,  8.7891e-03, -3.1934e-01,
         3.3203e-01, -1.4282e-01, -7.5378e-03, -3.2568e-01,  2.1338e-01,
         2.1240e-01, -2.2827e-01, -3.9111e-01,  4.1650e-01, -9.1699e-01,
        -7.4219e-02, -4.7168e-01, -2.3633e-01, -3.7109e-02, -7.7637e-01,
         3.7549e-01, -3.1738e-02, -4.8584e-01, -7.6172e-02, -4.4751e-01,
         7.0996e-01,  4.7363e-02, -2.6074e-01,  2.8076e-01,  3.7305e-01,
        -6.0596e-01, -3.4839e-01, -3.2324e-01,  5.2490e-02,  1.2012e-01,
        -3.8672e-01, -2.2998e-01, -4.0186e-01, -1.0608e-01, -2.0361e-01,
        -7.7246e-01,  2.3853e-01, -1.3794e-01, -1.0278e-01, -3.5742e-01,
        -2.6221e-01, -1.4758e-01, -5.9204e-02, -5.7251e-02, -3.7744e-01,
        -2.4634e-01, -4.5459e-01, -1.1499e-01,  4.0137e-01,  3.6328e-01,
        -2.3218e-01,  8.5986e-01, -3.3032e-01,  1.6797e-01, -2.1924e-01,
         3.7012e-01, -8.0566e-03, -2.1729e-02, -1.5552e-01, -5.0537e-01,
        -1.2329e-01,  4.2938e-02,  4.2090e-01,  6.7676e-01, -7.8076e-01,
         1.4331e-01,  5.1221e-01,  1.2671e-01, -3.4717e-01,  5.0488e-01,
        -1.0303e-01, -4.5215e-01,  4.9951e-01, -2.8320e-02, -3.7109e-01,
         2.4634e-01,  6.5918e-02, -3.2349e-01, -2.5781e-01, -7.9102e-02,
        -6.2500e-02, -6.2012e-01,  2.7612e-01, -3.6865e-02,  1.2109e-01,
         4.9438e-01, -1.0986e-01, -1.1896e-01, -5.3076e-01,  5.8838e-01,
        -7.1045e-02, -3.3984e-01,  3.6621e-02,  5.1611e-01,  4.8340e-02,
         1.2488e-01,  4.1772e-01,  9.9609e-01,  1.0742e-01, -1.4001e-01,
        -6.2695e-01,  5.4932e-03, -5.2539e-01, -2.3047e-01,  5.9387e-02,
         5.8203e-01, -4.5239e-01,  1.5967e-01, -4.3115e-01, -5.2832e-01,
         1.9141e-01, -6.8457e-01,  8.2764e-02,  5.0568e-02, -3.1860e-01,
        -3.8501e-01, -5.1318e-01,  4.7168e-01, -2.9761e-01, -3.8672e-01,
        -6.0107e-01,  2.8174e-01, -2.1875e-01, -2.4426e-01, -1.3013e-01,
         3.6377e-01,  5.2783e-01,  4.6680e-01, -2.5293e-01, -1.2561e-01,
        -3.7769e-01,  3.6914e-01, -9.0430e-01, -8.9874e-03, -2.6611e-01,
         1.5491e-01, -6.6992e-01,  9.1919e-02, -2.3853e-01,  1.0034e-01,
        -4.1699e-01, -4.5752e-01, -3.2593e-02,  5.7861e-01, -3.2654e-03,
        -1.4966e-01,  2.2717e-01, -1.7896e-01,  6.4355e-01, -3.5474e-01,
         3.3105e-01,  3.9111e-01, -1.3574e-01, -2.8052e-01, -7.5635e-01,
        -1.2964e-01, -5.4749e-02, -1.1279e-01,  8.3740e-02, -3.2617e-01,
         3.7305e-01,  1.7981e-01, -2.2534e-01,  1.3330e-01, -7.3340e-01,
        -4.5264e-01, -4.3848e-01, -2.4512e-01,  1.2207e-04, -1.9189e-01,
        -1.0065e-01,  2.1680e-01,  3.3740e-01, -1.0999e-01,  1.1829e-01,
        -4.3945e-01, -4.7485e-01, -1.4258e-01,  2.6123e-01, -5.2002e-01,
         2.6758e-01,  4.8291e-01,  2.7344e-01,  6.2842e-01,  3.7817e-01,
        -5.3467e-01, -4.5898e-01, -1.7920e-01, -3.8757e-03,  1.7310e-01,
         8.7158e-01,  6.3623e-01,  3.2544e-01,  3.2227e-02,  2.8320e-01,
        -3.6230e-01,  1.1914e-01,  2.9175e-01,  5.7068e-03, -7.8369e-02,
         4.5312e-01,  6.8848e-02,  3.4033e-01,  4.5996e-01, -1.8604e-01,
         1.8286e-01,  1.8774e-01,  1.0376e-01, -2.6562e-01,  7.2070e-01,
        -3.7256e-01,  4.0039e-02, -5.0146e-01,  4.2041e-01, -1.3782e-01,
         9.1736e-02,  1.5723e-01,  1.5710e-01, -4.8999e-01,  6.3672e-01,
        -6.9580e-02, -6.6357e-01,  3.6670e-01,  1.4087e-01, -6.3416e-02,
        -1.5869e-02,  3.1104e-01, -4.9707e-01,  1.6836e+00,  6.0547e-02,
        -7.5195e-02, -1.4526e-01,  3.4375e-01, -5.7373e-01, -5.5957e-01,
         2.7441e-01, -1.1060e-01, -8.0688e-02,  2.6465e-01, -3.6804e-02,
        -2.4805e-01, -5.9473e-01,  5.6152e-01,  4.7333e-02, -9.1187e-02,
        -3.9520e-02, -8.0566e-02, -4.9365e-01,  4.8828e-04,  2.7441e-01,
         2.3621e-01, -2.8320e-02, -9.2773e-02, -3.4375e-01,  1.6235e-01,
         2.9370e-01,  1.1285e-01,  1.6309e-01,  3.9917e-01,  3.4009e-01,
        -3.9868e-01, -8.1055e-01,  1.7151e-01,  3.0151e-01, -7.3438e-01,
         1.5588e-01,  7.6123e-01, -1.9934e-01,  1.8799e-01,  4.3604e-01,
        -3.4448e-01, -4.5508e-01, -8.3984e-02,  1.4404e-01,  3.9966e-01,
        -4.0405e-01, -1.6260e-01,  1.1865e-01,  4.3408e-01,  4.1406e-01,
        -1.1652e-01,  1.1609e-01,  1.2708e-01,  4.3701e-01,  1.9971e-01,
         4.1016e-02, -5.3320e-01, -7.6904e-02, -6.0107e-01,  2.8687e-02,
         1.7200e-01, -3.5889e-02, -1.1758e+00,  2.9785e-02, -6.9434e-01,
         9.6924e-02,  2.6050e-01,  4.7119e-01, -5.1904e-01,  4.1943e-01,
        -4.1309e-01,  6.6895e-01, -1.3879e-01,  2.2949e-01,  4.0283e-02,
         5.3711e-03, -2.7441e-01,  6.9092e-02,  4.1260e-01,  6.0938e-01,
        -4.0234e-01,  1.8494e-02, -2.9761e-01,  1.7090e-02,  1.1914e-01,
         4.4434e-01, -4.7119e-02,  8.9600e-02,  3.0273e-02,  1.8115e-01,
        -5.3955e-02,  7.0459e-01, -9.5398e-02,  1.1401e-01,  5.8398e-01,
         6.9580e-03,  3.9062e-03,  3.1689e-01,  4.8315e-01, -3.8208e-01,
         1.4783e-01,  6.6162e-01, -1.5918e-01,  2.4512e-01,  4.1992e-02,
         2.1533e-01,  8.3008e-03, -1.0229e-01, -6.4941e-02,  2.4536e-01,
        -7.3242e-02,  4.3530e-01,  3.3301e-01,  4.6704e-01, -1.0620e-01,
         3.4448e-01, -1.2646e-01,  5.8105e-01,  9.2627e-01,  5.3955e-02,
         2.8247e-01,  2.9980e-01,  1.9385e-01,  5.8350e-01,  1.8457e-01,
         5.5371e-01,  6.6846e-01,  1.0205e-01, -1.1230e-01, -1.8530e-01,
        -9.0234e-01,  1.2476e-01,  1.2964e-01,  5.4541e-01,  2.6367e-01,
        -2.5970e-02, -3.4863e-01, -5.6543e-01,  1.1016e+00, -3.1958e-01,
        -2.4231e-01, -7.8857e-02,  6.5918e-02, -1.5271e-01, -2.6855e-01,
        -2.7100e-01, -1.1981e-01, -1.9678e-01, -4.1284e-01, -1.1652e-01,
        -3.7354e-01, -3.3716e-01,  5.4443e-01,  3.3447e-02, -5.0049e-02,
        -3.0273e-01, -3.8867e-01, -2.7417e-01, -2.6465e-01,  2.9663e-01,
        -4.0771e-01,  9.6680e-02, -6.9824e-01, -3.3984e-01, -4.6313e-01,
        -5.6787e-01, -4.2529e-01, -1.5125e-01,  3.4766e-01, -1.9617e-01,
        -4.0820e-01,  3.6426e-01,  4.6021e-01, -3.3447e-01,  1.0107e-01,
         4.8389e-01, -4.5068e-01,  3.2373e-01,  7.3047e-01, -1.7944e-01,
        -5.4102e-01, -1.2854e-01, -3.4497e-01,  1.3745e-01,  4.5117e-01,
         6.4453e-02, -3.6670e-01,  4.8901e-01,  1.6663e-01, -3.2788e-01,
         1.5369e-01, -4.3848e-01], device='cuda:4', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor(0.6172, device='cuda:4', dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True task_net module.module.task_net.layer_norm.weight True tensor(0.2090, device='cuda:6', dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor(-0.7109, device='cuda:6', dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([ 2.1118e-01,  3.6719e-01, -9.4531e-01, -7.3828e-01,  1.0684e+00,
        -1.4648e+00, -1.3271e+00, -9.0430e-01,  1.6174e-01, -1.0205e+00,
         1.7461e+00, -1.1230e+00, -2.5049e-01,  6.7139e-02, -9.6387e-01,
        -6.0303e-01,  4.7314e-01, -3.7842e-02, -1.5049e+00, -1.6611e+00,
         1.4092e+00,  1.2695e+00, -8.3350e-01,  2.3657e-01, -1.8887e+00,
        -5.2441e-01, -3.6450e-01,  9.9609e-02, -1.7227e+00, -4.0137e-01,
         8.4473e-01,  9.8730e-01,  5.6406e+00,  9.9609e-01, -2.0898e-01,
        -3.3105e-01,  1.8518e-01, -4.6338e-01,  1.1611e+00, -4.9658e-01,
        -7.8809e-01, -4.2285e-01,  7.7734e-01,  6.4746e-01,  1.9141e-01,
         2.9688e-01, -2.1426e+00, -2.3535e-01, -3.6816e-01,  5.5615e-01,
         1.3682e+00,  2.4844e+00, -6.7578e-01, -6.6113e-01, -1.3555e+00,
        -2.6562e-01,  2.3301e+00, -1.0830e+00, -9.9512e-01, -5.4492e-01,
        -1.0664e+00,  1.2305e-01,  9.8438e-01,  1.2402e-01,  1.2998e+00,
         2.4551e+00, -4.5703e-01, -1.0459e+00, -4.9268e-01,  2.0801e+00,
        -1.1113e+00,  1.0059e+00, -1.9336e-01,  1.7627e+00, -9.3164e-01,
        -1.5117e+00,  3.2935e-01,  2.3547e-01,  3.1641e-01,  3.1323e-01,
         8.2861e-01,  1.0156e-01,  2.6270e-01, -2.1045e-01,  5.2930e-01,
         4.8730e-01,  2.0752e-01,  2.3438e-01,  7.0996e-01,  4.5801e-01,
         1.9717e+00, -7.9590e-01, -1.1064e+00, -1.0547e-01,  4.6680e-01,
         1.0342e+00, -1.2979e+00, -1.8564e+00,  7.1143e-01, -2.0684e+00,
        -8.9648e-01,  9.5898e-01, -1.4141e+00, -5.4395e-01, -1.0479e+00,
         8.2275e-01,  1.4199e+00, -2.4805e+00, -5.5469e-01, -2.7539e-01,
         4.1113e-01, -6.7188e-01, -5.5273e-01, -9.7461e-01,  1.2344e+00,
        -2.6582e+00, -6.6113e-01, -3.7939e-01,  2.2461e-01, -5.8008e-01,
         3.6719e-01, -1.4180e+00, -4.9414e-01,  8.3203e-01, -1.2334e+00,
        -1.4023e+00,  1.4102e+00,  3.2959e-01,  4.2383e-01, -1.0645e+00,
        -3.6768e-01, -1.4648e-02,  1.8408e+00,  1.9150e+00, -9.4531e-01,
        -1.1240e+00, -1.7881e+00, -1.1133e-01, -4.8340e-02,  6.0547e-02,
        -8.8379e-01,  2.4258e+00, -6.4697e-01,  4.3164e-01, -1.2637e+00,
         9.4922e-01, -3.0713e-01, -1.1719e-02,  4.9487e-01, -8.5107e-01,
         1.8555e-02,  7.8125e-02,  6.5820e-01, -7.2266e-02, -1.1729e+00,
        -3.3691e-02, -4.9707e-01,  1.0273e+00,  4.9512e-01,  1.6289e+00,
        -1.0537e+00, -2.0142e-01,  3.0859e-01,  5.6445e-01, -1.9902e+00,
         1.3789e+00, -5.8984e-01, -6.1816e-01, -3.0371e-01,  3.3105e-01,
         2.6855e-01, -4.8438e-01, -6.7871e-02,  8.6328e-01, -4.4287e-01,
         1.3203e+00, -4.3164e-01, -1.0479e+00, -1.2178e+00,  8.7500e-01,
         4.7607e-01, -7.3682e-01,  3.7695e-01,  1.2148e+00, -9.1406e-01,
        -4.1846e-01,  1.4688e+00,  1.3711e+00,  1.2285e+00, -6.5527e-01,
        -2.0508e+00, -1.1094e+00, -4.4116e-01, -1.9590e+00,  1.4087e-01,
         2.1152e+00, -3.5938e-01,  1.1914e-01, -7.9883e-01, -5.5078e-01,
         1.1758e+00, -4.7510e-01,  1.2451e-01,  2.4902e-01, -1.4648e-01,
        -1.0488e+00, -7.9248e-01, -7.6172e-01, -7.3633e-01, -1.9336e+00,
        -5.7617e-01,  1.0859e+00, -9.0869e-01,  1.9922e-01, -4.3945e-02,
         3.2812e-01,  2.0972e-01, -9.0820e-01, -9.1113e-01,  1.2734e+00,
         4.7656e-01,  4.5117e-01, -1.2246e+00,  1.1914e-01, -1.0195e+00,
         2.0276e-01, -2.2168e+00,  1.9531e-02, -1.9395e+00, -8.2812e-01,
        -1.1875e+00, -2.0645e+00, -1.0703e+00,  3.9258e-01,  6.1572e-01,
        -1.2073e-01,  1.6602e-01, -3.3203e-01,  3.8672e-01, -1.5361e+00,
         1.3477e+00,  3.2227e-02,  1.7444e-01, -3.3051e-02, -3.8574e-01,
         4.1528e-01, -6.6553e-01, -6.7529e-01,  6.6211e-01, -1.4258e-01,
         1.9277e+00,  4.7314e-01, -1.7188e+00,  1.7578e-01, -1.6963e+00,
        -1.7764e+00,  5.4443e-01, -1.9971e+00,  2.7734e-01, -1.8372e-01,
         6.4404e-01, -8.8281e-01,  4.4165e-01, -4.9805e-01, -1.7383e-01,
        -1.8125e+00, -7.7881e-02, -1.1445e+00,  4.4824e-01, -1.9473e+00,
         1.7734e+00,  1.7559e+00,  2.8833e-01,  1.1758e+00,  5.3369e-01,
        -6.4941e-02, -8.1152e-01, -1.6553e-01,  7.9053e-01,  5.8887e-01,
         1.4502e+00,  1.3301e+00,  7.4170e-01,  1.9336e-01,  6.5234e-01,
         5.5078e-01,  7.6660e-01,  8.5840e-01,  1.0479e+00, -1.8066e-01,
         1.5527e-01,  3.8770e-01, -1.5894e-01,  2.0093e-01,  1.3369e+00,
        -9.3555e-01,  8.5254e-01,  3.3325e-01, -2.5938e+00, -1.6406e-01,
        -3.7158e-01,  9.4922e-01, -2.7656e+00, -3.2324e-01, -7.2363e-01,
        -8.5059e-01, -1.2891e+00, -9.8633e-01, -3.0000e+00,  8.7891e-01,
        -5.5518e-01, -1.4141e+00, -6.0156e-01,  2.2839e-01, -2.6318e-01,
         5.9082e-01,  9.2920e-01,  7.1094e-01,  8.9219e+00, -7.8320e-01,
        -1.4160e+00, -7.1680e-01,  5.2393e-01, -5.5078e-01, -1.0781e+00,
         7.7783e-01,  6.3184e-01, -1.2683e-01,  1.1172e+00,  2.4243e-01,
         6.1230e-01, -1.8398e+00,  1.4053e+00,  6.2305e-01,  1.5879e+00,
         5.2051e-01, -8.7402e-01,  7.8223e-01,  6.1133e-01,  6.4307e-01,
         1.4141e+00, -1.1992e+00, -7.9004e-01, -2.5879e-01, -7.0264e-01,
         1.4961e+00,  7.3242e-02, -2.2461e-01,  1.1240e+00,  1.6387e+00,
        -1.7441e+00, -1.6553e+00,  7.5977e-01, -4.6802e-01, -1.2666e+00,
        -6.0059e-01, -1.2012e-01,  3.1812e-01, -2.1484e-01,  1.8369e+00,
         7.7148e-02, -6.7480e-01,  1.1758e+00,  1.7246e+00, -6.3916e-01,
         4.2969e-02, -1.4648e-01,  3.6133e-01,  1.0693e+00,  4.5215e-01,
        -4.4800e-01, -6.6357e-01,  7.7637e-01,  1.8066e+00, -5.1758e-02,
        -9.9414e-01, -8.2568e-01, -7.5195e-02, -2.5020e+00, -6.9275e-02,
        -2.2119e-01,  6.8701e-01, -2.7461e+00,  6.5527e-01, -2.0449e+00,
         9.3652e-01, -7.4316e-01, -4.6265e-01, -3.6719e-01,  1.0938e+00,
        -1.8350e+00,  9.1406e-01, -1.1357e+00,  2.5879e-01, -8.9697e-01,
        -1.3184e-02, -6.3281e-01,  5.1953e-01,  8.3984e-02,  2.7109e+00,
         7.2510e-02,  1.1060e-01, -6.8115e-01,  1.4258e+00, -5.8398e-01,
        -7.0703e-01,  1.8555e-01, -3.3789e-01,  1.6577e-01,  1.0479e+00,
        -1.5996e+00,  1.9600e+00,  1.5762e+00, -3.3496e-01, -2.5391e-02,
         1.2024e-01, -4.2432e-01, -2.5781e-01,  2.3477e+00, -1.0859e+00,
         4.0576e-01,  1.9668e+00, -2.7734e-01, -1.0674e+00, -1.2324e+00,
        -5.8887e-01,  2.0547e+00,  1.4453e+00, -7.7832e-01,  6.0547e-01,
        -1.2432e+00,  1.9590e+00,  8.9355e-01,  2.3418e+00,  1.3906e+00,
        -1.6221e+00,  3.0078e-01,  5.2051e-01,  3.6230e-01,  3.2275e-01,
         1.0928e+00,  3.3740e-01, -4.3066e-01,  1.9268e+00,  2.3594e+00,
         1.5312e+00,  5.7568e-01,  1.1797e+00,  1.9922e-01, -1.4375e+00,
        -4.5391e+00,  1.1016e+00, -2.4780e-01, -5.9570e-01, -3.4961e-01,
        -3.1763e-01, -2.2285e+00, -6.8115e-01,  4.2266e+00, -6.8555e-01,
        -7.2754e-02,  2.8809e-02,  2.0508e-01, -1.0312e+00, -4.3677e-01,
        -2.1558e-01,  1.2910e+00, -3.9160e-01, -1.3125e+00,  1.3750e+00,
        -1.3115e+00, -1.2578e+00,  2.5879e-01, -1.1475e+00, -2.1855e+00,
         1.0205e+00, -3.9868e-01, -8.0273e-01, -3.3423e-01, -6.8311e-01,
        -2.9248e-01,  1.9336e-01, -1.3008e+00,  8.8477e-01, -1.6787e+00,
        -7.7246e-01, -2.4512e-01, -1.1768e+00,  1.6904e+00,  5.9814e-03,
        -2.3477e+00,  1.2500e+00,  1.7041e+00, -1.0195e+00,  6.8652e-01,
         1.8281e+00, -2.6953e-01,  1.7031e+00,  1.1914e+00, -9.8633e-02,
        -1.8701e+00, -1.7188e-01, -1.1592e+00,  2.6904e-01,  9.9707e-01,
        -8.0664e-01,  6.7725e-01,  9.1064e-02,  2.9736e-01, -2.1362e-01,
         3.2129e-01, -7.1289e-01], device='cuda:6', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor(2.0391, device='cuda:6', dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([-6.2561e-03, -2.0142e-03,  9.0027e-04,  2.0410e-01,  1.7639e-02,
         4.2236e-02,  5.4871e-02,  4.2725e-03, -7.8735e-03,  4.1077e-02,
        -1.1322e-02,  2.5330e-02, -2.0386e-02,  4.8157e-02,  4.9744e-03,
        -2.0599e-03,  5.1147e-02,  5.1697e-02, -1.0010e-02,  9.3689e-03,
        -3.9253e-03, -1.0742e-02,  4.1321e-02,  2.1436e-01, -1.2436e-03,
         1.9849e-01,  3.4180e-02, -8.4351e-02, -2.0813e-02, -4.2236e-02,
         6.6772e-02,  6.1707e-02,  8.9783e-02,  3.6621e-03, -1.7822e-02,
         1.2772e-02,  3.3478e-02, -2.1057e-02, -1.7548e-02, -8.3008e-03,
         1.0645e-01,  3.2349e-03,  5.4443e-02, -1.3901e-02,  1.9775e-02,
        -5.7251e-02, -1.4587e-02,  1.3626e-02, -1.5091e-02,  9.2773e-02,
         4.7607e-03,  9.4604e-04,  1.0767e-01,  3.3936e-02,  8.0566e-02,
         2.4963e-02,  6.7261e-02,  5.0537e-02,  2.1240e-02, -4.5776e-04,
         3.2959e-03,  2.7405e-02, -4.6082e-03,  9.2529e-02,  1.2219e-01,
         8.3984e-02,  3.8147e-02, -2.4567e-02,  4.7241e-02, -7.8613e-02,
         3.4546e-02,  1.7303e-02,  5.1514e-02, -5.5389e-03, -3.1311e-02,
         1.9836e-02,  9.9487e-02,  6.4697e-03,  9.7656e-03, -7.8659e-03,
         3.2043e-03, -6.0425e-03, -5.5237e-03,  5.3894e-02,  9.3994e-03,
         3.0548e-02, -9.2163e-03,  6.2439e-02, -1.3611e-02,  6.4941e-02,
         8.8379e-02,  1.6937e-02,  5.3589e-02,  1.0880e-02,  4.6539e-03,
        -8.3008e-03, -3.7201e-02,  2.4170e-02,  8.5754e-03,  9.9243e-02,
         6.2317e-02, -8.7280e-03,  2.0605e-01, -7.3242e-03,  1.3867e-01,
        -1.7853e-02, -1.6418e-02, -5.5695e-04, -1.5259e-03,  1.6187e-01,
         1.4258e-01,  1.5637e-01, -2.7649e-02,  1.5515e-01, -2.4414e-03,
         8.6548e-02, -1.0376e-03, -1.3123e-03,  9.5337e-02, -5.6763e-03,
        -5.4443e-02,  1.9226e-02, -5.6458e-03, -1.9958e-02,  2.3926e-02,
        -2.1057e-02,  8.0688e-02,  4.0894e-02, -1.4404e-02,  2.2583e-02,
         1.9379e-02,  4.4250e-02,  2.2949e-02,  4.7699e-02,  7.5684e-02,
         7.2876e-02, -9.1858e-03,  4.2358e-02,  5.4474e-03,  2.2720e-02,
         1.0803e-01,  1.7990e-02,  7.6599e-03,  1.4343e-02,  1.0889e-01,
         1.2451e-02, -1.7042e-03, -2.6428e-02,  3.6621e-03, -2.8076e-03,
         1.4990e-01, -2.6550e-03,  6.6467e-02,  1.6968e-02,  7.6172e-02,
         1.3159e-01,  2.3877e-01, -2.0386e-02,  1.4587e-02,  2.0508e-02,
         8.3313e-03,  9.7351e-02,  1.0571e-01,  1.3428e-02,  2.6318e-01,
         4.5776e-04,  6.0425e-03,  4.1504e-02,  2.5452e-02,  6.1760e-03,
         6.1569e-03,  2.2827e-02, -2.3804e-03, -1.6968e-02, -1.5137e-02,
         1.3513e-01,  7.7881e-02,  6.5155e-03,  7.2327e-03, -2.5024e-03,
        -1.0986e-02,  3.2776e-02, -3.2959e-03,  2.1240e-02,  6.8665e-04,
         2.6855e-03,  2.6855e-03,  7.6065e-03,  5.9082e-02,  2.2217e-02,
        -6.6071e-03,  5.4871e-02,  5.1086e-02, -1.1368e-03, -3.6987e-02,
         7.8613e-02,  2.6932e-02, -6.7749e-03,  7.1899e-02,  2.1021e-01,
         9.5825e-03, -3.5400e-03,  4.1412e-02,  7.8583e-03,  9.8267e-03,
         1.5454e-01, -4.9591e-04,  4.2480e-02,  2.8671e-02,  1.3000e-02,
         9.2529e-02,  1.2497e-02,  1.0693e-01,  7.1777e-02,  4.7363e-02,
         1.1963e-02, -6.5918e-03, -5.7373e-03,  2.3926e-02,  4.1351e-02,
         1.1328e-01, -6.0959e-03, -2.9556e-02,  2.6428e-02,  7.7148e-02,
         1.6724e-02, -2.1973e-03,  1.2115e-02,  1.8936e-02,  8.2764e-02,
        -3.6560e-02,  3.5889e-01, -3.3722e-02,  1.9287e-02,  7.0801e-03,
        -1.2283e-02,  3.6926e-02,  3.6255e-02,  6.8359e-02,  6.2256e-02,
         6.9397e-02,  2.5330e-02, -4.6387e-03,  1.9318e-02,  3.1250e-02,
         4.0039e-02, -2.1637e-02,  6.5063e-02, -1.2604e-02,  4.7241e-02,
         1.6357e-02, -6.2866e-03,  2.0233e-02,  3.2959e-02,  6.7383e-02,
         2.4414e-04], device='cuda:2', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor(0.0845, device='cuda:2', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([-7.3853e-03, -1.5991e-02, -3.7155e-03, -1.2378e-01,  3.1433e-03,
        -2.4048e-02,  3.9673e-03, -2.9968e-02, -1.9043e-02, -2.7618e-02,
        -1.2672e-02,  3.9062e-03, -2.5650e-02, -4.5471e-03, -9.3384e-03,
        -4.1122e-03, -1.0803e-02, -1.4526e-02, -3.7109e-02, -2.7313e-03,
        -1.4706e-03, -2.5391e-02, -1.6846e-02,  3.0457e-02, -5.0735e-04,
         3.0273e-02, -5.9814e-02, -5.1392e-02, -2.2308e-02, -9.7168e-02,
         1.6296e-02,  1.4038e-03,  1.8616e-02, -3.1891e-02, -4.3610e-02,
         6.7902e-04, -5.7678e-03, -2.7344e-02, -1.7761e-02, -1.6541e-02,
        -3.5400e-02, -2.4185e-03, -5.7983e-02, -1.3527e-02, -1.3611e-02,
        -7.9529e-02, -1.0696e-02, -6.3629e-03, -2.4902e-02, -4.8828e-03,
        -2.9053e-02, -3.2883e-03, -5.3223e-02, -2.4353e-02, -9.7046e-03,
        -7.9346e-03,  1.0284e-02, -9.7900e-02, -3.1372e-02, -1.0941e-02,
        -4.4861e-03, -1.8280e-02, -6.3477e-03, -3.8086e-02,  2.0203e-02,
        -5.6152e-02, -1.2024e-02, -2.2171e-02, -1.1108e-02, -6.6528e-02,
        -2.3804e-02,  1.2512e-03, -5.0598e-02, -1.0010e-02, -5.0537e-02,
         1.8997e-03, -3.9917e-02, -1.1536e-02,  2.1362e-03, -1.1093e-02,
        -2.1057e-03, -1.1154e-02, -1.0391e-02,  2.4414e-04, -7.7057e-03,
         2.5635e-03, -1.3336e-02, -1.6357e-02, -1.7441e-02, -4.3945e-03,
         9.9792e-03, -5.6915e-03, -2.4353e-02, -3.5477e-03,  6.9809e-04,
        -4.4373e-02, -3.0807e-02, -4.7729e-02, -4.5776e-05,  1.5320e-02,
         3.6621e-04, -1.4282e-02, -1.0034e-01, -1.0277e-02,  8.2397e-03,
        -1.2344e-02, -2.2369e-02, -8.2245e-03, -1.8631e-02,  3.2257e-02,
         1.0132e-02,  2.1179e-02, -1.9287e-02,  1.2939e-02, -1.2299e-02,
         9.5215e-03, -4.6021e-02, -9.7504e-03, -7.8735e-03, -1.8188e-02,
        -6.6772e-02,  1.7700e-03, -1.0773e-02, -2.4750e-02, -2.5452e-02,
        -3.0762e-02, -2.9724e-02,  1.7151e-02, -3.9948e-02, -6.1951e-03,
         6.8665e-05, -5.3101e-03, -1.2573e-02,  6.3171e-03, -1.9165e-02,
        -4.8218e-03, -1.0361e-02, -6.4697e-03, -3.2959e-03, -2.1667e-03,
         1.0254e-02, -8.2397e-04, -7.3853e-03, -2.0874e-02, -1.7700e-02,
        -2.5635e-02, -6.9380e-04, -3.7048e-02, -7.9346e-03, -9.9182e-03,
        -3.0518e-02, -9.4986e-03,  1.0376e-02, -3.0945e-02,  1.1292e-02,
        -8.9722e-03,  6.7688e-02, -4.8035e-02, -1.0498e-02, -1.2573e-02,
        -6.1035e-03,  2.4658e-02, -8.4961e-02, -8.5754e-03, -1.5625e-02,
        -2.6321e-03, -4.5166e-03,  1.1604e-02, -1.9531e-03, -1.6041e-03,
        -1.7242e-03, -1.4587e-02, -4.7333e-02, -3.2379e-02, -3.4180e-02,
         3.1311e-02, -1.0803e-02, -6.2103e-03, -1.2848e-02, -2.0142e-03,
        -3.1067e-02,  5.0354e-03, -1.0315e-02, -3.1616e-02, -9.2850e-03,
        -2.8748e-02, -3.8300e-03, -5.9509e-04, -3.0365e-02,  2.1210e-03,
        -1.1070e-02,  4.9133e-03,  1.6510e-02, -7.4234e-03, -4.1443e-02,
         2.4261e-02, -1.7090e-03, -2.1118e-02, -1.0437e-02,  2.1484e-02,
        -6.3171e-03, -2.5314e-02,  6.8665e-03,  1.9531e-03, -1.4236e-02,
         2.3804e-02,  3.3188e-04, -1.3672e-02,  5.9509e-04, -1.9333e-02,
         9.7046e-03,  3.5095e-04,  3.3478e-02, -1.4038e-03,  3.0823e-03,
        -6.6833e-03, -3.0670e-02, -5.4626e-02, -1.7853e-03,  7.5531e-03,
         6.3477e-03, -3.1033e-03, -7.4539e-03, -1.6571e-02, -3.9368e-02,
        -2.8320e-02, -2.4223e-03, -9.8419e-03,  7.9041e-03, -3.7598e-02,
        -3.6713e-02,  9.0881e-02, -3.4943e-02, -2.3346e-02, -2.5864e-02,
        -1.0498e-02, -9.3994e-03, -1.2024e-02, -4.8218e-03,  8.1482e-03,
        -6.5308e-03, -9.8877e-03, -1.7639e-02,  7.1907e-03,  4.1199e-03,
        -1.0803e-02, -1.4923e-02, -6.3477e-03, -9.3994e-03,  1.4343e-03,
        -5.4932e-04, -3.8910e-02,  1.4038e-03, -1.7853e-02, -9.4727e-02,
        -1.4877e-02], device='cuda:5', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor(-0.0520, device='cuda:5', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([-0.0144, -0.0681, -0.0066, -0.2827,  0.0043, -0.1027,  0.0053, -0.0663,
        -0.0306, -0.0792, -0.0205, -0.0158, -0.0612, -0.0267, -0.0348, -0.0153,
        -0.0125, -0.0708, -0.1310, -0.0085, -0.0046, -0.0576, -0.0286,  0.0516,
        -0.0042,  0.0527, -0.2085, -0.1882, -0.0637, -0.1389,  0.0374, -0.0314,
         0.0137, -0.0902, -0.0633, -0.0089, -0.0047, -0.0674, -0.0414, -0.0535,
        -0.1328, -0.0045, -0.1960, -0.0180, -0.0563, -0.1975, -0.0297, -0.0410,
        -0.0250, -0.0382, -0.0922, -0.0106, -0.1599, -0.0831, -0.0566, -0.0052,
         0.0389, -0.1604, -0.1086, -0.0169, -0.0367, -0.0671, -0.0084, -0.1125,
        -0.0101, -0.2014, -0.0334, -0.0397, -0.0302, -0.1635, -0.1030, -0.0041,
        -0.1169, -0.0180, -0.1116,  0.0107, -0.0706, -0.0357, -0.0049, -0.0157,
        -0.0025, -0.0355, -0.0177, -0.0350, -0.0070,  0.0134, -0.0767, -0.0685,
        -0.0533, -0.0288,  0.0122, -0.0078, -0.0587, -0.0067, -0.0009, -0.1121,
        -0.0712, -0.1797, -0.0054,  0.0005, -0.0126, -0.0515, -0.2217, -0.0315,
         0.0337, -0.0745, -0.0505, -0.0116, -0.0246,  0.0359,  0.0518,  0.0774,
        -0.0887, -0.0122, -0.0602,  0.0085, -0.0774, -0.0376, -0.0870, -0.0170,
        -0.1733, -0.0111, -0.0320, -0.0728, -0.0234, -0.1003, -0.1028,  0.0261,
        -0.0905, -0.0188,  0.0031, -0.0135, -0.0667, -0.0026, -0.0194, -0.0443,
        -0.0280, -0.0093, -0.0101, -0.0111,  0.0120, -0.0085, -0.0117, -0.0599,
        -0.1199, -0.0812, -0.0004, -0.0381, -0.0250, -0.0153, -0.1218, -0.0282,
         0.0137, -0.0848,  0.0208, -0.0549,  0.0850, -0.1365,  0.0021, -0.0475,
        -0.0197,  0.0391, -0.2646, -0.0113, -0.2134, -0.0096, -0.0129,  0.0142,
        -0.0041,  0.0105,  0.0015, -0.0612, -0.0588, -0.0807, -0.1154,  0.0444,
        -0.0475, -0.0181, -0.0268, -0.0090, -0.0848, -0.0036, -0.0284, -0.1133,
        -0.0459, -0.0820, -0.0199, -0.0067, -0.0004, -0.0031, -0.0201, -0.0229,
         0.0213, -0.0070, -0.0906,  0.0300, -0.0004, -0.0252, -0.0869,  0.0427,
        -0.0065, -0.0555, -0.0028, -0.0096, -0.0158,  0.0258, -0.0007, -0.0631,
         0.0056, -0.0220,  0.0109,  0.0005,  0.0414, -0.0209,  0.0045, -0.0061,
        -0.0845, -0.1624, -0.0175,  0.0150,  0.0103, -0.0103, -0.0336, -0.0751,
        -0.0994, -0.1321, -0.0126, -0.0085,  0.0112, -0.1536, -0.0754,  0.1475,
        -0.1141, -0.0225, -0.0336, -0.0127, -0.0193, -0.0662, -0.0183,  0.0449,
        -0.0057, -0.0363, -0.0247,  0.0144,  0.0100, -0.0196, -0.0732, -0.0220,
        -0.0168,  0.0056, -0.0154, -0.0459,  0.0043, -0.0488, -0.2288, -0.0265],
       device='cuda:3', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor(-0.1165, device='cuda:3', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([-5.9853e-03, -2.1729e-02, -6.0272e-03, -6.0059e-02,  4.7607e-03,
        -1.2390e-02,  1.4252e-02, -1.9897e-02, -1.0590e-02, -8.4839e-03,
        -8.6594e-03,  4.3488e-03, -1.9470e-02, -1.2207e-04, -5.9967e-03,
        -3.8109e-03, -6.1035e-03, -9.5215e-03, -2.8992e-02,  1.6479e-03,
        -8.9550e-04, -1.8097e-02, -2.6855e-03,  6.9275e-02, -1.8616e-03,
         5.4382e-02, -3.7170e-02, -4.7546e-02, -1.3718e-02, -6.8970e-02,
         2.5238e-02,  7.6904e-03,  1.0071e-02, -9.9182e-03, -2.4384e-02,
        -5.1880e-04,  4.0741e-03, -1.4679e-02, -1.3260e-02, -1.4496e-02,
        -1.7761e-02,  1.1253e-03, -3.7842e-02, -8.1940e-03, -1.7761e-02,
        -5.7373e-02, -7.3242e-03, -6.0806e-03, -1.7944e-02, -9.5825e-03,
        -3.0060e-02, -2.7771e-03, -1.4404e-02, -1.1200e-02, -6.4087e-03,
         2.9449e-03,  2.0935e-02, -6.5308e-02, -3.2959e-03, -3.2425e-03,
        -3.3417e-03, -5.8289e-03, -3.7460e-03, -1.5625e-02,  3.4943e-02,
        -1.7090e-02,  4.9744e-03, -1.4343e-02, -1.4557e-02, -3.3600e-02,
        -1.3245e-02,  1.1597e-03, -2.5879e-02, -7.4272e-03, -3.3295e-02,
         8.9722e-03, -2.3438e-02, -2.3499e-03,  1.8387e-03, -6.5689e-03,
        -1.1520e-03, -6.4850e-03, -5.2414e-03,  2.1484e-02, -3.7537e-03,
         1.1124e-02, -1.4618e-02, -8.8501e-03, -1.4656e-02,  1.1230e-02,
         2.0813e-02, -3.4332e-03, -1.0590e-02, -1.8539e-03, -7.6675e-04,
        -2.6855e-02, -2.6337e-02, -2.7039e-02,  2.5940e-04,  3.3142e-02,
         7.6294e-03, -7.1106e-03, -5.5908e-02, -7.1793e-03,  3.2959e-02,
        -8.8196e-03, -1.6129e-02, -3.4561e-03, -1.3611e-02,  3.9093e-02,
         3.1433e-02,  4.2908e-02, -1.6815e-02,  5.0537e-02, -4.8218e-03,
         2.6764e-02, -2.8992e-02, -8.0566e-03,  1.2543e-02, -1.2047e-02,
        -3.9795e-02,  4.4861e-03, -8.6823e-03, -1.7166e-02, -7.2937e-03,
        -2.4750e-02, -1.4404e-02,  1.2894e-02, -3.3600e-02, -4.0894e-03,
         6.3171e-03,  3.2349e-03, -2.1790e-02,  1.1627e-02,  3.9673e-04,
        -8.4229e-03, -6.6223e-03,  4.3640e-03, -1.7967e-03,  6.0272e-03,
         2.1362e-02,  8.5449e-04, -5.6458e-04, -4.9133e-03,  1.7090e-02,
        -1.4801e-02, -5.4550e-04, -2.3880e-02,  1.6785e-03, -5.9052e-03,
         4.2725e-03, -5.6000e-03,  1.8127e-02, -1.4587e-02,  3.6987e-02,
         1.3367e-02,  7.9041e-02, -2.7039e-02, -4.7302e-03,  1.8005e-03,
         2.7924e-03,  3.6865e-02, -3.9551e-02, -7.4768e-04,  1.3306e-02,
        -8.8501e-04,  3.1128e-03,  2.2491e-02,  3.1891e-03,  2.3651e-04,
        -8.1635e-04, -1.6052e-02, -3.5583e-02, -1.5137e-02, -2.2766e-02,
         4.8645e-02,  9.8267e-03, -6.7940e-03, -5.8594e-03, -3.4981e-03,
        -2.0477e-02,  9.3994e-03, -5.5771e-03, -2.7679e-02, -9.9487e-03,
        -2.0966e-02,  4.0588e-03, -1.7929e-03, -9.0332e-03,  7.7972e-03,
        -8.5449e-03,  8.6060e-03,  2.5330e-02, -6.1531e-03, -3.6163e-02,
         2.4902e-02,  4.8370e-03, -1.4572e-02,  1.3428e-03,  4.4128e-02,
        -8.3923e-04, -1.8372e-02,  8.0261e-03,  4.1428e-03, -9.0485e-03,
         3.4241e-02, -4.7684e-04,  1.5625e-02,  3.6240e-03, -6.9580e-03,
         1.9287e-02,  3.0060e-03,  3.8025e-02,  1.9226e-03,  1.0284e-02,
         1.1444e-03, -1.6724e-02, -2.5330e-02,  5.0659e-03,  1.4511e-02,
         4.1809e-02, -2.9182e-03,  1.2390e-02, -9.9792e-03, -1.6418e-02,
        -1.6602e-02, -2.8038e-03, -5.0964e-03,  1.3077e-02, -3.3142e-02,
        -2.7679e-02,  1.3135e-01, -2.9907e-02, -7.4768e-03, -1.3214e-02,
        -9.6436e-03, -3.6011e-03, -6.1646e-03,  1.4038e-03,  1.8463e-02,
         1.1169e-02, -5.4016e-03, -1.3130e-02,  8.4305e-03,  9.0942e-03,
        -1.6174e-03, -1.7487e-02, -1.4221e-02, -9.8572e-03,  7.6599e-03,
         1.5030e-03, -2.6611e-02,  3.3569e-03,  9.6741e-03, -5.7617e-02,
        -7.1182e-03], device='cuda:1', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor(-0.0239, device='cuda:1', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
task_net module.module.task_net.layer_norm.weight True tensor(-0.4736, device='cuda:0', dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor(0.5176, device='cuda:0', dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([-1.6113e-02,  1.1108e-02,  1.4502e-01, -1.6650e-01,  1.4258e-01,
        -2.3438e-02, -5.5713e-01, -2.8320e-02,  5.2368e-02, -3.9893e-01,
         6.1133e-01, -3.8428e-01,  8.0566e-03, -1.5137e-02, -2.5708e-01,
        -4.6753e-02,  8.4961e-02, -8.6670e-02, -3.3716e-01,  2.7344e-02,
         1.6931e-01,  3.4155e-01, -4.2432e-01,  2.7319e-01, -5.9717e-01,
        -1.5137e-01,  1.8127e-01,  1.6138e-01, -3.4619e-01, -1.2061e-01,
         4.9121e-01,  8.1104e-01,  1.2422e+00,  6.9336e-01,  3.6719e-01,
        -3.6865e-01,  7.5562e-02, -7.3730e-02,  7.2412e-01, -4.5581e-01,
         3.0957e-01, -4.5312e-01,  5.3809e-01, -1.3086e-01,  3.9648e-01,
         5.7812e-01, -6.5039e-01,  4.8071e-01, -3.5474e-01, -3.3838e-01,
         7.2217e-01,  6.7285e-01,  1.6821e-01, -2.7417e-01, -5.1660e-01,
        -7.3486e-02,  5.7422e-01, -1.1725e-01, -5.5664e-02, -1.5137e-01,
        -1.1660e+00, -1.0107e-01,  4.5752e-01,  1.0498e-01, -1.2305e-01,
         8.8574e-01, -5.8789e-01, -2.4854e-01, -6.1035e-03,  8.7598e-01,
         2.4097e-01,  5.2783e-01,  5.1709e-01,  8.3789e-01, -4.1504e-02,
        -5.1953e-01,  6.9629e-01, -3.4546e-01,  7.6758e-01,  3.4790e-01,
         6.6602e-01, -2.5122e-01, -1.4160e-02,  2.8369e-01,  2.3145e-01,
         6.0547e-01,  2.3242e-01,  2.1582e-01, -1.4661e-01, -2.9297e-02,
         7.3486e-01,  1.5527e-01, -1.4209e-01, -2.4976e-01, -3.2031e-01,
         5.6836e-01, -3.2373e-01, -1.5723e-01,  3.5693e-01, -6.3672e-01,
        -2.0117e-01, -4.3457e-02, -6.1621e-01,  5.8350e-02, -5.0684e-01,
         6.8457e-01, -1.8311e-03, -7.8955e-01, -1.2451e-01, -4.1187e-01,
         4.1016e-02, -3.9111e-01,  4.4580e-01, -2.2021e-01,  4.5776e-01,
        -9.4189e-01, -2.5098e-01,  1.2720e-01, -3.2568e-01,  1.7017e-01,
         1.1304e-01, -6.9336e-02, -9.3213e-01,  1.9165e-01, -3.0225e-01,
        -3.8232e-01,  6.8018e-01,  3.6304e-01,  1.4697e-01,  1.1230e-02,
        -1.0596e-01, -3.3105e-01,  2.2534e-01,  3.2080e-01,  4.6143e-02,
        -3.0957e-01, -4.1431e-01, -3.0835e-01, -4.9658e-01,  6.2988e-01,
        -2.4158e-01,  9.0820e-01, -2.9224e-01,  5.6738e-01, -1.0864e-01,
         7.7637e-02,  2.5391e-02, -2.5195e-01,  4.1895e-01, -4.6338e-01,
         1.6748e-01,  3.4082e-01,  8.1665e-02,  3.1299e-01, -2.7979e-01,
        -4.9121e-01, -3.9404e-01,  4.1406e-01,  1.9482e-01,  4.4653e-01,
        -1.0107e-01, -5.9570e-02,  2.5317e-01, -6.9336e-02, -7.5684e-01,
         5.3809e-01, -1.2002e+00,  2.1338e-01,  7.2998e-01,  2.3438e-01,
        -9.1553e-02, -3.8916e-01,  3.2495e-01,  3.8525e-01,  3.5645e-02,
         1.1292e-02, -9.5215e-02,  8.5938e-02, -6.3477e-01,  5.7715e-01,
         3.6621e-02, -8.0762e-01, -1.9556e-01,  4.9756e-01, -3.2422e-01,
        -5.3711e-02,  7.0312e-01,  5.9619e-01,  1.0996e+00,  3.9062e-03,
        -6.5576e-01, -1.4111e-01, -9.2432e-01, -6.9336e-01,  5.8740e-01,
         8.7549e-01, -2.1582e-01,  2.4219e-01, -2.4573e-01, -8.1250e-01,
         2.8271e-01,  8.3496e-02, -2.6855e-01,  2.9370e-01, -4.3750e-01,
        -3.4595e-01, -4.4556e-01, -4.2578e-01, -2.8101e-01, -7.3145e-01,
        -1.0132e-01,  4.5215e-01, -4.1943e-01, -1.4380e-01,  4.7388e-01,
         1.1621e-01, -2.3071e-01,  5.2948e-03, -7.0801e-02,  4.8926e-01,
        -1.5820e-01,  1.7480e-01, -4.6362e-01,  4.6814e-02, -3.4058e-01,
         2.4353e-01, -5.5225e-01,  2.0142e-01, -3.5205e-01, -1.5381e-02,
         8.5449e-04, -5.0586e-01, -2.5903e-01,  3.4546e-01,  1.2109e-01,
        -5.6982e-01,  1.2183e-01,  3.1250e-02,  1.9629e-01, -4.6411e-01,
        -1.4795e-01,  4.5117e-01,  6.9238e-01, -1.3501e-01, -3.2324e-01,
        -9.7412e-02,  2.7734e-01,  7.8613e-02, -5.9326e-02, -2.8418e-01,
         6.2842e-01,  2.2168e-01, -3.6426e-01,  5.1270e-02, -4.1797e-01,
        -3.0396e-01, -2.6733e-02, -2.2156e-01, -6.5625e-01, -2.5684e-01,
        -1.9897e-02, -4.0479e-01,  9.9902e-01, -2.5122e-01, -9.1309e-02,
        -4.0479e-01, -5.3613e-01,  1.8481e-01,  2.2070e-01, -7.8076e-01,
         2.0850e-01,  5.3418e-01,  7.5391e-01,  1.7871e-01,  8.3398e-01,
        -1.3223e+00, -6.3672e-01, -4.3652e-01,  2.9053e-01, -5.3320e-01,
         5.1416e-01,  2.8516e-01,  3.3521e-01, -6.9336e-02,  3.3496e-01,
        -4.9609e-01, -5.1123e-01,  2.2900e-01, -2.3828e-01,  6.4941e-02,
         2.4048e-01, -2.5098e-01,  2.7490e-01,  9.1699e-01,  5.4004e-01,
        -4.1016e-01,  2.9785e-02,  2.3218e-01, -5.5078e-01,  1.0557e+00,
        -5.3418e-01,  3.7598e-02, -5.7520e-01, -2.0020e-01,  5.0293e-01,
        -3.4619e-01,  3.5742e-01,  7.6294e-04, -7.2461e-01,  6.0596e-01,
         1.9604e-01, -8.0615e-01,  3.4180e-02,  7.6660e-02, -2.6367e-01,
         4.3628e-01,  3.8770e-01, -4.9146e-01,  1.2734e+00, -1.6821e-01,
        -1.4050e-01, -4.6387e-01,  1.1738e+00, -5.7275e-01, -4.1577e-01,
         3.5913e-01,  5.6396e-02,  1.1401e-01,  2.8223e-01, -4.5215e-01,
         2.9834e-01, -1.0781e+00,  6.5283e-01, -1.3330e-01,  4.3311e-01,
        -1.9873e-01,  3.0762e-01, -3.8574e-01,  3.4155e-01,  1.2500e-01,
         1.7432e-01,  3.1641e-01, -2.9224e-01, -4.5361e-01, -1.1792e-01,
         2.4878e-01,  4.5654e-01,  2.0410e-01,  4.2896e-01,  6.8066e-01,
        -6.5527e-01, -1.0215e+00,  9.4971e-02,  2.7075e-01, -8.3496e-01,
         2.7979e-01,  2.9346e-01, -1.1768e-01,  1.3721e-01,  8.5645e-01,
         4.8828e-04,  4.5264e-01,  3.1445e-01,  3.1152e-01, -1.7529e-01,
         2.6709e-01, -2.0605e-01,  2.6367e-01,  6.3232e-02,  4.0625e-01,
        -5.6250e-01, -1.5063e-01,  2.0782e-02,  6.3037e-01,  1.6357e-01,
        -6.2305e-01, -2.6709e-01, -4.0088e-01, -3.9673e-01, -3.1177e-01,
        -2.7832e-02, -5.3906e-01, -6.2402e-01, -4.9609e-01, -7.7002e-01,
         3.2007e-01, -2.4414e-02,  1.4355e-01, -6.6504e-01,  5.6641e-01,
        -9.9023e-01,  1.6406e-01,  1.1035e-01, -2.7173e-01, -1.4185e-01,
        -1.4844e-01, -5.5225e-01, -5.4199e-01,  5.2148e-01,  2.5586e-01,
         4.8120e-01,  7.6904e-02, -5.2734e-01, -1.6064e-01, -2.0020e-01,
         5.1074e-01, -4.4434e-02, -9.0820e-02,  2.6807e-01,  6.3574e-01,
        -7.8809e-01,  9.7754e-01,  5.6299e-01, -6.8848e-02, -9.4849e-02,
        -1.7871e-01,  2.5391e-01,  2.3853e-01,  3.4424e-01, -1.0889e+00,
        -2.9297e-01,  8.8965e-01,  1.1426e-01, -3.4277e-01,  3.4912e-02,
         3.2349e-01, -1.0156e-01, -1.0242e-01,  3.2275e-01,  9.4238e-02,
        -2.4097e-01,  2.3938e-01,  3.0347e-01,  5.1074e-01,  5.9521e-01,
        -1.8799e-01, -1.8164e-01, -1.0059e-01,  1.0547e+00, -3.3154e-01,
         1.0864e-01,  1.3940e-01, -3.1860e-01,  6.1621e-01, -2.1484e-02,
         2.1692e-01, -2.1460e-01,  7.5781e-01, -4.5013e-02,  8.8867e-02,
        -3.8281e-01,  4.3286e-01,  3.8330e-02, -2.8625e-02, -6.7773e-01,
        -3.0737e-01, -3.5303e-01, -9.7733e-03,  1.2461e+00, -7.9590e-02,
         3.3203e-02,  1.0840e-01,  3.0176e-01, -1.6113e-01,  2.5146e-02,
        -1.6541e-02,  4.6460e-01, -1.3953e-01, -4.1650e-01, -2.6416e-01,
        -9.0430e-01, -6.2988e-01,  5.3711e-03, -9.6191e-02, -4.2261e-01,
         1.9641e-01,  7.5317e-02, -1.7578e-01,  3.5986e-01,  1.1597e-02,
        -2.4731e-01,  3.8477e-01, -5.1172e-01, -1.5039e-01, -4.2676e-01,
        -8.3252e-01, -6.7285e-01, -2.2510e-01,  5.4004e-01,  5.2002e-02,
        -5.9521e-01,  2.1484e-02,  7.0264e-01, -4.8828e-02,  1.1719e-01,
         3.5059e-01, -3.6475e-01,  1.4478e-01,  8.7646e-01, -2.4854e-01,
        -6.8799e-01,  2.9590e-01, -8.6328e-01,  2.8442e-01,  5.8594e-02,
        -7.4902e-01, -1.1804e-01,  2.9199e-01,  2.1448e-01, -6.6260e-01,
        -4.9097e-01, -3.8232e-01], device='cuda:0', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor(-0.2695, device='cuda:0', dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([-1.4175e-02, -2.5146e-02,  2.2430e-02,  1.2500e-01,  1.5030e-02,
         3.1738e-02,  7.0801e-02, -4.6387e-03, -7.3242e-04,  2.4414e-03,
        -3.0304e-02,  3.7720e-02, -6.6406e-02,  6.7871e-02, -1.7395e-02,
        -1.7273e-02,  5.5176e-02,  2.7588e-02, -5.6641e-02,  2.6062e-02,
        -1.4610e-03, -2.9419e-02,  2.4658e-02,  2.4146e-01, -5.4016e-03,
         2.1533e-01, -5.6152e-02, -9.5947e-02, -6.1523e-02, -7.3242e-02,
         1.1426e-01,  5.9326e-02,  8.9111e-02, -3.8330e-02, -7.2876e-02,
         1.5198e-02,  2.1484e-02, -4.0283e-02, -5.4993e-02, -1.6968e-02,
         9.6191e-02, -1.0071e-03, -2.6367e-02, -2.9327e-02, -3.4790e-02,
        -1.6821e-01, -1.1780e-02, -7.6904e-03, -9.0027e-03,  7.1533e-02,
        -4.8828e-03, -5.7831e-03,  2.7344e-02,  5.8594e-03,  8.2764e-02,
         3.3813e-02,  9.6985e-02, -7.3242e-03, -7.3242e-03, -8.9417e-03,
        -8.5449e-03, -1.4404e-02, -2.6855e-03,  7.1289e-02,  1.3464e-01,
        -3.6133e-02,  8.0566e-03, -5.2643e-02,  1.4648e-02, -1.8018e-01,
         4.8828e-04,  1.1353e-02,  4.8828e-03, -1.4954e-02, -6.7627e-02,
         4.4891e-02,  9.2529e-02, -3.0823e-02,  1.1658e-02, -2.0752e-03,
         7.1716e-03, -2.9205e-02, -1.2100e-02,  8.4717e-02, -3.6621e-03,
         5.5725e-02, -1.8860e-02,  6.2134e-02, -4.3884e-02,  1.1011e-01,
         1.1438e-01,  2.6855e-02,  5.3711e-03,  1.7731e-02,  1.5015e-02,
        -2.6855e-03, -7.1716e-02, -5.7129e-02,  9.6130e-03,  8.4717e-02,
         5.9204e-02, -3.0518e-02,  1.7871e-01, -3.0518e-03,  2.0715e-01,
        -2.6550e-02, -1.7883e-02, -5.4626e-03, -7.5684e-03,  2.0459e-01,
         2.2205e-01,  2.1375e-01, -4.8706e-02,  1.9141e-01, -3.9062e-03,
         6.5308e-02, -2.2949e-02, -1.8860e-02,  8.2520e-02, -2.1362e-03,
        -1.2573e-01,  4.3030e-03, -1.6296e-02, -1.9653e-02,  2.4536e-02,
        -8.3008e-02,  3.2715e-02,  4.9652e-02, -7.2632e-02,  8.4839e-03,
         7.9651e-03,  4.4128e-02, -3.9795e-02,  3.8666e-02,  2.1729e-02,
         7.3730e-02, -1.0559e-02,  3.3813e-02,  1.1078e-02,  5.2490e-03,
         1.9897e-01,  7.1106e-03,  6.0730e-03, -2.5024e-02,  9.0820e-02,
        -3.6621e-04, -1.5030e-03, -3.2501e-02,  7.6904e-03, -7.8430e-03,
         1.2549e-01, -2.4628e-02,  9.0576e-02, -4.5166e-03,  8.5938e-02,
         1.0938e-01,  2.6660e-01, -8.7280e-02,  2.2766e-02,  3.1738e-03,
         2.2278e-03,  1.6113e-01, -1.2695e-02,  3.3569e-03,  2.0605e-01,
         1.3184e-02,  1.7456e-02,  5.5115e-02,  1.6785e-02,  6.7291e-03,
         6.4240e-03,  4.0527e-02, -1.1597e-02, -4.8950e-02, -5.4932e-02,
         1.8689e-01,  6.6406e-02,  1.5259e-03,  1.2878e-02, -6.6681e-03,
        -4.0405e-02,  6.1951e-02, -7.0801e-03, -4.8828e-03, -4.5227e-02,
        -2.8320e-02, -9.1553e-03,  5.4932e-03,  1.2805e-01,  2.5848e-02,
        -5.0354e-03,  3.6987e-02,  6.7871e-02,  1.7242e-03, -6.0791e-02,
         9.0454e-02,  1.5808e-02, -5.2490e-03,  1.3428e-02,  3.3521e-01,
        -1.0010e-02, -2.3254e-02,  4.9133e-02,  6.7444e-03,  6.6528e-03,
         2.0105e-01,  4.3640e-03,  3.3691e-02,  2.7344e-02,  4.2603e-02,
         9.8267e-02,  1.8280e-02,  1.1548e-01,  7.1289e-02,  6.0303e-02,
         1.1047e-02, -4.6997e-02, -1.1890e-01,  1.1230e-02,  3.9429e-02,
         1.0620e-01, -1.3657e-02, -3.1403e-02,  2.1973e-03, -2.2949e-02,
        -3.4180e-03, -1.3420e-02,  2.5208e-02,  4.6387e-02, -3.3691e-02,
        -5.9204e-02,  4.9561e-01, -1.0931e-01, -1.4648e-02, -1.8311e-03,
        -1.3901e-02,  5.4443e-02,  6.3477e-02,  3.7598e-02,  7.6538e-02,
         6.6406e-02,  5.6641e-02, -6.2561e-03,  4.7241e-02,  3.2257e-02,
         2.3315e-02, -6.2805e-02,  1.0498e-01,  6.4697e-03,  5.8960e-02,
         1.3000e-02, -1.9409e-02,  3.3661e-02,  1.9775e-02, -5.3711e-03,
        -9.1858e-03], device='cuda:0', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor(0.0674, device='cuda:0', dtype=torch.float16)
2023-09-02 21:54:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/crash.pt
tensor([-1.5854e-02, -5.5542e-02, -6.7902e-03, -6.5430e-02,  9.8419e-04,
        -5.2246e-02,  1.8127e-02, -6.8359e-02, -5.1331e-02, -4.6387e-02,
        -2.8320e-02,  8.6060e-03, -7.0312e-02, -2.5635e-03, -1.1780e-02,
        -8.3389e-03, -4.9561e-02, -5.2490e-03, -6.9946e-02, -5.5847e-03,
        -1.6403e-03, -3.1189e-02, -2.4048e-02,  1.1816e-01, -5.1956e-03,
         7.3730e-02, -1.1230e-01, -1.6040e-01, -5.6366e-02, -8.8806e-02,
         4.1199e-02, -4.1260e-02,  3.0518e-02, -5.3223e-02, -1.0034e-01,
        -7.8888e-03,  8.5449e-04, -5.0232e-02, -2.7924e-02, -4.9927e-02,
        -5.2002e-02, -1.0803e-02, -1.1182e-01, -2.3727e-02, -2.4536e-02,
        -1.2671e-01, -2.1973e-02, -1.9836e-02, -2.1057e-02,  6.3477e-03,
        -8.8684e-02, -5.7220e-03, -5.6152e-02, -3.6377e-02, -4.3335e-02,
        -9.0942e-03,  2.4109e-02, -5.7617e-02, -5.3101e-02, -2.3590e-02,
        -1.1292e-02, -3.2959e-02, -1.7639e-02, -5.3711e-02,  3.8818e-02,
        -1.2158e-01, -2.0508e-02, -4.9927e-02, -2.5635e-03, -1.5576e-01,
        -4.8950e-02, -9.4452e-03, -4.7607e-02, -2.3682e-02, -7.4951e-02,
         1.2024e-02, -7.4951e-02, -8.6060e-03, -5.0049e-03, -9.8572e-03,
        -1.0834e-03, -3.2867e-02, -1.5411e-02, -2.5269e-02, -3.0243e-02,
         1.9684e-02, -4.5593e-02, -9.2773e-03, -4.8096e-02,  3.1738e-02,
         4.3213e-02, -1.3245e-02, -3.8330e-02,  3.4027e-03,  3.3340e-03,
        -1.1340e-01, -6.2012e-02, -7.9590e-02, -3.0060e-03,  2.3560e-02,
        -1.0254e-02, -5.4169e-02, -4.7363e-02, -2.9327e-02,  6.6162e-02,
        -4.4342e-02, -4.4098e-02, -5.3406e-03, -2.5513e-02,  5.6519e-02,
         5.8716e-02,  1.1255e-01, -6.7444e-02,  4.2358e-02, -5.8899e-02,
         4.0344e-02, -7.5073e-02, -2.8625e-02, -3.2959e-02, -1.8539e-02,
        -1.5002e-01,  5.2185e-03, -2.2644e-02, -9.0088e-02, -8.7280e-03,
        -4.8828e-02, -2.1484e-02,  1.8478e-02, -7.9407e-02, -1.0559e-02,
         8.1329e-03,  6.7749e-03, -6.0547e-02,  2.9053e-02, -3.5522e-02,
        -3.8330e-02, -1.8555e-02,  1.3489e-02, -8.6975e-03,  4.4556e-03,
         4.1138e-02,  8.2092e-03, -1.0010e-02, -5.0964e-02, -4.5654e-02,
        -3.9368e-02, -1.0128e-03, -3.5889e-02, -2.1667e-02, -7.8125e-03,
        -5.1514e-02, -1.8112e-02,  3.0762e-02, -4.0588e-02,  3.6682e-02,
         6.8359e-03,  1.5796e-01, -6.9458e-02,  7.6599e-03, -3.8391e-02,
        -3.2654e-03,  6.8787e-02, -1.5967e-01,  1.0376e-02,  2.4902e-02,
        -9.7961e-03, -2.3926e-02,  2.2034e-02,  1.7273e-02,  0.0000e+00,
        -3.9062e-03, -4.9927e-02, -2.4780e-02, -7.1228e-02, -7.0068e-02,
         9.3323e-02,  2.5635e-03, -1.0208e-02, -1.6876e-02, -1.3336e-02,
        -3.8879e-02, -1.8311e-04, -1.4221e-02, -6.1890e-02, -2.4643e-02,
        -4.6387e-02, -2.1454e-02, -4.8065e-03,  2.2339e-02,  4.3640e-03,
        -1.8173e-02, -3.8452e-03,  3.3417e-02, -1.0597e-02, -8.1116e-02,
         2.4384e-02,  2.8076e-03, -1.6571e-02, -2.2705e-02,  8.9600e-02,
        -2.7466e-03, -2.7374e-02,  2.6886e-02, -1.1383e-02, -3.4180e-03,
         8.1665e-02, -9.9182e-05, -4.5044e-02,  7.1716e-03, -1.4282e-02,
         3.3203e-02, -1.2207e-04,  5.2307e-02, -3.2959e-03,  4.3945e-03,
         5.2490e-03, -7.5012e-02, -1.6931e-01,  1.5564e-03,  5.5237e-03,
         5.4932e-02, -1.4687e-02, -3.2898e-02, -6.2256e-02, -1.2097e-01,
        -7.7515e-02, -1.0658e-02,  6.7444e-03,  3.5736e-02, -3.5889e-02,
        -8.8379e-02,  2.4597e-01, -6.6650e-02, -3.7170e-02, -1.8005e-02,
        -1.4420e-02, -7.9346e-04,  1.9043e-02, -3.3569e-02,  4.0283e-02,
         1.9470e-02, -2.2400e-02, -1.6083e-02,  1.8600e-02,  2.0264e-02,
        -7.9956e-03, -3.6255e-02, -6.1035e-03, -2.3499e-02,  6.7749e-03,
        -5.9662e-03, -4.6906e-02, -5.3101e-03, -1.9775e-02, -9.1064e-02,
        -1.4099e-02], device='cuda:4', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor(-0.0522, device='cuda:4', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([-2.4658e-02, -1.2158e-01, -8.9722e-03, -5.8984e-01,  1.6525e-02,
        -9.7656e-02, -4.1504e-02, -1.5967e-01, -9.9854e-02, -1.2451e-01,
        -6.7261e-02,  1.6357e-02, -1.1230e-01,  6.9824e-02, -1.4441e-01,
        -3.5004e-02,  4.1260e-02, -4.1016e-02, -2.9150e-01, -1.9470e-02,
        -9.3613e-03, -1.7627e-01,  1.2451e-02,  9.0332e-02, -1.6998e-02,
         2.1826e-01, -4.7852e-01, -3.4375e-01, -1.6150e-01, -2.0825e-01,
         7.5195e-02,  7.8125e-02,  9.6191e-02, -3.1128e-01, -2.0154e-01,
        -2.3376e-02, -5.2979e-02, -2.0349e-01, -7.5806e-02, -1.3965e-01,
        -4.7070e-01,  6.8970e-03, -4.9805e-01, -4.7607e-02, -6.7993e-02,
        -5.1709e-01, -8.8074e-02, -2.9663e-02, -1.2268e-02,  3.7598e-02,
        -1.9434e-01, -2.7649e-02, -3.9746e-01,  3.7109e-02, -1.4258e-01,
         6.8359e-03,  1.1108e-01, -3.6523e-01, -1.5820e-01, -3.8086e-02,
        -5.0415e-02, -1.5112e-01, -7.2937e-03, -2.2168e-01,  9.8633e-02,
        -5.5078e-01, -5.6152e-02, -1.1505e-01, -1.0254e-02, -4.4263e-01,
        -2.1680e-01, -4.5166e-03, -3.4473e-01, -1.3855e-02, -2.3901e-01,
         8.6670e-03, -8.6914e-02, -1.3831e-01, -6.1035e-03, -9.6436e-03,
        -1.2634e-02, -4.8279e-02, -1.1322e-02, -2.6367e-02, -3.2959e-03,
         7.2998e-02, -1.2952e-01, -1.1572e-01, -1.3147e-01, -1.6699e-01,
         1.1719e-02,  1.3611e-02, -1.4795e-01, -1.8738e-02,  3.8910e-03,
        -1.3428e-01, -9.5764e-02, -3.6182e-01,  7.2021e-03, -3.9062e-02,
        -9.6924e-02, -1.1572e-01, -4.7461e-01, -8.9539e-02,  1.2573e-01,
        -1.0193e-01, -6.8604e-02, -2.5085e-02, -5.9692e-02,  1.9385e-01,
         3.8330e-02,  1.9702e-01, -2.1753e-01,  4.6875e-02, -9.3750e-02,
         4.4678e-02, -1.3171e-01, -6.4087e-02, -1.6650e-01, -7.9041e-03,
        -3.8770e-01, -4.0894e-03, -3.8086e-02, -2.1045e-01,  8.3008e-03,
        -1.9141e-01, -2.0264e-01,  8.4839e-02, -1.2476e-01, -1.9287e-02,
         1.1749e-02, -9.0332e-03, -2.1826e-01,  6.5063e-02, -2.2461e-02,
        -4.6875e-02, -6.7627e-02, -5.0049e-02, -5.4626e-03,  1.1292e-02,
         4.7363e-02, -2.1912e-02, -3.3386e-02, -1.5723e-01, -3.1934e-01,
        -1.2280e-01, -6.1646e-03, -5.2826e-02, -3.7598e-02, -1.6907e-02,
        -2.9492e-01, -3.7048e-02, -2.6489e-02, -1.1230e-01,  6.7139e-02,
        -1.0645e-01,  2.3267e-01, -3.5400e-01, -2.4231e-02, -6.2256e-02,
        -8.1177e-03,  8.0566e-02, -6.4941e-01,  2.1973e-03, -5.8398e-01,
        -8.4534e-03, -2.5635e-02,  3.4729e-02,  1.2207e-04,  1.1475e-02,
        -2.4353e-02, -1.6113e-01, -5.6152e-02, -1.8530e-01, -2.4512e-01,
         4.8828e-04, -8.4473e-02, -3.8544e-02, -3.3081e-02, -1.4282e-02,
        -2.1533e-01,  4.6631e-02, -4.8523e-02, -1.6138e-01, -1.2390e-01,
        -2.0898e-01, -9.3262e-02, -1.9257e-02,  1.2207e-01,  1.2146e-02,
        -3.0060e-02, -4.6387e-02,  1.2415e-01, -7.6599e-03, -2.1423e-01,
         1.0486e-01, -3.3630e-02, -9.5215e-03, -2.2852e-01,  1.9336e-01,
        -1.0449e-01, -5.9937e-02,  7.6904e-03, -5.9143e-02, -2.8687e-03,
         1.4819e-01,  7.6904e-03, -1.4502e-01,  3.9276e-02, -3.1616e-02,
         6.1035e-03,  2.5635e-03, -2.4170e-02, -4.8828e-02,  3.5950e-02,
        -3.4180e-03, -1.5796e-01, -4.3848e-01, -8.9111e-03,  1.6968e-02,
         6.0791e-02, -2.9831e-02, -8.4106e-02, -1.2061e-01, -3.6035e-01,
        -1.4917e-01, -1.6022e-02,  1.3550e-02,  1.1395e-01, -4.5801e-01,
        -2.7490e-01,  3.6914e-01, -2.8809e-01, -9.7900e-02, -5.9937e-02,
        -2.3163e-02, -8.2520e-02, -8.4473e-02, -8.2520e-02,  1.4111e-01,
        -2.6733e-02, -9.2163e-02, -1.7517e-02,  5.1392e-02,  2.8198e-02,
        -6.3721e-02, -2.1313e-01,  8.7402e-02, -2.8076e-02,  4.3335e-02,
        -2.5635e-02, -5.4565e-02, -1.4954e-02, -2.8540e-01, -5.8301e-01,
        -1.5686e-02], device='cuda:6', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor(-0.1909, device='cuda:6', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
task_net module.module.task_net.layer_norm.weight True tensor(0.1865, device='cuda:7', dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor(-0.4204, device='cuda:7', dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([ 8.7830e-02,  6.8848e-02, -3.1152e-01, -3.7573e-01, -1.8701e-01,
        -1.0879e+00, -3.7598e-01,  6.6162e-02,  2.5049e-01, -9.2163e-02,
         3.3740e-01,  1.5381e-02,  1.0406e-01,  1.0132e-02, -5.8545e-01,
        -3.7109e-02,  2.6953e-01,  5.3711e-03, -1.6479e-01, -4.6680e-01,
         1.3733e-01,  2.6294e-01, -4.0356e-01,  8.7158e-02,  1.3062e-02,
         1.2482e-02,  5.0385e-02, -1.5967e-01, -4.0894e-01, -1.9275e-01,
         1.2598e-01,  5.3955e-02,  2.7539e+00,  2.8271e-01, -1.5894e-01,
         1.7236e-01,  1.6455e-01, -1.1304e-01,  2.7710e-01, -2.4329e-01,
        -3.4082e-01,  2.2437e-01, -5.1636e-02,  2.5781e-01,  8.4656e-02,
         1.6138e-01, -5.5176e-01,  8.4961e-02, -1.6699e-01,  9.4482e-02,
         2.1558e-01,  6.6943e-01, -1.2817e-01, -6.5063e-02, -6.9214e-02,
        -2.5122e-01,  4.2944e-01, -4.0625e-01, -2.5977e-01, -3.0884e-01,
        -5.1172e-01, -4.7852e-02,  7.3975e-02, -1.0724e-01, -1.1230e-02,
         3.1226e-01, -1.5796e-01, -1.3696e-01, -2.0984e-01,  1.0156e-01,
        -2.9517e-01,  1.0358e-01,  1.8335e-01,  2.1216e-01, -3.9893e-01,
        -8.6035e-01,  1.9263e-01, -1.2805e-01,  2.7417e-01, -6.3965e-02,
         1.4319e-01,  3.9551e-02, -1.3110e-01, -2.6367e-01, -2.8809e-02,
         2.7710e-01,  4.0894e-02, -2.9639e-01, -1.1194e-01,  1.8555e-02,
         1.0303e-01, -1.6382e-01, -4.3945e-03,  4.5410e-02,  1.6504e-01,
        -1.8689e-01, -1.1829e-01, -8.8013e-02,  2.4500e-01, -5.1270e-01,
        -2.8125e-01,  3.9160e-01, -1.2012e-01,  7.6782e-02, -8.1543e-02,
         2.2437e-01,  9.3262e-02, -4.9438e-01, -1.5503e-01, -3.1738e-01,
        -5.3833e-02, -7.7881e-02, -1.8750e-01,  1.3721e-01,  1.3232e-01,
        -5.7715e-01, -1.9080e-01, -1.1401e-01, -2.6855e-01,  2.5415e-01,
        -5.2246e-02, -2.3193e-01, -7.4646e-02, -1.9800e-01, -2.6758e-01,
        -2.8369e-01,  1.4905e-01, -1.4819e-01,  2.0020e-02, -4.3750e-01,
        -6.5247e-02,  1.1755e-01,  2.2119e-01,  4.9072e-02, -1.5210e-01,
         1.6113e-02, -1.6504e-01, -3.7231e-03,  1.4868e-01,  9.5703e-02,
        -2.9663e-01,  4.8438e-01,  4.0161e-02, -1.7798e-01, -2.4561e-01,
        -9.1309e-02, -8.4412e-02,  3.6926e-02,  2.5781e-01, -2.3193e-02,
         2.0874e-01,  3.0322e-01, -5.0720e-02,  1.5857e-01,  1.3000e-02,
        -5.6183e-02, -3.5767e-02,  1.2903e-01, -1.8213e-01,  2.2607e-01,
        -5.8105e-01, -9.8145e-02, -2.2461e-01,  2.9785e-02, -3.2739e-01,
         4.7510e-01, -7.2656e-01, -4.5972e-01, -5.7434e-02, -2.0581e-01,
        -8.1787e-02, -2.8662e-01, -5.2643e-03, -8.6243e-02, -1.3379e-01,
         5.2832e-01, -3.6548e-01, -3.8330e-01, -3.2178e-01,  1.4111e-01,
         1.6101e-01, -1.1829e-01,  3.5107e-01,  9.2529e-02, -3.7305e-01,
         6.4697e-02,  2.0874e-01,  3.8623e-01,  1.6895e-01,  3.1982e-02,
        -2.4561e-01,  1.5649e-01,  1.3818e-01, -3.7671e-01,  8.5083e-02,
         3.6768e-01, -1.6260e-01,  9.3262e-02, -1.2109e-01, -3.3789e-01,
         7.7881e-02, -1.6272e-01,  3.3691e-02,  3.8037e-01, -6.3782e-03,
        -1.2122e-01, -3.2227e-02, -2.5781e-01,  1.0300e-02, -1.8677e-01,
        -2.7637e-01,  5.0140e-02, -1.7896e-01,  6.5918e-03, -2.8345e-01,
        -4.0039e-02,  1.9104e-01, -1.0577e-01, -2.3535e-01, -4.9927e-02,
         2.1741e-01,  3.5986e-01, -3.2715e-01, -3.4204e-01, -2.9395e-01,
         1.1938e-01, -5.7520e-01,  1.7651e-01, -5.8545e-01, -9.8267e-02,
        -2.2021e-01, -1.5833e-01, -5.4053e-01,  4.5825e-01,  1.5710e-01,
         1.2720e-01,  2.2583e-01, -1.7383e-01,  2.6123e-01, -3.5645e-01,
         4.7070e-01,  1.3208e-01,  2.4463e-01, -1.8042e-01, -1.6919e-01,
        -9.6924e-02, -3.1464e-02,  3.0884e-02, -1.4136e-01, -3.4595e-01,
         8.3008e-02, -2.4414e-02,  7.1228e-02, -4.0039e-02, -6.2744e-01,
        -3.6621e-03,  3.2349e-02, -3.4595e-01,  2.1484e-02, -2.5464e-01,
        -8.8379e-02, -1.3916e-01,  1.6931e-01, -1.4331e-01, -1.5259e-01,
        -6.4941e-01, -2.7100e-01, -3.3179e-01, -1.2549e-01, -3.2520e-01,
         1.2018e-01,  8.1421e-02, -9.5337e-02,  2.3682e-01,  1.4062e-01,
        -2.6855e-03, -2.5537e-01, -2.8101e-01,  1.3354e-01, -3.4668e-02,
         8.8379e-02,  2.0825e-01,  2.2205e-01, -1.9861e-01,  1.4185e-01,
         3.1738e-02, -1.1768e-01,  4.1748e-02,  2.8174e-01,  9.7168e-02,
         2.5757e-02,  1.6602e-01,  2.1191e-01, -8.4229e-03,  3.2520e-01,
        -1.2421e-01,  1.3269e-01, -1.4478e-01, -1.0518e+00,  1.0620e-01,
         1.7151e-02,  2.3486e-01, -8.7402e-01, -3.6011e-03, -1.1169e-01,
         1.0492e-01, -4.1162e-01,  1.8555e-02, -3.3740e-01,  3.4326e-01,
        -3.2642e-01, -6.1279e-02,  3.7170e-02,  3.5840e-01,  1.7029e-01,
         4.6509e-02, -1.0864e-02,  3.1152e-01,  4.4062e+00, -2.8125e-01,
        -7.4707e-02, -2.4609e-01,  1.7310e-01, -1.3403e-01, -1.2372e-01,
         1.6968e-01,  8.3679e-02,  1.6235e-02, -1.8994e-01, -1.9958e-01,
         1.6602e-01,  2.4304e-01,  6.9385e-01,  1.3623e-01,  2.5977e-01,
         1.6626e-01,  1.4233e-01,  1.0254e-02,  1.1548e-01, -2.8564e-02,
         3.2410e-02, -4.6484e-01,  6.1768e-02, -2.1497e-01,  2.6782e-01,
         1.9971e-01, -1.9751e-01,  2.1826e-01,  5.3760e-01,  2.1777e-01,
        -1.3245e-01, -1.5063e-01,  1.1523e-01, -3.0493e-01, -2.9224e-01,
        -9.5581e-02,  1.7896e-01, -1.1792e-01, -1.0132e-01, -3.7598e-02,
         1.3306e-02,  5.6152e-02,  1.4185e-01,  1.8481e-01,  1.9043e-02,
        -4.1504e-03, -5.3711e-02,  8.6426e-02,  3.0127e-01, -4.5410e-02,
         4.6753e-02, -9.9731e-02,  3.9258e-01,  1.4929e-01,  1.1902e-01,
        -1.4221e-01, -2.4463e-01,  2.2437e-01, -2.3328e-01, -3.3667e-01,
        -1.7065e-01,  1.9019e-01, -1.2476e-01,  2.6221e-01, -3.8086e-01,
        -1.3013e-01,  1.4185e-01,  8.2886e-02, -3.8086e-02,  2.2449e-01,
        -4.5117e-01,  3.6987e-01, -3.7207e-01, -6.4697e-03, -6.9946e-02,
        -2.0361e-01,  1.4557e-02,  6.6040e-02,  2.0361e-01,  9.5312e-01,
        -1.5234e-01,  1.5869e-02, -3.9746e-01,  1.8835e-01,  1.3916e-02,
         2.8564e-02, -6.9702e-02,  6.3354e-02,  2.6611e-02, -1.4771e-01,
        -1.0687e-01,  3.5620e-01,  3.6890e-01,  4.9103e-02,  3.5010e-01,
        -3.4180e-02, -4.8022e-01,  4.8096e-02,  4.5752e-01, -2.6489e-01,
        -1.2646e-01,  3.1689e-01, -1.7334e-02, -2.5635e-01, -2.9980e-01,
        -2.2705e-01,  2.7026e-01,  2.7197e-01,  1.7468e-01,  2.9761e-01,
         4.1138e-02,  5.4004e-01,  2.1692e-01,  3.9404e-01,  2.6562e-01,
        -2.7393e-01,  1.8799e-01, -8.5449e-02,  2.4902e-02, -1.7212e-02,
         9.8816e-02,  1.9006e-01,  2.1021e-01,  5.4688e-01, -2.6855e-02,
         1.3745e-01,  6.1084e-01,  3.0762e-02, -1.0620e-01, -3.6450e-01,
        -2.8906e+00,  1.4563e-01,  1.6943e-01,  9.4238e-02, -1.0718e-01,
        -4.3213e-02, -1.4771e-01, -1.3306e-01,  1.8428e+00, -5.5371e-01,
        -3.7079e-02,  3.0078e-01, -1.5552e-01, -8.8867e-02, -4.6387e-02,
        -1.7554e-01, -2.3453e-02, -1.8896e-01, -5.8105e-02,  4.7791e-02,
        -1.9519e-01, -1.2488e-01, -2.8076e-02, -2.3071e-01, -3.1079e-01,
         3.7256e-01, -3.2056e-01, -1.0376e-02, -1.9238e-01,  1.5869e-03,
        -6.5369e-02, -1.5234e-01, -1.3965e-01,  3.1689e-01, -4.9072e-01,
        -2.5562e-01,  1.0889e-01, -2.1484e-02,  3.1958e-01, -1.0120e-01,
        -3.8013e-01, -1.0156e-01,  8.7158e-02, -1.4990e-01,  1.7090e-03,
         2.3218e-01, -2.7930e-01,  2.6270e-01,  5.2588e-01, -2.6764e-02,
        -4.6777e-01,  2.6855e-03, -1.9995e-01, -9.4177e-02,  4.4434e-02,
        -1.6345e-01,  1.0986e-02, -3.0518e-03, -1.0358e-01, -8.9722e-02,
         2.4927e-01, -4.8645e-02], device='cuda:7', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor(1.6992, device='cuda:7', dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([-1.4145e-02, -4.5227e-02, -9.3613e-03, -2.4780e-01,  3.8757e-03,
        -5.7556e-02, -8.5449e-04, -6.5430e-02, -3.6255e-02, -4.9683e-02,
        -2.2232e-02,  3.8147e-03, -3.7048e-02, -2.1423e-02, -1.7197e-02,
        -1.0506e-02, -3.5278e-02, -2.6794e-02, -7.8735e-02, -7.9803e-03,
        -2.2545e-03, -5.3864e-02, -3.2257e-02,  4.3640e-02, -3.8834e-03,
         2.2217e-02, -1.1646e-01, -1.2250e-01, -2.5208e-02, -1.1285e-01,
         3.0930e-02, -3.8940e-02, -2.4231e-02, -4.9866e-02, -6.6895e-02,
        -6.0120e-03, -8.6975e-04, -3.5919e-02, -2.2247e-02, -4.8737e-02,
        -8.1909e-02, -2.5024e-03, -1.4099e-01, -1.9775e-02, -2.5726e-02,
        -1.2585e-01, -1.5541e-02, -7.4615e-03, -2.5345e-02, -3.4180e-02,
        -4.9316e-02, -9.6130e-03, -1.0205e-01, -5.3925e-02, -4.8157e-02,
        -1.3077e-02,  1.4099e-02, -1.8616e-01, -4.3640e-02, -8.5144e-03,
        -1.2772e-02, -3.1586e-02, -1.2421e-02, -8.9111e-02,  4.5776e-03,
        -9.8755e-02, -2.4506e-02, -2.9877e-02, -4.0283e-02, -9.8267e-02,
        -6.6711e-02, -6.1188e-03, -9.7717e-02, -1.7242e-02, -8.7036e-02,
        -1.6174e-03, -8.8989e-02, -8.4534e-03, -5.8746e-03, -1.1528e-02,
        -7.4539e-03, -1.9684e-02, -1.6052e-02, -1.5015e-02, -2.3315e-02,
        -3.0518e-04, -2.8687e-02, -5.0293e-02, -2.7374e-02, -1.1597e-02,
        -1.8311e-04, -7.7209e-03, -4.5898e-02, -5.5237e-03, -2.5864e-03,
        -8.0139e-02, -4.2145e-02, -9.3750e-02, -3.5553e-03,  5.2490e-03,
        -1.3794e-02, -3.7903e-02, -2.2314e-01, -2.0172e-02,  2.5574e-02,
        -3.8818e-02, -2.7954e-02, -7.5455e-03, -2.2263e-02,  3.3752e-02,
         6.3477e-03,  2.8870e-02, -3.2867e-02,  7.6904e-03, -4.0466e-02,
         9.0332e-03, -6.0547e-02, -1.9958e-02, -1.8250e-02, -1.8768e-02,
        -1.1145e-01,  1.8616e-03, -2.1179e-02, -4.1229e-02, -3.6316e-02,
        -4.3945e-02, -6.2988e-02,  9.1095e-03, -7.1777e-02, -2.4719e-02,
         4.3335e-03, -1.1139e-02, -6.1462e-02,  7.6294e-03, -2.8870e-02,
        -3.9795e-02, -1.5244e-02, -1.7090e-02, -1.0254e-02, -9.4757e-03,
        -9.1553e-03, -3.1891e-03, -1.3519e-02, -3.4546e-02, -4.0039e-02,
        -5.2582e-02, -5.7650e-04, -4.5898e-02, -2.2522e-02, -1.1497e-02,
        -7.3486e-02, -1.3870e-02, -3.0518e-03, -3.1952e-02,  3.4729e-02,
        -8.4229e-03,  7.5317e-02, -7.2876e-02, -1.8005e-02, -1.2207e-02,
        -8.1940e-03,  2.2888e-02, -1.6895e-01, -1.2497e-02, -1.0645e-01,
        -5.8746e-03, -2.2156e-02,  1.6586e-02,  1.4343e-03, -8.7357e-04,
        -3.9444e-03, -4.7607e-02, -7.8369e-02, -4.0039e-02, -4.5898e-02,
         3.0396e-02, -2.3926e-02, -7.9041e-03, -1.3641e-02, -7.3357e-03,
        -5.1239e-02, -1.0986e-03, -9.1553e-03, -5.7068e-02, -1.4999e-02,
        -7.0801e-02, -2.3193e-03, -2.4567e-03, -2.1729e-02, -2.5787e-03,
        -2.3026e-02, -8.6060e-03,  1.8021e-02, -1.1154e-02, -6.9641e-02,
         2.4994e-02, -6.1493e-03, -2.6642e-02, -2.8259e-02, -1.9531e-02,
        -1.4404e-02, -2.8809e-02, -5.1270e-03,  1.4954e-03, -2.1561e-02,
         3.3020e-02, -7.2479e-05, -1.2329e-02, -2.3041e-03, -2.8412e-02,
         9.5825e-03, -4.2877e-03,  3.2623e-02, -3.7292e-02,  9.0637e-03,
        -5.1727e-03, -4.5380e-02, -9.6069e-02,  6.8665e-03,  5.2185e-03,
         2.1240e-02, -5.7983e-03,  7.1182e-03, -4.3243e-02, -6.2134e-02,
        -5.3711e-02, -7.9498e-03, -1.6846e-02,  1.3550e-02, -1.0461e-01,
        -6.7627e-02,  1.1548e-01, -4.2725e-02, -4.5319e-02, -3.5034e-02,
        -1.2306e-02, -1.4465e-02, -4.9561e-02, -3.5767e-02,  1.3504e-02,
        -4.8828e-03, -2.0752e-02, -2.4643e-02,  9.4757e-03,  1.4496e-03,
        -3.5645e-02, -3.8116e-02, -2.6001e-02, -2.1469e-02,  1.7395e-03,
        -6.1035e-04, -5.3436e-02, -5.7526e-03, -1.4465e-02, -1.7224e-01,
        -1.5274e-02], device='cuda:7', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor(-0.0988, device='cuda:7', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 693, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 548, in _per_task_train_loss
    print("task_net", name, param.requires_grad, param.grad[-5])
IndexError: index -5 is out of bounds for dimension 0 with size 1

/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-09-02 21:56:37 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:10943
2023-09-02 21:56:37 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:10943
2023-09-02 21:56:37 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:10943
2023-09-02 21:56:37 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:10943
2023-09-02 21:56:37 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-09-02 21:56:37 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:10943
2023-09-02 21:56:37 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-09-02 21:56:37 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:10943
2023-09-02 21:56:37 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-09-02 21:56:37 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:10943
2023-09-02 21:56:37 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-09-02 21:56:37 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:10943
2023-09-02 21:56:37 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-09-02 21:56:38 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-09-02 21:56:38 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-09-02 21:56:38 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:56:38 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-09-02 21:56:38 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:56:38 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 5
2023-09-02 21:56:38 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 0
2023-09-02 21:56:38 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:56:38 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 7
2023-09-02 21:56:38 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:56:38 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 4
2023-09-02 21:56:38 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:56:38 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 2
2023-09-02 21:56:38 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:56:38 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 6
2023-09-02 21:56:38 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:56:38 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 21:56:38 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 3
2023-09-02 21:56:38 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 1
2023-09-02 21:56:42 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10943', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=False, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=False, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=False, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-09-02 21:56:42 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-09-02 21:56:42 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-09-02 21:56:42 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-09-02 21:56:42 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-09-02 21:56:42 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-09-02 21:56:46 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-09-02 21:56:46 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-09-02 21:56:46 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-09-02 21:56:47 | INFO | root | load pretrained hubert
2023-09-02 21:56:54 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-09-02 21:56:56 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-09-02 21:57:01 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-09-02 21:57:01 | INFO | root | share the sematic adapter and textual encoder
2023-09-02 21:57:01 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-09-02 21:57:01 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-09-02 21:57:01 | INFO | fairseq_cli.train | model: S2TJoint
2023-09-02 21:57:01 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-09-02 21:57:01 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-09-02 21:57:01 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-09-02 21:57:01 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-02 21:57:01 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 21:57:01 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 21:57:01 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-02 21:57:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-09-02 21:57:21 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-09-02 21:57:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-09-02 21:57:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-02 21:57:22 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:57:22 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:57:22 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:57:22 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:57:22 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:57:22 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:57:22 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:57:22 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 21:57:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-02 21:57:22 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-09-02 21:57:22 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-09-02 21:57:22 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_last.pt
2023-09-02 21:57:24 | INFO | fairseq.trainer | load the task parameters
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
2023-09-02 21:57:40 | INFO | fairseq.optim.adam | using FusedAdam
2023-09-02 21:57:41 | INFO | fairseq.trainer | Loaded checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_last.pt (epoch 34 @ 50000 updates)
2023-09-02 21:57:42 | INFO | fairseq.trainer | loading train data for epoch 34
2023-09-02 21:57:42 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-02 21:57:42 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 21:57:42 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 21:57:45 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-02 21:57:47 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
2023-09-02 21:58:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 21:58:28 | INFO | fairseq.trainer | begin training epoch 34
2023-09-02 21:58:28 | INFO | fairseq_cli.train | Start iterating over samples
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
task_net module.module.task_net.layer_norm.weight True tensor([-0.7480, -0.7930, -0.1367,  0.1099, -0.0688], device='cuda:0',
       dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([-0.9102, -0.5195,  0.6211,  0.0979,  0.1099], device='cuda:0',
       dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.1724,  0.1626,  0.0784,  ..., -0.4067, -0.7427, -0.2292],
        [ 0.9331, -0.2539,  2.1719,  ..., -2.3105, -2.1367, -1.0488],
        [-0.3271, -0.3894,  0.2124,  ..., -0.7710,  0.8638, -0.9336],
        [ 0.1206,  0.0342,  0.0579,  ...,  0.4521, -0.1655,  0.3591],
        [-0.9565,  0.5210, -2.0859,  ..., -0.4087,  2.6953, -2.9453]],
       device='cuda:0', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 3.4883,  1.5938, -2.8359,  0.5898, -7.3516], device='cuda:0',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 0.0355,  0.0613, -0.0469,  ..., -0.0413,  0.0117, -0.0029],
        [ 0.0126,  0.0050, -0.0272,  ..., -0.0621, -0.1294, -0.0121],
        [-0.0214, -0.0120,  0.0361,  ...,  0.0190,  0.0986,  0.0135],
        [-0.0113, -0.0230,  0.0086,  ..., -0.0649, -0.1343, -0.0137],
        [ 0.0023,  0.0086,  0.0034,  ...,  0.0412,  0.1194,  0.0090]],
       device='cuda:0', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([-0.1602, -0.1580,  0.1514, -0.0599,  0.0583], device='cuda:0',
       dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[  8.7656,  17.2500,  -3.1094,  12.4375, -10.5312,  15.3125,  14.3438,
         -14.9062, -21.8750,   6.8125, -15.0000,  -5.9219,  -7.5156, -21.4375,
          -1.7734,  14.6250, -14.1250,   5.1406, -29.8750,  -0.0508,  19.3750,
          -3.5742,   7.9844,  11.7188,   1.0000,  -1.3828, -10.2812, -28.6875,
          28.3125,  19.0625, -13.5312,   4.9844, -35.6250,  11.4062,  -2.7969,
           6.6094,   3.1172, -19.8438, -15.6562,  16.3750, -21.2500,  -6.9922,
          -3.0469,   5.1094,   4.2344, -10.0156,  13.8125,  23.2500, -17.2812,
           6.5938,   2.2656,  -2.8984,  10.0312,  11.3047,  14.2812,  18.0000,
          18.0625,  19.4375,  19.9375,  18.3125,  -0.2812, -16.9062,   7.9375,
         -19.8438, -10.6562,  13.0625,  -1.6250,  20.0000,   8.3438, -15.0312,
          21.5625, -23.9375, -12.9062,  11.6250,   9.9688,  10.9375,  20.1250,
           5.7812, -12.5938,   4.2188, -10.6562, -24.3125,  -1.2188,   6.8438,
           0.3281, -13.2578, -17.8750, -13.8438,  10.1562,  -1.0039, -22.1875,
         -17.7500,  20.0312,   4.1094, -13.4062, -12.1562, -22.0000, -21.6250,
          -8.0000,  15.7188, -21.8750,  -0.1719,  -7.7500,  -0.4062,   4.4531,
         -12.8906, -14.3125, -11.8125,  22.5000,  -4.5625,  -1.1289, -25.6250,
          -9.3125,   8.2812, -24.1250,  -1.7344,   9.0156,  14.5938,  15.0312,
           5.3672, -12.7188,  -7.4375,   1.7812,  14.2812,   3.0938, -12.0625,
          13.4062,  -4.1875,  -9.3750,   3.2031,  22.3125,   7.6523, -14.8125,
         -10.0156,  -8.1250,  -9.3438, -24.5625,  -7.5391,  18.2500, -16.0625,
          16.1875,  -1.9219,  23.8125,   2.2344,  20.1875, -25.3125,   1.3027,
           3.2988,  -4.7344, -15.1562, -21.3438,  -9.9922,   1.6094,   1.4102,
           0.2358, -11.7500, -11.5625, -16.9844,  11.3750,  14.0938,  16.9375,
          16.8125,  16.1875,   3.1250,   3.2656,   1.8125,  11.6562,   9.3906,
         -12.0312,  -7.8125, -11.9375,  15.0312,  15.8750,  -8.2969,  -2.1152,
          -4.6719, -10.3750,  21.5312,   5.6406,   7.2500,  12.6875, -21.5000,
           4.2344,   7.1094,  13.6875,   4.6406,   1.0156,  -1.5156, -32.1250,
           4.5625,  -3.3438,  -9.9688,  -3.8125,   3.5938,  -4.2109,  -8.1250,
          -4.7891,  15.5312,  -3.3125,   7.3438,  10.1562, -20.7500, -12.7500,
           6.2344,   9.7969,  -6.5625,  -0.5391,  17.0938,  12.6875,  10.6406,
           1.8398,  -8.5938, -22.8125, -24.5625,  -9.1875,   5.6875, -25.0000,
           7.6875,  -5.1484,  16.7812,   2.8125,  -4.3438,  10.9375, -11.9375,
          -4.4219,  -9.1562,  22.8750,  14.5625,   9.2266,  10.1406, -16.9375,
         -18.8750,  -2.9844,   1.2500, -17.2500,  13.2500,  -5.6719,   4.3438,
           3.9844,   9.6562,   3.3750, -11.4062,  -6.0469,  27.3125,   2.7969,
          17.1250, -23.1250,  13.8750,  -0.1875,  13.4375, -13.4688,   6.0156,
          10.0625,   3.0469,  27.8750,   9.7500, -22.7500, -10.2500,  -4.3906,
         -11.4766,  16.7812,  -7.7812,  -5.4492,  -9.4375,   6.5781,  18.3125,
          12.7656,   5.0469,  18.0625,   5.6094, -23.6875,  -0.0938,   4.6875,
          -7.6719,   4.1523,   7.4844, -16.8125, -10.9062, -10.8125,  10.7969,
          15.8750, -14.9688,  -0.7969,  -8.8594,  16.5625,  19.0625,   6.7344,
         -33.8750, -12.4688,   3.9922, -27.3750, -13.7500,  14.9375, -11.9375,
         -25.3125, -11.3750, -21.9375,  -2.1016, -13.5781, -16.7500,  -2.1289,
         -18.8125,  -7.6562, -27.1250, -25.8125,  -7.5312,  -0.6328, -18.4062,
           8.6875,   6.2812,   3.7656,  -4.3438,  19.3750, -17.8125, -18.2500,
          18.5000,   9.2812, -10.8750, -20.6562,  11.5312,  -2.2656,   4.3281,
         -11.7188,  20.4531,  23.5312,   0.5000,  21.0000,   0.0781,  18.4375,
          14.6562,  14.2500,  26.9375,  12.2188,  23.1562,  -1.4766, -12.8438,
          -5.5469, -12.0625,  -1.0742,  13.1875, -26.1875,   1.0469,  19.4375,
           8.5000,   3.4141, -27.2500,  -8.2188, -20.3750,  11.8750,   6.7031,
          -2.1016,   1.8594,   5.4688,  -3.7500,  -6.1875,  20.3125,  -4.3477,
          -7.1250,  15.8438,   8.8750,  26.2500, -11.7969, -14.6250, -14.6875,
          -1.5781,   7.4688, -21.5625,  -5.2344, -24.8125,  10.2500,  13.1875,
           8.6250,   1.5625,  29.4375, -15.9062,  -5.1484,  19.9375,  -5.0156,
          -9.5000,  -9.4688,   9.3750, -16.3750,  20.1875,  20.8750,  12.9844,
         -20.4375,  -2.9375,  -0.6484,  15.0781,  24.3125,   2.9648,  25.5000,
          15.8750, -13.4062,  14.1250,  -4.0469,  -1.0469, -17.6875,  13.2188,
         -22.9375, -15.2500,  20.1875,  -5.0938,  -5.7344,  -1.5078, -21.4375,
          -1.5078, -10.5469,  -6.5156,   0.2695,  -8.2656, -12.9531,  -3.3125,
          30.9375, -19.7500,  12.2969,  -8.2656,  12.2812,  -9.7500,   4.4844,
          16.6250,  10.2500, -18.1250,  17.7500,   9.1562,  12.5000,  10.5312,
          -9.7734, -20.0625,   3.5000,  21.9375, -11.7812,  17.8125, -21.8125,
          16.9375,   1.2891,  -6.1406,  17.9375,   4.7656,  13.7188,   0.3984,
           6.7812,   0.4531, -19.3438,  14.5000, -15.0938,   4.7188,  15.3125,
          -2.2656,  14.4062,  32.2500, -32.6875, -13.5625,  -9.7500,   8.1094,
          -6.3047, -23.6250, -18.7500, -29.4375,  21.3125,  -5.0000,  -9.8438,
          14.1875, -19.1875, -10.3438,  -5.9844,   8.9297,  11.0625,  23.3750,
          10.2188,  10.5000,  10.0625,  12.5000,  15.6875, -13.6250, -12.4375,
          21.3125, -12.7812,  26.6250,  24.3750, -20.4375,  25.5000, -13.0000,
           8.0625,   7.7969,  -8.9219,  -0.3984,  13.3125,   5.3672,   9.0000,
          19.6875,  10.6875,   6.8125,  19.8750, -10.0625,  -8.1562,  -0.9219,
          17.8125, -14.2969,  13.2188,  12.2812, -24.0625,  -9.2500,   2.9531,
          13.7812,  -5.1562,  12.0625, -10.0625,  30.1875, -21.6250,  -1.3594,
         -12.6562]], device='cuda:0', dtype=torch.float16)
2023-09-02 21:58:38 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/crash.pt
task_net module.module.task_net.layer_norm.weight True tensor([ 0.8164,  1.0391,  0.5732, -0.1436,  0.0889], device='cuda:6',
       dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([ 1.2500,  0.7109, -0.8516, -0.1338, -0.1509], device='cuda:6',
       dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.6440, -0.0345, -0.1127,  ..., -1.2383, -0.3826, -0.6440],
        [ 4.0430,  0.8926, -0.1094,  ..., -2.9570, -0.9844, -0.6113],
        [ 0.2500,  0.8286, -1.1602,  ..., -0.6265,  0.9111, -1.6738],
        [-0.4319, -0.3499,  0.0151,  ...,  0.0703, -0.4336,  0.8022],
        [-1.7354,  2.4277, -4.2227,  ...,  1.0645,  4.4922, -3.3164]],
       device='cuda:6', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 8.2031, 18.9688,  0.7969,  0.7109, -0.1250], device='cuda:6',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 5.7739e-02,  2.4219e-01,  9.6436e-03,  ...,  6.9531e-01,
          1.2031e+00,  2.8076e-02],
        [ 1.8570e-02,  7.4951e-02, -1.8524e-02,  ...,  2.6416e-01,
          4.0918e-01, -1.8341e-02],
        [-4.2450e-02, -1.6992e-01, -1.9287e-02,  ..., -4.0869e-01,
         -7.8027e-01, -6.5308e-03],
        [-1.6953e-02, -1.0626e-01, -1.7456e-02,  ..., -2.3376e-01,
         -4.9561e-01, -2.5665e-02],
        [ 1.9550e-04,  1.5701e-02, -1.8215e-04,  ...,  4.2297e-02,
          1.3660e-01,  1.6098e-02]], device='cuda:6', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([ 0.3750,  0.0322, -0.2646, -0.2700,  0.0792], device='cuda:6',
       dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[ -8.9062, -22.2500,  13.4375, -16.3125,  13.8750, -36.3750, -23.9375,
          -7.1875,  27.2500, -12.8750,  11.2500,   7.5938,   8.3750,  28.2500,
           1.2500, -21.1250,  27.6250,  -0.5469,   7.5000,   3.4531,  -7.3750,
         -13.7812,   0.0781,  -4.8125,  24.4375,  -2.4609,   8.5000,   6.5000,
         -11.6250,  -9.1875, -13.4375,  -6.6875,  34.7500,  -1.5000,  21.2500,
          -2.6875,  14.4219,   7.5625,   4.7500, -24.0000,  14.0625,  -1.7188,
           5.8438,  -8.8438,  -1.0156, -27.2812, -11.3125, -13.7500,   3.8125,
          -0.9375, -15.9062,  -3.8555,  -0.5625,   6.5625,   5.0000, -11.0000,
         -24.3750, -29.2500, -21.8750, -14.8750,  -3.1328,  21.1250,  -6.4375,
           7.6250,  15.1875, -23.9375,   5.7969, -15.7500, -14.6250,   8.5625,
         -12.0000,  14.0000,  10.6875, -23.1250,  -6.8750, -14.5000, -16.8750,
          -8.5938,  13.3125,   3.8438,   1.4375,   7.5000,   5.2344,  -6.5625,
          16.6875, -10.6875,  22.1250,  -6.2500, -16.6875,  -1.3701,  13.2500,
          22.0000, -13.8750,   7.8906,  16.2500,  -4.9062,  17.5000,  13.6250,
         -14.7344, -11.7500,  11.6250,  21.7812,   9.6250, -11.5000,  -2.6875,
         -10.1250,  11.3750,   1.2500, -23.8750, -18.0312,   2.9062,  14.5000,
           1.0312,   1.6562,   6.0000,  10.6562,  -1.5625,   5.6250, -19.8750,
          25.6719,  -9.0938,  18.1250,  27.8438, -21.9375, -17.0625,   2.6875,
           1.0000,   5.2188,  14.3750,  -2.0625,  -8.0000,  18.5156,  12.1875,
           9.1875,   7.6875,  11.0000,  26.7500,  -4.7031, -10.7500,   8.5000,
         -10.7500, -17.3438,  -7.8750,  -7.1094, -19.2500,  13.5000,   4.5664,
           5.9219,   2.9062,  15.6250,  10.9375,  -4.6562, -11.3125,   3.1914,
          -2.6094,  -6.8750,   6.3750,  -9.9375,  -8.3750,  -8.8125, -14.5000,
         -23.2500, -21.3750, -12.5312,  13.8594, -22.7188, -23.0000,  -4.0000,
          14.0000,  10.5625,  20.6875,  -3.4375, -12.4375,  -4.5312,  -6.2969,
         -22.9531,   0.6250,  -8.5625,   9.7734,  -1.5938, -11.1250,  20.6250,
         -11.8750,   6.4258,   2.5625,  -3.0312, -11.8750, -13.5156,   8.0000,
          28.5312,  28.2500,  13.6250, -24.3125, -13.7500, -15.6016,   0.0000,
         -16.2656,   0.8750,   7.6250,   3.6406,  -2.8438,   6.7500,  23.8750,
           4.1406,   4.6719,  23.3125, -11.7344, -23.2500, -24.5000,  18.9531,
           5.2266,   5.6562,  17.0000,  13.1250,   5.8750, -11.7500,  12.2500,
           3.0312,  -4.8594, -15.0000,  -4.0469,   1.6562, -15.8125,  19.3125,
           6.0000,  -6.0312,  -7.6250, -11.0000,  10.1016,  -1.7188,  17.7500,
          18.7500,   0.3750, -31.8750,  23.6250, -14.8750,  12.2812,  -5.6875,
          -7.3281,  -4.3750,  19.0938,  22.5625,  10.5312,  -7.3750,  22.1562,
         -21.0000,   8.0000, -16.2500, -29.6250,  -6.4375,  10.5000,  11.2812,
          -5.3750,  18.0000, -20.8750, -15.0625,  15.8750,   5.1250,   0.1250,
          -9.3750,   2.1250,  14.0625,  -1.0938,   8.4375,   0.1562, -15.1250,
           3.5000,  -5.8438, -21.7500,  -3.1875,   8.7500, -16.5000,  -1.4844,
          -2.7812,  16.6406,  13.6562,  14.1875,  16.5625,   5.0000,  -4.8750,
          -2.7500,   0.6875,  -9.6641,   3.6562, -19.3750, -17.0000,  -2.2656,
           8.5000,   7.6250,  -1.0000,  11.0000,  25.3750,  -7.0000,  16.5000,
          10.8750,  16.7500,  18.8750,   5.9844,  -6.9375,  10.5625,   1.3984,
         -11.2500,  15.0938,  28.3750,  19.5000,  12.1875, -12.9688,  17.8125,
           7.0312,  -7.0000, -17.8125,  10.7812, -18.6250,  24.1250,  19.3125,
           5.9375,  -6.2500,  28.9375,   4.3750,  -7.3750,  17.2188,  -4.7188,
           8.7500,  19.0312,  -5.3750,  -4.9844, -10.8750, -22.1562, -23.7500,
         -16.7500, -15.2500,  -9.0000, -13.3125,  -6.3750,   3.2188,  -1.6875,
          10.9688,  14.1875,  -4.0586,  -0.5000,  12.2500, -12.1875,   5.3750,
           2.1875,  -1.2656,  15.0000,   1.1562,  40.2500,   3.6875,  -5.2500,
           4.8281,   1.0625, -22.7500,  -4.5078,  17.2500, -10.5000, -14.4766,
          -8.2500, -11.3750, -10.9375,  -8.5000,   2.0625,  20.5000,   3.5625,
          13.8438,  -4.6562,   6.2500,   3.7500,  22.1250, -21.1250,  -3.7500,
          -4.2500,  16.6719,  -8.7500,  17.6250, -15.3828,   3.2500,  12.5312,
          23.3750, -10.2031,  -4.0625,  11.4375,  -2.2500, -16.3750,  -0.3750,
          35.7500,   3.2500, -10.6719,  13.4375, -10.7500,   1.9062, -28.2500,
         -17.7500,  18.3750, -19.0000,  12.5312,  -1.4062,  24.1875,  -4.7500,
          18.6250,  25.5625, -14.3750,  17.8750,  -4.5938,  -4.6328,  14.8750,
           2.5781,  -7.1875,  -0.1406,  -1.2715,   0.5625,  -1.5000, -29.2188,
          -6.1250,  20.8750,  17.8125,   0.7500,  -3.8125,  -4.0312,  14.2344,
         -24.0000, -32.3125,  19.1250, -31.6250, -16.3750, -20.1250,  -5.8750,
          -1.5469,  14.6250, -29.9375, -25.2500,  21.0000, -19.8750,  11.3750,
         -24.5000,   0.7656,   7.3125, -13.3750, -10.0312,   1.0000,  -2.9844,
         -19.8750,  12.3750,  11.0625, -24.4375,   1.1875,  -1.1797, -28.8750,
           6.2188,  -0.8750, -54.5000,  27.1250,  15.6250,   8.5000,  10.7109,
         -23.8438,  18.6250,  14.8750,  19.6250,  -7.6875,  10.4688,   3.4688,
         -17.2500,  16.2500,  -2.4688,   1.3750,   7.5625, -15.0625,  -2.1250,
          -6.1250,   5.6562,   1.6875,   7.5938, -12.5625,  30.5000,  12.0000,
         -13.2500,  16.3750, -16.0000, -31.0000,  14.5000, -10.8750,  22.2500,
           8.1719,   1.3438,   0.2188,  -2.3379,   0.0000,  14.3594, -17.3750,
          18.9688, -29.2500,  -9.9688,  -8.0000,   6.8125,  16.1250, -14.7500,
          -8.3125, -13.0000,   3.3125,  10.2188,  13.7500,  17.4375, -18.0312,
          -2.2500,   3.0625,  -2.6250,   2.1250, -18.0000,  11.5000,  -8.8750,
           1.0625]], device='cuda:6', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
task_net module.module.task_net.layer_norm.weight True tensor([ 0.5781,  0.7812,  0.2305, -0.1252,  0.0402], device='cuda:3',
       dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([ 0.9629,  0.5498, -0.6562, -0.1033, -0.1165], device='cuda:3',
       dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[-0.1475,  0.2778, -0.1736,  ..., -0.5708, -0.2993,  0.2412],
        [ 0.1365,  0.3281, -1.5742,  ..., -2.1211, -0.6890, -0.4756],
        [ 0.0493,  0.3018, -0.4575,  ...,  0.2593,  0.1201, -0.2546],
        [ 0.0282, -0.0801,  0.2664,  ...,  0.0590, -0.1844,  0.1885],
        [-0.6865,  0.4556, -0.9277,  ..., -0.9995,  0.6763, -1.2715]],
       device='cuda:3', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 4.4883, 11.8281,  1.8398, -0.4707,  1.8438], device='cuda:3',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 3.6041e-02,  1.5112e-01,  9.2773e-03,  ...,  1.7798e-01,
          4.4629e-01,  2.0020e-02],
        [ 1.0178e-02,  4.9866e-02,  2.4872e-03,  ...,  6.1035e-02,
          9.6191e-02, -1.0262e-02],
        [-2.4994e-02, -9.2529e-02, -1.2512e-03,  ..., -1.1548e-01,
         -3.1055e-01, -1.7181e-02],
        [-1.1879e-02, -4.8370e-02, -9.7809e-03,  ..., -5.7495e-02,
         -2.3059e-01, -3.9032e-02],
        [-1.3351e-05,  6.9313e-03,  3.0117e-03,  ...,  1.8295e-02,
          7.1350e-02, -6.1369e-04]], device='cuda:3', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([ 0.2368,  0.0332, -0.1709, -0.1473,  0.0250], device='cuda:3',
       dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[-6.0938e+00, -1.6750e+01,  5.3203e+00, -1.4375e+01,  7.3750e+00,
         -2.7125e+01, -1.2594e+01,  1.7031e+00,  2.3625e+01, -9.2344e+00,
          1.3031e+01,  5.1875e+00,  6.9531e+00,  1.8812e+01,  2.6953e+00,
         -1.2969e+01,  1.6719e+01, -1.1719e-02,  1.6438e+01,  3.7266e+00,
         -8.2188e+00, -5.3945e+00, -3.1406e+00, -4.8594e+00,  8.0312e+00,
         -1.7031e+00,  5.9531e+00,  1.1562e+01, -2.3375e+01, -9.8125e+00,
         -6.4062e-01, -3.5156e+00,  3.4188e+01, -6.0781e+00,  5.5938e+00,
         -5.7656e+00,  2.9824e+00,  9.5312e+00,  6.5000e+00, -1.3438e+01,
          1.2062e+01,  5.3906e-01,  3.0312e+00, -4.6094e+00, -4.3750e+00,
         -6.1875e+00, -9.9375e+00, -1.2469e+01,  7.4062e+00, -6.3438e+00,
         -7.4531e+00, -1.6992e+00, -3.5625e+00, -4.6875e-01, -2.9844e+00,
         -8.4062e+00, -1.5531e+01, -1.5438e+01, -1.4281e+01, -9.6250e+00,
          2.0664e+00,  1.2688e+01, -7.8125e+00,  1.2500e+01,  6.8125e+00,
         -1.4438e+01,  1.4688e+00, -1.6656e+01, -1.3094e+01,  1.2469e+01,
         -1.0812e+01,  1.3031e+01,  7.4688e+00, -1.6344e+01, -8.7500e+00,
         -6.0156e+00, -1.3062e+01, -7.9375e+00,  1.0188e+01,  1.5059e+00,
          3.3438e+00,  1.5000e+01,  2.1914e+00, -3.4688e+00,  7.2812e+00,
         -1.2344e+00,  1.6719e+01,  3.7656e+00, -8.7812e+00, -1.2178e+00,
          1.4156e+01,  1.5719e+01, -8.6562e+00, -3.3828e+00,  1.4156e+01,
          4.9531e+00,  1.6250e+01,  1.6656e+01, -1.1172e+00, -6.5312e+00,
          1.2469e+01,  8.2656e+00,  8.1250e+00, -9.0625e+00, -3.2344e+00,
         -2.8438e+00,  8.5000e+00,  8.0000e+00, -1.7438e+01, -6.5117e+00,
          2.1582e+00,  1.3750e+01,  4.2812e+00, -3.1406e+00,  1.2938e+01,
          7.9453e+00, -3.5469e+00, -3.2812e+00, -1.1938e+01,  6.5625e+00,
          1.1562e+00,  1.0969e+01,  1.0844e+01, -1.2531e+01, -1.0234e+01,
          3.7188e+00, -3.2969e+00,  3.2969e+00,  7.4688e+00, -2.5391e+00,
         -1.1812e+01,  5.0078e+00,  8.1875e+00,  6.3750e+00,  5.1562e+00,
          8.0938e+00,  2.3000e+01,  3.5938e-01, -1.3781e+01,  1.0406e+01,
         -1.0938e+01, -7.2812e+00, -1.7938e+01, -1.8438e+00, -1.5844e+01,
          1.3938e+01,  1.5381e+00,  6.7578e-01,  2.6641e+00,  9.5938e+00,
          9.9688e+00, -2.3438e-01, -6.6484e+00, -1.9531e-01, -1.7334e+00,
         -1.5625e-01,  6.5469e+00,  3.4219e+00, -6.8281e+00, -1.1188e+01,
         -1.5125e+01, -1.5312e+01, -1.6625e+01, -9.2188e+00,  5.8477e+00,
         -7.5625e+00, -1.3188e+01, -5.2500e+00,  1.0656e+01,  6.2969e+00,
          1.4062e+01, -8.3750e+00, -1.2250e+01,  2.2578e+00, -8.8379e-01,
         -7.8789e+00,  4.5156e+00, -9.9688e+00, -3.0547e+00, -7.5625e+00,
         -1.0844e+01,  1.3938e+01, -5.7031e+00,  1.7344e+00, -2.7656e+00,
         -3.8828e+00, -7.3750e+00, -3.5059e+00,  7.8750e+00,  1.0938e+01,
          1.2094e+01,  1.4312e+01, -9.0391e+00, -6.5156e+00, -3.3984e+00,
          3.2656e+00, -5.4805e+00, -7.5625e+00,  4.9844e+00, -1.8203e+00,
         -5.8125e+00,  9.1875e+00,  1.4531e+01, -9.1406e-01, -1.1719e-01,
          1.1172e+01, -2.2773e+00, -1.5375e+01, -1.7031e+01,  3.7812e+00,
          3.5078e+00,  3.6094e+00,  1.5438e+01,  1.5562e+01,  6.2344e+00,
         -8.0156e+00,  1.2812e+01, -3.0938e+00, -9.5703e-01, -1.0625e+01,
         -1.8672e+00,  1.3477e+00, -6.5000e+00,  1.2594e+01,  3.2266e+00,
          2.0312e+00, -9.4062e+00, -9.1875e+00,  1.1719e-01, -2.6719e+00,
          1.2781e+01,  1.6188e+01,  1.6172e+00, -1.4047e+01,  1.3500e+01,
         -1.5219e+01,  6.7344e+00, -8.7188e+00, -3.7656e+00, -2.3906e+00,
          6.7812e+00,  1.4312e+01,  5.1250e+00, -1.1562e+01,  7.6016e+00,
         -1.4094e+01,  1.2750e+01, -1.0906e+01, -1.0016e+01, -5.1875e+00,
          5.3438e+00,  3.3691e+00, -3.9688e+00,  5.6797e+00, -1.4281e+01,
         -6.8281e+00,  1.5219e+01,  7.3906e+00,  3.6719e+00, -8.2812e-01,
         -5.3750e+00,  8.2812e+00, -6.7578e-01,  8.5469e+00, -3.8125e+00,
         -1.4438e+01, -1.1719e+00, -3.5859e+00, -2.1281e+01, -3.0000e+00,
          1.2031e+01, -1.0773e+01, -2.4766e+00,  1.0078e+00,  4.8125e+00,
          2.7539e+00,  1.5719e+01,  1.3375e+01,  5.0938e+00, -4.7031e+00,
         -8.1562e+00,  2.7188e+00, -3.8281e+00,  3.2188e+00, -1.2750e+01,
         -1.2438e+01, -5.2344e-01,  1.7125e+01,  6.2812e+00, -2.7969e+00,
          1.1188e+01,  1.3000e+01, -1.3750e+01,  1.0000e+01,  9.6875e+00,
          7.5312e+00,  1.8938e+01,  4.5078e+00,  3.7344e+00,  1.1188e+01,
         -5.4688e-02, -1.3125e+00,  8.3750e+00,  1.8125e+01,  1.6688e+01,
          9.0312e+00, -5.5625e+00,  1.3438e+01, -1.1328e+00, -5.5781e+00,
         -9.0781e+00,  6.2344e+00, -1.6156e+01,  1.6812e+01,  1.1844e+01,
         -2.4375e+00, -6.1406e+00,  1.7656e+01,  5.9688e+00, -6.3125e+00,
          4.9531e+00, -2.8125e+00,  7.8750e+00, -2.3594e+00, -8.5000e+00,
         -1.1680e+00, -1.6750e+01, -8.9219e+00, -1.3469e+01, -1.1281e+01,
         -1.3781e+01, -1.5438e+01, -1.0250e+01, -1.1656e+01,  3.3438e+00,
          3.8906e+00,  7.0312e+00,  1.1969e+01, -1.1934e+00, -6.4531e+00,
          1.6875e+01, -4.6875e+00, -3.5625e+00, -2.3438e+00, -3.1016e+00,
          1.1875e+01,  4.9375e+00,  2.4312e+01, -3.9688e+00, -3.7812e+00,
          7.8711e-01,  5.6875e+00, -1.1500e+01, -1.8750e+00,  1.2203e+01,
         -1.2062e+01, -5.7266e+00,  3.5156e+00, -1.0375e+01, -8.2969e+00,
         -1.2656e+01,  3.8750e+00,  8.7188e+00,  8.1562e+00,  4.3281e+00,
         -4.1875e+00,  1.5125e+01,  5.1406e+00,  1.4188e+01, -1.8406e+01,
         -4.7500e+00, -5.4219e+00,  5.7188e+00, -9.3125e+00,  1.4625e+01,
         -6.9688e+00, -1.5625e+00,  6.2969e+00,  1.1625e+01,  2.0938e+00,
         -4.4062e+00,  1.3469e+01, -8.2188e+00, -1.3125e+01, -4.1875e+00,
          2.1375e+01,  2.0078e+00, -4.3359e+00, -4.8438e-01, -1.2375e+01,
         -5.5859e-01, -2.2812e+01, -1.5469e+01,  1.3281e+01, -1.3625e+01,
          4.5938e+00,  5.3828e+00,  1.5781e+01, -9.5625e+00,  1.6312e+01,
          1.6031e+01, -1.5125e+01,  1.2719e+01,  1.1016e+00, -3.6621e-01,
          2.1438e+01,  2.6797e+00,  4.5938e+00,  3.6094e+00,  5.1123e-01,
          1.8750e+00,  4.5625e+00, -1.0211e+01, -1.3125e+01,  1.7375e+01,
         -5.3125e+00,  4.1094e+00, -7.8750e+00,  3.2812e+00,  7.1875e+00,
         -1.8375e+01, -1.3844e+01,  1.6219e+01, -1.9844e+01, -7.8750e+00,
         -1.4312e+01, -9.3906e+00,  1.4297e+00,  1.2594e+01, -1.3500e+01,
         -2.2812e+01,  1.1281e+01, -1.3344e+01,  1.3781e+01, -1.5281e+01,
         -2.1484e+00,  6.9062e+00, -1.3500e+01, -4.0547e+00, -3.0938e+00,
         -2.0820e+00, -9.3125e+00,  5.2812e+00,  8.7188e+00, -1.6844e+01,
          5.7188e+00, -2.4688e+00, -1.7688e+01,  4.4141e+00, -4.5938e+00,
         -3.2438e+01,  2.3750e+01,  1.0438e+01,  1.0438e+01,  1.0547e+00,
         -6.5977e+00,  1.7312e+01,  1.1781e+01,  1.9625e+01, -8.2812e+00,
          5.1875e+00,  6.0938e+00, -9.9375e+00,  1.2000e+01,  4.7812e+00,
          5.4688e+00, -5.0000e-01, -1.0781e+01, -1.0531e+01, -6.0625e+00,
          1.7500e+00, -5.2656e+00, -1.1250e+00, -8.1250e+00,  2.7312e+01,
          1.4094e+01, -1.0125e+01,  1.0656e+01, -8.8125e+00, -1.9875e+01,
          1.2062e+01, -1.6812e+01,  1.2188e+01,  1.0215e+00, -1.6094e+00,
          4.2812e+00, -2.1934e+00, -1.7500e+00,  4.3047e+00, -9.4688e+00,
          2.6250e+00, -1.4750e+01, -6.9531e+00, -1.0844e+01,  1.0719e+01,
          1.0453e+01, -3.6406e+00, -1.4844e+01, -1.0156e+00, -4.0312e+00,
         -3.9375e+00,  1.1438e+01,  1.1250e+01, -8.0938e+00, -5.5312e+00,
          5.4688e+00, -5.5781e+00,  2.0156e+00, -2.0812e+01,  1.1688e+01,
         -5.8672e+00,  6.2500e+00]], device='cuda:3', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
task_net module.module.task_net.layer_norm.weight True tensor([ 0.0850,  0.1484,  0.0551, -0.0308,  0.0095], device='cuda:5',
       dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([ 0.2705,  0.1543, -0.1841, -0.0289, -0.0327], device='cuda:5',
       dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.0502, -0.0452, -0.2593,  ..., -0.3130, -0.0463,  0.1808],
        [ 0.1670, -0.7197, -0.6079,  ..., -1.1387, -0.3828,  0.0151],
        [ 0.0129,  0.1445, -0.2354,  ..., -0.4092, -0.0967,  0.1145],
        [-0.0080,  0.0262,  0.0833,  ...,  0.0486, -0.1266,  0.0314],
        [ 0.2568,  0.0157, -0.5835,  ..., -0.0256,  1.3887, -0.1438]],
       device='cuda:5', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 1.8711,  3.3047,  1.2383, -0.3013, -0.8359], device='cuda:5',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 0.0150,  0.0247,  0.0032,  ...,  0.0483,  0.1292,  0.0071],
        [ 0.0029, -0.0052, -0.0004,  ...,  0.0099, -0.0270, -0.0234],
        [-0.0090, -0.0128, -0.0024,  ..., -0.0256, -0.0691, -0.0012],
        [-0.0055, -0.0178, -0.0021,  ..., -0.0165, -0.1003, -0.0159],
        [ 0.0009,  0.0033,  0.0010,  ...,  0.0014,  0.0364,  0.0020]],
       device='cuda:5', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([ 0.0576, -0.0325, -0.0355, -0.0586,  0.0139], device='cuda:5',
       dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[-5.5469e-01, -2.9219e+00,  1.2637e+00, -3.4531e+00,  1.8672e+00,
         -5.4375e+00, -1.6719e+00,  1.2500e-01,  5.5000e+00, -1.5391e+00,
          2.9844e+00,  7.6562e-01,  1.8047e+00,  3.8438e+00,  9.1016e-01,
         -2.5000e+00,  3.2500e+00,  6.7188e-01,  3.0000e+00,  5.2930e-01,
         -1.9062e+00, -1.8359e+00, -2.5000e-01, -7.1094e-01,  2.0469e+00,
         -1.2988e+00,  8.9062e-01,  1.3281e+00, -4.0000e+00, -1.2969e+00,
         -4.7656e-01,  1.7578e-01,  4.8438e+00, -1.6250e+00,  1.4609e+00,
         -1.0547e+00,  1.0645e+00,  1.5156e+00,  3.3594e-01, -3.1875e+00,
          1.6719e+00,  1.4453e-01,  1.2422e+00,  1.1719e-01, -4.2578e-01,
         -3.0254e+00, -1.3281e+00, -1.7344e+00,  4.0625e-01, -1.4727e+00,
         -1.2617e+00, -8.4229e-01,  7.0312e-01,  1.5547e+00, -7.2656e-01,
         -1.2969e+00, -3.0000e+00, -2.5625e+00, -2.2969e+00, -2.1094e+00,
          2.0957e+00,  2.6719e+00, -2.2344e+00,  2.5000e+00,  7.8125e-01,
         -3.8906e+00,  3.1055e-01, -2.8750e+00, -3.2969e+00,  2.9531e+00,
         -3.1250e+00,  2.8750e+00,  6.8750e-01, -4.0781e+00, -2.3047e+00,
         -1.1875e+00, -2.8750e+00, -1.5859e+00,  1.4062e+00,  1.0059e+00,
         -3.5156e-01,  2.0000e+00, -1.8945e-01, -1.5234e-01,  1.5664e+00,
         -7.6562e-01,  1.5156e+00,  8.1250e-01, -2.1016e+00, -1.6980e-01,
          2.6094e+00,  2.0625e+00, -1.7344e+00, -1.6250e+00,  2.2344e+00,
          2.4219e-01,  3.8281e+00,  2.7969e+00, -6.5234e-01, -5.6250e-01,
          2.1875e+00,  9.2578e-01,  2.4062e+00, -2.5312e+00, -1.0312e+00,
         -1.6494e+00,  1.6719e+00,  2.4531e+00, -2.8438e+00, -1.3760e+00,
         -4.8828e-02,  2.3750e+00,  7.4219e-01,  6.8750e-01,  1.1406e+00,
          1.5469e+00, -1.3281e-01, -9.5312e-01, -3.4688e+00,  2.2031e+00,
          2.3438e-02,  3.7344e+00,  3.7188e+00, -2.3281e+00, -2.8828e+00,
          5.0000e-01, -1.3516e+00,  3.1250e-02,  1.5234e+00,  2.3438e-02,
         -1.4062e+00,  1.1387e+00,  1.5938e+00, -3.1250e-01,  1.6406e+00,
          2.0156e+00,  5.9375e+00, -5.4688e-02, -3.9062e+00,  9.0625e-01,
         -1.2188e+00, -2.6602e+00, -2.3750e+00, -1.0020e+00, -4.0000e+00,
          2.2812e+00,  8.1445e-01,  9.5117e-01,  8.4766e-01,  2.6406e+00,
          1.0312e+00, -1.4062e+00, -2.2266e+00, -2.3242e-01, -4.4189e-01,
         -6.1719e-01,  1.4609e+00,  1.5625e-01, -8.7500e-01, -1.1406e+00,
         -1.6250e+00, -3.0625e+00, -3.0625e+00, -2.5000e+00,  3.9023e+00,
         -1.9453e+00, -2.3594e+00, -1.0938e-01,  1.8203e+00,  1.9297e+00,
          2.1406e+00, -7.1094e-01, -2.2500e+00, -4.2578e-01, -4.2236e-01,
         -2.4512e+00,  7.9688e-01, -2.1875e-01, -5.3281e+00, -1.6875e+00,
         -2.8125e+00,  2.5781e+00, -1.2305e+00,  9.1992e-01, -3.0234e+00,
         -1.0391e+00, -1.6953e+00, -1.1748e+00,  5.9375e-01,  2.4531e+00,
          3.0234e+00,  3.0781e+00, -2.2852e+00, -1.7812e+00, -1.8037e+00,
          4.2969e-01, -1.9473e+00, -2.0703e+00,  1.9258e+00, -3.7500e-01,
         -2.6250e+00,  3.1094e+00,  3.3906e+00, -8.0859e-01,  5.1758e-01,
          2.7109e+00,  2.3047e-01, -1.0625e+00, -3.8281e+00,  2.4844e+00,
          1.3496e+00,  1.2422e+00,  3.9844e+00,  1.7656e+00,  1.1484e+00,
         -1.4453e+00,  3.2031e+00,  3.4375e-01, -1.1348e+00, -1.5781e+00,
          1.0156e-01, -9.3555e-01, -6.8750e-01,  2.8594e+00,  1.6406e+00,
          1.1914e+00, -1.1250e+00, -2.6562e+00,  3.6328e-01,  3.2812e-01,
          3.1406e+00,  2.7188e+00,  1.1797e+00, -3.7500e+00,  2.7969e+00,
         -3.9062e+00,  8.2812e-01, -2.5000e+00, -9.1797e-01, -6.7188e-01,
          1.3906e+00,  3.2812e+00,  1.6172e+00, -2.8438e+00,  2.6641e+00,
         -3.1719e+00,  2.1562e+00, -2.5938e+00, -3.3047e+00, -4.2969e-01,
          9.5312e-01,  8.9893e-01, -1.7812e+00,  1.6758e+00, -1.5156e+00,
         -8.5156e-01,  2.5156e+00,  2.6406e+00,  1.2188e+00, -3.7109e-01,
         -9.3750e-02,  2.2812e+00, -1.4736e+00,  1.3906e+00, -2.2305e+00,
         -2.8750e+00, -1.1484e+00, -7.8125e-01, -5.1250e+00, -8.5156e-01,
          2.4062e+00, -5.1523e+00, -6.0938e-01,  4.9609e-01,  6.7578e-01,
          1.8223e+00,  2.4062e+00,  3.0938e+00,  1.0703e+00, -2.5000e-01,
         -2.8047e+00,  7.8125e-03, -5.3906e-01, -7.0312e-01, -2.6562e+00,
         -1.9062e+00, -1.6016e-01,  1.0625e+00,  8.7500e-01, -1.2031e+00,
          2.0000e+00,  3.8281e+00, -2.0781e+00,  3.8125e+00,  3.3750e+00,
          1.6328e+00,  3.6875e+00,  1.1055e+00,  7.7344e-01,  1.2656e+00,
          4.3506e-01, -7.0312e-02,  1.4062e+00,  3.0938e+00,  3.2344e+00,
          2.2031e+00, -1.7793e+00,  2.9219e+00,  7.8125e-02, -1.9609e+00,
         -2.2656e+00,  2.1953e+00, -3.8125e+00,  3.4062e+00,  3.2188e+00,
         -1.7188e-01, -1.6328e+00,  3.1562e+00, -1.0469e+00, -9.9219e-01,
          1.4883e+00, -7.5391e-01, -1.5625e-02, -2.4219e-01, -1.0000e+00,
          2.3828e-01, -3.1562e+00, -2.9219e+00, -4.1094e+00, -1.5312e+00,
         -1.7812e+00, -3.5781e+00, -3.2500e+00, -2.2656e+00,  5.6055e-01,
          1.0391e+00,  2.6562e+00,  3.5156e+00, -3.0762e-01, -6.4062e-01,
          3.6719e+00, -2.5898e+00,  9.2969e-01,  5.8203e-01,  2.0703e-01,
          3.4062e+00,  4.0625e-01,  6.6250e+00, -3.7500e-01, -5.3516e-01,
         -5.8594e-03,  4.8867e+00, -2.5781e+00, -1.5000e+00,  1.9375e+00,
         -1.7969e+00, -1.2871e+00,  2.9570e+00, -1.9844e+00, -1.2656e+00,
         -1.8125e+00,  9.6875e-01,  2.5156e+00,  3.9062e-01,  1.2246e+00,
         -1.3633e+00,  2.5000e+00,  1.2969e+00,  1.8125e+00, -4.8438e+00,
         -4.4531e-01, -1.3906e+00,  1.4219e+00, -3.8594e+00,  1.3750e+00,
         -1.6113e+00,  1.1875e+00,  1.1406e+00,  3.2500e+00,  1.6641e+00,
         -1.7734e+00,  2.9531e+00,  2.3438e-01, -1.7500e+00, -2.3438e-01,
          5.5312e+00,  5.4102e-01, -8.5938e-01,  3.9453e-01, -3.6094e+00,
          5.7812e-01, -4.5000e+00, -4.0625e+00,  2.2812e+00, -3.7656e+00,
          9.9609e-01,  3.5156e+00,  3.0781e+00, -1.9219e+00,  3.0000e+00,
          3.6406e+00, -1.9375e+00,  2.7188e+00,  3.6719e-01, -4.5703e-01,
          3.8906e+00,  6.8359e-02,  3.7422e+00, -3.9844e-01,  8.6621e-01,
         -2.3438e-01,  1.0312e+00, -3.0508e+00, -1.3281e+00,  4.3594e+00,
         -4.0234e+00,  1.7734e+00, -2.3281e+00,  1.0234e+00,  2.1074e+00,
         -4.3906e+00, -3.4844e+00,  3.2344e+00, -3.6719e+00, -8.2812e-01,
         -3.2031e+00, -1.0625e+00, -7.1875e-01,  1.6719e+00, -2.9453e+00,
         -4.6250e+00,  2.3438e+00, -3.7812e+00,  1.5469e+00, -4.8906e+00,
         -8.8672e-01,  2.3203e+00, -3.0156e+00, -1.1055e+00, -5.0000e-01,
         -3.2520e-01, -3.2891e+00,  1.7031e+00,  9.8438e-01, -2.9688e+00,
          4.4531e-01, -6.3672e-01, -3.9375e+00,  1.7930e+00, -4.5312e-01,
         -7.0000e+00,  5.0000e+00,  1.5156e+00,  2.3047e+00,  3.3984e-01,
         -2.3047e+00,  2.2969e+00,  2.7344e+00,  3.6875e+00, -7.9688e-01,
          1.7500e+00,  1.5781e+00, -2.7344e+00,  3.8281e+00,  1.0703e+00,
          1.9180e+00, -2.6953e-01, -1.9375e+00, -6.5625e-01, -2.1094e+00,
          8.0078e-01, -1.0391e+00, -1.3281e-01, -1.3125e+00,  5.4688e+00,
          2.2656e+00, -2.6406e+00,  1.5781e+00, -1.7344e+00, -4.3750e+00,
          2.0469e+00, -3.6406e+00,  2.9375e+00,  2.6367e-01,  1.3906e+00,
          1.1719e+00, -7.1680e-01, -3.9062e-01,  1.4482e+00, -2.8594e+00,
          1.5117e+00, -3.7188e+00, -1.9453e+00, -1.4688e+00,  2.7578e+00,
          2.6406e+00, -6.9922e-01, -1.4844e+00, -1.3555e+00, -1.0703e+00,
         -3.5195e+00,  4.2188e+00,  2.7578e+00, -1.8516e+00, -1.0391e+00,
          2.3516e+00, -6.7969e-01,  3.0469e-01, -4.0625e+00,  1.7188e+00,
         -4.1094e+00,  1.7188e+00]], device='cuda:5', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
task_net module.module.task_net.layer_norm.weight True tensor([ 0.4121,  0.5283,  0.0758, -0.0909,  0.0349], device='cuda:7',
       dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([ 0.7363,  0.4214, -0.5020, -0.0791, -0.0891], device='cuda:7',
       dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 6.8054e-02,  1.8262e-01, -4.8767e-02,  ..., -2.5757e-01,
         -1.5381e-02,  2.1680e-01],
        [ 6.5625e-01,  1.3916e-01, -3.0957e-01,  ..., -1.1309e+00,
         -2.7344e-01, -4.2383e-01],
        [ 2.5366e-01,  5.9326e-02, -9.7900e-02,  ..., -2.9199e-01,
          4.8022e-01, -2.3572e-01],
        [-1.3123e-03, -2.6459e-02,  7.6050e-02,  ...,  6.4697e-02,
         -1.1523e-01,  1.0217e-01],
        [-4.4922e-02,  4.9487e-01, -1.5850e+00,  ..., -4.4434e-02,
          2.3711e+00, -7.0264e-01]], device='cuda:7', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 3.6367,  7.6953,  2.4707, -0.4390,  1.3047], device='cuda:7',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 0.0298,  0.1011,  0.0207,  ...,  0.0612,  0.3376,  0.0076],
        [ 0.0095,  0.0282,  0.0028,  ...,  0.0014,  0.0372, -0.0227],
        [-0.0191, -0.0607, -0.0117,  ..., -0.0356, -0.1497, -0.0066],
        [-0.0083, -0.0294, -0.0086,  ..., -0.0322, -0.1492, -0.0191],
        [ 0.0011,  0.0050,  0.0024,  ...,  0.0088,  0.0576,  0.0024]],
       device='cuda:7', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([ 0.1958,  0.0098, -0.1075, -0.0968,  0.0229], device='cuda:7',
       dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[-4.1953e+00, -1.1031e+01,  1.6797e+00, -1.0359e+01,  6.1328e+00,
         -1.4719e+01, -6.9375e+00,  1.7656e+00,  1.6406e+01, -5.4844e+00,
          8.4844e+00,  3.5586e+00,  4.2578e+00,  1.4594e+01,  2.8359e+00,
         -1.1766e+01,  1.1953e+01, -2.5586e-01,  1.2531e+01,  1.6074e+00,
         -9.0469e+00, -3.0859e+00, -9.8438e-01, -4.0781e+00,  4.5508e+00,
         -2.3887e+00,  5.0156e+00,  1.0109e+01, -1.5594e+01, -7.2188e+00,
          1.2109e+00, -1.8672e+00,  1.8562e+01, -5.3828e+00,  3.0469e+00,
         -4.9219e+00,  2.0781e+00,  7.2031e+00,  5.9844e+00, -1.0828e+01,
          9.1719e+00,  1.3242e+00,  3.1133e+00, -2.0000e+00, -3.2070e+00,
         -3.4941e+00, -7.1094e+00, -9.6875e+00,  4.9219e+00, -4.0391e+00,
         -4.1367e+00, -1.0020e+00, -1.6328e+00,  5.9375e-01, -3.9609e+00,
         -5.2969e+00, -9.9219e+00, -1.1938e+01, -8.7969e+00, -9.7500e+00,
          3.2344e+00,  9.8438e+00, -6.2656e+00,  1.0359e+01,  7.2031e+00,
         -1.2125e+01,  1.7754e+00, -1.0891e+01, -9.5469e+00,  9.8125e+00,
         -1.1578e+01,  1.3188e+01,  6.4844e+00, -1.2391e+01, -6.0703e+00,
         -4.6484e+00, -1.1531e+01, -5.7188e+00,  6.4531e+00,  1.0303e+00,
          2.7500e+00,  1.0719e+01,  7.1094e-01, -1.4609e+00,  3.9453e+00,
          8.6719e-01,  1.2047e+01,  3.6094e+00, -6.2812e+00,  2.2363e-01,
          1.1031e+01,  9.6406e+00, -6.2344e+00, -4.0508e+00,  7.6562e+00,
          3.8906e+00,  1.2734e+01,  1.0672e+01, -6.5527e-01, -4.6250e+00,
          1.1109e+01,  4.2227e+00,  6.0469e+00, -5.7344e+00, -4.2734e+00,
         -1.5703e+00,  8.0781e+00,  7.1406e+00, -1.1500e+01, -2.9805e+00,
          3.6133e-01,  1.0016e+01,  3.4609e+00, -2.2891e+00,  8.0938e+00,
          3.8750e+00, -2.2578e+00, -3.6875e+00, -8.7969e+00,  2.5176e+00,
          1.0977e+00,  7.5625e+00,  6.5859e+00, -7.6406e+00, -6.8984e+00,
          4.1250e+00, -3.6797e+00,  2.5703e+00,  4.7734e+00, -1.6016e+00,
         -1.2250e+01,  2.2246e+00,  7.5000e+00,  2.7344e+00,  5.1016e+00,
          6.9844e+00,  1.7062e+01,  1.5117e+00, -1.0531e+01,  7.8438e+00,
         -6.9219e+00, -5.7344e+00, -1.2562e+01, -1.5156e+00, -1.2203e+01,
          1.0422e+01,  1.4580e+00,  7.9395e-01,  1.9961e+00,  8.5000e+00,
          7.9531e+00, -3.6719e-01, -4.9062e+00, -1.9473e+00, -4.7314e-01,
          3.8281e-01,  3.8359e+00,  3.4922e+00, -4.9375e+00, -8.0938e+00,
         -7.8594e+00, -9.0312e+00, -8.6719e+00, -6.0312e+00,  4.3672e+00,
         -4.9180e+00, -9.0938e+00, -2.9453e+00,  6.9531e+00,  4.5625e+00,
          8.7812e+00, -4.6562e+00, -8.8750e+00,  1.6875e+00,  8.6914e-02,
         -4.4805e+00,  3.9922e+00, -8.4062e+00, -7.0547e+00, -4.3047e+00,
         -1.0297e+01,  1.1453e+01, -3.3867e+00, -1.7578e-01, -5.6250e+00,
         -2.9102e+00, -4.5117e+00, -1.7607e+00,  1.1500e+01,  5.6055e+00,
          7.3125e+00,  9.9688e+00, -5.8633e+00, -4.3594e+00, -1.8799e+00,
          3.8359e+00, -3.1816e+00, -6.6094e+00,  3.4297e+00, -2.1211e+00,
         -4.7969e+00,  1.1297e+01,  1.0469e+01, -1.7773e+00, -6.6602e-01,
          7.1562e+00, -1.4141e+00, -8.8125e+00, -1.1219e+01,  1.7383e+00,
          2.0918e+00,  4.2188e+00,  1.2094e+01,  1.0531e+01,  4.1328e+00,
         -6.8125e+00,  8.0000e+00, -1.5859e+00, -1.2168e+00, -7.6562e+00,
         -1.9453e+00, -9.3750e-02, -4.5469e+00,  8.3594e+00,  2.6367e+00,
          3.0391e+00, -8.4688e+00, -8.6094e+00, -8.0859e-01, -1.6250e+00,
          1.1641e+01,  1.0062e+01,  3.4570e+00, -8.7188e+00,  1.0750e+01,
         -1.1469e+01,  3.7422e+00, -6.9375e+00, -3.5977e+00, -3.5781e+00,
          3.6133e+00,  9.4844e+00,  4.0000e+00, -1.3375e+01,  4.8828e+00,
         -1.0844e+01,  1.0688e+01, -7.8594e+00, -6.9531e+00, -5.9297e+00,
          5.6250e+00,  1.6953e+00, -4.5938e+00,  3.4082e+00, -9.8750e+00,
         -5.1406e+00,  1.0641e+01,  6.7422e+00,  3.4844e+00,  1.1953e+00,
         -3.9062e+00,  6.6016e+00,  5.9375e-01,  5.3281e+00, -3.9102e+00,
         -1.0000e+01, -3.6562e+00, -1.7148e+00, -1.5266e+01, -3.6797e+00,
          9.4844e+00, -6.8594e+00, -1.7656e+00,  1.9414e+00,  1.0078e+00,
          1.0840e+00,  1.1672e+01,  8.5938e+00,  4.0078e+00, -3.8047e+00,
         -8.0000e+00,  3.1562e+00, -8.3398e-01,  2.0938e+00, -1.1844e+01,
         -9.3438e+00, -1.6211e+00,  1.1969e+01,  4.5234e+00, -2.3281e+00,
          1.1688e+01,  1.0234e+01, -8.0781e+00,  1.0125e+01,  1.1328e+01,
          5.9922e+00,  1.2781e+01,  2.0664e+00,  3.7109e+00,  7.7031e+00,
          6.1816e-01,  2.1328e+00,  4.9375e+00,  1.3000e+01,  1.1750e+01,
          6.3438e+00, -2.1680e+00,  8.4219e+00, -1.5078e+00, -3.7734e+00,
         -5.4688e+00,  5.0781e+00, -1.4047e+01,  1.0297e+01,  1.1531e+01,
         -2.5859e+00, -5.1172e+00,  1.0172e+01,  4.4844e+00, -3.7578e+00,
          3.0586e+00, -2.8047e+00,  4.7344e+00, -1.7188e+00, -6.2031e+00,
         -3.9844e-01, -1.2203e+01, -5.8594e+00, -1.2875e+01, -7.4375e+00,
         -8.9219e+00, -1.0250e+01, -8.4844e+00, -9.6719e+00,  1.7402e+00,
          3.6953e+00,  4.8047e+00,  1.1703e+01, -7.3145e-01, -5.2969e+00,
          1.1750e+01, -3.3984e+00, -2.1328e+00, -1.3906e+00, -2.0859e+00,
          1.3500e+01,  4.2031e+00,  1.7062e+01, -3.5000e+00, -3.1211e+00,
          2.3828e-01,  5.4531e+00, -6.6172e+00, -1.8047e+00,  5.8281e+00,
         -1.0422e+01, -2.6895e+00,  5.6484e+00, -6.7031e+00, -4.3750e+00,
         -7.9062e+00,  3.5859e+00,  7.6719e+00,  5.6484e+00,  1.8242e+00,
         -3.4414e+00,  8.7812e+00,  4.1250e+00,  1.2219e+01, -1.1203e+01,
         -4.7578e+00, -4.5391e+00,  3.9961e+00, -1.2031e+01,  9.9375e+00,
         -3.4473e+00, -2.1875e+00,  3.2500e+00,  8.8438e+00,  2.3711e+00,
         -4.8672e+00,  8.0781e+00, -6.0000e+00, -1.0531e+01, -4.0625e+00,
          1.6562e+01,  1.3281e+00, -1.8242e+00, -6.0938e-01, -1.4094e+01,
         -8.1055e-01, -1.6625e+01, -1.3203e+01,  9.0469e+00, -1.0500e+01,
          2.9492e+00,  4.6172e+00,  1.1328e+01, -6.2500e+00,  1.2062e+01,
          1.0359e+01, -1.1000e+01,  9.9688e+00,  1.6719e+00, -4.8096e-01,
          1.2969e+01,  1.4062e+00,  7.3516e+00,  2.0391e+00,  6.7188e-01,
          1.2109e+00,  3.3750e+00, -5.1758e+00, -1.0875e+01,  1.3859e+01,
         -6.1641e+00,  4.4766e+00, -5.3672e+00,  3.2891e+00,  3.7832e+00,
         -1.2031e+01, -1.0078e+01,  1.2547e+01, -1.4281e+01, -4.3359e+00,
         -8.7344e+00, -6.4531e+00,  4.8828e-01,  9.0938e+00, -7.1953e+00,
         -1.4438e+01,  9.5156e+00, -1.1234e+01,  9.1094e+00, -1.2359e+01,
         -1.5273e+00,  5.3672e+00, -1.1500e+01, -3.3984e+00, -2.7812e+00,
         -1.1699e+00, -7.4141e+00,  3.9414e+00,  6.1719e+00, -1.0031e+01,
          5.9844e+00, -2.2578e+00, -1.3172e+01,  2.9805e+00, -4.2656e+00,
         -1.7031e+01,  1.7469e+01,  8.2812e+00,  6.3828e+00, -6.2109e-01,
         -2.8281e+00,  1.1000e+01,  8.5938e+00,  1.2812e+01, -7.9688e+00,
          3.6016e+00,  4.8203e+00, -9.1562e+00,  1.0578e+01,  3.4609e+00,
          3.1641e+00, -1.5625e-02, -8.2500e+00, -5.9531e+00, -5.1172e+00,
          2.1875e-01, -5.1406e+00, -1.9688e+00, -6.5938e+00,  1.6938e+01,
          8.1875e+00, -9.3594e+00,  6.4688e+00, -9.0000e+00, -1.5656e+01,
          7.3594e+00, -1.3812e+01,  1.0656e+01,  2.0410e-01, -1.2500e+00,
          3.3047e+00, -2.0195e+00, -2.1094e+00,  1.6162e+00, -7.6094e+00,
         -1.5625e-02, -1.1625e+01, -5.1328e+00, -6.9844e+00,  8.1406e+00,
          6.5156e+00, -7.4023e-01, -9.6094e+00,  0.0000e+00, -4.8516e+00,
         -3.8906e+00,  1.0531e+01,  8.0469e+00, -4.2812e+00, -4.3672e+00,
          5.1406e+00, -5.5156e+00,  2.9297e+00, -1.3719e+01,  1.0891e+01,
         -6.4102e+00,  6.6562e+00]], device='cuda:7', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
task_net module.module.task_net.layer_norm.weight True tensor([-0.0396, -0.0044, -0.0017, -0.0159, -0.0105], device='cuda:1',
       dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([ 0.0771,  0.0439, -0.0522, -0.0083, -0.0093], device='cuda:1',
       dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.0312,  0.0587, -0.2335,  ..., -0.0903,  0.0142, -0.0813],
        [-0.0217, -0.0374, -0.3135,  ..., -0.4004, -0.2323, -0.2496],
        [ 0.0063,  0.1616, -0.2029,  ..., -0.1311,  0.0068, -0.1309],
        [ 0.0262, -0.0367,  0.0546,  ...,  0.0146, -0.1151,  0.0390],
        [-0.1699,  0.0117, -0.6084,  ...,  0.2335,  1.1650, -0.1241]],
       device='cuda:1', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 1.5234,  2.9141,  1.4062, -0.0537, -0.9414], device='cuda:1',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 0.0118,  0.0396,  0.0121,  ...,  0.0009,  0.0972,  0.0028],
        [ 0.0012,  0.0049,  0.0027,  ..., -0.0207, -0.0483, -0.0176],
        [-0.0080, -0.0234, -0.0053,  ...,  0.0038, -0.0380, -0.0040],
        [-0.0065, -0.0190, -0.0060,  ..., -0.0199, -0.0867, -0.0166],
        [ 0.0003,  0.0008,  0.0013,  ...,  0.0053,  0.0212, -0.0049]],
       device='cuda:1', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([ 0.0396, -0.0415, -0.0215, -0.0579,  0.0003], device='cuda:1',
       dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[ 0.7969,  0.3281, -0.0566, -1.9375, -1.0000,  0.6250,  0.8438, -1.3008,
          0.5781,  0.0547,  0.6250, -0.1289,  0.3672, -0.9375,  0.6641,  0.3125,
          0.9375,  0.4883,  0.2500,  0.0547,  1.3281, -1.1621,  0.6406,  0.6172,
          1.2598, -0.8672, -0.1562, -1.4531,  1.0312,  1.8906, -1.4023,  1.2812,
          0.7812,  0.1094,  0.1172, -0.5586,  0.0632, -0.7656, -0.8047,  0.2656,
         -1.1875, -0.1230,  0.6777,  1.1367, -0.5898, -2.2871,  0.4844,  0.7188,
         -0.9375, -0.6250,  0.0781, -0.5166,  1.8945,  2.0742,  1.0234,  1.5000,
          1.3594,  0.0938,  0.4688,  1.2188,  0.7539, -0.1094, -0.6328,  0.4375,
         -0.5391, -0.5781, -0.5088, -0.9688, -0.5547,  0.5625,  0.8906,  0.2812,
         -0.9766, -0.1406, -0.0703,  1.2578,  0.5469, -0.4922,  0.0781,  1.1475,
         -1.3555, -0.2188, -1.0273,  0.8828,  0.5742, -1.4727, -0.4062, -0.6719,
         -0.1328, -0.0864, -0.3594, -0.7500,  0.1250, -1.9492, -0.4375, -0.7891,
          1.0156, -0.2500, -0.9746,  1.6484, -1.5469,  0.8477,  0.4609, -1.3242,
         -0.8672, -2.1660, -0.4062,  0.2422,  0.1875, -1.3496,  0.2363, -1.1875,
          0.3203,  0.4922, -1.8750,  0.7109,  1.2969,  1.0156, -0.5156,  1.5625,
         -0.8906,  1.1875,  1.4883, -0.0078, -1.0703, -0.9375,  0.5859,  0.0195,
         -0.3516,  0.8809,  0.6406,  1.3584, -1.1562, -0.8789, -0.0469,  0.7031,
          0.1562, -0.5742, -0.0625, -0.0625,  0.6250, -1.6055,  1.2656, -0.2031,
         -0.0469, -1.6875,  0.2090,  1.2734, -0.1602, -0.6562, -1.4531, -1.9297,
         -0.3711, -1.1748, -0.3103, -1.6484, -1.0547, -1.7109,  0.4375,  0.3750,
          0.5312, -0.4062,  0.2031, -1.1797,  2.7363, -0.1836, -0.4219,  1.2148,
          0.1250, -0.0664, -0.5000,  1.6562,  0.2969, -1.0352, -0.9297, -1.8086,
         -0.4375,  2.1094, -3.8750, -0.9414, -0.2188, -0.1562, -0.0078,  0.7539,
         -0.3672, -0.0469, -0.8164, -0.7832, -2.0469,  1.2383,  1.3906,  1.4219,
         -1.3164, -0.7500, -0.8848, -0.5234, -1.3477,  0.2031,  0.7148,  0.1172,
         -0.3750, -1.1406, -0.7812,  0.2383,  0.5020,  0.5234,  0.7471, -0.0312,
         -0.9219,  2.4023,  0.7227, -0.1484,  0.8125, -1.4375, -0.1875,  0.2266,
         -1.7500,  0.9609, -1.1963,  1.2188,  0.2949, -1.0840,  0.9297,  0.9609,
          0.1289,  0.1328,  1.1719, -0.6328,  0.2148,  1.2930, -0.7812, -0.2812,
          0.9609, -1.6484, -0.2344, -0.2344, -0.2266, -1.1719, -0.3945,  0.6836,
          0.6328,  0.2500, -0.0391,  1.0781,  1.4785,  0.4531, -1.8750,  0.4219,
         -1.2930,  0.7656, -1.7891,  0.8101,  0.1094,  0.9971,  1.2969,  0.7812,
          0.4531,  0.1875,  0.3047, -0.9453,  1.2109,  0.3125, -1.2539, -0.5547,
         -1.7070,  0.4688,  0.3555, -0.0703, -0.6719,  0.2930, -1.3594, -4.0625,
          0.0898, -0.6992,  0.4766,  1.4512,  0.5156,  0.1875, -0.2500,  0.8594,
          0.0703, -1.9453, -0.2461, -1.3320, -0.1250, -0.2188,  0.3359, -2.6406,
         -0.9766, -0.4316, -0.9219,  0.6406,  0.3594,  0.0625, -1.3750, -0.2188,
         -0.7031,  0.1641,  0.1719, -0.5469,  0.1039, -2.0508, -0.1797, -1.7969,
          0.2344,  0.4297, -0.7021,  0.0781,  1.1289, -0.3438, -1.2617,  0.9844,
         -0.5469, -0.1406, -0.4219,  2.2578, -0.2578, -0.5156, -3.3672,  0.8438,
          0.3125,  0.0547, -1.0781,  2.0781,  1.5156,  0.4404,  0.7969, -1.0352,
         -0.3281,  0.5078,  0.3750, -0.5469, -0.4375,  0.6562,  0.1777, -0.0898,
          1.0156,  1.5938, -0.3799,  0.6797,  0.0312, -1.4707,  2.0156,  1.2461,
          0.2441, -2.1875, -0.0938,  0.8281,  0.4531,  0.4375,  0.0254,  4.1484,
         -0.5391, -1.1475,  0.1172,  0.5781, -1.1094,  1.6445,  1.8438,  0.0859,
          0.9062, -0.6797, -1.1172, -0.1250,  0.7090,  0.1797, -0.6406,  0.8477,
         -1.9375, -1.1875,  0.6875,  0.5859,  1.0039,  0.6250, -0.6719, -1.6758,
          2.3047, -0.2109,  1.0078,  1.0859, -0.0547, -0.7969,  2.2812,  1.2031,
          0.1875,  0.3125,  0.1855, -0.1445,  1.3594,  0.5000,  0.5488, -0.5000,
         -0.4531,  0.5000,  0.1875, -0.4023,  2.6230, -0.0156, -0.1719, -1.2344,
         -0.0469,  1.8594,  1.2266, -0.3047,  0.0259,  0.9531, -0.5127,  2.1172,
         -0.8984,  0.0729, -1.2539,  0.1016, -0.9570,  1.1094,  0.4844, -3.1328,
          0.2656, -0.1875,  0.0859,  1.8652, -0.5781, -0.0859, -0.4062, -1.3438,
          0.7031,  0.1406, -0.1719, -1.7148, -1.2500, -0.8047, -0.7188,  0.4453,
          0.9375, -1.5781, -0.0938, -1.2285,  1.2500,  0.6562,  0.1992,  0.6094,
         -0.1816, -0.4453,  1.2656, -1.4297, -0.4062, -0.3906, -0.6523, -0.4062,
          1.1914,  1.2266, -2.0312,  0.8438,  0.2422,  0.6328,  0.7188, -1.0898,
         -0.4219, -0.1562, -0.0625,  1.9375,  0.1641, -0.0625,  0.9688, -0.0781,
         -0.0430,  0.6172,  0.9121, -0.6875,  1.6562, -0.6797,  1.8203, -0.1875,
          0.3828,  1.0312,  0.9375, -0.2812,  0.3594, -0.6172,  2.5156, -0.5781,
         -1.5469,  0.8750,  0.5781,  0.4590,  1.2148, -0.3125, -0.3574,  0.9141,
          0.8799, -0.2891,  2.3477, -0.9375, -0.0547,  0.7188,  0.5234, -0.2031,
          0.4678, -0.2188, -1.9336,  0.4766, -1.4570, -0.7656,  0.5859, -0.4688,
          0.7500,  0.1484,  0.3750, -1.6133,  0.0625, -0.6562, -3.9824, -0.1484]],
       device='cuda:1', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
task_net module.module.task_net.layer_norm.weight True tensor([-0.6689, -0.6904, -0.1492,  0.1265, -0.0719], device='cuda:2',
       dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([-0.9707, -0.5557,  0.6621,  0.1042,  0.1174], device='cuda:2',
       dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.3506, -0.0823, -0.0337,  ..., -0.3767, -0.5132,  0.0778],
        [ 1.0420, -0.6113,  0.4297,  ..., -2.5137, -1.0215, -0.7114],
        [ 0.0646, -0.0857,  0.1099,  ..., -0.6338, -0.3428, -0.1383],
        [ 0.1199, -0.1530,  0.0734,  ...,  0.2269, -0.1980,  0.1643],
        [-0.9355,  0.5225, -1.7422,  ..., -0.4844,  3.8828, -2.1973]],
       device='cuda:2', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 2.7344, -1.4219, -0.9805,  0.5264, -6.8828], device='cuda:2',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 0.0086,  0.0093, -0.0060,  ..., -0.0422, -0.2168, -0.0168],
        [ 0.0006, -0.0121,  0.0018,  ..., -0.0551, -0.1965, -0.0214],
        [-0.0051,  0.0120,  0.0060,  ...,  0.0457,  0.1582,  0.0177],
        [-0.0057, -0.0164,  0.0055,  ..., -0.0182, -0.0743, -0.0050],
        [ 0.0007,  0.0077,  0.0009,  ...,  0.0162,  0.0704,  0.0023]],
       device='cuda:2', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([-0.2310, -0.1641,  0.1633, -0.0236,  0.0335], device='cuda:2',
       dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[ 7.4219e+00,  1.4375e+01, -3.3867e+00,  1.4531e+01, -1.1078e+01,
          2.5781e+01,  1.4031e+01, -9.4453e+00, -2.7594e+01,  6.4531e+00,
         -1.2078e+01, -6.7812e+00, -6.3906e+00, -2.8031e+01, -1.2695e+00,
          2.0719e+01, -1.8250e+01,  1.9980e+00, -2.6875e+01, -1.3398e+00,
          2.1562e+01, -2.3223e+00,  7.5781e+00,  9.3438e+00, -1.9062e+00,
          2.7734e-01, -8.1250e+00, -2.2844e+01,  2.8844e+01,  1.8250e+01,
         -9.5000e+00,  8.3828e+00, -2.6500e+01,  1.0141e+01, -4.8984e+00,
          4.7578e+00,  1.1738e+00, -1.7250e+01, -1.2312e+01,  1.5906e+01,
         -1.3719e+01, -5.0547e+00, -3.8750e+00,  4.2812e+00,  3.3359e+00,
         -5.6875e+00,  1.2234e+01,  2.5219e+01, -1.3562e+01,  5.0312e+00,
          3.1250e+00, -2.8301e+00,  8.3359e+00,  8.3906e+00,  1.2531e+01,
          1.6719e+01,  1.8719e+01,  2.0156e+01,  1.7688e+01,  1.7344e+01,
          8.9844e-02, -1.5156e+01,  7.9688e+00, -1.7781e+01, -1.2016e+01,
          1.5719e+01, -2.2305e+00,  1.9969e+01,  1.2688e+01, -1.5500e+01,
          2.2344e+01, -2.0750e+01, -1.3234e+01,  1.4938e+01,  1.0047e+01,
          1.1781e+01,  2.0656e+01,  5.9844e+00, -1.2562e+01,  2.3926e+00,
         -8.9297e+00, -2.3406e+01, -4.2930e+00,  6.0156e+00, -4.3594e+00,
         -9.6797e+00, -1.9531e+01, -1.0047e+01,  1.1750e+01,  1.8164e-01,
         -1.9125e+01, -1.7969e+01,  1.5562e+01,  1.6094e+00, -1.7031e+01,
         -1.0500e+01, -2.0438e+01, -1.9344e+01, -4.9844e+00,  1.5438e+01,
         -2.1375e+01, -3.0469e+00, -7.8281e+00,  3.9062e-02,  6.4766e+00,
         -6.2969e+00, -1.5625e+01, -1.2438e+01,  2.4094e+01, -1.5801e+00,
         -2.0781e+00, -2.1000e+01, -9.5781e+00,  7.8125e+00, -2.3844e+01,
         -2.1953e+00,  7.2969e+00,  1.2875e+01,  1.4594e+01,  4.5312e-01,
         -7.2578e+00, -8.0000e+00, -2.1719e+00,  1.3578e+01,  6.6719e+00,
         -1.0625e+01,  1.0438e+01, -4.4219e+00, -9.7031e+00,  1.9492e+00,
          2.1531e+01,  3.2676e+00, -1.5766e+01, -9.1562e+00, -8.8594e+00,
         -8.9062e+00, -2.6219e+01, -4.7930e+00,  1.9250e+01, -1.5375e+01,
          1.1609e+01,  1.2578e+00,  2.3906e+01,  1.7227e+00,  2.1719e+01,
         -2.5719e+01,  1.0664e+00,  1.2969e+00, -5.9922e+00, -1.2938e+01,
         -2.0203e+01, -6.3867e+00,  4.7188e+00,  2.1328e+00, -1.9287e+00,
         -8.4766e+00, -1.1641e+01, -1.2578e+01,  1.1453e+01,  1.4125e+01,
          1.5625e+01,  1.6500e+01,  1.9594e+01,  3.5938e+00,  1.5117e+00,
          4.6641e+00,  9.3438e+00,  7.5391e+00, -1.3359e+01, -8.8594e+00,
         -1.2719e+01,  1.2859e+01,  1.4375e+01, -5.5508e+00, -1.9756e+00,
         -1.8984e+00, -8.4766e+00,  1.8938e+01,  1.5391e+00,  5.5469e+00,
          1.5062e+01, -1.7969e+01,  3.9766e+00,  4.5430e+00,  1.1281e+01,
          5.6484e+00,  2.0781e+00, -6.6602e-01, -2.8625e+01, -1.0391e+00,
         -5.7500e+00, -1.0547e+01,  1.8906e+00,  6.4219e+00, -3.5859e+00,
         -1.0047e+01, -2.6484e+00,  1.2625e+01, -2.7344e+00,  5.4766e+00,
          6.4375e+00, -2.0031e+01, -1.7500e+01,  5.2969e+00,  4.9492e+00,
         -7.6875e+00,  1.3984e+00,  1.7406e+01,  1.5094e+01,  1.0359e+01,
         -6.1133e-01, -1.0203e+01, -2.1719e+01, -2.7688e+01, -6.5469e+00,
          8.4375e+00, -2.6188e+01,  5.2891e+00, -2.4863e+00,  1.7125e+01,
          4.5781e+00, -3.1914e+00,  9.1562e+00, -1.4156e+01, -2.9062e+00,
         -7.9297e+00,  2.0344e+01,  1.0891e+01,  6.4766e+00,  7.8984e+00,
         -1.9375e+01, -1.6562e+01, -3.7891e+00,  4.2031e+00, -1.7969e+01,
          1.5688e+01, -9.4844e+00,  6.1094e+00,  2.4062e+00,  8.3984e+00,
         -3.4062e+00, -1.4406e+01, -7.6719e+00,  2.7750e+01,  3.8281e-01,
          1.8375e+01, -2.1719e+01,  1.4297e+01,  2.6406e+00,  1.0625e+01,
         -1.3234e+01,  1.1152e+00,  8.5312e+00, -1.4844e-01,  2.1750e+01,
          9.8125e+00, -1.8750e+01, -9.5625e+00, -4.0156e+00, -7.5625e+00,
          1.3141e+01, -1.0625e+01, -4.0898e+00, -9.9844e+00,  4.4922e+00,
          1.8188e+01,  8.4062e+00,  4.6562e+00,  1.8875e+01,  8.3672e+00,
         -2.0094e+01, -1.1953e+00,  3.2969e+00, -5.3516e+00,  2.8516e-01,
          6.0000e+00, -1.6656e+01, -1.1172e+01, -1.1078e+01,  8.4219e+00,
          1.3750e+01, -1.2031e+01,  2.4004e+00, -6.5000e+00,  1.5375e+01,
          1.9188e+01,  3.8789e+00, -2.9688e+01, -1.2703e+01,  3.0078e+00,
         -2.2250e+01, -1.3625e+01,  1.3688e+01, -1.3719e+01, -2.3562e+01,
         -1.0609e+01, -2.0656e+01, -9.4531e-01, -1.1125e+01, -1.8062e+01,
          2.6953e-01, -1.2828e+01, -7.5938e+00, -2.3438e+01, -2.3094e+01,
         -9.2500e+00,  1.1172e+00, -1.8719e+01,  6.6094e+00,  5.0312e+00,
          3.9219e+00, -6.5703e+00,  1.8406e+01, -1.6969e+01, -1.7625e+01,
          1.4688e+01,  8.1562e+00, -1.4938e+01, -1.8844e+01,  8.1406e+00,
         -4.4453e+00,  3.7031e+00, -1.2453e+01,  1.3266e+01,  1.9281e+01,
          2.0527e+00,  2.2062e+01,  3.0000e+00,  2.0500e+01,  1.3672e+01,
          1.4906e+01,  2.2531e+01,  1.4031e+01,  1.6375e+01, -7.2656e-01,
         -9.5938e+00, -5.7031e+00, -1.4750e+01,  1.7578e-02,  1.0328e+01,
         -1.9125e+01,  2.2578e+00,  1.4703e+01,  9.5625e+00,  3.4336e+00,
         -2.5656e+01, -4.9922e+00, -2.1500e+01,  9.6562e+00,  5.4609e+00,
         -1.3281e+00,  3.8125e+00,  7.4844e+00,  2.5586e-01, -7.5469e+00,
          1.9531e+01, -2.8652e+00, -6.5000e+00,  1.6500e+01,  9.6719e+00,
          2.4250e+01, -8.5000e+00, -1.3266e+01, -8.9219e+00, -2.0625e+00,
          6.4219e+00, -1.9062e+01, -5.3594e+00, -2.5469e+01,  1.1875e+01,
          1.1609e+01,  7.2969e+00, -3.1953e+00,  2.9531e+01, -1.3000e+01,
         -2.0234e+00,  1.5031e+01, -7.3125e+00, -1.1422e+01, -5.2656e+00,
          8.4844e+00, -1.6281e+01,  1.6641e+01,  1.8125e+01,  1.0594e+01,
         -2.7500e+01, -2.9102e+00,  1.7305e+00,  9.2031e+00,  2.0750e+01,
          1.9258e+00,  2.4406e+01,  1.6188e+01, -1.3750e+01,  1.6625e+01,
         -4.2109e+00,  5.9375e-01, -1.5531e+01,  1.3281e+01, -2.3688e+01,
         -1.5375e+01,  1.9312e+01, -9.3125e+00, -5.1133e+00, -6.5430e-01,
         -2.1344e+01, -3.7383e+00, -4.5156e+00, -4.7266e+00,  1.8643e+00,
         -7.5391e+00, -9.5156e+00,  1.2422e+00,  2.3375e+01, -2.0656e+01,
          6.8750e+00, -7.3516e+00,  1.0516e+01, -7.5859e+00,  1.7188e-01,
          1.8031e+01,  1.2344e+01, -1.9188e+01,  2.0969e+01,  9.6250e+00,
          1.5500e+01,  9.6562e+00, -6.9609e+00, -2.0906e+01,  8.4844e+00,
          2.4062e+01, -1.1188e+01,  2.0281e+01, -2.0375e+01,  1.5750e+01,
          1.7969e+00, -7.1875e+00,  1.9312e+01,  4.9844e+00,  1.0164e+01,
          1.5430e+00,  6.7344e+00, -3.1797e+00, -1.5781e+01,  1.6438e+01,
         -1.2578e+01,  3.5352e+00,  2.0969e+01, -3.2031e+00,  1.3391e+01,
          3.6250e+01, -3.1344e+01, -1.3391e+01, -8.6562e+00,  5.3477e+00,
         -9.1406e-01, -2.2594e+01, -1.6188e+01, -2.8156e+01,  1.6781e+01,
         -6.6875e+00, -7.6719e+00,  1.3734e+01, -2.0094e+01, -7.2266e+00,
         -6.4453e+00,  5.6875e+00,  1.4891e+01,  2.4125e+01,  8.4531e+00,
          9.0938e+00,  1.0531e+01,  9.4375e+00,  1.5438e+01, -2.0812e+01,
         -1.3812e+01,  2.0688e+01, -1.1969e+01,  2.8281e+01,  2.2062e+01,
         -1.9469e+01,  2.2500e+01, -1.5250e+01,  3.6016e+00,  7.7344e+00,
         -6.9375e+00,  1.7773e+00,  9.2656e+00,  1.5469e+00,  1.1016e+01,
          1.1875e+01,  1.1750e+01,  7.3750e+00,  1.5969e+01, -1.2250e+01,
         -1.0016e+01,  1.1250e+00,  1.6438e+01, -1.0594e+01,  1.0266e+01,
          7.6328e+00, -1.9656e+01, -1.0719e+01,  4.6641e+00,  1.1219e+01,
         -3.5469e+00,  7.8281e+00, -7.8125e+00,  2.6844e+01, -1.9156e+01,
         -1.0938e-01, -1.0656e+01]], device='cuda:2', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
task_net module.module.task_net.layer_norm.weight True tensor([ 2.9980e-01,  5.8398e-01, -2.4414e-04, -7.7881e-02,  1.6724e-02],
       device='cuda:4', dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([ 0.6270,  0.3584, -0.4268, -0.0671, -0.0759], device='cuda:4',
       dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.0403, -0.1467, -0.6025,  ..., -0.7178,  0.1104,  0.0349],
        [-0.5601, -0.1162, -0.7363,  ..., -2.2480, -0.9121, -1.1592],
        [-0.4856,  0.0586, -0.5352,  ..., -0.8643,  0.0569, -0.4456],
        [ 0.0524, -0.1545,  0.2170,  ...,  0.1971, -0.2454,  0.2522],
        [-0.9688,  1.0283, -2.8086,  ...,  0.0796,  2.4023, -1.9971]],
       device='cuda:4', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([4.3008, 8.9688, 2.4961, 0.0713, 1.6406], device='cuda:4',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 0.0400,  0.1449,  0.0233,  ...,  0.0867,  0.2183,  0.0078],
        [ 0.0114,  0.0480,  0.0021,  ...,  0.0167,  0.0120, -0.0132],
        [-0.0301, -0.0865, -0.0067,  ..., -0.0697, -0.1069,  0.0023],
        [-0.0176, -0.0423, -0.0065,  ..., -0.0547, -0.1527, -0.0148],
        [ 0.0016,  0.0041,  0.0022,  ...,  0.0152,  0.0487,  0.0078]],
       device='cuda:4', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([ 0.1260,  0.0005, -0.0684, -0.0895,  0.0213], device='cuda:4',
       dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[-2.8125e+00, -1.2750e+01, -1.0938e-01, -8.8750e+00,  3.6875e+00,
         -1.4188e+01,  6.2500e-02, -1.6406e+00,  1.6062e+01, -3.6094e+00,
          7.5938e+00,  1.4375e+00,  4.6875e+00,  1.3156e+01,  3.7305e+00,
         -1.0156e+01,  1.0844e+01,  1.0859e+00,  1.0281e+01,  3.8594e+00,
         -7.1562e+00, -5.6719e+00, -1.3047e+00, -1.3906e+00,  2.6719e+00,
         -5.0156e+00,  2.5781e+00,  8.4688e+00, -1.4562e+01, -3.5312e+00,
         -1.9688e+00, -2.9766e+00,  2.8938e+01, -1.8281e+00,  1.7266e+00,
         -5.3516e+00,  2.1309e+00,  5.9375e+00,  3.5781e+00, -6.6562e+00,
          8.4062e+00, -1.4570e+00,  1.6719e+00, -2.9219e+00, -2.2891e+00,
         -4.4883e+00, -2.0312e+00, -2.9375e+00,  5.1875e+00, -2.8906e+00,
         -3.8828e+00, -2.0977e+00,  1.9844e+00,  1.4844e+00,  1.1250e+00,
         -4.2188e+00, -9.8750e+00, -4.9375e+00, -1.0125e+01, -6.5312e+00,
          4.4297e+00,  4.6562e+00, -3.9688e+00,  8.0938e+00,  4.2500e+00,
         -8.0625e+00, -2.3125e+00, -9.3438e+00, -9.5312e+00,  9.4375e+00,
         -1.0250e+01,  1.1562e+01,  4.7812e+00, -7.9688e+00, -3.0469e+00,
         -1.2656e+00, -7.4375e+00, -4.6562e+00,  2.1250e+00,  1.1426e+00,
         -2.0312e-01,  7.2812e+00,  1.7578e+00, -1.0938e-01,  4.7891e+00,
         -3.5859e+00,  8.7188e+00, -1.1094e+00, -7.1250e+00,  1.7578e-01,
          6.3750e+00,  5.6562e+00, -8.5938e+00, -1.7578e+00,  4.8438e+00,
          3.5156e+00,  9.0625e+00,  1.4000e+01, -2.6035e+00, -1.8906e+00,
          5.0000e+00,  3.6094e+00,  3.4531e+00, -7.0625e+00, -3.9297e+00,
         -5.1133e+00,  6.2500e+00,  4.6562e+00, -1.3469e+01, -1.4219e+00,
          1.9141e-01,  5.6250e+00,  3.2969e+00, -1.2188e+00,  7.7188e+00,
          5.8750e+00, -3.0781e+00, -1.1562e+00, -7.6562e+00,  7.0312e-01,
         -9.8438e-01,  8.3750e+00,  4.3281e+00, -5.9062e+00, -7.0625e+00,
          6.5000e+00, -2.8125e-01, -9.7656e-01,  2.7656e+00, -3.3203e+00,
         -8.8750e+00,  9.1895e-01,  4.0312e+00,  2.9844e+00,  1.5000e+00,
          4.7031e+00,  1.4125e+01, -2.0312e+00, -1.0469e+01,  8.4688e+00,
         -4.2188e+00, -5.6953e+00, -1.2688e+01,  5.8203e-01, -8.5000e+00,
          6.0938e+00,  1.5156e+00, -6.9922e-01,  2.6328e+00,  7.7188e+00,
          5.7500e+00, -4.3789e+00, -4.8984e+00, -1.2656e+00,  1.1162e+00,
         -5.3125e-01,  1.7812e+00, -8.7500e-01, -1.7031e+00, -7.8750e+00,
         -1.0281e+01, -8.6562e+00, -5.8438e+00, -4.9688e+00,  4.2656e+00,
         -3.0156e+00, -9.9375e+00, -1.2344e+00,  4.0000e+00,  1.6875e+00,
          8.4375e+00, -3.6875e+00, -7.4375e+00,  9.4531e-01, -5.7227e-01,
         -5.6797e+00,  3.9062e-01, -4.0312e+00, -5.0977e+00, -2.7344e+00,
         -7.2812e+00,  1.1188e+01, -7.1875e-01, -6.3281e-01,  1.4062e-01,
         -2.6875e+00, -5.7109e+00, -1.9980e+00,  8.0000e+00,  5.7969e+00,
          7.3906e+00,  5.4375e+00, -4.4297e+00, -1.2500e+00, -7.1289e-01,
          1.4844e+00, -1.5020e+00, -3.0938e+00,  2.8438e+00, -1.9531e+00,
         -3.3594e+00,  6.5312e+00,  5.2500e+00, -1.6484e+00,  3.6836e+00,
          6.3438e+00,  2.0742e+00, -1.1438e+01, -1.0688e+01,  1.6641e+00,
          4.3203e+00,  6.7188e-01,  7.8750e+00,  8.4062e+00,  2.1719e+00,
         -5.7812e+00,  1.0125e+01, -1.3750e+00, -6.1328e-01, -1.8750e+00,
         -1.6797e+00, -2.6406e+00,  1.2500e-01,  8.1250e+00,  7.1875e-01,
         -6.6406e-01, -8.0000e+00, -5.4375e+00, -3.5938e-01,  1.6250e+00,
          6.6562e+00,  6.3750e+00,  4.0430e+00, -8.5469e+00,  6.4062e+00,
         -1.1844e+01,  3.0312e+00, -8.8594e+00, -4.2695e+00, -7.9688e-01,
          4.4531e+00,  9.3438e+00,  2.6562e+00, -7.2500e+00,  6.1797e+00,
         -9.6562e+00,  1.0000e+01, -8.0312e+00, -6.0625e+00, -3.5938e+00,
          4.3750e+00,  4.0156e+00, -1.1562e+00,  1.9727e+00, -4.7812e+00,
         -3.0781e+00,  6.9375e+00,  2.8281e+00,  1.0000e+00, -1.7500e+00,
         -6.5625e-01,  3.7500e+00, -4.0625e-01,  6.3125e+00, -3.6094e+00,
         -6.5938e+00,  1.4062e-01, -1.3125e+00, -9.6875e+00, -3.0781e+00,
          7.6875e+00, -7.7109e+00, -9.2188e-01,  8.5156e-01,  1.8105e+00,
          1.4062e+00,  8.3750e+00,  6.1875e+00,  3.9844e+00, -3.3906e+00,
         -4.7344e+00,  6.8750e-01, -3.3711e+00,  2.3438e+00, -9.5938e+00,
         -9.7812e+00, -1.6250e+00,  4.0000e+00,  1.4219e+00, -2.5938e+00,
          9.9375e+00,  7.2500e+00, -6.7500e+00,  7.9062e+00,  1.1375e+01,
          4.5469e+00,  9.6875e+00,  3.3086e+00,  6.5469e+00,  1.0750e+01,
         -7.2607e-01, -2.8125e+00,  5.9688e+00,  9.9062e+00,  9.6250e+00,
          3.9219e+00, -3.9453e-01,  9.1875e+00,  9.3750e-02, -2.6562e+00,
         -5.2812e+00,  4.5625e+00, -1.1125e+01,  7.1562e+00,  7.9688e+00,
          2.3281e+00, -4.5156e+00,  8.8750e+00, -1.2188e+00, -5.0469e+00,
          2.1406e+00,  4.1406e-01,  4.1719e+00,  1.5000e+00, -4.0625e+00,
         -6.4062e-01, -9.6562e+00, -5.0156e+00, -9.8438e+00, -4.6562e+00,
         -7.7812e+00, -5.0000e+00, -8.5625e+00, -5.6562e+00,  1.5391e+00,
         -2.0312e-01,  3.8047e+00,  1.0812e+01, -3.4258e+00, -6.0156e+00,
          9.2188e+00,  2.4453e+00,  1.7031e+00, -9.8438e-01, -8.6719e-01,
          1.0375e+01,  3.0234e+00,  2.0031e+01,  6.2500e-02, -6.5625e-01,
         -1.5986e+00,  1.0938e+01, -3.5781e+00, -5.2188e+00,  7.2188e+00,
         -8.9688e+00, -6.7578e-01,  3.5625e+00, -5.6875e+00, -3.1875e+00,
         -9.5938e+00,  1.0312e+00,  7.6875e+00,  3.3125e+00, -6.1719e-01,
         -3.9062e-01,  3.6562e+00,  3.0781e+00,  1.0688e+01, -7.6250e+00,
         -3.3906e+00, -2.2344e+00,  5.0859e+00, -7.0000e+00,  8.0938e+00,
         -5.0703e+00,  8.9062e-01,  3.4688e+00,  8.5312e+00,  3.7500e-01,
         -4.6406e+00,  8.3750e+00, -2.3125e+00, -9.3438e+00, -2.4062e+00,
          1.6812e+01,  7.1484e-01, -4.0430e+00, -2.3438e-01, -1.3188e+01,
         -1.4766e+00, -1.2125e+01, -9.4375e+00,  5.3750e+00, -5.0938e+00,
         -5.3906e-01,  3.8984e+00,  9.3125e+00, -5.8594e+00,  8.8438e+00,
          8.6250e+00, -8.7188e+00,  9.0938e+00,  1.3125e+00,  2.9492e-01,
          9.1562e+00,  2.5156e+00,  3.5938e+00,  1.2578e+00,  1.8730e+00,
          5.3906e-01,  7.0312e-01, -5.7266e+00, -8.3125e+00,  1.0281e+01,
         -6.9688e+00,  1.9688e+00, -2.3906e+00, -2.0312e-01,  5.5625e+00,
         -8.3750e+00, -6.6562e+00,  8.8438e+00, -1.1281e+01, -3.0000e+00,
         -1.0625e+01, -7.9531e+00, -6.6406e-01,  5.6875e+00, -2.7031e+00,
         -1.2750e+01,  8.2812e+00, -8.0312e+00,  3.3750e+00, -1.2062e+01,
          6.0156e-01,  3.7344e+00, -8.8438e+00, -8.0469e-01,  1.3125e+00,
         -2.5820e+00, -4.5000e+00,  3.7500e+00,  3.1875e+00, -8.0938e+00,
          5.7812e+00, -4.4922e+00, -1.3219e+01,  2.2891e+00, -6.4062e-01,
         -1.7812e+01,  1.2375e+01,  4.2812e+00,  4.4219e+00, -4.8047e-01,
         -1.4062e+00,  9.5000e+00,  9.8125e+00,  1.4062e+01, -3.4688e+00,
          1.9219e+00,  2.6562e+00, -6.2812e+00,  8.5938e+00,  2.4062e+00,
          2.9141e+00,  1.2266e+00, -7.2188e+00, -4.6875e+00, -4.1406e+00,
         -1.5625e-02, -1.7344e+00, -3.4062e+00, -2.7500e+00,  1.4406e+01,
          7.6562e+00, -5.5625e+00,  2.2812e+00, -4.8750e+00, -1.4562e+01,
          6.4375e+00, -1.2125e+01,  9.5938e+00,  2.9180e+00, -5.7031e-01,
          3.1094e+00, -3.9043e+00,  3.7500e-01,  2.3203e+00, -5.5625e+00,
          3.1094e+00, -9.0938e+00, -2.9688e+00, -6.7500e+00,  4.9531e+00,
          3.9375e+00,  7.3828e-01, -7.1250e+00, -3.2812e+00, -1.1719e+00,
         -1.0625e+00,  1.2094e+01,  4.7812e+00, -4.7031e+00, -1.5312e+00,
          2.7188e+00, -5.2031e+00,  1.3438e+00, -1.2750e+01,  8.0625e+00,
         -5.8125e+00,  2.0000e+00]], device='cuda:4', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 6 terminated with the following error:
Traceback (most recent call last):
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 693, in train_step
    with maybe_no_sync():
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 550, in _per_task_train_loss
AssertionError

/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-09-02 22:27:50 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:10742
2023-09-02 22:27:50 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:10742
2023-09-02 22:27:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-09-02 22:27:50 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:10742
2023-09-02 22:27:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-09-02 22:27:51 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:10742
2023-09-02 22:27:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-09-02 22:27:51 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:10742
2023-09-02 22:27:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-09-02 22:27:51 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:10742
2023-09-02 22:27:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-09-02 22:27:51 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:10742
2023-09-02 22:27:51 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:10742
2023-09-02 22:27:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-09-02 22:27:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-09-02 22:27:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-09-02 22:27:51 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 22:27:51 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 22:27:51 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 22:27:51 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 3
2023-09-02 22:27:51 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 2
2023-09-02 22:27:51 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 1
2023-09-02 22:27:51 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 22:27:51 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 5
2023-09-02 22:27:51 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 22:27:51 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 0
2023-09-02 22:27:51 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 22:27:51 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 22:27:51 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 4
2023-09-02 22:27:51 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 7
2023-09-02 22:27:51 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-02 22:27:51 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 6
2023-09-02 22:27:54 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10742', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=False, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=False, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=False, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-09-02 22:27:54 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-09-02 22:27:54 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-09-02 22:27:54 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-09-02 22:27:54 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-09-02 22:27:54 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-09-02 22:27:58 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-09-02 22:27:58 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-09-02 22:27:58 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-09-02 22:28:00 | INFO | root | load pretrained hubert
2023-09-02 22:28:07 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-09-02 22:28:11 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-09-02 22:28:18 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-09-02 22:28:18 | INFO | root | share the sematic adapter and textual encoder
2023-09-02 22:28:18 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-09-02 22:28:18 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-09-02 22:28:18 | INFO | fairseq_cli.train | model: S2TJoint
2023-09-02 22:28:18 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-09-02 22:28:18 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-09-02 22:28:18 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-09-02 22:28:18 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-02 22:28:18 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 22:28:18 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 22:28:18 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-02 22:28:33 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-09-02 22:28:33 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-09-02 22:28:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-09-02 22:28:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-02 22:28:34 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 22:28:34 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 22:28:34 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 22:28:34 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 22:28:34 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 22:28:34 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 22:28:34 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 22:28:34 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-02 22:28:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-02 22:28:34 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-09-02 22:28:34 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-09-02 22:28:34 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_last.pt
2023-09-02 22:28:36 | INFO | fairseq.trainer | load the task parameters
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
2023-09-02 22:28:53 | INFO | fairseq.optim.adam | using FusedAdam
2023-09-02 22:28:54 | INFO | fairseq.trainer | Loaded checkpoint ./checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/checkpoint_last.pt (epoch 34 @ 50000 updates)
2023-09-02 22:28:54 | INFO | fairseq.trainer | loading train data for epoch 34
2023-09-02 22:28:54 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-02 22:28:54 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 22:28:54 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-02 22:28:57 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-02 22:28:59 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-02 22:29:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-02 22:29:40 | INFO | fairseq.trainer | begin training epoch 34
2023-09-02 22:29:40 | INFO | fairseq_cli.train | Start iterating over samples
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
task_net module.module.task_net.layer_norm.weight True tensor([-0.7480, -0.7930, -0.1367,  0.1099, -0.0688], device='cuda:0',
       dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([-0.9102, -0.5195,  0.6211,  0.0979,  0.1099], device='cuda:0',
       dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.1724,  0.1626,  0.0784,  ..., -0.4067, -0.7427, -0.2292],
        [ 0.9331, -0.2539,  2.1719,  ..., -2.3105, -2.1367, -1.0488],
        [-0.3271, -0.3894,  0.2124,  ..., -0.7710,  0.8638, -0.9336],
        [ 0.1206,  0.0342,  0.0579,  ...,  0.4521, -0.1655,  0.3591],
        [-0.9565,  0.5210, -2.0859,  ..., -0.4087,  2.6953, -2.9453]],
       device='cuda:0', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 3.4883,  1.5938, -2.8359,  0.5898, -7.3516], device='cuda:0',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 0.0355,  0.0613, -0.0469,  ..., -0.0413,  0.0117, -0.0029],
        [ 0.0126,  0.0050, -0.0272,  ..., -0.0621, -0.1294, -0.0121],
        [-0.0214, -0.0120,  0.0361,  ...,  0.0190,  0.0986,  0.0135],
        [-0.0113, -0.0230,  0.0086,  ..., -0.0649, -0.1343, -0.0137],
        [ 0.0023,  0.0086,  0.0034,  ...,  0.0412,  0.1194,  0.0090]],
       device='cuda:0', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([-0.1602, -0.1580,  0.1514, -0.0599,  0.0583], device='cuda:0',
       dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[  8.7656,  17.2500,  -3.1094,  12.4375, -10.5312,  15.3125,  14.3438,
         -14.9062, -21.8750,   6.8125, -15.0000,  -5.9219,  -7.5156, -21.4375,
          -1.7734,  14.6250, -14.1250,   5.1406, -29.8750,  -0.0508,  19.3750,
          -3.5742,   7.9844,  11.7188,   1.0000,  -1.3828, -10.2812, -28.6875,
          28.3125,  19.0625, -13.5312,   4.9844, -35.6250,  11.4062,  -2.7969,
           6.6094,   3.1172, -19.8438, -15.6562,  16.3750, -21.2500,  -6.9922,
          -3.0469,   5.1094,   4.2344, -10.0156,  13.8125,  23.2500, -17.2812,
           6.5938,   2.2656,  -2.8984,  10.0312,  11.3047,  14.2812,  18.0000,
          18.0625,  19.4375,  19.9375,  18.3125,  -0.2812, -16.9062,   7.9375,
         -19.8438, -10.6562,  13.0625,  -1.6250,  20.0000,   8.3438, -15.0312,
          21.5625, -23.9375, -12.9062,  11.6250,   9.9688,  10.9375,  20.1250,
           5.7812, -12.5938,   4.2188, -10.6562, -24.3125,  -1.2188,   6.8438,
           0.3281, -13.2578, -17.8750, -13.8438,  10.1562,  -1.0039, -22.1875,
         -17.7500,  20.0312,   4.1094, -13.4062, -12.1562, -22.0000, -21.6250,
          -8.0000,  15.7188, -21.8750,  -0.1719,  -7.7500,  -0.4062,   4.4531,
         -12.8906, -14.3125, -11.8125,  22.5000,  -4.5625,  -1.1289, -25.6250,
          -9.3125,   8.2812, -24.1250,  -1.7344,   9.0156,  14.5938,  15.0312,
           5.3672, -12.7188,  -7.4375,   1.7812,  14.2812,   3.0938, -12.0625,
          13.4062,  -4.1875,  -9.3750,   3.2031,  22.3125,   7.6523, -14.8125,
         -10.0156,  -8.1250,  -9.3438, -24.5625,  -7.5391,  18.2500, -16.0625,
          16.1875,  -1.9219,  23.8125,   2.2344,  20.1875, -25.3125,   1.3027,
           3.2988,  -4.7344, -15.1562, -21.3438,  -9.9922,   1.6094,   1.4102,
           0.2358, -11.7500, -11.5625, -16.9844,  11.3750,  14.0938,  16.9375,
          16.8125,  16.1875,   3.1250,   3.2656,   1.8125,  11.6562,   9.3906,
         -12.0312,  -7.8125, -11.9375,  15.0312,  15.8750,  -8.2969,  -2.1152,
          -4.6719, -10.3750,  21.5312,   5.6406,   7.2500,  12.6875, -21.5000,
           4.2344,   7.1094,  13.6875,   4.6406,   1.0156,  -1.5156, -32.1250,
           4.5625,  -3.3438,  -9.9688,  -3.8125,   3.5938,  -4.2109,  -8.1250,
          -4.7891,  15.5312,  -3.3125,   7.3438,  10.1562, -20.7500, -12.7500,
           6.2344,   9.7969,  -6.5625,  -0.5391,  17.0938,  12.6875,  10.6406,
           1.8398,  -8.5938, -22.8125, -24.5625,  -9.1875,   5.6875, -25.0000,
           7.6875,  -5.1484,  16.7812,   2.8125,  -4.3438,  10.9375, -11.9375,
          -4.4219,  -9.1562,  22.8750,  14.5625,   9.2266,  10.1406, -16.9375,
         -18.8750,  -2.9844,   1.2500, -17.2500,  13.2500,  -5.6719,   4.3438,
           3.9844,   9.6562,   3.3750, -11.4062,  -6.0469,  27.3125,   2.7969,
          17.1250, -23.1250,  13.8750,  -0.1875,  13.4375, -13.4688,   6.0156,
          10.0625,   3.0469,  27.8750,   9.7500, -22.7500, -10.2500,  -4.3906,
         -11.4766,  16.7812,  -7.7812,  -5.4492,  -9.4375,   6.5781,  18.3125,
          12.7656,   5.0469,  18.0625,   5.6094, -23.6875,  -0.0938,   4.6875,
          -7.6719,   4.1523,   7.4844, -16.8125, -10.9062, -10.8125,  10.7969,
          15.8750, -14.9688,  -0.7969,  -8.8594,  16.5625,  19.0625,   6.7344,
         -33.8750, -12.4688,   3.9922, -27.3750, -13.7500,  14.9375, -11.9375,
         -25.3125, -11.3750, -21.9375,  -2.1016, -13.5781, -16.7500,  -2.1289,
         -18.8125,  -7.6562, -27.1250, -25.8125,  -7.5312,  -0.6328, -18.4062,
           8.6875,   6.2812,   3.7656,  -4.3438,  19.3750, -17.8125, -18.2500,
          18.5000,   9.2812, -10.8750, -20.6562,  11.5312,  -2.2656,   4.3281,
         -11.7188,  20.4531,  23.5312,   0.5000,  21.0000,   0.0781,  18.4375,
          14.6562,  14.2500,  26.9375,  12.2188,  23.1562,  -1.4766, -12.8438,
          -5.5469, -12.0625,  -1.0742,  13.1875, -26.1875,   1.0469,  19.4375,
           8.5000,   3.4141, -27.2500,  -8.2188, -20.3750,  11.8750,   6.7031,
          -2.1016,   1.8594,   5.4688,  -3.7500,  -6.1875,  20.3125,  -4.3477,
          -7.1250,  15.8438,   8.8750,  26.2500, -11.7969, -14.6250, -14.6875,
          -1.5781,   7.4688, -21.5625,  -5.2344, -24.8125,  10.2500,  13.1875,
           8.6250,   1.5625,  29.4375, -15.9062,  -5.1484,  19.9375,  -5.0156,
          -9.5000,  -9.4688,   9.3750, -16.3750,  20.1875,  20.8750,  12.9844,
         -20.4375,  -2.9375,  -0.6484,  15.0781,  24.3125,   2.9648,  25.5000,
          15.8750, -13.4062,  14.1250,  -4.0469,  -1.0469, -17.6875,  13.2188,
         -22.9375, -15.2500,  20.1875,  -5.0938,  -5.7344,  -1.5078, -21.4375,
          -1.5078, -10.5469,  -6.5156,   0.2695,  -8.2656, -12.9531,  -3.3125,
          30.9375, -19.7500,  12.2969,  -8.2656,  12.2812,  -9.7500,   4.4844,
          16.6250,  10.2500, -18.1250,  17.7500,   9.1562,  12.5000,  10.5312,
          -9.7734, -20.0625,   3.5000,  21.9375, -11.7812,  17.8125, -21.8125,
          16.9375,   1.2891,  -6.1406,  17.9375,   4.7656,  13.7188,   0.3984,
           6.7812,   0.4531, -19.3438,  14.5000, -15.0938,   4.7188,  15.3125,
          -2.2656,  14.4062,  32.2500, -32.6875, -13.5625,  -9.7500,   8.1094,
          -6.3047, -23.6250, -18.7500, -29.4375,  21.3125,  -5.0000,  -9.8438,
          14.1875, -19.1875, -10.3438,  -5.9844,   8.9297,  11.0625,  23.3750,
          10.2188,  10.5000,  10.0625,  12.5000,  15.6875, -13.6250, -12.4375,
          21.3125, -12.7812,  26.6250,  24.3750, -20.4375,  25.5000, -13.0000,
           8.0625,   7.7969,  -8.9219,  -0.3984,  13.3125,   5.3672,   9.0000,
          19.6875,  10.6875,   6.8125,  19.8750, -10.0625,  -8.1562,  -0.9219,
          17.8125, -14.2969,  13.2188,  12.2812, -24.0625,  -9.2500,   2.9531,
          13.7812,  -5.1562,  12.0625, -10.0625,  30.1875, -21.6250,  -1.3594,
         -12.6562]], device='cuda:0', dtype=torch.float16)
2023-09-02 22:29:49 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v3_merge_wmt_0902_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5/crash.pt
task_net module.module.task_net.layer_norm.weight True tensor([ 0.0850,  0.1484,  0.0551, -0.0308,  0.0095], device='cuda:5',
       dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([ 0.2705,  0.1543, -0.1841, -0.0289, -0.0327], device='cuda:5',
       dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.0502, -0.0452, -0.2593,  ..., -0.3130, -0.0463,  0.1808],
        [ 0.1670, -0.7197, -0.6079,  ..., -1.1387, -0.3828,  0.0151],
        [ 0.0129,  0.1445, -0.2354,  ..., -0.4092, -0.0967,  0.1145],
        [-0.0080,  0.0262,  0.0833,  ...,  0.0486, -0.1266,  0.0314],
        [ 0.2568,  0.0157, -0.5835,  ..., -0.0256,  1.3887, -0.1438]],
       device='cuda:5', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 1.8711,  3.3047,  1.2383, -0.3013, -0.8359], device='cuda:5',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 0.0150,  0.0247,  0.0032,  ...,  0.0483,  0.1292,  0.0071],
        [ 0.0029, -0.0052, -0.0004,  ...,  0.0099, -0.0270, -0.0234],
        [-0.0090, -0.0128, -0.0024,  ..., -0.0256, -0.0691, -0.0012],
        [-0.0055, -0.0178, -0.0021,  ..., -0.0165, -0.1003, -0.0159],
        [ 0.0009,  0.0033,  0.0010,  ...,  0.0014,  0.0364,  0.0020]],
       device='cuda:5', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([ 0.0576, -0.0325, -0.0355, -0.0586,  0.0139], device='cuda:5',
       dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[-5.5469e-01, -2.9219e+00,  1.2637e+00, -3.4531e+00,  1.8672e+00,
         -5.4375e+00, -1.6719e+00,  1.2500e-01,  5.5000e+00, -1.5391e+00,
          2.9844e+00,  7.6562e-01,  1.8047e+00,  3.8438e+00,  9.1016e-01,
         -2.5000e+00,  3.2500e+00,  6.7188e-01,  3.0000e+00,  5.2930e-01,
         -1.9062e+00, -1.8359e+00, -2.5000e-01, -7.1094e-01,  2.0469e+00,
         -1.2988e+00,  8.9062e-01,  1.3281e+00, -4.0000e+00, -1.2969e+00,
         -4.7656e-01,  1.7578e-01,  4.8438e+00, -1.6250e+00,  1.4609e+00,
         -1.0547e+00,  1.0645e+00,  1.5156e+00,  3.3594e-01, -3.1875e+00,
          1.6719e+00,  1.4453e-01,  1.2422e+00,  1.1719e-01, -4.2578e-01,
         -3.0254e+00, -1.3281e+00, -1.7344e+00,  4.0625e-01, -1.4727e+00,
         -1.2617e+00, -8.4229e-01,  7.0312e-01,  1.5547e+00, -7.2656e-01,
         -1.2969e+00, -3.0000e+00, -2.5625e+00, -2.2969e+00, -2.1094e+00,
          2.0957e+00,  2.6719e+00, -2.2344e+00,  2.5000e+00,  7.8125e-01,
         -3.8906e+00,  3.1055e-01, -2.8750e+00, -3.2969e+00,  2.9531e+00,
         -3.1250e+00,  2.8750e+00,  6.8750e-01, -4.0781e+00, -2.3047e+00,
         -1.1875e+00, -2.8750e+00, -1.5859e+00,  1.4062e+00,  1.0059e+00,
         -3.5156e-01,  2.0000e+00, -1.8945e-01, -1.5234e-01,  1.5664e+00,
         -7.6562e-01,  1.5156e+00,  8.1250e-01, -2.1016e+00, -1.6980e-01,
          2.6094e+00,  2.0625e+00, -1.7344e+00, -1.6250e+00,  2.2344e+00,
          2.4219e-01,  3.8281e+00,  2.7969e+00, -6.5234e-01, -5.6250e-01,
          2.1875e+00,  9.2578e-01,  2.4062e+00, -2.5312e+00, -1.0312e+00,
         -1.6494e+00,  1.6719e+00,  2.4531e+00, -2.8438e+00, -1.3760e+00,
         -4.8828e-02,  2.3750e+00,  7.4219e-01,  6.8750e-01,  1.1406e+00,
          1.5469e+00, -1.3281e-01, -9.5312e-01, -3.4688e+00,  2.2031e+00,
          2.3438e-02,  3.7344e+00,  3.7188e+00, -2.3281e+00, -2.8828e+00,
          5.0000e-01, -1.3516e+00,  3.1250e-02,  1.5234e+00,  2.3438e-02,
         -1.4062e+00,  1.1387e+00,  1.5938e+00, -3.1250e-01,  1.6406e+00,
          2.0156e+00,  5.9375e+00, -5.4688e-02, -3.9062e+00,  9.0625e-01,
         -1.2188e+00, -2.6602e+00, -2.3750e+00, -1.0020e+00, -4.0000e+00,
          2.2812e+00,  8.1445e-01,  9.5117e-01,  8.4766e-01,  2.6406e+00,
          1.0312e+00, -1.4062e+00, -2.2266e+00, -2.3242e-01, -4.4189e-01,
         -6.1719e-01,  1.4609e+00,  1.5625e-01, -8.7500e-01, -1.1406e+00,
         -1.6250e+00, -3.0625e+00, -3.0625e+00, -2.5000e+00,  3.9023e+00,
         -1.9453e+00, -2.3594e+00, -1.0938e-01,  1.8203e+00,  1.9297e+00,
          2.1406e+00, -7.1094e-01, -2.2500e+00, -4.2578e-01, -4.2236e-01,
         -2.4512e+00,  7.9688e-01, -2.1875e-01, -5.3281e+00, -1.6875e+00,
         -2.8125e+00,  2.5781e+00, -1.2305e+00,  9.1992e-01, -3.0234e+00,
         -1.0391e+00, -1.6953e+00, -1.1748e+00,  5.9375e-01,  2.4531e+00,
          3.0234e+00,  3.0781e+00, -2.2852e+00, -1.7812e+00, -1.8037e+00,
          4.2969e-01, -1.9473e+00, -2.0703e+00,  1.9258e+00, -3.7500e-01,
         -2.6250e+00,  3.1094e+00,  3.3906e+00, -8.0859e-01,  5.1758e-01,
          2.7109e+00,  2.3047e-01, -1.0625e+00, -3.8281e+00,  2.4844e+00,
          1.3496e+00,  1.2422e+00,  3.9844e+00,  1.7656e+00,  1.1484e+00,
         -1.4453e+00,  3.2031e+00,  3.4375e-01, -1.1348e+00, -1.5781e+00,
          1.0156e-01, -9.3555e-01, -6.8750e-01,  2.8594e+00,  1.6406e+00,
          1.1914e+00, -1.1250e+00, -2.6562e+00,  3.6328e-01,  3.2812e-01,
          3.1406e+00,  2.7188e+00,  1.1797e+00, -3.7500e+00,  2.7969e+00,
         -3.9062e+00,  8.2812e-01, -2.5000e+00, -9.1797e-01, -6.7188e-01,
          1.3906e+00,  3.2812e+00,  1.6172e+00, -2.8438e+00,  2.6641e+00,
         -3.1719e+00,  2.1562e+00, -2.5938e+00, -3.3047e+00, -4.2969e-01,
          9.5312e-01,  8.9893e-01, -1.7812e+00,  1.6758e+00, -1.5156e+00,
         -8.5156e-01,  2.5156e+00,  2.6406e+00,  1.2188e+00, -3.7109e-01,
         -9.3750e-02,  2.2812e+00, -1.4736e+00,  1.3906e+00, -2.2305e+00,
         -2.8750e+00, -1.1484e+00, -7.8125e-01, -5.1250e+00, -8.5156e-01,
          2.4062e+00, -5.1523e+00, -6.0938e-01,  4.9609e-01,  6.7578e-01,
          1.8223e+00,  2.4062e+00,  3.0938e+00,  1.0703e+00, -2.5000e-01,
         -2.8047e+00,  7.8125e-03, -5.3906e-01, -7.0312e-01, -2.6562e+00,
         -1.9062e+00, -1.6016e-01,  1.0625e+00,  8.7500e-01, -1.2031e+00,
          2.0000e+00,  3.8281e+00, -2.0781e+00,  3.8125e+00,  3.3750e+00,
          1.6328e+00,  3.6875e+00,  1.1055e+00,  7.7344e-01,  1.2656e+00,
          4.3506e-01, -7.0312e-02,  1.4062e+00,  3.0938e+00,  3.2344e+00,
          2.2031e+00, -1.7793e+00,  2.9219e+00,  7.8125e-02, -1.9609e+00,
         -2.2656e+00,  2.1953e+00, -3.8125e+00,  3.4062e+00,  3.2188e+00,
         -1.7188e-01, -1.6328e+00,  3.1562e+00, -1.0469e+00, -9.9219e-01,
          1.4883e+00, -7.5391e-01, -1.5625e-02, -2.4219e-01, -1.0000e+00,
          2.3828e-01, -3.1562e+00, -2.9219e+00, -4.1094e+00, -1.5312e+00,
         -1.7812e+00, -3.5781e+00, -3.2500e+00, -2.2656e+00,  5.6055e-01,
          1.0391e+00,  2.6562e+00,  3.5156e+00, -3.0762e-01, -6.4062e-01,
          3.6719e+00, -2.5898e+00,  9.2969e-01,  5.8203e-01,  2.0703e-01,
          3.4062e+00,  4.0625e-01,  6.6250e+00, -3.7500e-01, -5.3516e-01,
         -5.8594e-03,  4.8867e+00, -2.5781e+00, -1.5000e+00,  1.9375e+00,
         -1.7969e+00, -1.2871e+00,  2.9570e+00, -1.9844e+00, -1.2656e+00,
         -1.8125e+00,  9.6875e-01,  2.5156e+00,  3.9062e-01,  1.2246e+00,
         -1.3633e+00,  2.5000e+00,  1.2969e+00,  1.8125e+00, -4.8438e+00,
         -4.4531e-01, -1.3906e+00,  1.4219e+00, -3.8594e+00,  1.3750e+00,
         -1.6113e+00,  1.1875e+00,  1.1406e+00,  3.2500e+00,  1.6641e+00,
         -1.7734e+00,  2.9531e+00,  2.3438e-01, -1.7500e+00, -2.3438e-01,
          5.5312e+00,  5.4102e-01, -8.5938e-01,  3.9453e-01, -3.6094e+00,
          5.7812e-01, -4.5000e+00, -4.0625e+00,  2.2812e+00, -3.7656e+00,
          9.9609e-01,  3.5156e+00,  3.0781e+00, -1.9219e+00,  3.0000e+00,
          3.6406e+00, -1.9375e+00,  2.7188e+00,  3.6719e-01, -4.5703e-01,
          3.8906e+00,  6.8359e-02,  3.7422e+00, -3.9844e-01,  8.6621e-01,
         -2.3438e-01,  1.0312e+00, -3.0508e+00, -1.3281e+00,  4.3594e+00,
         -4.0234e+00,  1.7734e+00, -2.3281e+00,  1.0234e+00,  2.1074e+00,
         -4.3906e+00, -3.4844e+00,  3.2344e+00, -3.6719e+00, -8.2812e-01,
         -3.2031e+00, -1.0625e+00, -7.1875e-01,  1.6719e+00, -2.9453e+00,
         -4.6250e+00,  2.3438e+00, -3.7812e+00,  1.5469e+00, -4.8906e+00,
         -8.8672e-01,  2.3203e+00, -3.0156e+00, -1.1055e+00, -5.0000e-01,
         -3.2520e-01, -3.2891e+00,  1.7031e+00,  9.8438e-01, -2.9688e+00,
          4.4531e-01, -6.3672e-01, -3.9375e+00,  1.7930e+00, -4.5312e-01,
         -7.0000e+00,  5.0000e+00,  1.5156e+00,  2.3047e+00,  3.3984e-01,
         -2.3047e+00,  2.2969e+00,  2.7344e+00,  3.6875e+00, -7.9688e-01,
          1.7500e+00,  1.5781e+00, -2.7344e+00,  3.8281e+00,  1.0703e+00,
          1.9180e+00, -2.6953e-01, -1.9375e+00, -6.5625e-01, -2.1094e+00,
          8.0078e-01, -1.0391e+00, -1.3281e-01, -1.3125e+00,  5.4688e+00,
          2.2656e+00, -2.6406e+00,  1.5781e+00, -1.7344e+00, -4.3750e+00,
          2.0469e+00, -3.6406e+00,  2.9375e+00,  2.6367e-01,  1.3906e+00,
          1.1719e+00, -7.1680e-01, -3.9062e-01,  1.4482e+00, -2.8594e+00,
          1.5117e+00, -3.7188e+00, -1.9453e+00, -1.4688e+00,  2.7578e+00,
          2.6406e+00, -6.9922e-01, -1.4844e+00, -1.3555e+00, -1.0703e+00,
         -3.5195e+00,  4.2188e+00,  2.7578e+00, -1.8516e+00, -1.0391e+00,
          2.3516e+00, -6.7969e-01,  3.0469e-01, -4.0625e+00,  1.7188e+00,
         -4.1094e+00,  1.7188e+00]], device='cuda:5', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
task_net module.module.task_net.layer_norm.weight True tensor([-0.0396, -0.0044, -0.0017, -0.0159, -0.0105], device='cuda:1',
       dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([ 0.0771,  0.0439, -0.0522, -0.0083, -0.0093], device='cuda:1',
       dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.0312,  0.0587, -0.2335,  ..., -0.0903,  0.0142, -0.0813],
        [-0.0217, -0.0374, -0.3135,  ..., -0.4004, -0.2323, -0.2496],
        [ 0.0063,  0.1616, -0.2029,  ..., -0.1311,  0.0068, -0.1309],
        [ 0.0262, -0.0367,  0.0546,  ...,  0.0146, -0.1151,  0.0390],
        [-0.1699,  0.0117, -0.6084,  ...,  0.2335,  1.1650, -0.1241]],
       device='cuda:1', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 1.5234,  2.9141,  1.4062, -0.0537, -0.9414], device='cuda:1',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 0.0118,  0.0396,  0.0121,  ...,  0.0009,  0.0972,  0.0028],
        [ 0.0012,  0.0049,  0.0027,  ..., -0.0207, -0.0483, -0.0176],
        [-0.0080, -0.0234, -0.0053,  ...,  0.0038, -0.0380, -0.0040],
        [-0.0065, -0.0190, -0.0060,  ..., -0.0199, -0.0867, -0.0166],
        [ 0.0003,  0.0008,  0.0013,  ...,  0.0053,  0.0212, -0.0049]],
       device='cuda:1', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([ 0.0396, -0.0415, -0.0215, -0.0579,  0.0003], device='cuda:1',
       dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[ 0.7969,  0.3281, -0.0566, -1.9375, -1.0000,  0.6250,  0.8438, -1.3008,
          0.5781,  0.0547,  0.6250, -0.1289,  0.3672, -0.9375,  0.6641,  0.3125,
          0.9375,  0.4883,  0.2500,  0.0547,  1.3281, -1.1621,  0.6406,  0.6172,
          1.2598, -0.8672, -0.1562, -1.4531,  1.0312,  1.8906, -1.4023,  1.2812,
          0.7812,  0.1094,  0.1172, -0.5586,  0.0632, -0.7656, -0.8047,  0.2656,
         -1.1875, -0.1230,  0.6777,  1.1367, -0.5898, -2.2871,  0.4844,  0.7188,
         -0.9375, -0.6250,  0.0781, -0.5166,  1.8945,  2.0742,  1.0234,  1.5000,
          1.3594,  0.0938,  0.4688,  1.2188,  0.7539, -0.1094, -0.6328,  0.4375,
         -0.5391, -0.5781, -0.5088, -0.9688, -0.5547,  0.5625,  0.8906,  0.2812,
         -0.9766, -0.1406, -0.0703,  1.2578,  0.5469, -0.4922,  0.0781,  1.1475,
         -1.3555, -0.2188, -1.0273,  0.8828,  0.5742, -1.4727, -0.4062, -0.6719,
         -0.1328, -0.0864, -0.3594, -0.7500,  0.1250, -1.9492, -0.4375, -0.7891,
          1.0156, -0.2500, -0.9746,  1.6484, -1.5469,  0.8477,  0.4609, -1.3242,
         -0.8672, -2.1660, -0.4062,  0.2422,  0.1875, -1.3496,  0.2363, -1.1875,
          0.3203,  0.4922, -1.8750,  0.7109,  1.2969,  1.0156, -0.5156,  1.5625,
         -0.8906,  1.1875,  1.4883, -0.0078, -1.0703, -0.9375,  0.5859,  0.0195,
         -0.3516,  0.8809,  0.6406,  1.3584, -1.1562, -0.8789, -0.0469,  0.7031,
          0.1562, -0.5742, -0.0625, -0.0625,  0.6250, -1.6055,  1.2656, -0.2031,
         -0.0469, -1.6875,  0.2090,  1.2734, -0.1602, -0.6562, -1.4531, -1.9297,
         -0.3711, -1.1748, -0.3103, -1.6484, -1.0547, -1.7109,  0.4375,  0.3750,
          0.5312, -0.4062,  0.2031, -1.1797,  2.7363, -0.1836, -0.4219,  1.2148,
          0.1250, -0.0664, -0.5000,  1.6562,  0.2969, -1.0352, -0.9297, -1.8086,
         -0.4375,  2.1094, -3.8750, -0.9414, -0.2188, -0.1562, -0.0078,  0.7539,
         -0.3672, -0.0469, -0.8164, -0.7832, -2.0469,  1.2383,  1.3906,  1.4219,
         -1.3164, -0.7500, -0.8848, -0.5234, -1.3477,  0.2031,  0.7148,  0.1172,
         -0.3750, -1.1406, -0.7812,  0.2383,  0.5020,  0.5234,  0.7471, -0.0312,
         -0.9219,  2.4023,  0.7227, -0.1484,  0.8125, -1.4375, -0.1875,  0.2266,
         -1.7500,  0.9609, -1.1963,  1.2188,  0.2949, -1.0840,  0.9297,  0.9609,
          0.1289,  0.1328,  1.1719, -0.6328,  0.2148,  1.2930, -0.7812, -0.2812,
          0.9609, -1.6484, -0.2344, -0.2344, -0.2266, -1.1719, -0.3945,  0.6836,
          0.6328,  0.2500, -0.0391,  1.0781,  1.4785,  0.4531, -1.8750,  0.4219,
         -1.2930,  0.7656, -1.7891,  0.8101,  0.1094,  0.9971,  1.2969,  0.7812,
          0.4531,  0.1875,  0.3047, -0.9453,  1.2109,  0.3125, -1.2539, -0.5547,
         -1.7070,  0.4688,  0.3555, -0.0703, -0.6719,  0.2930, -1.3594, -4.0625,
          0.0898, -0.6992,  0.4766,  1.4512,  0.5156,  0.1875, -0.2500,  0.8594,
          0.0703, -1.9453, -0.2461, -1.3320, -0.1250, -0.2188,  0.3359, -2.6406,
         -0.9766, -0.4316, -0.9219,  0.6406,  0.3594,  0.0625, -1.3750, -0.2188,
         -0.7031,  0.1641,  0.1719, -0.5469,  0.1039, -2.0508, -0.1797, -1.7969,
          0.2344,  0.4297, -0.7021,  0.0781,  1.1289, -0.3438, -1.2617,  0.9844,
         -0.5469, -0.1406, -0.4219,  2.2578, -0.2578, -0.5156, -3.3672,  0.8438,
          0.3125,  0.0547, -1.0781,  2.0781,  1.5156,  0.4404,  0.7969, -1.0352,
         -0.3281,  0.5078,  0.3750, -0.5469, -0.4375,  0.6562,  0.1777, -0.0898,
          1.0156,  1.5938, -0.3799,  0.6797,  0.0312, -1.4707,  2.0156,  1.2461,
          0.2441, -2.1875, -0.0938,  0.8281,  0.4531,  0.4375,  0.0254,  4.1484,
         -0.5391, -1.1475,  0.1172,  0.5781, -1.1094,  1.6445,  1.8438,  0.0859,
          0.9062, -0.6797, -1.1172, -0.1250,  0.7090,  0.1797, -0.6406,  0.8477,
         -1.9375, -1.1875,  0.6875,  0.5859,  1.0039,  0.6250, -0.6719, -1.6758,
          2.3047, -0.2109,  1.0078,  1.0859, -0.0547, -0.7969,  2.2812,  1.2031,
          0.1875,  0.3125,  0.1855, -0.1445,  1.3594,  0.5000,  0.5488, -0.5000,
         -0.4531,  0.5000,  0.1875, -0.4023,  2.6230, -0.0156, -0.1719, -1.2344,
         -0.0469,  1.8594,  1.2266, -0.3047,  0.0259,  0.9531, -0.5127,  2.1172,
         -0.8984,  0.0729, -1.2539,  0.1016, -0.9570,  1.1094,  0.4844, -3.1328,
          0.2656, -0.1875,  0.0859,  1.8652, -0.5781, -0.0859, -0.4062, -1.3438,
          0.7031,  0.1406, -0.1719, -1.7148, -1.2500, -0.8047, -0.7188,  0.4453,
          0.9375, -1.5781, -0.0938, -1.2285,  1.2500,  0.6562,  0.1992,  0.6094,
         -0.1816, -0.4453,  1.2656, -1.4297, -0.4062, -0.3906, -0.6523, -0.4062,
          1.1914,  1.2266, -2.0312,  0.8438,  0.2422,  0.6328,  0.7188, -1.0898,
         -0.4219, -0.1562, -0.0625,  1.9375,  0.1641, -0.0625,  0.9688, -0.0781,
         -0.0430,  0.6172,  0.9121, -0.6875,  1.6562, -0.6797,  1.8203, -0.1875,
          0.3828,  1.0312,  0.9375, -0.2812,  0.3594, -0.6172,  2.5156, -0.5781,
         -1.5469,  0.8750,  0.5781,  0.4590,  1.2148, -0.3125, -0.3574,  0.9141,
          0.8799, -0.2891,  2.3477, -0.9375, -0.0547,  0.7188,  0.5234, -0.2031,
          0.4678, -0.2188, -1.9336,  0.4766, -1.4570, -0.7656,  0.5859, -0.4688,
          0.7500,  0.1484,  0.3750, -1.6133,  0.0625, -0.6562, -3.9824, -0.1484]],
       device='cuda:1', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
task_net module.module.task_net.layer_norm.weight True tensor([-0.6689, -0.6904, -0.1492,  0.1265, -0.0719], device='cuda:2',
       dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([-0.9707, -0.5557,  0.6621,  0.1042,  0.1174], device='cuda:2',
       dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.3506, -0.0823, -0.0337,  ..., -0.3767, -0.5132,  0.0778],
        [ 1.0420, -0.6113,  0.4297,  ..., -2.5137, -1.0215, -0.7114],
        [ 0.0646, -0.0857,  0.1099,  ..., -0.6338, -0.3428, -0.1383],
        [ 0.1199, -0.1530,  0.0734,  ...,  0.2269, -0.1980,  0.1643],
        [-0.9355,  0.5225, -1.7422,  ..., -0.4844,  3.8828, -2.1973]],
       device='cuda:2', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 2.7344, -1.4219, -0.9805,  0.5264, -6.8828], device='cuda:2',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 0.0086,  0.0093, -0.0060,  ..., -0.0422, -0.2168, -0.0168],
        [ 0.0006, -0.0121,  0.0018,  ..., -0.0551, -0.1965, -0.0214],
        [-0.0051,  0.0120,  0.0060,  ...,  0.0457,  0.1582,  0.0177],
        [-0.0057, -0.0164,  0.0055,  ..., -0.0182, -0.0743, -0.0050],
        [ 0.0007,  0.0077,  0.0009,  ...,  0.0162,  0.0704,  0.0023]],
       device='cuda:2', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([-0.2310, -0.1641,  0.1633, -0.0236,  0.0335], device='cuda:2',
       dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[ 7.4219e+00,  1.4375e+01, -3.3867e+00,  1.4531e+01, -1.1078e+01,
          2.5781e+01,  1.4031e+01, -9.4453e+00, -2.7594e+01,  6.4531e+00,
         -1.2078e+01, -6.7812e+00, -6.3906e+00, -2.8031e+01, -1.2695e+00,
          2.0719e+01, -1.8250e+01,  1.9980e+00, -2.6875e+01, -1.3398e+00,
          2.1562e+01, -2.3223e+00,  7.5781e+00,  9.3438e+00, -1.9062e+00,
          2.7734e-01, -8.1250e+00, -2.2844e+01,  2.8844e+01,  1.8250e+01,
         -9.5000e+00,  8.3828e+00, -2.6500e+01,  1.0141e+01, -4.8984e+00,
          4.7578e+00,  1.1738e+00, -1.7250e+01, -1.2312e+01,  1.5906e+01,
         -1.3719e+01, -5.0547e+00, -3.8750e+00,  4.2812e+00,  3.3359e+00,
         -5.6875e+00,  1.2234e+01,  2.5219e+01, -1.3562e+01,  5.0312e+00,
          3.1250e+00, -2.8301e+00,  8.3359e+00,  8.3906e+00,  1.2531e+01,
          1.6719e+01,  1.8719e+01,  2.0156e+01,  1.7688e+01,  1.7344e+01,
          8.9844e-02, -1.5156e+01,  7.9688e+00, -1.7781e+01, -1.2016e+01,
          1.5719e+01, -2.2305e+00,  1.9969e+01,  1.2688e+01, -1.5500e+01,
          2.2344e+01, -2.0750e+01, -1.3234e+01,  1.4938e+01,  1.0047e+01,
          1.1781e+01,  2.0656e+01,  5.9844e+00, -1.2562e+01,  2.3926e+00,
         -8.9297e+00, -2.3406e+01, -4.2930e+00,  6.0156e+00, -4.3594e+00,
         -9.6797e+00, -1.9531e+01, -1.0047e+01,  1.1750e+01,  1.8164e-01,
         -1.9125e+01, -1.7969e+01,  1.5562e+01,  1.6094e+00, -1.7031e+01,
         -1.0500e+01, -2.0438e+01, -1.9344e+01, -4.9844e+00,  1.5438e+01,
         -2.1375e+01, -3.0469e+00, -7.8281e+00,  3.9062e-02,  6.4766e+00,
         -6.2969e+00, -1.5625e+01, -1.2438e+01,  2.4094e+01, -1.5801e+00,
         -2.0781e+00, -2.1000e+01, -9.5781e+00,  7.8125e+00, -2.3844e+01,
         -2.1953e+00,  7.2969e+00,  1.2875e+01,  1.4594e+01,  4.5312e-01,
         -7.2578e+00, -8.0000e+00, -2.1719e+00,  1.3578e+01,  6.6719e+00,
         -1.0625e+01,  1.0438e+01, -4.4219e+00, -9.7031e+00,  1.9492e+00,
          2.1531e+01,  3.2676e+00, -1.5766e+01, -9.1562e+00, -8.8594e+00,
         -8.9062e+00, -2.6219e+01, -4.7930e+00,  1.9250e+01, -1.5375e+01,
          1.1609e+01,  1.2578e+00,  2.3906e+01,  1.7227e+00,  2.1719e+01,
         -2.5719e+01,  1.0664e+00,  1.2969e+00, -5.9922e+00, -1.2938e+01,
         -2.0203e+01, -6.3867e+00,  4.7188e+00,  2.1328e+00, -1.9287e+00,
         -8.4766e+00, -1.1641e+01, -1.2578e+01,  1.1453e+01,  1.4125e+01,
          1.5625e+01,  1.6500e+01,  1.9594e+01,  3.5938e+00,  1.5117e+00,
          4.6641e+00,  9.3438e+00,  7.5391e+00, -1.3359e+01, -8.8594e+00,
         -1.2719e+01,  1.2859e+01,  1.4375e+01, -5.5508e+00, -1.9756e+00,
         -1.8984e+00, -8.4766e+00,  1.8938e+01,  1.5391e+00,  5.5469e+00,
          1.5062e+01, -1.7969e+01,  3.9766e+00,  4.5430e+00,  1.1281e+01,
          5.6484e+00,  2.0781e+00, -6.6602e-01, -2.8625e+01, -1.0391e+00,
         -5.7500e+00, -1.0547e+01,  1.8906e+00,  6.4219e+00, -3.5859e+00,
         -1.0047e+01, -2.6484e+00,  1.2625e+01, -2.7344e+00,  5.4766e+00,
          6.4375e+00, -2.0031e+01, -1.7500e+01,  5.2969e+00,  4.9492e+00,
         -7.6875e+00,  1.3984e+00,  1.7406e+01,  1.5094e+01,  1.0359e+01,
         -6.1133e-01, -1.0203e+01, -2.1719e+01, -2.7688e+01, -6.5469e+00,
          8.4375e+00, -2.6188e+01,  5.2891e+00, -2.4863e+00,  1.7125e+01,
          4.5781e+00, -3.1914e+00,  9.1562e+00, -1.4156e+01, -2.9062e+00,
         -7.9297e+00,  2.0344e+01,  1.0891e+01,  6.4766e+00,  7.8984e+00,
         -1.9375e+01, -1.6562e+01, -3.7891e+00,  4.2031e+00, -1.7969e+01,
          1.5688e+01, -9.4844e+00,  6.1094e+00,  2.4062e+00,  8.3984e+00,
         -3.4062e+00, -1.4406e+01, -7.6719e+00,  2.7750e+01,  3.8281e-01,
          1.8375e+01, -2.1719e+01,  1.4297e+01,  2.6406e+00,  1.0625e+01,
         -1.3234e+01,  1.1152e+00,  8.5312e+00, -1.4844e-01,  2.1750e+01,
          9.8125e+00, -1.8750e+01, -9.5625e+00, -4.0156e+00, -7.5625e+00,
          1.3141e+01, -1.0625e+01, -4.0898e+00, -9.9844e+00,  4.4922e+00,
          1.8188e+01,  8.4062e+00,  4.6562e+00,  1.8875e+01,  8.3672e+00,
         -2.0094e+01, -1.1953e+00,  3.2969e+00, -5.3516e+00,  2.8516e-01,
          6.0000e+00, -1.6656e+01, -1.1172e+01, -1.1078e+01,  8.4219e+00,
          1.3750e+01, -1.2031e+01,  2.4004e+00, -6.5000e+00,  1.5375e+01,
          1.9188e+01,  3.8789e+00, -2.9688e+01, -1.2703e+01,  3.0078e+00,
         -2.2250e+01, -1.3625e+01,  1.3688e+01, -1.3719e+01, -2.3562e+01,
         -1.0609e+01, -2.0656e+01, -9.4531e-01, -1.1125e+01, -1.8062e+01,
          2.6953e-01, -1.2828e+01, -7.5938e+00, -2.3438e+01, -2.3094e+01,
         -9.2500e+00,  1.1172e+00, -1.8719e+01,  6.6094e+00,  5.0312e+00,
          3.9219e+00, -6.5703e+00,  1.8406e+01, -1.6969e+01, -1.7625e+01,
          1.4688e+01,  8.1562e+00, -1.4938e+01, -1.8844e+01,  8.1406e+00,
         -4.4453e+00,  3.7031e+00, -1.2453e+01,  1.3266e+01,  1.9281e+01,
          2.0527e+00,  2.2062e+01,  3.0000e+00,  2.0500e+01,  1.3672e+01,
          1.4906e+01,  2.2531e+01,  1.4031e+01,  1.6375e+01, -7.2656e-01,
         -9.5938e+00, -5.7031e+00, -1.4750e+01,  1.7578e-02,  1.0328e+01,
         -1.9125e+01,  2.2578e+00,  1.4703e+01,  9.5625e+00,  3.4336e+00,
         -2.5656e+01, -4.9922e+00, -2.1500e+01,  9.6562e+00,  5.4609e+00,
         -1.3281e+00,  3.8125e+00,  7.4844e+00,  2.5586e-01, -7.5469e+00,
          1.9531e+01, -2.8652e+00, -6.5000e+00,  1.6500e+01,  9.6719e+00,
          2.4250e+01, -8.5000e+00, -1.3266e+01, -8.9219e+00, -2.0625e+00,
          6.4219e+00, -1.9062e+01, -5.3594e+00, -2.5469e+01,  1.1875e+01,
          1.1609e+01,  7.2969e+00, -3.1953e+00,  2.9531e+01, -1.3000e+01,
         -2.0234e+00,  1.5031e+01, -7.3125e+00, -1.1422e+01, -5.2656e+00,
          8.4844e+00, -1.6281e+01,  1.6641e+01,  1.8125e+01,  1.0594e+01,
         -2.7500e+01, -2.9102e+00,  1.7305e+00,  9.2031e+00,  2.0750e+01,
          1.9258e+00,  2.4406e+01,  1.6188e+01, -1.3750e+01,  1.6625e+01,
         -4.2109e+00,  5.9375e-01, -1.5531e+01,  1.3281e+01, -2.3688e+01,
         -1.5375e+01,  1.9312e+01, -9.3125e+00, -5.1133e+00, -6.5430e-01,
         -2.1344e+01, -3.7383e+00, -4.5156e+00, -4.7266e+00,  1.8643e+00,
         -7.5391e+00, -9.5156e+00,  1.2422e+00,  2.3375e+01, -2.0656e+01,
          6.8750e+00, -7.3516e+00,  1.0516e+01, -7.5859e+00,  1.7188e-01,
          1.8031e+01,  1.2344e+01, -1.9188e+01,  2.0969e+01,  9.6250e+00,
          1.5500e+01,  9.6562e+00, -6.9609e+00, -2.0906e+01,  8.4844e+00,
          2.4062e+01, -1.1188e+01,  2.0281e+01, -2.0375e+01,  1.5750e+01,
          1.7969e+00, -7.1875e+00,  1.9312e+01,  4.9844e+00,  1.0164e+01,
          1.5430e+00,  6.7344e+00, -3.1797e+00, -1.5781e+01,  1.6438e+01,
         -1.2578e+01,  3.5352e+00,  2.0969e+01, -3.2031e+00,  1.3391e+01,
          3.6250e+01, -3.1344e+01, -1.3391e+01, -8.6562e+00,  5.3477e+00,
         -9.1406e-01, -2.2594e+01, -1.6188e+01, -2.8156e+01,  1.6781e+01,
         -6.6875e+00, -7.6719e+00,  1.3734e+01, -2.0094e+01, -7.2266e+00,
         -6.4453e+00,  5.6875e+00,  1.4891e+01,  2.4125e+01,  8.4531e+00,
          9.0938e+00,  1.0531e+01,  9.4375e+00,  1.5438e+01, -2.0812e+01,
         -1.3812e+01,  2.0688e+01, -1.1969e+01,  2.8281e+01,  2.2062e+01,
         -1.9469e+01,  2.2500e+01, -1.5250e+01,  3.6016e+00,  7.7344e+00,
         -6.9375e+00,  1.7773e+00,  9.2656e+00,  1.5469e+00,  1.1016e+01,
          1.1875e+01,  1.1750e+01,  7.3750e+00,  1.5969e+01, -1.2250e+01,
         -1.0016e+01,  1.1250e+00,  1.6438e+01, -1.0594e+01,  1.0266e+01,
          7.6328e+00, -1.9656e+01, -1.0719e+01,  4.6641e+00,  1.1219e+01,
         -3.5469e+00,  7.8281e+00, -7.8125e+00,  2.6844e+01, -1.9156e+01,
         -1.0938e-01, -1.0656e+01]], device='cuda:2', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
task_net module.module.task_net.layer_norm.weight True tensor([ 0.8164,  1.0391,  0.5732, -0.1436,  0.0889], device='cuda:6',
       dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([ 1.2500,  0.7109, -0.8516, -0.1338, -0.1509], device='cuda:6',
       dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 0.6440, -0.0345, -0.1127,  ..., -1.2383, -0.3826, -0.6440],
        [ 4.0430,  0.8926, -0.1094,  ..., -2.9570, -0.9844, -0.6113],
        [ 0.2500,  0.8286, -1.1602,  ..., -0.6265,  0.9111, -1.6738],
        [-0.4319, -0.3499,  0.0151,  ...,  0.0703, -0.4336,  0.8022],
        [-1.7354,  2.4277, -4.2227,  ...,  1.0645,  4.4922, -3.3164]],
       device='cuda:6', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 8.2031, 18.9688,  0.7969,  0.7109, -0.1250], device='cuda:6',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 5.7739e-02,  2.4219e-01,  9.6436e-03,  ...,  6.9531e-01,
          1.2031e+00,  2.8076e-02],
        [ 1.8570e-02,  7.4951e-02, -1.8524e-02,  ...,  2.6416e-01,
          4.0918e-01, -1.8341e-02],
        [-4.2450e-02, -1.6992e-01, -1.9287e-02,  ..., -4.0869e-01,
         -7.8027e-01, -6.5308e-03],
        [-1.6953e-02, -1.0626e-01, -1.7456e-02,  ..., -2.3376e-01,
         -4.9561e-01, -2.5665e-02],
        [ 1.9550e-04,  1.5701e-02, -1.8215e-04,  ...,  4.2297e-02,
          1.3660e-01,  1.6098e-02]], device='cuda:6', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([ 0.3750,  0.0322, -0.2646, -0.2700,  0.0792], device='cuda:6',
       dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[ -8.9062, -22.2500,  13.4375, -16.3125,  13.8750, -36.3750, -23.9375,
          -7.1875,  27.2500, -12.8750,  11.2500,   7.5938,   8.3750,  28.2500,
           1.2500, -21.1250,  27.6250,  -0.5469,   7.5000,   3.4531,  -7.3750,
         -13.7812,   0.0781,  -4.8125,  24.4375,  -2.4609,   8.5000,   6.5000,
         -11.6250,  -9.1875, -13.4375,  -6.6875,  34.7500,  -1.5000,  21.2500,
          -2.6875,  14.4219,   7.5625,   4.7500, -24.0000,  14.0625,  -1.7188,
           5.8438,  -8.8438,  -1.0156, -27.2812, -11.3125, -13.7500,   3.8125,
          -0.9375, -15.9062,  -3.8555,  -0.5625,   6.5625,   5.0000, -11.0000,
         -24.3750, -29.2500, -21.8750, -14.8750,  -3.1328,  21.1250,  -6.4375,
           7.6250,  15.1875, -23.9375,   5.7969, -15.7500, -14.6250,   8.5625,
         -12.0000,  14.0000,  10.6875, -23.1250,  -6.8750, -14.5000, -16.8750,
          -8.5938,  13.3125,   3.8438,   1.4375,   7.5000,   5.2344,  -6.5625,
          16.6875, -10.6875,  22.1250,  -6.2500, -16.6875,  -1.3701,  13.2500,
          22.0000, -13.8750,   7.8906,  16.2500,  -4.9062,  17.5000,  13.6250,
         -14.7344, -11.7500,  11.6250,  21.7812,   9.6250, -11.5000,  -2.6875,
         -10.1250,  11.3750,   1.2500, -23.8750, -18.0312,   2.9062,  14.5000,
           1.0312,   1.6562,   6.0000,  10.6562,  -1.5625,   5.6250, -19.8750,
          25.6719,  -9.0938,  18.1250,  27.8438, -21.9375, -17.0625,   2.6875,
           1.0000,   5.2188,  14.3750,  -2.0625,  -8.0000,  18.5156,  12.1875,
           9.1875,   7.6875,  11.0000,  26.7500,  -4.7031, -10.7500,   8.5000,
         -10.7500, -17.3438,  -7.8750,  -7.1094, -19.2500,  13.5000,   4.5664,
           5.9219,   2.9062,  15.6250,  10.9375,  -4.6562, -11.3125,   3.1914,
          -2.6094,  -6.8750,   6.3750,  -9.9375,  -8.3750,  -8.8125, -14.5000,
         -23.2500, -21.3750, -12.5312,  13.8594, -22.7188, -23.0000,  -4.0000,
          14.0000,  10.5625,  20.6875,  -3.4375, -12.4375,  -4.5312,  -6.2969,
         -22.9531,   0.6250,  -8.5625,   9.7734,  -1.5938, -11.1250,  20.6250,
         -11.8750,   6.4258,   2.5625,  -3.0312, -11.8750, -13.5156,   8.0000,
          28.5312,  28.2500,  13.6250, -24.3125, -13.7500, -15.6016,   0.0000,
         -16.2656,   0.8750,   7.6250,   3.6406,  -2.8438,   6.7500,  23.8750,
           4.1406,   4.6719,  23.3125, -11.7344, -23.2500, -24.5000,  18.9531,
           5.2266,   5.6562,  17.0000,  13.1250,   5.8750, -11.7500,  12.2500,
           3.0312,  -4.8594, -15.0000,  -4.0469,   1.6562, -15.8125,  19.3125,
           6.0000,  -6.0312,  -7.6250, -11.0000,  10.1016,  -1.7188,  17.7500,
          18.7500,   0.3750, -31.8750,  23.6250, -14.8750,  12.2812,  -5.6875,
          -7.3281,  -4.3750,  19.0938,  22.5625,  10.5312,  -7.3750,  22.1562,
         -21.0000,   8.0000, -16.2500, -29.6250,  -6.4375,  10.5000,  11.2812,
          -5.3750,  18.0000, -20.8750, -15.0625,  15.8750,   5.1250,   0.1250,
          -9.3750,   2.1250,  14.0625,  -1.0938,   8.4375,   0.1562, -15.1250,
           3.5000,  -5.8438, -21.7500,  -3.1875,   8.7500, -16.5000,  -1.4844,
          -2.7812,  16.6406,  13.6562,  14.1875,  16.5625,   5.0000,  -4.8750,
          -2.7500,   0.6875,  -9.6641,   3.6562, -19.3750, -17.0000,  -2.2656,
           8.5000,   7.6250,  -1.0000,  11.0000,  25.3750,  -7.0000,  16.5000,
          10.8750,  16.7500,  18.8750,   5.9844,  -6.9375,  10.5625,   1.3984,
         -11.2500,  15.0938,  28.3750,  19.5000,  12.1875, -12.9688,  17.8125,
           7.0312,  -7.0000, -17.8125,  10.7812, -18.6250,  24.1250,  19.3125,
           5.9375,  -6.2500,  28.9375,   4.3750,  -7.3750,  17.2188,  -4.7188,
           8.7500,  19.0312,  -5.3750,  -4.9844, -10.8750, -22.1562, -23.7500,
         -16.7500, -15.2500,  -9.0000, -13.3125,  -6.3750,   3.2188,  -1.6875,
          10.9688,  14.1875,  -4.0586,  -0.5000,  12.2500, -12.1875,   5.3750,
           2.1875,  -1.2656,  15.0000,   1.1562,  40.2500,   3.6875,  -5.2500,
           4.8281,   1.0625, -22.7500,  -4.5078,  17.2500, -10.5000, -14.4766,
          -8.2500, -11.3750, -10.9375,  -8.5000,   2.0625,  20.5000,   3.5625,
          13.8438,  -4.6562,   6.2500,   3.7500,  22.1250, -21.1250,  -3.7500,
          -4.2500,  16.6719,  -8.7500,  17.6250, -15.3828,   3.2500,  12.5312,
          23.3750, -10.2031,  -4.0625,  11.4375,  -2.2500, -16.3750,  -0.3750,
          35.7500,   3.2500, -10.6719,  13.4375, -10.7500,   1.9062, -28.2500,
         -17.7500,  18.3750, -19.0000,  12.5312,  -1.4062,  24.1875,  -4.7500,
          18.6250,  25.5625, -14.3750,  17.8750,  -4.5938,  -4.6328,  14.8750,
           2.5781,  -7.1875,  -0.1406,  -1.2715,   0.5625,  -1.5000, -29.2188,
          -6.1250,  20.8750,  17.8125,   0.7500,  -3.8125,  -4.0312,  14.2344,
         -24.0000, -32.3125,  19.1250, -31.6250, -16.3750, -20.1250,  -5.8750,
          -1.5469,  14.6250, -29.9375, -25.2500,  21.0000, -19.8750,  11.3750,
         -24.5000,   0.7656,   7.3125, -13.3750, -10.0312,   1.0000,  -2.9844,
         -19.8750,  12.3750,  11.0625, -24.4375,   1.1875,  -1.1797, -28.8750,
           6.2188,  -0.8750, -54.5000,  27.1250,  15.6250,   8.5000,  10.7109,
         -23.8438,  18.6250,  14.8750,  19.6250,  -7.6875,  10.4688,   3.4688,
         -17.2500,  16.2500,  -2.4688,   1.3750,   7.5625, -15.0625,  -2.1250,
          -6.1250,   5.6562,   1.6875,   7.5938, -12.5625,  30.5000,  12.0000,
         -13.2500,  16.3750, -16.0000, -31.0000,  14.5000, -10.8750,  22.2500,
           8.1719,   1.3438,   0.2188,  -2.3379,   0.0000,  14.3594, -17.3750,
          18.9688, -29.2500,  -9.9688,  -8.0000,   6.8125,  16.1250, -14.7500,
          -8.3125, -13.0000,   3.3125,  10.2188,  13.7500,  17.4375, -18.0312,
          -2.2500,   3.0625,  -2.6250,   2.1250, -18.0000,  11.5000,  -8.8750,
           1.0625]], device='cuda:6', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
task_net module.module.task_net.layer_norm.weight True tensor([ 0.4121,  0.5283,  0.0758, -0.0909,  0.0349], device='cuda:7',
       dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([ 0.7363,  0.4214, -0.5020, -0.0791, -0.0891], device='cuda:7',
       dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[ 6.8054e-02,  1.8262e-01, -4.8767e-02,  ..., -2.5757e-01,
         -1.5381e-02,  2.1680e-01],
        [ 6.5625e-01,  1.3916e-01, -3.0957e-01,  ..., -1.1309e+00,
         -2.7344e-01, -4.2383e-01],
        [ 2.5366e-01,  5.9326e-02, -9.7900e-02,  ..., -2.9199e-01,
          4.8022e-01, -2.3572e-01],
        [-1.3123e-03, -2.6459e-02,  7.6050e-02,  ...,  6.4697e-02,
         -1.1523e-01,  1.0217e-01],
        [-4.4922e-02,  4.9487e-01, -1.5850e+00,  ..., -4.4434e-02,
          2.3711e+00, -7.0264e-01]], device='cuda:7', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 3.6367,  7.6953,  2.4707, -0.4390,  1.3047], device='cuda:7',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 0.0298,  0.1011,  0.0207,  ...,  0.0612,  0.3376,  0.0076],
        [ 0.0095,  0.0282,  0.0028,  ...,  0.0014,  0.0372, -0.0227],
        [-0.0191, -0.0607, -0.0117,  ..., -0.0356, -0.1497, -0.0066],
        [-0.0083, -0.0294, -0.0086,  ..., -0.0322, -0.1492, -0.0191],
        [ 0.0011,  0.0050,  0.0024,  ...,  0.0088,  0.0576,  0.0024]],
       device='cuda:7', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([ 0.1958,  0.0098, -0.1075, -0.0968,  0.0229], device='cuda:7',
       dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[-4.1953e+00, -1.1031e+01,  1.6797e+00, -1.0359e+01,  6.1328e+00,
         -1.4719e+01, -6.9375e+00,  1.7656e+00,  1.6406e+01, -5.4844e+00,
          8.4844e+00,  3.5586e+00,  4.2578e+00,  1.4594e+01,  2.8359e+00,
         -1.1766e+01,  1.1953e+01, -2.5586e-01,  1.2531e+01,  1.6074e+00,
         -9.0469e+00, -3.0859e+00, -9.8438e-01, -4.0781e+00,  4.5508e+00,
         -2.3887e+00,  5.0156e+00,  1.0109e+01, -1.5594e+01, -7.2188e+00,
          1.2109e+00, -1.8672e+00,  1.8562e+01, -5.3828e+00,  3.0469e+00,
         -4.9219e+00,  2.0781e+00,  7.2031e+00,  5.9844e+00, -1.0828e+01,
          9.1719e+00,  1.3242e+00,  3.1133e+00, -2.0000e+00, -3.2070e+00,
         -3.4941e+00, -7.1094e+00, -9.6875e+00,  4.9219e+00, -4.0391e+00,
         -4.1367e+00, -1.0020e+00, -1.6328e+00,  5.9375e-01, -3.9609e+00,
         -5.2969e+00, -9.9219e+00, -1.1938e+01, -8.7969e+00, -9.7500e+00,
          3.2344e+00,  9.8438e+00, -6.2656e+00,  1.0359e+01,  7.2031e+00,
         -1.2125e+01,  1.7754e+00, -1.0891e+01, -9.5469e+00,  9.8125e+00,
         -1.1578e+01,  1.3188e+01,  6.4844e+00, -1.2391e+01, -6.0703e+00,
         -4.6484e+00, -1.1531e+01, -5.7188e+00,  6.4531e+00,  1.0303e+00,
          2.7500e+00,  1.0719e+01,  7.1094e-01, -1.4609e+00,  3.9453e+00,
          8.6719e-01,  1.2047e+01,  3.6094e+00, -6.2812e+00,  2.2363e-01,
          1.1031e+01,  9.6406e+00, -6.2344e+00, -4.0508e+00,  7.6562e+00,
          3.8906e+00,  1.2734e+01,  1.0672e+01, -6.5527e-01, -4.6250e+00,
          1.1109e+01,  4.2227e+00,  6.0469e+00, -5.7344e+00, -4.2734e+00,
         -1.5703e+00,  8.0781e+00,  7.1406e+00, -1.1500e+01, -2.9805e+00,
          3.6133e-01,  1.0016e+01,  3.4609e+00, -2.2891e+00,  8.0938e+00,
          3.8750e+00, -2.2578e+00, -3.6875e+00, -8.7969e+00,  2.5176e+00,
          1.0977e+00,  7.5625e+00,  6.5859e+00, -7.6406e+00, -6.8984e+00,
          4.1250e+00, -3.6797e+00,  2.5703e+00,  4.7734e+00, -1.6016e+00,
         -1.2250e+01,  2.2246e+00,  7.5000e+00,  2.7344e+00,  5.1016e+00,
          6.9844e+00,  1.7062e+01,  1.5117e+00, -1.0531e+01,  7.8438e+00,
         -6.9219e+00, -5.7344e+00, -1.2562e+01, -1.5156e+00, -1.2203e+01,
          1.0422e+01,  1.4580e+00,  7.9395e-01,  1.9961e+00,  8.5000e+00,
          7.9531e+00, -3.6719e-01, -4.9062e+00, -1.9473e+00, -4.7314e-01,
          3.8281e-01,  3.8359e+00,  3.4922e+00, -4.9375e+00, -8.0938e+00,
         -7.8594e+00, -9.0312e+00, -8.6719e+00, -6.0312e+00,  4.3672e+00,
         -4.9180e+00, -9.0938e+00, -2.9453e+00,  6.9531e+00,  4.5625e+00,
          8.7812e+00, -4.6562e+00, -8.8750e+00,  1.6875e+00,  8.6914e-02,
         -4.4805e+00,  3.9922e+00, -8.4062e+00, -7.0547e+00, -4.3047e+00,
         -1.0297e+01,  1.1453e+01, -3.3867e+00, -1.7578e-01, -5.6250e+00,
         -2.9102e+00, -4.5117e+00, -1.7607e+00,  1.1500e+01,  5.6055e+00,
          7.3125e+00,  9.9688e+00, -5.8633e+00, -4.3594e+00, -1.8799e+00,
          3.8359e+00, -3.1816e+00, -6.6094e+00,  3.4297e+00, -2.1211e+00,
         -4.7969e+00,  1.1297e+01,  1.0469e+01, -1.7773e+00, -6.6602e-01,
          7.1562e+00, -1.4141e+00, -8.8125e+00, -1.1219e+01,  1.7383e+00,
          2.0918e+00,  4.2188e+00,  1.2094e+01,  1.0531e+01,  4.1328e+00,
         -6.8125e+00,  8.0000e+00, -1.5859e+00, -1.2168e+00, -7.6562e+00,
         -1.9453e+00, -9.3750e-02, -4.5469e+00,  8.3594e+00,  2.6367e+00,
          3.0391e+00, -8.4688e+00, -8.6094e+00, -8.0859e-01, -1.6250e+00,
          1.1641e+01,  1.0062e+01,  3.4570e+00, -8.7188e+00,  1.0750e+01,
         -1.1469e+01,  3.7422e+00, -6.9375e+00, -3.5977e+00, -3.5781e+00,
          3.6133e+00,  9.4844e+00,  4.0000e+00, -1.3375e+01,  4.8828e+00,
         -1.0844e+01,  1.0688e+01, -7.8594e+00, -6.9531e+00, -5.9297e+00,
          5.6250e+00,  1.6953e+00, -4.5938e+00,  3.4082e+00, -9.8750e+00,
         -5.1406e+00,  1.0641e+01,  6.7422e+00,  3.4844e+00,  1.1953e+00,
         -3.9062e+00,  6.6016e+00,  5.9375e-01,  5.3281e+00, -3.9102e+00,
         -1.0000e+01, -3.6562e+00, -1.7148e+00, -1.5266e+01, -3.6797e+00,
          9.4844e+00, -6.8594e+00, -1.7656e+00,  1.9414e+00,  1.0078e+00,
          1.0840e+00,  1.1672e+01,  8.5938e+00,  4.0078e+00, -3.8047e+00,
         -8.0000e+00,  3.1562e+00, -8.3398e-01,  2.0938e+00, -1.1844e+01,
         -9.3438e+00, -1.6211e+00,  1.1969e+01,  4.5234e+00, -2.3281e+00,
          1.1688e+01,  1.0234e+01, -8.0781e+00,  1.0125e+01,  1.1328e+01,
          5.9922e+00,  1.2781e+01,  2.0664e+00,  3.7109e+00,  7.7031e+00,
          6.1816e-01,  2.1328e+00,  4.9375e+00,  1.3000e+01,  1.1750e+01,
          6.3438e+00, -2.1680e+00,  8.4219e+00, -1.5078e+00, -3.7734e+00,
         -5.4688e+00,  5.0781e+00, -1.4047e+01,  1.0297e+01,  1.1531e+01,
         -2.5859e+00, -5.1172e+00,  1.0172e+01,  4.4844e+00, -3.7578e+00,
          3.0586e+00, -2.8047e+00,  4.7344e+00, -1.7188e+00, -6.2031e+00,
         -3.9844e-01, -1.2203e+01, -5.8594e+00, -1.2875e+01, -7.4375e+00,
         -8.9219e+00, -1.0250e+01, -8.4844e+00, -9.6719e+00,  1.7402e+00,
          3.6953e+00,  4.8047e+00,  1.1703e+01, -7.3145e-01, -5.2969e+00,
          1.1750e+01, -3.3984e+00, -2.1328e+00, -1.3906e+00, -2.0859e+00,
          1.3500e+01,  4.2031e+00,  1.7062e+01, -3.5000e+00, -3.1211e+00,
          2.3828e-01,  5.4531e+00, -6.6172e+00, -1.8047e+00,  5.8281e+00,
         -1.0422e+01, -2.6895e+00,  5.6484e+00, -6.7031e+00, -4.3750e+00,
         -7.9062e+00,  3.5859e+00,  7.6719e+00,  5.6484e+00,  1.8242e+00,
         -3.4414e+00,  8.7812e+00,  4.1250e+00,  1.2219e+01, -1.1203e+01,
         -4.7578e+00, -4.5391e+00,  3.9961e+00, -1.2031e+01,  9.9375e+00,
         -3.4473e+00, -2.1875e+00,  3.2500e+00,  8.8438e+00,  2.3711e+00,
         -4.8672e+00,  8.0781e+00, -6.0000e+00, -1.0531e+01, -4.0625e+00,
          1.6562e+01,  1.3281e+00, -1.8242e+00, -6.0938e-01, -1.4094e+01,
         -8.1055e-01, -1.6625e+01, -1.3203e+01,  9.0469e+00, -1.0500e+01,
          2.9492e+00,  4.6172e+00,  1.1328e+01, -6.2500e+00,  1.2062e+01,
          1.0359e+01, -1.1000e+01,  9.9688e+00,  1.6719e+00, -4.8096e-01,
          1.2969e+01,  1.4062e+00,  7.3516e+00,  2.0391e+00,  6.7188e-01,
          1.2109e+00,  3.3750e+00, -5.1758e+00, -1.0875e+01,  1.3859e+01,
         -6.1641e+00,  4.4766e+00, -5.3672e+00,  3.2891e+00,  3.7832e+00,
         -1.2031e+01, -1.0078e+01,  1.2547e+01, -1.4281e+01, -4.3359e+00,
         -8.7344e+00, -6.4531e+00,  4.8828e-01,  9.0938e+00, -7.1953e+00,
         -1.4438e+01,  9.5156e+00, -1.1234e+01,  9.1094e+00, -1.2359e+01,
         -1.5273e+00,  5.3672e+00, -1.1500e+01, -3.3984e+00, -2.7812e+00,
         -1.1699e+00, -7.4141e+00,  3.9414e+00,  6.1719e+00, -1.0031e+01,
          5.9844e+00, -2.2578e+00, -1.3172e+01,  2.9805e+00, -4.2656e+00,
         -1.7031e+01,  1.7469e+01,  8.2812e+00,  6.3828e+00, -6.2109e-01,
         -2.8281e+00,  1.1000e+01,  8.5938e+00,  1.2812e+01, -7.9688e+00,
          3.6016e+00,  4.8203e+00, -9.1562e+00,  1.0578e+01,  3.4609e+00,
          3.1641e+00, -1.5625e-02, -8.2500e+00, -5.9531e+00, -5.1172e+00,
          2.1875e-01, -5.1406e+00, -1.9688e+00, -6.5938e+00,  1.6938e+01,
          8.1875e+00, -9.3594e+00,  6.4688e+00, -9.0000e+00, -1.5656e+01,
          7.3594e+00, -1.3812e+01,  1.0656e+01,  2.0410e-01, -1.2500e+00,
          3.3047e+00, -2.0195e+00, -2.1094e+00,  1.6162e+00, -7.6094e+00,
         -1.5625e-02, -1.1625e+01, -5.1328e+00, -6.9844e+00,  8.1406e+00,
          6.5156e+00, -7.4023e-01, -9.6094e+00,  0.0000e+00, -4.8516e+00,
         -3.8906e+00,  1.0531e+01,  8.0469e+00, -4.2812e+00, -4.3672e+00,
          5.1406e+00, -5.5156e+00,  2.9297e+00, -1.3719e+01,  1.0891e+01,
         -6.4102e+00,  6.6562e+00]], device='cuda:7', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
task_net module.module.task_net.layer_norm.weight True tensor([ 0.5781,  0.7812,  0.2305, -0.1252,  0.0402], device='cuda:3',
       dtype=torch.float16)
task_net module.module.task_net.layer_norm.bias True tensor([ 0.9629,  0.5498, -0.6562, -0.1033, -0.1165], device='cuda:3',
       dtype=torch.float16)
task_net module.module.task_net.down_proj.weight True tensor([[-0.1475,  0.2778, -0.1736,  ..., -0.5708, -0.2993,  0.2412],
        [ 0.1365,  0.3281, -1.5742,  ..., -2.1211, -0.6890, -0.4756],
        [ 0.0493,  0.3018, -0.4575,  ...,  0.2593,  0.1201, -0.2546],
        [ 0.0282, -0.0801,  0.2664,  ...,  0.0590, -0.1844,  0.1885],
        [-0.6865,  0.4556, -0.9277,  ..., -0.9995,  0.6763, -1.2715]],
       device='cuda:3', dtype=torch.float16)
task_net module.module.task_net.down_proj.bias True tensor([ 4.4883, 11.8281,  1.8398, -0.4707,  1.8438], device='cuda:3',
       dtype=torch.float16)
task_net module.module.task_net.up_proj.weight True tensor([[ 3.6041e-02,  1.5112e-01,  9.2773e-03,  ...,  1.7798e-01,
          4.4629e-01,  2.0020e-02],
        [ 1.0178e-02,  4.9866e-02,  2.4872e-03,  ...,  6.1035e-02,
          9.6191e-02, -1.0262e-02],
        [-2.4994e-02, -9.2529e-02, -1.2512e-03,  ..., -1.1548e-01,
         -3.1055e-01, -1.7181e-02],
        [-1.1879e-02, -4.8370e-02, -9.7809e-03,  ..., -5.7495e-02,
         -2.3059e-01, -3.9032e-02],
        [-1.3351e-05,  6.9313e-03,  3.0117e-03,  ...,  1.8295e-02,
          7.1350e-02, -6.1369e-04]], device='cuda:3', dtype=torch.float16)
task_net module.module.task_net.up_proj.bias True tensor([ 0.2368,  0.0332, -0.1709, -0.1473,  0.0250], device='cuda:3',
       dtype=torch.float16)
task_net module.module.task_net.task_proj.weight True tensor([[-6.0938e+00, -1.6750e+01,  5.3203e+00, -1.4375e+01,  7.3750e+00,
         -2.7125e+01, -1.2594e+01,  1.7031e+00,  2.3625e+01, -9.2344e+00,
          1.3031e+01,  5.1875e+00,  6.9531e+00,  1.8812e+01,  2.6953e+00,
         -1.2969e+01,  1.6719e+01, -1.1719e-02,  1.6438e+01,  3.7266e+00,
         -8.2188e+00, -5.3945e+00, -3.1406e+00, -4.8594e+00,  8.0312e+00,
         -1.7031e+00,  5.9531e+00,  1.1562e+01, -2.3375e+01, -9.8125e+00,
         -6.4062e-01, -3.5156e+00,  3.4188e+01, -6.0781e+00,  5.5938e+00,
         -5.7656e+00,  2.9824e+00,  9.5312e+00,  6.5000e+00, -1.3438e+01,
          1.2062e+01,  5.3906e-01,  3.0312e+00, -4.6094e+00, -4.3750e+00,
         -6.1875e+00, -9.9375e+00, -1.2469e+01,  7.4062e+00, -6.3438e+00,
         -7.4531e+00, -1.6992e+00, -3.5625e+00, -4.6875e-01, -2.9844e+00,
         -8.4062e+00, -1.5531e+01, -1.5438e+01, -1.4281e+01, -9.6250e+00,
          2.0664e+00,  1.2688e+01, -7.8125e+00,  1.2500e+01,  6.8125e+00,
         -1.4438e+01,  1.4688e+00, -1.6656e+01, -1.3094e+01,  1.2469e+01,
         -1.0812e+01,  1.3031e+01,  7.4688e+00, -1.6344e+01, -8.7500e+00,
         -6.0156e+00, -1.3062e+01, -7.9375e+00,  1.0188e+01,  1.5059e+00,
          3.3438e+00,  1.5000e+01,  2.1914e+00, -3.4688e+00,  7.2812e+00,
         -1.2344e+00,  1.6719e+01,  3.7656e+00, -8.7812e+00, -1.2178e+00,
          1.4156e+01,  1.5719e+01, -8.6562e+00, -3.3828e+00,  1.4156e+01,
          4.9531e+00,  1.6250e+01,  1.6656e+01, -1.1172e+00, -6.5312e+00,
          1.2469e+01,  8.2656e+00,  8.1250e+00, -9.0625e+00, -3.2344e+00,
         -2.8438e+00,  8.5000e+00,  8.0000e+00, -1.7438e+01, -6.5117e+00,
          2.1582e+00,  1.3750e+01,  4.2812e+00, -3.1406e+00,  1.2938e+01,
          7.9453e+00, -3.5469e+00, -3.2812e+00, -1.1938e+01,  6.5625e+00,
          1.1562e+00,  1.0969e+01,  1.0844e+01, -1.2531e+01, -1.0234e+01,
          3.7188e+00, -3.2969e+00,  3.2969e+00,  7.4688e+00, -2.5391e+00,
         -1.1812e+01,  5.0078e+00,  8.1875e+00,  6.3750e+00,  5.1562e+00,
          8.0938e+00,  2.3000e+01,  3.5938e-01, -1.3781e+01,  1.0406e+01,
         -1.0938e+01, -7.2812e+00, -1.7938e+01, -1.8438e+00, -1.5844e+01,
          1.3938e+01,  1.5381e+00,  6.7578e-01,  2.6641e+00,  9.5938e+00,
          9.9688e+00, -2.3438e-01, -6.6484e+00, -1.9531e-01, -1.7334e+00,
         -1.5625e-01,  6.5469e+00,  3.4219e+00, -6.8281e+00, -1.1188e+01,
         -1.5125e+01, -1.5312e+01, -1.6625e+01, -9.2188e+00,  5.8477e+00,
         -7.5625e+00, -1.3188e+01, -5.2500e+00,  1.0656e+01,  6.2969e+00,
          1.4062e+01, -8.3750e+00, -1.2250e+01,  2.2578e+00, -8.8379e-01,
         -7.8789e+00,  4.5156e+00, -9.9688e+00, -3.0547e+00, -7.5625e+00,
         -1.0844e+01,  1.3938e+01, -5.7031e+00,  1.7344e+00, -2.7656e+00,
         -3.8828e+00, -7.3750e+00, -3.5059e+00,  7.8750e+00,  1.0938e+01,
          1.2094e+01,  1.4312e+01, -9.0391e+00, -6.5156e+00, -3.3984e+00,
          3.2656e+00, -5.4805e+00, -7.5625e+00,  4.9844e+00, -1.8203e+00,
         -5.8125e+00,  9.1875e+00,  1.4531e+01, -9.1406e-01, -1.1719e-01,
          1.1172e+01, -2.2773e+00, -1.5375e+01, -1.7031e+01,  3.7812e+00,
          3.5078e+00,  3.6094e+00,  1.5438e+01,  1.5562e+01,  6.2344e+00,
         -8.0156e+00,  1.2812e+01, -3.0938e+00, -9.5703e-01, -1.0625e+01,
         -1.8672e+00,  1.3477e+00, -6.5000e+00,  1.2594e+01,  3.2266e+00,
          2.0312e+00, -9.4062e+00, -9.1875e+00,  1.1719e-01, -2.6719e+00,
          1.2781e+01,  1.6188e+01,  1.6172e+00, -1.4047e+01,  1.3500e+01,
         -1.5219e+01,  6.7344e+00, -8.7188e+00, -3.7656e+00, -2.3906e+00,
          6.7812e+00,  1.4312e+01,  5.1250e+00, -1.1562e+01,  7.6016e+00,
         -1.4094e+01,  1.2750e+01, -1.0906e+01, -1.0016e+01, -5.1875e+00,
          5.3438e+00,  3.3691e+00, -3.9688e+00,  5.6797e+00, -1.4281e+01,
         -6.8281e+00,  1.5219e+01,  7.3906e+00,  3.6719e+00, -8.2812e-01,
         -5.3750e+00,  8.2812e+00, -6.7578e-01,  8.5469e+00, -3.8125e+00,
         -1.4438e+01, -1.1719e+00, -3.5859e+00, -2.1281e+01, -3.0000e+00,
          1.2031e+01, -1.0773e+01, -2.4766e+00,  1.0078e+00,  4.8125e+00,
          2.7539e+00,  1.5719e+01,  1.3375e+01,  5.0938e+00, -4.7031e+00,
         -8.1562e+00,  2.7188e+00, -3.8281e+00,  3.2188e+00, -1.2750e+01,
         -1.2438e+01, -5.2344e-01,  1.7125e+01,  6.2812e+00, -2.7969e+00,
          1.1188e+01,  1.3000e+01, -1.3750e+01,  1.0000e+01,  9.6875e+00,
          7.5312e+00,  1.8938e+01,  4.5078e+00,  3.7344e+00,  1.1188e+01,
         -5.4688e-02, -1.3125e+00,  8.3750e+00,  1.8125e+01,  1.6688e+01,
          9.0312e+00, -5.5625e+00,  1.3438e+01, -1.1328e+00, -5.5781e+00,
         -9.0781e+00,  6.2344e+00, -1.6156e+01,  1.6812e+01,  1.1844e+01,
         -2.4375e+00, -6.1406e+00,  1.7656e+01,  5.9688e+00, -6.3125e+00,
          4.9531e+00, -2.8125e+00,  7.8750e+00, -2.3594e+00, -8.5000e+00,
         -1.1680e+00, -1.6750e+01, -8.9219e+00, -1.3469e+01, -1.1281e+01,
         -1.3781e+01, -1.5438e+01, -1.0250e+01, -1.1656e+01,  3.3438e+00,
          3.8906e+00,  7.0312e+00,  1.1969e+01, -1.1934e+00, -6.4531e+00,
          1.6875e+01, -4.6875e+00, -3.5625e+00, -2.3438e+00, -3.1016e+00,
          1.1875e+01,  4.9375e+00,  2.4312e+01, -3.9688e+00, -3.7812e+00,
          7.8711e-01,  5.6875e+00, -1.1500e+01, -1.8750e+00,  1.2203e+01,
         -1.2062e+01, -5.7266e+00,  3.5156e+00, -1.0375e+01, -8.2969e+00,
         -1.2656e+01,  3.8750e+00,  8.7188e+00,  8.1562e+00,  4.3281e+00,
         -4.1875e+00,  1.5125e+01,  5.1406e+00,  1.4188e+01, -1.8406e+01,
         -4.7500e+00, -5.4219e+00,  5.7188e+00, -9.3125e+00,  1.4625e+01,
         -6.9688e+00, -1.5625e+00,  6.2969e+00,  1.1625e+01,  2.0938e+00,
         -4.4062e+00,  1.3469e+01, -8.2188e+00, -1.3125e+01, -4.1875e+00,
          2.1375e+01,  2.0078e+00, -4.3359e+00, -4.8438e-01, -1.2375e+01,
         -5.5859e-01, -2.2812e+01, -1.5469e+01,  1.3281e+01, -1.3625e+01,
          4.5938e+00,  5.3828e+00,  1.5781e+01, -9.5625e+00,  1.6312e+01,
          1.6031e+01, -1.5125e+01,  1.2719e+01,  1.1016e+00, -3.6621e-01,
          2.1438e+01,  2.6797e+00,  4.5938e+00,  3.6094e+00,  5.1123e-01,
          1.8750e+00,  4.5625e+00, -1.0211e+01, -1.3125e+01,  1.7375e+01,
         -5.3125e+00,  4.1094e+00, -7.8750e+00,  3.2812e+00,  7.1875e+00,
         -1.8375e+01, -1.3844e+01,  1.6219e+01, -1.9844e+01, -7.8750e+00,
         -1.4312e+01, -9.3906e+00,  1.4297e+00,  1.2594e+01, -1.3500e+01,
         -2.2812e+01,  1.1281e+01, -1.3344e+01,  1.3781e+01, -1.5281e+01,
         -2.1484e+00,  6.9062e+00, -1.3500e+01, -4.0547e+00, -3.0938e+00,
         -2.0820e+00, -9.3125e+00,  5.2812e+00,  8.7188e+00, -1.6844e+01,
          5.7188e+00, -2.4688e+00, -1.7688e+01,  4.4141e+00, -4.5938e+00,
         -3.2438e+01,  2.3750e+01,  1.0438e+01,  1.0438e+01,  1.0547e+00,
         -6.5977e+00,  1.7312e+01,  1.1781e+01,  1.9625e+01, -8.2812e+00,
          5.1875e+00,  6.0938e+00, -9.9375e+00,  1.2000e+01,  4.7812e+00,
          5.4688e+00, -5.0000e-01, -1.0781e+01, -1.0531e+01, -6.0625e+00,
          1.7500e+00, -5.2656e+00, -1.1250e+00, -8.1250e+00,  2.7312e+01,
          1.4094e+01, -1.0125e+01,  1.0656e+01, -8.8125e+00, -1.9875e+01,
          1.2062e+01, -1.6812e+01,  1.2188e+01,  1.0215e+00, -1.6094e+00,
          4.2812e+00, -2.1934e+00, -1.7500e+00,  4.3047e+00, -9.4688e+00,
          2.6250e+00, -1.4750e+01, -6.9531e+00, -1.0844e+01,  1.0719e+01,
          1.0453e+01, -3.6406e+00, -1.4844e+01, -1.0156e+00, -4.0312e+00,
         -3.9375e+00,  1.1438e+01,  1.1250e+01, -8.0938e+00, -5.5312e+00,
          5.4688e+00, -5.5781e+00,  2.0156e+00, -2.0812e+01,  1.1688e+01,
         -5.8672e+00,  6.2500e+00]], device='cuda:3', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
asr_weight tensor(0.1445)
mt_weight tensor(0.5000)
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 5 terminated with the following error:
Traceback (most recent call last):
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 695, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 552, in _per_task_train_loss
    assert False
AssertionError

/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 43 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
