2023-08-12 02:05:59 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:11040
2023-08-12 02:05:59 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:11040
2023-08-12 02:05:59 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:11040
2023-08-12 02:05:59 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:11040
2023-08-12 02:06:00 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:11040
2023-08-12 02:06:00 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:11040
2023-08-12 02:06:00 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:11040
2023-08-12 02:06:00 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-12 02:06:00 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:11040
2023-08-12 02:06:00 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-12 02:06:00 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-12 02:06:00 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-12 02:06:00 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-12 02:06:00 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-12 02:06:01 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-12 02:06:01 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-12 02:06:01 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-12 02:06:01 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-12 02:06:01 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-12 02:06:01 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-12 02:06:01 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-12 02:06:01 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-12 02:06:01 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-12 02:06:01 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-12 02:06:01 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-12 02:06:01 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-12 02:06:01 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-12 02:06:01 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-12 02:06:01 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-12 02:06:01 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-12 02:06:01 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-12 02:06:01 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-12 02:06:05 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11040', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-12 02:06:05 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-08-12 02:06:05 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-08-12 02:06:05 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-12 02:06:05 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-08-12 02:06:05 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-12 02:06:09 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-12 02:06:09 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-12 02:06:09 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-12 02:06:12 | INFO | root | load pretrained hubert
2023-08-12 02:06:14 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-12 02:06:15 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-12 02:06:18 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-12 02:06:18 | INFO | root | share the sematic adapter and textual encoder
2023-08-12 02:06:18 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-12 02:06:18 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-08-12 02:06:18 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-12 02:06:18 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-08-12 02:06:18 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-08-12 02:06:18 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-12 02:06:18 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-12 02:06:18 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-12 02:06:18 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-12 02:06:18 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-12 02:06:22 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-12 02:06:22 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-12 02:06:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-12 02:06:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-12 02:06:23 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-12 02:06:23 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-12 02:06:23 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-12 02:06:23 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-12 02:06:23 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-12 02:06:23 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-12 02:06:23 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-12 02:06:23 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-12 02:06:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-12 02:06:23 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-12 02:06:23 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-12 02:06:23 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-12 02:06:23 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-12 02:06:23 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-12 02:06:23 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-12 02:06:23 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-12 02:06:23 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-12 02:06:24 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-12 02:06:26 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-12 02:07:16 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-12 02:07:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 02:07:16 | INFO | fairseq.trainer | begin training epoch 1
2023-08-12 02:07:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 02:07:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-12 02:07:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-12 02:07:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-12 02:07:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-12 02:08:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-12 02:08:34 | INFO | train_inner | epoch 001:    105 / 1474 loss=20.538, trans_loss=5.871, nll_loss=4.679, w2v_ctc_loss=22.868, task_loss=0, contrastive_loss=3.274, total=4219.16, n_correct=124.25, ppl=25.62, accuracy=2.945, wps=19116.5, ups=1.52, wpb=12588, bsz=476, num_updates=100, lr=4.098e-06, gnorm=2.899, clip=0, loss_scale=4, train_wall=69, gb_free=19.5, wall=132
2023-08-12 02:09:37 | INFO | train_inner | epoch 001:    205 / 1474 loss=16.967, trans_loss=5.857, nll_loss=4.688, w2v_ctc_loss=17.451, task_loss=0, contrastive_loss=3.236, total=4114.86, n_correct=115.02, ppl=25.77, accuracy=2.795, wps=19527.3, ups=1.59, wpb=12286.8, bsz=458.8, num_updates=200, lr=8.096e-06, gnorm=7.409, clip=19, loss_scale=4, train_wall=62, gb_free=19.4, wall=195
2023-08-12 02:10:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-12 02:10:40 | INFO | train_inner | epoch 001:    306 / 1474 loss=10.145, trans_loss=5.843, nll_loss=4.709, w2v_ctc_loss=7.038, task_loss=0, contrastive_loss=3.171, total=4082.14, n_correct=113.13, ppl=26.16, accuracy=2.771, wps=19421, ups=1.59, wpb=12193.3, bsz=439.2, num_updates=300, lr=1.2094e-05, gnorm=2.208, clip=0, loss_scale=2, train_wall=62, gb_free=18.6, wall=258
2023-08-12 02:11:42 | INFO | train_inner | epoch 001:    406 / 1474 loss=9.628, trans_loss=5.783, nll_loss=4.666, w2v_ctc_loss=6.258, task_loss=0, contrastive_loss=3.205, total=4172.39, n_correct=101.53, ppl=25.38, accuracy=2.433, wps=20042.1, ups=1.61, wpb=12458.9, bsz=460.4, num_updates=400, lr=1.6092e-05, gnorm=1.375, clip=0, loss_scale=2, train_wall=62, gb_free=18.6, wall=320
2023-08-12 02:12:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-08-12 02:12:46 | INFO | train_inner | epoch 001:    507 / 1474 loss=9.447, trans_loss=5.735, nll_loss=4.624, w2v_ctc_loss=5.961, task_loss=0, contrastive_loss=3.301, total=4176.88, n_correct=96.76, ppl=24.66, accuracy=2.317, wps=19659, ups=1.58, wpb=12481.8, bsz=487.3, num_updates=500, lr=2.009e-05, gnorm=1.114, clip=0, loss_scale=1, train_wall=63, gb_free=19.2, wall=383
2023-08-12 02:13:49 | INFO | train_inner | epoch 001:    607 / 1474 loss=9.322, trans_loss=5.771, nll_loss=4.67, w2v_ctc_loss=5.791, task_loss=0, contrastive_loss=3.252, total=4146.02, n_correct=90.29, ppl=25.46, accuracy=2.178, wps=19529.2, ups=1.58, wpb=12362.3, bsz=474.7, num_updates=600, lr=2.4088e-05, gnorm=1.217, clip=1, loss_scale=1, train_wall=63, gb_free=19.2, wall=447
2023-08-12 02:14:51 | INFO | train_inner | epoch 001:    707 / 1474 loss=9.227, trans_loss=5.793, nll_loss=4.698, w2v_ctc_loss=5.714, task_loss=0, contrastive_loss=3.124, total=4136.69, n_correct=81.52, ppl=25.95, accuracy=1.971, wps=19914.3, ups=1.61, wpb=12353.2, bsz=453, num_updates=700, lr=2.8086e-05, gnorm=0.991, clip=0, loss_scale=1, train_wall=62, gb_free=19.4, wall=509
2023-08-12 02:15:53 | INFO | train_inner | epoch 001:    807 / 1474 loss=9.121, trans_loss=5.898, nll_loss=4.823, w2v_ctc_loss=5.49, task_loss=0, contrastive_loss=3.136, total=4132.5, n_correct=65.89, ppl=28.31, accuracy=1.594, wps=19948.4, ups=1.62, wpb=12330.1, bsz=464, num_updates=800, lr=3.2084e-05, gnorm=1.212, clip=0, loss_scale=1, train_wall=61, gb_free=18.9, wall=570
2023-08-12 02:16:55 | INFO | train_inner | epoch 001:    907 / 1474 loss=8.968, trans_loss=5.978, nll_loss=4.921, w2v_ctc_loss=5.256, task_loss=0, contrastive_loss=3.038, total=4165.04, n_correct=54.33, ppl=30.3, accuracy=1.304, wps=20024.4, ups=1.61, wpb=12438.6, bsz=458.3, num_updates=900, lr=3.6082e-05, gnorm=1.371, clip=0, loss_scale=1, train_wall=62, gb_free=18.8, wall=633
2023-08-12 02:17:58 | INFO | train_inner | epoch 001:   1007 / 1474 loss=8.804, trans_loss=6.049, nll_loss=5.007, w2v_ctc_loss=4.969, task_loss=0, contrastive_loss=3.031, total=4135.88, n_correct=45.78, ppl=32.16, accuracy=1.107, wps=19659.2, ups=1.59, wpb=12354.5, bsz=457.9, num_updates=1000, lr=4.008e-05, gnorm=1.746, clip=0, loss_scale=1, train_wall=62, gb_free=19.3, wall=695
2023-08-12 02:19:01 | INFO | train_inner | epoch 001:   1107 / 1474 loss=8.613, trans_loss=6.101, nll_loss=5.064, w2v_ctc_loss=4.737, task_loss=0, contrastive_loss=2.939, total=4152.66, n_correct=42.13, ppl=33.46, accuracy=1.015, wps=19720, ups=1.59, wpb=12385.6, bsz=454, num_updates=1100, lr=4.4078e-05, gnorm=1.959, clip=0, loss_scale=1, train_wall=62, gb_free=18.6, wall=758
2023-08-12 02:20:02 | INFO | train_inner | epoch 001:   1207 / 1474 loss=8.473, trans_loss=6.159, nll_loss=5.142, w2v_ctc_loss=4.577, task_loss=0, contrastive_loss=2.835, total=4122.37, n_correct=40.95, ppl=35.32, accuracy=0.993, wps=19958.1, ups=1.62, wpb=12314, bsz=436.2, num_updates=1200, lr=4.8076e-05, gnorm=2.677, clip=0, loss_scale=1, train_wall=61, gb_free=18.7, wall=820
2023-08-12 02:21:04 | INFO | train_inner | epoch 001:   1307 / 1474 loss=8.31, trans_loss=6.113, nll_loss=5.083, w2v_ctc_loss=4.418, task_loss=0, contrastive_loss=2.784, total=4071.58, n_correct=52.99, ppl=33.9, accuracy=1.301, wps=19719.1, ups=1.62, wpb=12154.5, bsz=447.9, num_updates=1300, lr=5.2074e-05, gnorm=2.51, clip=0, loss_scale=1, train_wall=61, gb_free=18.7, wall=882
2023-08-12 02:22:06 | INFO | train_inner | epoch 001:   1407 / 1474 loss=8.181, trans_loss=6.144, nll_loss=5.13, w2v_ctc_loss=4.27, task_loss=0, contrastive_loss=2.844, total=4117.88, n_correct=50.25, ppl=35.02, accuracy=1.22, wps=19833.5, ups=1.61, wpb=12305.9, bsz=449.1, num_updates=1400, lr=5.6072e-05, gnorm=2.456, clip=0, loss_scale=1, train_wall=62, gb_free=18.8, wall=944
2023-08-12 02:22:48 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 02:23:27 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 12.948 | trans_loss 13.982 | nll_loss 13.861 | w2v_ctc_loss 5.616 | task_loss 0 | contrastive_loss 4.01 | total 4003.4 | n_correct 22.4 | ppl 14875 | accuracy 0.56 | uer 69.564 | wer 67.522 | raw_wer 67.522 | bleu 0 | wps 1147.8 | wpb 4003.4 | bsz 141.8 | num_updates 1467
2023-08-12 02:23:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1467 updates
2023-08-12 02:23:27 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 02:23:31 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 02:23:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt (epoch 1 @ 1467 updates, score 0.0) (writing took 6.468814596533775 seconds)
2023-08-12 02:23:34 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-08-12 02:23:34 | INFO | train | epoch 001 | loss 10.318 | trans_loss 5.945 | nll_loss 4.864 | w2v_ctc_loss 7.352 | task_loss 0 | contrastive_loss 3.074 | total 4138.76 | n_correct 75.4104 | ppl 29.11 | accuracy 1.822 | wps 18780.8 | ups 1.52 | wpb 12356.2 | bsz 458.7 | num_updates 1467 | lr 5.87507e-05 | gnorm 2.246 | clip 1.4 | loss_scale 1 | train_wall 917 | gb_free 18.9 | wall 1031
2023-08-12 02:23:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 02:23:34 | INFO | fairseq.trainer | begin training epoch 2
2023-08-12 02:23:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 02:24:04 | INFO | train_inner | epoch 002:     33 / 1474 loss=8.098, trans_loss=6.171, nll_loss=5.161, w2v_ctc_loss=4.13, task_loss=0, contrastive_loss=2.811, total=4166.55, n_correct=47, ppl=35.78, accuracy=1.128, wps=10583.9, ups=0.85, wpb=12424.8, bsz=469.9, num_updates=1500, lr=6.007e-05, gnorm=2.804, clip=0, loss_scale=1, train_wall=63, gb_free=18.9, wall=1061
2023-08-12 02:25:05 | INFO | train_inner | epoch 002:    133 / 1474 loss=7.953, trans_loss=6.151, nll_loss=5.139, w2v_ctc_loss=4.041, task_loss=0, contrastive_loss=2.674, total=4154.58, n_correct=47.99, ppl=35.23, accuracy=1.155, wps=20042, ups=1.62, wpb=12394, bsz=455, num_updates=1600, lr=6.4068e-05, gnorm=2.724, clip=0, loss_scale=1, train_wall=61, gb_free=18.7, wall=1123
2023-08-12 02:26:07 | INFO | train_inner | epoch 002:    233 / 1474 loss=7.882, trans_loss=6.142, nll_loss=5.131, w2v_ctc_loss=3.904, task_loss=0, contrastive_loss=2.736, total=4202.18, n_correct=50.71, ppl=35.04, accuracy=1.207, wps=20366.7, ups=1.62, wpb=12548.4, bsz=492.4, num_updates=1700, lr=6.8066e-05, gnorm=2.954, clip=0, loss_scale=1, train_wall=61, gb_free=18.8, wall=1184
2023-08-12 02:27:09 | INFO | train_inner | epoch 002:    333 / 1474 loss=7.682, trans_loss=6.116, nll_loss=5.099, w2v_ctc_loss=3.847, task_loss=0, contrastive_loss=2.515, total=4125.8, n_correct=53.85, ppl=34.26, accuracy=1.305, wps=19909.9, ups=1.62, wpb=12318.7, bsz=447, num_updates=1800, lr=7.2064e-05, gnorm=2.888, clip=0, loss_scale=1, train_wall=61, gb_free=19.6, wall=1246
2023-08-12 02:28:11 | INFO | train_inner | epoch 002:    433 / 1474 loss=7.517, trans_loss=6.096, nll_loss=5.075, w2v_ctc_loss=3.792, task_loss=0, contrastive_loss=2.323, total=4029.94, n_correct=55.67, ppl=33.7, accuracy=1.381, wps=19328.6, ups=1.6, wpb=12045.2, bsz=410.6, num_updates=1900, lr=7.6062e-05, gnorm=2.724, clip=0, loss_scale=1, train_wall=62, gb_free=18.9, wall=1309
2023-08-12 02:29:14 | INFO | train_inner | epoch 002:    533 / 1474 loss=7.473, trans_loss=6.073, nll_loss=5.042, w2v_ctc_loss=3.66, task_loss=0, contrastive_loss=2.481, total=4192.5, n_correct=65.34, ppl=32.95, accuracy=1.558, wps=20034.2, ups=1.6, wpb=12507.1, bsz=470.4, num_updates=2000, lr=8.006e-05, gnorm=2.604, clip=0, loss_scale=1, train_wall=62, gb_free=19.1, wall=1371
2023-08-12 02:29:14 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 02:29:54 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 12.102 | trans_loss 13.398 | nll_loss 13.119 | w2v_ctc_loss 4.798 | task_loss 0 | contrastive_loss 3.515 | total 4003.4 | n_correct 60.9 | ppl 8897.02 | accuracy 1.521 | uer 62.732 | wer 60.643 | raw_wer 60.643 | bleu 0 | wps 1171.9 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_bleu 0
2023-08-12 02:29:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-08-12 02:29:54 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_2_2000.pt
2023-08-12 02:29:58 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_2_2000.pt
2023-08-12 02:30:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.0) (writing took 52.576484855264425 seconds)
2023-08-12 02:31:48 | INFO | train_inner | epoch 002:    633 / 1474 loss=7.327, trans_loss=6.066, nll_loss=5.033, w2v_ctc_loss=3.57, task_loss=0, contrastive_loss=2.3, total=4121.32, n_correct=63.56, ppl=32.74, accuracy=1.542, wps=7943.8, ups=0.65, wpb=12299.3, bsz=449.2, num_updates=2100, lr=8.4058e-05, gnorm=2.594, clip=0, loss_scale=1, train_wall=61, gb_free=19.5, wall=1526
2023-08-12 02:32:50 | INFO | train_inner | epoch 002:    733 / 1474 loss=7.267, trans_loss=6.071, nll_loss=5.038, w2v_ctc_loss=3.504, task_loss=0, contrastive_loss=2.399, total=4155.52, n_correct=62.34, ppl=32.85, accuracy=1.5, wps=20148.7, ups=1.62, wpb=12404.1, bsz=466, num_updates=2200, lr=8.8056e-05, gnorm=2.333, clip=0, loss_scale=1, train_wall=61, gb_free=17.7, wall=1587
2023-08-12 02:33:52 | INFO | train_inner | epoch 002:    833 / 1474 loss=7.147, trans_loss=6.057, nll_loss=5.022, w2v_ctc_loss=3.443, task_loss=0, contrastive_loss=2.279, total=4157.05, n_correct=66.25, ppl=32.5, accuracy=1.594, wps=20105.1, ups=1.62, wpb=12420.1, bsz=457.6, num_updates=2300, lr=9.2054e-05, gnorm=2.373, clip=0, loss_scale=1, train_wall=61, gb_free=19.1, wall=1649
2023-08-12 02:34:55 | INFO | train_inner | epoch 002:    933 / 1474 loss=7.008, trans_loss=6.041, nll_loss=4.999, w2v_ctc_loss=3.35, task_loss=0, contrastive_loss=2.236, total=4105.43, n_correct=64.53, ppl=31.98, accuracy=1.572, wps=19453.4, ups=1.59, wpb=12253.9, bsz=443.6, num_updates=2400, lr=9.6052e-05, gnorm=2.405, clip=0, loss_scale=1, train_wall=62, gb_free=18.8, wall=1712
2023-08-12 02:35:56 | INFO | train_inner | epoch 002:   1033 / 1474 loss=6.923, trans_loss=6.036, nll_loss=4.993, w2v_ctc_loss=3.294, task_loss=0, contrastive_loss=2.128, total=4097.3, n_correct=65.24, ppl=31.85, accuracy=1.592, wps=19917.5, ups=1.63, wpb=12232.3, bsz=452.7, num_updates=2500, lr=0.00010005, gnorm=2.206, clip=0, loss_scale=1, train_wall=61, gb_free=18.7, wall=1774
2023-08-12 02:36:59 | INFO | train_inner | epoch 002:   1133 / 1474 loss=6.898, trans_loss=6.02, nll_loss=4.974, w2v_ctc_loss=3.2, task_loss=0, contrastive_loss=2.347, total=4213.49, n_correct=68.87, ppl=31.42, accuracy=1.635, wps=20143.1, ups=1.6, wpb=12579.3, bsz=498.9, num_updates=2600, lr=0.000104048, gnorm=2.208, clip=0, loss_scale=2, train_wall=62, gb_free=18.9, wall=1836
2023-08-12 02:38:01 | INFO | train_inner | epoch 002:   1233 / 1474 loss=6.792, trans_loss=6.014, nll_loss=4.963, w2v_ctc_loss=3.164, task_loss=0, contrastive_loss=2.157, total=4212.12, n_correct=70.31, ppl=31.19, accuracy=1.669, wps=20285.1, ups=1.61, wpb=12569.9, bsz=486.3, num_updates=2700, lr=0.000108046, gnorm=2.185, clip=0, loss_scale=2, train_wall=62, gb_free=19.1, wall=1898
2023-08-12 02:39:03 | INFO | train_inner | epoch 002:   1333 / 1474 loss=6.658, trans_loss=6.003, nll_loss=4.954, w2v_ctc_loss=3.122, task_loss=0, contrastive_loss=1.931, total=4139.37, n_correct=69.65, ppl=30.99, accuracy=1.683, wps=19966.1, ups=1.61, wpb=12372.6, bsz=455.1, num_updates=2800, lr=0.000112044, gnorm=2.011, clip=0, loss_scale=2, train_wall=62, gb_free=19.6, wall=1960
2023-08-12 02:40:05 | INFO | train_inner | epoch 002:   1433 / 1474 loss=6.57, trans_loss=5.997, nll_loss=4.943, w2v_ctc_loss=3.07, task_loss=0, contrastive_loss=1.995, total=4066.75, n_correct=68.45, ppl=30.77, accuracy=1.683, wps=19460.1, ups=1.6, wpb=12139.7, bsz=445.6, num_updates=2900, lr=0.000116042, gnorm=2.074, clip=0, loss_scale=2, train_wall=62, gb_free=19.2, wall=2022
2023-08-12 02:40:30 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 02:41:09 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 11.238 | trans_loss 13.037 | nll_loss 12.631 | w2v_ctc_loss 3.936 | task_loss 0 | contrastive_loss 2.658 | total 4003.4 | n_correct 85.6 | ppl 6344.27 | accuracy 2.138 | uer 53.375 | wer 52.597 | raw_wer 52.597 | bleu 0 | wps 1171.2 | wpb 4003.4 | bsz 141.8 | num_updates 2941 | best_bleu 0
2023-08-12 02:41:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2941 updates
2023-08-12 02:41:09 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 02:41:25 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 02:41:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt (epoch 2 @ 2941 updates, score 0.0) (writing took 28.469633108004928 seconds)
2023-08-12 02:41:38 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-08-12 02:41:38 | INFO | train | epoch 002 | loss 7.221 | trans_loss 6.064 | nll_loss 5.03 | w2v_ctc_loss 3.498 | task_loss 0 | contrastive_loss 2.322 | total 4138.65 | n_correct 62.1703 | ppl 32.67 | accuracy 1.502 | wps 16797.2 | ups 1.36 | wpb 12355.8 | bsz 458.5 | num_updates 2941 | lr 0.000117681 | gnorm 2.445 | clip 0 | loss_scale 2 | train_wall 907 | gb_free 19 | wall 2115
2023-08-12 02:41:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 02:41:38 | INFO | fairseq.trainer | begin training epoch 3
2023-08-12 02:41:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 02:42:22 | INFO | train_inner | epoch 003:     59 / 1474 loss=6.461, trans_loss=5.982, nll_loss=4.924, w2v_ctc_loss=3.016, task_loss=0, contrastive_loss=1.845, total=4050.57, n_correct=70.04, ppl=30.36, accuracy=1.729, wps=8801.7, ups=0.73, wpb=12091.9, bsz=434.7, num_updates=3000, lr=0.00012004, gnorm=1.848, clip=0, loss_scale=2, train_wall=61, gb_free=19.3, wall=2160
2023-08-12 02:43:42 | INFO | train_inner | epoch 003:    159 / 1474 loss=5.787, trans_loss=5.354, nll_loss=4.134, w2v_ctc_loss=2.744, task_loss=0, contrastive_loss=1.765, total=4157.08, n_correct=237.87, ppl=17.56, accuracy=5.722, wps=15519, ups=1.25, wpb=12413.1, bsz=461.9, num_updates=3100, lr=0.000124038, gnorm=3.694, clip=2, loss_scale=2, train_wall=79, gb_free=16.4, wall=2240
2023-08-12 02:45:03 | INFO | train_inner | epoch 003:    259 / 1474 loss=5.128, trans_loss=5.069, nll_loss=3.764, w2v_ctc_loss=2.432, task_loss=0, contrastive_loss=1.511, total=4155.72, n_correct=427.64, ppl=13.59, accuracy=10.29, wps=15376, ups=1.24, wpb=12415.9, bsz=465.5, num_updates=3200, lr=0.000128036, gnorm=2.326, clip=0, loss_scale=2, train_wall=80, gb_free=17.5, wall=2321
2023-08-12 02:45:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-08-12 02:46:25 | INFO | train_inner | epoch 003:    360 / 1474 loss=4.842, trans_loss=4.925, nll_loss=3.569, w2v_ctc_loss=2.298, task_loss=0, contrastive_loss=1.449, total=4170.17, n_correct=535.81, ppl=11.87, accuracy=12.849, wps=15231.8, ups=1.22, wpb=12443.5, bsz=470.9, num_updates=3300, lr=0.000132034, gnorm=2.112, clip=0, loss_scale=1, train_wall=81, gb_free=16, wall=2402
2023-08-12 02:47:45 | INFO | train_inner | epoch 003:    460 / 1474 loss=4.571, trans_loss=4.796, nll_loss=3.396, w2v_ctc_loss=2.194, task_loss=0, contrastive_loss=1.24, total=4196.46, n_correct=645.44, ppl=10.53, accuracy=15.381, wps=15588.4, ups=1.24, wpb=12527.1, bsz=468.6, num_updates=3400, lr=0.000136032, gnorm=1.922, clip=0, loss_scale=1, train_wall=80, gb_free=15.7, wall=2483
2023-08-12 02:49:06 | INFO | train_inner | epoch 003:    560 / 1474 loss=4.348, trans_loss=4.694, nll_loss=3.261, w2v_ctc_loss=2.109, task_loss=0, contrastive_loss=1.135, total=4085.25, n_correct=722.19, ppl=9.59, accuracy=17.678, wps=15194.2, ups=1.25, wpb=12203.2, bsz=439.8, num_updates=3500, lr=0.00014003, gnorm=2.057, clip=0, loss_scale=1, train_wall=80, gb_free=15.8, wall=2563
2023-08-12 02:50:26 | INFO | train_inner | epoch 003:    660 / 1474 loss=4.213, trans_loss=4.59, nll_loss=3.118, w2v_ctc_loss=2.036, task_loss=0, contrastive_loss=1.197, total=4229.91, n_correct=872, ppl=8.68, accuracy=20.615, wps=15606.7, ups=1.24, wpb=12610.8, bsz=484.8, num_updates=3600, lr=0.000144028, gnorm=1.75, clip=0, loss_scale=1, train_wall=80, gb_free=17.2, wall=2644
2023-08-12 02:51:47 | INFO | train_inner | epoch 003:    760 / 1474 loss=3.993, trans_loss=4.464, nll_loss=2.956, w2v_ctc_loss=1.997, task_loss=0, contrastive_loss=0.912, total=4157.48, n_correct=987.53, ppl=7.76, accuracy=23.753, wps=15431.1, ups=1.24, wpb=12420.7, bsz=467.8, num_updates=3700, lr=0.000148026, gnorm=1.712, clip=0, loss_scale=1, train_wall=80, gb_free=11.7, wall=2724
2023-08-12 02:53:07 | INFO | train_inner | epoch 003:    860 / 1474 loss=3.807, trans_loss=4.331, nll_loss=2.779, w2v_ctc_loss=1.948, task_loss=0, contrastive_loss=0.835, total=4172.27, n_correct=1157.09, ppl=6.86, accuracy=27.733, wps=15573, ups=1.25, wpb=12457.6, bsz=458.8, num_updates=3800, lr=0.000152024, gnorm=1.663, clip=0, loss_scale=1, train_wall=79, gb_free=16.7, wall=2804
2023-08-12 02:54:27 | INFO | train_inner | epoch 003:    960 / 1474 loss=3.637, trans_loss=4.177, nll_loss=2.574, w2v_ctc_loss=1.91, task_loss=0, contrastive_loss=0.826, total=4171.53, n_correct=1374.94, ppl=5.95, accuracy=32.96, wps=15466.7, ups=1.24, wpb=12442.2, bsz=473.5, num_updates=3900, lr=0.000156022, gnorm=1.564, clip=0, loss_scale=1, train_wall=80, gb_free=16.1, wall=2885
2023-08-12 02:55:47 | INFO | train_inner | epoch 003:   1060 / 1474 loss=3.482, trans_loss=4.075, nll_loss=2.446, w2v_ctc_loss=1.898, task_loss=0, contrastive_loss=0.698, total=4051.14, n_correct=1473.39, ppl=5.45, accuracy=36.37, wps=15165.9, ups=1.25, wpb=12099.4, bsz=436.8, num_updates=4000, lr=0.00016002, gnorm=1.467, clip=0, loss_scale=1, train_wall=79, gb_free=16.7, wall=2964
2023-08-12 02:55:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 02:56:15 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.684 | trans_loss 6.697 | nll_loss 4.335 | w2v_ctc_loss 2.284 | task_loss 0 | contrastive_loss 0.947 | total 4003.4 | n_correct 1776.3 | ppl 20.19 | accuracy 44.37 | uer 31.827 | wer 32.038 | raw_wer 32.038 | bleu 7.04 | wps 1656 | wpb 4003.4 | bsz 141.8 | num_updates 4000 | best_bleu 7.04
2023-08-12 02:56:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-08-12 02:56:15 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_3_4000.pt
2023-08-12 02:56:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_3_4000.pt
2023-08-12 02:56:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 7.04) (writing took 40.35754696838558 seconds)
2023-08-12 02:58:16 | INFO | train_inner | epoch 003:   1160 / 1474 loss=3.367, trans_loss=4.019, nll_loss=2.372, w2v_ctc_loss=1.845, task_loss=0, contrastive_loss=0.654, total=4050.25, n_correct=1572.63, ppl=5.18, accuracy=38.828, wps=8137.7, ups=0.67, wpb=12088.6, bsz=435.7, num_updates=4100, lr=0.000164018, gnorm=1.398, clip=0, loss_scale=1, train_wall=79, gb_free=16.3, wall=3113
2023-08-12 02:59:35 | INFO | train_inner | epoch 003:   1260 / 1474 loss=3.263, trans_loss=3.962, nll_loss=2.3, w2v_ctc_loss=1.803, task_loss=0, contrastive_loss=0.595, total=4058.28, n_correct=1654.07, ppl=4.92, accuracy=40.758, wps=15309.1, ups=1.26, wpb=12119.7, bsz=431.2, num_updates=4200, lr=0.000168016, gnorm=1.323, clip=0, loss_scale=1, train_wall=79, gb_free=16.4, wall=3192
2023-08-12 03:00:55 | INFO | train_inner | epoch 003:   1360 / 1474 loss=3.226, trans_loss=3.924, nll_loss=2.251, w2v_ctc_loss=1.766, task_loss=0, contrastive_loss=0.682, total=4134.29, n_correct=1749.18, ppl=4.76, accuracy=42.309, wps=15370.1, ups=1.25, wpb=12343, bsz=460.8, num_updates=4300, lr=0.000172014, gnorm=1.323, clip=0, loss_scale=1, train_wall=80, gb_free=16.6, wall=3273
2023-08-12 03:02:16 | INFO | train_inner | epoch 003:   1460 / 1474 loss=3.157, trans_loss=3.89, nll_loss=2.209, w2v_ctc_loss=1.737, task_loss=0, contrastive_loss=0.639, total=4206.08, n_correct=1838.87, ppl=4.62, accuracy=43.719, wps=15544.4, ups=1.24, wpb=12563.6, bsz=476.5, num_updates=4400, lr=0.000176012, gnorm=1.239, clip=0, loss_scale=1, train_wall=80, gb_free=14.5, wall=3353
2023-08-12 03:02:27 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 03:02:55 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.234 | trans_loss 6.25 | nll_loss 3.779 | w2v_ctc_loss 2.071 | task_loss 0 | contrastive_loss 0.747 | total 4003.4 | n_correct 2021.4 | ppl 13.73 | accuracy 50.492 | uer 30.449 | wer 30.536 | raw_wer 30.536 | bleu 10.61 | wps 1672.2 | wpb 4003.4 | bsz 141.8 | num_updates 4414 | best_bleu 10.61
2023-08-12 03:02:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4414 updates
2023-08-12 03:02:55 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 03:03:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 03:03:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt (epoch 3 @ 4414 updates, score 10.61) (writing took 28.418102987110615 seconds)
2023-08-12 03:03:24 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-08-12 03:03:24 | INFO | train | epoch 003 | loss 4.148 | trans_loss 4.506 | nll_loss 3.012 | w2v_ctc_loss 2.087 | task_loss 0 | contrastive_loss 1.045 | total 4139.15 | n_correct 1054.73 | ppl 8.07 | accuracy 25.482 | wps 13938 | ups 1.13 | wpb 12357.3 | bsz 458.6 | num_updates 4414 | lr 0.000176572 | gnorm 1.819 | clip 0.1 | loss_scale 1 | train_wall 1165 | gb_free 16.4 | wall 3421
2023-08-12 03:03:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 03:03:24 | INFO | fairseq.trainer | begin training epoch 4
2023-08-12 03:03:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 03:04:40 | INFO | train_inner | epoch 004:     86 / 1474 loss=3.03, trans_loss=3.857, nll_loss=2.164, w2v_ctc_loss=1.689, task_loss=0, contrastive_loss=0.468, total=4096.88, n_correct=1842.64, ppl=4.48, accuracy=44.977, wps=8493.3, ups=0.69, wpb=12226.6, bsz=438.5, num_updates=4500, lr=0.00018001, gnorm=1.209, clip=0, loss_scale=1, train_wall=79, gb_free=14.2, wall=3497
2023-08-12 03:05:59 | INFO | train_inner | epoch 004:    186 / 1474 loss=2.994, trans_loss=3.819, nll_loss=2.116, w2v_ctc_loss=1.672, task_loss=0, contrastive_loss=0.48, total=4177.87, n_correct=1940.68, ppl=4.34, accuracy=46.451, wps=15731.3, ups=1.26, wpb=12473.8, bsz=469, num_updates=4600, lr=0.000184008, gnorm=1.18, clip=0, loss_scale=1, train_wall=79, gb_free=15.4, wall=3577
2023-08-12 03:07:19 | INFO | train_inner | epoch 004:    286 / 1474 loss=2.986, trans_loss=3.811, nll_loss=2.108, w2v_ctc_loss=1.658, task_loss=0, contrastive_loss=0.587, total=4147.79, n_correct=1935.45, ppl=4.31, accuracy=46.662, wps=15438.3, ups=1.25, wpb=12391.1, bsz=464.6, num_updates=4700, lr=0.000188006, gnorm=1.144, clip=0, loss_scale=1, train_wall=80, gb_free=16.1, wall=3657
2023-08-12 03:08:39 | INFO | train_inner | epoch 004:    386 / 1474 loss=2.917, trans_loss=3.798, nll_loss=2.088, w2v_ctc_loss=1.643, task_loss=0, contrastive_loss=0.418, total=4120.11, n_correct=1951.45, ppl=4.25, accuracy=47.364, wps=15369.2, ups=1.25, wpb=12293.6, bsz=440.8, num_updates=4800, lr=0.000192004, gnorm=1.2, clip=0, loss_scale=1, train_wall=79, gb_free=17.4, wall=3737
2023-08-12 03:10:00 | INFO | train_inner | epoch 004:    486 / 1474 loss=2.957, trans_loss=3.775, nll_loss=2.061, w2v_ctc_loss=1.6, task_loss=0, contrastive_loss=0.789, total=4223.31, n_correct=2037.01, ppl=4.17, accuracy=48.233, wps=15601.1, ups=1.24, wpb=12605.5, bsz=500.9, num_updates=4900, lr=0.000196002, gnorm=1.13, clip=0, loss_scale=1, train_wall=80, gb_free=16.1, wall=3818
2023-08-12 03:11:21 | INFO | train_inner | epoch 004:    586 / 1474 loss=2.874, trans_loss=3.751, nll_loss=2.03, w2v_ctc_loss=1.613, task_loss=0, contrastive_loss=0.477, total=4228.66, n_correct=2084.74, ppl=4.08, accuracy=49.3, wps=15710.7, ups=1.24, wpb=12623.9, bsz=488.4, num_updates=5000, lr=0.0002, gnorm=1.105, clip=0, loss_scale=1, train_wall=80, gb_free=15.4, wall=3898
mt_weight tensor(0.5000)
asr_weight tensor(0.5146, device='cuda:0')
2023-08-12 03:12:42 | INFO | train_inner | epoch 004:    686 / 1474 loss=2.823, trans_loss=3.746, nll_loss=2.018, w2v_ctc_loss=1.58, task_loss=0, contrastive_loss=0.501, total=4172.53, n_correct=2073.18, ppl=4.05, accuracy=49.686, wps=15282.8, ups=1.23, wpb=12437.1, bsz=453.2, num_updates=5100, lr=0.00019803, gnorm=0.728, clip=0, loss_scale=1, train_wall=81, gb_free=17, wall=3979
2023-08-12 03:14:02 | INFO | train_inner | epoch 004:    786 / 1474 loss=2.778, trans_loss=3.722, nll_loss=1.994, w2v_ctc_loss=1.588, task_loss=0, contrastive_loss=0.366, total=4019.56, n_correct=2025.94, ppl=3.98, accuracy=50.402, wps=14923, ups=1.24, wpb=12004.8, bsz=419.9, num_updates=5200, lr=0.000196116, gnorm=0.727, clip=0, loss_scale=1, train_wall=80, gb_free=15, wall=4060
2023-08-12 03:15:23 | INFO | train_inner | epoch 004:    886 / 1474 loss=2.811, trans_loss=3.707, nll_loss=1.973, w2v_ctc_loss=1.58, task_loss=0, contrastive_loss=0.538, total=4183.92, n_correct=2129.57, ppl=3.93, accuracy=50.899, wps=15515, ups=1.24, wpb=12492.9, bsz=465.6, num_updates=5300, lr=0.000194257, gnorm=0.725, clip=0, loss_scale=1, train_wall=80, gb_free=17.5, wall=4140
2023-08-12 03:16:43 | INFO | train_inner | epoch 004:    986 / 1474 loss=2.751, trans_loss=3.692, nll_loss=1.955, w2v_ctc_loss=1.561, task_loss=0, contrastive_loss=0.405, total=4126.25, n_correct=2124.68, ppl=3.88, accuracy=51.492, wps=15321.3, ups=1.24, wpb=12324.7, bsz=455.9, num_updates=5400, lr=0.00019245, gnorm=0.713, clip=0, loss_scale=2, train_wall=80, gb_free=14.6, wall=4221
2023-08-12 03:18:03 | INFO | train_inner | epoch 004:   1086 / 1474 loss=2.726, trans_loss=3.688, nll_loss=1.95, w2v_ctc_loss=1.555, task_loss=0, contrastive_loss=0.376, total=4084.57, n_correct=2115.03, ppl=3.86, accuracy=51.781, wps=15229.3, ups=1.25, wpb=12192, bsz=441.2, num_updates=5500, lr=0.000190693, gnorm=0.695, clip=0, loss_scale=2, train_wall=80, gb_free=16.6, wall=4301
2023-08-12 03:19:24 | INFO | train_inner | epoch 004:   1186 / 1474 loss=2.737, trans_loss=3.677, nll_loss=1.938, w2v_ctc_loss=1.545, task_loss=0, contrastive_loss=0.472, total=4162.44, n_correct=2178.8, ppl=3.83, accuracy=52.344, wps=15410.3, ups=1.24, wpb=12433, bsz=482.7, num_updates=5600, lr=0.000188982, gnorm=0.687, clip=0, loss_scale=2, train_wall=80, gb_free=16.8, wall=4382
2023-08-12 03:20:44 | INFO | train_inner | epoch 004:   1286 / 1474 loss=2.696, trans_loss=3.659, nll_loss=1.914, w2v_ctc_loss=1.525, task_loss=0, contrastive_loss=0.432, total=4144.75, n_correct=2193.81, ppl=3.77, accuracy=52.93, wps=15510.2, ups=1.25, wpb=12378.7, bsz=470, num_updates=5700, lr=0.000187317, gnorm=0.674, clip=0, loss_scale=2, train_wall=79, gb_free=16.6, wall=4461
2023-08-12 03:22:03 | INFO | train_inner | epoch 004:   1386 / 1474 loss=2.642, trans_loss=3.648, nll_loss=1.9, w2v_ctc_loss=1.517, task_loss=0, contrastive_loss=0.307, total=4107.86, n_correct=2188.01, ppl=3.73, accuracy=53.264, wps=15536.1, ups=1.27, wpb=12268.5, bsz=439.2, num_updates=5800, lr=0.000185695, gnorm=0.66, clip=0, loss_scale=2, train_wall=78, gb_free=17, wall=4540
2023-08-12 03:23:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.5146, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.5146, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.5146, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.5146, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.5146, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.5146, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.5146, device='cuda:6')
2023-08-12 03:23:36 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 4.589 | trans_loss 5.628 | nll_loss 3.003 | w2v_ctc_loss 1.697 | task_loss 0 | contrastive_loss 0.482 | total 4003.4 | n_correct 2372 | ppl 8.02 | accuracy 59.25 | uer 24.675 | wer 26.099 | raw_wer 26.099 | bleu 16.56 | wps 2129.1 | wpb 4003.4 | bsz 141.8 | num_updates 5888 | best_bleu 16.56
2023-08-12 03:23:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5888 updates
2023-08-12 03:23:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 03:23:51 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 03:24:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt (epoch 4 @ 5888 updates, score 16.56) (writing took 27.571198031306267 seconds)
2023-08-12 03:24:04 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-08-12 03:24:04 | INFO | train | epoch 004 | loss 2.823 | trans_loss 3.733 | nll_loss 2.006 | w2v_ctc_loss 1.588 | task_loss 0 | contrastive_loss 0.468 | total 4138.65 | n_correct 2068.04 | ppl 4.02 | accuracy 49.969 | wps 14687.6 | ups 1.19 | wpb 12355.8 | bsz 458.5 | num_updates 5888 | lr 0.000184302 | gnorm 0.881 | clip 0 | loss_scale 2 | train_wall 1173 | gb_free 14.8 | wall 4661
2023-08-12 03:24:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 03:24:04 | INFO | fairseq.trainer | begin training epoch 5
2023-08-12 03:24:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 03:24:22 | INFO | train_inner | epoch 005:     12 / 1474 loss=2.616, trans_loss=3.639, nll_loss=1.887, w2v_ctc_loss=1.486, task_loss=0, contrastive_loss=0.318, total=4036.09, n_correct=2167.64, ppl=3.7, accuracy=53.706, wps=8689, ups=0.72, wpb=12049.9, bsz=436.9, num_updates=5900, lr=0.000184115, gnorm=0.66, clip=0, loss_scale=2, train_wall=79, gb_free=11.9, wall=4679
2023-08-12 03:25:42 | INFO | train_inner | epoch 005:    112 / 1474 loss=2.55, trans_loss=3.602, nll_loss=1.84, w2v_ctc_loss=1.408, task_loss=0, contrastive_loss=0.334, total=4256.69, n_correct=2344.33, ppl=3.58, accuracy=55.074, wps=15723.2, ups=1.24, wpb=12710.6, bsz=500.9, num_updates=6000, lr=0.000182574, gnorm=0.627, clip=0, loss_scale=2, train_wall=80, gb_free=17.3, wall=4760
2023-08-12 03:25:42 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 03:26:06 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.564 | trans_loss 5.608 | nll_loss 2.976 | w2v_ctc_loss 1.671 | task_loss 0 | contrastive_loss 0.469 | total 4003.4 | n_correct 2377 | ppl 7.87 | accuracy 59.375 | uer 24.378 | wer 25.983 | raw_wer 25.983 | bleu 16.53 | wps 2321.6 | wpb 4003.4 | bsz 141.8 | num_updates 6000 | best_bleu 16.56
2023-08-12 03:26:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6000 updates
2023-08-12 03:26:06 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_5_6000.pt
2023-08-12 03:26:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_5_6000.pt
2023-08-12 03:26:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_5_6000.pt (epoch 5 @ 6000 updates, score 16.53) (writing took 40.04857851751149 seconds)
2023-08-12 03:28:05 | INFO | train_inner | epoch 005:    212 / 1474 loss=2.591, trans_loss=3.608, nll_loss=1.846, w2v_ctc_loss=1.426, task_loss=0, contrastive_loss=0.522, total=4186.1, n_correct=2301.72, ppl=3.6, accuracy=54.985, wps=8761.9, ups=0.7, wpb=12490.5, bsz=484.8, num_updates=6100, lr=0.000181071, gnorm=0.64, clip=0, loss_scale=2, train_wall=79, gb_free=14.7, wall=4902
2023-08-12 03:29:24 | INFO | train_inner | epoch 005:    312 / 1474 loss=2.556, trans_loss=3.593, nll_loss=1.832, w2v_ctc_loss=1.436, task_loss=0, contrastive_loss=0.379, total=4098.24, n_correct=2254.57, ppl=3.56, accuracy=55.013, wps=15506, ups=1.27, wpb=12253.4, bsz=447.9, num_updates=6200, lr=0.000179605, gnorm=0.635, clip=0, loss_scale=2, train_wall=79, gb_free=16.2, wall=4981
2023-08-12 03:30:44 | INFO | train_inner | epoch 005:    412 / 1474 loss=2.553, trans_loss=3.585, nll_loss=1.821, w2v_ctc_loss=1.408, task_loss=0, contrastive_loss=0.455, total=4138.6, n_correct=2296.85, ppl=3.53, accuracy=55.498, wps=15447.5, ups=1.25, wpb=12367.6, bsz=466.6, num_updates=6300, lr=0.000178174, gnorm=0.643, clip=0, loss_scale=2, train_wall=80, gb_free=13, wall=5062
2023-08-12 03:32:04 | INFO | train_inner | epoch 005:    512 / 1474 loss=2.488, trans_loss=3.585, nll_loss=1.819, w2v_ctc_loss=1.407, task_loss=0, contrastive_loss=0.247, total=4018.65, n_correct=2232.08, ppl=3.53, accuracy=55.543, wps=14992, ups=1.25, wpb=12005.2, bsz=417.4, num_updates=6400, lr=0.000176777, gnorm=0.622, clip=0, loss_scale=2, train_wall=80, gb_free=16.6, wall=5142
2023-08-12 03:33:24 | INFO | train_inner | epoch 005:    612 / 1474 loss=2.52, trans_loss=3.589, nll_loss=1.821, w2v_ctc_loss=1.396, task_loss=0, contrastive_loss=0.412, total=4121.58, n_correct=2293.39, ppl=3.53, accuracy=55.643, wps=15381.8, ups=1.25, wpb=12295, bsz=451, num_updates=6500, lr=0.000175412, gnorm=0.625, clip=0, loss_scale=2, train_wall=79, gb_free=16.5, wall=5222
2023-08-12 03:34:44 | INFO | train_inner | epoch 005:    712 / 1474 loss=2.523, trans_loss=3.582, nll_loss=1.815, w2v_ctc_loss=1.397, task_loss=0, contrastive_loss=0.394, total=4169.57, n_correct=2334.71, ppl=3.52, accuracy=55.994, wps=15570.1, ups=1.25, wpb=12447.3, bsz=482.2, num_updates=6600, lr=0.000174078, gnorm=0.618, clip=0, loss_scale=2, train_wall=79, gb_free=17, wall=5302
2023-08-12 03:36:05 | INFO | train_inner | epoch 005:    812 / 1474 loss=2.483, trans_loss=3.571, nll_loss=1.8, w2v_ctc_loss=1.386, task_loss=0, contrastive_loss=0.318, total=4123.32, n_correct=2320.02, ppl=3.48, accuracy=56.266, wps=15252.1, ups=1.24, wpb=12307, bsz=448.5, num_updates=6700, lr=0.000172774, gnorm=0.605, clip=0, loss_scale=2, train_wall=80, gb_free=17.6, wall=5382
2023-08-12 03:37:25 | INFO | train_inner | epoch 005:    912 / 1474 loss=2.454, trans_loss=3.563, nll_loss=1.791, w2v_ctc_loss=1.374, task_loss=0, contrastive_loss=0.277, total=4109.54, n_correct=2327.05, ppl=3.46, accuracy=56.626, wps=15203.8, ups=1.24, wpb=12270.4, bsz=449.3, num_updates=6800, lr=0.000171499, gnorm=0.604, clip=0, loss_scale=2, train_wall=80, gb_free=16.3, wall=5463
2023-08-12 03:38:45 | INFO | train_inner | epoch 005:   1012 / 1474 loss=2.468, trans_loss=3.568, nll_loss=1.797, w2v_ctc_loss=1.377, task_loss=0, contrastive_loss=0.346, total=4157.73, n_correct=2350.95, ppl=3.48, accuracy=56.544, wps=15589.3, ups=1.26, wpb=12411.9, bsz=458.8, num_updates=6900, lr=0.000170251, gnorm=0.599, clip=0, loss_scale=2, train_wall=79, gb_free=17.2, wall=5543
2023-08-12 03:40:06 | INFO | train_inner | epoch 005:   1112 / 1474 loss=2.482, trans_loss=3.562, nll_loss=1.787, w2v_ctc_loss=1.383, task_loss=0, contrastive_loss=0.358, total=4172.61, n_correct=2371.06, ppl=3.45, accuracy=56.824, wps=15418.1, ups=1.24, wpb=12446.4, bsz=466.7, num_updates=7000, lr=0.000169031, gnorm=0.605, clip=0, loss_scale=2, train_wall=80, gb_free=13.7, wall=5623
2023-08-12 03:41:26 | INFO | train_inner | epoch 005:   1212 / 1474 loss=2.43, trans_loss=3.555, nll_loss=1.779, w2v_ctc_loss=1.356, task_loss=0, contrastive_loss=0.256, total=4166.97, n_correct=2379.15, ppl=3.43, accuracy=57.095, wps=15550.8, ups=1.25, wpb=12430.2, bsz=454.5, num_updates=7100, lr=0.000167836, gnorm=0.597, clip=0, loss_scale=2, train_wall=79, gb_free=16.9, wall=5703
2023-08-12 03:42:46 | INFO | train_inner | epoch 005:   1312 / 1474 loss=2.404, trans_loss=3.549, nll_loss=1.772, w2v_ctc_loss=1.343, task_loss=0, contrastive_loss=0.223, total=4132.22, n_correct=2365.14, ppl=3.42, accuracy=57.237, wps=15296.2, ups=1.24, wpb=12332.7, bsz=445.1, num_updates=7200, lr=0.000166667, gnorm=0.592, clip=0, loss_scale=2, train_wall=80, gb_free=16.6, wall=5784
2023-08-12 03:44:06 | INFO | train_inner | epoch 005:   1412 / 1474 loss=2.421, trans_loss=3.552, nll_loss=1.779, w2v_ctc_loss=1.342, task_loss=0, contrastive_loss=0.283, total=4135.72, n_correct=2364.28, ppl=3.43, accuracy=57.167, wps=15520.9, ups=1.26, wpb=12352.5, bsz=457.5, num_updates=7300, lr=0.000165521, gnorm=0.619, clip=0, loss_scale=2, train_wall=79, gb_free=16.3, wall=5863
2023-08-12 03:44:56 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 03:45:18 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.414 | trans_loss 5.475 | nll_loss 2.819 | w2v_ctc_loss 1.536 | task_loss 0 | contrastive_loss 0.43 | total 4003.4 | n_correct 2465.2 | ppl 7.06 | accuracy 61.578 | uer 23.255 | wer 24.719 | raw_wer 24.719 | bleu 18.22 | wps 2263.5 | wpb 4003.4 | bsz 141.8 | num_updates 7362 | best_bleu 18.22
2023-08-12 03:45:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7362 updates
2023-08-12 03:45:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 03:45:34 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 03:45:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt (epoch 5 @ 7362 updates, score 18.22) (writing took 30.45989950746298 seconds)
2023-08-12 03:45:49 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-08-12 03:45:49 | INFO | train | epoch 005 | loss 2.493 | trans_loss 3.575 | nll_loss 1.806 | w2v_ctc_loss 1.388 | task_loss 0 | contrastive_loss 0.343 | total 4138.65 | n_correct 2323.76 | ppl 3.5 | accuracy 56.148 | wps 13960.7 | ups 1.13 | wpb 12355.8 | bsz 458.5 | num_updates 7362 | lr 0.000164823 | gnorm 0.617 | clip 0 | loss_scale 4 | train_wall 1172 | gb_free 16.2 | wall 5966
2023-08-12 03:45:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 03:45:49 | INFO | fairseq.trainer | begin training epoch 6
2023-08-12 03:45:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 03:46:27 | INFO | train_inner | epoch 006:     38 / 1474 loss=2.402, trans_loss=3.534, nll_loss=1.752, w2v_ctc_loss=1.333, task_loss=0, contrastive_loss=0.279, total=4115.39, n_correct=2374.61, ppl=3.37, accuracy=57.701, wps=8701, ups=0.71, wpb=12279.5, bsz=446.7, num_updates=7400, lr=0.000164399, gnorm=0.6, clip=0, loss_scale=4, train_wall=80, gb_free=16.9, wall=6005
2023-08-12 03:47:47 | INFO | train_inner | epoch 006:    138 / 1474 loss=2.356, trans_loss=3.505, nll_loss=1.717, w2v_ctc_loss=1.279, task_loss=0, contrastive_loss=0.319, total=4157.06, n_correct=2431.12, ppl=3.29, accuracy=58.482, wps=15569.4, ups=1.25, wpb=12417.1, bsz=457.8, num_updates=7500, lr=0.000163299, gnorm=0.58, clip=0, loss_scale=4, train_wall=79, gb_free=17.6, wall=6084
2023-08-12 03:49:07 | INFO | train_inner | epoch 006:    238 / 1474 loss=2.359, trans_loss=3.516, nll_loss=1.733, w2v_ctc_loss=1.313, task_loss=0, contrastive_loss=0.228, total=4115.6, n_correct=2395.05, ppl=3.32, accuracy=58.194, wps=15347.3, ups=1.25, wpb=12295.8, bsz=440.7, num_updates=7600, lr=0.000162221, gnorm=0.579, clip=0, loss_scale=4, train_wall=80, gb_free=16.8, wall=6164
2023-08-12 03:50:28 | INFO | train_inner | epoch 006:    338 / 1474 loss=2.395, trans_loss=3.504, nll_loss=1.717, w2v_ctc_loss=1.266, task_loss=0, contrastive_loss=0.521, total=4163.86, n_correct=2437.91, ppl=3.29, accuracy=58.549, wps=15267.1, ups=1.23, wpb=12432.8, bsz=484.8, num_updates=7700, lr=0.000161165, gnorm=0.59, clip=0, loss_scale=4, train_wall=81, gb_free=16.1, wall=6246
2023-08-12 03:51:48 | INFO | train_inner | epoch 006:    438 / 1474 loss=2.322, trans_loss=3.5, nll_loss=1.71, w2v_ctc_loss=1.27, task_loss=0, contrastive_loss=0.234, total=4156.74, n_correct=2452.4, ppl=3.27, accuracy=58.998, wps=15579.1, ups=1.26, wpb=12410.9, bsz=471.8, num_updates=7800, lr=0.000160128, gnorm=0.575, clip=0, loss_scale=4, train_wall=79, gb_free=15.5, wall=6326
2023-08-12 03:53:08 | INFO | train_inner | epoch 006:    538 / 1474 loss=2.324, trans_loss=3.503, nll_loss=1.714, w2v_ctc_loss=1.283, task_loss=0, contrastive_loss=0.222, total=4173.1, n_correct=2458.39, ppl=3.28, accuracy=58.91, wps=15602.9, ups=1.25, wpb=12456.1, bsz=456.3, num_updates=7900, lr=0.000159111, gnorm=0.57, clip=0, loss_scale=4, train_wall=79, gb_free=15.8, wall=6405
2023-08-12 03:54:29 | INFO | train_inner | epoch 006:    638 / 1474 loss=2.331, trans_loss=3.504, nll_loss=1.716, w2v_ctc_loss=1.268, task_loss=0, contrastive_loss=0.278, total=4147.23, n_correct=2442.25, ppl=3.29, accuracy=58.889, wps=15356.7, ups=1.24, wpb=12379.1, bsz=471.9, num_updates=8000, lr=0.000158114, gnorm=0.591, clip=0, loss_scale=4, train_wall=80, gb_free=12.8, wall=6486
2023-08-12 03:54:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 03:54:51 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.35 | trans_loss 5.4 | nll_loss 2.724 | w2v_ctc_loss 1.545 | task_loss 0 | contrastive_loss 0.394 | total 4003.4 | n_correct 2503.1 | ppl 6.61 | accuracy 62.524 | uer 22.398 | wer 24.112 | raw_wer 24.112 | bleu 18.94 | wps 2337 | wpb 4003.4 | bsz 141.8 | num_updates 8000 | best_bleu 18.94
2023-08-12 03:54:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8000 updates
2023-08-12 03:54:51 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_6_8000.pt
2023-08-12 03:54:55 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_6_8000.pt
2023-08-12 03:55:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_6_8000.pt (epoch 6 @ 8000 updates, score 18.94) (writing took 49.675605703145266 seconds)
2023-08-12 03:57:01 | INFO | train_inner | epoch 006:    738 / 1474 loss=2.324, trans_loss=3.504, nll_loss=1.716, w2v_ctc_loss=1.282, task_loss=0, contrastive_loss=0.231, total=4147.61, n_correct=2444.83, ppl=3.29, accuracy=58.946, wps=8134.4, ups=0.66, wpb=12383.5, bsz=453.2, num_updates=8100, lr=0.000157135, gnorm=0.568, clip=0, loss_scale=4, train_wall=79, gb_free=16.2, wall=6638
2023-08-12 03:58:21 | INFO | train_inner | epoch 006:    838 / 1474 loss=2.31, trans_loss=3.505, nll_loss=1.717, w2v_ctc_loss=1.273, task_loss=0, contrastive_loss=0.21, total=4114.7, n_correct=2421.2, ppl=3.29, accuracy=58.843, wps=15375.7, ups=1.25, wpb=12284.9, bsz=442.6, num_updates=8200, lr=0.000156174, gnorm=0.569, clip=0, loss_scale=4, train_wall=79, gb_free=17.6, wall=6718
2023-08-12 03:59:41 | INFO | train_inner | epoch 006:    938 / 1474 loss=2.339, trans_loss=3.508, nll_loss=1.72, w2v_ctc_loss=1.278, task_loss=0, contrastive_loss=0.306, total=4082.44, n_correct=2399.84, ppl=3.3, accuracy=58.784, wps=15200.6, ups=1.25, wpb=12184.1, bsz=442.4, num_updates=8300, lr=0.00015523, gnorm=0.569, clip=0, loss_scale=4, train_wall=80, gb_free=16.2, wall=6798
2023-08-12 04:01:01 | INFO | train_inner | epoch 006:   1038 / 1474 loss=2.333, trans_loss=3.493, nll_loss=1.701, w2v_ctc_loss=1.255, task_loss=0, contrastive_loss=0.378, total=4168.55, n_correct=2469.54, ppl=3.25, accuracy=59.242, wps=15501.3, ups=1.25, wpb=12442.3, bsz=478.7, num_updates=8400, lr=0.000154303, gnorm=0.58, clip=0, loss_scale=4, train_wall=80, gb_free=15.6, wall=6879
2023-08-12 04:02:21 | INFO | train_inner | epoch 006:   1138 / 1474 loss=2.3, trans_loss=3.493, nll_loss=1.702, w2v_ctc_loss=1.268, task_loss=0, contrastive_loss=0.211, total=4075.88, n_correct=2408.36, ppl=3.25, accuracy=59.088, wps=15247.8, ups=1.25, wpb=12168.6, bsz=430.6, num_updates=8500, lr=0.000153393, gnorm=0.567, clip=0, loss_scale=4, train_wall=79, gb_free=14.1, wall=6958
2023-08-12 04:03:41 | INFO | train_inner | epoch 006:   1238 / 1474 loss=2.358, trans_loss=3.484, nll_loss=1.693, w2v_ctc_loss=1.254, task_loss=0, contrastive_loss=0.522, total=4136.41, n_correct=2455.79, ppl=3.23, accuracy=59.37, wps=15368.7, ups=1.24, wpb=12356.2, bsz=470.5, num_updates=8600, lr=0.000152499, gnorm=0.575, clip=0, loss_scale=4, train_wall=80, gb_free=15.3, wall=7039
2023-08-12 04:05:01 | INFO | train_inner | epoch 006:   1338 / 1474 loss=2.274, trans_loss=3.489, nll_loss=1.695, w2v_ctc_loss=1.248, task_loss=0, contrastive_loss=0.192, total=4123.87, n_correct=2462.76, ppl=3.24, accuracy=59.72, wps=15411.9, ups=1.25, wpb=12299.9, bsz=453.6, num_updates=8700, lr=0.00015162, gnorm=0.563, clip=0, loss_scale=4, train_wall=79, gb_free=16.3, wall=7119
2023-08-12 04:06:22 | INFO | train_inner | epoch 006:   1438 / 1474 loss=2.275, trans_loss=3.483, nll_loss=1.689, w2v_ctc_loss=1.247, task_loss=0, contrastive_loss=0.199, total=4197.44, n_correct=2505.46, ppl=3.22, accuracy=59.69, wps=15584.6, ups=1.24, wpb=12530.9, bsz=462.9, num_updates=8800, lr=0.000150756, gnorm=0.552, clip=0, loss_scale=4, train_wall=80, gb_free=16.4, wall=7199
2023-08-12 04:06:50 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 04:07:13 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.285 | trans_loss 5.356 | nll_loss 2.674 | w2v_ctc_loss 1.46 | task_loss 0 | contrastive_loss 0.367 | total 4003.4 | n_correct 2526.8 | ppl 6.38 | accuracy 63.116 | uer 21.23 | wer 22.837 | raw_wer 22.837 | bleu 19.41 | wps 2175.1 | wpb 4003.4 | bsz 141.8 | num_updates 8836 | best_bleu 19.41
2023-08-12 04:07:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8836 updates
2023-08-12 04:07:13 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 04:07:28 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 04:07:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt (epoch 6 @ 8836 updates, score 19.41) (writing took 27.9710107780993 seconds)
2023-08-12 04:07:41 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-08-12 04:07:41 | INFO | train | epoch 006 | loss 2.327 | trans_loss 3.499 | nll_loss 1.71 | w2v_ctc_loss 1.27 | task_loss 0 | contrastive_loss 0.288 | total 4138.65 | n_correct 2441.17 | ppl 3.27 | accuracy 58.985 | wps 13873.4 | ups 1.12 | wpb 12355.8 | bsz 458.5 | num_updates 8836 | lr 0.000150448 | gnorm 0.573 | clip 0 | loss_scale 4 | train_wall 1172 | gb_free 15.1 | wall 7279
2023-08-12 04:07:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 04:07:42 | INFO | fairseq.trainer | begin training epoch 7
2023-08-12 04:07:42 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 04:08:41 | INFO | train_inner | epoch 007:     64 / 1474 loss=2.244, trans_loss=3.469, nll_loss=1.672, w2v_ctc_loss=1.216, task_loss=0, contrastive_loss=0.21, total=4105.94, n_correct=2469.69, ppl=3.19, accuracy=60.149, wps=8778.8, ups=0.72, wpb=12258, bsz=460.8, num_updates=8900, lr=0.000149906, gnorm=0.557, clip=0, loss_scale=4, train_wall=79, gb_free=14.9, wall=7339
2023-08-12 04:10:01 | INFO | train_inner | epoch 007:    164 / 1474 loss=2.242, trans_loss=3.46, nll_loss=1.66, w2v_ctc_loss=1.199, task_loss=0, contrastive_loss=0.279, total=4101.13, n_correct=2473.81, ppl=3.16, accuracy=60.32, wps=15385.5, ups=1.26, wpb=12245.3, bsz=452.9, num_updates=9000, lr=0.000149071, gnorm=0.555, clip=0, loss_scale=4, train_wall=79, gb_free=16.8, wall=7418
2023-08-12 04:11:21 | INFO | train_inner | epoch 007:    264 / 1474 loss=2.222, trans_loss=3.455, nll_loss=1.653, w2v_ctc_loss=1.203, task_loss=0, contrastive_loss=0.19, total=4143.65, n_correct=2507.71, ppl=3.14, accuracy=60.519, wps=15413.4, ups=1.25, wpb=12365.3, bsz=458, num_updates=9100, lr=0.00014825, gnorm=0.552, clip=0, loss_scale=4, train_wall=80, gb_free=16.2, wall=7498
2023-08-12 04:12:41 | INFO | train_inner | epoch 007:    364 / 1474 loss=2.273, trans_loss=3.46, nll_loss=1.659, w2v_ctc_loss=1.196, task_loss=0, contrastive_loss=0.446, total=4190.59, n_correct=2529.92, ppl=3.16, accuracy=60.371, wps=15546.1, ups=1.24, wpb=12506.9, bsz=477.2, num_updates=9200, lr=0.000147442, gnorm=0.553, clip=0, loss_scale=4, train_wall=80, gb_free=16.7, wall=7579
2023-08-12 04:14:01 | INFO | train_inner | epoch 007:    464 / 1474 loss=2.254, trans_loss=3.46, nll_loss=1.661, w2v_ctc_loss=1.191, task_loss=0, contrastive_loss=0.367, total=4154.13, n_correct=2507.15, ppl=3.16, accuracy=60.353, wps=15563.8, ups=1.25, wpb=12405.4, bsz=461.6, num_updates=9300, lr=0.000146647, gnorm=0.554, clip=0, loss_scale=4, train_wall=79, gb_free=16.3, wall=7659
2023-08-12 04:15:20 | INFO | train_inner | epoch 007:    564 / 1474 loss=2.214, trans_loss=3.457, nll_loss=1.654, w2v_ctc_loss=1.195, task_loss=0, contrastive_loss=0.194, total=4171.52, n_correct=2528.15, ppl=3.15, accuracy=60.605, wps=15733, ups=1.26, wpb=12446, bsz=461, num_updates=9400, lr=0.000145865, gnorm=0.55, clip=0, loss_scale=4, train_wall=79, gb_free=16.8, wall=7738
2023-08-12 04:16:41 | INFO | train_inner | epoch 007:    664 / 1474 loss=2.203, trans_loss=3.453, nll_loss=1.649, w2v_ctc_loss=1.187, task_loss=0, contrastive_loss=0.181, total=4151.13, n_correct=2526.27, ppl=3.14, accuracy=60.857, wps=15397.5, ups=1.24, wpb=12385.9, bsz=454, num_updates=9500, lr=0.000145095, gnorm=0.555, clip=0, loss_scale=8, train_wall=80, gb_free=12.7, wall=7818
2023-08-12 04:18:01 | INFO | train_inner | epoch 007:    764 / 1474 loss=2.201, trans_loss=3.446, nll_loss=1.642, w2v_ctc_loss=1.189, task_loss=0, contrastive_loss=0.175, total=4124.23, n_correct=2504.55, ppl=3.12, accuracy=60.728, wps=15319.4, ups=1.24, wpb=12314.5, bsz=446.8, num_updates=9600, lr=0.000144338, gnorm=0.552, clip=0, loss_scale=8, train_wall=80, gb_free=14.2, wall=7899
2023-08-12 04:19:22 | INFO | train_inner | epoch 007:    864 / 1474 loss=2.211, trans_loss=3.457, nll_loss=1.655, w2v_ctc_loss=1.192, task_loss=0, contrastive_loss=0.197, total=4148.43, n_correct=2514.5, ppl=3.15, accuracy=60.613, wps=15367.8, ups=1.24, wpb=12380.2, bsz=461.8, num_updates=9700, lr=0.000143592, gnorm=0.555, clip=0, loss_scale=8, train_wall=80, gb_free=16.4, wall=7979
2023-08-12 04:20:42 | INFO | train_inner | epoch 007:    964 / 1474 loss=2.224, trans_loss=3.449, nll_loss=1.647, w2v_ctc_loss=1.178, task_loss=0, contrastive_loss=0.291, total=4141.1, n_correct=2515.03, ppl=3.13, accuracy=60.733, wps=15399.4, ups=1.25, wpb=12362.4, bsz=473.7, num_updates=9800, lr=0.000142857, gnorm=0.6, clip=0, loss_scale=8, train_wall=80, gb_free=13.8, wall=8059
2023-08-12 04:22:02 | INFO | train_inner | epoch 007:   1064 / 1474 loss=2.196, trans_loss=3.456, nll_loss=1.656, w2v_ctc_loss=1.189, task_loss=0, contrastive_loss=0.161, total=4100.93, n_correct=2487.74, ppl=3.15, accuracy=60.663, wps=15253.7, ups=1.25, wpb=12243.4, bsz=437.6, num_updates=9900, lr=0.000142134, gnorm=0.54, clip=0, loss_scale=8, train_wall=80, gb_free=14.7, wall=8140
2023-08-12 04:23:23 | INFO | train_inner | epoch 007:   1164 / 1474 loss=2.251, trans_loss=3.441, nll_loss=1.639, w2v_ctc_loss=1.178, task_loss=0, contrastive_loss=0.418, total=4139.88, n_correct=2522, ppl=3.11, accuracy=60.92, wps=15359.1, ups=1.24, wpb=12369.6, bsz=471.4, num_updates=10000, lr=0.000141421, gnorm=0.556, clip=0, loss_scale=8, train_wall=80, gb_free=16.5, wall=8220
2023-08-12 04:23:23 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 04:23:46 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.22 | trans_loss 5.309 | nll_loss 2.614 | w2v_ctc_loss 1.371 | task_loss 0 | contrastive_loss 0.352 | total 4003.4 | n_correct 2559.6 | ppl 6.12 | accuracy 63.936 | uer 19.988 | wer 21.718 | raw_wer 21.718 | bleu 19.89 | wps 2335.6 | wpb 4003.4 | bsz 141.8 | num_updates 10000 | best_bleu 19.89
2023-08-12 04:23:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10000 updates
2023-08-12 04:23:46 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_7_10000.pt
2023-08-12 04:23:50 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_7_10000.pt
2023-08-12 04:24:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_7_10000.pt (epoch 7 @ 10000 updates, score 19.89) (writing took 51.60199434123933 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:0')
2023-08-12 04:25:56 | INFO | train_inner | epoch 007:   1264 / 1474 loss=2.189, trans_loss=3.447, nll_loss=1.645, w2v_ctc_loss=1.175, task_loss=0, contrastive_loss=0.189, total=4129.16, n_correct=2511.43, ppl=3.13, accuracy=60.822, wps=8031, ups=0.65, wpb=12331, bsz=448.9, num_updates=10100, lr=0.00014072, gnorm=0.427, clip=0, loss_scale=8, train_wall=79, gb_free=16.7, wall=8374
2023-08-12 04:27:16 | INFO | train_inner | epoch 007:   1364 / 1474 loss=2.209, trans_loss=3.44, nll_loss=1.636, w2v_ctc_loss=1.186, task_loss=0, contrastive_loss=0.225, total=4177.71, n_correct=2553.7, ppl=3.11, accuracy=61.127, wps=15721.6, ups=1.26, wpb=12473.7, bsz=478.6, num_updates=10200, lr=0.000140028, gnorm=0.428, clip=0, loss_scale=8, train_wall=79, gb_free=16.8, wall=8453
2023-08-12 04:28:38 | INFO | train_inner | epoch 007:   1464 / 1474 loss=2.214, trans_loss=3.445, nll_loss=1.644, w2v_ctc_loss=1.18, task_loss=0, contrastive_loss=0.285, total=4107.01, n_correct=2500.49, ppl=3.13, accuracy=60.883, wps=14849.8, ups=1.21, wpb=12270.2, bsz=442.8, num_updates=10300, lr=0.000139347, gnorm=0.432, clip=0, loss_scale=8, train_wall=82, gb_free=13.3, wall=8536
2023-08-12 04:28:46 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:4')
2023-08-12 04:29:08 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.229 | trans_loss 5.299 | nll_loss 2.6 | w2v_ctc_loss 1.435 | task_loss 0 | contrastive_loss 0.343 | total 4003.4 | n_correct 2565.3 | ppl 6.06 | accuracy 64.078 | uer 20.89 | wer 22.855 | raw_wer 22.855 | bleu 19.64 | wps 2369 | wpb 4003.4 | bsz 141.8 | num_updates 10310 | best_bleu 19.89
2023-08-12 04:29:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10310 updates
2023-08-12 04:29:08 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_19.6407.pt
2023-08-12 04:29:12 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_19.6407.pt
2023-08-12 04:29:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_19.6407.pt (epoch 7 @ 10310 updates, score 19.64) (writing took 21.41389543376863 seconds)
2023-08-12 04:29:29 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-08-12 04:29:29 | INFO | train | epoch 007 | loss 2.222 | trans_loss 3.452 | nll_loss 1.65 | w2v_ctc_loss 1.189 | task_loss 0 | contrastive_loss 0.255 | total 4138.65 | n_correct 2510.85 | ppl 3.14 | accuracy 60.668 | wps 13923.8 | ups 1.13 | wpb 12355.8 | bsz 458.5 | num_updates 10310 | lr 0.000139279 | gnorm 0.53 | clip 0 | loss_scale 8 | train_wall 1173 | gb_free 13.3 | wall 8587
2023-08-12 04:29:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 04:29:30 | INFO | fairseq.trainer | begin training epoch 8
2023-08-12 04:29:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 04:30:51 | INFO | train_inner | epoch 008:     90 / 1474 loss=2.158, trans_loss=3.435, nll_loss=1.624, w2v_ctc_loss=1.146, task_loss=0, contrastive_loss=0.18, total=4106.01, n_correct=2521.73, ppl=3.08, accuracy=61.416, wps=9237.9, ups=0.75, wpb=12242.2, bsz=440.9, num_updates=10400, lr=0.000138675, gnorm=0.428, clip=0, loss_scale=8, train_wall=80, gb_free=17, wall=8668
2023-08-12 04:32:10 | INFO | train_inner | epoch 008:    190 / 1474 loss=2.154, trans_loss=3.423, nll_loss=1.61, w2v_ctc_loss=1.14, task_loss=0, contrastive_loss=0.2, total=4043.12, n_correct=2495.88, ppl=3.05, accuracy=61.732, wps=15160.9, ups=1.26, wpb=12058.4, bsz=430, num_updates=10500, lr=0.000138013, gnorm=0.433, clip=0, loss_scale=8, train_wall=79, gb_free=13.3, wall=8748
2023-08-12 04:33:30 | INFO | train_inner | epoch 008:    290 / 1474 loss=2.157, trans_loss=3.418, nll_loss=1.606, w2v_ctc_loss=1.143, task_loss=0, contrastive_loss=0.196, total=4207.9, n_correct=2601.26, ppl=3.04, accuracy=61.818, wps=15755.9, ups=1.25, wpb=12558.3, bsz=488.1, num_updates=10600, lr=0.000137361, gnorm=0.429, clip=0, loss_scale=8, train_wall=79, gb_free=14.1, wall=8828
2023-08-12 04:34:52 | INFO | train_inner | epoch 008:    390 / 1474 loss=2.169, trans_loss=3.424, nll_loss=1.613, w2v_ctc_loss=1.156, task_loss=0, contrastive_loss=0.22, total=4134.6, n_correct=2544.25, ppl=3.06, accuracy=61.536, wps=15152.2, ups=1.23, wpb=12337.2, bsz=444.7, num_updates=10700, lr=0.000136717, gnorm=0.427, clip=0, loss_scale=8, train_wall=81, gb_free=17.2, wall=8909
2023-08-12 04:36:12 | INFO | train_inner | epoch 008:    490 / 1474 loss=2.23, trans_loss=3.422, nll_loss=1.612, w2v_ctc_loss=1.137, task_loss=0, contrastive_loss=0.484, total=4196.6, n_correct=2588.57, ppl=3.06, accuracy=61.683, wps=15546.7, ups=1.24, wpb=12528.1, bsz=501.4, num_updates=10800, lr=0.000136083, gnorm=0.436, clip=0, loss_scale=8, train_wall=80, gb_free=13, wall=8990
2023-08-12 04:37:32 | INFO | train_inner | epoch 008:    590 / 1474 loss=2.153, trans_loss=3.419, nll_loss=1.611, w2v_ctc_loss=1.155, task_loss=0, contrastive_loss=0.155, total=4065.55, n_correct=2503.75, ppl=3.05, accuracy=61.585, wps=15281.4, ups=1.26, wpb=12153.4, bsz=428.3, num_updates=10900, lr=0.000135457, gnorm=0.434, clip=0, loss_scale=8, train_wall=79, gb_free=16.1, wall=9069
2023-08-12 04:38:52 | INFO | train_inner | epoch 008:    690 / 1474 loss=2.144, trans_loss=3.415, nll_loss=1.603, w2v_ctc_loss=1.147, task_loss=0, contrastive_loss=0.165, total=4135.41, n_correct=2562.37, ppl=3.04, accuracy=61.962, wps=15420.1, ups=1.25, wpb=12343.8, bsz=447.2, num_updates=11000, lr=0.00013484, gnorm=0.424, clip=0, loss_scale=8, train_wall=79, gb_free=16, wall=9149
2023-08-12 04:40:13 | INFO | train_inner | epoch 008:    790 / 1474 loss=2.156, trans_loss=3.411, nll_loss=1.601, w2v_ctc_loss=1.138, task_loss=0, contrastive_loss=0.248, total=4128.86, n_correct=2555.83, ppl=3.03, accuracy=61.902, wps=15262.9, ups=1.24, wpb=12339, bsz=452.1, num_updates=11100, lr=0.000134231, gnorm=0.424, clip=0, loss_scale=8, train_wall=80, gb_free=16.2, wall=9230
2023-08-12 04:41:32 | INFO | train_inner | epoch 008:    890 / 1474 loss=2.158, trans_loss=3.414, nll_loss=1.604, w2v_ctc_loss=1.13, task_loss=0, contrastive_loss=0.258, total=4166.92, n_correct=2584.59, ppl=3.04, accuracy=62.026, wps=15573.9, ups=1.25, wpb=12446.8, bsz=471.5, num_updates=11200, lr=0.000133631, gnorm=0.43, clip=0, loss_scale=8, train_wall=79, gb_free=14.5, wall=9310
2023-08-12 04:42:52 | INFO | train_inner | epoch 008:    990 / 1474 loss=2.131, trans_loss=3.415, nll_loss=1.603, w2v_ctc_loss=1.132, task_loss=0, contrastive_loss=0.158, total=4150.39, n_correct=2579.65, ppl=3.04, accuracy=62.154, wps=15579.4, ups=1.26, wpb=12390.2, bsz=462.8, num_updates=11300, lr=0.000133038, gnorm=0.419, clip=0, loss_scale=8, train_wall=79, gb_free=17.2, wall=9389
2023-08-12 04:44:12 | INFO | train_inner | epoch 008:   1090 / 1474 loss=2.178, trans_loss=3.417, nll_loss=1.606, w2v_ctc_loss=1.13, task_loss=0, contrastive_loss=0.381, total=4197.39, n_correct=2594.97, ppl=3.04, accuracy=61.823, wps=15646.9, ups=1.25, wpb=12529.5, bsz=466.4, num_updates=11400, lr=0.000132453, gnorm=0.425, clip=0, loss_scale=8, train_wall=80, gb_free=16.7, wall=9470
2023-08-12 04:45:32 | INFO | train_inner | epoch 008:   1190 / 1474 loss=2.136, trans_loss=3.408, nll_loss=1.596, w2v_ctc_loss=1.135, task_loss=0, contrastive_loss=0.169, total=4180.55, n_correct=2595.5, ppl=3.02, accuracy=62.085, wps=15612.2, ups=1.25, wpb=12487.2, bsz=472.7, num_updates=11500, lr=0.000131876, gnorm=0.424, clip=0, loss_scale=16, train_wall=79, gb_free=17.2, wall=9550
2023-08-12 04:46:52 | INFO | train_inner | epoch 008:   1290 / 1474 loss=2.145, trans_loss=3.415, nll_loss=1.604, w2v_ctc_loss=1.144, task_loss=0, contrastive_loss=0.191, total=4062.6, n_correct=2512.37, ppl=3.04, accuracy=61.841, wps=15260.2, ups=1.26, wpb=12135.3, bsz=437.8, num_updates=11600, lr=0.000131306, gnorm=0.429, clip=0, loss_scale=16, train_wall=79, gb_free=13.1, wall=9629
2023-08-12 04:48:11 | INFO | train_inner | epoch 008:   1390 / 1474 loss=2.158, trans_loss=3.415, nll_loss=1.605, w2v_ctc_loss=1.132, task_loss=0, contrastive_loss=0.257, total=4159.11, n_correct=2583.64, ppl=3.04, accuracy=62.12, wps=15590.1, ups=1.26, wpb=12419, bsz=468.7, num_updates=11700, lr=0.000130744, gnorm=0.422, clip=0, loss_scale=16, train_wall=79, gb_free=13.3, wall=9709
2023-08-12 04:49:18 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 04:49:41 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 4.183 | trans_loss 5.254 | nll_loss 2.543 | w2v_ctc_loss 1.401 | task_loss 0 | contrastive_loss 0.324 | total 4003.4 | n_correct 2597.8 | ppl 5.83 | accuracy 64.89 | uer 19.43 | wer 21.136 | raw_wer 21.136 | bleu 20.54 | wps 2254.1 | wpb 4003.4 | bsz 141.8 | num_updates 11784 | best_bleu 20.54
2023-08-12 04:49:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11784 updates
2023-08-12 04:49:41 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 04:49:56 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 04:50:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt (epoch 8 @ 11784 updates, score 20.54) (writing took 29.172030126675963 seconds)
2023-08-12 04:50:11 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-08-12 04:50:11 | INFO | train | epoch 008 | loss 2.158 | trans_loss 3.418 | nll_loss 1.607 | w2v_ctc_loss 1.139 | task_loss 0 | contrastive_loss 0.238 | total 4138.65 | n_correct 2559.84 | ppl 3.05 | accuracy 61.852 | wps 14671.7 | ups 1.19 | wpb 12355.8 | bsz 458.5 | num_updates 11784 | lr 0.000130277 | gnorm 0.427 | clip 0 | loss_scale 16 | train_wall 1172 | gb_free 16.9 | wall 9828
2023-08-12 04:50:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 04:50:11 | INFO | fairseq.trainer | begin training epoch 9
2023-08-12 04:50:11 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 04:50:33 | INFO | train_inner | epoch 009:     16 / 1474 loss=2.154, trans_loss=3.41, nll_loss=1.596, w2v_ctc_loss=1.116, task_loss=0, contrastive_loss=0.346, total=4121.25, n_correct=2562.78, ppl=3.02, accuracy=62.185, wps=8674.8, ups=0.71, wpb=12298.5, bsz=466, num_updates=11800, lr=0.000130189, gnorm=0.427, clip=0, loss_scale=16, train_wall=80, gb_free=17.6, wall=9850
2023-08-12 04:51:53 | INFO | train_inner | epoch 009:    116 / 1474 loss=2.095, trans_loss=3.383, nll_loss=1.563, w2v_ctc_loss=1.091, task_loss=0, contrastive_loss=0.186, total=4191.82, n_correct=2641.08, ppl=2.96, accuracy=63.006, wps=15616, ups=1.25, wpb=12518.2, bsz=479.9, num_updates=11900, lr=0.000129641, gnorm=0.417, clip=0, loss_scale=16, train_wall=80, gb_free=15.9, wall=9931
2023-08-12 04:53:14 | INFO | train_inner | epoch 009:    216 / 1474 loss=2.084, trans_loss=3.388, nll_loss=1.568, w2v_ctc_loss=1.092, task_loss=0, contrastive_loss=0.142, total=4061.27, n_correct=2553.66, ppl=2.96, accuracy=62.878, wps=15079.5, ups=1.24, wpb=12126.5, bsz=431.4, num_updates=12000, lr=0.000129099, gnorm=0.42, clip=0, loss_scale=16, train_wall=80, gb_free=17.4, wall=10011
2023-08-12 04:53:14 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 04:53:36 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.174 | trans_loss 5.259 | nll_loss 2.545 | w2v_ctc_loss 1.358 | task_loss 0 | contrastive_loss 0.334 | total 4003.4 | n_correct 2596.6 | ppl 5.84 | accuracy 64.86 | uer 19.16 | wer 21.077 | raw_wer 21.077 | bleu 20.79 | wps 2241.4 | wpb 4003.4 | bsz 141.8 | num_updates 12000 | best_bleu 20.79
2023-08-12 04:53:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12000 updates
2023-08-12 04:53:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_9_12000.pt
2023-08-12 04:53:40 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_9_12000.pt
2023-08-12 04:54:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_9_12000.pt (epoch 9 @ 12000 updates, score 20.79) (writing took 48.96650142595172 seconds)
2023-08-12 04:55:46 | INFO | train_inner | epoch 009:    316 / 1474 loss=2.087, trans_loss=3.378, nll_loss=1.558, w2v_ctc_loss=1.08, task_loss=0, contrastive_loss=0.193, total=4146.43, n_correct=2617.18, ppl=2.94, accuracy=63.119, wps=8142.5, ups=0.66, wpb=12389.9, bsz=474.6, num_updates=12100, lr=0.000128565, gnorm=0.42, clip=0, loss_scale=16, train_wall=79, gb_free=16.2, wall=10163
2023-08-12 04:57:06 | INFO | train_inner | epoch 009:    416 / 1474 loss=2.09, trans_loss=3.393, nll_loss=1.575, w2v_ctc_loss=1.091, task_loss=0, contrastive_loss=0.158, total=4194.84, n_correct=2627.03, ppl=2.98, accuracy=62.625, wps=15597.9, ups=1.25, wpb=12524.6, bsz=466.7, num_updates=12200, lr=0.000128037, gnorm=0.413, clip=0, loss_scale=16, train_wall=80, gb_free=16.1, wall=10244
2023-08-12 04:58:27 | INFO | train_inner | epoch 009:    516 / 1474 loss=2.119, trans_loss=3.394, nll_loss=1.575, w2v_ctc_loss=1.115, task_loss=0, contrastive_loss=0.21, total=4124.3, n_correct=2584.16, ppl=2.98, accuracy=62.657, wps=15251.3, ups=1.24, wpb=12310.3, bsz=439.5, num_updates=12300, lr=0.000127515, gnorm=0.419, clip=0, loss_scale=16, train_wall=80, gb_free=11.8, wall=10324
2023-08-12 04:59:47 | INFO | train_inner | epoch 009:    616 / 1474 loss=2.081, trans_loss=3.384, nll_loss=1.567, w2v_ctc_loss=1.085, task_loss=0, contrastive_loss=0.168, total=4120.96, n_correct=2587.57, ppl=2.96, accuracy=62.79, wps=15414.5, ups=1.25, wpb=12316.4, bsz=453.4, num_updates=12400, lr=0.000127, gnorm=0.412, clip=0, loss_scale=16, train_wall=79, gb_free=16.1, wall=10404
2023-08-12 05:01:06 | INFO | train_inner | epoch 009:    716 / 1474 loss=2.121, trans_loss=3.393, nll_loss=1.577, w2v_ctc_loss=1.109, task_loss=0, contrastive_loss=0.249, total=4088.53, n_correct=2559.44, ppl=2.98, accuracy=62.6, wps=15327.6, ups=1.25, wpb=12213.7, bsz=451.4, num_updates=12500, lr=0.000126491, gnorm=0.426, clip=0, loss_scale=16, train_wall=79, gb_free=16.9, wall=10484
2023-08-12 05:02:28 | INFO | train_inner | epoch 009:    816 / 1474 loss=2.157, trans_loss=3.386, nll_loss=1.569, w2v_ctc_loss=1.1, task_loss=0, contrastive_loss=0.387, total=4220.43, n_correct=2651.57, ppl=2.97, accuracy=62.827, wps=15508.8, ups=1.23, wpb=12611.1, bsz=501.1, num_updates=12600, lr=0.000125988, gnorm=0.425, clip=0, loss_scale=16, train_wall=81, gb_free=14.3, wall=10565
2023-08-12 05:03:49 | INFO | train_inner | epoch 009:    916 / 1474 loss=2.132, trans_loss=3.391, nll_loss=1.571, w2v_ctc_loss=1.098, task_loss=0, contrastive_loss=0.373, total=4146.05, n_correct=2599.4, ppl=2.97, accuracy=62.696, wps=15192.4, ups=1.23, wpb=12371.5, bsz=450.3, num_updates=12700, lr=0.000125491, gnorm=0.42, clip=0, loss_scale=16, train_wall=81, gb_free=17.7, wall=10647
2023-08-12 05:05:09 | INFO | train_inner | epoch 009:   1016 / 1474 loss=2.093, trans_loss=3.397, nll_loss=1.579, w2v_ctc_loss=1.104, task_loss=0, contrastive_loss=0.157, total=4101.48, n_correct=2570.08, ppl=2.99, accuracy=62.662, wps=15366.6, ups=1.26, wpb=12241.7, bsz=424.4, num_updates=12800, lr=0.000125, gnorm=0.419, clip=0, loss_scale=16, train_wall=79, gb_free=15.9, wall=10726
2023-08-12 05:06:28 | INFO | train_inner | epoch 009:   1116 / 1474 loss=2.096, trans_loss=3.394, nll_loss=1.573, w2v_ctc_loss=1.095, task_loss=0, contrastive_loss=0.177, total=4179.09, n_correct=2627.83, ppl=2.98, accuracy=62.88, wps=15681.3, ups=1.26, wpb=12457.7, bsz=474.7, num_updates=12900, lr=0.000124515, gnorm=0.422, clip=0, loss_scale=16, train_wall=79, gb_free=15.2, wall=10806
2023-08-12 05:07:49 | INFO | train_inner | epoch 009:   1216 / 1474 loss=2.098, trans_loss=3.391, nll_loss=1.574, w2v_ctc_loss=1.106, task_loss=0, contrastive_loss=0.162, total=4140.66, n_correct=2596.99, ppl=2.98, accuracy=62.719, wps=15254, ups=1.23, wpb=12363.4, bsz=448.1, num_updates=13000, lr=0.000124035, gnorm=0.422, clip=0, loss_scale=16, train_wall=80, gb_free=17, wall=10887
2023-08-12 05:09:10 | INFO | train_inner | epoch 009:   1316 / 1474 loss=2.129, trans_loss=3.387, nll_loss=1.566, w2v_ctc_loss=1.087, task_loss=0, contrastive_loss=0.352, total=4204.43, n_correct=2648.21, ppl=2.96, accuracy=62.986, wps=15510, ups=1.24, wpb=12544.9, bsz=492.5, num_updates=13100, lr=0.00012356, gnorm=0.418, clip=0, loss_scale=16, train_wall=80, gb_free=17.6, wall=10968
2023-08-12 05:10:29 | INFO | train_inner | epoch 009:   1416 / 1474 loss=2.089, trans_loss=3.398, nll_loss=1.58, w2v_ctc_loss=1.103, task_loss=0, contrastive_loss=0.139, total=4069.19, n_correct=2550.84, ppl=2.99, accuracy=62.687, wps=15334.9, ups=1.26, wpb=12143.2, bsz=427.7, num_updates=13200, lr=0.000123091, gnorm=0.423, clip=0, loss_scale=16, train_wall=79, gb_free=16.5, wall=11047
2023-08-12 05:11:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 05:11:37 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.148 | trans_loss 5.231 | nll_loss 2.516 | w2v_ctc_loss 1.351 | task_loss 0 | contrastive_loss 0.321 | total 4003.4 | n_correct 2617.3 | ppl 5.72 | accuracy 65.377 | uer 19.109 | wer 21.017 | raw_wer 21.017 | bleu 20.99 | wps 2421.8 | wpb 4003.4 | bsz 141.8 | num_updates 13258 | best_bleu 20.99
2023-08-12 05:11:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 13258 updates
2023-08-12 05:11:37 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 05:11:51 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 05:12:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt (epoch 9 @ 13258 updates, score 20.99) (writing took 27.03477424941957 seconds)
2023-08-12 05:12:04 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-08-12 05:12:04 | INFO | train | epoch 009 | loss 2.106 | trans_loss 3.39 | nll_loss 1.571 | w2v_ctc_loss 1.097 | task_loss 0 | contrastive_loss 0.224 | total 4138.65 | n_correct 2599.23 | ppl 2.97 | accuracy 62.804 | wps 13865.4 | ups 1.12 | wpb 12355.8 | bsz 458.5 | num_updates 13258 | lr 0.000122822 | gnorm 0.42 | clip 0 | loss_scale 16 | train_wall 1174 | gb_free 11.7 | wall 11142
2023-08-12 05:12:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 05:12:05 | INFO | fairseq.trainer | begin training epoch 10
2023-08-12 05:12:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 05:12:46 | INFO | train_inner | epoch 010:     42 / 1474 loss=2.09, trans_loss=3.38, nll_loss=1.558, w2v_ctc_loss=1.077, task_loss=0, contrastive_loss=0.234, total=4100.8, n_correct=2594.24, ppl=2.95, accuracy=63.262, wps=8953.4, ups=0.73, wpb=12238.2, bsz=469.4, num_updates=13300, lr=0.000122628, gnorm=0.416, clip=0, loss_scale=16, train_wall=78, gb_free=16.2, wall=11184
2023-08-12 05:14:06 | INFO | train_inner | epoch 010:    142 / 1474 loss=2.042, trans_loss=3.363, nll_loss=1.537, w2v_ctc_loss=1.048, task_loss=0, contrastive_loss=0.157, total=4247.35, n_correct=2709.96, ppl=2.9, accuracy=63.804, wps=15818, ups=1.25, wpb=12684.5, bsz=479.6, num_updates=13400, lr=0.000122169, gnorm=0.409, clip=0, loss_scale=16, train_wall=80, gb_free=11.9, wall=11264
2023-08-12 05:15:27 | INFO | train_inner | epoch 010:    242 / 1474 loss=2.075, trans_loss=3.362, nll_loss=1.534, w2v_ctc_loss=1.059, task_loss=0, contrastive_loss=0.279, total=4122.82, n_correct=2625.71, ppl=2.9, accuracy=63.687, wps=15333, ups=1.25, wpb=12303.3, bsz=461.4, num_updates=13500, lr=0.000121716, gnorm=0.416, clip=0, loss_scale=16, train_wall=80, gb_free=16.2, wall=11344
2023-08-12 05:16:46 | INFO | train_inner | epoch 010:    342 / 1474 loss=2.05, trans_loss=3.361, nll_loss=1.538, w2v_ctc_loss=1.052, task_loss=0, contrastive_loss=0.191, total=4138.27, n_correct=2635.7, ppl=2.9, accuracy=63.691, wps=15519.7, ups=1.25, wpb=12371, bsz=453.8, num_updates=13600, lr=0.000121268, gnorm=0.417, clip=0, loss_scale=32, train_wall=79, gb_free=16.3, wall=11424
2023-08-12 05:18:07 | INFO | train_inner | epoch 010:    442 / 1474 loss=2.078, trans_loss=3.366, nll_loss=1.541, w2v_ctc_loss=1.039, task_loss=0, contrastive_loss=0.36, total=4196.37, n_correct=2668.86, ppl=2.91, accuracy=63.599, wps=15586.9, ups=1.24, wpb=12528, bsz=481.1, num_updates=13700, lr=0.000120824, gnorm=0.414, clip=0, loss_scale=32, train_wall=80, gb_free=16, wall=11504
2023-08-12 05:19:27 | INFO | train_inner | epoch 010:    542 / 1474 loss=2.062, trans_loss=3.377, nll_loss=1.552, w2v_ctc_loss=1.076, task_loss=0, contrastive_loss=0.146, total=4102.8, n_correct=2598.37, ppl=2.93, accuracy=63.332, wps=15269.6, ups=1.25, wpb=12234.1, bsz=437.8, num_updates=13800, lr=0.000120386, gnorm=0.418, clip=0, loss_scale=32, train_wall=80, gb_free=16.9, wall=11584
2023-08-12 05:20:47 | INFO | train_inner | epoch 010:    642 / 1474 loss=2.083, trans_loss=3.372, nll_loss=1.547, w2v_ctc_loss=1.065, task_loss=0, contrastive_loss=0.26, total=4176.56, n_correct=2650.92, ppl=2.92, accuracy=63.471, wps=15479.3, ups=1.24, wpb=12464, bsz=477.2, num_updates=13900, lr=0.000119952, gnorm=0.421, clip=0, loss_scale=32, train_wall=80, gb_free=16.2, wall=11665
2023-08-12 05:22:06 | INFO | train_inner | epoch 010:    742 / 1474 loss=2.064, trans_loss=3.37, nll_loss=1.546, w2v_ctc_loss=1.082, task_loss=0, contrastive_loss=0.143, total=4125.87, n_correct=2617.21, ppl=2.92, accuracy=63.434, wps=15602.9, ups=1.27, wpb=12315.3, bsz=454.4, num_updates=14000, lr=0.000119523, gnorm=0.423, clip=0, loss_scale=32, train_wall=78, gb_free=14.5, wall=11744
2023-08-12 05:22:06 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 05:22:28 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.156 | trans_loss 5.227 | nll_loss 2.505 | w2v_ctc_loss 1.384 | task_loss 0 | contrastive_loss 0.325 | total 4003.4 | n_correct 2614.9 | ppl 5.68 | accuracy 65.317 | uer 19.253 | wer 21.095 | raw_wer 21.095 | bleu 20.83 | wps 2335.8 | wpb 4003.4 | bsz 141.8 | num_updates 14000 | best_bleu 20.99
2023-08-12 05:22:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14000 updates
2023-08-12 05:22:28 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_10_14000.pt
2023-08-12 05:22:32 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_10_14000.pt
2023-08-12 05:23:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_10_14000.pt (epoch 10 @ 14000 updates, score 20.83) (writing took 39.307083170861006 seconds)
2023-08-12 05:24:28 | INFO | train_inner | epoch 010:    842 / 1474 loss=2.039, trans_loss=3.367, nll_loss=1.542, w2v_ctc_loss=1.052, task_loss=0, contrastive_loss=0.146, total=4128.44, n_correct=2626.37, ppl=2.91, accuracy=63.617, wps=8681, ups=0.7, wpb=12327.8, bsz=456.3, num_updates=14100, lr=0.000119098, gnorm=0.414, clip=0, loss_scale=32, train_wall=79, gb_free=14.7, wall=11886
2023-08-12 05:25:48 | INFO | train_inner | epoch 010:    942 / 1474 loss=2.061, trans_loss=3.366, nll_loss=1.539, w2v_ctc_loss=1.063, task_loss=0, contrastive_loss=0.184, total=4160.94, n_correct=2647.66, ppl=2.91, accuracy=63.631, wps=15594.5, ups=1.26, wpb=12411.1, bsz=468.1, num_updates=14200, lr=0.000118678, gnorm=0.417, clip=0, loss_scale=32, train_wall=79, gb_free=15.4, wall=11965
2023-08-12 05:27:08 | INFO | train_inner | epoch 010:   1042 / 1474 loss=2.054, trans_loss=3.367, nll_loss=1.542, w2v_ctc_loss=1.065, task_loss=0, contrastive_loss=0.159, total=4067.53, n_correct=2581.67, ppl=2.91, accuracy=63.47, wps=15201.9, ups=1.25, wpb=12145, bsz=434.3, num_updates=14300, lr=0.000118262, gnorm=0.424, clip=0, loss_scale=32, train_wall=79, gb_free=16.8, wall=12045
2023-08-12 05:28:27 | INFO | train_inner | epoch 010:   1142 / 1474 loss=2.06, trans_loss=3.375, nll_loss=1.553, w2v_ctc_loss=1.079, task_loss=0, contrastive_loss=0.141, total=4044.03, n_correct=2555.31, ppl=2.93, accuracy=63.187, wps=15332.5, ups=1.27, wpb=12074.4, bsz=422.3, num_updates=14400, lr=0.000117851, gnorm=0.42, clip=0, loss_scale=32, train_wall=78, gb_free=17.1, wall=12124
2023-08-12 05:29:46 | INFO | train_inner | epoch 010:   1242 / 1474 loss=2.046, trans_loss=3.36, nll_loss=1.538, w2v_ctc_loss=1.069, task_loss=0, contrastive_loss=0.135, total=4110.41, n_correct=2612.55, ppl=2.9, accuracy=63.559, wps=15500.8, ups=1.26, wpb=12291.6, bsz=445.2, num_updates=14500, lr=0.000117444, gnorm=0.414, clip=0, loss_scale=32, train_wall=79, gb_free=16.4, wall=12203
2023-08-12 05:31:06 | INFO | train_inner | epoch 010:   1342 / 1474 loss=2.048, trans_loss=3.367, nll_loss=1.544, w2v_ctc_loss=1.063, task_loss=0, contrastive_loss=0.147, total=4121.38, n_correct=2623.2, ppl=2.92, accuracy=63.649, wps=15317.3, ups=1.24, wpb=12308.4, bsz=451, num_updates=14600, lr=0.000117041, gnorm=0.414, clip=0, loss_scale=32, train_wall=80, gb_free=14.1, wall=12284
2023-08-12 05:32:27 | INFO | train_inner | epoch 010:   1442 / 1474 loss=2.116, trans_loss=3.377, nll_loss=1.553, w2v_ctc_loss=1.055, task_loss=0, contrastive_loss=0.401, total=4192.39, n_correct=2653.92, ppl=2.93, accuracy=63.303, wps=15551, ups=1.24, wpb=12506.1, bsz=482, num_updates=14700, lr=0.000116642, gnorm=0.428, clip=0, loss_scale=32, train_wall=80, gb_free=16.9, wall=12364
2023-08-12 05:32:52 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 05:33:14 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.135 | trans_loss 5.211 | nll_loss 2.488 | w2v_ctc_loss 1.361 | task_loss 0 | contrastive_loss 0.316 | total 4003.4 | n_correct 2626.8 | ppl 5.61 | accuracy 65.614 | uer 18.578 | wer 20.346 | raw_wer 20.346 | bleu 21.53 | wps 2347.7 | wpb 4003.4 | bsz 141.8 | num_updates 14732 | best_bleu 21.53
2023-08-12 05:33:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14732 updates
2023-08-12 05:33:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 05:33:31 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 05:33:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt (epoch 10 @ 14732 updates, score 21.53) (writing took 30.51617313735187 seconds)
2023-08-12 05:33:45 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-08-12 05:33:45 | INFO | train | epoch 010 | loss 2.064 | trans_loss 3.368 | nll_loss 1.543 | w2v_ctc_loss 1.06 | task_loss 0 | contrastive_loss 0.214 | total 4138.65 | n_correct 2629.91 | ppl 2.91 | accuracy 63.545 | wps 14001.7 | ups 1.13 | wpb 12355.8 | bsz 458.5 | num_updates 14732 | lr 0.000116516 | gnorm 0.417 | clip 0 | loss_scale 32 | train_wall 1168 | gb_free 17.1 | wall 12443
2023-08-12 05:33:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 05:33:45 | INFO | fairseq.trainer | begin training epoch 11
2023-08-12 05:33:45 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 05:34:46 | INFO | train_inner | epoch 011:     68 / 1474 loss=2.033, trans_loss=3.347, nll_loss=1.516, w2v_ctc_loss=1.031, task_loss=0, contrastive_loss=0.219, total=4175.24, n_correct=2683.96, ppl=2.86, accuracy=64.283, wps=8921.2, ups=0.72, wpb=12463.5, bsz=478.8, num_updates=14800, lr=0.000116248, gnorm=0.405, clip=0, loss_scale=32, train_wall=78, gb_free=16.7, wall=12504
2023-08-12 05:36:06 | INFO | train_inner | epoch 011:    168 / 1474 loss=2.015, trans_loss=3.348, nll_loss=1.519, w2v_ctc_loss=1.033, task_loss=0, contrastive_loss=0.14, total=4087.78, n_correct=2624.49, ppl=2.87, accuracy=64.203, wps=15255.3, ups=1.25, wpb=12214.2, bsz=445.9, num_updates=14900, lr=0.000115857, gnorm=0.414, clip=0, loss_scale=32, train_wall=80, gb_free=16.4, wall=12584
2023-08-12 05:37:26 | INFO | train_inner | epoch 011:    268 / 1474 loss=2.003, trans_loss=3.344, nll_loss=1.513, w2v_ctc_loss=1.023, task_loss=0, contrastive_loss=0.134, total=4118.77, n_correct=2649.33, ppl=2.85, accuracy=64.323, wps=15441.9, ups=1.26, wpb=12299.1, bsz=446.5, num_updates=15000, lr=0.00011547, gnorm=0.407, clip=0, loss_scale=32, train_wall=79, gb_free=12.5, wall=12663
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:0')
2023-08-12 05:38:23 | INFO | train_inner | epoch 011:    368 / 1474 loss=2.122, trans_loss=4.971, nll_loss=2.253, w2v_ctc_loss=0.779, task_loss=0, contrastive_loss=0.107, total=4097.83, n_correct=2629.61, ppl=4.77, accuracy=64.171, wps=14366.4, ups=1.74, wpb=8240.8, bsz=298, num_updates=15100, lr=0.000115087, gnorm=0.551, clip=0, loss_scale=32, train_wall=57, gb_free=15.6, wall=12721
2023-08-12 05:39:21 | INFO | train_inner | epoch 011:    468 / 1474 loss=2.141, trans_loss=5.006, nll_loss=2.276, w2v_ctc_loss=0.774, task_loss=0, contrastive_loss=0.224, total=4110.64, n_correct=2629.37, ppl=4.84, accuracy=63.965, wps=14320.3, ups=1.74, wpb=8221.3, bsz=300.4, num_updates=15200, lr=0.000114708, gnorm=0.558, clip=0, loss_scale=32, train_wall=57, gb_free=16.4, wall=12778
2023-08-12 05:40:19 | INFO | train_inner | epoch 011:    568 / 1474 loss=2.144, trans_loss=5.007, nll_loss=2.277, w2v_ctc_loss=0.785, task_loss=0, contrastive_loss=0.224, total=4071.69, n_correct=2598.83, ppl=4.85, accuracy=63.827, wps=14060.7, ups=1.73, wpb=8143.4, bsz=293.7, num_updates=15300, lr=0.000114332, gnorm=0.553, clip=0, loss_scale=32, train_wall=57, gb_free=16, wall=12836
2023-08-12 05:41:17 | INFO | train_inner | epoch 011:    668 / 1474 loss=2.146, trans_loss=4.997, nll_loss=2.264, w2v_ctc_loss=0.781, task_loss=0, contrastive_loss=0.277, total=4157.2, n_correct=2663.63, ppl=4.8, accuracy=64.073, wps=14309.9, ups=1.72, wpb=8314.4, bsz=309.6, num_updates=15400, lr=0.000113961, gnorm=0.55, clip=0, loss_scale=32, train_wall=58, gb_free=16.7, wall=12894
2023-08-12 05:42:14 | INFO | train_inner | epoch 011:    768 / 1474 loss=2.138, trans_loss=5.012, nll_loss=2.284, w2v_ctc_loss=0.796, task_loss=0, contrastive_loss=0.106, total=4174.91, n_correct=2671.12, ppl=4.87, accuracy=63.98, wps=14579.1, ups=1.75, wpb=8349.8, bsz=306.9, num_updates=15500, lr=0.000113592, gnorm=0.553, clip=0, loss_scale=32, train_wall=57, gb_free=17.1, wall=12952
2023-08-12 05:43:11 | INFO | train_inner | epoch 011:    868 / 1474 loss=2.131, trans_loss=5.008, nll_loss=2.279, w2v_ctc_loss=0.788, task_loss=0, contrastive_loss=0.096, total=4118.44, n_correct=2631.22, ppl=4.85, accuracy=63.889, wps=14462.1, ups=1.76, wpb=8236.9, bsz=293.7, num_updates=15600, lr=0.000113228, gnorm=0.556, clip=0, loss_scale=64, train_wall=56, gb_free=11.2, wall=13008
2023-08-12 05:44:09 | INFO | train_inner | epoch 011:    968 / 1474 loss=2.133, trans_loss=5.008, nll_loss=2.278, w2v_ctc_loss=0.792, task_loss=0, contrastive_loss=0.107, total=4140.92, n_correct=2647.63, ppl=4.85, accuracy=63.938, wps=14358.7, ups=1.73, wpb=8281.8, bsz=301.9, num_updates=15700, lr=0.000112867, gnorm=0.553, clip=0, loss_scale=64, train_wall=57, gb_free=15.7, wall=13066
2023-08-12 05:45:06 | INFO | train_inner | epoch 011:   1068 / 1474 loss=2.131, trans_loss=5, nll_loss=2.269, w2v_ctc_loss=0.788, task_loss=0, contrastive_loss=0.125, total=4136.99, n_correct=2655.4, ppl=4.82, accuracy=64.187, wps=14366.7, ups=1.74, wpb=8274, bsz=308.8, num_updates=15800, lr=0.000112509, gnorm=0.551, clip=0, loss_scale=64, train_wall=57, gb_free=17.7, wall=13124
2023-08-12 05:46:03 | INFO | train_inner | epoch 011:   1168 / 1474 loss=2.133, trans_loss=5.008, nll_loss=2.279, w2v_ctc_loss=0.79, task_loss=0, contrastive_loss=0.111, total=4185.65, n_correct=2676.3, ppl=4.85, accuracy=63.94, wps=14679.3, ups=1.75, wpb=8371.3, bsz=309.8, num_updates=15900, lr=0.000112154, gnorm=0.551, clip=0, loss_scale=64, train_wall=56, gb_free=14.2, wall=13181
2023-08-12 05:46:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-12 05:47:01 | INFO | train_inner | epoch 011:   1269 / 1474 loss=2.14, trans_loss=5.003, nll_loss=2.273, w2v_ctc_loss=0.795, task_loss=0, contrastive_loss=0.157, total=4149.99, n_correct=2654.14, ppl=4.83, accuracy=63.955, wps=14376.4, ups=1.73, wpb=8300, bsz=308.6, num_updates=16000, lr=0.000111803, gnorm=0.554, clip=0, loss_scale=32, train_wall=57, gb_free=17.5, wall=13239
2023-08-12 05:47:01 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:7')
2023-08-12 05:47:23 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.148 | trans_loss 5.213 | nll_loss 2.491 | w2v_ctc_loss 1.393 | task_loss 0 | contrastive_loss 0.324 | total 4003.4 | n_correct 2627.3 | ppl 5.62 | accuracy 65.627 | uer 18.655 | wer 20.368 | raw_wer 20.368 | bleu 21.26 | wps 2328.8 | wpb 4003.4 | bsz 141.8 | num_updates 16000 | best_bleu 21.53
2023-08-12 05:47:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16000 updates
2023-08-12 05:47:24 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_11_16000.pt
2023-08-12 05:47:27 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_11_16000.pt
2023-08-12 05:48:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_11_16000.pt (epoch 11 @ 16000 updates, score 21.26) (writing took 37.99671909958124 seconds)
2023-08-12 05:49:00 | INFO | train_inner | epoch 011:   1369 / 1474 loss=2.15, trans_loss=4.999, nll_loss=2.269, w2v_ctc_loss=0.774, task_loss=0, contrastive_loss=0.339, total=4191.56, n_correct=2685.69, ppl=4.82, accuracy=64.074, wps=7051.4, ups=0.84, wpb=8383.1, bsz=327.7, num_updates=16100, lr=0.000111456, gnorm=0.549, clip=0, loss_scale=32, train_wall=57, gb_free=17.5, wall=13357
2023-08-12 05:49:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-12 05:49:58 | INFO | train_inner | epoch 011:   1470 / 1474 loss=2.131, trans_loss=5.006, nll_loss=2.277, w2v_ctc_loss=0.787, task_loss=0, contrastive_loss=0.114, total=4152.69, n_correct=2658.47, ppl=4.85, accuracy=64.018, wps=14247, ups=1.72, wpb=8305.4, bsz=310.6, num_updates=16200, lr=0.000111111, gnorm=0.552, clip=0, loss_scale=16, train_wall=58, gb_free=15, wall=13416
2023-08-12 05:50:01 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 05:50:23 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.126 | trans_loss 5.212 | nll_loss 2.488 | w2v_ctc_loss 1.328 | task_loss 0 | contrastive_loss 0.317 | total 4003.4 | n_correct 2625.1 | ppl 5.61 | accuracy 65.572 | uer 18.459 | wer 20.383 | raw_wer 20.383 | bleu 21.18 | wps 2330.9 | wpb 4003.4 | bsz 141.8 | num_updates 16204 | best_bleu 21.53
2023-08-12 05:50:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16204 updates
2023-08-12 05:50:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_21.1803.pt
2023-08-12 05:50:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_21.1803.pt
2023-08-12 05:50:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_21.1803.pt (epoch 11 @ 16204 updates, score 21.18) (writing took 21.147558771073818 seconds)
2023-08-12 05:50:44 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-08-12 05:50:44 | INFO | train | epoch 011 | loss 2.105 | trans_loss 4.59 | nll_loss 2.084 | w2v_ctc_loss 0.847 | task_loss 0 | contrastive_loss 0.159 | total 4136.94 | n_correct 2649.91 | ppl 4.24 | accuracy 64.055 | wps 13024.3 | ups 1.44 | wpb 9018.6 | bsz 333 | num_updates 16204 | lr 0.000111097 | gnorm 0.527 | clip 0 | loss_scale 16 | train_wall 899 | gb_free 17.2 | wall 13462
2023-08-12 05:50:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 05:50:45 | INFO | fairseq.trainer | begin training epoch 12
2023-08-12 05:50:45 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 05:51:47 | INFO | train_inner | epoch 012:     96 / 1474 loss=2.112, trans_loss=4.966, nll_loss=2.224, w2v_ctc_loss=0.767, task_loss=0, contrastive_loss=0.147, total=4145.21, n_correct=2683.93, ppl=4.67, accuracy=64.748, wps=7600.4, ups=0.92, wpb=8290.4, bsz=313.9, num_updates=16300, lr=0.00011077, gnorm=0.547, clip=0, loss_scale=16, train_wall=57, gb_free=16.9, wall=13525
2023-08-12 05:52:44 | INFO | train_inner | epoch 012:    196 / 1474 loss=2.111, trans_loss=4.971, nll_loss=2.23, w2v_ctc_loss=0.774, task_loss=0, contrastive_loss=0.101, total=4124.1, n_correct=2665.16, ppl=4.69, accuracy=64.624, wps=14594.8, ups=1.77, wpb=8248.2, bsz=296.1, num_updates=16400, lr=0.000110432, gnorm=0.548, clip=0, loss_scale=16, train_wall=56, gb_free=15.5, wall=13581
2023-08-12 05:53:42 | INFO | train_inner | epoch 012:    296 / 1474 loss=2.113, trans_loss=4.973, nll_loss=2.235, w2v_ctc_loss=0.763, task_loss=0, contrastive_loss=0.13, total=4208.07, n_correct=2722.73, ppl=4.71, accuracy=64.703, wps=14611.3, ups=1.74, wpb=8416.1, bsz=321.7, num_updates=16500, lr=0.000110096, gnorm=0.547, clip=0, loss_scale=16, train_wall=57, gb_free=12.6, wall=13639
2023-08-12 05:54:39 | INFO | train_inner | epoch 012:    396 / 1474 loss=2.115, trans_loss=4.977, nll_loss=2.239, w2v_ctc_loss=0.775, task_loss=0, contrastive_loss=0.112, total=4144.42, n_correct=2675.38, ppl=4.72, accuracy=64.554, wps=14299.5, ups=1.73, wpb=8288.8, bsz=305.3, num_updates=16600, lr=0.000109764, gnorm=0.543, clip=0, loss_scale=16, train_wall=57, gb_free=16.4, wall=13697
2023-08-12 05:55:36 | INFO | train_inner | epoch 012:    496 / 1474 loss=2.121, trans_loss=4.986, nll_loss=2.252, w2v_ctc_loss=0.781, task_loss=0, contrastive_loss=0.12, total=4095.26, n_correct=2638.84, ppl=4.76, accuracy=64.436, wps=14381, ups=1.76, wpb=8190.5, bsz=299.8, num_updates=16700, lr=0.000109435, gnorm=0.548, clip=0, loss_scale=16, train_wall=56, gb_free=17.7, wall=13754
2023-08-12 05:56:34 | INFO | train_inner | epoch 012:    596 / 1474 loss=2.121, trans_loss=4.974, nll_loss=2.235, w2v_ctc_loss=0.773, task_loss=0, contrastive_loss=0.183, total=4204.6, n_correct=2715.87, ppl=4.71, accuracy=64.593, wps=14637.6, ups=1.74, wpb=8409.2, bsz=319.2, num_updates=16800, lr=0.000109109, gnorm=0.55, clip=0, loss_scale=16, train_wall=57, gb_free=16.5, wall=13811
2023-08-12 05:57:31 | INFO | train_inner | epoch 012:    696 / 1474 loss=2.12, trans_loss=4.971, nll_loss=2.231, w2v_ctc_loss=0.756, task_loss=0, contrastive_loss=0.27, total=4197.19, n_correct=2719.02, ppl=4.7, accuracy=64.782, wps=14709.6, ups=1.75, wpb=8394.4, bsz=323.2, num_updates=16900, lr=0.000108786, gnorm=0.541, clip=0, loss_scale=16, train_wall=56, gb_free=16.7, wall=13868
2023-08-12 05:58:29 | INFO | train_inner | epoch 012:    796 / 1474 loss=2.11, trans_loss=4.971, nll_loss=2.232, w2v_ctc_loss=0.772, task_loss=0, contrastive_loss=0.109, total=4094.06, n_correct=2647.26, ppl=4.7, accuracy=64.661, wps=14179.2, ups=1.73, wpb=8188.1, bsz=298.2, num_updates=17000, lr=0.000108465, gnorm=0.548, clip=0, loss_scale=16, train_wall=57, gb_free=14.9, wall=13926
2023-08-12 05:59:26 | INFO | train_inner | epoch 012:    896 / 1474 loss=2.12, trans_loss=4.976, nll_loss=2.238, w2v_ctc_loss=0.773, task_loss=0, contrastive_loss=0.159, total=4163.5, n_correct=2687.06, ppl=4.72, accuracy=64.538, wps=14590.6, ups=1.75, wpb=8327, bsz=305.1, num_updates=17100, lr=0.000108148, gnorm=0.553, clip=0, loss_scale=16, train_wall=57, gb_free=13, wall=13983
2023-08-12 06:00:23 | INFO | train_inner | epoch 012:    996 / 1474 loss=2.121, trans_loss=4.98, nll_loss=2.243, w2v_ctc_loss=0.778, task_loss=0, contrastive_loss=0.166, total=4124.99, n_correct=2662.99, ppl=4.74, accuracy=64.557, wps=14445.8, ups=1.75, wpb=8250, bsz=302.7, num_updates=17200, lr=0.000107833, gnorm=0.555, clip=0, loss_scale=16, train_wall=57, gb_free=11.5, wall=14040
2023-08-12 06:01:20 | INFO | train_inner | epoch 012:   1096 / 1474 loss=2.13, trans_loss=4.983, nll_loss=2.247, w2v_ctc_loss=0.781, task_loss=0, contrastive_loss=0.214, total=4046.6, n_correct=2609.08, ppl=4.75, accuracy=64.476, wps=14144, ups=1.75, wpb=8093.2, bsz=289.8, num_updates=17300, lr=0.000107521, gnorm=0.563, clip=0, loss_scale=16, train_wall=57, gb_free=16.5, wall=14098
2023-08-12 06:02:18 | INFO | train_inner | epoch 012:   1196 / 1474 loss=2.138, trans_loss=5.001, nll_loss=2.27, w2v_ctc_loss=0.79, task_loss=0, contrastive_loss=0.183, total=4196.85, n_correct=2690.39, ppl=4.82, accuracy=64.105, wps=14468.6, ups=1.72, wpb=8393.7, bsz=319, num_updates=17400, lr=0.000107211, gnorm=0.549, clip=0, loss_scale=16, train_wall=57, gb_free=16.5, wall=14156
2023-08-12 06:03:15 | INFO | train_inner | epoch 012:   1296 / 1474 loss=2.122, trans_loss=4.984, nll_loss=2.249, w2v_ctc_loss=0.794, task_loss=0, contrastive_loss=0.095, total=4067.78, n_correct=2618.33, ppl=4.75, accuracy=64.368, wps=14275.8, ups=1.75, wpb=8135.6, bsz=285.5, num_updates=17500, lr=0.000106904, gnorm=0.556, clip=0, loss_scale=16, train_wall=56, gb_free=15.3, wall=14213
2023-08-12 06:04:12 | INFO | train_inner | epoch 012:   1396 / 1474 loss=2.121, trans_loss=4.987, nll_loss=2.253, w2v_ctc_loss=0.766, task_loss=0, contrastive_loss=0.198, total=4142.88, n_correct=2670.25, ppl=4.77, accuracy=64.454, wps=14497.2, ups=1.75, wpb=8285.8, bsz=306.2, num_updates=17600, lr=0.0001066, gnorm=0.554, clip=0, loss_scale=16, train_wall=57, gb_free=15.7, wall=14270
2023-08-12 06:04:57 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 06:05:19 | INFO | dev_st | epoch 012 | valid on 'dev_st' subset | loss 4.125 | trans_loss 5.197 | nll_loss 2.469 | w2v_ctc_loss 1.369 | task_loss 0 | contrastive_loss 0.304 | total 4003.4 | n_correct 2637.9 | ppl 5.54 | accuracy 65.891 | uer 18.674 | wer 20.469 | raw_wer 20.469 | bleu 21.34 | wps 2379.2 | wpb 4003.4 | bsz 141.8 | num_updates 17678 | best_bleu 21.53
2023-08-12 06:05:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 17678 updates
2023-08-12 06:05:19 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_21.3406.pt
2023-08-12 06:05:23 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_21.3406.pt
2023-08-12 06:05:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_21.3406.pt (epoch 12 @ 17678 updates, score 21.34) (writing took 20.203229313716292 seconds)
2023-08-12 06:05:40 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-08-12 06:05:40 | INFO | train | epoch 012 | loss 2.119 | trans_loss 4.979 | nll_loss 2.242 | w2v_ctc_loss 0.775 | task_loss 0 | contrastive_loss 0.154 | total 4138.65 | n_correct 2671.15 | ppl 4.73 | accuracy 64.542 | wps 13627.3 | ups 1.65 | wpb 8277.3 | bsz 305.7 | num_updates 17678 | lr 0.000106365 | gnorm 0.55 | clip 0 | loss_scale 16 | train_wall 837 | gb_free 13.1 | wall 14357
2023-08-12 06:05:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 06:05:40 | INFO | fairseq.trainer | begin training epoch 13
2023-08-12 06:05:40 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 06:06:01 | INFO | train_inner | epoch 013:     22 / 1474 loss=2.116, trans_loss=4.984, nll_loss=2.247, w2v_ctc_loss=0.779, task_loss=0, contrastive_loss=0.104, total=4097.08, n_correct=2642.89, ppl=4.75, accuracy=64.507, wps=7568.3, ups=0.92, wpb=8194.2, bsz=296.6, num_updates=17700, lr=0.000106299, gnorm=0.553, clip=0, loss_scale=16, train_wall=57, gb_free=16.5, wall=14378
2023-08-12 06:06:58 | INFO | train_inner | epoch 013:    122 / 1474 loss=2.098, trans_loss=4.954, nll_loss=2.209, w2v_ctc_loss=0.757, task_loss=0, contrastive_loss=0.116, total=4164.24, n_correct=2708.93, ppl=4.62, accuracy=65.052, wps=14477.4, ups=1.74, wpb=8328.5, bsz=301.9, num_updates=17800, lr=0.000106, gnorm=0.542, clip=0, loss_scale=16, train_wall=57, gb_free=17, wall=14436
2023-08-12 06:07:56 | INFO | train_inner | epoch 013:    222 / 1474 loss=2.123, trans_loss=4.96, nll_loss=2.218, w2v_ctc_loss=0.756, task_loss=0, contrastive_loss=0.327, total=4201.52, n_correct=2728.09, ppl=4.65, accuracy=64.931, wps=14580.8, ups=1.74, wpb=8403, bsz=328.5, num_updates=17900, lr=0.000105703, gnorm=0.549, clip=0, loss_scale=16, train_wall=57, gb_free=13.3, wall=14493
2023-08-12 06:08:53 | INFO | train_inner | epoch 013:    322 / 1474 loss=2.09, trans_loss=4.946, nll_loss=2.198, w2v_ctc_loss=0.753, task_loss=0, contrastive_loss=0.099, total=4102.53, n_correct=2677.82, ppl=4.59, accuracy=65.272, wps=14387.2, ups=1.75, wpb=8205.1, bsz=293.9, num_updates=18000, lr=0.000105409, gnorm=0.555, clip=0, loss_scale=16, train_wall=56, gb_free=16.2, wall=14550
2023-08-12 06:08:53 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 06:09:15 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.116 | trans_loss 5.2 | nll_loss 2.473 | w2v_ctc_loss 1.327 | task_loss 0 | contrastive_loss 0.313 | total 4003.4 | n_correct 2634.4 | ppl 5.55 | accuracy 65.804 | uer 18.507 | wer 20.652 | raw_wer 20.652 | bleu 21.08 | wps 2403.3 | wpb 4003.4 | bsz 141.8 | num_updates 18000 | best_bleu 21.53
2023-08-12 06:09:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 18000 updates
2023-08-12 06:09:15 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_13_18000.pt
2023-08-12 06:09:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_13_18000.pt
2023-08-12 06:09:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_13_18000.pt (epoch 13 @ 18000 updates, score 21.08) (writing took 41.29079516977072 seconds)
2023-08-12 06:10:54 | INFO | train_inner | epoch 013:    422 / 1474 loss=2.101, trans_loss=4.95, nll_loss=2.204, w2v_ctc_loss=0.762, task_loss=0, contrastive_loss=0.145, total=4190.45, n_correct=2731.07, ppl=4.61, accuracy=65.174, wps=6895.1, ups=0.82, wpb=8380.9, bsz=320.3, num_updates=18100, lr=0.000105118, gnorm=0.541, clip=0, loss_scale=16, train_wall=57, gb_free=15.9, wall=14672
2023-08-12 06:11:52 | INFO | train_inner | epoch 013:    522 / 1474 loss=2.109, trans_loss=4.96, nll_loss=2.217, w2v_ctc_loss=0.762, task_loss=0, contrastive_loss=0.181, total=4194.45, n_correct=2722.02, ppl=4.65, accuracy=64.896, wps=14545.8, ups=1.73, wpb=8388.9, bsz=319, num_updates=18200, lr=0.000104828, gnorm=0.542, clip=0, loss_scale=32, train_wall=57, gb_free=17.4, wall=14729
2023-08-12 06:12:49 | INFO | train_inner | epoch 013:    622 / 1474 loss=2.093, trans_loss=4.955, nll_loss=2.211, w2v_ctc_loss=0.756, task_loss=0, contrastive_loss=0.095, total=4158.04, n_correct=2709.8, ppl=4.63, accuracy=65.17, wps=14559.1, ups=1.75, wpb=8316.1, bsz=306.7, num_updates=18300, lr=0.000104542, gnorm=0.544, clip=0, loss_scale=32, train_wall=57, gb_free=13.7, wall=14787
2023-08-12 06:13:46 | INFO | train_inner | epoch 013:    722 / 1474 loss=2.106, trans_loss=4.961, nll_loss=2.218, w2v_ctc_loss=0.782, task_loss=0, contrastive_loss=0.096, total=4099.91, n_correct=2659.15, ppl=4.65, accuracy=64.859, wps=14302.1, ups=1.74, wpb=8199.8, bsz=285.5, num_updates=18400, lr=0.000104257, gnorm=0.549, clip=0, loss_scale=32, train_wall=57, gb_free=16.7, wall=14844
2023-08-12 06:14:44 | INFO | train_inner | epoch 013:    822 / 1474 loss=2.107, trans_loss=4.962, nll_loss=2.22, w2v_ctc_loss=0.764, task_loss=0, contrastive_loss=0.142, total=4122.78, n_correct=2676.53, ppl=4.66, accuracy=64.921, wps=14256.4, ups=1.73, wpb=8245.6, bsz=306, num_updates=18500, lr=0.000103975, gnorm=0.552, clip=0, loss_scale=32, train_wall=57, gb_free=15, wall=14902
2023-08-12 06:15:41 | INFO | train_inner | epoch 013:    922 / 1474 loss=2.102, trans_loss=4.963, nll_loss=2.221, w2v_ctc_loss=0.766, task_loss=0, contrastive_loss=0.106, total=4102.59, n_correct=2665.71, ppl=4.66, accuracy=64.976, wps=14356.8, ups=1.75, wpb=8205.2, bsz=296.6, num_updates=18600, lr=0.000103695, gnorm=0.56, clip=0, loss_scale=32, train_wall=57, gb_free=17.3, wall=14959
2023-08-12 06:16:38 | INFO | train_inner | epoch 013:   1022 / 1474 loss=2.112, trans_loss=4.965, nll_loss=2.223, w2v_ctc_loss=0.77, task_loss=0, contrastive_loss=0.157, total=4087.8, n_correct=2648.04, ppl=4.67, accuracy=64.779, wps=14419.9, ups=1.76, wpb=8175.6, bsz=293.7, num_updates=18700, lr=0.000103418, gnorm=0.556, clip=0, loss_scale=32, train_wall=56, gb_free=16.7, wall=15016
2023-08-12 06:17:35 | INFO | train_inner | epoch 013:   1122 / 1474 loss=2.099, trans_loss=4.953, nll_loss=2.208, w2v_ctc_loss=0.759, task_loss=0, contrastive_loss=0.137, total=4098.77, n_correct=2669.05, ppl=4.62, accuracy=65.118, wps=14388.1, ups=1.76, wpb=8197.5, bsz=304.8, num_updates=18800, lr=0.000103142, gnorm=0.545, clip=0, loss_scale=32, train_wall=56, gb_free=13.8, wall=15073
2023-08-12 06:18:33 | INFO | train_inner | epoch 013:   1222 / 1474 loss=2.103, trans_loss=4.963, nll_loss=2.221, w2v_ctc_loss=0.77, task_loss=0, contrastive_loss=0.097, total=4115.57, n_correct=2670.67, ppl=4.66, accuracy=64.892, wps=14290.2, ups=1.74, wpb=8231.1, bsz=295.9, num_updates=18900, lr=0.000102869, gnorm=0.545, clip=0, loss_scale=32, train_wall=57, gb_free=15.2, wall=15130
2023-08-12 06:19:30 | INFO | train_inner | epoch 013:   1322 / 1474 loss=2.106, trans_loss=4.953, nll_loss=2.209, w2v_ctc_loss=0.762, task_loss=0, contrastive_loss=0.195, total=4111.02, n_correct=2681.18, ppl=4.62, accuracy=65.219, wps=14343.7, ups=1.74, wpb=8222, bsz=307.8, num_updates=19000, lr=0.000102598, gnorm=0.552, clip=0, loss_scale=32, train_wall=57, gb_free=17.9, wall=15188
2023-08-12 06:20:27 | INFO | train_inner | epoch 013:   1422 / 1474 loss=2.113, trans_loss=4.962, nll_loss=2.221, w2v_ctc_loss=0.761, task_loss=0, contrastive_loss=0.206, total=4179.06, n_correct=2712.69, ppl=4.66, accuracy=64.911, wps=14679.5, ups=1.76, wpb=8358.1, bsz=311.9, num_updates=19100, lr=0.000102329, gnorm=0.552, clip=0, loss_scale=32, train_wall=56, gb_free=16.1, wall=15244
2023-08-12 06:20:57 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 06:21:19 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.113 | trans_loss 5.192 | nll_loss 2.461 | w2v_ctc_loss 1.337 | task_loss 0 | contrastive_loss 0.312 | total 4003.4 | n_correct 2638.2 | ppl 5.5 | accuracy 65.899 | uer 18.738 | wer 20.618 | raw_wer 20.618 | bleu 21.64 | wps 2362.5 | wpb 4003.4 | bsz 141.8 | num_updates 19152 | best_bleu 21.64
2023-08-12 06:21:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 19152 updates
2023-08-12 06:21:19 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 06:21:34 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 06:21:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt (epoch 13 @ 19152 updates, score 21.64) (writing took 27.751155085861683 seconds)
2023-08-12 06:21:47 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-08-12 06:21:47 | INFO | train | epoch 013 | loss 2.104 | trans_loss 4.957 | nll_loss 2.213 | w2v_ctc_loss 0.763 | task_loss 0 | contrastive_loss 0.15 | total 4138.65 | n_correct 2691.09 | ppl 4.64 | accuracy 65.023 | wps 12612.1 | ups 1.52 | wpb 8277.3 | bsz 305.7 | num_updates 19152 | lr 0.00010219 | gnorm 0.549 | clip 0 | loss_scale 32 | train_wall 836 | gb_free 17.7 | wall 15325
2023-08-12 06:21:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 06:21:47 | INFO | fairseq.trainer | begin training epoch 14
2023-08-12 06:21:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 06:22:23 | INFO | train_inner | epoch 014:     48 / 1474 loss=2.084, trans_loss=4.929, nll_loss=2.179, w2v_ctc_loss=0.751, task_loss=0, contrastive_loss=0.11, total=4179.66, n_correct=2742.7, ppl=4.53, accuracy=65.62, wps=7234.9, ups=0.87, wpb=8359.3, bsz=322, num_updates=19200, lr=0.000102062, gnorm=0.547, clip=0, loss_scale=32, train_wall=57, gb_free=10.9, wall=15360
2023-08-12 06:23:20 | INFO | train_inner | epoch 014:    148 / 1474 loss=2.079, trans_loss=4.923, nll_loss=2.169, w2v_ctc_loss=0.752, task_loss=0, contrastive_loss=0.093, total=4081.01, n_correct=2684.24, ppl=4.5, accuracy=65.774, wps=14310.9, ups=1.75, wpb=8162, bsz=300.6, num_updates=19300, lr=0.000101797, gnorm=0.547, clip=0, loss_scale=32, train_wall=57, gb_free=16, wall=15417
2023-08-12 06:24:17 | INFO | train_inner | epoch 014:    248 / 1474 loss=2.092, trans_loss=4.936, nll_loss=2.185, w2v_ctc_loss=0.75, task_loss=0, contrastive_loss=0.192, total=4109.83, n_correct=2691.87, ppl=4.55, accuracy=65.498, wps=14335.2, ups=1.74, wpb=8219.7, bsz=295.2, num_updates=19400, lr=0.000101535, gnorm=0.555, clip=0, loss_scale=32, train_wall=57, gb_free=15.7, wall=15474
2023-08-12 06:25:14 | INFO | train_inner | epoch 014:    348 / 1474 loss=2.086, trans_loss=4.936, nll_loss=2.186, w2v_ctc_loss=0.747, task_loss=0, contrastive_loss=0.128, total=4171.83, n_correct=2737.87, ppl=4.55, accuracy=65.628, wps=14707.7, ups=1.76, wpb=8343.7, bsz=319.7, num_updates=19500, lr=0.000101274, gnorm=0.545, clip=0, loss_scale=32, train_wall=56, gb_free=16.8, wall=15531
2023-08-12 06:26:11 | INFO | train_inner | epoch 014:    448 / 1474 loss=2.083, trans_loss=4.94, nll_loss=2.192, w2v_ctc_loss=0.746, task_loss=0, contrastive_loss=0.101, total=4142.75, n_correct=2711.66, ppl=4.57, accuracy=65.456, wps=14417.2, ups=1.74, wpb=8285.5, bsz=302.2, num_updates=19600, lr=0.000101015, gnorm=0.551, clip=0, loss_scale=32, train_wall=57, gb_free=16.7, wall=15589
2023-08-12 06:27:09 | INFO | train_inner | epoch 014:    548 / 1474 loss=2.094, trans_loss=4.941, nll_loss=2.192, w2v_ctc_loss=0.767, task_loss=0, contrastive_loss=0.111, total=4073.76, n_correct=2657.84, ppl=4.57, accuracy=65.243, wps=14141.4, ups=1.74, wpb=8147.5, bsz=290.9, num_updates=19700, lr=0.000100759, gnorm=0.551, clip=0, loss_scale=32, train_wall=57, gb_free=15.6, wall=15646
2023-08-12 06:28:06 | INFO | train_inner | epoch 014:    648 / 1474 loss=2.093, trans_loss=4.939, nll_loss=2.19, w2v_ctc_loss=0.75, task_loss=0, contrastive_loss=0.168, total=4158.79, n_correct=2718.61, ppl=4.56, accuracy=65.37, wps=14537.8, ups=1.75, wpb=8317.6, bsz=306.8, num_updates=19800, lr=0.000100504, gnorm=0.541, clip=0, loss_scale=32, train_wall=57, gb_free=17.3, wall=15703
2023-08-12 06:29:03 | INFO | train_inner | epoch 014:    748 / 1474 loss=2.08, trans_loss=4.927, nll_loss=2.175, w2v_ctc_loss=0.746, task_loss=0, contrastive_loss=0.099, total=4145.47, n_correct=2720.75, ppl=4.52, accuracy=65.632, wps=14629.9, ups=1.76, wpb=8290.9, bsz=309.6, num_updates=19900, lr=0.000100251, gnorm=0.544, clip=0, loss_scale=32, train_wall=56, gb_free=16.3, wall=15760
2023-08-12 06:30:00 | INFO | train_inner | epoch 014:    848 / 1474 loss=2.096, trans_loss=4.93, nll_loss=2.179, w2v_ctc_loss=0.75, task_loss=0, contrastive_loss=0.208, total=4171.1, n_correct=2734, ppl=4.53, accuracy=65.546, wps=14487, ups=1.74, wpb=8342.2, bsz=319.7, num_updates=20000, lr=0.0001, gnorm=0.542, clip=0, loss_scale=32, train_wall=57, gb_free=16.7, wall=15818
2023-08-12 06:30:00 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 06:30:22 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.123 | trans_loss 5.187 | nll_loss 2.454 | w2v_ctc_loss 1.391 | task_loss 0 | contrastive_loss 0.302 | total 4003.4 | n_correct 2641.4 | ppl 5.48 | accuracy 65.979 | uer 18.339 | wer 20.238 | raw_wer 20.238 | bleu 21.95 | wps 2423.2 | wpb 4003.4 | bsz 141.8 | num_updates 20000 | best_bleu 21.95
2023-08-12 06:30:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20000 updates
2023-08-12 06:30:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_14_20000.pt
2023-08-12 06:30:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_14_20000.pt
2023-08-12 06:31:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_14_20000.pt (epoch 14 @ 20000 updates, score 21.95) (writing took 59.026651019230485 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:0')
2023-08-12 06:32:22 | INFO | train_inner | epoch 014:    948 / 1474 loss=2.09, trans_loss=4.938, nll_loss=2.189, w2v_ctc_loss=0.75, task_loss=0, contrastive_loss=0.142, total=4167.75, n_correct=2722.93, ppl=4.56, accuracy=65.333, wps=5878.5, ups=0.71, wpb=8335.5, bsz=310.1, num_updates=20100, lr=9.97509e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=57, gb_free=16.3, wall=15959
2023-08-12 06:33:20 | INFO | train_inner | epoch 014:   1048 / 1474 loss=2.086, trans_loss=4.939, nll_loss=2.19, w2v_ctc_loss=0.747, task_loss=0, contrastive_loss=0.121, total=4143.92, n_correct=2713.39, ppl=4.56, accuracy=65.479, wps=14361.9, ups=1.73, wpb=8287.8, bsz=300.7, num_updates=20200, lr=9.95037e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=57, gb_free=16.1, wall=16017
2023-08-12 06:34:17 | INFO | train_inner | epoch 014:   1148 / 1474 loss=2.122, trans_loss=4.942, nll_loss=2.195, w2v_ctc_loss=0.76, task_loss=0, contrastive_loss=0.389, total=4228.69, n_correct=2759.23, ppl=4.58, accuracy=65.25, wps=14702.3, ups=1.74, wpb=8457.4, bsz=327.2, num_updates=20300, lr=9.92583e-05, gnorm=0.544, clip=0, loss_scale=64, train_wall=57, gb_free=15.8, wall=16075
2023-08-12 06:35:14 | INFO | train_inner | epoch 014:   1248 / 1474 loss=2.093, trans_loss=4.948, nll_loss=2.202, w2v_ctc_loss=0.769, task_loss=0, contrastive_loss=0.083, total=4021.19, n_correct=2621.11, ppl=4.6, accuracy=65.182, wps=14076.6, ups=1.75, wpb=8042.4, bsz=271.7, num_updates=20400, lr=9.90148e-05, gnorm=0.558, clip=0, loss_scale=64, train_wall=57, gb_free=16.4, wall=16132
2023-08-12 06:36:12 | INFO | train_inner | epoch 014:   1348 / 1474 loss=2.083, trans_loss=4.94, nll_loss=2.192, w2v_ctc_loss=0.744, task_loss=0, contrastive_loss=0.099, total=4213.9, n_correct=2760.89, ppl=4.57, accuracy=65.519, wps=14673, ups=1.74, wpb=8427.8, bsz=319.4, num_updates=20500, lr=9.8773e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=57, gb_free=16.2, wall=16189
2023-08-12 06:37:09 | INFO | train_inner | epoch 014:   1448 / 1474 loss=2.092, trans_loss=4.944, nll_loss=2.197, w2v_ctc_loss=0.752, task_loss=0, contrastive_loss=0.138, total=4130.28, n_correct=2697.62, ppl=4.59, accuracy=65.313, wps=14428.8, ups=1.75, wpb=8260.6, bsz=304.1, num_updates=20600, lr=9.85329e-05, gnorm=0.549, clip=0, loss_scale=64, train_wall=57, gb_free=15.4, wall=16247
2023-08-12 06:37:24 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:1')
2023-08-12 06:37:45 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.113 | trans_loss 5.182 | nll_loss 2.45 | w2v_ctc_loss 1.367 | task_loss 0 | contrastive_loss 0.308 | total 4003.4 | n_correct 2650.6 | ppl 5.46 | accuracy 66.209 | uer 18.135 | wer 19.884 | raw_wer 19.884 | bleu 21.61 | wps 2422.2 | wpb 4003.4 | bsz 141.8 | num_updates 20626 | best_bleu 21.95
2023-08-12 06:37:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20626 updates
2023-08-12 06:37:46 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_21.6100.pt
2023-08-12 06:37:49 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_21.6100.pt
2023-08-12 06:38:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_21.6100.pt (epoch 14 @ 20626 updates, score 21.61) (writing took 18.755612092092633 seconds)
2023-08-12 06:38:07 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-08-12 06:38:07 | INFO | train | epoch 014 | loss 2.091 | trans_loss 4.937 | nll_loss 2.188 | w2v_ctc_loss 0.752 | task_loss 0 | contrastive_loss 0.147 | total 4138.65 | n_correct 2708.56 | ppl 4.56 | accuracy 65.445 | wps 12452.1 | ups 1.5 | wpb 8277.3 | bsz 305.7 | num_updates 20626 | lr 9.84708e-05 | gnorm 0.548 | clip 0 | loss_scale 64 | train_wall 838 | gb_free 16.4 | wall 16304
2023-08-12 06:38:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 06:38:07 | INFO | fairseq.trainer | begin training epoch 15
2023-08-12 06:38:07 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 06:38:57 | INFO | train_inner | epoch 015:     74 / 1474 loss=2.085, trans_loss=4.927, nll_loss=2.175, w2v_ctc_loss=0.741, task_loss=0, contrastive_loss=0.186, total=4083.88, n_correct=2683.09, ppl=4.51, accuracy=65.7, wps=7582.1, ups=0.93, wpb=8167.8, bsz=300.2, num_updates=20700, lr=9.82946e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=56, gb_free=15.9, wall=16354
2023-08-12 06:39:54 | INFO | train_inner | epoch 015:    174 / 1474 loss=2.074, trans_loss=4.916, nll_loss=2.16, w2v_ctc_loss=0.749, task_loss=0, contrastive_loss=0.097, total=4115.73, n_correct=2716.47, ppl=4.47, accuracy=66.002, wps=14402.3, ups=1.75, wpb=8231.5, bsz=297.8, num_updates=20800, lr=9.80581e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=57, gb_free=16.8, wall=16411
2023-08-12 06:40:52 | INFO | train_inner | epoch 015:    274 / 1474 loss=2.068, trans_loss=4.917, nll_loss=2.162, w2v_ctc_loss=0.736, task_loss=0, contrastive_loss=0.089, total=4193.15, n_correct=2768.5, ppl=4.47, accuracy=66.024, wps=14538.8, ups=1.73, wpb=8386.3, bsz=312.6, num_updates=20900, lr=9.78232e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=57, gb_free=13.1, wall=16469
2023-08-12 06:41:49 | INFO | train_inner | epoch 015:    374 / 1474 loss=2.074, trans_loss=4.914, nll_loss=2.158, w2v_ctc_loss=0.74, task_loss=0, contrastive_loss=0.113, total=4167.66, n_correct=2742.32, ppl=4.46, accuracy=65.8, wps=14636.7, ups=1.76, wpb=8335.3, bsz=306, num_updates=21000, lr=9.759e-05, gnorm=0.551, clip=0, loss_scale=64, train_wall=56, gb_free=16.2, wall=16526
2023-08-12 06:42:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-12 06:42:46 | INFO | train_inner | epoch 015:    475 / 1474 loss=2.066, trans_loss=4.916, nll_loss=2.16, w2v_ctc_loss=0.733, task_loss=0, contrastive_loss=0.082, total=4045.97, n_correct=2663.77, ppl=4.47, accuracy=65.838, wps=14047.1, ups=1.74, wpb=8091.9, bsz=285.2, num_updates=21100, lr=9.73585e-05, gnorm=0.554, clip=0, loss_scale=32, train_wall=57, gb_free=16.7, wall=16584
2023-08-12 06:43:44 | INFO | train_inner | epoch 015:    575 / 1474 loss=2.074, trans_loss=4.917, nll_loss=2.161, w2v_ctc_loss=0.743, task_loss=0, contrastive_loss=0.114, total=4156.05, n_correct=2736.45, ppl=4.47, accuracy=65.843, wps=14447.3, ups=1.74, wpb=8312.1, bsz=302.8, num_updates=21200, lr=9.71286e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=57, gb_free=12.1, wall=16641
2023-08-12 06:44:42 | INFO | train_inner | epoch 015:    675 / 1474 loss=2.081, trans_loss=4.914, nll_loss=2.158, w2v_ctc_loss=0.746, task_loss=0, contrastive_loss=0.157, total=4118.87, n_correct=2710.61, ppl=4.46, accuracy=65.81, wps=14247.1, ups=1.73, wpb=8237.7, bsz=303, num_updates=21300, lr=9.69003e-05, gnorm=0.555, clip=0, loss_scale=32, train_wall=57, gb_free=17, wall=16699
2023-08-12 06:45:39 | INFO | train_inner | epoch 015:    775 / 1474 loss=2.075, trans_loss=4.923, nll_loss=2.169, w2v_ctc_loss=0.747, task_loss=0, contrastive_loss=0.099, total=4176.64, n_correct=2744.63, ppl=4.5, accuracy=65.714, wps=14667.1, ups=1.76, wpb=8353.3, bsz=305.3, num_updates=21400, lr=9.66736e-05, gnorm=0.549, clip=0, loss_scale=32, train_wall=56, gb_free=14.2, wall=16756
2023-08-12 06:46:35 | INFO | train_inner | epoch 015:    875 / 1474 loss=2.078, trans_loss=4.926, nll_loss=2.174, w2v_ctc_loss=0.75, task_loss=0, contrastive_loss=0.092, total=4056.99, n_correct=2661.15, ppl=4.51, accuracy=65.594, wps=14269.3, ups=1.76, wpb=8114, bsz=288, num_updates=21500, lr=9.64486e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=56, gb_free=17.3, wall=16813
2023-08-12 06:47:32 | INFO | train_inner | epoch 015:    975 / 1474 loss=2.079, trans_loss=4.921, nll_loss=2.168, w2v_ctc_loss=0.738, task_loss=0, contrastive_loss=0.175, total=4134.44, n_correct=2721.96, ppl=4.49, accuracy=65.836, wps=14517.2, ups=1.76, wpb=8268.9, bsz=304.8, num_updates=21600, lr=9.6225e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=56, gb_free=16.1, wall=16870
2023-08-12 06:48:31 | INFO | train_inner | epoch 015:   1075 / 1474 loss=2.104, trans_loss=4.931, nll_loss=2.18, w2v_ctc_loss=0.746, task_loss=0, contrastive_loss=0.333, total=4185.02, n_correct=2742.84, ppl=4.53, accuracy=65.539, wps=14385, ups=1.72, wpb=8370, bsz=324.2, num_updates=21700, lr=9.60031e-05, gnorm=0.549, clip=0, loss_scale=32, train_wall=58, gb_free=16.8, wall=16928
2023-08-12 06:49:27 | INFO | train_inner | epoch 015:   1175 / 1474 loss=2.068, trans_loss=4.916, nll_loss=2.162, w2v_ctc_loss=0.72, task_loss=0, contrastive_loss=0.139, total=4187.68, n_correct=2768.28, ppl=4.48, accuracy=66.105, wps=14735.2, ups=1.76, wpb=8375.4, bsz=329.7, num_updates=21800, lr=9.57826e-05, gnorm=0.536, clip=0, loss_scale=32, train_wall=56, gb_free=16.7, wall=16985
2023-08-12 06:50:24 | INFO | train_inner | epoch 015:   1275 / 1474 loss=2.078, trans_loss=4.92, nll_loss=2.165, w2v_ctc_loss=0.754, task_loss=0, contrastive_loss=0.096, total=4141.6, n_correct=2724.65, ppl=4.49, accuracy=65.787, wps=14520.1, ups=1.75, wpb=8283.2, bsz=301.1, num_updates=21900, lr=9.55637e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=57, gb_free=13.7, wall=17042
2023-08-12 06:51:22 | INFO | train_inner | epoch 015:   1375 / 1474 loss=2.069, trans_loss=4.919, nll_loss=2.164, w2v_ctc_loss=0.74, task_loss=0, contrastive_loss=0.082, total=4099.6, n_correct=2699.52, ppl=4.48, accuracy=65.848, wps=14363.8, ups=1.75, wpb=8199.2, bsz=293.2, num_updates=22000, lr=9.53463e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=57, gb_free=14.5, wall=17099
2023-08-12 06:51:22 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 06:51:44 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 4.097 | trans_loss 5.178 | nll_loss 2.442 | w2v_ctc_loss 1.324 | task_loss 0 | contrastive_loss 0.307 | total 4003.4 | n_correct 2656.6 | ppl 5.43 | accuracy 66.359 | uer 18.215 | wer 20.212 | raw_wer 20.212 | bleu 22 | wps 2273.6 | wpb 4003.4 | bsz 141.8 | num_updates 22000 | best_bleu 22
2023-08-12 06:51:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22000 updates
2023-08-12 06:51:44 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_15_22000.pt
2023-08-12 06:51:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_15_22000.pt
2023-08-12 06:52:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_15_22000.pt (epoch 15 @ 22000 updates, score 22.0) (writing took 43.02958425693214 seconds)
2023-08-12 06:53:26 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 06:53:48 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 4.101 | trans_loss 5.169 | nll_loss 2.435 | w2v_ctc_loss 1.351 | task_loss 0 | contrastive_loss 0.316 | total 4003.4 | n_correct 2657.7 | ppl 5.41 | accuracy 66.386 | uer 18.329 | wer 20.383 | raw_wer 20.383 | bleu 22 | wps 2397.9 | wpb 4003.4 | bsz 141.8 | num_updates 22099 | best_bleu 22
2023-08-12 06:53:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22099 updates
2023-08-12 06:53:48 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 06:54:02 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 06:54:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt (epoch 15 @ 22099 updates, score 22.0) (writing took 28.169561931863427 seconds)
2023-08-12 06:54:16 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-08-12 06:54:16 | INFO | train | epoch 015 | loss 2.077 | trans_loss 4.92 | nll_loss 2.165 | w2v_ctc_loss 0.741 | task_loss 0 | contrastive_loss 0.137 | total 4136.86 | n_correct 2723.09 | ppl 4.49 | accuracy 65.825 | wps 12572.2 | ups 1.52 | wpb 8273.7 | bsz 305.1 | num_updates 22099 | lr 9.51325e-05 | gnorm 0.546 | clip 0 | loss_scale 32 | train_wall 837 | gb_free 16.9 | wall 17274
2023-08-12 06:54:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 06:54:17 | INFO | fairseq.trainer | begin training epoch 16
2023-08-12 06:54:17 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 06:54:25 | INFO | train_inner | epoch 016:      1 / 1474 loss=2.087, trans_loss=4.927, nll_loss=2.177, w2v_ctc_loss=0.746, task_loss=0, contrastive_loss=0.171, total=4149.9, n_correct=2725.47, ppl=4.52, accuracy=65.676, wps=4535.1, ups=0.55, wpb=8299.8, bsz=316.3, num_updates=22100, lr=9.51303e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=58, gb_free=17.5, wall=17282
2023-08-12 06:55:22 | INFO | train_inner | epoch 016:    101 / 1474 loss=2.063, trans_loss=4.902, nll_loss=2.143, w2v_ctc_loss=0.731, task_loss=0, contrastive_loss=0.112, total=4118.73, n_correct=2727.85, ppl=4.42, accuracy=66.23, wps=14455.3, ups=1.75, wpb=8237.5, bsz=314, num_updates=22200, lr=9.49158e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=56, gb_free=16.3, wall=17339
2023-08-12 06:56:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-12 06:56:19 | INFO | train_inner | epoch 016:    202 / 1474 loss=2.051, trans_loss=4.891, nll_loss=2.128, w2v_ctc_loss=0.721, task_loss=0, contrastive_loss=0.088, total=4116.16, n_correct=2736.33, ppl=4.37, accuracy=66.478, wps=14244.8, ups=1.73, wpb=8232.3, bsz=298.7, num_updates=22300, lr=9.47027e-05, gnorm=0.539, clip=0, loss_scale=16, train_wall=57, gb_free=15.2, wall=17397
2023-08-12 06:57:17 | INFO | train_inner | epoch 016:    302 / 1474 loss=2.07, trans_loss=4.902, nll_loss=2.142, w2v_ctc_loss=0.734, task_loss=0, contrastive_loss=0.162, total=4164.1, n_correct=2753.18, ppl=4.41, accuracy=66.117, wps=14525.1, ups=1.74, wpb=8328.2, bsz=308.6, num_updates=22400, lr=9.44911e-05, gnorm=0.549, clip=0, loss_scale=16, train_wall=57, gb_free=17.3, wall=17454
2023-08-12 06:58:14 | INFO | train_inner | epoch 016:    402 / 1474 loss=2.071, trans_loss=4.9, nll_loss=2.139, w2v_ctc_loss=0.741, task_loss=0, contrastive_loss=0.174, total=4065.22, n_correct=2690.7, ppl=4.41, accuracy=66.188, wps=14104.9, ups=1.73, wpb=8130.4, bsz=286.4, num_updates=22500, lr=9.42809e-05, gnorm=0.551, clip=0, loss_scale=16, train_wall=57, gb_free=16.4, wall=17512
2023-08-12 06:59:12 | INFO | train_inner | epoch 016:    502 / 1474 loss=2.065, trans_loss=4.905, nll_loss=2.147, w2v_ctc_loss=0.73, task_loss=0, contrastive_loss=0.119, total=4181.93, n_correct=2771, ppl=4.43, accuracy=66.261, wps=14586.4, ups=1.74, wpb=8363.9, bsz=320.3, num_updates=22600, lr=9.40721e-05, gnorm=0.546, clip=0, loss_scale=16, train_wall=57, gb_free=15.9, wall=17569
2023-08-12 07:00:09 | INFO | train_inner | epoch 016:    602 / 1474 loss=2.056, trans_loss=4.901, nll_loss=2.142, w2v_ctc_loss=0.725, task_loss=0, contrastive_loss=0.083, total=4122.97, n_correct=2732.7, ppl=4.41, accuracy=66.28, wps=14435, ups=1.75, wpb=8245.9, bsz=299, num_updates=22700, lr=9.38647e-05, gnorm=0.542, clip=0, loss_scale=16, train_wall=57, gb_free=16.2, wall=17626
2023-08-12 07:01:05 | INFO | train_inner | epoch 016:    702 / 1474 loss=2.061, trans_loss=4.903, nll_loss=2.144, w2v_ctc_loss=0.735, task_loss=0, contrastive_loss=0.086, total=4093.15, n_correct=2708.08, ppl=4.42, accuracy=66.161, wps=14512.4, ups=1.77, wpb=8186.3, bsz=296.5, num_updates=22800, lr=9.36586e-05, gnorm=0.547, clip=0, loss_scale=16, train_wall=56, gb_free=17.9, wall=17683
2023-08-12 07:02:03 | INFO | train_inner | epoch 016:    802 / 1474 loss=2.063, trans_loss=4.899, nll_loss=2.139, w2v_ctc_loss=0.724, task_loss=0, contrastive_loss=0.146, total=4183.24, n_correct=2770.39, ppl=4.41, accuracy=66.226, wps=14552.4, ups=1.74, wpb=8366.5, bsz=312.1, num_updates=22900, lr=9.34539e-05, gnorm=0.544, clip=0, loss_scale=16, train_wall=57, gb_free=17.8, wall=17740
2023-08-12 07:03:00 | INFO | train_inner | epoch 016:    902 / 1474 loss=2.064, trans_loss=4.902, nll_loss=2.143, w2v_ctc_loss=0.728, task_loss=0, contrastive_loss=0.139, total=4150.23, n_correct=2750.75, ppl=4.42, accuracy=66.279, wps=14575, ups=1.76, wpb=8300.5, bsz=306.5, num_updates=23000, lr=9.32505e-05, gnorm=0.54, clip=0, loss_scale=16, train_wall=56, gb_free=12.4, wall=17797
2023-08-12 07:03:57 | INFO | train_inner | epoch 016:   1002 / 1474 loss=2.074, trans_loss=4.91, nll_loss=2.153, w2v_ctc_loss=0.743, task_loss=0, contrastive_loss=0.135, total=4116.59, n_correct=2713.65, ppl=4.45, accuracy=65.92, wps=14256.2, ups=1.73, wpb=8233.2, bsz=300.6, num_updates=23100, lr=9.30484e-05, gnorm=0.548, clip=0, loss_scale=16, train_wall=57, gb_free=16.9, wall=17855
2023-08-12 07:04:55 | INFO | train_inner | epoch 016:   1102 / 1474 loss=2.075, trans_loss=4.915, nll_loss=2.16, w2v_ctc_loss=0.749, task_loss=0, contrastive_loss=0.113, total=4112.71, n_correct=2710.12, ppl=4.47, accuracy=65.896, wps=14390.5, ups=1.75, wpb=8225.4, bsz=295.7, num_updates=23200, lr=9.28477e-05, gnorm=0.558, clip=0, loss_scale=16, train_wall=57, gb_free=12.5, wall=17912
2023-08-12 07:05:53 | INFO | train_inner | epoch 016:   1202 / 1474 loss=2.073, trans_loss=4.909, nll_loss=2.152, w2v_ctc_loss=0.72, task_loss=0, contrastive_loss=0.208, total=4161.11, n_correct=2748.27, ppl=4.44, accuracy=66.047, wps=14341.9, ups=1.72, wpb=8322.2, bsz=308.2, num_updates=23300, lr=9.26482e-05, gnorm=0.543, clip=0, loss_scale=16, train_wall=57, gb_free=15.7, wall=17970
2023-08-12 07:06:50 | INFO | train_inner | epoch 016:   1302 / 1474 loss=2.076, trans_loss=4.906, nll_loss=2.149, w2v_ctc_loss=0.741, task_loss=0, contrastive_loss=0.188, total=4149.14, n_correct=2745.09, ppl=4.44, accuracy=66.16, wps=14437.5, ups=1.74, wpb=8298.3, bsz=311.9, num_updates=23400, lr=9.245e-05, gnorm=0.54, clip=0, loss_scale=16, train_wall=57, gb_free=11.7, wall=18028
2023-08-12 07:07:48 | INFO | train_inner | epoch 016:   1402 / 1474 loss=2.067, trans_loss=4.904, nll_loss=2.147, w2v_ctc_loss=0.739, task_loss=0, contrastive_loss=0.115, total=4200.01, n_correct=2779.18, ppl=4.43, accuracy=66.171, wps=14502.5, ups=1.73, wpb=8400, bsz=322.2, num_updates=23500, lr=9.22531e-05, gnorm=0.549, clip=0, loss_scale=16, train_wall=57, gb_free=17, wall=18085
2023-08-12 07:08:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 07:08:52 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 4.088 | trans_loss 5.167 | nll_loss 2.428 | w2v_ctc_loss 1.326 | task_loss 0 | contrastive_loss 0.3 | total 4003.4 | n_correct 2660.5 | ppl 5.38 | accuracy 66.456 | uer 18.047 | wer 19.969 | raw_wer 19.969 | bleu 22.24 | wps 2328.7 | wpb 4003.4 | bsz 141.8 | num_updates 23572 | best_bleu 22.24
2023-08-12 07:08:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 23572 updates
2023-08-12 07:08:52 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 07:09:07 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 07:09:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt (epoch 16 @ 23572 updates, score 22.24) (writing took 29.047373805195093 seconds)
2023-08-12 07:09:22 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-08-12 07:09:22 | INFO | train | epoch 016 | loss 2.067 | trans_loss 4.904 | nll_loss 2.145 | w2v_ctc_loss 0.733 | task_loss 0 | contrastive_loss 0.142 | total 4139 | n_correct 2738.87 | ppl 4.42 | accuracy 66.172 | wps 13471.6 | ups 1.63 | wpb 8278 | bsz 305.7 | num_updates 23572 | lr 9.21121e-05 | gnorm 0.546 | clip 0 | loss_scale 16 | train_wall 837 | gb_free 15.6 | wall 18179
2023-08-12 07:09:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 07:09:22 | INFO | fairseq.trainer | begin training epoch 17
2023-08-12 07:09:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 07:09:46 | INFO | train_inner | epoch 017:     28 / 1474 loss=2.069, trans_loss=4.893, nll_loss=2.132, w2v_ctc_loss=0.723, task_loss=0, contrastive_loss=0.25, total=4141.79, n_correct=2745.65, ppl=4.38, accuracy=66.291, wps=6998.5, ups=0.84, wpb=8283.6, bsz=301.5, num_updates=23600, lr=9.20575e-05, gnorm=0.548, clip=0, loss_scale=16, train_wall=57, gb_free=16.2, wall=18204
2023-08-12 07:10:44 | INFO | train_inner | epoch 017:    128 / 1474 loss=2.052, trans_loss=4.883, nll_loss=2.118, w2v_ctc_loss=0.73, task_loss=0, contrastive_loss=0.088, total=4110.88, n_correct=2732.93, ppl=4.34, accuracy=66.48, wps=14317.9, ups=1.74, wpb=8221.8, bsz=295.4, num_updates=23700, lr=9.1863e-05, gnorm=0.549, clip=0, loss_scale=16, train_wall=57, gb_free=16.4, wall=18261
2023-08-12 07:11:41 | INFO | train_inner | epoch 017:    228 / 1474 loss=2.067, trans_loss=4.882, nll_loss=2.116, w2v_ctc_loss=0.714, task_loss=0, contrastive_loss=0.253, total=4171.95, n_correct=2777.17, ppl=4.34, accuracy=66.568, wps=14549.7, ups=1.74, wpb=8343.9, bsz=320.4, num_updates=23800, lr=9.16698e-05, gnorm=0.55, clip=0, loss_scale=16, train_wall=57, gb_free=16.1, wall=18319
2023-08-12 07:12:38 | INFO | train_inner | epoch 017:    328 / 1474 loss=2.064, trans_loss=4.888, nll_loss=2.124, w2v_ctc_loss=0.718, task_loss=0, contrastive_loss=0.256, total=4157.94, n_correct=2764.21, ppl=4.36, accuracy=66.48, wps=14566.7, ups=1.75, wpb=8315.9, bsz=305, num_updates=23900, lr=9.14779e-05, gnorm=0.538, clip=0, loss_scale=16, train_wall=57, gb_free=14.7, wall=18376
2023-08-12 07:13:36 | INFO | train_inner | epoch 017:    428 / 1474 loss=2.047, trans_loss=4.884, nll_loss=2.12, w2v_ctc_loss=0.72, task_loss=0, contrastive_loss=0.085, total=4141.8, n_correct=2758.97, ppl=4.35, accuracy=66.613, wps=14353.4, ups=1.73, wpb=8283.6, bsz=306.7, num_updates=24000, lr=9.12871e-05, gnorm=0.541, clip=0, loss_scale=16, train_wall=57, gb_free=17.4, wall=18433
2023-08-12 07:13:36 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 07:14:00 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 4.088 | trans_loss 5.164 | nll_loss 2.425 | w2v_ctc_loss 1.336 | task_loss 0 | contrastive_loss 0.292 | total 4003.4 | n_correct 2660.7 | ppl 5.37 | accuracy 66.461 | uer 17.893 | wer 19.776 | raw_wer 19.776 | bleu 22.05 | wps 2098.7 | wpb 4003.4 | bsz 141.8 | num_updates 24000 | best_bleu 22.24
2023-08-12 07:14:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 24000 updates
2023-08-12 07:14:00 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_17_24000.pt
2023-08-12 07:14:04 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_17_24000.pt
2023-08-12 07:14:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_17_24000.pt (epoch 17 @ 24000 updates, score 22.05) (writing took 38.721422323957086 seconds)
2023-08-12 07:15:37 | INFO | train_inner | epoch 017:    528 / 1474 loss=2.057, trans_loss=4.892, nll_loss=2.129, w2v_ctc_loss=0.725, task_loss=0, contrastive_loss=0.131, total=4180.09, n_correct=2776.25, ppl=4.37, accuracy=66.416, wps=6883.7, ups=0.82, wpb=8360.2, bsz=307.5, num_updates=24100, lr=9.10975e-05, gnorm=0.54, clip=0, loss_scale=16, train_wall=58, gb_free=17.2, wall=18555
2023-08-12 07:16:34 | INFO | train_inner | epoch 017:    628 / 1474 loss=2.049, trans_loss=4.89, nll_loss=2.128, w2v_ctc_loss=0.721, task_loss=0, contrastive_loss=0.083, total=4166.6, n_correct=2771.5, ppl=4.37, accuracy=66.517, wps=14631.9, ups=1.76, wpb=8333.2, bsz=302.3, num_updates=24200, lr=9.09091e-05, gnorm=0.536, clip=0, loss_scale=16, train_wall=56, gb_free=15.8, wall=18612
2023-08-12 07:17:32 | INFO | train_inner | epoch 017:    728 / 1474 loss=2.063, trans_loss=4.892, nll_loss=2.129, w2v_ctc_loss=0.736, task_loss=0, contrastive_loss=0.131, total=4168.97, n_correct=2767.07, ppl=4.38, accuracy=66.373, wps=14554.9, ups=1.75, wpb=8337.9, bsz=308.4, num_updates=24300, lr=9.07218e-05, gnorm=0.548, clip=0, loss_scale=16, train_wall=57, gb_free=17, wall=18669
2023-08-12 07:18:29 | INFO | train_inner | epoch 017:    828 / 1474 loss=2.053, trans_loss=4.89, nll_loss=2.128, w2v_ctc_loss=0.725, task_loss=0, contrastive_loss=0.096, total=4097.38, n_correct=2726.21, ppl=4.37, accuracy=66.535, wps=14306.5, ups=1.75, wpb=8194.8, bsz=297.3, num_updates=24400, lr=9.05357e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=57, gb_free=10.9, wall=18726
2023-08-12 07:19:25 | INFO | train_inner | epoch 017:    928 / 1474 loss=2.049, trans_loss=4.888, nll_loss=2.125, w2v_ctc_loss=0.719, task_loss=0, contrastive_loss=0.092, total=4105.01, n_correct=2728.77, ppl=4.36, accuracy=66.474, wps=14551.3, ups=1.77, wpb=8210, bsz=304.1, num_updates=24500, lr=9.03508e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=56, gb_free=16.7, wall=18783
2023-08-12 07:20:22 | INFO | train_inner | epoch 017:   1028 / 1474 loss=2.054, trans_loss=4.891, nll_loss=2.13, w2v_ctc_loss=0.725, task_loss=0, contrastive_loss=0.096, total=4105.88, n_correct=2727.9, ppl=4.38, accuracy=66.439, wps=14481.1, ups=1.76, wpb=8211.8, bsz=303.4, num_updates=24600, lr=9.0167e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=56, gb_free=16.1, wall=18840
2023-08-12 07:21:19 | INFO | train_inner | epoch 017:   1128 / 1474 loss=2.048, trans_loss=4.888, nll_loss=2.125, w2v_ctc_loss=0.719, task_loss=0, contrastive_loss=0.087, total=4095.58, n_correct=2723.65, ppl=4.36, accuracy=66.502, wps=14395.7, ups=1.76, wpb=8191.2, bsz=298.5, num_updates=24700, lr=8.99843e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=56, gb_free=15.6, wall=18896
2023-08-12 07:22:17 | INFO | train_inner | epoch 017:   1228 / 1474 loss=2.08, trans_loss=4.897, nll_loss=2.137, w2v_ctc_loss=0.716, task_loss=0, contrastive_loss=0.323, total=4162.14, n_correct=2753.41, ppl=4.4, accuracy=66.154, wps=14310.4, ups=1.72, wpb=8324.3, bsz=320.2, num_updates=24800, lr=8.98027e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=58, gb_free=16.1, wall=18955
2023-08-12 07:23:15 | INFO | train_inner | epoch 017:   1328 / 1474 loss=2.057, trans_loss=4.891, nll_loss=2.129, w2v_ctc_loss=0.713, task_loss=0, contrastive_loss=0.167, total=4149.03, n_correct=2759.42, ppl=4.37, accuracy=66.508, wps=14363.3, ups=1.73, wpb=8298.1, bsz=306.7, num_updates=24900, lr=8.96221e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=57, gb_free=16.9, wall=19012
2023-08-12 07:24:12 | INFO | train_inner | epoch 017:   1428 / 1474 loss=2.05, trans_loss=4.891, nll_loss=2.129, w2v_ctc_loss=0.72, task_loss=0, contrastive_loss=0.088, total=4117.13, n_correct=2738.28, ppl=4.37, accuracy=66.509, wps=14398.2, ups=1.75, wpb=8234.3, bsz=303.7, num_updates=25000, lr=8.94427e-05, gnorm=0.552, clip=0, loss_scale=32, train_wall=57, gb_free=17.5, wall=19070
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:0')
2023-08-12 07:24:39 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:3')
2023-08-12 07:25:01 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 4.092 | trans_loss 5.164 | nll_loss 2.427 | w2v_ctc_loss 1.343 | task_loss 0 | contrastive_loss 0.308 | total 4003.4 | n_correct 2661.2 | ppl 5.38 | accuracy 66.473 | uer 17.875 | wer 19.79 | raw_wer 19.79 | bleu 21.81 | wps 2306.2 | wpb 4003.4 | bsz 141.8 | num_updates 25046 | best_bleu 22.24
2023-08-12 07:25:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 25046 updates
2023-08-12 07:25:01 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_21.8102.pt
2023-08-12 07:25:05 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_21.8102.pt
2023-08-12 07:25:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_21.8102.pt (epoch 17 @ 25046 updates, score 21.81) (writing took 21.516013402491808 seconds)
2023-08-12 07:25:23 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-08-12 07:25:23 | INFO | train | epoch 017 | loss 2.056 | trans_loss 4.889 | nll_loss 2.126 | w2v_ctc_loss 0.722 | task_loss 0 | contrastive_loss 0.14 | total 4138.65 | n_correct 2751.09 | ppl 4.36 | accuracy 66.473 | wps 12689.4 | ups 1.53 | wpb 8277.3 | bsz 305.7 | num_updates 25046 | lr 8.93605e-05 | gnorm 0.546 | clip 0 | loss_scale 32 | train_wall 837 | gb_free 16.4 | wall 19141
2023-08-12 07:25:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 07:25:23 | INFO | fairseq.trainer | begin training epoch 18
2023-08-12 07:25:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 07:26:02 | INFO | train_inner | epoch 018:     54 / 1474 loss=2.049, trans_loss=4.882, nll_loss=2.118, w2v_ctc_loss=0.725, task_loss=0, contrastive_loss=0.097, total=4138.21, n_correct=2757.85, ppl=4.34, accuracy=66.644, wps=7509.8, ups=0.91, wpb=8276.4, bsz=303.2, num_updates=25100, lr=8.92644e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=57, gb_free=17.8, wall=19180
2023-08-12 07:27:00 | INFO | train_inner | epoch 018:    154 / 1474 loss=2.047, trans_loss=4.864, nll_loss=2.093, w2v_ctc_loss=0.697, task_loss=0, contrastive_loss=0.218, total=4158.88, n_correct=2786.05, ppl=4.27, accuracy=66.99, wps=14435.7, ups=1.74, wpb=8317.8, bsz=314, num_updates=25200, lr=8.90871e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=57, gb_free=16.7, wall=19237
2023-08-12 07:27:58 | INFO | train_inner | epoch 018:    254 / 1474 loss=2.032, trans_loss=4.863, nll_loss=2.092, w2v_ctc_loss=0.705, task_loss=0, contrastive_loss=0.087, total=4164.11, n_correct=2794.03, ppl=4.26, accuracy=67.098, wps=14365.8, ups=1.72, wpb=8328.2, bsz=312.5, num_updates=25300, lr=8.89108e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=57, gb_free=15.1, wall=19295
2023-08-12 07:28:55 | INFO | train_inner | epoch 018:    354 / 1474 loss=2.04, trans_loss=4.873, nll_loss=2.104, w2v_ctc_loss=0.711, task_loss=0, contrastive_loss=0.101, total=4163.13, n_correct=2783.09, ppl=4.3, accuracy=66.851, wps=14551.7, ups=1.75, wpb=8326.3, bsz=301.5, num_updates=25400, lr=8.87357e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=57, gb_free=17.6, wall=19353
2023-08-12 07:29:53 | INFO | train_inner | epoch 018:    454 / 1474 loss=2.052, trans_loss=4.876, nll_loss=2.109, w2v_ctc_loss=0.713, task_loss=0, contrastive_loss=0.191, total=4087.83, n_correct=2727.32, ppl=4.31, accuracy=66.718, wps=14154, ups=1.73, wpb=8175.7, bsz=295.3, num_updates=25500, lr=8.85615e-05, gnorm=0.549, clip=0, loss_scale=32, train_wall=57, gb_free=16.4, wall=19410
2023-08-12 07:30:50 | INFO | train_inner | epoch 018:    554 / 1474 loss=2.036, trans_loss=4.863, nll_loss=2.093, w2v_ctc_loss=0.705, task_loss=0, contrastive_loss=0.1, total=4204.41, n_correct=2819.15, ppl=4.27, accuracy=67.052, wps=14658.8, ups=1.74, wpb=8408.8, bsz=328, num_updates=25600, lr=8.83883e-05, gnorm=0.536, clip=0, loss_scale=32, train_wall=57, gb_free=17.5, wall=19468
2023-08-12 07:31:48 | INFO | train_inner | epoch 018:    654 / 1474 loss=2.053, trans_loss=4.88, nll_loss=2.115, w2v_ctc_loss=0.719, task_loss=0, contrastive_loss=0.169, total=4096.81, n_correct=2732.14, ppl=4.33, accuracy=66.689, wps=14289.9, ups=1.74, wpb=8193.6, bsz=298.9, num_updates=25700, lr=8.82162e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=57, gb_free=16.7, wall=19525
2023-08-12 07:32:45 | INFO | train_inner | epoch 018:    754 / 1474 loss=2.065, trans_loss=4.881, nll_loss=2.116, w2v_ctc_loss=0.722, task_loss=0, contrastive_loss=0.259, total=4208.29, n_correct=2803.6, ppl=4.34, accuracy=66.621, wps=14726.9, ups=1.75, wpb=8416.6, bsz=322.8, num_updates=25800, lr=8.80451e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=57, gb_free=17.8, wall=19582
2023-08-12 07:33:42 | INFO | train_inner | epoch 018:    854 / 1474 loss=2.04, trans_loss=4.877, nll_loss=2.11, w2v_ctc_loss=0.712, task_loss=0, contrastive_loss=0.078, total=4166.81, n_correct=2782.39, ppl=4.32, accuracy=66.775, wps=14533.4, ups=1.74, wpb=8333.6, bsz=301.9, num_updates=25900, lr=8.7875e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=57, gb_free=12.9, wall=19640
2023-08-12 07:34:39 | INFO | train_inner | epoch 018:    954 / 1474 loss=2.037, trans_loss=4.869, nll_loss=2.1, w2v_ctc_loss=0.703, task_loss=0, contrastive_loss=0.101, total=4142.65, n_correct=2774.13, ppl=4.29, accuracy=66.965, wps=14610.8, ups=1.76, wpb=8285.3, bsz=316.1, num_updates=26000, lr=8.77058e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=56, gb_free=14.9, wall=19696
2023-08-12 07:34:39 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 07:35:01 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 4.096 | trans_loss 5.168 | nll_loss 2.429 | w2v_ctc_loss 1.356 | task_loss 0 | contrastive_loss 0.29 | total 4003.4 | n_correct 2661.8 | ppl 5.39 | accuracy 66.488 | uer 18.292 | wer 20.238 | raw_wer 20.238 | bleu 22.34 | wps 2323.3 | wpb 4003.4 | bsz 141.8 | num_updates 26000 | best_bleu 22.34
2023-08-12 07:35:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26000 updates
2023-08-12 07:35:01 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_18_26000.pt
2023-08-12 07:35:05 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_18_26000.pt
2023-08-12 07:35:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_18_26000.pt (epoch 18 @ 26000 updates, score 22.34) (writing took 46.68009775876999 seconds)
2023-08-12 07:36:47 | INFO | train_inner | epoch 018:   1054 / 1474 loss=2.037, trans_loss=4.873, nll_loss=2.106, w2v_ctc_loss=0.704, task_loss=0, contrastive_loss=0.087, total=4137.77, n_correct=2766.65, ppl=4.3, accuracy=66.863, wps=6460.3, ups=0.78, wpb=8275.5, bsz=300.5, num_updates=26100, lr=8.75376e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=57, gb_free=17.4, wall=19824
2023-08-12 07:37:44 | INFO | train_inner | epoch 018:   1154 / 1474 loss=2.048, trans_loss=4.866, nll_loss=2.097, w2v_ctc_loss=0.707, task_loss=0, contrastive_loss=0.195, total=4153.69, n_correct=2778.98, ppl=4.28, accuracy=66.904, wps=14519.5, ups=1.75, wpb=8307.4, bsz=314.9, num_updates=26200, lr=8.73704e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=57, gb_free=15.3, wall=19882
2023-08-12 07:38:41 | INFO | train_inner | epoch 018:   1254 / 1474 loss=2.04, trans_loss=4.882, nll_loss=2.118, w2v_ctc_loss=0.711, task_loss=0, contrastive_loss=0.081, total=4087.62, n_correct=2726.22, ppl=4.34, accuracy=66.695, wps=14397.3, ups=1.76, wpb=8175.2, bsz=287.1, num_updates=26300, lr=8.72041e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=56, gb_free=16.7, wall=19938
2023-08-12 07:39:38 | INFO | train_inner | epoch 018:   1354 / 1474 loss=2.054, trans_loss=4.885, nll_loss=2.121, w2v_ctc_loss=0.731, task_loss=0, contrastive_loss=0.109, total=4070.69, n_correct=2706.73, ppl=4.35, accuracy=66.493, wps=14265, ups=1.75, wpb=8141.4, bsz=291.7, num_updates=26400, lr=8.70388e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=56, gb_free=17.4, wall=19996
2023-08-12 07:40:36 | INFO | train_inner | epoch 018:   1454 / 1474 loss=2.045, trans_loss=4.88, nll_loss=2.115, w2v_ctc_loss=0.72, task_loss=0, contrastive_loss=0.093, total=4113.2, n_correct=2744.07, ppl=4.33, accuracy=66.714, wps=14216.6, ups=1.73, wpb=8226.4, bsz=297.5, num_updates=26500, lr=8.68744e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=57, gb_free=16.5, wall=20053
2023-08-12 07:40:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 07:41:10 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 4.091 | trans_loss 5.159 | nll_loss 2.421 | w2v_ctc_loss 1.358 | task_loss 0 | contrastive_loss 0.298 | total 4003.4 | n_correct 2669.4 | ppl 5.35 | accuracy 66.678 | uer 17.684 | wer 19.611 | raw_wer 19.611 | bleu 22.26 | wps 2272.2 | wpb 4003.4 | bsz 141.8 | num_updates 26520 | best_bleu 22.34
2023-08-12 07:41:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26520 updates
2023-08-12 07:41:10 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.2604.pt
2023-08-12 07:41:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.2604.pt
2023-08-12 07:41:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.2604.pt (epoch 18 @ 26520 updates, score 22.26) (writing took 21.419905012473464 seconds)
2023-08-12 07:41:32 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-08-12 07:41:32 | INFO | train | epoch 018 | loss 2.045 | trans_loss 4.874 | nll_loss 2.106 | w2v_ctc_loss 0.711 | task_loss 0 | contrastive_loss 0.137 | total 4138.65 | n_correct 2764.93 | ppl 4.31 | accuracy 66.807 | wps 12594.2 | ups 1.52 | wpb 8277.3 | bsz 305.7 | num_updates 26520 | lr 8.68417e-05 | gnorm 0.543 | clip 0 | loss_scale 64 | train_wall 838 | gb_free 16 | wall 20109
2023-08-12 07:41:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 07:41:32 | INFO | fairseq.trainer | begin training epoch 19
2023-08-12 07:41:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 07:42:25 | INFO | train_inner | epoch 019:     80 / 1474 loss=2.036, trans_loss=4.858, nll_loss=2.086, w2v_ctc_loss=0.703, task_loss=0, contrastive_loss=0.143, total=4102.06, n_correct=2750.48, ppl=4.25, accuracy=67.051, wps=7507.6, ups=0.92, wpb=8204.1, bsz=296.9, num_updates=26600, lr=8.6711e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=57, gb_free=17.5, wall=20163
2023-08-12 07:43:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-12 07:43:24 | INFO | train_inner | epoch 019:    181 / 1474 loss=2.039, trans_loss=4.856, nll_loss=2.083, w2v_ctc_loss=0.715, task_loss=0, contrastive_loss=0.121, total=4210.09, n_correct=2828.72, ppl=4.24, accuracy=67.189, wps=14446.7, ups=1.72, wpb=8420.2, bsz=319.1, num_updates=26700, lr=8.65485e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=58, gb_free=12, wall=20221
2023-08-12 07:44:21 | INFO | train_inner | epoch 019:    281 / 1474 loss=2.026, trans_loss=4.853, nll_loss=2.079, w2v_ctc_loss=0.702, task_loss=0, contrastive_loss=0.079, total=4187.37, n_correct=2817.35, ppl=4.22, accuracy=67.282, wps=14693.4, ups=1.75, wpb=8374.7, bsz=307, num_updates=26800, lr=8.63868e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=56, gb_free=17.6, wall=20278
2023-08-12 07:45:18 | INFO | train_inner | epoch 019:    381 / 1474 loss=2.038, trans_loss=4.852, nll_loss=2.079, w2v_ctc_loss=0.695, task_loss=0, contrastive_loss=0.186, total=4170.67, n_correct=2799.93, ppl=4.22, accuracy=67.134, wps=14493.7, ups=1.74, wpb=8341.3, bsz=310.5, num_updates=26900, lr=8.62261e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=57, gb_free=17.3, wall=20336
2023-08-12 07:46:15 | INFO | train_inner | epoch 019:    481 / 1474 loss=2.035, trans_loss=4.862, nll_loss=2.09, w2v_ctc_loss=0.71, task_loss=0, contrastive_loss=0.094, total=4115.22, n_correct=2760.98, ppl=4.26, accuracy=67.092, wps=14385.6, ups=1.75, wpb=8230.4, bsz=301.9, num_updates=27000, lr=8.60663e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=57, gb_free=15.2, wall=20393
2023-08-12 07:47:13 | INFO | train_inner | epoch 019:    581 / 1474 loss=2.033, trans_loss=4.855, nll_loss=2.082, w2v_ctc_loss=0.699, task_loss=0, contrastive_loss=0.155, total=4129.22, n_correct=2777.02, ppl=4.24, accuracy=67.253, wps=14322.6, ups=1.73, wpb=8258.4, bsz=306, num_updates=27100, lr=8.59074e-05, gnorm=0.537, clip=0, loss_scale=32, train_wall=57, gb_free=15.9, wall=20450
2023-08-12 07:48:10 | INFO | train_inner | epoch 019:    681 / 1474 loss=2.023, trans_loss=4.859, nll_loss=2.088, w2v_ctc_loss=0.685, task_loss=0, contrastive_loss=0.085, total=4197.2, n_correct=2823.96, ppl=4.25, accuracy=67.282, wps=14719, ups=1.75, wpb=8394.4, bsz=320.8, num_updates=27200, lr=8.57493e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=57, gb_free=14.8, wall=20507
2023-08-12 07:49:07 | INFO | train_inner | epoch 019:    781 / 1474 loss=2.034, trans_loss=4.86, nll_loss=2.088, w2v_ctc_loss=0.709, task_loss=0, contrastive_loss=0.098, total=4142.6, n_correct=2779.35, ppl=4.25, accuracy=67.092, wps=14446.6, ups=1.74, wpb=8285.2, bsz=305.1, num_updates=27300, lr=8.55921e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=57, gb_free=16.3, wall=20565
2023-08-12 07:50:05 | INFO | train_inner | epoch 019:    881 / 1474 loss=2.034, trans_loss=4.865, nll_loss=2.095, w2v_ctc_loss=0.709, task_loss=0, contrastive_loss=0.082, total=4153.47, n_correct=2781.95, ppl=4.27, accuracy=66.979, wps=14519.6, ups=1.75, wpb=8306.9, bsz=303.1, num_updates=27400, lr=8.54358e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=57, gb_free=16.3, wall=20622
2023-08-12 07:51:03 | INFO | train_inner | epoch 019:    981 / 1474 loss=2.061, trans_loss=4.874, nll_loss=2.107, w2v_ctc_loss=0.706, task_loss=0, contrastive_loss=0.315, total=4101.29, n_correct=2738.34, ppl=4.31, accuracy=66.768, wps=14054.8, ups=1.71, wpb=8202.6, bsz=309.9, num_updates=27500, lr=8.52803e-05, gnorm=0.557, clip=0, loss_scale=32, train_wall=58, gb_free=16.7, wall=20680
2023-08-12 07:52:00 | INFO | train_inner | epoch 019:   1081 / 1474 loss=2.038, trans_loss=4.87, nll_loss=2.101, w2v_ctc_loss=0.706, task_loss=0, contrastive_loss=0.123, total=4036.97, n_correct=2700.41, ppl=4.29, accuracy=66.892, wps=14143.1, ups=1.75, wpb=8073.9, bsz=291, num_updates=27600, lr=8.51257e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=57, gb_free=14.8, wall=20737
2023-08-12 07:52:58 | INFO | train_inner | epoch 019:   1181 / 1474 loss=2.054, trans_loss=4.872, nll_loss=2.103, w2v_ctc_loss=0.712, task_loss=0, contrastive_loss=0.206, total=4137.49, n_correct=2761.2, ppl=4.3, accuracy=66.736, wps=14296.1, ups=1.73, wpb=8275, bsz=307.7, num_updates=27700, lr=8.49719e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=57, gb_free=15, wall=20795
2023-08-12 07:53:55 | INFO | train_inner | epoch 019:   1281 / 1474 loss=2.033, trans_loss=4.866, nll_loss=2.097, w2v_ctc_loss=0.699, task_loss=0, contrastive_loss=0.105, total=4141.89, n_correct=2775.91, ppl=4.28, accuracy=67.02, wps=14608.4, ups=1.76, wpb=8283.8, bsz=300.1, num_updates=27800, lr=8.48189e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=56, gb_free=15.6, wall=20852
2023-08-12 07:54:52 | INFO | train_inner | epoch 019:   1381 / 1474 loss=2.034, trans_loss=4.865, nll_loss=2.095, w2v_ctc_loss=0.706, task_loss=0, contrastive_loss=0.09, total=4133.26, n_correct=2769.03, ppl=4.27, accuracy=66.994, wps=14380, ups=1.74, wpb=8266.5, bsz=301, num_updates=27900, lr=8.46668e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=57, gb_free=16.2, wall=20910
2023-08-12 07:55:45 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 07:56:07 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 4.072 | trans_loss 5.15 | nll_loss 2.406 | w2v_ctc_loss 1.32 | task_loss 0 | contrastive_loss 0.294 | total 4003.4 | n_correct 2667.5 | ppl 5.3 | accuracy 66.631 | uer 17.631 | wer 19.488 | raw_wer 19.488 | bleu 22.4 | wps 2374.4 | wpb 4003.4 | bsz 141.8 | num_updates 27993 | best_bleu 22.4
2023-08-12 07:56:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 27993 updates
2023-08-12 07:56:07 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 07:56:23 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 07:56:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt (epoch 19 @ 27993 updates, score 22.4) (writing took 29.01845351792872 seconds)
2023-08-12 07:56:37 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-08-12 07:56:37 | INFO | train | epoch 019 | loss 2.036 | trans_loss 4.861 | nll_loss 2.09 | w2v_ctc_loss 0.704 | task_loss 0 | contrastive_loss 0.134 | total 4137.6 | n_correct 2775.4 | ppl 4.26 | accuracy 67.078 | wps 13466.3 | ups 1.63 | wpb 8275.2 | bsz 305.3 | num_updates 27993 | lr 8.4526e-05 | gnorm 0.543 | clip 0 | loss_scale 32 | train_wall 838 | gb_free 17.4 | wall 21015
2023-08-12 07:56:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 07:56:37 | INFO | fairseq.trainer | begin training epoch 20
2023-08-12 07:56:37 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 07:56:49 | INFO | train_inner | epoch 020:      7 / 1474 loss=2.034, trans_loss=4.855, nll_loss=2.083, w2v_ctc_loss=0.697, task_loss=0, contrastive_loss=0.174, total=4119.08, n_correct=2771.32, ppl=4.24, accuracy=67.28, wps=7036.8, ups=0.85, wpb=8238.2, bsz=304.1, num_updates=28000, lr=8.45154e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=57, gb_free=16.2, wall=21027
2023-08-12 07:56:49 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 07:57:12 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 4.073 | trans_loss 5.155 | nll_loss 2.411 | w2v_ctc_loss 1.316 | task_loss 0 | contrastive_loss 0.292 | total 4003.4 | n_correct 2663.1 | ppl 5.32 | accuracy 66.521 | uer 17.67 | wer 19.619 | raw_wer 19.619 | bleu 22.32 | wps 2243.1 | wpb 4003.4 | bsz 141.8 | num_updates 28000 | best_bleu 22.4
2023-08-12 07:57:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 28000 updates
2023-08-12 07:57:12 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_20_28000.pt
2023-08-12 07:57:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_20_28000.pt
2023-08-12 07:57:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_20_28000.pt (epoch 20 @ 28000 updates, score 22.32) (writing took 17.337022453546524 seconds)
2023-08-12 07:58:27 | INFO | train_inner | epoch 020:    107 / 1474 loss=2.016, trans_loss=4.838, nll_loss=2.059, w2v_ctc_loss=0.685, task_loss=0, contrastive_loss=0.097, total=4195.03, n_correct=2834.03, ppl=4.17, accuracy=67.557, wps=8540.9, ups=1.02, wpb=8390.1, bsz=313.7, num_updates=28100, lr=8.43649e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=57, gb_free=15.2, wall=21125
2023-08-12 07:59:25 | INFO | train_inner | epoch 020:    207 / 1474 loss=2.026, trans_loss=4.845, nll_loss=2.068, w2v_ctc_loss=0.694, task_loss=0, contrastive_loss=0.148, total=4154.14, n_correct=2799.83, ppl=4.19, accuracy=67.399, wps=14462.9, ups=1.74, wpb=8308.3, bsz=301, num_updates=28200, lr=8.42152e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=57, gb_free=17.3, wall=21182
2023-08-12 08:00:22 | INFO | train_inner | epoch 020:    307 / 1474 loss=2.02, trans_loss=4.84, nll_loss=2.063, w2v_ctc_loss=0.696, task_loss=0, contrastive_loss=0.087, total=4188.05, n_correct=2829.78, ppl=4.18, accuracy=67.568, wps=14549.3, ups=1.74, wpb=8376.1, bsz=326.2, num_updates=28300, lr=8.40663e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=57, gb_free=15.6, wall=21240
2023-08-12 08:01:20 | INFO | train_inner | epoch 020:    407 / 1474 loss=2.014, trans_loss=4.837, nll_loss=2.059, w2v_ctc_loss=0.686, task_loss=0, contrastive_loss=0.086, total=4115.16, n_correct=2779.71, ppl=4.17, accuracy=67.548, wps=14332.6, ups=1.74, wpb=8230.3, bsz=297, num_updates=28400, lr=8.39181e-05, gnorm=0.537, clip=0, loss_scale=32, train_wall=57, gb_free=15.6, wall=21297
2023-08-12 08:02:17 | INFO | train_inner | epoch 020:    507 / 1474 loss=2.031, trans_loss=4.852, nll_loss=2.078, w2v_ctc_loss=0.693, task_loss=0, contrastive_loss=0.172, total=4108.46, n_correct=2764.65, ppl=4.22, accuracy=67.292, wps=14357.2, ups=1.75, wpb=8216.9, bsz=300.4, num_updates=28500, lr=8.37708e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=57, gb_free=16.5, wall=21355
2023-08-12 08:03:14 | INFO | train_inner | epoch 020:    607 / 1474 loss=2.035, trans_loss=4.852, nll_loss=2.077, w2v_ctc_loss=0.696, task_loss=0, contrastive_loss=0.176, total=4094.9, n_correct=2753.83, ppl=4.22, accuracy=67.25, wps=14420.4, ups=1.76, wpb=8189.8, bsz=296.1, num_updates=28600, lr=8.36242e-05, gnorm=0.553, clip=0, loss_scale=32, train_wall=56, gb_free=12.2, wall=21411
2023-08-12 08:04:11 | INFO | train_inner | epoch 020:    707 / 1474 loss=2.023, trans_loss=4.852, nll_loss=2.077, w2v_ctc_loss=0.7, task_loss=0, contrastive_loss=0.079, total=4140.23, n_correct=2785.45, ppl=4.22, accuracy=67.278, wps=14451.2, ups=1.75, wpb=8280.5, bsz=300.6, num_updates=28700, lr=8.34784e-05, gnorm=0.536, clip=0, loss_scale=32, train_wall=57, gb_free=16.2, wall=21469
2023-08-12 08:05:08 | INFO | train_inner | epoch 020:    807 / 1474 loss=2.025, trans_loss=4.852, nll_loss=2.078, w2v_ctc_loss=0.704, task_loss=0, contrastive_loss=0.082, total=4140.66, n_correct=2788.57, ppl=4.22, accuracy=67.346, wps=14570.5, ups=1.76, wpb=8281.3, bsz=305.6, num_updates=28800, lr=8.33333e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=56, gb_free=17.6, wall=21526
2023-08-12 08:06:06 | INFO | train_inner | epoch 020:    907 / 1474 loss=2.062, trans_loss=4.86, nll_loss=2.089, w2v_ctc_loss=0.699, task_loss=0, contrastive_loss=0.371, total=4157.15, n_correct=2787.54, ppl=4.25, accuracy=67.054, wps=14376.1, ups=1.73, wpb=8314.3, bsz=322.6, num_updates=28900, lr=8.3189e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=57, gb_free=17.8, wall=21583
2023-08-12 08:07:03 | INFO | train_inner | epoch 020:   1007 / 1474 loss=2.021, trans_loss=4.85, nll_loss=2.076, w2v_ctc_loss=0.69, task_loss=0, contrastive_loss=0.089, total=4171.86, n_correct=2809.86, ppl=4.22, accuracy=67.353, wps=14590.7, ups=1.75, wpb=8343.7, bsz=308.6, num_updates=29000, lr=8.30455e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=57, gb_free=15.7, wall=21641
2023-08-12 08:08:01 | INFO | train_inner | epoch 020:   1107 / 1474 loss=2.041, trans_loss=4.853, nll_loss=2.079, w2v_ctc_loss=0.695, task_loss=0, contrastive_loss=0.225, total=4162.96, n_correct=2799.58, ppl=4.23, accuracy=67.25, wps=14417.1, ups=1.73, wpb=8325.9, bsz=314.9, num_updates=29100, lr=8.29027e-05, gnorm=0.551, clip=0, loss_scale=64, train_wall=57, gb_free=17.1, wall=21698
2023-08-12 08:08:58 | INFO | train_inner | epoch 020:   1207 / 1474 loss=2.024, trans_loss=4.843, nll_loss=2.066, w2v_ctc_loss=0.708, task_loss=0, contrastive_loss=0.077, total=4033.74, n_correct=2711.79, ppl=4.19, accuracy=67.228, wps=14139.4, ups=1.75, wpb=8067.5, bsz=285.2, num_updates=29200, lr=8.27606e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=56, gb_free=17.3, wall=21755
2023-08-12 08:09:55 | INFO | train_inner | epoch 020:   1307 / 1474 loss=2.024, trans_loss=4.853, nll_loss=2.079, w2v_ctc_loss=0.698, task_loss=0, contrastive_loss=0.083, total=4124.42, n_correct=2777.48, ppl=4.23, accuracy=67.342, wps=14392.6, ups=1.74, wpb=8248.8, bsz=297, num_updates=29300, lr=8.26192e-05, gnorm=0.543, clip=0, loss_scale=64, train_wall=57, gb_free=15.9, wall=21813
2023-08-12 08:10:53 | INFO | train_inner | epoch 020:   1407 / 1474 loss=2.022, trans_loss=4.852, nll_loss=2.079, w2v_ctc_loss=0.694, task_loss=0, contrastive_loss=0.079, total=4114.1, n_correct=2767.32, ppl=4.22, accuracy=67.264, wps=14370.9, ups=1.75, wpb=8228.2, bsz=293.7, num_updates=29400, lr=8.24786e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=57, gb_free=14.6, wall=21870
2023-08-12 08:11:31 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 08:11:54 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 4.071 | trans_loss 5.156 | nll_loss 2.41 | w2v_ctc_loss 1.305 | task_loss 0 | contrastive_loss 0.291 | total 4003.4 | n_correct 2671.3 | ppl 5.32 | accuracy 66.726 | uer 17.535 | wer 19.447 | raw_wer 19.447 | bleu 22.35 | wps 2228.2 | wpb 4003.4 | bsz 141.8 | num_updates 29467 | best_bleu 22.4
2023-08-12 08:11:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 29467 updates
2023-08-12 08:11:54 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.3507.pt
2023-08-12 08:11:58 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.3507.pt
2023-08-12 08:12:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.3507.pt (epoch 20 @ 29467 updates, score 22.35) (writing took 26.54534723609686 seconds)
2023-08-12 08:12:21 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-08-12 08:12:21 | INFO | train | epoch 020 | loss 2.027 | trans_loss 4.848 | nll_loss 2.074 | w2v_ctc_loss 0.695 | task_loss 0 | contrastive_loss 0.133 | total 4138.65 | n_correct 2787.01 | ppl 4.21 | accuracy 67.341 | wps 12924 | ups 1.56 | wpb 8277.3 | bsz 305.7 | num_updates 29467 | lr 8.23848e-05 | gnorm 0.543 | clip 0 | loss_scale 64 | train_wall 838 | gb_free 16.2 | wall 21959
2023-08-12 08:12:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 08:12:21 | INFO | fairseq.trainer | begin training epoch 21
2023-08-12 08:12:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 08:12:48 | INFO | train_inner | epoch 021:     33 / 1474 loss=2.036, trans_loss=4.848, nll_loss=2.074, w2v_ctc_loss=0.695, task_loss=0, contrastive_loss=0.2, total=4155.01, n_correct=2798.2, ppl=4.21, accuracy=67.345, wps=7216.5, ups=0.87, wpb=8310, bsz=317.6, num_updates=29500, lr=8.23387e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=57, gb_free=16.4, wall=21985
2023-08-12 08:13:45 | INFO | train_inner | epoch 021:    133 / 1474 loss=2.022, trans_loss=4.829, nll_loss=2.048, w2v_ctc_loss=0.682, task_loss=0, contrastive_loss=0.189, total=4186.67, n_correct=2833.85, ppl=4.14, accuracy=67.687, wps=14647, ups=1.75, wpb=8373.3, bsz=317.4, num_updates=29600, lr=8.21995e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=57, gb_free=13.3, wall=22042
2023-08-12 08:14:42 | INFO | train_inner | epoch 021:    233 / 1474 loss=2.01, trans_loss=4.83, nll_loss=2.05, w2v_ctc_loss=0.673, task_loss=0, contrastive_loss=0.144, total=4166.37, n_correct=2823.75, ppl=4.14, accuracy=67.775, wps=14562.8, ups=1.75, wpb=8332.7, bsz=315.2, num_updates=29700, lr=8.2061e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=57, gb_free=14.3, wall=22099
2023-08-12 08:15:39 | INFO | train_inner | epoch 021:    333 / 1474 loss=2.02, trans_loss=4.835, nll_loss=2.056, w2v_ctc_loss=0.691, task_loss=0, contrastive_loss=0.145, total=4132.25, n_correct=2791.58, ppl=4.16, accuracy=67.556, wps=14429.7, ups=1.75, wpb=8264.5, bsz=305, num_updates=29800, lr=8.19232e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=57, gb_free=16.7, wall=22157
2023-08-12 08:16:36 | INFO | train_inner | epoch 021:    433 / 1474 loss=2.007, trans_loss=4.831, nll_loss=2.051, w2v_ctc_loss=0.678, task_loss=0, contrastive_loss=0.079, total=4195.53, n_correct=2841.87, ppl=4.15, accuracy=67.736, wps=14684.2, ups=1.75, wpb=8391.1, bsz=311.6, num_updates=29900, lr=8.17861e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=57, gb_free=16.5, wall=22214
2023-08-12 08:17:34 | INFO | train_inner | epoch 021:    533 / 1474 loss=2.007, trans_loss=4.824, nll_loss=2.042, w2v_ctc_loss=0.687, task_loss=0, contrastive_loss=0.074, total=4085.05, n_correct=2774.6, ppl=4.12, accuracy=67.921, wps=14272.1, ups=1.75, wpb=8170.1, bsz=296.1, num_updates=30000, lr=8.16497e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=57, gb_free=16.3, wall=22271
2023-08-12 08:17:34 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 08:17:56 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 4.085 | trans_loss 5.161 | nll_loss 2.417 | w2v_ctc_loss 1.338 | task_loss 0 | contrastive_loss 0.294 | total 4003.4 | n_correct 2667 | ppl 5.34 | accuracy 66.618 | uer 17.681 | wer 19.679 | raw_wer 19.679 | bleu 22 | wps 2327.2 | wpb 4003.4 | bsz 141.8 | num_updates 30000 | best_bleu 22.4
2023-08-12 08:17:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30000 updates
2023-08-12 08:17:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_21_30000.pt
2023-08-12 08:18:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_21_30000.pt
2023-08-12 08:18:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_21_30000.pt (epoch 21 @ 30000 updates, score 22.0) (writing took 23.67975357361138 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:0')
2023-08-12 08:19:18 | INFO | train_inner | epoch 021:    633 / 1474 loss=2.026, trans_loss=4.831, nll_loss=2.051, w2v_ctc_loss=0.682, task_loss=0, contrastive_loss=0.243, total=4220.3, n_correct=2857.56, ppl=4.14, accuracy=67.71, wps=8057.4, ups=0.95, wpb=8440.6, bsz=315.8, num_updates=30100, lr=8.15139e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=57, gb_free=15.8, wall=22376
2023-08-12 08:20:16 | INFO | train_inner | epoch 021:    733 / 1474 loss=2.02, trans_loss=4.843, nll_loss=2.067, w2v_ctc_loss=0.688, task_loss=0, contrastive_loss=0.107, total=4148.18, n_correct=2800.04, ppl=4.19, accuracy=67.5, wps=14388.2, ups=1.73, wpb=8296.4, bsz=308.3, num_updates=30200, lr=8.13788e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=57, gb_free=12.2, wall=22434
2023-08-12 08:21:14 | INFO | train_inner | epoch 021:    833 / 1474 loss=2.021, trans_loss=4.844, nll_loss=2.068, w2v_ctc_loss=0.688, task_loss=0, contrastive_loss=0.118, total=4062.56, n_correct=2738.26, ppl=4.19, accuracy=67.402, wps=14167.7, ups=1.74, wpb=8125.1, bsz=293, num_updates=30300, lr=8.12444e-05, gnorm=0.549, clip=0, loss_scale=64, train_wall=57, gb_free=16.3, wall=22491
2023-08-12 08:22:11 | INFO | train_inner | epoch 021:    933 / 1474 loss=2.013, trans_loss=4.834, nll_loss=2.054, w2v_ctc_loss=0.687, task_loss=0, contrastive_loss=0.092, total=4103.66, n_correct=2776.9, ppl=4.15, accuracy=67.669, wps=14390.2, ups=1.75, wpb=8207.3, bsz=301.5, num_updates=30400, lr=8.11107e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=56, gb_free=17.2, wall=22548
2023-08-12 08:23:07 | INFO | train_inner | epoch 021:   1033 / 1474 loss=2.017, trans_loss=4.844, nll_loss=2.068, w2v_ctc_loss=0.69, task_loss=0, contrastive_loss=0.089, total=4100.54, n_correct=2769.51, ppl=4.19, accuracy=67.54, wps=14460.6, ups=1.76, wpb=8201.1, bsz=298.2, num_updates=30500, lr=8.09776e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=56, gb_free=17.9, wall=22605
2023-08-12 08:24:05 | INFO | train_inner | epoch 021:   1133 / 1474 loss=2.015, trans_loss=4.836, nll_loss=2.056, w2v_ctc_loss=0.689, task_loss=0, contrastive_loss=0.092, total=4119.98, n_correct=2784.9, ppl=4.16, accuracy=67.595, wps=14310.9, ups=1.74, wpb=8240, bsz=294, num_updates=30600, lr=8.08452e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=57, gb_free=17.9, wall=22662
2023-08-12 08:25:02 | INFO | train_inner | epoch 021:   1233 / 1474 loss=2.021, trans_loss=4.838, nll_loss=2.061, w2v_ctc_loss=0.686, task_loss=0, contrastive_loss=0.144, total=4161.49, n_correct=2811.65, ppl=4.17, accuracy=67.564, wps=14585.7, ups=1.75, wpb=8323, bsz=313, num_updates=30700, lr=8.07134e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=57, gb_free=16.8, wall=22719
2023-08-12 08:25:59 | INFO | train_inner | epoch 021:   1333 / 1474 loss=2.019, trans_loss=4.84, nll_loss=2.064, w2v_ctc_loss=0.689, task_loss=0, contrastive_loss=0.106, total=4141.76, n_correct=2797.94, ppl=4.18, accuracy=67.554, wps=14423.1, ups=1.74, wpb=8283.5, bsz=311.7, num_updates=30800, lr=8.05823e-05, gnorm=0.547, clip=0, loss_scale=128, train_wall=57, gb_free=17.4, wall=22777
2023-08-12 08:26:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-12 08:26:58 | INFO | train_inner | epoch 021:   1434 / 1474 loss=2.028, trans_loss=4.847, nll_loss=2.071, w2v_ctc_loss=0.708, task_loss=0, contrastive_loss=0.094, total=4116, n_correct=2770.15, ppl=4.2, accuracy=67.302, wps=14163.7, ups=1.72, wpb=8232, bsz=296.9, num_updates=30900, lr=8.04518e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=58, gb_free=15.7, wall=22835
2023-08-12 08:27:20 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:6')
2023-08-12 08:27:43 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 4.092 | trans_loss 5.165 | nll_loss 2.425 | w2v_ctc_loss 1.35 | task_loss 0 | contrastive_loss 0.294 | total 4003.4 | n_correct 2667.6 | ppl 5.37 | accuracy 66.633 | uer 17.931 | wer 19.936 | raw_wer 19.936 | bleu 22.27 | wps 2260 | wpb 4003.4 | bsz 141.8 | num_updates 30940 | best_bleu 22.4
2023-08-12 08:27:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30940 updates
2023-08-12 08:27:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.2707.pt
2023-08-12 08:27:47 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.2707.pt
2023-08-12 08:28:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.2707.pt (epoch 21 @ 30940 updates, score 22.27) (writing took 22.50245606712997 seconds)
2023-08-12 08:28:06 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-08-12 08:28:06 | INFO | train | epoch 021 | loss 2.018 | trans_loss 4.836 | nll_loss 2.058 | w2v_ctc_loss 0.687 | task_loss 0 | contrastive_loss 0.127 | total 4137.4 | n_correct 2797.17 | ppl 4.16 | accuracy 67.607 | wps 12896.8 | ups 1.56 | wpb 8274.8 | bsz 305.2 | num_updates 30940 | lr 8.03998e-05 | gnorm 0.545 | clip 0 | loss_scale 64 | train_wall 836 | gb_free 15.5 | wall 22904
2023-08-12 08:28:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 08:28:07 | INFO | fairseq.trainer | begin training epoch 22
2023-08-12 08:28:07 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 08:28:48 | INFO | train_inner | epoch 022:     60 / 1474 loss=2.005, trans_loss=4.822, nll_loss=2.04, w2v_ctc_loss=0.685, task_loss=0, contrastive_loss=0.074, total=4128.84, n_correct=2805.22, ppl=4.11, accuracy=67.942, wps=7465.7, ups=0.9, wpb=8257.7, bsz=297.7, num_updates=31000, lr=8.03219e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=57, gb_free=14.3, wall=22946
2023-08-12 08:29:45 | INFO | train_inner | epoch 022:    160 / 1474 loss=2.013, trans_loss=4.82, nll_loss=2.037, w2v_ctc_loss=0.68, task_loss=0, contrastive_loss=0.155, total=4123.35, n_correct=2798.51, ppl=4.1, accuracy=67.87, wps=14449.3, ups=1.75, wpb=8246.7, bsz=310.3, num_updates=31100, lr=8.01927e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=57, gb_free=14.9, wall=23003
2023-08-12 08:30:43 | INFO | train_inner | epoch 022:    260 / 1474 loss=1.998, trans_loss=4.814, nll_loss=2.029, w2v_ctc_loss=0.666, task_loss=0, contrastive_loss=0.097, total=4267.16, n_correct=2909.25, ppl=4.08, accuracy=68.178, wps=14765.3, ups=1.73, wpb=8534.3, bsz=329.9, num_updates=31200, lr=8.00641e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=57, gb_free=16.7, wall=23060
2023-08-12 08:31:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-12 08:31:42 | INFO | train_inner | epoch 022:    361 / 1474 loss=2.015, trans_loss=4.826, nll_loss=2.044, w2v_ctc_loss=0.684, task_loss=0, contrastive_loss=0.14, total=4154.17, n_correct=2813.14, ppl=4.12, accuracy=67.718, wps=14159.5, ups=1.7, wpb=8308.3, bsz=302.4, num_updates=31300, lr=7.99361e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=58, gb_free=15.3, wall=23119
2023-08-12 08:32:39 | INFO | train_inner | epoch 022:    461 / 1474 loss=2.015, trans_loss=4.826, nll_loss=2.044, w2v_ctc_loss=0.682, task_loss=0, contrastive_loss=0.138, total=4132.96, n_correct=2801.97, ppl=4.12, accuracy=67.796, wps=14373.1, ups=1.74, wpb=8265.9, bsz=297.5, num_updates=31400, lr=7.98087e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=57, gb_free=17, wall=23177
2023-08-12 08:33:36 | INFO | train_inner | epoch 022:    561 / 1474 loss=2.005, trans_loss=4.82, nll_loss=2.037, w2v_ctc_loss=0.681, task_loss=0, contrastive_loss=0.084, total=4158.17, n_correct=2825.76, ppl=4.1, accuracy=67.957, wps=14532.2, ups=1.75, wpb=8316.3, bsz=307.8, num_updates=31500, lr=7.96819e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=57, gb_free=16.7, wall=23234
2023-08-12 08:34:33 | INFO | train_inner | epoch 022:    661 / 1474 loss=2.005, trans_loss=4.814, nll_loss=2.029, w2v_ctc_loss=0.666, task_loss=0, contrastive_loss=0.167, total=4139.66, n_correct=2820.5, ppl=4.08, accuracy=68.134, wps=14533.9, ups=1.76, wpb=8279.3, bsz=311.1, num_updates=31600, lr=7.95557e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=56, gb_free=16.2, wall=23291
2023-08-12 08:35:31 | INFO | train_inner | epoch 022:    761 / 1474 loss=2.005, trans_loss=4.819, nll_loss=2.035, w2v_ctc_loss=0.683, task_loss=0, contrastive_loss=0.086, total=4167.89, n_correct=2831.35, ppl=4.1, accuracy=67.932, wps=14402.2, ups=1.73, wpb=8335.8, bsz=303.9, num_updates=31700, lr=7.94301e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=57, gb_free=13.1, wall=23349
2023-08-12 08:36:29 | INFO | train_inner | epoch 022:    861 / 1474 loss=2.009, trans_loss=4.832, nll_loss=2.052, w2v_ctc_loss=0.684, task_loss=0, contrastive_loss=0.074, total=4075.79, n_correct=2752.91, ppl=4.15, accuracy=67.543, wps=14115.2, ups=1.73, wpb=8151.6, bsz=289, num_updates=31800, lr=7.93052e-05, gnorm=0.556, clip=0, loss_scale=32, train_wall=57, gb_free=16.4, wall=23406
2023-08-12 08:37:26 | INFO | train_inner | epoch 022:    961 / 1474 loss=1.999, trans_loss=4.82, nll_loss=2.037, w2v_ctc_loss=0.672, task_loss=0, contrastive_loss=0.073, total=4134.72, n_correct=2809.14, ppl=4.1, accuracy=67.94, wps=14559.1, ups=1.76, wpb=8269.4, bsz=303.2, num_updates=31900, lr=7.91808e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=56, gb_free=14.4, wall=23463
2023-08-12 08:38:23 | INFO | train_inner | epoch 022:   1061 / 1474 loss=2.017, trans_loss=4.821, nll_loss=2.039, w2v_ctc_loss=0.672, task_loss=0, contrastive_loss=0.242, total=4160.57, n_correct=2824.99, ppl=4.11, accuracy=67.899, wps=14595.2, ups=1.75, wpb=8321.1, bsz=315.5, num_updates=32000, lr=7.90569e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=56, gb_free=17.3, wall=23520
2023-08-12 08:38:23 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 08:38:46 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 4.086 | trans_loss 5.161 | nll_loss 2.421 | w2v_ctc_loss 1.347 | task_loss 0 | contrastive_loss 0.285 | total 4003.4 | n_correct 2662 | ppl 5.35 | accuracy 66.493 | uer 17.655 | wer 19.664 | raw_wer 19.664 | bleu 22.23 | wps 2251.8 | wpb 4003.4 | bsz 141.8 | num_updates 32000 | best_bleu 22.4
2023-08-12 08:38:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32000 updates
2023-08-12 08:38:46 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_22_32000.pt
2023-08-12 08:38:50 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_22_32000.pt
2023-08-12 08:39:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_22_32000.pt (epoch 22 @ 32000 updates, score 22.23) (writing took 37.07114344090223 seconds)
2023-08-12 08:40:21 | INFO | train_inner | epoch 022:   1161 / 1474 loss=2.023, trans_loss=4.843, nll_loss=2.067, w2v_ctc_loss=0.693, task_loss=0, contrastive_loss=0.128, total=4099.59, n_correct=2767.3, ppl=4.19, accuracy=67.502, wps=6928.4, ups=0.85, wpb=8199.2, bsz=296.2, num_updates=32100, lr=7.89337e-05, gnorm=0.553, clip=0, loss_scale=32, train_wall=57, gb_free=15.1, wall=23639
2023-08-12 08:41:19 | INFO | train_inner | epoch 022:   1261 / 1474 loss=2.018, trans_loss=4.839, nll_loss=2.062, w2v_ctc_loss=0.682, task_loss=0, contrastive_loss=0.121, total=4182.05, n_correct=2826.09, ppl=4.18, accuracy=67.577, wps=14514.3, ups=1.74, wpb=8364.1, bsz=323, num_updates=32200, lr=7.8811e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=57, gb_free=15.6, wall=23696
2023-08-12 08:42:16 | INFO | train_inner | epoch 022:   1361 / 1474 loss=2.008, trans_loss=4.823, nll_loss=2.041, w2v_ctc_loss=0.674, task_loss=0, contrastive_loss=0.141, total=4062.31, n_correct=2757.27, ppl=4.12, accuracy=67.874, wps=14290.7, ups=1.76, wpb=8124.6, bsz=299, num_updates=32300, lr=7.86889e-05, gnorm=0.554, clip=0, loss_scale=32, train_wall=56, gb_free=15.2, wall=23753
2023-08-12 08:43:13 | INFO | train_inner | epoch 022:   1461 / 1474 loss=2.016, trans_loss=4.84, nll_loss=2.061, w2v_ctc_loss=0.694, task_loss=0, contrastive_loss=0.09, total=4081.88, n_correct=2758.45, ppl=4.17, accuracy=67.578, wps=14267.8, ups=1.75, wpb=8163.8, bsz=288.9, num_updates=32400, lr=7.85674e-05, gnorm=0.55, clip=0, loss_scale=32, train_wall=57, gb_free=17.3, wall=23810
2023-08-12 08:43:21 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 08:43:43 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 4.076 | trans_loss 5.155 | nll_loss 2.409 | w2v_ctc_loss 1.328 | task_loss 0 | contrastive_loss 0.287 | total 4003.4 | n_correct 2673 | ppl 5.31 | accuracy 66.768 | uer 17.609 | wer 19.567 | raw_wer 19.567 | bleu 22.52 | wps 2225.4 | wpb 4003.4 | bsz 141.8 | num_updates 32413 | best_bleu 22.52
2023-08-12 08:43:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32413 updates
2023-08-12 08:43:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 08:43:59 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 08:44:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt (epoch 22 @ 32413 updates, score 22.52) (writing took 28.77865107730031 seconds)
2023-08-12 08:44:13 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-08-12 08:44:13 | INFO | train | epoch 022 | loss 2.01 | trans_loss 4.825 | nll_loss 2.043 | w2v_ctc_loss 0.68 | task_loss 0 | contrastive_loss 0.122 | total 4136.85 | n_correct 2806.09 | ppl 4.12 | accuracy 67.832 | wps 12607.6 | ups 1.52 | wpb 8273.7 | bsz 305.1 | num_updates 32413 | lr 7.85517e-05 | gnorm 0.546 | clip 0 | loss_scale 32 | train_wall 838 | gb_free 12 | wall 23870
2023-08-12 08:44:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 08:44:13 | INFO | fairseq.trainer | begin training epoch 23
2023-08-12 08:44:13 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 08:45:10 | INFO | train_inner | epoch 023:     87 / 1474 loss=1.998, trans_loss=4.808, nll_loss=2.021, w2v_ctc_loss=0.68, task_loss=0, contrastive_loss=0.079, total=4096.09, n_correct=2792.78, ppl=4.06, accuracy=68.182, wps=6987.4, ups=0.85, wpb=8192.2, bsz=301.2, num_updates=32500, lr=7.84465e-05, gnorm=0.549, clip=0, loss_scale=32, train_wall=57, gb_free=16.4, wall=23928
2023-08-12 08:46:08 | INFO | train_inner | epoch 023:    187 / 1474 loss=1.99, trans_loss=4.801, nll_loss=2.011, w2v_ctc_loss=0.666, task_loss=0, contrastive_loss=0.077, total=4107.77, n_correct=2805.87, ppl=4.03, accuracy=68.306, wps=14260, ups=1.74, wpb=8215.5, bsz=293.4, num_updates=32600, lr=7.8326e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=57, gb_free=16.4, wall=23985
2023-08-12 08:47:05 | INFO | train_inner | epoch 023:    287 / 1474 loss=2.003, trans_loss=4.815, nll_loss=2.03, w2v_ctc_loss=0.661, task_loss=0, contrastive_loss=0.155, total=4153.12, n_correct=2827.89, ppl=4.08, accuracy=68.091, wps=14561.8, ups=1.75, wpb=8306.2, bsz=306.4, num_updates=32700, lr=7.82062e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=56, gb_free=17, wall=24042
2023-08-12 08:48:02 | INFO | train_inner | epoch 023:    387 / 1474 loss=1.992, trans_loss=4.806, nll_loss=2.018, w2v_ctc_loss=0.669, task_loss=0, contrastive_loss=0.068, total=4116.7, n_correct=2808.79, ppl=4.05, accuracy=68.229, wps=14342.3, ups=1.74, wpb=8233.4, bsz=294, num_updates=32800, lr=7.80869e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=57, gb_free=15.4, wall=24100
2023-08-12 08:48:59 | INFO | train_inner | epoch 023:    487 / 1474 loss=2.005, trans_loss=4.815, nll_loss=2.03, w2v_ctc_loss=0.673, task_loss=0, contrastive_loss=0.126, total=4157.6, n_correct=2832.34, ppl=4.08, accuracy=68.124, wps=14573.3, ups=1.75, wpb=8315.2, bsz=313.5, num_updates=32900, lr=7.79681e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=56, gb_free=17.3, wall=24157
2023-08-12 08:49:56 | INFO | train_inner | epoch 023:    587 / 1474 loss=1.989, trans_loss=4.803, nll_loss=2.015, w2v_ctc_loss=0.664, task_loss=0, contrastive_loss=0.075, total=4173.42, n_correct=2852.9, ppl=4.04, accuracy=68.359, wps=14667.8, ups=1.76, wpb=8346.8, bsz=316, num_updates=33000, lr=7.78499e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=56, gb_free=12.9, wall=24214
2023-08-12 08:50:53 | INFO | train_inner | epoch 023:    687 / 1474 loss=2, trans_loss=4.813, nll_loss=2.028, w2v_ctc_loss=0.669, task_loss=0, contrastive_loss=0.115, total=4137.82, n_correct=2820.7, ppl=4.08, accuracy=68.169, wps=14562.3, ups=1.76, wpb=8275.6, bsz=302.5, num_updates=33100, lr=7.77322e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=56, gb_free=17.2, wall=24270
2023-08-12 08:51:50 | INFO | train_inner | epoch 023:    787 / 1474 loss=2.002, trans_loss=4.817, nll_loss=2.033, w2v_ctc_loss=0.675, task_loss=0, contrastive_loss=0.093, total=4150.99, n_correct=2822.99, ppl=4.09, accuracy=68.008, wps=14465.1, ups=1.74, wpb=8302, bsz=305.4, num_updates=33200, lr=7.76151e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=57, gb_free=16.4, wall=24328
2023-08-12 08:52:47 | INFO | train_inner | epoch 023:    887 / 1474 loss=2.006, trans_loss=4.81, nll_loss=2.024, w2v_ctc_loss=0.671, task_loss=0, contrastive_loss=0.171, total=4181.99, n_correct=2853.65, ppl=4.07, accuracy=68.237, wps=14684.3, ups=1.76, wpb=8364, bsz=324.7, num_updates=33300, lr=7.74984e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=56, gb_free=16.5, wall=24385
2023-08-12 08:53:45 | INFO | train_inner | epoch 023:    987 / 1474 loss=2.017, trans_loss=4.812, nll_loss=2.026, w2v_ctc_loss=0.663, task_loss=0, contrastive_loss=0.328, total=4168.73, n_correct=2838.98, ppl=4.07, accuracy=68.102, wps=14494.3, ups=1.74, wpb=8337.5, bsz=310.5, num_updates=33400, lr=7.73823e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=57, gb_free=11.3, wall=24442
2023-08-12 08:54:42 | INFO | train_inner | epoch 023:   1087 / 1474 loss=2.003, trans_loss=4.819, nll_loss=2.035, w2v_ctc_loss=0.682, task_loss=0, contrastive_loss=0.082, total=4088.49, n_correct=2778.67, ppl=4.1, accuracy=67.963, wps=14316.9, ups=1.75, wpb=8177, bsz=290, num_updates=33500, lr=7.72667e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=57, gb_free=15.9, wall=24499
2023-08-12 08:55:39 | INFO | train_inner | epoch 023:   1187 / 1474 loss=2, trans_loss=4.82, nll_loss=2.037, w2v_ctc_loss=0.677, task_loss=0, contrastive_loss=0.074, total=4162.7, n_correct=2829.59, ppl=4.11, accuracy=67.975, wps=14492.9, ups=1.74, wpb=8325.4, bsz=309.3, num_updates=33600, lr=7.71517e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=57, gb_free=16.1, wall=24557
2023-08-12 08:56:36 | INFO | train_inner | epoch 023:   1287 / 1474 loss=1.995, trans_loss=4.815, nll_loss=2.031, w2v_ctc_loss=0.666, task_loss=0, contrastive_loss=0.085, total=4135.53, n_correct=2817.64, ppl=4.09, accuracy=68.133, wps=14582.5, ups=1.76, wpb=8271.1, bsz=308.9, num_updates=33700, lr=7.70371e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=56, gb_free=16.9, wall=24614
2023-08-12 08:57:34 | INFO | train_inner | epoch 023:   1387 / 1474 loss=2.012, trans_loss=4.827, nll_loss=2.046, w2v_ctc_loss=0.677, task_loss=0, contrastive_loss=0.144, total=4143.98, n_correct=2810.96, ppl=4.13, accuracy=67.832, wps=14416.8, ups=1.74, wpb=8288, bsz=305.2, num_updates=33800, lr=7.69231e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=57, gb_free=15.9, wall=24671
2023-08-12 08:57:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-12 08:58:23 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 08:58:45 | INFO | dev_st | epoch 023 | valid on 'dev_st' subset | loss 4.081 | trans_loss 5.157 | nll_loss 2.413 | w2v_ctc_loss 1.341 | task_loss 0 | contrastive_loss 0.284 | total 4003.4 | n_correct 2671 | ppl 5.33 | accuracy 66.718 | uer 17.448 | wer 19.22 | raw_wer 19.22 | bleu 22.05 | wps 2337.9 | wpb 4003.4 | bsz 141.8 | num_updates 33886 | best_bleu 22.52
2023-08-12 08:58:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 33886 updates
2023-08-12 08:58:45 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-12 08:59:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-12 08:59:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_last.pt (epoch 23 @ 33886 updates, score 22.05) (writing took 15.229982435703278 seconds)
2023-08-12 08:59:01 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-08-12 08:59:01 | INFO | train | epoch 023 | loss 2.001 | trans_loss 4.814 | nll_loss 2.028 | w2v_ctc_loss 0.671 | task_loss 0 | contrastive_loss 0.12 | total 4136.71 | n_correct 2817.56 | ppl 4.08 | accuracy 68.111 | wps 13729 | ups 1.66 | wpb 8273.4 | bsz 305 | num_updates 33886 | lr 7.68254e-05 | gnorm 0.545 | clip 0 | loss_scale 32 | train_wall 835 | gb_free 13.9 | wall 24758
2023-08-12 08:59:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 08:59:01 | INFO | fairseq.trainer | begin training epoch 24
2023-08-12 08:59:01 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 08:59:16 | INFO | train_inner | epoch 024:     14 / 1474 loss=2.005, trans_loss=4.822, nll_loss=2.04, w2v_ctc_loss=0.675, task_loss=0, contrastive_loss=0.107, total=4056.29, n_correct=2757.08, ppl=4.11, accuracy=67.97, wps=7927, ups=0.98, wpb=8112.6, bsz=293.8, num_updates=33900, lr=7.68095e-05, gnorm=0.554, clip=0, loss_scale=32, train_wall=57, gb_free=15.4, wall=24773
2023-08-12 09:00:13 | INFO | train_inner | epoch 024:    114 / 1474 loss=2.003, trans_loss=4.796, nll_loss=2.005, w2v_ctc_loss=0.657, task_loss=0, contrastive_loss=0.235, total=4168.61, n_correct=2850.14, ppl=4.02, accuracy=68.371, wps=14549.1, ups=1.75, wpb=8337.2, bsz=324.6, num_updates=34000, lr=7.66965e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=57, gb_free=11.8, wall=24831
2023-08-12 09:00:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 09:00:36 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 4.088 | trans_loss 5.162 | nll_loss 2.416 | w2v_ctc_loss 1.352 | task_loss 0 | contrastive_loss 0.289 | total 4003.4 | n_correct 2666.2 | ppl 5.34 | accuracy 66.598 | uer 17.519 | wer 19.433 | raw_wer 19.433 | bleu 22.31 | wps 2386.8 | wpb 4003.4 | bsz 141.8 | num_updates 34000 | best_bleu 22.52
2023-08-12 09:00:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 34000 updates
2023-08-12 09:00:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_24_34000.pt
2023-08-12 09:00:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_24_34000.pt
2023-08-12 09:01:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_24_34000.pt (epoch 24 @ 34000 updates, score 22.31) (writing took 37.06916104443371 seconds)
2023-08-12 09:02:11 | INFO | train_inner | epoch 024:    214 / 1474 loss=2.008, trans_loss=4.8, nll_loss=2.012, w2v_ctc_loss=0.651, task_loss=0, contrastive_loss=0.289, total=4252.53, n_correct=2913.6, ppl=4.03, accuracy=68.515, wps=7199.7, ups=0.85, wpb=8505.1, bsz=341.3, num_updates=34100, lr=7.6584e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=57, gb_free=16.8, wall=24949
2023-08-12 09:03:08 | INFO | train_inner | epoch 024:    314 / 1474 loss=1.986, trans_loss=4.8, nll_loss=2.01, w2v_ctc_loss=0.661, task_loss=0, contrastive_loss=0.071, total=4138.44, n_correct=2830.95, ppl=4.03, accuracy=68.406, wps=14600.9, ups=1.76, wpb=8276.9, bsz=307.1, num_updates=34200, lr=7.64719e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=56, gb_free=15.6, wall=25006
2023-08-12 09:04:06 | INFO | train_inner | epoch 024:    414 / 1474 loss=2.012, trans_loss=4.802, nll_loss=2.013, w2v_ctc_loss=0.672, task_loss=0, contrastive_loss=0.217, total=4153.83, n_correct=2830.9, ppl=4.04, accuracy=68.152, wps=14372, ups=1.73, wpb=8307.7, bsz=298.4, num_updates=34300, lr=7.63604e-05, gnorm=0.556, clip=0, loss_scale=32, train_wall=57, gb_free=16.2, wall=25063
2023-08-12 09:05:03 | INFO | train_inner | epoch 024:    514 / 1474 loss=1.994, trans_loss=4.798, nll_loss=2.008, w2v_ctc_loss=0.663, task_loss=0, contrastive_loss=0.139, total=4141.88, n_correct=2830.91, ppl=4.02, accuracy=68.348, wps=14430.7, ups=1.74, wpb=8283.8, bsz=302.6, num_updates=34400, lr=7.62493e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=57, gb_free=15.3, wall=25121
2023-08-12 09:06:01 | INFO | train_inner | epoch 024:    614 / 1474 loss=1.99, trans_loss=4.801, nll_loss=2.013, w2v_ctc_loss=0.657, task_loss=0, contrastive_loss=0.103, total=4162.06, n_correct=2843.25, ppl=4.03, accuracy=68.314, wps=14417, ups=1.73, wpb=8324.1, bsz=308.1, num_updates=34500, lr=7.61387e-05, gnorm=0.55, clip=0, loss_scale=32, train_wall=57, gb_free=16.9, wall=25179
2023-08-12 09:06:58 | INFO | train_inner | epoch 024:    714 / 1474 loss=1.997, trans_loss=4.809, nll_loss=2.022, w2v_ctc_loss=0.666, task_loss=0, contrastive_loss=0.115, total=4097.35, n_correct=2792.51, ppl=4.06, accuracy=68.154, wps=14305, ups=1.75, wpb=8194.7, bsz=293.8, num_updates=34600, lr=7.60286e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=57, gb_free=17.3, wall=25236
2023-08-12 09:07:56 | INFO | train_inner | epoch 024:    814 / 1474 loss=1.996, trans_loss=4.813, nll_loss=2.028, w2v_ctc_loss=0.666, task_loss=0, contrastive_loss=0.092, total=4124.25, n_correct=2808.55, ppl=4.08, accuracy=68.098, wps=14405.6, ups=1.75, wpb=8248.5, bsz=308.2, num_updates=34700, lr=7.5919e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=57, gb_free=16.1, wall=25293
2023-08-12 09:08:53 | INFO | train_inner | epoch 024:    914 / 1474 loss=1.993, trans_loss=4.807, nll_loss=2.019, w2v_ctc_loss=0.674, task_loss=0, contrastive_loss=0.067, total=4041.44, n_correct=2755.35, ppl=4.05, accuracy=68.177, wps=14179.4, ups=1.75, wpb=8082.9, bsz=280.5, num_updates=34800, lr=7.58098e-05, gnorm=0.561, clip=0, loss_scale=32, train_wall=56, gb_free=16.2, wall=25350
2023-08-12 09:09:50 | INFO | train_inner | epoch 024:   1014 / 1474 loss=1.991, trans_loss=4.81, nll_loss=2.023, w2v_ctc_loss=0.665, task_loss=0, contrastive_loss=0.072, total=4128.8, n_correct=2817.29, ppl=4.06, accuracy=68.235, wps=14405.8, ups=1.74, wpb=8257.6, bsz=296.5, num_updates=34900, lr=7.57011e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=57, gb_free=15.1, wall=25407
2023-08-12 09:10:47 | INFO | train_inner | epoch 024:   1114 / 1474 loss=1.995, trans_loss=4.798, nll_loss=2.009, w2v_ctc_loss=0.672, task_loss=0, contrastive_loss=0.115, total=4130.49, n_correct=2822.06, ppl=4.02, accuracy=68.323, wps=14512, ups=1.76, wpb=8261, bsz=308.9, num_updates=35000, lr=7.55929e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=56, gb_free=16.1, wall=25464
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:0')
2023-08-12 09:11:45 | INFO | train_inner | epoch 024:   1214 / 1474 loss=1.993, trans_loss=4.805, nll_loss=2.017, w2v_ctc_loss=0.661, task_loss=0, contrastive_loss=0.104, total=4157.47, n_correct=2839.45, ppl=4.05, accuracy=68.298, wps=14433.7, ups=1.74, wpb=8314.9, bsz=311.2, num_updates=35100, lr=7.54851e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=57, gb_free=13, wall=25522
2023-08-12 09:12:42 | INFO | train_inner | epoch 024:   1314 / 1474 loss=1.999, trans_loss=4.813, nll_loss=2.027, w2v_ctc_loss=0.679, task_loss=0, contrastive_loss=0.076, total=4107.23, n_correct=2794.72, ppl=4.08, accuracy=68.044, wps=14352.1, ups=1.75, wpb=8214.5, bsz=294.3, num_updates=35200, lr=7.53778e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=57, gb_free=17.6, wall=25579
2023-08-12 09:13:39 | INFO | train_inner | epoch 024:   1414 / 1474 loss=1.997, trans_loss=4.812, nll_loss=2.027, w2v_ctc_loss=0.678, task_loss=0, contrastive_loss=0.074, total=4094.39, n_correct=2791.1, ppl=4.08, accuracy=68.169, wps=14238.8, ups=1.74, wpb=8188.8, bsz=292.9, num_updates=35300, lr=7.5271e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=57, gb_free=16.2, wall=25637
2023-08-12 09:14:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:7')
2023-08-12 09:14:36 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 4.075 | trans_loss 5.151 | nll_loss 2.407 | w2v_ctc_loss 1.332 | task_loss 0 | contrastive_loss 0.287 | total 4003.4 | n_correct 2674 | ppl 5.3 | accuracy 66.793 | uer 17.323 | wer 19.283 | raw_wer 19.283 | bleu 22.28 | wps 2212.1 | wpb 4003.4 | bsz 141.8 | num_updates 35360 | best_bleu 22.52
2023-08-12 09:14:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 35360 updates
2023-08-12 09:14:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.2807.pt
2023-08-12 09:14:40 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.2807.pt
2023-08-12 09:14:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.2807.pt (epoch 24 @ 35360 updates, score 22.28) (writing took 21.3749830275774 seconds)
2023-08-12 09:14:58 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-08-12 09:14:58 | INFO | train | epoch 024 | loss 1.996 | trans_loss 4.805 | nll_loss 2.017 | w2v_ctc_loss 0.665 | task_loss 0 | contrastive_loss 0.126 | total 4138.65 | n_correct 2825.44 | ppl 4.05 | accuracy 68.27 | wps 12744.6 | ups 1.54 | wpb 8277.3 | bsz 305.7 | num_updates 35360 | lr 7.52071e-05 | gnorm 0.546 | clip 0 | loss_scale 32 | train_wall 836 | gb_free 16.2 | wall 25715
2023-08-12 09:14:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 09:14:58 | INFO | fairseq.trainer | begin training epoch 25
2023-08-12 09:14:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 09:15:28 | INFO | train_inner | epoch 025:     40 / 1474 loss=1.984, trans_loss=4.796, nll_loss=2.006, w2v_ctc_loss=0.659, task_loss=0, contrastive_loss=0.081, total=4165.57, n_correct=2859.11, ppl=4.02, accuracy=68.637, wps=7656.7, ups=0.92, wpb=8331.1, bsz=311.2, num_updates=35400, lr=7.51646e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=56, gb_free=13.4, wall=25746
2023-08-12 09:16:25 | INFO | train_inner | epoch 025:    140 / 1474 loss=1.975, trans_loss=4.782, nll_loss=1.987, w2v_ctc_loss=0.65, task_loss=0, contrastive_loss=0.078, total=4135.43, n_correct=2845.09, ppl=3.96, accuracy=68.798, wps=14477.7, ups=1.75, wpb=8270.9, bsz=308.9, num_updates=35500, lr=7.50587e-05, gnorm=0.536, clip=0, loss_scale=32, train_wall=57, gb_free=17.8, wall=25803
2023-08-12 09:17:23 | INFO | train_inner | epoch 025:    240 / 1474 loss=1.979, trans_loss=4.787, nll_loss=1.993, w2v_ctc_loss=0.655, task_loss=0, contrastive_loss=0.083, total=4116.13, n_correct=2828.85, ppl=3.98, accuracy=68.726, wps=14242.7, ups=1.73, wpb=8232.3, bsz=303, num_updates=35600, lr=7.49532e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=57, gb_free=17.3, wall=25860
2023-08-12 09:18:21 | INFO | train_inner | epoch 025:    340 / 1474 loss=1.986, trans_loss=4.791, nll_loss=1.997, w2v_ctc_loss=0.659, task_loss=0, contrastive_loss=0.112, total=4141.49, n_correct=2836.05, ppl=3.99, accuracy=68.479, wps=14243.2, ups=1.72, wpb=8283, bsz=294.2, num_updates=35700, lr=7.48481e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=58, gb_free=15.3, wall=25919
2023-08-12 09:18:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-12 09:19:19 | INFO | train_inner | epoch 025:    441 / 1474 loss=1.99, trans_loss=4.791, nll_loss=1.999, w2v_ctc_loss=0.678, task_loss=0, contrastive_loss=0.074, total=4149.36, n_correct=2840.58, ppl=4, accuracy=68.458, wps=14285.8, ups=1.72, wpb=8298.7, bsz=290.1, num_updates=35800, lr=7.47435e-05, gnorm=0.554, clip=0, loss_scale=16, train_wall=58, gb_free=16.9, wall=25977
2023-08-12 09:20:17 | INFO | train_inner | epoch 025:    541 / 1474 loss=1.989, trans_loss=4.801, nll_loss=2.012, w2v_ctc_loss=0.661, task_loss=0, contrastive_loss=0.083, total=4154.79, n_correct=2841.77, ppl=4.03, accuracy=68.397, wps=14459.4, ups=1.74, wpb=8309.6, bsz=313.5, num_updates=35900, lr=7.46393e-05, gnorm=0.554, clip=0, loss_scale=16, train_wall=57, gb_free=17.6, wall=26034
2023-08-12 09:21:14 | INFO | train_inner | epoch 025:    641 / 1474 loss=1.992, trans_loss=4.788, nll_loss=1.996, w2v_ctc_loss=0.665, task_loss=0, contrastive_loss=0.149, total=4156.33, n_correct=2846.38, ppl=3.99, accuracy=68.483, wps=14508.7, ups=1.75, wpb=8312.7, bsz=309.3, num_updates=36000, lr=7.45356e-05, gnorm=0.553, clip=0, loss_scale=16, train_wall=57, gb_free=16.4, wall=26092
2023-08-12 09:21:14 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 09:21:37 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 4.079 | trans_loss 5.16 | nll_loss 2.414 | w2v_ctc_loss 1.329 | task_loss 0 | contrastive_loss 0.284 | total 4003.4 | n_correct 2671.2 | ppl 5.33 | accuracy 66.723 | uer 17.286 | wer 19.28 | raw_wer 19.28 | bleu 22.12 | wps 2239.3 | wpb 4003.4 | bsz 141.8 | num_updates 36000 | best_bleu 22.52
2023-08-12 09:21:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36000 updates
2023-08-12 09:21:37 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_25_36000.pt
2023-08-12 09:21:41 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_25_36000.pt
2023-08-12 09:21:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_25_36000.pt (epoch 25 @ 36000 updates, score 22.12) (writing took 18.935617236420512 seconds)
2023-08-12 09:22:55 | INFO | train_inner | epoch 025:    741 / 1474 loss=1.992, trans_loss=4.79, nll_loss=1.998, w2v_ctc_loss=0.66, task_loss=0, contrastive_loss=0.144, total=4133.94, n_correct=2834.37, ppl=3.99, accuracy=68.563, wps=8231.3, ups=1, wpb=8267.9, bsz=303.3, num_updates=36100, lr=7.44323e-05, gnorm=0.55, clip=0, loss_scale=16, train_wall=57, gb_free=15.2, wall=26192
2023-08-12 09:23:51 | INFO | train_inner | epoch 025:    841 / 1474 loss=1.986, trans_loss=4.795, nll_loss=2.004, w2v_ctc_loss=0.659, task_loss=0, contrastive_loss=0.09, total=4174.24, n_correct=2863.39, ppl=4.01, accuracy=68.597, wps=14693.5, ups=1.76, wpb=8348.5, bsz=324, num_updates=36200, lr=7.43294e-05, gnorm=0.545, clip=0, loss_scale=16, train_wall=56, gb_free=16.4, wall=26249
2023-08-12 09:24:49 | INFO | train_inner | epoch 025:    941 / 1474 loss=1.996, trans_loss=4.798, nll_loss=2.009, w2v_ctc_loss=0.665, task_loss=0, contrastive_loss=0.149, total=4154.13, n_correct=2842.24, ppl=4.02, accuracy=68.42, wps=14406.8, ups=1.73, wpb=8308.3, bsz=316.7, num_updates=36300, lr=7.4227e-05, gnorm=0.551, clip=0, loss_scale=16, train_wall=57, gb_free=11.3, wall=26306
2023-08-12 09:25:47 | INFO | train_inner | epoch 025:   1041 / 1474 loss=2.003, trans_loss=4.803, nll_loss=2.015, w2v_ctc_loss=0.652, task_loss=0, contrastive_loss=0.258, total=4178.3, n_correct=2854.18, ppl=4.04, accuracy=68.31, wps=14413.7, ups=1.72, wpb=8356.6, bsz=309.7, num_updates=36400, lr=7.41249e-05, gnorm=0.544, clip=0, loss_scale=16, train_wall=57, gb_free=17, wall=26364
2023-08-12 09:26:44 | INFO | train_inner | epoch 025:   1141 / 1474 loss=1.979, trans_loss=4.795, nll_loss=2.003, w2v_ctc_loss=0.652, task_loss=0, contrastive_loss=0.066, total=4042.33, n_correct=2770.9, ppl=4.01, accuracy=68.547, wps=14121.3, ups=1.75, wpb=8084.7, bsz=286.5, num_updates=36500, lr=7.40233e-05, gnorm=0.545, clip=0, loss_scale=16, train_wall=57, gb_free=17.8, wall=26422
2023-08-12 09:27:41 | INFO | train_inner | epoch 025:   1241 / 1474 loss=1.984, trans_loss=4.801, nll_loss=2.013, w2v_ctc_loss=0.657, task_loss=0, contrastive_loss=0.075, total=4087.78, n_correct=2796.22, ppl=4.04, accuracy=68.404, wps=14433, ups=1.77, wpb=8175.6, bsz=295.1, num_updates=36600, lr=7.39221e-05, gnorm=0.542, clip=0, loss_scale=16, train_wall=56, gb_free=17.7, wall=26478
2023-08-12 09:28:39 | INFO | train_inner | epoch 025:   1341 / 1474 loss=1.994, trans_loss=4.796, nll_loss=2.006, w2v_ctc_loss=0.66, task_loss=0, contrastive_loss=0.168, total=4166.64, n_correct=2853.18, ppl=4.02, accuracy=68.477, wps=14401.9, ups=1.73, wpb=8333.3, bsz=309.3, num_updates=36700, lr=7.38213e-05, gnorm=0.542, clip=0, loss_scale=16, train_wall=57, gb_free=16.6, wall=26536
2023-08-12 09:29:37 | INFO | train_inner | epoch 025:   1441 / 1474 loss=2.004, trans_loss=4.814, nll_loss=2.028, w2v_ctc_loss=0.668, task_loss=0, contrastive_loss=0.136, total=4114.64, n_correct=2797.62, ppl=4.08, accuracy=67.992, wps=14158.3, ups=1.72, wpb=8229.3, bsz=304.3, num_updates=36800, lr=7.3721e-05, gnorm=0.557, clip=0, loss_scale=16, train_wall=58, gb_free=16.5, wall=26594
2023-08-12 09:29:56 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 09:30:19 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 4.085 | trans_loss 5.156 | nll_loss 2.411 | w2v_ctc_loss 1.354 | task_loss 0 | contrastive_loss 0.295 | total 4003.4 | n_correct 2669.3 | ppl 5.32 | accuracy 66.676 | uer 17.49 | wer 19.656 | raw_wer 19.656 | bleu 22.1 | wps 2177.8 | wpb 4003.4 | bsz 141.8 | num_updates 36833 | best_bleu 22.52
2023-08-12 09:30:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36833 updates
2023-08-12 09:30:19 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-12 09:30:34 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-12 09:30:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_last.pt (epoch 25 @ 36833 updates, score 22.1) (writing took 14.78378164768219 seconds)
2023-08-12 09:30:34 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-08-12 09:30:34 | INFO | train | epoch 025 | loss 1.989 | trans_loss 4.795 | nll_loss 2.004 | w2v_ctc_loss 0.66 | task_loss 0 | contrastive_loss 0.117 | total 4137.25 | n_correct 2833.31 | ppl 4.01 | accuracy 68.483 | wps 13020.9 | ups 1.57 | wpb 8274.5 | bsz 305.1 | num_updates 36833 | lr 7.36879e-05 | gnorm 0.547 | clip 0 | loss_scale 16 | train_wall 840 | gb_free 14.5 | wall 26651
2023-08-12 09:30:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 09:30:34 | INFO | fairseq.trainer | begin training epoch 26
2023-08-12 09:30:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 09:31:20 | INFO | train_inner | epoch 026:     67 / 1474 loss=1.977, trans_loss=4.78, nll_loss=1.985, w2v_ctc_loss=0.649, task_loss=0, contrastive_loss=0.1, total=4172.16, n_correct=2871.29, ppl=3.96, accuracy=68.82, wps=8065.7, ups=0.97, wpb=8344.3, bsz=316.9, num_updates=36900, lr=7.3621e-05, gnorm=0.542, clip=0, loss_scale=16, train_wall=57, gb_free=17.1, wall=26698
2023-08-12 09:32:18 | INFO | train_inner | epoch 026:    167 / 1474 loss=1.993, trans_loss=4.781, nll_loss=1.987, w2v_ctc_loss=0.638, task_loss=0, contrastive_loss=0.275, total=4265.22, n_correct=2936.24, ppl=3.96, accuracy=68.841, wps=14901.1, ups=1.75, wpb=8530.4, bsz=338.7, num_updates=37000, lr=7.35215e-05, gnorm=0.538, clip=0, loss_scale=16, train_wall=57, gb_free=15.6, wall=26755
2023-08-12 09:33:15 | INFO | train_inner | epoch 026:    267 / 1474 loss=1.986, trans_loss=4.779, nll_loss=1.984, w2v_ctc_loss=0.657, task_loss=0, contrastive_loss=0.161, total=4123.94, n_correct=2834.72, ppl=3.96, accuracy=68.738, wps=14270, ups=1.73, wpb=8247.9, bsz=306.6, num_updates=37100, lr=7.34223e-05, gnorm=0.547, clip=0, loss_scale=16, train_wall=57, gb_free=16.9, wall=26813
2023-08-12 09:34:13 | INFO | train_inner | epoch 026:    367 / 1474 loss=1.984, trans_loss=4.784, nll_loss=1.99, w2v_ctc_loss=0.655, task_loss=0, contrastive_loss=0.119, total=4168.11, n_correct=2863.24, ppl=3.97, accuracy=68.694, wps=14587.4, ups=1.75, wpb=8336.2, bsz=315.2, num_updates=37200, lr=7.33236e-05, gnorm=0.539, clip=0, loss_scale=16, train_wall=57, gb_free=17.3, wall=26870
2023-08-12 09:35:10 | INFO | train_inner | epoch 026:    467 / 1474 loss=1.984, trans_loss=4.772, nll_loss=1.975, w2v_ctc_loss=0.653, task_loss=0, contrastive_loss=0.168, total=4167.53, n_correct=2871.58, ppl=3.93, accuracy=68.904, wps=14612.2, ups=1.75, wpb=8335.1, bsz=314.6, num_updates=37300, lr=7.32252e-05, gnorm=0.552, clip=0, loss_scale=16, train_wall=56, gb_free=14.3, wall=26927
2023-08-12 09:36:07 | INFO | train_inner | epoch 026:    567 / 1474 loss=1.984, trans_loss=4.786, nll_loss=1.992, w2v_ctc_loss=0.664, task_loss=0, contrastive_loss=0.087, total=4158.48, n_correct=2856.65, ppl=3.98, accuracy=68.695, wps=14549.6, ups=1.75, wpb=8317, bsz=304.4, num_updates=37400, lr=7.31272e-05, gnorm=0.544, clip=0, loss_scale=16, train_wall=57, gb_free=13.1, wall=26984
2023-08-12 09:37:04 | INFO | train_inner | epoch 026:    667 / 1474 loss=1.976, trans_loss=4.784, nll_loss=1.99, w2v_ctc_loss=0.649, task_loss=0, contrastive_loss=0.072, total=4129.11, n_correct=2833.6, ppl=3.97, accuracy=68.625, wps=14370.3, ups=1.74, wpb=8258.2, bsz=297.7, num_updates=37500, lr=7.30297e-05, gnorm=0.553, clip=0, loss_scale=16, train_wall=57, gb_free=14.1, wall=27042
2023-08-12 09:38:01 | INFO | train_inner | epoch 026:    767 / 1474 loss=1.992, trans_loss=4.789, nll_loss=1.996, w2v_ctc_loss=0.653, task_loss=0, contrastive_loss=0.186, total=4096.84, n_correct=2811.41, ppl=3.99, accuracy=68.624, wps=14349.6, ups=1.75, wpb=8193.7, bsz=300.5, num_updates=37600, lr=7.29325e-05, gnorm=0.558, clip=0, loss_scale=16, train_wall=57, gb_free=16.6, wall=27099
2023-08-12 09:38:59 | INFO | train_inner | epoch 026:    867 / 1474 loss=1.983, trans_loss=4.786, nll_loss=1.992, w2v_ctc_loss=0.66, task_loss=0, contrastive_loss=0.088, total=4176.27, n_correct=2864.28, ppl=3.98, accuracy=68.585, wps=14508.3, ups=1.74, wpb=8352.5, bsz=306.4, num_updates=37700, lr=7.28357e-05, gnorm=0.544, clip=0, loss_scale=16, train_wall=57, gb_free=16.3, wall=27156
2023-08-12 09:39:56 | INFO | train_inner | epoch 026:    967 / 1474 loss=1.984, trans_loss=4.791, nll_loss=1.999, w2v_ctc_loss=0.646, task_loss=0, contrastive_loss=0.139, total=4141.01, n_correct=2838.72, ppl=4, accuracy=68.551, wps=14498.7, ups=1.75, wpb=8282, bsz=299.7, num_updates=37800, lr=7.27393e-05, gnorm=0.548, clip=0, loss_scale=16, train_wall=57, gb_free=15.6, wall=27213
2023-08-12 09:40:53 | INFO | train_inner | epoch 026:   1067 / 1474 loss=1.976, trans_loss=4.784, nll_loss=1.99, w2v_ctc_loss=0.654, task_loss=0, contrastive_loss=0.071, total=4113.69, n_correct=2831.09, ppl=3.97, accuracy=68.821, wps=14387.5, ups=1.75, wpb=8227.4, bsz=293, num_updates=37900, lr=7.26433e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=57, gb_free=15.6, wall=27271
2023-08-12 09:41:51 | INFO | train_inner | epoch 026:   1167 / 1474 loss=1.984, trans_loss=4.793, nll_loss=2.002, w2v_ctc_loss=0.652, task_loss=0, contrastive_loss=0.113, total=4116.78, n_correct=2821.49, ppl=4.01, accuracy=68.536, wps=14308, ups=1.74, wpb=8233.6, bsz=299, num_updates=38000, lr=7.25476e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=57, gb_free=16.7, wall=27328
2023-08-12 09:41:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 09:42:14 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 4.088 | trans_loss 5.16 | nll_loss 2.418 | w2v_ctc_loss 1.364 | task_loss 0 | contrastive_loss 0.278 | total 4003.4 | n_correct 2668.8 | ppl 5.34 | accuracy 66.663 | uer 17.45 | wer 19.403 | raw_wer 19.403 | bleu 22.24 | wps 2207.4 | wpb 4003.4 | bsz 141.8 | num_updates 38000 | best_bleu 22.52
2023-08-12 09:42:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38000 updates
2023-08-12 09:42:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_26_38000.pt
2023-08-12 09:42:17 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_26_38000.pt
2023-08-12 09:42:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_26_38000.pt (epoch 26 @ 38000 updates, score 22.24) (writing took 23.603837300091982 seconds)
2023-08-12 09:43:35 | INFO | train_inner | epoch 026:   1267 / 1474 loss=1.989, trans_loss=4.802, nll_loss=2.012, w2v_ctc_loss=0.671, task_loss=0, contrastive_loss=0.075, total=4001.06, n_correct=2735.26, ppl=4.03, accuracy=68.363, wps=7703.5, ups=0.96, wpb=8002.1, bsz=280.6, num_updates=38100, lr=7.24524e-05, gnorm=0.559, clip=0, loss_scale=32, train_wall=56, gb_free=15.7, wall=27432
2023-08-12 09:44:32 | INFO | train_inner | epoch 026:   1367 / 1474 loss=1.981, trans_loss=4.795, nll_loss=2.004, w2v_ctc_loss=0.648, task_loss=0, contrastive_loss=0.087, total=4157.69, n_correct=2854.72, ppl=4.01, accuracy=68.661, wps=14479.6, ups=1.74, wpb=8315.4, bsz=310.5, num_updates=38200, lr=7.23575e-05, gnorm=0.553, clip=0, loss_scale=32, train_wall=57, gb_free=16.1, wall=27490
2023-08-12 09:45:29 | INFO | train_inner | epoch 026:   1467 / 1474 loss=1.977, trans_loss=4.788, nll_loss=1.995, w2v_ctc_loss=0.648, task_loss=0, contrastive_loss=0.081, total=4158.47, n_correct=2858.83, ppl=3.99, accuracy=68.747, wps=14564.1, ups=1.75, wpb=8316.9, bsz=316.5, num_updates=38300, lr=7.22629e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=57, gb_free=17.4, wall=27547
2023-08-12 09:45:33 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 09:45:56 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 4.079 | trans_loss 5.156 | nll_loss 2.415 | w2v_ctc_loss 1.34 | task_loss 0 | contrastive_loss 0.283 | total 4003.4 | n_correct 2672.3 | ppl 5.33 | accuracy 66.751 | uer 17.623 | wer 19.604 | raw_wer 19.604 | bleu 22.61 | wps 2184.8 | wpb 4003.4 | bsz 141.8 | num_updates 38307 | best_bleu 22.61
2023-08-12 09:45:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38307 updates
2023-08-12 09:45:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 09:46:11 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-12 09:46:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_best.pt (epoch 26 @ 38307 updates, score 22.61) (writing took 28.115486819297075 seconds)
2023-08-12 09:46:25 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-08-12 09:46:25 | INFO | train | epoch 026 | loss 1.983 | trans_loss 4.786 | nll_loss 1.992 | w2v_ctc_loss 0.653 | task_loss 0 | contrastive_loss 0.124 | total 4138.65 | n_correct 2842.76 | ppl 3.98 | accuracy 68.688 | wps 12829.4 | ups 1.55 | wpb 8277.3 | bsz 305.7 | num_updates 38307 | lr 7.22563e-05 | gnorm 0.548 | clip 0 | loss_scale 32 | train_wall 836 | gb_free 16 | wall 27602
2023-08-12 09:46:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 09:46:25 | INFO | fairseq.trainer | begin training epoch 27
2023-08-12 09:46:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 09:47:27 | INFO | train_inner | epoch 027:     93 / 1474 loss=1.96, trans_loss=4.756, nll_loss=1.953, w2v_ctc_loss=0.639, task_loss=0, contrastive_loss=0.063, total=4067.62, n_correct=2818.75, ppl=3.87, accuracy=69.297, wps=6911.2, ups=0.85, wpb=8135.2, bsz=284.4, num_updates=38400, lr=7.21688e-05, gnorm=0.559, clip=0, loss_scale=32, train_wall=57, gb_free=15, wall=27664
2023-08-12 09:48:24 | INFO | train_inner | epoch 027:    193 / 1474 loss=1.97, trans_loss=4.768, nll_loss=1.97, w2v_ctc_loss=0.646, task_loss=0, contrastive_loss=0.088, total=4185.52, n_correct=2891.03, ppl=3.92, accuracy=69.072, wps=14618.3, ups=1.75, wpb=8371, bsz=321.7, num_updates=38500, lr=7.2075e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=57, gb_free=17.7, wall=27722
2023-08-12 09:49:22 | INFO | train_inner | epoch 027:    293 / 1474 loss=1.97, trans_loss=4.773, nll_loss=1.975, w2v_ctc_loss=0.647, task_loss=0, contrastive_loss=0.073, total=4167.92, n_correct=2878.5, ppl=3.93, accuracy=69.063, wps=14523.9, ups=1.74, wpb=8335.8, bsz=306.8, num_updates=38600, lr=7.19816e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=57, gb_free=17, wall=27779
2023-08-12 09:50:20 | INFO | train_inner | epoch 027:    393 / 1474 loss=1.989, trans_loss=4.776, nll_loss=1.98, w2v_ctc_loss=0.64, task_loss=0, contrastive_loss=0.253, total=4075.21, n_correct=2805.75, ppl=3.95, accuracy=68.849, wps=14067.5, ups=1.73, wpb=8150.4, bsz=296, num_updates=38700, lr=7.18885e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=57, gb_free=17.7, wall=27837
2023-08-12 09:51:18 | INFO | train_inner | epoch 027:    493 / 1474 loss=1.988, trans_loss=4.784, nll_loss=1.99, w2v_ctc_loss=0.646, task_loss=0, contrastive_loss=0.19, total=4249.35, n_correct=2922.05, ppl=3.97, accuracy=68.765, wps=14611.4, ups=1.72, wpb=8498.7, bsz=331.9, num_updates=38800, lr=7.17958e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=57, gb_free=12.4, wall=27895
2023-08-12 09:52:15 | INFO | train_inner | epoch 027:    593 / 1474 loss=1.981, trans_loss=4.775, nll_loss=1.979, w2v_ctc_loss=0.652, task_loss=0, contrastive_loss=0.131, total=4133.39, n_correct=2846.49, ppl=3.94, accuracy=68.866, wps=14457.3, ups=1.75, wpb=8266.8, bsz=312, num_updates=38900, lr=7.17035e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=57, gb_free=12.4, wall=27952
2023-08-12 09:53:12 | INFO | train_inner | epoch 027:    693 / 1474 loss=1.98, trans_loss=4.778, nll_loss=1.982, w2v_ctc_loss=0.654, task_loss=0, contrastive_loss=0.111, total=4162.71, n_correct=2865.46, ppl=3.95, accuracy=68.836, wps=14534.3, ups=1.75, wpb=8325.4, bsz=305.4, num_updates=39000, lr=7.16115e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=57, gb_free=16.4, wall=28010
2023-08-12 09:54:09 | INFO | train_inner | epoch 027:    793 / 1474 loss=1.972, trans_loss=4.775, nll_loss=1.978, w2v_ctc_loss=0.652, task_loss=0, contrastive_loss=0.073, total=4103.81, n_correct=2830.44, ppl=3.94, accuracy=68.971, wps=14341.3, ups=1.75, wpb=8207.6, bsz=294.2, num_updates=39100, lr=7.15199e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=57, gb_free=16.3, wall=28067
2023-08-12 09:55:07 | INFO | train_inner | epoch 027:    893 / 1474 loss=1.968, trans_loss=4.781, nll_loss=1.986, w2v_ctc_loss=0.64, task_loss=0, contrastive_loss=0.065, total=4101.56, n_correct=2828.69, ppl=3.96, accuracy=68.966, wps=14298.2, ups=1.74, wpb=8203.1, bsz=292.1, num_updates=39200, lr=7.14286e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=57, gb_free=17.7, wall=28124
2023-08-12 09:56:04 | INFO | train_inner | epoch 027:    993 / 1474 loss=1.988, trans_loss=4.779, nll_loss=1.984, w2v_ctc_loss=0.642, task_loss=0, contrastive_loss=0.251, total=4199.56, n_correct=2891.59, ppl=3.96, accuracy=68.855, wps=14615.2, ups=1.74, wpb=8399.1, bsz=316.8, num_updates=39300, lr=7.13376e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=57, gb_free=12, wall=28182
2023-08-12 09:57:02 | INFO | train_inner | epoch 027:   1093 / 1474 loss=1.968, trans_loss=4.775, nll_loss=1.978, w2v_ctc_loss=0.639, task_loss=0, contrastive_loss=0.082, total=4150.97, n_correct=2860.37, ppl=3.94, accuracy=68.908, wps=14463.7, ups=1.74, wpb=8301.9, bsz=305, num_updates=39400, lr=7.1247e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=57, gb_free=12.4, wall=28239
2023-08-12 09:57:59 | INFO | train_inner | epoch 027:   1193 / 1474 loss=1.98, trans_loss=4.781, nll_loss=1.986, w2v_ctc_loss=0.659, task_loss=0, contrastive_loss=0.088, total=4103.06, n_correct=2821.71, ppl=3.96, accuracy=68.771, wps=14346.7, ups=1.75, wpb=8206.1, bsz=297.7, num_updates=39500, lr=7.11568e-05, gnorm=0.553, clip=0, loss_scale=32, train_wall=57, gb_free=16.6, wall=28296
2023-08-12 09:58:57 | INFO | train_inner | epoch 027:   1293 / 1474 loss=1.986, trans_loss=4.785, nll_loss=1.991, w2v_ctc_loss=0.653, task_loss=0, contrastive_loss=0.139, total=4062.52, n_correct=2788.05, ppl=3.97, accuracy=68.629, wps=14102, ups=1.74, wpb=8125, bsz=292.2, num_updates=39600, lr=7.10669e-05, gnorm=0.55, clip=0, loss_scale=32, train_wall=57, gb_free=16.7, wall=28354
2023-08-12 09:59:53 | INFO | train_inner | epoch 027:   1393 / 1474 loss=1.976, trans_loss=4.781, nll_loss=1.987, w2v_ctc_loss=0.641, task_loss=0, contrastive_loss=0.119, total=4152, n_correct=2857.14, ppl=3.97, accuracy=68.814, wps=14721.4, ups=1.77, wpb=8304, bsz=312.5, num_updates=39700, lr=7.09773e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=56, gb_free=16.7, wall=28410
2023-08-12 10:00:39 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 10:01:02 | INFO | dev_st | epoch 027 | valid on 'dev_st' subset | loss 4.09 | trans_loss 5.156 | nll_loss 2.411 | w2v_ctc_loss 1.377 | task_loss 0 | contrastive_loss 0.286 | total 4003.4 | n_correct 2674.2 | ppl 5.32 | accuracy 66.798 | uer 17.522 | wer 19.358 | raw_wer 19.358 | bleu 22.15 | wps 2314.5 | wpb 4003.4 | bsz 141.8 | num_updates 39781 | best_bleu 22.61
2023-08-12 10:01:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 39781 updates
2023-08-12 10:01:02 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-12 10:01:17 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-12 10:01:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_last.pt (epoch 27 @ 39781 updates, score 22.15) (writing took 15.687614532187581 seconds)
2023-08-12 10:01:18 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-08-12 10:01:18 | INFO | train | epoch 027 | loss 1.976 | trans_loss 4.776 | nll_loss 1.98 | w2v_ctc_loss 0.646 | task_loss 0 | contrastive_loss 0.122 | total 4138.65 | n_correct 2852.01 | ppl 3.94 | accuracy 68.912 | wps 13670.1 | ups 1.65 | wpb 8277.3 | bsz 305.7 | num_updates 39781 | lr 7.0905e-05 | gnorm 0.546 | clip 0 | loss_scale 32 | train_wall 837 | gb_free 17.8 | wall 28495
2023-08-12 10:01:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 10:01:18 | INFO | fairseq.trainer | begin training epoch 28
2023-08-12 10:01:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 10:01:36 | INFO | train_inner | epoch 028:     19 / 1474 loss=1.965, trans_loss=4.771, nll_loss=1.973, w2v_ctc_loss=0.639, task_loss=0, contrastive_loss=0.074, total=4108.43, n_correct=2839.62, ppl=3.93, accuracy=69.117, wps=7934.9, ups=0.97, wpb=8216.9, bsz=305.1, num_updates=39800, lr=7.08881e-05, gnorm=0.552, clip=0, loss_scale=32, train_wall=56, gb_free=16.3, wall=28514
2023-08-12 10:02:34 | INFO | train_inner | epoch 028:    119 / 1474 loss=1.956, trans_loss=4.751, nll_loss=1.946, w2v_ctc_loss=0.635, task_loss=0, contrastive_loss=0.069, total=4113.41, n_correct=2859.24, ppl=3.85, accuracy=69.51, wps=14322.1, ups=1.74, wpb=8226.8, bsz=293.9, num_updates=39900, lr=7.07992e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=57, gb_free=16.9, wall=28571
2023-08-12 10:03:32 | INFO | train_inner | epoch 028:    219 / 1474 loss=1.961, trans_loss=4.761, nll_loss=1.96, w2v_ctc_loss=0.635, task_loss=0, contrastive_loss=0.077, total=4191.56, n_correct=2903.39, ppl=3.89, accuracy=69.268, wps=14566, ups=1.74, wpb=8383.1, bsz=315.2, num_updates=40000, lr=7.07107e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=57, gb_free=15.2, wall=28629
2023-08-12 10:03:32 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 10:03:54 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 4.095 | trans_loss 5.163 | nll_loss 2.42 | w2v_ctc_loss 1.377 | task_loss 0 | contrastive_loss 0.289 | total 4003.4 | n_correct 2665.9 | ppl 5.35 | accuracy 66.591 | uer 17.49 | wer 19.254 | raw_wer 19.254 | bleu 22.25 | wps 2391.5 | wpb 4003.4 | bsz 141.8 | num_updates 40000 | best_bleu 22.61
2023-08-12 10:03:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 40000 updates
2023-08-12 10:03:54 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_28_40000.pt
2023-08-12 10:03:57 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_28_40000.pt
2023-08-12 10:04:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_28_40000.pt (epoch 28 @ 40000 updates, score 22.25) (writing took 18.02655685506761 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:0')
2023-08-12 10:05:10 | INFO | train_inner | epoch 028:    319 / 1474 loss=2, trans_loss=4.771, nll_loss=1.973, w2v_ctc_loss=0.632, task_loss=0, contrastive_loss=0.402, total=4145.32, n_correct=2854.84, ppl=3.93, accuracy=68.869, wps=8436.7, ups=1.02, wpb=8290.6, bsz=316.1, num_updates=40100, lr=7.06225e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=57, gb_free=15.8, wall=28727
2023-08-12 10:06:08 | INFO | train_inner | epoch 028:    419 / 1474 loss=1.962, trans_loss=4.76, nll_loss=1.958, w2v_ctc_loss=0.644, task_loss=0, contrastive_loss=0.065, total=4092.14, n_correct=2833.15, ppl=3.89, accuracy=69.234, wps=14089.9, ups=1.72, wpb=8184.3, bsz=295.7, num_updates=40200, lr=7.05346e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=57, gb_free=16.3, wall=28785
2023-08-12 10:07:05 | INFO | train_inner | epoch 028:    519 / 1474 loss=1.965, trans_loss=4.766, nll_loss=1.966, w2v_ctc_loss=0.639, task_loss=0, contrastive_loss=0.077, total=4096.35, n_correct=2831.34, ppl=3.91, accuracy=69.119, wps=14336.1, ups=1.75, wpb=8192.7, bsz=295.5, num_updates=40300, lr=7.0447e-05, gnorm=0.557, clip=0, loss_scale=64, train_wall=56, gb_free=16.1, wall=28842
2023-08-12 10:08:03 | INFO | train_inner | epoch 028:    619 / 1474 loss=1.968, trans_loss=4.774, nll_loss=1.976, w2v_ctc_loss=0.642, task_loss=0, contrastive_loss=0.076, total=4178.12, n_correct=2881.91, ppl=3.93, accuracy=68.976, wps=14489.8, ups=1.73, wpb=8356.2, bsz=305.3, num_updates=40400, lr=7.03598e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=57, gb_free=15.9, wall=28900
2023-08-12 10:09:00 | INFO | train_inner | epoch 028:    719 / 1474 loss=1.979, trans_loss=4.774, nll_loss=1.977, w2v_ctc_loss=0.638, task_loss=0, contrastive_loss=0.186, total=4185.82, n_correct=2888.87, ppl=3.94, accuracy=69.016, wps=14646, ups=1.75, wpb=8371.6, bsz=326.4, num_updates=40500, lr=7.02728e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=56, gb_free=16, wall=28957
2023-08-12 10:09:57 | INFO | train_inner | epoch 028:    819 / 1474 loss=1.962, trans_loss=4.768, nll_loss=1.969, w2v_ctc_loss=0.637, task_loss=0, contrastive_loss=0.069, total=4096.2, n_correct=2833.55, ppl=3.91, accuracy=69.175, wps=14347, ups=1.75, wpb=8192.4, bsz=307, num_updates=40600, lr=7.01862e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=56, gb_free=15.8, wall=29014
2023-08-12 10:10:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-12 10:10:55 | INFO | train_inner | epoch 028:    920 / 1474 loss=1.97, trans_loss=4.772, nll_loss=1.974, w2v_ctc_loss=0.648, task_loss=0, contrastive_loss=0.072, total=4101.4, n_correct=2827.55, ppl=3.93, accuracy=68.941, wps=14056.2, ups=1.71, wpb=8202.8, bsz=294.5, num_updates=40700, lr=7.01e-05, gnorm=0.556, clip=0, loss_scale=32, train_wall=58, gb_free=15.4, wall=29073
2023-08-12 10:11:53 | INFO | train_inner | epoch 028:   1020 / 1474 loss=1.986, trans_loss=4.774, nll_loss=1.976, w2v_ctc_loss=0.653, task_loss=0, contrastive_loss=0.183, total=4182.85, n_correct=2880.26, ppl=3.93, accuracy=68.859, wps=14536.4, ups=1.74, wpb=8365.7, bsz=312, num_updates=40800, lr=7.0014e-05, gnorm=0.561, clip=0, loss_scale=32, train_wall=57, gb_free=16.4, wall=29130
2023-08-12 10:12:50 | INFO | train_inner | epoch 028:   1120 / 1474 loss=1.964, trans_loss=4.765, nll_loss=1.966, w2v_ctc_loss=0.635, task_loss=0, contrastive_loss=0.088, total=4220.16, n_correct=2915.87, ppl=3.91, accuracy=69.094, wps=14786.2, ups=1.75, wpb=8440.3, bsz=321, num_updates=40900, lr=6.99284e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=57, gb_free=16, wall=29187
2023-08-12 10:13:47 | INFO | train_inner | epoch 028:   1220 / 1474 loss=1.965, trans_loss=4.772, nll_loss=1.974, w2v_ctc_loss=0.635, task_loss=0, contrastive_loss=0.075, total=4092.46, n_correct=2824.13, ppl=3.93, accuracy=69.008, wps=14336.2, ups=1.75, wpb=8184.9, bsz=303.1, num_updates=41000, lr=6.9843e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=56, gb_free=17.7, wall=29244
2023-08-12 10:14:45 | INFO | train_inner | epoch 028:   1320 / 1474 loss=1.976, trans_loss=4.775, nll_loss=1.978, w2v_ctc_loss=0.657, task_loss=0, contrastive_loss=0.091, total=4084.55, n_correct=2810.09, ppl=3.94, accuracy=68.798, wps=14199.5, ups=1.74, wpb=8169.1, bsz=285.1, num_updates=41100, lr=6.9758e-05, gnorm=0.555, clip=0, loss_scale=32, train_wall=57, gb_free=15.8, wall=29302
2023-08-12 10:15:42 | INFO | train_inner | epoch 028:   1420 / 1474 loss=1.972, trans_loss=4.772, nll_loss=1.974, w2v_ctc_loss=0.642, task_loss=0, contrastive_loss=0.113, total=4154.09, n_correct=2864.55, ppl=3.93, accuracy=68.957, wps=14410.9, ups=1.73, wpb=8308.2, bsz=299.3, num_updates=41200, lr=6.96733e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=57, gb_free=16.1, wall=29360
2023-08-12 10:16:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:5')
2023-08-12 10:16:36 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 4.085 | trans_loss 5.154 | nll_loss 2.409 | w2v_ctc_loss 1.364 | task_loss 0 | contrastive_loss 0.289 | total 4003.4 | n_correct 2677.1 | ppl 5.31 | accuracy 66.871 | uer 17.235 | wer 19.019 | raw_wer 19.019 | bleu 22.33 | wps 2284.8 | wpb 4003.4 | bsz 141.8 | num_updates 41254 | best_bleu 22.61
2023-08-12 10:16:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 41254 updates
2023-08-12 10:16:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.3308.pt
2023-08-12 10:16:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.3308.pt
2023-08-12 10:16:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.3308.pt (epoch 28 @ 41254 updates, score 22.33) (writing took 21.110179215669632 seconds)
2023-08-12 10:16:57 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-08-12 10:16:57 | INFO | train | epoch 028 | loss 1.97 | trans_loss 4.768 | nll_loss 1.969 | w2v_ctc_loss 0.641 | task_loss 0 | contrastive_loss 0.117 | total 4137.68 | n_correct 2857.82 | ppl 3.91 | accuracy 69.068 | wps 12975 | ups 1.57 | wpb 8275.4 | bsz 305.3 | num_updates 41254 | lr 6.96277e-05 | gnorm 0.547 | clip 0 | loss_scale 32 | train_wall 838 | gb_free 16.5 | wall 29434
2023-08-12 10:16:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 10:16:57 | INFO | fairseq.trainer | begin training epoch 29
2023-08-12 10:16:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 10:17:31 | INFO | train_inner | epoch 029:     46 / 1474 loss=1.966, trans_loss=4.761, nll_loss=1.961, w2v_ctc_loss=0.643, task_loss=0, contrastive_loss=0.087, total=4169.12, n_correct=2885.83, ppl=3.89, accuracy=69.219, wps=7696.1, ups=0.92, wpb=8338.2, bsz=316.4, num_updates=41300, lr=6.95889e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=56, gb_free=16.4, wall=29468
2023-08-12 10:18:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-12 10:18:28 | INFO | train_inner | epoch 029:    147 / 1474 loss=1.966, trans_loss=4.76, nll_loss=1.958, w2v_ctc_loss=0.64, task_loss=0, contrastive_loss=0.103, total=4096.6, n_correct=2836.67, ppl=3.88, accuracy=69.244, wps=14218.4, ups=1.74, wpb=8193.2, bsz=301.8, num_updates=41400, lr=6.95048e-05, gnorm=0.554, clip=0, loss_scale=16, train_wall=57, gb_free=16.6, wall=29526
2023-08-12 10:19:26 | INFO | train_inner | epoch 029:    247 / 1474 loss=1.966, trans_loss=4.751, nll_loss=1.948, w2v_ctc_loss=0.621, task_loss=0, contrastive_loss=0.185, total=4197.89, n_correct=2912.61, ppl=3.86, accuracy=69.383, wps=14509.7, ups=1.73, wpb=8395.8, bsz=329.5, num_updates=41500, lr=6.9421e-05, gnorm=0.547, clip=0, loss_scale=16, train_wall=57, gb_free=17.5, wall=29584
2023-08-12 10:20:23 | INFO | train_inner | epoch 029:    347 / 1474 loss=1.965, trans_loss=4.767, nll_loss=1.966, w2v_ctc_loss=0.646, task_loss=0, contrastive_loss=0.072, total=4094.4, n_correct=2833.13, ppl=3.91, accuracy=69.195, wps=14373.4, ups=1.76, wpb=8188.8, bsz=291.2, num_updates=41600, lr=6.93375e-05, gnorm=0.558, clip=0, loss_scale=16, train_wall=56, gb_free=16.5, wall=29640
2023-08-12 10:21:20 | INFO | train_inner | epoch 029:    447 / 1474 loss=1.947, trans_loss=4.74, nll_loss=1.932, w2v_ctc_loss=0.627, task_loss=0, contrastive_loss=0.064, total=4157.41, n_correct=2896.9, ppl=3.82, accuracy=69.68, wps=14606.9, ups=1.76, wpb=8314.8, bsz=308.5, num_updates=41700, lr=6.92543e-05, gnorm=0.538, clip=0, loss_scale=16, train_wall=56, gb_free=14.9, wall=29697
2023-08-12 10:22:17 | INFO | train_inner | epoch 029:    547 / 1474 loss=1.974, trans_loss=4.767, nll_loss=1.967, w2v_ctc_loss=0.636, task_loss=0, contrastive_loss=0.161, total=4149.27, n_correct=2860.87, ppl=3.91, accuracy=68.949, wps=14469.8, ups=1.74, wpb=8298.5, bsz=293.3, num_updates=41800, lr=6.91714e-05, gnorm=0.543, clip=0, loss_scale=16, train_wall=57, gb_free=15.5, wall=29755
2023-08-12 10:23:15 | INFO | train_inner | epoch 029:    647 / 1474 loss=1.971, trans_loss=4.754, nll_loss=1.952, w2v_ctc_loss=0.629, task_loss=0, contrastive_loss=0.228, total=4145.39, n_correct=2876.96, ppl=3.87, accuracy=69.401, wps=14428.7, ups=1.74, wpb=8290.8, bsz=319.3, num_updates=41900, lr=6.90889e-05, gnorm=0.551, clip=0, loss_scale=16, train_wall=57, gb_free=17.5, wall=29812
2023-08-12 10:24:12 | INFO | train_inner | epoch 029:    747 / 1474 loss=1.964, trans_loss=4.755, nll_loss=1.952, w2v_ctc_loss=0.626, task_loss=0, contrastive_loss=0.146, total=4242.46, n_correct=2942.17, ppl=3.87, accuracy=69.351, wps=14727.6, ups=1.74, wpb=8484.9, bsz=329.9, num_updates=42000, lr=6.90066e-05, gnorm=0.549, clip=0, loss_scale=16, train_wall=57, gb_free=16.6, wall=29870
2023-08-12 10:24:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 10:24:34 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 4.088 | trans_loss 5.158 | nll_loss 2.412 | w2v_ctc_loss 1.366 | task_loss 0 | contrastive_loss 0.284 | total 4003.4 | n_correct 2677.3 | ppl 5.32 | accuracy 66.876 | uer 17.291 | wer 19.131 | raw_wer 19.131 | bleu 22.24 | wps 2075.7 | wpb 4003.4 | bsz 141.8 | num_updates 42000 | best_bleu 22.61
2023-08-12 10:24:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42000 updates
2023-08-12 10:24:34 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_29_42000.pt
2023-08-12 10:24:38 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_29_42000.pt
2023-08-12 10:24:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_29_42000.pt (epoch 29 @ 42000 updates, score 22.24) (writing took 19.08554570376873 seconds)
2023-08-12 10:25:51 | INFO | train_inner | epoch 029:    847 / 1474 loss=1.965, trans_loss=4.772, nll_loss=1.974, w2v_ctc_loss=0.64, task_loss=0, contrastive_loss=0.064, total=4027.03, n_correct=2775.16, ppl=3.93, accuracy=68.913, wps=8179.4, ups=1.02, wpb=8054.1, bsz=280.3, num_updates=42100, lr=6.89246e-05, gnorm=0.55, clip=0, loss_scale=16, train_wall=56, gb_free=17.4, wall=29968
2023-08-12 10:26:48 | INFO | train_inner | epoch 029:    947 / 1474 loss=1.963, trans_loss=4.764, nll_loss=1.964, w2v_ctc_loss=0.64, task_loss=0, contrastive_loss=0.076, total=4086.72, n_correct=2825.3, ppl=3.9, accuracy=69.134, wps=14398, ups=1.76, wpb=8173.4, bsz=296.3, num_updates=42200, lr=6.88428e-05, gnorm=0.56, clip=0, loss_scale=16, train_wall=56, gb_free=15.5, wall=30025
2023-08-12 10:27:45 | INFO | train_inner | epoch 029:   1047 / 1474 loss=1.963, trans_loss=4.757, nll_loss=1.955, w2v_ctc_loss=0.628, task_loss=0, contrastive_loss=0.146, total=4139.4, n_correct=2872.9, ppl=3.88, accuracy=69.404, wps=14409.1, ups=1.74, wpb=8278.8, bsz=307.4, num_updates=42300, lr=6.87614e-05, gnorm=0.546, clip=0, loss_scale=16, train_wall=57, gb_free=15.6, wall=30083
2023-08-12 10:28:42 | INFO | train_inner | epoch 029:   1147 / 1474 loss=1.963, trans_loss=4.771, nll_loss=1.972, w2v_ctc_loss=0.642, task_loss=0, contrastive_loss=0.062, total=4072.33, n_correct=2813.73, ppl=3.92, accuracy=69.094, wps=14294.1, ups=1.76, wpb=8144.7, bsz=284.1, num_updates=42400, lr=6.86803e-05, gnorm=0.549, clip=0, loss_scale=16, train_wall=56, gb_free=16.9, wall=30140
2023-08-12 10:29:39 | INFO | train_inner | epoch 029:   1247 / 1474 loss=1.963, trans_loss=4.766, nll_loss=1.967, w2v_ctc_loss=0.641, task_loss=0, contrastive_loss=0.069, total=4160.52, n_correct=2876.53, ppl=3.91, accuracy=69.139, wps=14567.1, ups=1.75, wpb=8321, bsz=301.5, num_updates=42500, lr=6.85994e-05, gnorm=0.543, clip=0, loss_scale=16, train_wall=57, gb_free=15.7, wall=30197
2023-08-12 10:30:37 | INFO | train_inner | epoch 029:   1347 / 1474 loss=1.966, trans_loss=4.757, nll_loss=1.955, w2v_ctc_loss=0.634, task_loss=0, contrastive_loss=0.131, total=4168.02, n_correct=2886.73, ppl=3.88, accuracy=69.259, wps=14517.7, ups=1.74, wpb=8336, bsz=310.2, num_updates=42600, lr=6.85189e-05, gnorm=0.547, clip=0, loss_scale=16, train_wall=57, gb_free=16.6, wall=30254
2023-08-12 10:31:34 | INFO | train_inner | epoch 029:   1447 / 1474 loss=1.971, trans_loss=4.759, nll_loss=1.958, w2v_ctc_loss=0.641, task_loss=0, contrastive_loss=0.161, total=4166.06, n_correct=2884.76, ppl=3.89, accuracy=69.244, wps=14547, ups=1.75, wpb=8332.1, bsz=313.1, num_updates=42700, lr=6.84386e-05, gnorm=0.551, clip=0, loss_scale=16, train_wall=57, gb_free=16.9, wall=30311
2023-08-12 10:31:49 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 10:32:12 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 4.084 | trans_loss 5.151 | nll_loss 2.404 | w2v_ctc_loss 1.369 | task_loss 0 | contrastive_loss 0.287 | total 4003.4 | n_correct 2676.9 | ppl 5.29 | accuracy 66.866 | uer 17.203 | wer 19.067 | raw_wer 19.067 | bleu 22.45 | wps 2227.9 | wpb 4003.4 | bsz 141.8 | num_updates 42727 | best_bleu 22.61
2023-08-12 10:32:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42727 updates
2023-08-12 10:32:12 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.4507.pt
2023-08-12 10:32:16 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.4507.pt
2023-08-12 10:32:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.4507.pt (epoch 29 @ 42727 updates, score 22.45) (writing took 19.520957173779607 seconds)
2023-08-12 10:32:32 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-08-12 10:32:32 | INFO | train | epoch 029 | loss 1.965 | trans_loss 4.76 | nll_loss 1.958 | w2v_ctc_loss 0.635 | task_loss 0 | contrastive_loss 0.119 | total 4137.79 | n_correct 2865.29 | ppl 3.89 | accuracy 69.247 | wps 13034.8 | ups 1.58 | wpb 8275.6 | bsz 305.4 | num_updates 42727 | lr 6.8417e-05 | gnorm 0.549 | clip 0 | loss_scale 16 | train_wall 835 | gb_free 16.1 | wall 30370
2023-08-12 10:32:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 10:32:33 | INFO | fairseq.trainer | begin training epoch 30
2023-08-12 10:32:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 10:33:21 | INFO | train_inner | epoch 030:     73 / 1474 loss=1.964, trans_loss=4.751, nll_loss=1.947, w2v_ctc_loss=0.623, task_loss=0, contrastive_loss=0.177, total=4175.11, n_correct=2896.81, ppl=3.86, accuracy=69.383, wps=7770.4, ups=0.93, wpb=8350.2, bsz=318.6, num_updates=42800, lr=6.83586e-05, gnorm=0.542, clip=0, loss_scale=16, train_wall=57, gb_free=17.1, wall=30419
2023-08-12 10:34:19 | INFO | train_inner | epoch 030:    173 / 1474 loss=1.954, trans_loss=4.735, nll_loss=1.926, w2v_ctc_loss=0.631, task_loss=0, contrastive_loss=0.11, total=4202.64, n_correct=2931.12, ppl=3.8, accuracy=69.745, wps=14617.1, ups=1.74, wpb=8405.3, bsz=318.3, num_updates=42900, lr=6.82789e-05, gnorm=0.549, clip=0, loss_scale=16, train_wall=57, gb_free=16.7, wall=30476
2023-08-12 10:35:16 | INFO | train_inner | epoch 030:    273 / 1474 loss=1.956, trans_loss=4.752, nll_loss=1.948, w2v_ctc_loss=0.637, task_loss=0, contrastive_loss=0.065, total=4120.21, n_correct=2863.47, ppl=3.86, accuracy=69.498, wps=14454, ups=1.75, wpb=8240.4, bsz=294.9, num_updates=43000, lr=6.81994e-05, gnorm=0.539, clip=0, loss_scale=16, train_wall=56, gb_free=15.4, wall=30533
2023-08-12 10:36:13 | INFO | train_inner | epoch 030:    373 / 1474 loss=1.946, trans_loss=4.739, nll_loss=1.932, w2v_ctc_loss=0.624, task_loss=0, contrastive_loss=0.067, total=4178.23, n_correct=2913.04, ppl=3.82, accuracy=69.719, wps=14561.8, ups=1.74, wpb=8356.5, bsz=307.5, num_updates=43100, lr=6.81203e-05, gnorm=0.538, clip=0, loss_scale=16, train_wall=57, gb_free=10.5, wall=30591
2023-08-12 10:37:10 | INFO | train_inner | epoch 030:    473 / 1474 loss=1.956, trans_loss=4.747, nll_loss=1.942, w2v_ctc_loss=0.622, task_loss=0, contrastive_loss=0.13, total=4124.47, n_correct=2869.38, ppl=3.84, accuracy=69.57, wps=14556.3, ups=1.76, wpb=8248.9, bsz=312.6, num_updates=43200, lr=6.80414e-05, gnorm=0.547, clip=0, loss_scale=16, train_wall=56, gb_free=17.6, wall=30647
2023-08-12 10:38:08 | INFO | train_inner | epoch 030:    573 / 1474 loss=1.956, trans_loss=4.75, nll_loss=1.946, w2v_ctc_loss=0.629, task_loss=0, contrastive_loss=0.092, total=4168.41, n_correct=2896.42, ppl=3.85, accuracy=69.485, wps=14500.9, ups=1.74, wpb=8336.8, bsz=312.4, num_updates=43300, lr=6.79628e-05, gnorm=0.546, clip=0, loss_scale=16, train_wall=57, gb_free=17.3, wall=30705
2023-08-12 10:39:05 | INFO | train_inner | epoch 030:    673 / 1474 loss=1.963, trans_loss=4.752, nll_loss=1.948, w2v_ctc_loss=0.638, task_loss=0, contrastive_loss=0.106, total=4187.95, n_correct=2902.62, ppl=3.86, accuracy=69.309, wps=14499.1, ups=1.73, wpb=8375.9, bsz=315, num_updates=43400, lr=6.78844e-05, gnorm=0.551, clip=0, loss_scale=16, train_wall=57, gb_free=16, wall=30763
2023-08-12 10:40:03 | INFO | train_inner | epoch 030:    773 / 1474 loss=1.978, trans_loss=4.76, nll_loss=1.958, w2v_ctc_loss=0.647, task_loss=0, contrastive_loss=0.186, total=4105.32, n_correct=2838.61, ppl=3.88, accuracy=69.145, wps=14321.5, ups=1.74, wpb=8210.6, bsz=302.6, num_updates=43500, lr=6.78064e-05, gnorm=0.558, clip=0, loss_scale=32, train_wall=57, gb_free=13.2, wall=30820
2023-08-12 10:41:00 | INFO | train_inner | epoch 030:    873 / 1474 loss=1.955, trans_loss=4.754, nll_loss=1.95, w2v_ctc_loss=0.628, task_loss=0, contrastive_loss=0.078, total=4102.11, n_correct=2848.57, ppl=3.86, accuracy=69.442, wps=14407.1, ups=1.76, wpb=8204.2, bsz=295.6, num_updates=43600, lr=6.77285e-05, gnorm=0.559, clip=0, loss_scale=32, train_wall=56, gb_free=17.5, wall=30877
2023-08-12 10:41:57 | INFO | train_inner | epoch 030:    973 / 1474 loss=1.962, trans_loss=4.759, nll_loss=1.957, w2v_ctc_loss=0.639, task_loss=0, contrastive_loss=0.08, total=4129.98, n_correct=2858.19, ppl=3.88, accuracy=69.206, wps=14414.3, ups=1.75, wpb=8260, bsz=300.4, num_updates=43700, lr=6.7651e-05, gnorm=0.557, clip=0, loss_scale=32, train_wall=57, gb_free=16.4, wall=30934
2023-08-12 10:42:54 | INFO | train_inner | epoch 030:   1073 / 1474 loss=1.971, trans_loss=4.762, nll_loss=1.96, w2v_ctc_loss=0.641, task_loss=0, contrastive_loss=0.159, total=4101.17, n_correct=2833.34, ppl=3.89, accuracy=69.086, wps=14288.3, ups=1.74, wpb=8202.3, bsz=282.3, num_updates=43800, lr=6.75737e-05, gnorm=0.553, clip=0, loss_scale=32, train_wall=57, gb_free=15.7, wall=30992
2023-08-12 10:43:52 | INFO | train_inner | epoch 030:   1173 / 1474 loss=1.958, trans_loss=4.752, nll_loss=1.949, w2v_ctc_loss=0.619, task_loss=0, contrastive_loss=0.137, total=4168.36, n_correct=2893.99, ppl=3.86, accuracy=69.428, wps=14494.5, ups=1.74, wpb=8336.7, bsz=314, num_updates=43900, lr=6.74967e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=57, gb_free=16, wall=31049
2023-08-12 10:44:49 | INFO | train_inner | epoch 030:   1273 / 1474 loss=1.961, trans_loss=4.758, nll_loss=1.956, w2v_ctc_loss=0.641, task_loss=0, contrastive_loss=0.072, total=4036.17, n_correct=2793.68, ppl=3.88, accuracy=69.216, wps=14119.4, ups=1.75, wpb=8072.3, bsz=284.3, num_updates=44000, lr=6.742e-05, gnorm=0.562, clip=0, loss_scale=32, train_wall=57, gb_free=15.8, wall=31106
2023-08-12 10:44:49 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 10:45:12 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 4.092 | trans_loss 5.152 | nll_loss 2.405 | w2v_ctc_loss 1.394 | task_loss 0 | contrastive_loss 0.284 | total 4003.4 | n_correct 2685.2 | ppl 5.3 | accuracy 67.073 | uer 17.434 | wer 19.272 | raw_wer 19.272 | bleu 22.85 | wps 2318.9 | wpb 4003.4 | bsz 141.8 | num_updates 44000 | best_bleu 22.85
2023-08-12 10:45:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44000 updates
2023-08-12 10:45:12 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_30_44000.pt
2023-08-12 10:45:16 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_30_44000.pt
2023-08-12 10:46:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_30_44000.pt (epoch 30 @ 44000 updates, score 22.85) (writing took 54.93013853020966 seconds)
2023-08-12 10:47:05 | INFO | train_inner | epoch 030:   1373 / 1474 loss=1.956, trans_loss=4.754, nll_loss=1.952, w2v_ctc_loss=0.63, task_loss=0, contrastive_loss=0.081, total=4165.07, n_correct=2890.91, ppl=3.87, accuracy=69.408, wps=6143.1, ups=0.74, wpb=8330.1, bsz=321.6, num_updates=44100, lr=6.73435e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=56, gb_free=16.7, wall=31242
2023-08-12 10:48:02 | INFO | train_inner | epoch 030:   1473 / 1474 loss=1.968, trans_loss=4.757, nll_loss=1.955, w2v_ctc_loss=0.622, task_loss=0, contrastive_loss=0.226, total=4141.76, n_correct=2872.27, ppl=3.88, accuracy=69.349, wps=14503.1, ups=1.75, wpb=8283.5, bsz=314.3, num_updates=44200, lr=6.72673e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=57, gb_free=16.4, wall=31299
2023-08-12 10:48:02 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 10:48:25 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 4.085 | trans_loss 5.155 | nll_loss 2.408 | w2v_ctc_loss 1.363 | task_loss 0 | contrastive_loss 0.285 | total 4003.4 | n_correct 2679.2 | ppl 5.31 | accuracy 66.923 | uer 17.644 | wer 19.731 | raw_wer 19.731 | bleu 22.58 | wps 2331.2 | wpb 4003.4 | bsz 141.8 | num_updates 44201 | best_bleu 22.85
2023-08-12 10:48:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44201 updates
2023-08-12 10:48:25 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.5806.pt
2023-08-12 10:48:28 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.5806.pt
2023-08-12 10:48:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.5806.pt (epoch 30 @ 44201 updates, score 22.58) (writing took 20.615615447983146 seconds)
2023-08-12 10:48:46 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-08-12 10:48:46 | INFO | train | epoch 030 | loss 1.96 | trans_loss 4.752 | nll_loss 1.948 | w2v_ctc_loss 0.631 | task_loss 0 | contrastive_loss 0.119 | total 4138.65 | n_correct 2872.33 | ppl 3.86 | accuracy 69.403 | wps 12536.2 | ups 1.51 | wpb 8277.3 | bsz 305.7 | num_updates 44201 | lr 6.72665e-05 | gnorm 0.55 | clip 0 | loss_scale 32 | train_wall 836 | gb_free 17.2 | wall 31343
2023-08-12 10:48:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 10:48:46 | INFO | fairseq.trainer | begin training epoch 31
2023-08-12 10:48:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 10:49:50 | INFO | train_inner | epoch 031:     99 / 1474 loss=1.951, trans_loss=4.741, nll_loss=1.934, w2v_ctc_loss=0.633, task_loss=0, contrastive_loss=0.066, total=4054.44, n_correct=2823.39, ppl=3.82, accuracy=69.637, wps=7473.2, ups=0.92, wpb=8108.9, bsz=288.2, num_updates=44300, lr=6.71913e-05, gnorm=0.564, clip=0, loss_scale=32, train_wall=57, gb_free=16.5, wall=31408
2023-08-12 10:50:47 | INFO | train_inner | epoch 031:    199 / 1474 loss=1.952, trans_loss=4.742, nll_loss=1.935, w2v_ctc_loss=0.629, task_loss=0, contrastive_loss=0.093, total=4147.4, n_correct=2886.78, ppl=3.82, accuracy=69.605, wps=14502.6, ups=1.75, wpb=8294.8, bsz=302.2, num_updates=44400, lr=6.71156e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=57, gb_free=16.9, wall=31465
2023-08-12 10:51:45 | INFO | train_inner | epoch 031:    299 / 1474 loss=1.954, trans_loss=4.738, nll_loss=1.93, w2v_ctc_loss=0.624, task_loss=0, contrastive_loss=0.132, total=4149.21, n_correct=2890.85, ppl=3.81, accuracy=69.672, wps=14293.9, ups=1.72, wpb=8298.4, bsz=301.6, num_updates=44500, lr=6.70402e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=57, gb_free=16.2, wall=31523
2023-08-12 10:52:43 | INFO | train_inner | epoch 031:    399 / 1474 loss=1.951, trans_loss=4.75, nll_loss=1.945, w2v_ctc_loss=0.626, task_loss=0, contrastive_loss=0.07, total=4092.62, n_correct=2842.23, ppl=3.85, accuracy=69.448, wps=14320.2, ups=1.75, wpb=8185.2, bsz=285.6, num_updates=44600, lr=6.6965e-05, gnorm=0.558, clip=0, loss_scale=32, train_wall=57, gb_free=17.2, wall=31580
2023-08-12 10:53:40 | INFO | train_inner | epoch 031:    499 / 1474 loss=1.955, trans_loss=4.744, nll_loss=1.937, w2v_ctc_loss=0.637, task_loss=0, contrastive_loss=0.078, total=4111.85, n_correct=2858.06, ppl=3.83, accuracy=69.508, wps=14242.8, ups=1.73, wpb=8223.7, bsz=300.3, num_updates=44700, lr=6.689e-05, gnorm=0.555, clip=0, loss_scale=32, train_wall=57, gb_free=11.4, wall=31638
2023-08-12 10:54:38 | INFO | train_inner | epoch 031:    599 / 1474 loss=1.944, trans_loss=4.739, nll_loss=1.931, w2v_ctc_loss=0.62, task_loss=0, contrastive_loss=0.068, total=4083.44, n_correct=2846.45, ppl=3.81, accuracy=69.707, wps=14190.8, ups=1.74, wpb=8166.9, bsz=294.5, num_updates=44800, lr=6.68153e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=57, gb_free=16.8, wall=31695
2023-08-12 10:55:35 | INFO | train_inner | epoch 031:    699 / 1474 loss=1.945, trans_loss=4.739, nll_loss=1.931, w2v_ctc_loss=0.619, task_loss=0, contrastive_loss=0.068, total=4213.98, n_correct=2937.55, ppl=3.81, accuracy=69.71, wps=14731.3, ups=1.75, wpb=8428, bsz=315.7, num_updates=44900, lr=6.67409e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=57, gb_free=16.3, wall=31753
2023-08-12 10:56:33 | INFO | train_inner | epoch 031:    799 / 1474 loss=1.962, trans_loss=4.75, nll_loss=1.945, w2v_ctc_loss=0.629, task_loss=0, contrastive_loss=0.139, total=4097.37, n_correct=2840.97, ppl=3.85, accuracy=69.336, wps=14264.3, ups=1.74, wpb=8194.7, bsz=295.8, num_updates=45000, lr=6.66667e-05, gnorm=0.552, clip=0, loss_scale=32, train_wall=57, gb_free=13.2, wall=31810
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:0')
2023-08-12 10:57:30 | INFO | train_inner | epoch 031:    899 / 1474 loss=1.949, trans_loss=4.736, nll_loss=1.927, w2v_ctc_loss=0.628, task_loss=0, contrastive_loss=0.085, total=4096.72, n_correct=2853.55, ppl=3.8, accuracy=69.655, wps=14337.5, ups=1.75, wpb=8193.4, bsz=296.1, num_updates=45100, lr=6.65927e-05, gnorm=0.552, clip=0, loss_scale=32, train_wall=57, gb_free=17.3, wall=31867
2023-08-12 10:58:27 | INFO | train_inner | epoch 031:    999 / 1474 loss=1.963, trans_loss=4.752, nll_loss=1.949, w2v_ctc_loss=0.625, task_loss=0, contrastive_loss=0.166, total=4187.84, n_correct=2907.69, ppl=3.86, accuracy=69.432, wps=14689.9, ups=1.75, wpb=8375.7, bsz=319.5, num_updates=45200, lr=6.6519e-05, gnorm=0.552, clip=0, loss_scale=32, train_wall=57, gb_free=17.3, wall=31924
2023-08-12 10:59:24 | INFO | train_inner | epoch 031:   1099 / 1474 loss=1.955, trans_loss=4.746, nll_loss=1.94, w2v_ctc_loss=0.623, task_loss=0, contrastive_loss=0.114, total=4149.44, n_correct=2886.7, ppl=3.84, accuracy=69.568, wps=14458.2, ups=1.74, wpb=8298.9, bsz=315, num_updates=45300, lr=6.64455e-05, gnorm=0.552, clip=0, loss_scale=32, train_wall=57, gb_free=17.6, wall=31982
2023-08-12 11:00:22 | INFO | train_inner | epoch 031:   1199 / 1474 loss=1.964, trans_loss=4.746, nll_loss=1.94, w2v_ctc_loss=0.621, task_loss=0, contrastive_loss=0.229, total=4189.76, n_correct=2915.1, ppl=3.84, accuracy=69.577, wps=14602.4, ups=1.74, wpb=8379.5, bsz=321.3, num_updates=45400, lr=6.63723e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=57, gb_free=13.5, wall=32039
2023-08-12 11:01:19 | INFO | train_inner | epoch 031:   1299 / 1474 loss=1.953, trans_loss=4.751, nll_loss=1.948, w2v_ctc_loss=0.626, task_loss=0, contrastive_loss=0.073, total=4227.44, n_correct=2942.09, ppl=3.86, accuracy=69.595, wps=14774, ups=1.75, wpb=8454.9, bsz=326.3, num_updates=45500, lr=6.62994e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=57, gb_free=16.4, wall=32096
2023-08-12 11:01:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-12 11:02:17 | INFO | train_inner | epoch 031:   1400 / 1474 loss=1.972, trans_loss=4.748, nll_loss=1.944, w2v_ctc_loss=0.628, task_loss=0, contrastive_loss=0.238, total=4179.85, n_correct=2904.28, ppl=3.85, accuracy=69.483, wps=14313.2, ups=1.71, wpb=8359.7, bsz=322.9, num_updates=45600, lr=6.62266e-05, gnorm=0.555, clip=0, loss_scale=32, train_wall=58, gb_free=16.6, wall=32155
2023-08-12 11:02:59 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2338, device='cuda:2')
2023-08-12 11:03:21 | INFO | dev_st | epoch 031 | valid on 'dev_st' subset | loss 4.089 | trans_loss 5.155 | nll_loss 2.406 | w2v_ctc_loss 1.376 | task_loss 0 | contrastive_loss 0.29 | total 4003.4 | n_correct 2685.5 | ppl 5.3 | accuracy 67.08 | uer 17.097 | wer 18.966 | raw_wer 18.966 | bleu 22.46 | wps 2387.6 | wpb 4003.4 | bsz 141.8 | num_updates 45674 | best_bleu 22.85
2023-08-12 11:03:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 45674 updates
2023-08-12 11:03:21 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.4602.pt
2023-08-12 11:03:24 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.4602.pt
2023-08-12 11:03:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.4602.pt (epoch 31 @ 45674 updates, score 22.46) (writing took 20.0478996001184 seconds)
2023-08-12 11:03:41 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-08-12 11:03:41 | INFO | train | epoch 031 | loss 1.955 | trans_loss 4.745 | nll_loss 1.938 | w2v_ctc_loss 0.627 | task_loss 0 | contrastive_loss 0.115 | total 4137.93 | n_correct 2878.35 | ppl 3.83 | accuracy 69.56 | wps 13607.2 | ups 1.64 | wpb 8275.9 | bsz 305.4 | num_updates 45674 | lr 6.61729e-05 | gnorm 0.551 | clip 0 | loss_scale 32 | train_wall 838 | gb_free 12.4 | wall 32239
2023-08-12 11:03:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 11:03:42 | INFO | fairseq.trainer | begin training epoch 32
2023-08-12 11:03:42 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 11:04:04 | INFO | train_inner | epoch 032:     26 / 1474 loss=1.948, trans_loss=4.741, nll_loss=1.934, w2v_ctc_loss=0.627, task_loss=0, contrastive_loss=0.064, total=4040.88, n_correct=2813.17, ppl=3.82, accuracy=69.618, wps=7536.7, ups=0.93, wpb=8081.8, bsz=288.7, num_updates=45700, lr=6.61541e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=56, gb_free=15.7, wall=32262
2023-08-12 11:05:02 | INFO | train_inner | epoch 032:    126 / 1474 loss=1.932, trans_loss=4.719, nll_loss=1.906, w2v_ctc_loss=0.604, task_loss=0, contrastive_loss=0.074, total=4222.14, n_correct=2960.66, ppl=3.75, accuracy=70.122, wps=14720, ups=1.74, wpb=8444.3, bsz=322.6, num_updates=45800, lr=6.60819e-05, gnorm=0.538, clip=0, loss_scale=32, train_wall=57, gb_free=15.5, wall=32319
2023-08-12 11:05:59 | INFO | train_inner | epoch 032:    226 / 1474 loss=1.945, trans_loss=4.737, nll_loss=1.929, w2v_ctc_loss=0.618, task_loss=0, contrastive_loss=0.083, total=4159.77, n_correct=2901.98, ppl=3.81, accuracy=69.763, wps=14490, ups=1.74, wpb=8319.5, bsz=320.8, num_updates=45900, lr=6.60098e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=57, gb_free=16.3, wall=32377
2023-08-12 11:06:57 | INFO | train_inner | epoch 032:    326 / 1474 loss=1.935, trans_loss=4.721, nll_loss=1.908, w2v_ctc_loss=0.608, task_loss=0, contrastive_loss=0.078, total=4179.65, n_correct=2931.77, ppl=3.75, accuracy=70.144, wps=14516.2, ups=1.74, wpb=8359.3, bsz=313.8, num_updates=46000, lr=6.5938e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=57, gb_free=16.4, wall=32434
2023-08-12 11:06:57 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 11:07:19 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 4.089 | trans_loss 5.16 | nll_loss 2.413 | w2v_ctc_loss 1.37 | task_loss 0 | contrastive_loss 0.284 | total 4003.4 | n_correct 2680.6 | ppl 5.33 | accuracy 66.958 | uer 17.312 | wer 19.186 | raw_wer 19.186 | bleu 22.71 | wps 2348.9 | wpb 4003.4 | bsz 141.8 | num_updates 46000 | best_bleu 22.85
2023-08-12 11:07:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 46000 updates
2023-08-12 11:07:19 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_32_46000.pt
2023-08-12 11:07:24 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_32_46000.pt
2023-08-12 11:08:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_32_46000.pt (epoch 32 @ 46000 updates, score 22.71) (writing took 40.95965129323304 seconds)
2023-08-12 11:08:58 | INFO | train_inner | epoch 032:    426 / 1474 loss=1.938, trans_loss=4.726, nll_loss=1.914, w2v_ctc_loss=0.616, task_loss=0, contrastive_loss=0.075, total=4172.34, n_correct=2920.74, ppl=3.77, accuracy=70.002, wps=6874.4, ups=0.82, wpb=8344.7, bsz=309.9, num_updates=46100, lr=6.58665e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=57, gb_free=17.2, wall=32556
2023-08-12 11:09:56 | INFO | train_inner | epoch 032:    526 / 1474 loss=1.957, trans_loss=4.739, nll_loss=1.932, w2v_ctc_loss=0.624, task_loss=0, contrastive_loss=0.157, total=4191.15, n_correct=2921.79, ppl=3.82, accuracy=69.713, wps=14436.1, ups=1.72, wpb=8382.3, bsz=315.1, num_updates=46200, lr=6.57952e-05, gnorm=0.549, clip=0, loss_scale=32, train_wall=57, gb_free=15.1, wall=32614
2023-08-12 11:10:54 | INFO | train_inner | epoch 032:    626 / 1474 loss=1.948, trans_loss=4.74, nll_loss=1.932, w2v_ctc_loss=0.625, task_loss=0, contrastive_loss=0.082, total=4138.05, n_correct=2882.76, ppl=3.82, accuracy=69.665, wps=14324.3, ups=1.73, wpb=8276.1, bsz=299.5, num_updates=46300, lr=6.57241e-05, gnorm=0.549, clip=0, loss_scale=32, train_wall=57, gb_free=13.6, wall=32672
2023-08-12 11:11:51 | INFO | train_inner | epoch 032:    726 / 1474 loss=1.948, trans_loss=4.739, nll_loss=1.931, w2v_ctc_loss=0.63, task_loss=0, contrastive_loss=0.067, total=4156.23, n_correct=2899.2, ppl=3.81, accuracy=69.756, wps=14488.5, ups=1.74, wpb=8312.5, bsz=303, num_updates=46400, lr=6.56532e-05, gnorm=0.557, clip=0, loss_scale=32, train_wall=57, gb_free=16.6, wall=32729
2023-08-12 11:12:48 | INFO | train_inner | epoch 032:    826 / 1474 loss=1.942, trans_loss=4.736, nll_loss=1.927, w2v_ctc_loss=0.62, task_loss=0, contrastive_loss=0.064, total=4112.3, n_correct=2870.44, ppl=3.8, accuracy=69.801, wps=14503.7, ups=1.76, wpb=8224.6, bsz=293.9, num_updates=46500, lr=6.55826e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=56, gb_free=16, wall=32786
2023-08-12 11:13:46 | INFO | train_inner | epoch 032:    926 / 1474 loss=1.938, trans_loss=4.735, nll_loss=1.927, w2v_ctc_loss=0.611, task_loss=0, contrastive_loss=0.061, total=4139.37, n_correct=2891.44, ppl=3.8, accuracy=69.852, wps=14427.5, ups=1.74, wpb=8278.7, bsz=298.6, num_updates=46600, lr=6.55122e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=57, gb_free=13.1, wall=32843
2023-08-12 11:14:43 | INFO | train_inner | epoch 032:   1026 / 1474 loss=1.957, trans_loss=4.743, nll_loss=1.936, w2v_ctc_loss=0.625, task_loss=0, contrastive_loss=0.156, total=4121.85, n_correct=2868.07, ppl=3.83, accuracy=69.582, wps=14464.4, ups=1.75, wpb=8243.7, bsz=306.1, num_updates=46700, lr=6.5442e-05, gnorm=0.556, clip=0, loss_scale=32, train_wall=56, gb_free=16.9, wall=32900
2023-08-12 11:15:40 | INFO | train_inner | epoch 032:   1126 / 1474 loss=1.953, trans_loss=4.743, nll_loss=1.936, w2v_ctc_loss=0.628, task_loss=0, contrastive_loss=0.099, total=4015.59, n_correct=2789.9, ppl=3.83, accuracy=69.477, wps=14054.2, ups=1.75, wpb=8031.2, bsz=270.1, num_updates=46800, lr=6.5372e-05, gnorm=0.561, clip=0, loss_scale=32, train_wall=57, gb_free=17.3, wall=32957
2023-08-12 11:16:37 | INFO | train_inner | epoch 032:   1226 / 1474 loss=1.969, trans_loss=4.752, nll_loss=1.948, w2v_ctc_loss=0.625, task_loss=0, contrastive_loss=0.203, total=4153.44, n_correct=2880.47, ppl=3.86, accuracy=69.351, wps=14501.8, ups=1.75, wpb=8306.9, bsz=310.8, num_updates=46900, lr=6.53023e-05, gnorm=0.568, clip=0, loss_scale=32, train_wall=57, gb_free=16.3, wall=33014
2023-08-12 11:17:34 | INFO | train_inner | epoch 032:   1326 / 1474 loss=1.948, trans_loss=4.74, nll_loss=1.933, w2v_ctc_loss=0.628, task_loss=0, contrastive_loss=0.063, total=4075.86, n_correct=2837.72, ppl=3.82, accuracy=69.623, wps=14221, ups=1.74, wpb=8151.7, bsz=295.4, num_updates=47000, lr=6.52328e-05, gnorm=0.554, clip=0, loss_scale=32, train_wall=57, gb_free=16.9, wall=33072
2023-08-12 11:18:32 | INFO | train_inner | epoch 032:   1426 / 1474 loss=1.973, trans_loss=4.742, nll_loss=1.936, w2v_ctc_loss=0.631, task_loss=0, contrastive_loss=0.292, total=4116.4, n_correct=2862.87, ppl=3.83, accuracy=69.548, wps=14309.4, ups=1.74, wpb=8232.8, bsz=307.6, num_updates=47100, lr=6.51635e-05, gnorm=0.559, clip=0, loss_scale=32, train_wall=57, gb_free=16.8, wall=33129
2023-08-12 11:18:59 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 11:19:21 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 4.087 | trans_loss 5.151 | nll_loss 2.405 | w2v_ctc_loss 1.382 | task_loss 0 | contrastive_loss 0.287 | total 4003.4 | n_correct 2681.2 | ppl 5.3 | accuracy 66.973 | uer 17.11 | wer 18.858 | raw_wer 18.858 | bleu 22.75 | wps 2398.8 | wpb 4003.4 | bsz 141.8 | num_updates 47148 | best_bleu 22.85
2023-08-12 11:19:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 47148 updates
2023-08-12 11:19:21 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.7500.pt
2023-08-12 11:19:25 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.7500.pt
2023-08-12 11:19:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint.best_bleu_22.7500.pt (epoch 32 @ 47148 updates, score 22.75) (writing took 22.182493336498737 seconds)
2023-08-12 11:19:44 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-08-12 11:19:44 | INFO | train | epoch 032 | loss 1.949 | trans_loss 4.737 | nll_loss 1.928 | w2v_ctc_loss 0.621 | task_loss 0 | contrastive_loss 0.116 | total 4138.65 | n_correct 2886.68 | ppl 3.81 | accuracy 69.749 | wps 12681.1 | ups 1.53 | wpb 8277.3 | bsz 305.7 | num_updates 47148 | lr 6.51303e-05 | gnorm 0.551 | clip 0 | loss_scale 32 | train_wall 837 | gb_free 16.5 | wall 33201
2023-08-12 11:19:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 11:19:44 | INFO | fairseq.trainer | begin training epoch 33
2023-08-12 11:19:44 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 11:20:21 | INFO | train_inner | epoch 033:     52 / 1474 loss=1.956, trans_loss=4.737, nll_loss=1.929, w2v_ctc_loss=0.615, task_loss=0, contrastive_loss=0.168, total=4149.21, n_correct=2894.51, ppl=3.81, accuracy=69.761, wps=7609.5, ups=0.92, wpb=8298.4, bsz=320.5, num_updates=47200, lr=6.50945e-05, gnorm=0.558, clip=0, loss_scale=32, train_wall=57, gb_free=17.1, wall=33238
2023-08-12 11:21:18 | INFO | train_inner | epoch 033:    152 / 1474 loss=1.928, trans_loss=4.719, nll_loss=1.904, w2v_ctc_loss=0.603, task_loss=0, contrastive_loss=0.055, total=4073.9, n_correct=2857.09, ppl=3.74, accuracy=70.132, wps=14343, ups=1.76, wpb=8147.8, bsz=284.8, num_updates=47300, lr=6.50256e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=56, gb_free=15.4, wall=33295
2023-08-12 11:22:16 | INFO | train_inner | epoch 033:    252 / 1474 loss=1.959, trans_loss=4.724, nll_loss=1.913, w2v_ctc_loss=0.612, task_loss=0, contrastive_loss=0.229, total=4280.14, n_correct=2995.74, ppl=3.77, accuracy=69.992, wps=14721.8, ups=1.72, wpb=8560.3, bsz=346.5, num_updates=47400, lr=6.4957e-05, gnorm=0.553, clip=0, loss_scale=32, train_wall=58, gb_free=16.5, wall=33353
2023-08-12 11:23:13 | INFO | train_inner | epoch 033:    352 / 1474 loss=1.939, trans_loss=4.727, nll_loss=1.916, w2v_ctc_loss=0.615, task_loss=0, contrastive_loss=0.082, total=4120.27, n_correct=2883.01, ppl=3.77, accuracy=69.971, wps=14370.6, ups=1.74, wpb=8240.5, bsz=300.7, num_updates=47500, lr=6.48886e-05, gnorm=0.552, clip=0, loss_scale=32, train_wall=57, gb_free=17.4, wall=33411
2023-08-12 11:24:10 | INFO | train_inner | epoch 033:    452 / 1474 loss=1.93, trans_loss=4.718, nll_loss=1.904, w2v_ctc_loss=0.609, task_loss=0, contrastive_loss=0.062, total=4141.22, n_correct=2906.7, ppl=3.74, accuracy=70.189, wps=14663.7, ups=1.77, wpb=8282.4, bsz=310.8, num_updates=47600, lr=6.48204e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=56, gb_free=16.5, wall=33467
2023-08-12 11:25:07 | INFO | train_inner | epoch 033:    552 / 1474 loss=1.945, trans_loss=4.731, nll_loss=1.92, w2v_ctc_loss=0.622, task_loss=0, contrastive_loss=0.085, total=4133.59, n_correct=2885.99, ppl=3.78, accuracy=69.818, wps=14475.6, ups=1.75, wpb=8267.2, bsz=294, num_updates=47700, lr=6.47524e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=57, gb_free=15.3, wall=33524
2023-08-12 11:26:05 | INFO | train_inner | epoch 033:    652 / 1474 loss=1.951, trans_loss=4.743, nll_loss=1.936, w2v_ctc_loss=0.617, task_loss=0, contrastive_loss=0.117, total=4157.63, n_correct=2892.38, ppl=3.83, accuracy=69.568, wps=14359.5, ups=1.73, wpb=8315.3, bsz=301.6, num_updates=47800, lr=6.46846e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=57, gb_free=17.8, wall=33582
2023-08-12 11:27:02 | INFO | train_inner | epoch 033:    752 / 1474 loss=1.947, trans_loss=4.736, nll_loss=1.927, w2v_ctc_loss=0.632, task_loss=0, contrastive_loss=0.064, total=4070.75, n_correct=2835.41, ppl=3.8, accuracy=69.653, wps=14253.7, ups=1.75, wpb=8141.5, bsz=287.2, num_updates=47900, lr=6.46171e-05, gnorm=0.564, clip=0, loss_scale=64, train_wall=57, gb_free=16.5, wall=33639
2023-08-12 11:27:59 | INFO | train_inner | epoch 033:    852 / 1474 loss=1.935, trans_loss=4.723, nll_loss=1.911, w2v_ctc_loss=0.598, task_loss=0, contrastive_loss=0.129, total=4130.24, n_correct=2895.49, ppl=3.76, accuracy=70.105, wps=14504.2, ups=1.76, wpb=8260.5, bsz=316.3, num_updates=48000, lr=6.45497e-05, gnorm=0.549, clip=0, loss_scale=64, train_wall=56, gb_free=16.7, wall=33696
2023-08-12 11:27:59 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 11:28:21 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 4.113 | trans_loss 5.164 | nll_loss 2.419 | w2v_ctc_loss 1.433 | task_loss 0 | contrastive_loss 0.292 | total 4003.4 | n_correct 2670.4 | ppl 5.35 | accuracy 66.703 | uer 17.591 | wer 19.399 | raw_wer 19.399 | bleu 22.42 | wps 2331.7 | wpb 4003.4 | bsz 141.8 | num_updates 48000 | best_bleu 22.85
2023-08-12 11:28:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48000 updates
2023-08-12 11:28:21 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_33_48000.pt
2023-08-12 11:28:25 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_33_48000.pt
2023-08-12 11:28:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_33_48000.pt (epoch 33 @ 48000 updates, score 22.42) (writing took 21.41546294465661 seconds)
2023-08-12 11:29:41 | INFO | train_inner | epoch 033:    952 / 1474 loss=1.945, trans_loss=4.731, nll_loss=1.922, w2v_ctc_loss=0.624, task_loss=0, contrastive_loss=0.076, total=4151.18, n_correct=2899.6, ppl=3.79, accuracy=69.85, wps=8148.9, ups=0.98, wpb=8302.4, bsz=308.3, num_updates=48100, lr=6.44826e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=57, gb_free=11.6, wall=33798
2023-08-12 11:30:39 | INFO | train_inner | epoch 033:   1052 / 1474 loss=1.952, trans_loss=4.728, nll_loss=1.917, w2v_ctc_loss=0.616, task_loss=0, contrastive_loss=0.179, total=4140.1, n_correct=2892.53, ppl=3.78, accuracy=69.866, wps=14140.8, ups=1.71, wpb=8280.2, bsz=307.6, num_updates=48200, lr=6.44157e-05, gnorm=0.551, clip=0, loss_scale=64, train_wall=58, gb_free=12, wall=33857
2023-08-12 11:31:37 | INFO | train_inner | epoch 033:   1152 / 1474 loss=1.952, trans_loss=4.739, nll_loss=1.931, w2v_ctc_loss=0.612, task_loss=0, contrastive_loss=0.165, total=4182.67, n_correct=2912.93, ppl=3.81, accuracy=69.643, wps=14606.7, ups=1.75, wpb=8365.3, bsz=309.5, num_updates=48300, lr=6.43489e-05, gnorm=0.553, clip=0, loss_scale=64, train_wall=57, gb_free=17.6, wall=33914
2023-08-12 11:32:34 | INFO | train_inner | epoch 033:   1252 / 1474 loss=1.942, trans_loss=4.73, nll_loss=1.92, w2v_ctc_loss=0.625, task_loss=0, contrastive_loss=0.067, total=4110.02, n_correct=2868.82, ppl=3.78, accuracy=69.801, wps=14372.7, ups=1.75, wpb=8220, bsz=294.4, num_updates=48400, lr=6.42824e-05, gnorm=0.553, clip=0, loss_scale=64, train_wall=57, gb_free=16.8, wall=33971
2023-08-12 11:33:31 | INFO | train_inner | epoch 033:   1352 / 1474 loss=1.944, trans_loss=4.733, nll_loss=1.924, w2v_ctc_loss=0.62, task_loss=0, contrastive_loss=0.086, total=4128.82, n_correct=2884.82, ppl=3.8, accuracy=69.87, wps=14428.3, ups=1.75, wpb=8257.6, bsz=312.4, num_updates=48500, lr=6.42161e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=57, gb_free=16.1, wall=34028
2023-08-12 11:34:29 | INFO | train_inner | epoch 033:   1452 / 1474 loss=1.956, trans_loss=4.732, nll_loss=1.923, w2v_ctc_loss=0.615, task_loss=0, contrastive_loss=0.231, total=4123.47, n_correct=2878.97, ppl=3.79, accuracy=69.819, wps=14181.4, ups=1.72, wpb=8246.9, bsz=308.9, num_updates=48600, lr=6.415e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=58, gb_free=16.6, wall=34087
2023-08-12 11:34:41 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 11:35:03 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 4.092 | trans_loss 5.162 | nll_loss 2.414 | w2v_ctc_loss 1.37 | task_loss 0 | contrastive_loss 0.292 | total 4003.4 | n_correct 2680.9 | ppl 5.33 | accuracy 66.966 | uer 17.166 | wer 19.164 | raw_wer 19.164 | bleu 22.32 | wps 2376.5 | wpb 4003.4 | bsz 141.8 | num_updates 48622 | best_bleu 22.85
2023-08-12 11:35:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48622 updates
2023-08-12 11:35:03 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-12 11:35:18 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-12 11:35:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_last.pt (epoch 33 @ 48622 updates, score 22.32) (writing took 14.72215954773128 seconds)
2023-08-12 11:35:18 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-08-12 11:35:18 | INFO | train | epoch 033 | loss 1.945 | trans_loss 4.73 | nll_loss 1.919 | w2v_ctc_loss 0.616 | task_loss 0 | contrastive_loss 0.115 | total 4138.65 | n_correct 2891.92 | ppl 3.78 | accuracy 69.876 | wps 13056.7 | ups 1.58 | wpb 8277.3 | bsz 305.7 | num_updates 48622 | lr 6.41355e-05 | gnorm 0.551 | clip 0 | loss_scale 64 | train_wall 837 | gb_free 17.9 | wall 34136
2023-08-12 11:35:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 11:35:18 | INFO | fairseq.trainer | begin training epoch 34
2023-08-12 11:35:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 11:36:10 | INFO | train_inner | epoch 034:     78 / 1474 loss=1.933, trans_loss=4.719, nll_loss=1.905, w2v_ctc_loss=0.612, task_loss=0, contrastive_loss=0.069, total=4128.94, n_correct=2895.08, ppl=3.75, accuracy=70.117, wps=8194.1, ups=0.99, wpb=8257.9, bsz=302.1, num_updates=48700, lr=6.40841e-05, gnorm=0.556, clip=0, loss_scale=64, train_wall=56, gb_free=15.3, wall=34187
2023-08-12 11:37:07 | INFO | train_inner | epoch 034:    178 / 1474 loss=1.928, trans_loss=4.708, nll_loss=1.891, w2v_ctc_loss=0.61, task_loss=0, contrastive_loss=0.07, total=4071.22, n_correct=2866.69, ppl=3.71, accuracy=70.414, wps=14276.1, ups=1.75, wpb=8142.4, bsz=295.1, num_updates=48800, lr=6.40184e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=57, gb_free=15.7, wall=34244
2023-08-12 11:38:05 | INFO | train_inner | epoch 034:    278 / 1474 loss=1.962, trans_loss=4.728, nll_loss=1.917, w2v_ctc_loss=0.611, task_loss=0, contrastive_loss=0.276, total=4237.89, n_correct=2960.45, ppl=3.78, accuracy=69.857, wps=14692.7, ups=1.73, wpb=8475.8, bsz=326.9, num_updates=48900, lr=6.39529e-05, gnorm=0.553, clip=0, loss_scale=64, train_wall=57, gb_free=10.9, wall=34302
2023-08-12 11:39:03 | INFO | train_inner | epoch 034:    378 / 1474 loss=1.937, trans_loss=4.71, nll_loss=1.894, w2v_ctc_loss=0.601, task_loss=0, contrastive_loss=0.164, total=4167, n_correct=2930.02, ppl=3.72, accuracy=70.315, wps=14361.7, ups=1.72, wpb=8334, bsz=319, num_updates=49000, lr=6.38877e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=57, gb_free=17.4, wall=34360
2023-08-12 11:40:00 | INFO | train_inner | epoch 034:    478 / 1474 loss=1.938, trans_loss=4.724, nll_loss=1.91, w2v_ctc_loss=0.623, task_loss=0, contrastive_loss=0.063, total=4071.65, n_correct=2847.84, ppl=3.76, accuracy=69.943, wps=14270.4, ups=1.75, wpb=8143.3, bsz=284.8, num_updates=49100, lr=6.38226e-05, gnorm=0.559, clip=0, loss_scale=64, train_wall=56, gb_free=11.9, wall=34417
2023-08-12 11:40:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-12 11:40:57 | INFO | train_inner | epoch 034:    579 / 1474 loss=1.931, trans_loss=4.713, nll_loss=1.897, w2v_ctc_loss=0.61, task_loss=0, contrastive_loss=0.066, total=4111.36, n_correct=2886.2, ppl=3.72, accuracy=70.201, wps=14361, ups=1.75, wpb=8222.7, bsz=299.9, num_updates=49200, lr=6.37577e-05, gnorm=0.554, clip=0, loss_scale=32, train_wall=57, gb_free=13.4, wall=34474
2023-08-12 11:41:54 | INFO | train_inner | epoch 034:    679 / 1474 loss=1.93, trans_loss=4.72, nll_loss=1.907, w2v_ctc_loss=0.607, task_loss=0, contrastive_loss=0.06, total=4124.83, n_correct=2892.41, ppl=3.75, accuracy=70.122, wps=14418.2, ups=1.75, wpb=8249.7, bsz=300.2, num_updates=49300, lr=6.3693e-05, gnorm=0.553, clip=0, loss_scale=32, train_wall=57, gb_free=14.4, wall=34532
2023-08-12 11:42:52 | INFO | train_inner | epoch 034:    779 / 1474 loss=1.943, trans_loss=4.739, nll_loss=1.931, w2v_ctc_loss=0.602, task_loss=0, contrastive_loss=0.129, total=4082.07, n_correct=2848.92, ppl=3.81, accuracy=69.791, wps=14157.8, ups=1.73, wpb=8164.1, bsz=295, num_updates=49400, lr=6.36285e-05, gnorm=0.554, clip=0, loss_scale=32, train_wall=57, gb_free=15.7, wall=34589
2023-08-12 11:43:49 | INFO | train_inner | epoch 034:    879 / 1474 loss=1.941, trans_loss=4.73, nll_loss=1.919, w2v_ctc_loss=0.615, task_loss=0, contrastive_loss=0.088, total=4100.9, n_correct=2869.18, ppl=3.78, accuracy=69.965, wps=14407.1, ups=1.76, wpb=8201.8, bsz=296.6, num_updates=49500, lr=6.35642e-05, gnorm=0.556, clip=0, loss_scale=32, train_wall=56, gb_free=12.5, wall=34646
2023-08-12 11:44:46 | INFO | train_inner | epoch 034:    979 / 1474 loss=1.942, trans_loss=4.727, nll_loss=1.915, w2v_ctc_loss=0.621, task_loss=0, contrastive_loss=0.083, total=4168.39, n_correct=2916.05, ppl=3.77, accuracy=69.956, wps=14567.4, ups=1.75, wpb=8336.8, bsz=311.9, num_updates=49600, lr=6.35001e-05, gnorm=0.55, clip=0, loss_scale=32, train_wall=57, gb_free=15.9, wall=34704
2023-08-12 11:45:43 | INFO | train_inner | epoch 034:   1079 / 1474 loss=1.939, trans_loss=4.725, nll_loss=1.912, w2v_ctc_loss=0.623, task_loss=0, contrastive_loss=0.066, total=4150.57, n_correct=2907.82, ppl=3.76, accuracy=70.058, wps=14547.4, ups=1.75, wpb=8301.1, bsz=308.5, num_updates=49700, lr=6.34361e-05, gnorm=0.549, clip=0, loss_scale=32, train_wall=56, gb_free=16.8, wall=34761
2023-08-12 11:46:41 | INFO | train_inner | epoch 034:   1179 / 1474 loss=1.937, trans_loss=4.726, nll_loss=1.915, w2v_ctc_loss=0.611, task_loss=0, contrastive_loss=0.077, total=4098.77, n_correct=2867.9, ppl=3.77, accuracy=69.97, wps=14221.9, ups=1.73, wpb=8197.5, bsz=297.1, num_updates=49800, lr=6.33724e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=57, gb_free=17, wall=34818
2023-08-12 11:47:38 | INFO | train_inner | epoch 034:   1279 / 1474 loss=1.933, trans_loss=4.721, nll_loss=1.908, w2v_ctc_loss=0.611, task_loss=0, contrastive_loss=0.062, total=4150.54, n_correct=2908.11, ppl=3.75, accuracy=70.066, wps=14570.2, ups=1.76, wpb=8301.1, bsz=301, num_updates=49900, lr=6.33089e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=56, gb_free=17.2, wall=34875
2023-08-12 11:48:35 | INFO | train_inner | epoch 034:   1379 / 1474 loss=1.952, trans_loss=4.73, nll_loss=1.92, w2v_ctc_loss=0.625, task_loss=0, contrastive_loss=0.128, total=4196.91, n_correct=2929.51, ppl=3.78, accuracy=69.802, wps=14572.2, ups=1.74, wpb=8393.8, bsz=321.4, num_updates=50000, lr=6.32456e-05, gnorm=0.55, clip=0, loss_scale=32, train_wall=57, gb_free=16.1, wall=34933
2023-08-12 11:48:35 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2023-08-12 11:48:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 11:48:58 | INFO | dev_st | epoch 034 | valid on 'dev_st' subset | loss 4.086 | trans_loss 5.16 | nll_loss 2.412 | w2v_ctc_loss 1.358 | task_loss 0 | contrastive_loss 0.285 | total 4003.4 | n_correct 2675.3 | ppl 5.32 | accuracy 66.826 | uer 16.935 | wer 18.825 | raw_wer 18.825 | bleu 22.41 | wps 2337.9 | wpb 4003.4 | bsz 141.8 | num_updates 50000 | best_bleu 22.85
2023-08-12 11:48:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 50000 updates
2023-08-12 11:48:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_34_50000.pt
2023-08-12 11:49:01 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_34_50000.pt
2023-08-12 11:49:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha1.5_mt0.5/checkpoint_34_50000.pt (epoch 34 @ 50000 updates, score 22.41) (writing took 39.58458238095045 seconds)
2023-08-12 11:49:40 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2023-08-12 11:49:40 | INFO | train | epoch 034 | loss 1.939 | trans_loss 4.723 | nll_loss 1.91 | w2v_ctc_loss 0.613 | task_loss 0 | contrastive_loss 0.102 | total 4132.97 | n_correct 2894.86 | ppl 3.76 | accuracy 70.043 | wps 13221 | ups 1.6 | wpb 8265.9 | bsz 304.3 | num_updates 50000 | lr 6.32456e-05 | gnorm 0.551 | clip 0 | loss_scale 32 | train_wall 782 | gb_free 16.1 | wall 34997
2023-08-12 11:49:40 | INFO | fairseq_cli.train | done training in 34943.2 seconds
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-6:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
Exception in thread Thread-7:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    raise EOFError
EOFError
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    raise EOFError
EOFError
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1472 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
