2023-08-06 10:42:30 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:16910
2023-08-06 10:42:30 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:16910
2023-08-06 10:42:30 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-06 10:42:30 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:16910
2023-08-06 10:42:30 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-06 10:42:31 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:16910
2023-08-06 10:42:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-06 10:42:31 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:16910
2023-08-06 10:42:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-06 10:42:31 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:16910
2023-08-06 10:42:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-06 10:42:31 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:16910
2023-08-06 10:42:31 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:16910
2023-08-06 10:42:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-06 10:42:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-06 10:42:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-06 10:42:31 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-06 10:42:31 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-06 10:42:31 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-06 10:42:31 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-06 10:42:31 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-06 10:42:31 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-06 10:42:31 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-06 10:42:31 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-06 10:42:31 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-06 10:42:31 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-06 10:42:31 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-06 10:42:31 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-06 10:42:31 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-06 10:42:31 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-06 10:42:31 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-06 10:42:31 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-06 10:42:35 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16910', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=2.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_rate=0.5, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=2.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_rate=0.5, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=2.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_rate=0.5, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-06 10:42:35 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-08-06 10:42:35 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-08-06 10:42:35 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-06 10:42:35 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-08-06 10:42:35 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-08-06 10:42:39 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-06 10:42:39 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-06 10:42:39 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-06 10:42:41 | INFO | root | load pretrained hubert
2023-08-06 10:42:42 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-08-06 10:42:44 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-08-06 10:42:46 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-08-06 10:42:46 | INFO | root | share the sematic adapter and textual encoder
2023-08-06 10:42:46 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-06 10:42:46 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-08-06 10:42:46 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-06 10:42:46 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-08-06 10:42:46 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-08-06 10:42:46 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-06 10:42:46 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-06 10:42:46 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-06 10:42:46 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-06 10:42:46 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-06 10:42:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-06 10:42:55 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-06 10:42:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-06 10:42:56 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-06 10:42:56 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-06 10:42:56 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-06 10:42:56 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-06 10:42:56 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-06 10:42:56 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-06 10:42:56 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-06 10:42:56 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-06 10:42:56 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-06 10:42:56 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-06 10:42:56 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-06 10:42:56 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-06 10:42:56 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_last.pt
2023-08-06 10:42:56 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_last.pt
2023-08-06 10:42:56 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-06 10:42:56 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-06 10:42:56 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-06 10:42:56 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-06 10:42:57 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-06 10:42:59 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-06 10:43:46 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-06 10:43:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 10:43:46 | INFO | fairseq.trainer | begin training epoch 1
2023-08-06 10:43:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 10:45:06 | INFO | train_inner | epoch 001:    100 / 1474 loss=20.947, trans_loss=5.598, nll_loss=4.162, w2v_ctc_loss=22.489, task_loss=1.749, contrastive_loss=3.325, total=4207.04, n_correct=209.46, ppl=17.9, accuracy=4.979, wps=19317.5, ups=1.54, wpb=12551.1, bsz=471.4, num_updates=100, lr=4.098e-06, gnorm=0.962, clip=0, loss_scale=128, train_wall=71, gb_free=19.5, wall=130
2023-08-06 10:46:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-06 10:46:11 | INFO | train_inner | epoch 001:    201 / 1474 loss=18.856, trans_loss=5.472, nll_loss=4.057, w2v_ctc_loss=19.512, task_loss=1.704, contrastive_loss=3.285, total=4123.37, n_correct=224.13, ppl=16.64, accuracy=5.436, wps=18915.1, ups=1.54, wpb=12310.5, bsz=462.6, num_updates=200, lr=8.096e-06, gnorm=3.523, clip=0, loss_scale=64, train_wall=65, gb_free=19.2, wall=195
2023-08-06 10:47:15 | INFO | train_inner | epoch 001:    301 / 1474 loss=11.839, trans_loss=5.494, nll_loss=4.138, w2v_ctc_loss=8.813, task_loss=1.706, contrastive_loss=3.202, total=4079.62, n_correct=203.44, ppl=17.6, accuracy=4.987, wps=19071.4, ups=1.56, wpb=12186.7, bsz=438.2, num_updates=300, lr=1.2094e-05, gnorm=4.65, clip=0, loss_scale=64, train_wall=63, gb_free=19.9, wall=259
2023-08-06 10:48:18 | INFO | train_inner | epoch 001:    401 / 1474 loss=10.583, trans_loss=5.517, nll_loss=4.189, w2v_ctc_loss=6.809, task_loss=1.496, contrastive_loss=3.233, total=4174.14, n_correct=196.12, ppl=18.24, accuracy=4.698, wps=19481.6, ups=1.56, wpb=12463.5, bsz=460.4, num_updates=400, lr=1.6092e-05, gnorm=2.949, clip=0, loss_scale=64, train_wall=64, gb_free=18.9, wall=323
2023-08-06 10:49:23 | INFO | train_inner | epoch 001:    501 / 1474 loss=10.141, trans_loss=5.495, nll_loss=4.177, w2v_ctc_loss=6.182, task_loss=1.369, contrastive_loss=3.219, total=4176.18, n_correct=191.43, ppl=18.09, accuracy=4.584, wps=19463.3, ups=1.56, wpb=12479.7, bsz=477.4, num_updates=500, lr=2.009e-05, gnorm=1.464, clip=0, loss_scale=64, train_wall=64, gb_free=19.2, wall=387
2023-08-06 10:50:28 | INFO | train_inner | epoch 001:    601 / 1474 loss=9.842, trans_loss=5.517, nll_loss=4.203, w2v_ctc_loss=5.828, task_loss=1.277, contrastive_loss=3.257, total=4147.79, n_correct=196.82, ppl=18.42, accuracy=4.745, wps=19010.6, ups=1.54, wpb=12371.6, bsz=484.2, num_updates=600, lr=2.4088e-05, gnorm=0.898, clip=0, loss_scale=64, train_wall=65, gb_free=18.9, wall=452
2023-08-06 10:51:31 | INFO | train_inner | epoch 001:    701 / 1474 loss=9.58, trans_loss=5.5, nll_loss=4.192, w2v_ctc_loss=5.732, task_loss=1.327, contrastive_loss=2.986, total=4152.1, n_correct=215.88, ppl=18.28, accuracy=5.199, wps=19505.6, ups=1.57, wpb=12395.5, bsz=456.2, num_updates=700, lr=2.8086e-05, gnorm=1.08, clip=0, loss_scale=64, train_wall=63, gb_free=19.5, wall=515
2023-08-06 10:52:35 | INFO | train_inner | epoch 001:    801 / 1474 loss=9.272, trans_loss=5.446, nll_loss=4.137, w2v_ctc_loss=5.575, task_loss=1.282, contrastive_loss=2.899, total=4123.83, n_correct=254.93, ppl=17.59, accuracy=6.182, wps=19355.8, ups=1.57, wpb=12306.1, bsz=464.1, num_updates=800, lr=3.2084e-05, gnorm=1.547, clip=0, loss_scale=64, train_wall=63, gb_free=19.2, wall=579
2023-08-06 10:53:38 | INFO | train_inner | epoch 001:    901 / 1474 loss=8.995, trans_loss=5.422, nll_loss=4.119, w2v_ctc_loss=5.485, task_loss=1.302, contrastive_loss=2.686, total=4163.61, n_correct=270.21, ppl=17.37, accuracy=6.49, wps=19534, ups=1.57, wpb=12433.9, bsz=457.1, num_updates=900, lr=3.6082e-05, gnorm=2.444, clip=0, loss_scale=64, train_wall=63, gb_free=18.9, wall=643
2023-08-06 10:54:43 | INFO | train_inner | epoch 001:   1001 / 1474 loss=8.709, trans_loss=5.399, nll_loss=4.096, w2v_ctc_loss=5.348, task_loss=1.31, contrastive_loss=2.563, total=4135.34, n_correct=291.79, ppl=17.1, accuracy=7.056, wps=19027.9, ups=1.54, wpb=12353.2, bsz=456.8, num_updates=1000, lr=4.008e-05, gnorm=2.629, clip=0, loss_scale=64, train_wall=64, gb_free=19, wall=708
2023-08-06 10:55:47 | INFO | train_inner | epoch 001:   1101 / 1474 loss=8.396, trans_loss=5.391, nll_loss=4.089, w2v_ctc_loss=5.211, task_loss=1.321, contrastive_loss=2.374, total=4147.38, n_correct=310.08, ppl=17.01, accuracy=7.477, wps=19456.2, ups=1.57, wpb=12367, bsz=454.9, num_updates=1100, lr=4.4078e-05, gnorm=3.105, clip=0, loss_scale=64, train_wall=63, gb_free=18.9, wall=771
2023-08-06 10:56:51 | INFO | train_inner | epoch 001:   1201 / 1474 loss=8.123, trans_loss=5.372, nll_loss=4.073, w2v_ctc_loss=5.096, task_loss=1.376, contrastive_loss=2.182, total=4139.9, n_correct=318.44, ppl=16.83, accuracy=7.692, wps=19397.2, ups=1.57, wpb=12366.5, bsz=440.1, num_updates=1200, lr=4.8076e-05, gnorm=3.066, clip=0, loss_scale=64, train_wall=63, gb_free=19, wall=835
2023-08-06 10:57:54 | INFO | train_inner | epoch 001:   1301 / 1474 loss=7.842, trans_loss=5.37, nll_loss=4.074, w2v_ctc_loss=4.928, task_loss=1.324, contrastive_loss=2.004, total=4046.58, n_correct=316.76, ppl=16.84, accuracy=7.828, wps=19140.2, ups=1.58, wpb=12081.6, bsz=439.3, num_updates=1300, lr=5.2074e-05, gnorm=3.13, clip=0, loss_scale=64, train_wall=63, gb_free=19.7, wall=898
2023-08-06 10:58:58 | INFO | train_inner | epoch 001:   1401 / 1474 loss=7.573, trans_loss=5.362, nll_loss=4.069, w2v_ctc_loss=4.735, task_loss=1.308, contrastive_loss=2.069, total=4133.18, n_correct=326.24, ppl=16.78, accuracy=7.893, wps=19123.7, ups=1.55, wpb=12350, bsz=455, num_updates=1400, lr=5.6072e-05, gnorm=2.84, clip=0, loss_scale=64, train_wall=64, gb_free=19.9, wall=963
2023-08-06 10:59:45 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 11:00:24 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 10.369 | trans_loss 10.977 | nll_loss 9.98 | w2v_ctc_loss 6.265 | task_loss 7.546 | contrastive_loss 2.484 | total 4003.4 | n_correct 375.6 | ppl 1009.83 | accuracy 9.382 | uer 79.248 | wer 78.726 | raw_wer 78.726 | bleu 0.02 | wps 1170.1 | wpb 4003.4 | bsz 141.8 | num_updates 1473
2023-08-06 11:00:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1473 updates
2023-08-06 11:00:24 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 11:00:27 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 11:00:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt (epoch 1 @ 1473 updates, score 0.02) (writing took 6.1524977050721645 seconds)
2023-08-06 11:00:30 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-08-06 11:00:30 | INFO | train | epoch 001 | loss 10.608 | trans_loss 5.45 | nll_loss 4.124 | w2v_ctc_loss 7.828 | task_loss 1.411 | contrastive_loss 2.768 | total 4138.5 | n_correct 255.772 | ppl 17.43 | accuracy 6.18 | wps 18391.1 | ups 1.49 | wpb 12355.3 | bsz 458.5 | num_updates 1473 | lr 5.89905e-05 | gnorm 2.475 | clip 0 | loss_scale 64 | train_wall 943 | gb_free 19.2 | wall 1054
2023-08-06 11:00:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 11:00:30 | INFO | fairseq.trainer | begin training epoch 2
2023-08-06 11:00:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 11:00:55 | INFO | train_inner | epoch 002:     27 / 1474 loss=7.327, trans_loss=5.36, nll_loss=4.063, w2v_ctc_loss=4.537, task_loss=1.246, contrastive_loss=1.916, total=4162.95, n_correct=333.51, ppl=16.71, accuracy=8.011, wps=10657.5, ups=0.86, wpb=12416.8, bsz=470.8, num_updates=1500, lr=6.007e-05, gnorm=2.972, clip=0, loss_scale=64, train_wall=63, gb_free=19.6, wall=1079
2023-08-06 11:01:59 | INFO | train_inner | epoch 002:    127 / 1474 loss=7.071, trans_loss=5.361, nll_loss=4.064, w2v_ctc_loss=4.412, task_loss=1.33, contrastive_loss=1.707, total=4155.98, n_correct=331.49, ppl=16.73, accuracy=7.976, wps=19355.6, ups=1.56, wpb=12394.8, bsz=451.6, num_updates=1600, lr=6.4068e-05, gnorm=3.11, clip=0, loss_scale=64, train_wall=64, gb_free=18.9, wall=1143
2023-08-06 11:03:03 | INFO | train_inner | epoch 002:    227 / 1474 loss=6.869, trans_loss=5.345, nll_loss=4.048, w2v_ctc_loss=4.195, task_loss=1.153, contrastive_loss=1.729, total=4179.21, n_correct=336.23, ppl=16.54, accuracy=8.045, wps=19463.2, ups=1.56, wpb=12484.6, bsz=488.9, num_updates=1700, lr=6.8066e-05, gnorm=2.757, clip=0, loss_scale=64, train_wall=64, gb_free=19, wall=1207
2023-08-06 11:04:08 | INFO | train_inner | epoch 002:    327 / 1474 loss=6.584, trans_loss=5.348, nll_loss=4.047, w2v_ctc_loss=4.085, task_loss=1.325, contrastive_loss=1.431, total=4146.1, n_correct=339.52, ppl=16.53, accuracy=8.189, wps=19054, ups=1.54, wpb=12374.1, bsz=447.8, num_updates=1800, lr=7.2064e-05, gnorm=2.487, clip=0, loss_scale=64, train_wall=64, gb_free=18.8, wall=1272
2023-08-06 11:05:12 | INFO | train_inner | epoch 002:    427 / 1474 loss=6.344, trans_loss=5.342, nll_loss=4.045, w2v_ctc_loss=3.965, task_loss=1.456, contrastive_loss=1.24, total=4037.99, n_correct=331.11, ppl=16.51, accuracy=8.2, wps=19019.6, ups=1.58, wpb=12069.2, bsz=415.4, num_updates=1900, lr=7.6062e-05, gnorm=2.569, clip=0, loss_scale=64, train_wall=63, gb_free=19, wall=1336
2023-08-06 11:06:15 | INFO | train_inner | epoch 002:    527 / 1474 loss=6.208, trans_loss=5.331, nll_loss=4.026, w2v_ctc_loss=3.771, task_loss=1.266, contrastive_loss=1.327, total=4176.97, n_correct=350.5, ppl=16.29, accuracy=8.391, wps=19633.7, ups=1.58, wpb=12463.5, bsz=468.6, num_updates=2000, lr=8.006e-05, gnorm=2.29, clip=0, loss_scale=64, train_wall=63, gb_free=19.6, wall=1399
2023-08-06 11:06:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 11:06:55 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 9.513 | trans_loss 10.871 | nll_loss 9.83 | w2v_ctc_loss 4.989 | task_loss 7.546 | contrastive_loss 1.671 | total 4003.4 | n_correct 393.9 | ppl 910.38 | accuracy 9.839 | uer 66.796 | wer 64.285 | raw_wer 64.285 | bleu 0.03 | wps 1144.6 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_bleu 0.03
2023-08-06 11:06:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-08-06 11:06:55 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_2_2000.pt
2023-08-06 11:06:58 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_2_2000.pt
2023-08-06 11:07:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.03) (writing took 23.554905926808715 seconds)
2023-08-06 11:08:23 | INFO | train_inner | epoch 002:    627 / 1474 loss=5.993, trans_loss=5.325, nll_loss=4.018, w2v_ctc_loss=3.64, task_loss=1.309, contrastive_loss=1.12, total=4126.49, n_correct=354.16, ppl=16.2, accuracy=8.583, wps=9648.8, ups=0.78, wpb=12314.9, bsz=445.5, num_updates=2100, lr=8.4058e-05, gnorm=2.073, clip=0, loss_scale=64, train_wall=64, gb_free=19.2, wall=1527
2023-08-06 11:09:27 | INFO | train_inner | epoch 002:    727 / 1474 loss=5.887, trans_loss=5.307, nll_loss=3.999, w2v_ctc_loss=3.522, task_loss=1.283, contrastive_loss=1.216, total=4149.06, n_correct=363.33, ppl=15.99, accuracy=8.757, wps=19260.7, ups=1.55, wpb=12386.4, bsz=465.4, num_updates=2200, lr=8.8056e-05, gnorm=1.896, clip=0, loss_scale=64, train_wall=64, gb_free=19.2, wall=1591
2023-08-06 11:10:31 | INFO | train_inner | epoch 002:    827 / 1474 loss=5.74, trans_loss=5.292, nll_loss=3.982, w2v_ctc_loss=3.43, task_loss=1.317, contrastive_loss=1.155, total=4175.4, n_correct=370.08, ppl=15.8, accuracy=8.863, wps=19586.1, ups=1.57, wpb=12471.9, bsz=460.9, num_updates=2300, lr=9.2054e-05, gnorm=1.716, clip=0, loss_scale=128, train_wall=63, gb_free=19.8, wall=1655
2023-08-06 11:11:34 | INFO | train_inner | epoch 002:    927 / 1474 loss=5.604, trans_loss=5.28, nll_loss=3.965, w2v_ctc_loss=3.314, task_loss=1.344, contrastive_loss=1.133, total=4104.2, n_correct=365.85, ppl=15.62, accuracy=8.914, wps=19410.4, ups=1.58, wpb=12253.1, bsz=445.9, num_updates=2400, lr=9.6052e-05, gnorm=1.698, clip=0, loss_scale=128, train_wall=63, gb_free=19, wall=1718
2023-08-06 11:12:38 | INFO | train_inner | epoch 002:   1027 / 1474 loss=5.47, trans_loss=5.274, nll_loss=3.96, w2v_ctc_loss=3.221, task_loss=1.305, contrastive_loss=0.982, total=4102.5, n_correct=370.95, ppl=15.56, accuracy=9.042, wps=19224.5, ups=1.57, wpb=12251.2, bsz=456.3, num_updates=2500, lr=0.00010005, gnorm=1.445, clip=0, loss_scale=128, train_wall=63, gb_free=19.2, wall=1782
2023-08-06 11:12:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-06 11:13:43 | INFO | train_inner | epoch 002:   1128 / 1474 loss=5.394, trans_loss=5.264, nll_loss=3.946, w2v_ctc_loss=3.129, task_loss=1.219, contrastive_loss=1.052, total=4166.43, n_correct=387.15, ppl=15.41, accuracy=9.292, wps=18940.1, ups=1.52, wpb=12432.5, bsz=474.9, num_updates=2600, lr=0.000104048, gnorm=1.463, clip=0, loss_scale=64, train_wall=65, gb_free=18.9, wall=1847
2023-08-06 11:14:47 | INFO | train_inner | epoch 002:   1228 / 1474 loss=5.331, trans_loss=5.249, nll_loss=3.928, w2v_ctc_loss=3.054, task_loss=1.197, contrastive_loss=1.103, total=4219.96, n_correct=402.49, ppl=15.22, accuracy=9.538, wps=19671.8, ups=1.56, wpb=12591.9, bsz=491.8, num_updates=2700, lr=0.000108046, gnorm=1.279, clip=0, loss_scale=64, train_wall=64, gb_free=18.9, wall=1911
2023-08-06 11:15:50 | INFO | train_inner | epoch 002:   1328 / 1474 loss=5.159, trans_loss=5.228, nll_loss=3.905, w2v_ctc_loss=3.003, task_loss=1.25, contrastive_loss=0.814, total=4163.26, n_correct=406.39, ppl=14.98, accuracy=9.761, wps=19666.1, ups=1.58, wpb=12441.6, bsz=463, num_updates=2800, lr=0.000112044, gnorm=1.196, clip=0, loss_scale=64, train_wall=63, gb_free=19, wall=1975
2023-08-06 11:16:54 | INFO | train_inner | epoch 002:   1428 / 1474 loss=5.094, trans_loss=5.237, nll_loss=3.915, w2v_ctc_loss=2.952, task_loss=1.417, contrastive_loss=0.895, total=4049.42, n_correct=393.61, ppl=15.08, accuracy=9.72, wps=19067.1, ups=1.58, wpb=12091.6, bsz=437.6, num_updates=2900, lr=0.000116042, gnorm=1.132, clip=0, loss_scale=64, train_wall=63, gb_free=19.2, wall=2038
2023-08-06 11:17:23 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 11:18:04 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 8.45 | trans_loss 10.308 | nll_loss 9.13 | w2v_ctc_loss 3.792 | task_loss 7.546 | contrastive_loss 0.972 | total 4003.4 | n_correct 494.7 | ppl 560.12 | accuracy 12.357 | uer 54.745 | wer 53.111 | raw_wer 53.111 | bleu 0.11 | wps 1139.8 | wpb 4003.4 | bsz 141.8 | num_updates 2946 | best_bleu 0.11
2023-08-06 11:18:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2946 updates
2023-08-06 11:18:04 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 11:18:17 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 11:18:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt (epoch 2 @ 2946 updates, score 0.11) (writing took 24.996744107455015 seconds)
2023-08-06 11:18:29 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-08-06 11:18:29 | INFO | train | epoch 002 | loss 5.909 | trans_loss 5.298 | nll_loss 3.988 | w2v_ctc_loss 3.547 | task_loss 1.294 | contrastive_loss 1.21 | total 4137.19 | n_correct 365.156 | ppl 15.86 | accuracy 8.826 | wps 16868.4 | ups 1.37 | wpb 12351.4 | bsz 457.7 | num_updates 2946 | lr 0.000117881 | gnorm 1.93 | clip 0 | loss_scale 64 | train_wall 935 | gb_free 19.3 | wall 2133
2023-08-06 11:18:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 11:18:29 | INFO | fairseq.trainer | begin training epoch 3
2023-08-06 11:18:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 11:19:11 | INFO | train_inner | epoch 003:     54 / 1474 loss=4.982, trans_loss=5.206, nll_loss=3.876, w2v_ctc_loss=2.88, task_loss=1.33, contrastive_loss=0.791, total=4067, n_correct=415.47, ppl=14.69, accuracy=10.216, wps=8886.5, ups=0.73, wpb=12142, bsz=441.4, num_updates=3000, lr=0.00012004, gnorm=1.095, clip=0, loss_scale=64, train_wall=64, gb_free=19.2, wall=2175
2023-08-06 11:19:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-06 11:19:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-06 11:19:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-06 11:19:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-06 11:20:46 | INFO | train_inner | epoch 003:    158 / 1474 loss=4.204, trans_loss=4.515, nll_loss=2.971, w2v_ctc_loss=2.539, task_loss=0.908, contrastive_loss=0.701, total=4144.24, n_correct=1043.6, ppl=7.84, accuracy=25.182, wps=13006.3, ups=1.05, wpb=12374.7, bsz=457.6, num_updates=3100, lr=0.000124038, gnorm=2.402, clip=1, loss_scale=4, train_wall=95, gb_free=16.4, wall=2270
2023-08-06 11:22:20 | INFO | train_inner | epoch 003:    258 / 1474 loss=3.669, trans_loss=4.234, nll_loss=2.604, w2v_ctc_loss=2.24, task_loss=0.915, contrastive_loss=0.585, total=4161.13, n_correct=1350.02, ppl=6.08, accuracy=32.444, wps=13173.8, ups=1.06, wpb=12431.5, bsz=467, num_updates=3200, lr=0.000128036, gnorm=1.465, clip=0, loss_scale=4, train_wall=94, gb_free=17.1, wall=2364
2023-08-06 11:23:53 | INFO | train_inner | epoch 003:    358 / 1474 loss=3.505, trans_loss=4.146, nll_loss=2.484, w2v_ctc_loss=2.117, task_loss=0.92, contrastive_loss=0.627, total=4150.02, n_correct=1463.71, ppl=5.59, accuracy=35.27, wps=13347.7, ups=1.08, wpb=12384.9, bsz=461.6, num_updates=3300, lr=0.000132034, gnorm=1.403, clip=0, loss_scale=4, train_wall=92, gb_free=17.1, wall=2457
2023-08-06 11:25:27 | INFO | train_inner | epoch 003:    458 / 1474 loss=3.325, trans_loss=4.071, nll_loss=2.387, w2v_ctc_loss=2.014, task_loss=0.89, contrastive_loss=0.477, total=4209.57, n_correct=1596.9, ppl=5.23, accuracy=37.935, wps=13311.7, ups=1.06, wpb=12566, bsz=476.8, num_updates=3400, lr=0.000136032, gnorm=1.273, clip=0, loss_scale=4, train_wall=94, gb_free=16, wall=2551
2023-08-06 11:26:59 | INFO | train_inner | epoch 003:    558 / 1474 loss=3.198, trans_loss=4.027, nll_loss=2.33, w2v_ctc_loss=1.935, task_loss=0.978, contrastive_loss=0.454, total=4088.48, n_correct=1614, ppl=5.03, accuracy=39.477, wps=13263.6, ups=1.09, wpb=12212.5, bsz=439.7, num_updates=3500, lr=0.00014003, gnorm=1.236, clip=0, loss_scale=4, train_wall=92, gb_free=17.6, wall=2643
2023-08-06 11:28:34 | INFO | train_inner | epoch 003:    658 / 1474 loss=3.134, trans_loss=3.98, nll_loss=2.264, w2v_ctc_loss=1.853, task_loss=0.879, contrastive_loss=0.561, total=4221.58, n_correct=1747.1, ppl=4.8, accuracy=41.385, wps=13288.6, ups=1.06, wpb=12587.8, bsz=481.9, num_updates=3600, lr=0.000144028, gnorm=1.148, clip=0, loss_scale=4, train_wall=94, gb_free=16.3, wall=2738
2023-08-06 11:30:07 | INFO | train_inner | epoch 003:    758 / 1474 loss=3.015, trans_loss=3.939, nll_loss=2.216, w2v_ctc_loss=1.82, task_loss=0.876, contrastive_loss=0.328, total=4167.41, n_correct=1782.5, ppl=4.65, accuracy=42.772, wps=13416.7, ups=1.08, wpb=12447.6, bsz=472.6, num_updates=3700, lr=0.000148026, gnorm=1.138, clip=0, loss_scale=4, train_wall=92, gb_free=16.2, wall=2831
2023-08-06 11:31:40 | INFO | train_inner | epoch 003:    858 / 1474 loss=2.94, trans_loss=3.92, nll_loss=2.189, w2v_ctc_loss=1.768, task_loss=0.929, contrastive_loss=0.291, total=4165.53, n_correct=1819.44, ppl=4.56, accuracy=43.678, wps=13351.1, ups=1.07, wpb=12437.8, bsz=456.1, num_updates=3800, lr=0.000152024, gnorm=1.082, clip=0, loss_scale=4, train_wall=93, gb_free=17, wall=2924
2023-08-06 11:33:14 | INFO | train_inner | epoch 003:    958 / 1474 loss=2.908, trans_loss=3.895, nll_loss=2.155, w2v_ctc_loss=1.742, task_loss=0.892, contrastive_loss=0.318, total=4162.3, n_correct=1868.58, ppl=4.45, accuracy=44.893, wps=13265.9, ups=1.07, wpb=12417, bsz=469.2, num_updates=3900, lr=0.000156022, gnorm=1.062, clip=0, loss_scale=4, train_wall=93, gb_free=16.7, wall=3018
2023-08-06 11:34:47 | INFO | train_inner | epoch 003:   1058 / 1474 loss=2.865, trans_loss=3.875, nll_loss=2.131, w2v_ctc_loss=1.731, task_loss=0.979, contrastive_loss=0.276, total=4069.95, n_correct=1845.09, ppl=4.38, accuracy=45.334, wps=13071.9, ups=1.08, wpb=12153.7, bsz=443.6, num_updates=4000, lr=0.00016002, gnorm=1.063, clip=0, loss_scale=4, train_wall=92, gb_free=16.2, wall=3111
2023-08-06 11:34:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 11:35:18 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.023 | trans_loss 6.34 | nll_loss 3.857 | w2v_ctc_loss 2.063 | task_loss 4.319 | contrastive_loss 0.382 | total 4003.4 | n_correct 2001.4 | ppl 14.49 | accuracy 49.993 | uer 30.027 | wer 30.856 | raw_wer 30.856 | bleu 12.18 | wps 1430.9 | wpb 4003.4 | bsz 141.8 | num_updates 4000 | best_bleu 12.18
2023-08-06 11:35:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-08-06 11:35:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_3_4000.pt
2023-08-06 11:35:22 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_3_4000.pt
2023-08-06 11:36:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 12.18) (writing took 41.50108496658504 seconds)
2023-08-06 11:37:33 | INFO | train_inner | epoch 003:   1158 / 1474 loss=2.813, trans_loss=3.866, nll_loss=2.118, w2v_ctc_loss=1.686, task_loss=0.996, contrastive_loss=0.257, total=4038.49, n_correct=1851.58, ppl=4.34, accuracy=45.848, wps=7239.1, ups=0.6, wpb=12054.8, bsz=432.5, num_updates=4100, lr=0.000164018, gnorm=1.038, clip=0, loss_scale=4, train_wall=93, gb_free=16.3, wall=3277
2023-08-06 11:39:05 | INFO | train_inner | epoch 003:   1258 / 1474 loss=2.763, trans_loss=3.843, nll_loss=2.09, w2v_ctc_loss=1.65, task_loss=0.976, contrastive_loss=0.239, total=4064.31, n_correct=1895.3, ppl=4.26, accuracy=46.633, wps=13235.2, ups=1.09, wpb=12136.8, bsz=433.9, num_updates=4200, lr=0.000168016, gnorm=0.995, clip=0, loss_scale=4, train_wall=91, gb_free=17.2, wall=3369
2023-08-06 11:40:39 | INFO | train_inner | epoch 003:   1358 / 1474 loss=2.767, trans_loss=3.826, nll_loss=2.068, w2v_ctc_loss=1.618, task_loss=0.93, contrastive_loss=0.348, total=4134.58, n_correct=1954.23, ppl=4.19, accuracy=47.266, wps=13144.3, ups=1.06, wpb=12343.8, bsz=460.7, num_updates=4300, lr=0.000172014, gnorm=1.026, clip=0, loss_scale=4, train_wall=93, gb_free=17.7, wall=3463
2023-08-06 11:42:12 | INFO | train_inner | epoch 003:   1458 / 1474 loss=2.737, trans_loss=3.814, nll_loss=2.054, w2v_ctc_loss=1.598, task_loss=0.877, contrastive_loss=0.328, total=4209.94, n_correct=2018.59, ppl=4.15, accuracy=47.948, wps=13522.3, ups=1.08, wpb=12573.5, bsz=477.4, num_updates=4400, lr=0.000176012, gnorm=0.986, clip=0, loss_scale=4, train_wall=93, gb_free=17, wall=3556
2023-08-06 11:42:26 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 11:42:55 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 4.896 | trans_loss 6.229 | nll_loss 3.714 | w2v_ctc_loss 1.906 | task_loss 4.178 | contrastive_loss 0.374 | total 4003.4 | n_correct 2081.9 | ppl 13.12 | accuracy 52.003 | uer 29.238 | wer 29.745 | raw_wer 29.745 | bleu 13.51 | wps 1591 | wpb 4003.4 | bsz 141.8 | num_updates 4416 | best_bleu 13.51
2023-08-06 11:42:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4416 updates
2023-08-06 11:42:55 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 11:43:08 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 11:43:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt (epoch 3 @ 4416 updates, score 13.51) (writing took 23.483037719503045 seconds)
2023-08-06 11:43:19 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-08-06 11:43:19 | INFO | train | epoch 003 | loss 3.196 | trans_loss 4.039 | nll_loss 2.346 | w2v_ctc_loss 1.913 | task_loss 0.938 | contrastive_loss 0.43 | total 4139.74 | n_correct 1658.87 | ppl 5.08 | accuracy 40.072 | wps 12190.5 | ups 0.99 | wpb 12359.1 | bsz 458.8 | num_updates 4416 | lr 0.000176652 | gnorm 1.228 | clip 0.1 | loss_scale 4 | train_wall 1350 | gb_free 16.3 | wall 3623
2023-08-06 11:43:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 11:43:20 | INFO | fairseq.trainer | begin training epoch 4
2023-08-06 11:43:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 11:44:44 | INFO | train_inner | epoch 004:     84 / 1474 loss=2.627, trans_loss=3.785, nll_loss=2.012, w2v_ctc_loss=1.55, task_loss=0.954, contrastive_loss=0.185, total=4099.41, n_correct=2000.72, ppl=4.03, accuracy=48.805, wps=8033.2, ups=0.66, wpb=12237, bsz=439.5, num_updates=4500, lr=0.00018001, gnorm=0.953, clip=0, loss_scale=4, train_wall=92, gb_free=16.1, wall=3708
2023-08-06 11:46:16 | INFO | train_inner | epoch 004:    184 / 1474 loss=2.614, trans_loss=3.766, nll_loss=1.989, w2v_ctc_loss=1.529, task_loss=0.882, contrastive_loss=0.209, total=4175.15, n_correct=2066.07, ppl=3.97, accuracy=49.485, wps=13616.9, ups=1.09, wpb=12464.9, bsz=468.3, num_updates=4600, lr=0.000184008, gnorm=0.935, clip=0, loss_scale=4, train_wall=91, gb_free=16.5, wall=3800
2023-08-06 11:47:49 | INFO | train_inner | epoch 004:    284 / 1474 loss=2.646, trans_loss=3.769, nll_loss=1.995, w2v_ctc_loss=1.53, task_loss=0.925, contrastive_loss=0.334, total=4145.23, n_correct=2051.07, ppl=3.99, accuracy=49.48, wps=13204.7, ups=1.07, wpb=12382.4, bsz=463, num_updates=4700, lr=0.000188006, gnorm=0.945, clip=0, loss_scale=4, train_wall=93, gb_free=15.8, wall=3894
2023-08-06 11:49:22 | INFO | train_inner | epoch 004:    384 / 1474 loss=2.586, trans_loss=3.771, nll_loss=1.993, w2v_ctc_loss=1.513, task_loss=0.963, contrastive_loss=0.183, total=4127.66, n_correct=2044.86, ppl=3.98, accuracy=49.54, wps=13364.6, ups=1.09, wpb=12314.6, bsz=443.5, num_updates=4800, lr=0.000192004, gnorm=0.918, clip=0, loss_scale=4, train_wall=92, gb_free=17.4, wall=3986
2023-08-06 11:50:55 | INFO | train_inner | epoch 004:    484 / 1474 loss=2.674, trans_loss=3.751, nll_loss=1.971, w2v_ctc_loss=1.476, task_loss=0.839, contrastive_loss=0.577, total=4218.78, n_correct=2121.25, ppl=3.92, accuracy=50.281, wps=13471, ups=1.07, wpb=12592.4, bsz=497.8, num_updates=4900, lr=0.000196002, gnorm=0.939, clip=0, loss_scale=4, train_wall=93, gb_free=16.4, wall=4079
2023-08-06 11:52:28 | INFO | train_inner | epoch 004:    584 / 1474 loss=2.589, trans_loss=3.747, nll_loss=1.966, w2v_ctc_loss=1.499, task_loss=0.871, contrastive_loss=0.255, total=4217.52, n_correct=2133.12, ppl=3.91, accuracy=50.578, wps=13601.8, ups=1.08, wpb=12591.1, bsz=485.9, num_updates=5000, lr=0.0002, gnorm=0.927, clip=0, loss_scale=4, train_wall=92, gb_free=15.9, wall=4172
Mixup rate:0.5, token after shrink shape:torch.Size([24, 53]), X shape:torch.Size([24, 53, 512])
CTC Tokens:tensor([0, 0, 0, 0, 0], device='cuda:0'), Shrink Mask:tensor([ True, False, False, False, False], device='cuda:0'), New Tokens:tensor([   0,   26,    0, 3666,    0], device='cuda:0')
Mixup Sent Mask:tensor([[ 2],
        [ 6],
        [ 7],
        [ 9],
        [14],
        [17],
        [19],
        [21],
        [23]], device='cuda:0'), 2,  Mixup Mask:tensor([ True, False,  True,  True, False], device='cuda:0'), 
                    Org X:tensor([[ 0.6982,  0.2334,  1.2441,  ...,  0.0699, -0.5840,  0.7876],
        [-0.3105,  0.8560,  1.2432,  ...,  0.0242, -0.1982, -1.1514],
        [ 0.7432,  0.6860,  1.3955,  ...,  0.0159,  1.2900, -0.8022],
        [-0.0616, -0.2217,  0.7124,  ...,  0.1416,  0.8574,  0.3696],
        [ 0.0360, -0.1990,  1.5752,  ...,  0.5415,  1.0293,  0.7603]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137],
        [-0.3105,  0.8560,  1.2432,  ...,  0.0242, -0.1982, -1.1514],
        [-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137],
        [-1.6680,  2.1152,  0.3677,  ...,  3.6016, -1.1113,  0.1506],
        [ 0.0360, -0.1990,  1.5752,  ...,  0.5415,  1.0293,  0.7603]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137],
        [-0.7524, -0.1970, -0.1132,  ..., -1.5303, -0.8071, -5.0312],
        [-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137],
        [-1.6680,  2.1152,  0.3677,  ...,  3.6016, -1.1113,  0.1506],
        [-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.4792, device='cuda:0')
2023-08-06 11:54:03 | INFO | train_inner | epoch 004:    684 / 1474 loss=2.554, trans_loss=3.749, nll_loss=1.964, w2v_ctc_loss=1.456, task_loss=0.951, contrastive_loss=0.301, total=4176.39, n_correct=2121.19, ppl=3.9, accuracy=50.79, wps=13068.9, ups=1.05, wpb=12448.9, bsz=455.8, num_updates=5100, lr=0.00019803, gnorm=0.571, clip=0, loss_scale=8, train_wall=95, gb_free=17, wall=4267
2023-08-06 11:55:36 | INFO | train_inner | epoch 004:    784 / 1474 loss=2.52, trans_loss=3.74, nll_loss=1.958, w2v_ctc_loss=1.472, task_loss=1.02, contrastive_loss=0.171, total=4026.63, n_correct=2053.29, ppl=3.88, accuracy=50.993, wps=12947.6, ups=1.08, wpb=12025, bsz=420.6, num_updates=5200, lr=0.000196116, gnorm=0.574, clip=0, loss_scale=8, train_wall=92, gb_free=13, wall=4360
2023-08-06 11:57:10 | INFO | train_inner | epoch 004:    884 / 1474 loss=2.57, trans_loss=3.726, nll_loss=1.94, w2v_ctc_loss=1.463, task_loss=0.927, contrastive_loss=0.349, total=4186.04, n_correct=2152.48, ppl=3.84, accuracy=51.42, wps=13265.2, ups=1.06, wpb=12501.4, bsz=466.3, num_updates=5300, lr=0.000194257, gnorm=0.589, clip=0, loss_scale=8, train_wall=94, gb_free=17.6, wall=4454
2023-08-06 11:58:43 | INFO | train_inner | epoch 004:    984 / 1474 loss=2.504, trans_loss=3.716, nll_loss=1.929, w2v_ctc_loss=1.441, task_loss=0.941, contrastive_loss=0.216, total=4125.02, n_correct=2139.41, ppl=3.81, accuracy=51.864, wps=13223.6, ups=1.07, wpb=12321, bsz=457.1, num_updates=5400, lr=0.00019245, gnorm=0.567, clip=0, loss_scale=8, train_wall=93, gb_free=12.6, wall=4547
2023-08-06 12:00:17 | INFO | train_inner | epoch 004:   1084 / 1474 loss=2.501, trans_loss=3.725, nll_loss=1.938, w2v_ctc_loss=1.445, task_loss=1.002, contrastive_loss=0.195, total=4075.6, n_correct=2106.91, ppl=3.83, accuracy=51.696, wps=13002.5, ups=1.07, wpb=12163.4, bsz=435.7, num_updates=5500, lr=0.000190693, gnorm=0.566, clip=0, loss_scale=8, train_wall=93, gb_free=15.9, wall=4641
2023-08-06 12:01:50 | INFO | train_inner | epoch 004:   1184 / 1474 loss=2.53, trans_loss=3.713, nll_loss=1.927, w2v_ctc_loss=1.437, task_loss=0.871, contrastive_loss=0.305, total=4161.18, n_correct=2167.2, ppl=3.8, accuracy=52.081, wps=13384.4, ups=1.08, wpb=12431.8, bsz=483.4, num_updates=5600, lr=0.000188982, gnorm=0.579, clip=0, loss_scale=8, train_wall=92, gb_free=16.7, wall=4734
2023-08-06 12:03:23 | INFO | train_inner | epoch 004:   1284 / 1474 loss=2.494, trans_loss=3.704, nll_loss=1.913, w2v_ctc_loss=1.417, task_loss=0.884, contrastive_loss=0.267, total=4156.53, n_correct=2181.95, ppl=3.77, accuracy=52.495, wps=13350.5, ups=1.08, wpb=12411.4, bsz=472.7, num_updates=5700, lr=0.000187317, gnorm=0.576, clip=0, loss_scale=8, train_wall=93, gb_free=15.7, wall=4827
2023-08-06 12:04:54 | INFO | train_inner | epoch 004:   1384 / 1474 loss=2.447, trans_loss=3.703, nll_loss=1.913, w2v_ctc_loss=1.417, task_loss=0.954, contrastive_loss=0.15, total=4101.23, n_correct=2153.32, ppl=3.77, accuracy=52.504, wps=13429.5, ups=1.1, wpb=12249, bsz=437.6, num_updates=5800, lr=0.000185695, gnorm=0.553, clip=0, loss_scale=8, train_wall=91, gb_free=15.5, wall=4918
2023-08-06 12:06:17 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([32, 46]), X shape:torch.Size([32, 46, 512])
CTC Tokens:tensor([   0,  194,    0,   44, 2728], device='cuda:4'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:4'), New Tokens:tensor([   0,  194,    0,   44, 2728], device='cuda:4')
Mixup Sent Mask:tensor([[ 2],
        [ 6],
        [ 7],
        [ 9],
        [14],
        [17],
        [19],
        [21],
        [23]], device='cuda:4'), 2,  Mixup Mask:tensor([False,  True, False, False, False], device='cuda:4'), 
                    Org X:tensor([[ 0.1815,  0.3813,  0.5122,  ..., -0.2119,  0.1342,  0.9106],
        [ 0.6138,  0.3511,  1.1465,  ..., -0.3091, -0.0393,  0.3635],
        [ 0.7500,  0.4758,  1.3350,  ..., -0.0800,  0.3572,  0.0294],
        [ 0.5391,  0.5513,  1.4023,  ...,  0.0046,  0.1351, -0.0328],
        [ 0.5483,  0.5254,  1.3477,  ...,  0.0649,  0.2102, -0.0424]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.1815,  0.3813,  0.5122,  ..., -0.2119,  0.1342,  0.9106],
        [-0.2285, -0.7207, -0.0503,  ...,  0.2546, -0.5781, -0.5947],
        [ 0.7500,  0.4758,  1.3350,  ..., -0.0800,  0.3572,  0.0294],
        [ 0.5391,  0.5513,  1.4023,  ...,  0.0046,  0.1351, -0.0328],
        [ 0.5483,  0.5254,  1.3477,  ...,  0.0649,  0.2102, -0.0424]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137],
        [-0.2285, -0.7207, -0.0503,  ...,  0.2546, -0.5781, -0.5947],
        [-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137],
        [ 0.3481, -0.2313,  0.3462,  ...,  1.2051, -2.4277, -6.0117],
        [ 2.1016,  0.6250,  1.2090,  ..., -3.5410, -0.3894,  0.9233]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.4792, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 110]), X shape:torch.Size([8, 110, 512])
CTC Tokens:tensor([   0, 1917,    0,    0, 2645], device='cuda:7'), Shrink Mask:tensor([ True,  True,  True, False,  True], device='cuda:7'), New Tokens:tensor([   0, 1917,    0, 2645,    0], device='cuda:7')
Mixup Sent Mask:tensor([[2],
        [6],
        [7]], device='cuda:7'), 2,  Mixup Mask:tensor([ True,  True, False,  True,  True], device='cuda:7'), 
                    Org X:tensor([[ 1.0352, -0.9409, -0.1274,  ...,  0.7832, -0.5918, -0.6284],
        [ 0.4546, -1.3184, -0.4470,  ...,  0.2793, -0.6553, -0.5112],
        [ 0.5151,  0.2395,  0.7402,  ..., -0.5332,  0.5552,  0.1696],
        [ 0.4326,  0.4622,  0.8896,  ...,  0.1001, -0.4368, -0.2183],
        [ 0.9741, -0.5420,  1.5400,  ...,  0.0820,  0.2798, -0.3198]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137],
        [ 1.2588, -0.4624, -1.1279,  ..., -3.6055, -2.0469,  1.2227],
        [ 0.5151,  0.2395,  0.7402,  ..., -0.5332,  0.5552,  0.1696],
        [ 0.1752,  0.9873,  0.8403,  ..., -0.0781, -3.0957, -3.2031],
        [-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137],
        [ 1.2588, -0.4624, -1.1279,  ..., -3.6055, -2.0469,  1.2227],
        [-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137],
        [ 0.1752,  0.9873,  0.8403,  ..., -0.0781, -3.0957, -3.2031],
        [-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.4792, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 91]), X shape:torch.Size([8, 91, 512])
CTC Tokens:tensor([67, 67,  0, 70, 24], device='cuda:1'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:1'), New Tokens:tensor([67,  0, 70, 24,  0], device='cuda:1')
Mixup Sent Mask:tensor([[2],
        [6],
        [7]], device='cuda:1'), 2,  Mixup Mask:tensor([False,  True,  True,  True,  True], device='cuda:1'), 
                    Org X:tensor([[-0.2925,  0.4028, -0.2181,  ..., -0.0324, -0.4021, -2.6816],
        [-2.1797, -0.6948,  0.0472,  ...,  0.1334, -0.4907, -2.6289],
        [-2.2793, -0.5273,  0.0982,  ...,  0.3484, -0.6406, -1.9053],
        [-2.4727, -0.5859, -0.4185,  ...,  0.1733,  1.2744, -2.4316],
        [ 0.3472, -0.9692, -0.0558,  ..., -0.5835,  0.7866, -1.2041]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-2.9248e-01,  4.0283e-01, -2.1814e-01,  ..., -3.2410e-02,
         -4.0210e-01, -2.6816e+00],
        [-5.5029e-01, -1.1230e+00, -6.1523e-01,  ..., -2.0176e+00,
         -1.6709e+00, -2.0137e+00],
        [ 8.8596e-04, -8.2910e-01, -3.6816e-01,  ...,  7.4219e-01,
         -2.2617e+00, -3.0156e+00],
        [-5.5078e-01, -5.7812e-01,  5.1465e-01,  ..., -4.3750e+00,
          1.3164e+00, -1.2979e+00],
        [-5.5029e-01, -1.1230e+00, -6.1523e-01,  ..., -2.0176e+00,
         -1.6709e+00, -2.0137e+00]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SliceBackward0>), Emb: tensor([[-1.5894e-01, -5.3857e-01, -1.0889e+00,  ...,  1.3604e+00,
         -1.5410e+00, -4.5898e+00],
        [-5.5029e-01, -1.1230e+00, -6.1523e-01,  ..., -2.0176e+00,
         -1.6709e+00, -2.0137e+00],
        [ 8.8596e-04, -8.2910e-01, -3.6816e-01,  ...,  7.4219e-01,
         -2.2617e+00, -3.0156e+00],
        [-5.5078e-01, -5.7812e-01,  5.1465e-01,  ..., -4.3750e+00,
          1.3164e+00, -1.2979e+00],
        [-5.5029e-01, -1.1230e+00, -6.1523e-01,  ..., -2.0176e+00,
         -1.6709e+00, -2.0137e+00]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.4792, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([40, 33]), X shape:torch.Size([40, 33, 512])
CTC Tokens:tensor([8, 8, 0, 0, 0], device='cuda:3'), Shrink Mask:tensor([ True, False,  True, False, False], device='cuda:3'), New Tokens:tensor([ 8,  0, 19,  0, 34], device='cuda:3')
Mixup Sent Mask:tensor([[ 2],
        [ 6],
        [ 7],
        [ 9],
        [14],
        [17],
        [19],
        [21],
        [23],
        [33],
        [34],
        [35],
        [39]], device='cuda:3'), 2,  Mixup Mask:tensor([False, False,  True, False, False], device='cuda:3'), 
                    Org X:tensor([[ 0.4209, -0.0527, -0.0332,  ...,  0.3684,  0.8149, -0.5547],
        [ 0.2791,  0.2355,  1.1191,  ...,  0.2961,  0.9814,  0.0123],
        [ 0.5298,  1.3926,  0.1956,  ..., -0.3994,  1.0225, -0.7246],
        [ 0.7559,  1.4531,  0.5229,  ..., -1.1416,  1.6611, -0.9058],
        [-0.6821,  0.5493,  1.3418,  ..., -0.5942, -1.5762, -0.7500]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.4209, -0.0527, -0.0332,  ...,  0.3684,  0.8149, -0.5547],
        [ 0.2791,  0.2355,  1.1191,  ...,  0.2961,  0.9814,  0.0123],
        [-0.1859, -0.4951, -0.2079,  ..., -0.2455, -0.0629, -1.2109],
        [ 0.7559,  1.4531,  0.5229,  ..., -1.1416,  1.6611, -0.9058],
        [-0.6821,  0.5493,  1.3418,  ..., -0.5942, -1.5762, -0.7500]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.2993, -0.4568, -0.8203,  ...,  1.3350, -1.9102, -1.4727],
        [-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137],
        [-0.1859, -0.4951, -0.2079,  ..., -0.2455, -0.0629, -1.2109],
        [-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137],
        [-0.0605, -0.8530, -0.1625,  ..., -1.9316, -3.0742, -3.3086]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.4792, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([64, 27]), X shape:torch.Size([64, 27, 512])
CTC Tokens:tensor([ 0, 29, 29,  0,  0], device='cuda:5'), Shrink Mask:tensor([ True,  True, False,  True, False], device='cuda:5'), New Tokens:tensor([  0,  29,   0, 168,   0], device='cuda:5')
Mixup Sent Mask:tensor([[ 2],
        [ 6],
        [ 7],
        [ 9],
        [14],
        [17],
        [19],
        [21],
        [23],
        [33],
        [34],
        [35],
        [39],
        [40],
        [41],
        [45],
        [48],
        [51],
        [56],
        [57]], device='cuda:5'), 2,  Mixup Mask:tensor([ True,  True,  True,  True, False], device='cuda:5'), 
                    Org X:tensor([[ 0.1865,  0.2336,  0.3896,  ...,  0.8218, -1.1611,  0.1534],
        [ 0.0932,  0.4031,  0.7095,  ...,  0.4373, -2.3203, -0.2422],
        [ 0.0102,  0.4324,  0.7930,  ...,  0.2510, -0.2571, -0.8667],
        [-0.9185,  0.7603, -1.0205,  ..., -0.2197, -0.2095,  0.7617],
        [-2.5488,  0.8359, -0.0968,  ..., -0.0801,  0.1199, -0.8159]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137],
        [-0.9980,  0.2244, -0.6118,  ..., -0.4971, -2.1953,  3.4746],
        [-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137],
        [ 0.5347,  0.6548, -1.4863,  ..., -2.5605, -2.6797,  1.3486],
        [-2.5488,  0.8359, -0.0968,  ..., -0.0801,  0.1199, -0.8159]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137],
        [-0.9980,  0.2244, -0.6118,  ..., -0.4971, -2.1953,  3.4746],
        [-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137],
        [ 0.5347,  0.6548, -1.4863,  ..., -2.5605, -2.6797,  1.3486],
        [-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.4792, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 62]), X shape:torch.Size([16, 62, 512])
CTC Tokens:tensor([53, 67,  6,  7,  0], device='cuda:2'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:2'), New Tokens:tensor([53, 67,  6,  7,  0], device='cuda:2')
Mixup Sent Mask:tensor([[ 2],
        [ 6],
        [ 7],
        [ 9],
        [14]], device='cuda:2'), 2,  Mixup Mask:tensor([ True,  True, False,  True,  True], device='cuda:2'), 
                    Org X:tensor([[-1.4736,  0.1145,  0.3093,  ...,  0.6890, -1.3350, -1.1719],
        [-0.1131,  0.2917,  0.4717,  ...,  0.3567, -1.7275, -1.9395],
        [ 0.4377,  0.4216, -1.1455,  ...,  0.2004, -1.8369, -2.0000],
        [-0.2515,  0.6143, -0.6074,  ...,  0.0546, -0.9409, -0.5576],
        [ 0.7686,  0.7314,  0.6631,  ..., -0.3379,  0.3750, -0.7290]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.0959, -0.3875, -0.0403,  ..., -2.8809,  3.6094, -2.6348],
        [-0.1589, -0.5386, -1.0889,  ...,  1.3604, -1.5410, -4.5898],
        [ 0.4377,  0.4216, -1.1455,  ...,  0.2004, -1.8369, -2.0000],
        [-0.3743, -0.6172, -0.2634,  ..., -0.4858, -1.7373, -1.3906],
        [-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[ 9.5886e-02, -3.8745e-01, -4.0344e-02,  ..., -2.8809e+00,
          3.6094e+00, -2.6348e+00],
        [-1.5894e-01, -5.3857e-01, -1.0889e+00,  ...,  1.3604e+00,
         -1.5410e+00, -4.5898e+00],
        [-2.2144e-03, -4.4751e-01, -6.3379e-01,  ..., -3.0137e+00,
         -3.4883e+00,  1.2393e+00],
        [-3.7427e-01, -6.1719e-01, -2.6343e-01,  ..., -4.8584e-01,
         -1.7373e+00, -1.3906e+00],
        [-5.5029e-01, -1.1230e+00, -6.1523e-01,  ..., -2.0176e+00,
         -1.6709e+00, -2.0137e+00]], device='cuda:2', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.4792, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([24, 61]), X shape:torch.Size([24, 61, 512])
CTC Tokens:tensor([101, 641,   0,   4,   0], device='cuda:6'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:6'), New Tokens:tensor([101, 641,   0,   4,   0], device='cuda:6')
Mixup Sent Mask:tensor([[ 2],
        [ 6],
        [ 7],
        [ 9],
        [14],
        [17],
        [19],
        [21],
        [23]], device='cuda:6'), 2,  Mixup Mask:tensor([False, False, False,  True,  True], device='cuda:6'), 
                    Org X:tensor([[ 0.6899,  1.4102, -0.1642,  ..., -0.2515,  1.2549,  0.6572],
        [ 0.4368,  1.9141, -1.0615,  ..., -0.9009,  0.9053,  1.0547],
        [ 0.4832,  0.6621, -0.4094,  ..., -1.0547,  0.5166,  0.6270],
        [ 0.3408,  0.5923,  0.1808,  ..., -0.4187,  0.1451,  0.2416],
        [ 0.1494,  0.6826,  0.3860,  ..., -0.0202,  1.0908, -0.3779]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.6899,  1.4102, -0.1642,  ..., -0.2515,  1.2549,  0.6572],
        [ 0.4368,  1.9141, -1.0615,  ..., -0.9009,  0.9053,  1.0547],
        [ 0.4832,  0.6621, -0.4094,  ..., -1.0547,  0.5166,  0.6270],
        [-0.0744, -0.6592, -0.6323,  ..., -1.6836, -1.0371, -2.3809],
        [-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.6768, -0.4509,  0.1814,  ...,  0.1431, -2.8047,  1.9961],
        [-1.5547,  1.9111, -0.8716,  ..., -3.8535,  0.1628,  0.6465],
        [-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137],
        [-0.0744, -0.6592, -0.6323,  ..., -1.6836, -1.0371, -2.3809],
        [-0.5503, -1.1230, -0.6152,  ..., -2.0176, -1.6709, -2.0137]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.4792, device='cuda:6')
2023-08-06 12:06:43 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 4.548 | trans_loss 5.906 | nll_loss 3.285 | w2v_ctc_loss 1.576 | task_loss 4.443 | contrastive_loss 0.291 | total 4003.4 | n_correct 2258.2 | ppl 9.74 | accuracy 56.407 | uer 24.009 | wer 25.622 | raw_wer 25.622 | bleu 16.77 | wps 1845.1 | wpb 4003.4 | bsz 141.8 | num_updates 5890 | best_bleu 16.77
2023-08-06 12:06:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5890 updates
2023-08-06 12:06:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 12:06:56 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 12:07:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt (epoch 4 @ 5890 updates, score 16.77) (writing took 23.599423367530107 seconds)
2023-08-06 12:07:07 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-08-06 12:07:07 | INFO | train | epoch 004 | loss 2.553 | trans_loss 3.737 | nll_loss 1.954 | w2v_ctc_loss 1.469 | task_loss 0.927 | contrastive_loss 0.263 | total 4138.65 | n_correct 2110.5 | ppl 3.87 | accuracy 50.995 | wps 12754 | ups 1.03 | wpb 12355.8 | bsz 458.5 | num_updates 5890 | lr 0.000184271 | gnorm 0.715 | clip 0 | loss_scale 8 | train_wall 1363 | gb_free 14.7 | wall 5051
2023-08-06 12:07:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 12:07:07 | INFO | fairseq.trainer | begin training epoch 5
2023-08-06 12:07:07 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 12:07:24 | INFO | train_inner | epoch 005:     10 / 1474 loss=2.432, trans_loss=3.696, nll_loss=1.903, w2v_ctc_loss=1.391, task_loss=0.964, contrastive_loss=0.169, total=4037.7, n_correct=2133.41, ppl=3.74, accuracy=52.837, wps=8026, ups=0.67, wpb=12055.9, bsz=439.3, num_updates=5900, lr=0.000184115, gnorm=0.558, clip=0, loss_scale=8, train_wall=92, gb_free=16.8, wall=5068
2023-08-06 12:08:57 | INFO | train_inner | epoch 005:    110 / 1474 loss=2.354, trans_loss=3.637, nll_loss=1.826, w2v_ctc_loss=1.311, task_loss=0.838, contrastive_loss=0.173, total=4247.37, n_correct=2320.12, ppl=3.55, accuracy=54.625, wps=13680.3, ups=1.08, wpb=12683.4, bsz=495.1, num_updates=6000, lr=0.000182574, gnorm=0.53, clip=0, loss_scale=8, train_wall=92, gb_free=16.6, wall=5161
2023-08-06 12:08:57 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 12:09:22 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.546 | trans_loss 5.901 | nll_loss 3.275 | w2v_ctc_loss 1.582 | task_loss 4.433 | contrastive_loss 0.293 | total 4003.4 | n_correct 2271.8 | ppl 9.68 | accuracy 56.747 | uer 23.768 | wer 25.491 | raw_wer 25.491 | bleu 16.76 | wps 2025.4 | wpb 4003.4 | bsz 141.8 | num_updates 6000 | best_bleu 16.77
2023-08-06 12:09:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6000 updates
2023-08-06 12:09:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_5_6000.pt
2023-08-06 12:09:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_5_6000.pt
2023-08-06 12:09:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_5_6000.pt (epoch 5 @ 6000 updates, score 16.76) (writing took 31.29979424364865 seconds)
2023-08-06 12:11:26 | INFO | train_inner | epoch 005:    210 / 1474 loss=2.422, trans_loss=3.65, nll_loss=1.841, w2v_ctc_loss=1.326, task_loss=0.858, contrastive_loss=0.393, total=4189.85, n_correct=2275.89, ppl=3.58, accuracy=54.319, wps=8372.4, ups=0.67, wpb=12500.5, bsz=488.2, num_updates=6100, lr=0.000181071, gnorm=0.534, clip=0, loss_scale=8, train_wall=92, gb_free=17.7, wall=5310
2023-08-06 12:12:58 | INFO | train_inner | epoch 005:    310 / 1474 loss=2.386, trans_loss=3.645, nll_loss=1.839, w2v_ctc_loss=1.343, task_loss=0.956, contrastive_loss=0.238, total=4090.1, n_correct=2216.87, ppl=3.58, accuracy=54.201, wps=13280.7, ups=1.09, wpb=12228.1, bsz=443.9, num_updates=6200, lr=0.000179605, gnorm=0.541, clip=0, loss_scale=8, train_wall=92, gb_free=16.1, wall=5402
2023-08-06 12:14:32 | INFO | train_inner | epoch 005:    410 / 1474 loss=2.395, trans_loss=3.637, nll_loss=1.83, w2v_ctc_loss=1.31, task_loss=0.899, contrastive_loss=0.325, total=4147.17, n_correct=2263.26, ppl=3.56, accuracy=54.574, wps=13247.1, ups=1.07, wpb=12395.1, bsz=472.5, num_updates=6300, lr=0.000178174, gnorm=0.546, clip=0, loss_scale=8, train_wall=93, gb_free=14.7, wall=5496
2023-08-06 12:16:05 | INFO | train_inner | epoch 005:    510 / 1474 loss=2.333, trans_loss=3.65, nll_loss=1.844, w2v_ctc_loss=1.32, task_loss=1.044, contrastive_loss=0.123, total=4026.81, n_correct=2186.67, ppl=3.59, accuracy=54.303, wps=12856.2, ups=1.07, wpb=12029.7, bsz=416.6, num_updates=6400, lr=0.000176777, gnorm=0.529, clip=0, loss_scale=8, train_wall=93, gb_free=17.3, wall=5589
2023-08-06 12:16:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-06 12:17:39 | INFO | train_inner | epoch 005:    611 / 1474 loss=2.341, trans_loss=3.654, nll_loss=1.846, w2v_ctc_loss=1.312, task_loss=0.982, contrastive_loss=0.157, total=4094.34, n_correct=2227.83, ppl=3.6, accuracy=54.412, wps=12996.8, ups=1.06, wpb=12214.3, bsz=439.9, num_updates=6500, lr=0.000175412, gnorm=0.536, clip=0, loss_scale=4, train_wall=93, gb_free=15.2, wall=5683
2023-08-06 12:19:13 | INFO | train_inner | epoch 005:    711 / 1474 loss=2.376, trans_loss=3.646, nll_loss=1.84, w2v_ctc_loss=1.307, task_loss=0.875, contrastive_loss=0.264, total=4176.83, n_correct=2285.59, ppl=3.58, accuracy=54.721, wps=13315.7, ups=1.07, wpb=12467.5, bsz=483.6, num_updates=6600, lr=0.000174078, gnorm=0.535, clip=0, loss_scale=4, train_wall=93, gb_free=16.9, wall=5777
2023-08-06 12:20:46 | INFO | train_inner | epoch 005:    811 / 1474 loss=2.346, trans_loss=3.647, nll_loss=1.84, w2v_ctc_loss=1.304, task_loss=0.961, contrastive_loss=0.196, total=4127.9, n_correct=2257.21, ppl=3.58, accuracy=54.682, wps=13225.1, ups=1.07, wpb=12321.2, bsz=447.5, num_updates=6700, lr=0.000172774, gnorm=0.527, clip=0, loss_scale=4, train_wall=93, gb_free=15.6, wall=5870
2023-08-06 12:22:19 | INFO | train_inner | epoch 005:    911 / 1474 loss=2.317, trans_loss=3.638, nll_loss=1.83, w2v_ctc_loss=1.29, task_loss=0.956, contrastive_loss=0.158, total=4101.19, n_correct=2250.89, ppl=3.56, accuracy=54.884, wps=13203.7, ups=1.08, wpb=12245.9, bsz=447.9, num_updates=6800, lr=0.000171499, gnorm=0.531, clip=0, loss_scale=4, train_wall=92, gb_free=17, wall=5963
2023-08-06 12:23:51 | INFO | train_inner | epoch 005:   1011 / 1474 loss=2.339, trans_loss=3.641, nll_loss=1.832, w2v_ctc_loss=1.293, task_loss=0.922, contrastive_loss=0.238, total=4164.27, n_correct=2285.13, ppl=3.56, accuracy=54.875, wps=13407.9, ups=1.08, wpb=12430.2, bsz=462, num_updates=6900, lr=0.000170251, gnorm=0.521, clip=0, loss_scale=4, train_wall=92, gb_free=14.8, wall=6056
2023-08-06 12:25:26 | INFO | train_inner | epoch 005:   1111 / 1474 loss=2.358, trans_loss=3.642, nll_loss=1.833, w2v_ctc_loss=1.304, task_loss=0.93, contrastive_loss=0.24, total=4168.94, n_correct=2296.32, ppl=3.56, accuracy=55.082, wps=13221.9, ups=1.06, wpb=12436.1, bsz=464.1, num_updates=7000, lr=0.000169031, gnorm=0.532, clip=0, loss_scale=4, train_wall=94, gb_free=16.5, wall=6150
2023-08-06 12:26:59 | INFO | train_inner | epoch 005:   1211 / 1474 loss=2.305, trans_loss=3.637, nll_loss=1.827, w2v_ctc_loss=1.282, task_loss=0.943, contrastive_loss=0.147, total=4171.16, n_correct=2304.12, ppl=3.55, accuracy=55.239, wps=13347.1, ups=1.07, wpb=12443.2, bsz=456.3, num_updates=7100, lr=0.000167836, gnorm=0.533, clip=0, loss_scale=4, train_wall=93, gb_free=15.7, wall=6243
2023-08-06 12:28:33 | INFO | train_inner | epoch 005:   1311 / 1474 loss=2.281, trans_loss=3.636, nll_loss=1.827, w2v_ctc_loss=1.267, task_loss=0.948, contrastive_loss=0.115, total=4126.97, n_correct=2278.84, ppl=3.55, accuracy=55.218, wps=13133.9, ups=1.07, wpb=12317.9, bsz=443.4, num_updates=7200, lr=0.000166667, gnorm=0.517, clip=0, loss_scale=4, train_wall=93, gb_free=15.3, wall=6337
2023-08-06 12:30:05 | INFO | train_inner | epoch 005:   1411 / 1474 loss=2.303, trans_loss=3.635, nll_loss=1.828, w2v_ctc_loss=1.265, task_loss=0.937, contrastive_loss=0.176, total=4138.54, n_correct=2289.54, ppl=3.55, accuracy=55.322, wps=13376.4, ups=1.08, wpb=12359.3, bsz=459, num_updates=7300, lr=0.000165521, gnorm=0.519, clip=0, loss_scale=4, train_wall=92, gb_free=16.7, wall=6429
2023-08-06 12:31:04 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 12:31:28 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.436 | trans_loss 5.805 | nll_loss 3.157 | w2v_ctc_loss 1.429 | task_loss 4.487 | contrastive_loss 0.295 | total 4003.4 | n_correct 2326.5 | ppl 8.92 | accuracy 58.113 | uer 21.851 | wer 23.634 | raw_wer 23.634 | bleu 17.48 | wps 1904.2 | wpb 4003.4 | bsz 141.8 | num_updates 7363 | best_bleu 17.48
2023-08-06 12:31:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7363 updates
2023-08-06 12:31:28 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 12:31:42 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 12:31:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt (epoch 5 @ 7363 updates, score 17.48) (writing took 25.161853086203337 seconds)
2023-08-06 12:31:54 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-08-06 12:31:54 | INFO | train | epoch 005 | loss 2.346 | trans_loss 3.642 | nll_loss 1.834 | w2v_ctc_loss 1.302 | task_loss 0.931 | contrastive_loss 0.211 | total 4137.68 | n_correct 2266 | ppl 3.57 | accuracy 54.765 | wps 12241 | ups 0.99 | wpb 12353 | bsz 457.8 | num_updates 7363 | lr 0.000164812 | gnorm 0.531 | clip 0 | loss_scale 4 | train_wall 1366 | gb_free 16.1 | wall 6538
2023-08-06 12:31:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 12:31:54 | INFO | fairseq.trainer | begin training epoch 6
2023-08-06 12:31:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 12:32:36 | INFO | train_inner | epoch 006:     37 / 1474 loss=2.286, trans_loss=3.613, nll_loss=1.796, w2v_ctc_loss=1.261, task_loss=0.957, contrastive_loss=0.173, total=4113.87, n_correct=2296.17, ppl=3.47, accuracy=55.815, wps=8113.6, ups=0.66, wpb=12276.7, bsz=447.4, num_updates=7400, lr=0.000164399, gnorm=0.53, clip=0, loss_scale=4, train_wall=94, gb_free=17.7, wall=6580
2023-08-06 12:34:09 | INFO | train_inner | epoch 006:    137 / 1474 loss=2.245, trans_loss=3.58, nll_loss=1.755, w2v_ctc_loss=1.208, task_loss=0.921, contrastive_loss=0.217, total=4161.2, n_correct=2359.3, ppl=3.38, accuracy=56.698, wps=13393.7, ups=1.08, wpb=12428.5, bsz=458.1, num_updates=7500, lr=0.000163299, gnorm=0.511, clip=0, loss_scale=4, train_wall=92, gb_free=16.8, wall=6673
2023-08-06 12:35:42 | INFO | train_inner | epoch 006:    237 / 1474 loss=2.244, trans_loss=3.594, nll_loss=1.773, w2v_ctc_loss=1.241, task_loss=0.997, contrastive_loss=0.126, total=4110.12, n_correct=2313.21, ppl=3.42, accuracy=56.281, wps=13241.5, ups=1.08, wpb=12279.2, bsz=437.3, num_updates=7600, lr=0.000162221, gnorm=0.512, clip=0, loss_scale=4, train_wall=92, gb_free=17, wall=6766
2023-08-06 12:37:17 | INFO | train_inner | epoch 006:    337 / 1474 loss=2.301, trans_loss=3.58, nll_loss=1.757, w2v_ctc_loss=1.192, task_loss=0.872, contrastive_loss=0.421, total=4170.52, n_correct=2368.22, ppl=3.38, accuracy=56.785, wps=13137.5, ups=1.05, wpb=12453.1, bsz=488.4, num_updates=7700, lr=0.000161165, gnorm=0.526, clip=0, loss_scale=4, train_wall=94, gb_free=15.5, wall=6861
2023-08-06 12:38:50 | INFO | train_inner | epoch 006:    437 / 1474 loss=2.218, trans_loss=3.584, nll_loss=1.761, w2v_ctc_loss=1.203, task_loss=0.894, contrastive_loss=0.14, total=4154.89, n_correct=2362.07, ppl=3.39, accuracy=56.85, wps=13343.7, ups=1.08, wpb=12405.6, bsz=470.4, num_updates=7800, lr=0.000160128, gnorm=0.512, clip=0, loss_scale=4, train_wall=93, gb_free=16.3, wall=6954
2023-08-06 12:40:22 | INFO | train_inner | epoch 006:    537 / 1474 loss=2.22, trans_loss=3.589, nll_loss=1.767, w2v_ctc_loss=1.213, task_loss=0.927, contrastive_loss=0.129, total=4174.46, n_correct=2373.5, ppl=3.4, accuracy=56.858, wps=13466.2, ups=1.08, wpb=12460.1, bsz=458.3, num_updates=7900, lr=0.000159111, gnorm=0.51, clip=0, loss_scale=4, train_wall=92, gb_free=17.1, wall=7046
2023-08-06 12:41:55 | INFO | train_inner | epoch 006:    637 / 1474 loss=2.232, trans_loss=3.591, nll_loss=1.77, w2v_ctc_loss=1.198, task_loss=0.885, contrastive_loss=0.189, total=4145.19, n_correct=2350.96, ppl=3.41, accuracy=56.715, wps=13315.3, ups=1.08, wpb=12372.7, bsz=470.9, num_updates=8000, lr=0.000158114, gnorm=0.54, clip=0, loss_scale=4, train_wall=92, gb_free=15.7, wall=7139
2023-08-06 12:41:55 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 12:42:19 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.412 | trans_loss 5.772 | nll_loss 3.104 | w2v_ctc_loss 1.439 | task_loss 4.552 | contrastive_loss 0.286 | total 4003.4 | n_correct 2346.9 | ppl 8.6 | accuracy 58.623 | uer 21.419 | wer 23.295 | raw_wer 23.295 | bleu 18 | wps 2150.8 | wpb 4003.4 | bsz 141.8 | num_updates 8000 | best_bleu 18
2023-08-06 12:42:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8000 updates
2023-08-06 12:42:19 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_6_8000.pt
2023-08-06 12:42:23 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_6_8000.pt
2023-08-06 12:43:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_6_8000.pt (epoch 6 @ 8000 updates, score 18.0) (writing took 43.92527292482555 seconds)
2023-08-06 12:44:37 | INFO | train_inner | epoch 006:    737 / 1474 loss=2.229, trans_loss=3.596, nll_loss=1.776, w2v_ctc_loss=1.216, task_loss=0.948, contrastive_loss=0.14, total=4151.01, n_correct=2350.56, ppl=3.43, accuracy=56.626, wps=7673.9, ups=0.62, wpb=12393.7, bsz=454.2, num_updates=8100, lr=0.000157135, gnorm=0.507, clip=0, loss_scale=4, train_wall=93, gb_free=12.8, wall=7301
2023-08-06 12:46:10 | INFO | train_inner | epoch 006:    837 / 1474 loss=2.219, trans_loss=3.603, nll_loss=1.786, w2v_ctc_loss=1.208, task_loss=0.985, contrastive_loss=0.12, total=4108.83, n_correct=2321.19, ppl=3.45, accuracy=56.493, wps=13184.1, ups=1.07, wpb=12267.1, bsz=439.4, num_updates=8200, lr=0.000156174, gnorm=0.508, clip=0, loss_scale=4, train_wall=93, gb_free=17, wall=7394
2023-08-06 12:47:43 | INFO | train_inner | epoch 006:    937 / 1474 loss=2.252, trans_loss=3.601, nll_loss=1.783, w2v_ctc_loss=1.211, task_loss=0.977, contrastive_loss=0.219, total=4076.46, n_correct=2307.01, ppl=3.44, accuracy=56.593, wps=13013.2, ups=1.07, wpb=12166, bsz=443, num_updates=8300, lr=0.00015523, gnorm=0.524, clip=0, loss_scale=4, train_wall=93, gb_free=12.4, wall=7487
2023-08-06 12:49:15 | INFO | train_inner | epoch 006:   1037 / 1474 loss=2.251, trans_loss=3.588, nll_loss=1.768, w2v_ctc_loss=1.19, task_loss=0.874, contrastive_loss=0.288, total=4175.9, n_correct=2378.66, ppl=3.41, accuracy=56.962, wps=13510, ups=1.08, wpb=12465, bsz=480.4, num_updates=8400, lr=0.000154303, gnorm=0.518, clip=0, loss_scale=4, train_wall=92, gb_free=14, wall=7580
2023-08-06 12:50:48 | INFO | train_inner | epoch 006:   1137 / 1474 loss=2.212, trans_loss=3.593, nll_loss=1.774, w2v_ctc_loss=1.202, task_loss=1.024, contrastive_loss=0.127, total=4077.2, n_correct=2310.65, ppl=3.42, accuracy=56.672, wps=13086.2, ups=1.08, wpb=12172.7, bsz=430.6, num_updates=8500, lr=0.000153393, gnorm=0.509, clip=0, loss_scale=8, train_wall=93, gb_free=16.1, wall=7673
2023-08-06 12:52:21 | INFO | train_inner | epoch 006:   1237 / 1474 loss=2.286, trans_loss=3.586, nll_loss=1.767, w2v_ctc_loss=1.186, task_loss=0.915, contrastive_loss=0.436, total=4133.46, n_correct=2348.93, ppl=3.4, accuracy=56.827, wps=13322.4, ups=1.08, wpb=12346.4, bsz=470.3, num_updates=8600, lr=0.000152499, gnorm=0.51, clip=0, loss_scale=8, train_wall=92, gb_free=12.1, wall=7765
2023-08-06 12:53:54 | INFO | train_inner | epoch 006:   1337 / 1474 loss=2.192, trans_loss=3.592, nll_loss=1.771, w2v_ctc_loss=1.184, task_loss=0.924, contrastive_loss=0.112, total=4127.77, n_correct=2354.52, ppl=3.41, accuracy=57.041, wps=13270.3, ups=1.08, wpb=12312.7, bsz=454.1, num_updates=8700, lr=0.00015162, gnorm=0.504, clip=0, loss_scale=8, train_wall=92, gb_free=16.9, wall=7858
2023-08-06 12:55:27 | INFO | train_inner | epoch 006:   1437 / 1474 loss=2.189, trans_loss=3.584, nll_loss=1.763, w2v_ctc_loss=1.184, task_loss=0.935, contrastive_loss=0.116, total=4190.32, n_correct=2401.63, ppl=3.39, accuracy=57.314, wps=13469.3, ups=1.08, wpb=12507.6, bsz=460.5, num_updates=8800, lr=0.000150756, gnorm=0.495, clip=0, loss_scale=8, train_wall=92, gb_free=16.7, wall=7951
2023-08-06 12:56:00 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 12:56:24 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.367 | trans_loss 5.734 | nll_loss 3.065 | w2v_ctc_loss 1.395 | task_loss 4.551 | contrastive_loss 0.267 | total 4003.4 | n_correct 2364.4 | ppl 8.37 | accuracy 59.06 | uer 20.211 | wer 22.054 | raw_wer 22.054 | bleu 17.82 | wps 2166.8 | wpb 4003.4 | bsz 141.8 | num_updates 8837 | best_bleu 18
2023-08-06 12:56:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8837 updates
2023-08-06 12:56:24 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_17.8203.pt
2023-08-06 12:56:28 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_17.8203.pt
2023-08-06 12:56:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_17.8203.pt (epoch 6 @ 8837 updates, score 17.82) (writing took 20.504149293527007 seconds)
2023-08-06 12:56:44 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-08-06 12:56:44 | INFO | train | epoch 006 | loss 2.233 | trans_loss 3.59 | nll_loss 1.769 | w2v_ctc_loss 1.202 | task_loss 0.931 | contrastive_loss 0.198 | total 4138.65 | n_correct 2350 | ppl 3.41 | accuracy 56.782 | wps 12216.7 | ups 0.99 | wpb 12355.8 | bsz 458.5 | num_updates 8837 | lr 0.00015044 | gnorm 0.513 | clip 0 | loss_scale 8 | train_wall 1364 | gb_free 14.9 | wall 8029
2023-08-06 12:56:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 12:56:45 | INFO | fairseq.trainer | begin training epoch 7
2023-08-06 12:56:45 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 12:57:52 | INFO | train_inner | epoch 007:     63 / 1474 loss=2.158, trans_loss=3.561, nll_loss=1.734, w2v_ctc_loss=1.152, task_loss=0.908, contrastive_loss=0.131, total=4110.43, n_correct=2376.67, ppl=3.33, accuracy=57.82, wps=8455.9, ups=0.69, wpb=12272.9, bsz=462.8, num_updates=8900, lr=0.000149906, gnorm=0.505, clip=0, loss_scale=8, train_wall=93, gb_free=17.3, wall=8096
2023-08-06 12:59:24 | INFO | train_inner | epoch 007:    163 / 1474 loss=2.165, trans_loss=3.552, nll_loss=1.72, w2v_ctc_loss=1.141, task_loss=0.945, contrastive_loss=0.201, total=4109.53, n_correct=2385.15, ppl=3.29, accuracy=58.039, wps=13335, ups=1.09, wpb=12269.6, bsz=454.1, num_updates=9000, lr=0.000149071, gnorm=0.506, clip=0, loss_scale=8, train_wall=92, gb_free=13.4, wall=8188
2023-08-06 13:00:56 | INFO | train_inner | epoch 007:    263 / 1474 loss=2.139, trans_loss=3.546, nll_loss=1.712, w2v_ctc_loss=1.143, task_loss=0.931, contrastive_loss=0.111, total=4133.29, n_correct=2410.46, ppl=3.28, accuracy=58.318, wps=13332.1, ups=1.08, wpb=12335.8, bsz=455.5, num_updates=9100, lr=0.00014825, gnorm=0.504, clip=0, loss_scale=8, train_wall=92, gb_free=15.1, wall=8281
2023-08-06 13:02:30 | INFO | train_inner | epoch 007:    363 / 1474 loss=2.21, trans_loss=3.555, nll_loss=1.725, w2v_ctc_loss=1.136, task_loss=0.905, contrastive_loss=0.369, total=4194.76, n_correct=2436.66, ppl=3.31, accuracy=58.088, wps=13438.9, ups=1.07, wpb=12518.2, bsz=477.6, num_updates=9200, lr=0.000147442, gnorm=0.502, clip=0, loss_scale=8, train_wall=93, gb_free=12.8, wall=8374
2023-08-06 13:04:03 | INFO | train_inner | epoch 007:    463 / 1474 loss=2.193, trans_loss=3.556, nll_loss=1.729, w2v_ctc_loss=1.133, task_loss=0.921, contrastive_loss=0.304, total=4153.22, n_correct=2402.28, ppl=3.31, accuracy=57.841, wps=13292.2, ups=1.07, wpb=12403.3, bsz=463, num_updates=9300, lr=0.000146647, gnorm=0.51, clip=0, loss_scale=8, train_wall=93, gb_free=16.7, wall=8467
2023-08-06 13:05:34 | INFO | train_inner | epoch 007:    563 / 1474 loss=2.14, trans_loss=3.554, nll_loss=1.722, w2v_ctc_loss=1.135, task_loss=0.911, contrastive_loss=0.12, total=4168.14, n_correct=2430.35, ppl=3.3, accuracy=58.308, wps=13571.7, ups=1.09, wpb=12434.7, bsz=459.8, num_updates=9400, lr=0.000145865, gnorm=0.5, clip=0, loss_scale=8, train_wall=91, gb_free=16.7, wall=8559
2023-08-06 13:07:08 | INFO | train_inner | epoch 007:    663 / 1474 loss=2.13, trans_loss=3.553, nll_loss=1.722, w2v_ctc_loss=1.128, task_loss=0.925, contrastive_loss=0.109, total=4157.82, n_correct=2424.85, ppl=3.3, accuracy=58.32, wps=13277.6, ups=1.07, wpb=12406.4, bsz=455.4, num_updates=9500, lr=0.000145095, gnorm=0.499, clip=0, loss_scale=8, train_wall=93, gb_free=15.4, wall=8652
2023-08-06 13:08:41 | INFO | train_inner | epoch 007:    763 / 1474 loss=2.13, trans_loss=3.55, nll_loss=1.72, w2v_ctc_loss=1.131, task_loss=0.975, contrastive_loss=0.105, total=4122.1, n_correct=2400.73, ppl=3.29, accuracy=58.24, wps=13279.7, ups=1.08, wpb=12308.4, bsz=446.2, num_updates=9600, lr=0.000144338, gnorm=0.498, clip=0, loss_scale=8, train_wall=92, gb_free=15.5, wall=8745
2023-08-06 13:10:14 | INFO | train_inner | epoch 007:    863 / 1474 loss=2.139, trans_loss=3.558, nll_loss=1.729, w2v_ctc_loss=1.133, task_loss=0.938, contrastive_loss=0.124, total=4147.23, n_correct=2410.23, ppl=3.32, accuracy=58.117, wps=13277.7, ups=1.07, wpb=12377.6, bsz=460.9, num_updates=9700, lr=0.000143592, gnorm=0.498, clip=0, loss_scale=8, train_wall=93, gb_free=17.4, wall=8838
2023-08-06 13:11:47 | INFO | train_inner | epoch 007:    963 / 1474 loss=2.157, trans_loss=3.553, nll_loss=1.725, w2v_ctc_loss=1.117, task_loss=0.886, contrastive_loss=0.214, total=4140.14, n_correct=2411.15, ppl=3.31, accuracy=58.238, wps=13307.4, ups=1.08, wpb=12359.1, bsz=474.6, num_updates=9800, lr=0.000142857, gnorm=0.506, clip=0, loss_scale=8, train_wall=92, gb_free=15.7, wall=8931
2023-08-06 13:13:19 | INFO | train_inner | epoch 007:   1063 / 1474 loss=2.13, trans_loss=3.564, nll_loss=1.739, w2v_ctc_loss=1.135, task_loss=0.979, contrastive_loss=0.091, total=4103.51, n_correct=2378.07, ppl=3.34, accuracy=57.952, wps=13222.7, ups=1.08, wpb=12251.1, bsz=437.5, num_updates=9900, lr=0.000142134, gnorm=0.498, clip=0, loss_scale=8, train_wall=92, gb_free=16.7, wall=9024
2023-08-06 13:14:52 | INFO | train_inner | epoch 007:   1163 / 1474 loss=2.206, trans_loss=3.55, nll_loss=1.724, w2v_ctc_loss=1.122, task_loss=0.907, contrastive_loss=0.356, total=4137.04, n_correct=2412.14, ppl=3.3, accuracy=58.306, wps=13303.3, ups=1.08, wpb=12361.6, bsz=470.9, num_updates=10000, lr=0.000141421, gnorm=0.523, clip=0, loss_scale=8, train_wall=92, gb_free=15.8, wall=9116
2023-08-06 13:14:52 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 13:15:15 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.321 | trans_loss 5.691 | nll_loss 3.007 | w2v_ctc_loss 1.344 | task_loss 4.573 | contrastive_loss 0.264 | total 4003.4 | n_correct 2391.2 | ppl 8.04 | accuracy 59.729 | uer 19.449 | wer 21.248 | raw_wer 21.248 | bleu 18.64 | wps 2216.7 | wpb 4003.4 | bsz 141.8 | num_updates 10000 | best_bleu 18.64
2023-08-06 13:15:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10000 updates
2023-08-06 13:15:15 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_7_10000.pt
2023-08-06 13:15:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_7_10000.pt
2023-08-06 13:16:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_7_10000.pt (epoch 7 @ 10000 updates, score 18.64) (writing took 47.63493021577597 seconds)
Mixup rate:0.5, token after shrink shape:torch.Size([8, 91]), X shape:torch.Size([8, 91, 512])
CTC Tokens:tensor([  19,   19, 1095, 1095,    0], device='cuda:0'), Shrink Mask:tensor([ True, False,  True, False,  True], device='cuda:0'), New Tokens:tensor([  19, 1095,    0,   91,    0], device='cuda:0')
Mixup Sent Mask:tensor([[4],
        [5],
        [6]], device='cuda:0'), 4,  Mixup Mask:tensor([False, False, False, False, False], device='cuda:0'), 
                    Org X:tensor([[ 0.4844,  0.1107, -0.3425,  ..., -1.3320,  1.0674, -0.5444],
        [ 1.7031,  1.0420,  1.6123,  ..., -0.3049, -1.1934, -1.4170],
        [ 0.4485, -0.1714,  1.3232,  ..., -0.3660, -0.6748,  0.5737],
        [-0.3169,  0.4395,  0.0292,  ..., -0.5156,  0.8975,  0.7466],
        [ 0.1032,  0.5420,  1.0078,  ..., -2.0684,  2.6484, -0.2394]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.4844,  0.1107, -0.3425,  ..., -1.3320,  1.0674, -0.5444],
        [ 1.7031,  1.0420,  1.6123,  ..., -0.3049, -1.1934, -1.4170],
        [ 0.4485, -0.1714,  1.3232,  ..., -0.3660, -0.6748,  0.5737],
        [-0.3169,  0.4395,  0.0292,  ..., -0.5156,  0.8975,  0.7466],
        [ 0.1032,  0.5420,  1.0078,  ..., -2.0684,  2.6484, -0.2394]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[ 0.0336, -0.5547, -0.4045,  ..., -0.4009, -0.3496, -1.3955],
        [ 0.6504,  0.5454,  0.7988,  ..., -0.2292, -2.8418, -0.3088],
        [-0.4094, -1.1230, -0.2212,  ..., -2.3750, -1.8711, -2.0059],
        [-0.3115, -0.1788,  0.1266,  ..., -0.4695,  0.5474, -1.8398],
        [-0.4094, -1.1230, -0.2212,  ..., -2.3750, -1.8711, -2.0059]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:0')
2023-08-06 13:17:36 | INFO | train_inner | epoch 007:   1263 / 1474 loss=2.121, trans_loss=3.556, nll_loss=1.729, w2v_ctc_loss=1.117, task_loss=0.941, contrastive_loss=0.116, total=4129.52, n_correct=2406.05, ppl=3.32, accuracy=58.265, wps=7529.8, ups=0.61, wpb=12331.4, bsz=450.2, num_updates=10100, lr=0.00014072, gnorm=0.401, clip=0, loss_scale=8, train_wall=92, gb_free=16.6, wall=9280
2023-08-06 13:19:09 | INFO | train_inner | epoch 007:   1363 / 1474 loss=2.144, trans_loss=3.549, nll_loss=1.721, w2v_ctc_loss=1.129, task_loss=0.875, contrastive_loss=0.15, total=4172.87, n_correct=2438.3, ppl=3.3, accuracy=58.432, wps=13392.8, ups=1.08, wpb=12458.1, bsz=476.2, num_updates=10200, lr=0.000140028, gnorm=0.409, clip=0, loss_scale=8, train_wall=93, gb_free=17.1, wall=9373
2023-08-06 13:20:43 | INFO | train_inner | epoch 007:   1463 / 1474 loss=2.157, trans_loss=3.555, nll_loss=1.73, w2v_ctc_loss=1.126, task_loss=1.003, contrastive_loss=0.216, total=4109.42, n_correct=2387.98, ppl=3.32, accuracy=58.11, wps=13007.7, ups=1.06, wpb=12278.1, bsz=443.8, num_updates=10300, lr=0.000139347, gnorm=0.411, clip=0, loss_scale=8, train_wall=94, gb_free=16.3, wall=9468
2023-08-06 13:20:53 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([32, 48]), X shape:torch.Size([32, 48, 512])
CTC Tokens:tensor([   0,   33,    0, 2353,    0], device='cuda:6'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:6'), New Tokens:tensor([   0,   33,    0, 2353,    0], device='cuda:6')
Mixup Sent Mask:tensor([[ 4],
        [ 5],
        [ 6],
        [17],
        [18],
        [19]], device='cuda:6'), 4,  Mixup Mask:tensor([False, False, False,  True,  True], device='cuda:6'), 
                    Org X:tensor([[-0.4426, -0.1376, -0.2917,  ..., -2.2715, -0.7188,  0.2927],
        [-0.3662,  0.2135, -0.7368,  ..., -1.0977,  0.1307,  0.5195],
        [-0.1384, -0.5015,  0.3127,  ..., -1.9727,  1.7393,  0.3557],
        [-0.7954, -0.6309,  1.2529,  ..., -0.2585,  1.6484, -0.2773],
        [-0.5874, -1.1572,  1.7100,  ..., -0.2356,  1.1689, -0.2981]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.4426, -0.1376, -0.2917,  ..., -2.2715, -0.7188,  0.2927],
        [-0.3662,  0.2135, -0.7368,  ..., -1.0977,  0.1307,  0.5195],
        [-0.1384, -0.5015,  0.3127,  ..., -1.9727,  1.7393,  0.3557],
        [-0.1023, -1.6826,  1.1611,  ..., -1.1758, -3.0605,  1.2363],
        [-0.4094, -1.1230, -0.2212,  ..., -2.3750, -1.8711, -2.0059]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4094, -1.1230, -0.2212,  ..., -2.3750, -1.8711, -2.0059],
        [-0.2141, -0.3179, -0.2391,  ..., -2.2637, -3.4609, -0.4004],
        [-0.4094, -1.1230, -0.2212,  ..., -2.3750, -1.8711, -2.0059],
        [-0.1023, -1.6826,  1.1611,  ..., -1.1758, -3.0605,  1.2363],
        [-0.4094, -1.1230, -0.2212,  ..., -2.3750, -1.8711, -2.0059]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([32, 40]), X shape:torch.Size([32, 40, 512])
CTC Tokens:tensor([   8,   19,   19,   34, 1290], device='cuda:7'), Shrink Mask:tensor([ True,  True, False,  True,  True], device='cuda:7'), New Tokens:tensor([   8,   19,   34, 1290,  712], device='cuda:7')
Mixup Sent Mask:tensor([[ 4],
        [ 5],
        [ 6],
        [17],
        [18],
        [19]], device='cuda:7'), 4,  Mixup Mask:tensor([ True, False,  True,  True, False], device='cuda:7'), 
                    Org X:tensor([[ 0.3591, -0.0293,  0.5977,  ...,  0.5542,  0.5327, -1.0176],
        [ 0.4834,  0.4680, -0.0246,  ..., -0.0980,  0.0591, -0.8013],
        [ 0.4912,  0.3694, -0.4158,  ..., -1.2344,  0.0600,  1.0049],
        [ 0.6270, -0.0196, -0.7974,  ..., -0.5093, -0.1919,  0.3738],
        [ 0.3792,  1.3486,  0.3647,  ..., -0.9790, -1.0156,  0.9282]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3845, -0.2374, -0.7935,  ...,  1.0762, -1.9004, -1.6738],
        [ 0.4834,  0.4680, -0.0246,  ..., -0.0980,  0.0591, -0.8013],
        [-0.3083, -1.0352, -0.1078,  ..., -1.8164, -3.2617, -3.4746],
        [-0.3594,  0.2312, -0.9868,  ..., -1.0869,  1.2275, -0.4717],
        [ 0.3792,  1.3486,  0.3647,  ..., -0.9790, -1.0156,  0.9282]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3845, -0.2374, -0.7935,  ...,  1.0762, -1.9004, -1.6738],
        [ 0.0336, -0.5547, -0.4045,  ..., -0.4009, -0.3496, -1.3955],
        [-0.3083, -1.0352, -0.1078,  ..., -1.8164, -3.2617, -3.4746],
        [-0.3594,  0.2312, -0.9868,  ..., -1.0869,  1.2275, -0.4717],
        [ 1.3008,  1.0674, -0.1104,  ...,  2.4863, -1.0361, -2.6523]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 94]), X shape:torch.Size([8, 94, 512])
CTC Tokens:tensor([53, 53,  0,  0,  0], device='cuda:5'), Shrink Mask:tensor([ True, False,  True, False, False], device='cuda:5'), New Tokens:tensor([  53,    0, 2318,    0,  388], device='cuda:5')
Mixup Sent Mask:tensor([[4],
        [5],
        [6]], device='cuda:5'), 4,  Mixup Mask:tensor([False,  True, False,  True,  True], device='cuda:5'), 
                    Org X:tensor([[-0.4165, -1.6924, -0.8164,  ..., -1.0156,  0.7056, -1.6748],
        [ 0.9380, -0.4609,  1.0088,  ..., -1.4326,  1.2822, -2.1250],
        [ 0.5693, -0.1920, -0.1802,  ..., -0.4316,  0.3352,  0.4299],
        [ 0.3340,  0.4583,  1.0381,  ...,  0.1628,  0.8623,  0.4604],
        [ 0.2847, -0.0887, -1.3682,  ..., -0.2318,  2.2227, -1.0078]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-4.1650e-01, -1.6924e+00, -8.1641e-01,  ..., -1.0156e+00,
          7.0557e-01, -1.6748e+00],
        [-4.0942e-01, -1.1230e+00, -2.2119e-01,  ..., -2.3750e+00,
         -1.8711e+00, -2.0059e+00],
        [ 5.6934e-01, -1.9202e-01, -1.8018e-01,  ..., -4.3164e-01,
          3.3521e-01,  4.2993e-01],
        [-4.0942e-01, -1.1230e+00, -2.2119e-01,  ..., -2.3750e+00,
         -1.8711e+00, -2.0059e+00],
        [-1.8662e+00, -1.0244e+00,  1.9092e-01,  ..., -6.6471e-04,
          2.2734e+00, -3.6875e+00]], device='cuda:5', dtype=torch.float16,
       grad_fn=<SliceBackward0>), Emb: tensor([[ 2.3767e-01, -3.5449e-01, -7.5562e-02,  ..., -2.3887e+00,
          3.2090e+00, -2.6426e+00],
        [-4.0942e-01, -1.1230e+00, -2.2119e-01,  ..., -2.3750e+00,
         -1.8711e+00, -2.0059e+00],
        [ 4.1595e-02,  1.0114e-01,  4.2334e-01,  ...,  1.5781e+00,
          1.4893e+00, -9.4727e-01],
        [-4.0942e-01, -1.1230e+00, -2.2119e-01,  ..., -2.3750e+00,
         -1.8711e+00, -2.0059e+00],
        [-1.8662e+00, -1.0244e+00,  1.9092e-01,  ..., -6.6471e-04,
          2.2734e+00, -3.6875e+00]], device='cuda:5', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 83]), X shape:torch.Size([8, 83, 512])
CTC Tokens:tensor([67, 67, 24, 24,  0], device='cuda:3'), Shrink Mask:tensor([ True, False,  True, False,  True], device='cuda:3'), New Tokens:tensor([ 67,  24,   0, 135,   0], device='cuda:3')
Mixup Sent Mask:tensor([[4],
        [5],
        [6]], device='cuda:3'), 4,  Mixup Mask:tensor([False,  True, False,  True,  True], device='cuda:3'), 
                    Org X:tensor([[-0.2245, -0.0157,  0.0829,  ...,  0.2542,  0.6606, -2.3496],
        [-2.8281,  0.1365,  0.6528,  ..., -0.3210,  1.8584, -2.0156],
        [ 1.4805,  0.7930,  1.3701,  ..., -2.4355,  0.5415,  0.3623],
        [ 1.1436,  1.2900,  0.0429,  ..., -0.0309, -1.4395,  0.2595],
        [ 1.3574,  0.3975,  0.1071,  ..., -1.0361, -2.2090,  0.3447]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.2245, -0.0157,  0.0829,  ...,  0.2542,  0.6606, -2.3496],
        [-0.6948, -0.5679,  0.8335,  ..., -4.4531,  1.4902, -1.5498],
        [ 1.4805,  0.7930,  1.3701,  ..., -2.4355,  0.5415,  0.3623],
        [-0.5796, -0.9370, -1.3086,  ...,  2.4023, -3.3027, -0.2744],
        [-0.4094, -1.1230, -0.2212,  ..., -2.3750, -1.8711, -2.0059]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-4.0894e-03, -3.0396e-01, -9.1016e-01,  ...,  1.5938e+00,
         -1.3740e+00, -4.2188e+00],
        [-6.9482e-01, -5.6787e-01,  8.3350e-01,  ..., -4.4531e+00,
          1.4902e+00, -1.5498e+00],
        [-4.0942e-01, -1.1230e+00, -2.2119e-01,  ..., -2.3750e+00,
         -1.8711e+00, -2.0059e+00],
        [-5.7959e-01, -9.3701e-01, -1.3086e+00,  ...,  2.4023e+00,
         -3.3027e+00, -2.7441e-01],
        [-4.0942e-01, -1.1230e+00, -2.2119e-01,  ..., -2.3750e+00,
         -1.8711e+00, -2.0059e+00]], device='cuda:3', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 66]), X shape:torch.Size([16, 66, 512])
CTC Tokens:tensor([ 19,  19,  66,   0, 121], device='cuda:4'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:4'), New Tokens:tensor([  19,   66,    0,  121, 8525], device='cuda:4')
Mixup Sent Mask:tensor([[4],
        [5],
        [6]], device='cuda:4'), 4,  Mixup Mask:tensor([False,  True,  True, False, False], device='cuda:4'), 
                    Org X:tensor([[ 0.7612,  0.0969,  0.2783,  ..., -0.7666,  0.7808, -0.4868],
        [ 0.5762,  1.0293, -0.1809,  ..., -0.3186,  1.2539,  0.8770],
        [ 0.0438,  0.9688,  0.2295,  ..., -0.1459,  0.8916,  0.4053],
        [ 1.7207,  0.3281, -0.3030,  ..., -1.9697,  0.6685, -0.0114],
        [ 1.1357, -0.1232, -0.9829,  ..., -1.9482,  0.5562, -0.9990]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.7612,  0.0969,  0.2783,  ..., -0.7666,  0.7808, -0.4868],
        [-0.4534,  0.0514, -0.4141,  ...,  1.6182,  2.4629, -0.3252],
        [-0.4094, -1.1230, -0.2212,  ..., -2.3750, -1.8711, -2.0059],
        [ 1.7207,  0.3281, -0.3030,  ..., -1.9697,  0.6685, -0.0114],
        [ 1.1357, -0.1232, -0.9829,  ..., -1.9482,  0.5562, -0.9990]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[ 0.0336, -0.5547, -0.4045,  ..., -0.4009, -0.3496, -1.3955],
        [-0.4534,  0.0514, -0.4141,  ...,  1.6182,  2.4629, -0.3252],
        [-0.4094, -1.1230, -0.2212,  ..., -2.3750, -1.8711, -2.0059],
        [ 0.4910,  1.0371,  0.2874,  ..., -0.4861,  1.0625,  1.0830],
        [ 0.9517, -0.7656, -1.3496,  ...,  0.1664, -0.1837, -0.8398]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 155]), X shape:torch.Size([8, 155, 512])
CTC Tokens:tensor([101, 246,   4,  38,   0], device='cuda:2'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:2'), New Tokens:tensor([101, 246,   4,  38,   0], device='cuda:2')
Mixup Sent Mask:tensor([[4],
        [5],
        [6]], device='cuda:2'), 4,  Mixup Mask:tensor([False,  True, False,  True,  True], device='cuda:2'), 
                    Org X:tensor([[-0.5410, -0.0632, -1.3320,  ..., -0.1187, -1.1064, -0.2474],
        [ 0.3665,  0.3630, -1.0234,  ..., -0.6353, -1.7842, -0.7080],
        [ 1.6904, -0.3320, -0.8418,  ..., -2.5957, -2.5859, -3.6406],
        [ 1.4463, -0.2664, -1.1768,  ..., -2.1953,  0.7192, -3.9043],
        [ 0.4473, -0.2150,  0.1893,  ..., -0.2683,  0.8623, -3.0137]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.5410, -0.0632, -1.3320,  ..., -0.1187, -1.1064, -0.2474],
        [ 0.3704, -0.0271, -1.6465,  ..., -2.1211, -1.0996, -3.1016],
        [ 1.6904, -0.3320, -0.8418,  ..., -2.5957, -2.5859, -3.6406],
        [-1.1221, -0.8364,  0.5371,  ...,  0.1008, -0.1281, -0.8975],
        [-0.4094, -1.1230, -0.2212,  ..., -2.3750, -1.8711, -2.0059]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.6294, -0.1964,  0.2164,  ...,  0.0698, -2.0391,  1.7275],
        [ 0.3704, -0.0271, -1.6465,  ..., -2.1211, -1.0996, -3.1016],
        [-0.1283, -0.5986, -0.5190,  ..., -1.7637, -1.3613, -2.5215],
        [-1.1221, -0.8364,  0.5371,  ...,  0.1008, -0.1281, -0.8975],
        [-0.4094, -1.1230, -0.2212,  ..., -2.3750, -1.8711, -2.0059]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([32, 38]), X shape:torch.Size([32, 38, 512])
CTC Tokens:tensor([  0, 409,  17,   0,   0], device='cuda:1'), Shrink Mask:tensor([ True,  True,  True,  True, False], device='cuda:1'), New Tokens:tensor([  0, 409,  17,   0,   4], device='cuda:1')
Mixup Sent Mask:tensor([[ 4],
        [ 5],
        [ 6],
        [17],
        [18],
        [19]], device='cuda:1'), 4,  Mixup Mask:tensor([False,  True,  True,  True,  True], device='cuda:1'), 
                    Org X:tensor([[ 0.7700, -1.5195, -0.4397,  ...,  0.6060, -1.5488, -0.3604],
        [ 0.6782, -0.7358, -0.5034,  ...,  0.4392, -0.3735,  0.0290],
        [-0.8174,  0.1182,  0.4458,  ...,  0.4182,  0.0238, -0.4172],
        [-0.7344, -0.2012,  0.2854,  ...,  0.3496,  0.0285, -0.3467],
        [ 0.3811,  0.0843, -0.2063,  ..., -0.3428,  1.2539, -2.0312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.7700, -1.5195, -0.4397,  ...,  0.6060, -1.5488, -0.3604],
        [-0.3533,  0.4011,  0.2898,  ...,  7.1328, -2.0527, -0.2661],
        [-0.2306, -0.3716, -0.3567,  ...,  0.2388, -1.8975, -2.2812],
        [-0.4094, -1.1230, -0.2212,  ..., -2.3750, -1.8711, -2.0059],
        [-0.1283, -0.5986, -0.5190,  ..., -1.7637, -1.3613, -2.5215]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4094, -1.1230, -0.2212,  ..., -2.3750, -1.8711, -2.0059],
        [-0.3533,  0.4011,  0.2898,  ...,  7.1328, -2.0527, -0.2661],
        [-0.2306, -0.3716, -0.3567,  ...,  0.2388, -1.8975, -2.2812],
        [-0.4094, -1.1230, -0.2212,  ..., -2.3750, -1.8711, -2.0059],
        [-0.1283, -0.5986, -0.5190,  ..., -1.7637, -1.3613, -2.5215]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:1')
2023-08-06 13:21:17 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.318 | trans_loss 5.687 | nll_loss 2.999 | w2v_ctc_loss 1.342 | task_loss 4.597 | contrastive_loss 0.264 | total 4003.4 | n_correct 2390.5 | ppl 7.99 | accuracy 59.712 | uer 19.499 | wer 21.3 | raw_wer 21.3 | bleu 18.81 | wps 2078.9 | wpb 4003.4 | bsz 141.8 | num_updates 10311 | best_bleu 18.81
2023-08-06 13:21:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10311 updates
2023-08-06 13:21:17 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 13:21:29 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 13:21:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt (epoch 7 @ 10311 updates, score 18.81) (writing took 23.362940905615687 seconds)
2023-08-06 13:21:41 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-08-06 13:21:41 | INFO | train | epoch 007 | loss 2.153 | trans_loss 3.553 | nll_loss 1.724 | w2v_ctc_loss 1.131 | task_loss 0.931 | contrastive_loss 0.183 | total 4138.65 | n_correct 2408.06 | ppl 3.3 | accuracy 58.185 | wps 12168.5 | ups 0.98 | wpb 12355.8 | bsz 458.5 | num_updates 10311 | lr 0.000139272 | gnorm 0.484 | clip 0 | loss_scale 8 | train_wall 1363 | gb_free 13 | wall 9525
2023-08-06 13:21:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 13:21:41 | INFO | fairseq.trainer | begin training epoch 8
2023-08-06 13:21:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 13:23:13 | INFO | train_inner | epoch 008:     89 / 1474 loss=2.086, trans_loss=3.53, nll_loss=1.691, w2v_ctc_loss=1.088, task_loss=0.985, contrastive_loss=0.11, total=4116.25, n_correct=2433.24, ppl=3.23, accuracy=59.113, wps=8227.5, ups=0.67, wpb=12273, bsz=443.3, num_updates=10400, lr=0.000138675, gnorm=0.405, clip=0, loss_scale=8, train_wall=93, gb_free=16.8, wall=9617
2023-08-06 13:24:44 | INFO | train_inner | epoch 008:    189 / 1474 loss=2.089, trans_loss=3.522, nll_loss=1.681, w2v_ctc_loss=1.085, task_loss=1.011, contrastive_loss=0.13, total=4037.23, n_correct=2392.28, ppl=3.21, accuracy=59.255, wps=13113.8, ups=1.09, wpb=12041.5, bsz=428.6, num_updates=10500, lr=0.000138013, gnorm=0.411, clip=0, loss_scale=16, train_wall=91, gb_free=12.5, wall=9709
2023-08-06 13:26:18 | INFO | train_inner | epoch 008:    289 / 1474 loss=2.086, trans_loss=3.517, nll_loss=1.677, w2v_ctc_loss=1.082, task_loss=0.876, contrastive_loss=0.127, total=4207.78, n_correct=2499.15, ppl=3.2, accuracy=59.394, wps=13428.7, ups=1.07, wpb=12556.5, bsz=488.1, num_updates=10600, lr=0.000137361, gnorm=0.408, clip=0, loss_scale=16, train_wall=93, gb_free=12.7, wall=9802
2023-08-06 13:27:51 | INFO | train_inner | epoch 008:    389 / 1474 loss=2.109, trans_loss=3.528, nll_loss=1.69, w2v_ctc_loss=1.103, task_loss=0.995, contrastive_loss=0.153, total=4127.24, n_correct=2435.31, ppl=3.23, accuracy=59.006, wps=13177.8, ups=1.07, wpb=12316.2, bsz=441.4, num_updates=10700, lr=0.000136717, gnorm=0.407, clip=0, loss_scale=16, train_wall=93, gb_free=11.5, wall=9896
2023-08-06 13:29:26 | INFO | train_inner | epoch 008:    489 / 1474 loss=2.177, trans_loss=3.522, nll_loss=1.685, w2v_ctc_loss=1.076, task_loss=0.834, contrastive_loss=0.408, total=4203.76, n_correct=2492.04, ppl=3.22, accuracy=59.281, wps=13251.6, ups=1.06, wpb=12548.2, bsz=504.5, num_updates=10800, lr=0.000136083, gnorm=0.407, clip=0, loss_scale=16, train_wall=94, gb_free=14.4, wall=9990
2023-08-06 13:30:59 | INFO | train_inner | epoch 008:    589 / 1474 loss=2.087, trans_loss=3.526, nll_loss=1.693, w2v_ctc_loss=1.101, task_loss=1.018, contrastive_loss=0.089, total=4062.5, n_correct=2390.3, ppl=3.23, accuracy=58.838, wps=13058.4, ups=1.08, wpb=12145.4, bsz=427.9, num_updates=10900, lr=0.000135457, gnorm=0.41, clip=0, loss_scale=16, train_wall=93, gb_free=11, wall=10083
2023-08-06 13:32:33 | INFO | train_inner | epoch 008:    689 / 1474 loss=2.08, trans_loss=3.52, nll_loss=1.682, w2v_ctc_loss=1.093, task_loss=0.961, contrastive_loss=0.099, total=4142.78, n_correct=2459.69, ppl=3.21, accuracy=59.373, wps=13226.8, ups=1.07, wpb=12364.4, bsz=448.6, num_updates=11000, lr=0.00013484, gnorm=0.401, clip=0, loss_scale=16, train_wall=93, gb_free=15.7, wall=10177
2023-08-06 13:34:05 | INFO | train_inner | epoch 008:    789 / 1474 loss=2.1, trans_loss=3.521, nll_loss=1.688, w2v_ctc_loss=1.086, task_loss=0.957, contrastive_loss=0.182, total=4118.9, n_correct=2437.7, ppl=3.22, accuracy=59.183, wps=13335.9, ups=1.08, wpb=12310.9, bsz=447.8, num_updates=11100, lr=0.000134231, gnorm=0.403, clip=0, loss_scale=16, train_wall=92, gb_free=15, wall=10269
2023-08-06 13:35:38 | INFO | train_inner | epoch 008:    889 / 1474 loss=2.102, trans_loss=3.522, nll_loss=1.688, w2v_ctc_loss=1.076, task_loss=0.896, contrastive_loss=0.193, total=4169.01, n_correct=2476.62, ppl=3.22, accuracy=59.405, wps=13393.8, ups=1.08, wpb=12452.5, bsz=473.7, num_updates=11200, lr=0.000133631, gnorm=0.409, clip=0, loss_scale=16, train_wall=92, gb_free=15.9, wall=10362
2023-08-06 13:37:10 | INFO | train_inner | epoch 008:    989 / 1474 loss=2.067, trans_loss=3.524, nll_loss=1.688, w2v_ctc_loss=1.076, task_loss=0.891, contrastive_loss=0.096, total=4154.69, n_correct=2468.44, ppl=3.22, accuracy=59.413, wps=13464.3, ups=1.09, wpb=12403.4, bsz=464.9, num_updates=11300, lr=0.000133038, gnorm=0.398, clip=0, loss_scale=16, train_wall=92, gb_free=17.6, wall=10454
2023-08-06 13:38:44 | INFO | train_inner | epoch 008:   1089 / 1474 loss=2.13, trans_loss=3.53, nll_loss=1.697, w2v_ctc_loss=1.077, task_loss=0.926, contrastive_loss=0.322, total=4199.1, n_correct=2480.81, ppl=3.24, accuracy=59.08, wps=13330.7, ups=1.06, wpb=12534.3, bsz=465.3, num_updates=11400, lr=0.000132453, gnorm=0.407, clip=0, loss_scale=16, train_wall=94, gb_free=12.4, wall=10548
2023-08-06 13:40:16 | INFO | train_inner | epoch 008:   1189 / 1474 loss=2.079, trans_loss=3.522, nll_loss=1.689, w2v_ctc_loss=1.079, task_loss=0.881, contrastive_loss=0.106, total=4177.31, n_correct=2479, ppl=3.22, accuracy=59.344, wps=13517.3, ups=1.08, wpb=12476.3, bsz=472.6, num_updates=11500, lr=0.000131876, gnorm=0.402, clip=0, loss_scale=16, train_wall=92, gb_free=14.7, wall=10641
2023-08-06 13:41:48 | INFO | train_inner | epoch 008:   1289 / 1474 loss=2.091, trans_loss=3.529, nll_loss=1.697, w2v_ctc_loss=1.091, task_loss=0.973, contrastive_loss=0.127, total=4063.85, n_correct=2399.58, ppl=3.24, accuracy=59.047, wps=13203.1, ups=1.09, wpb=12140.6, bsz=438.4, num_updates=11600, lr=0.000131306, gnorm=0.409, clip=0, loss_scale=16, train_wall=91, gb_free=16.6, wall=10733
2023-08-06 13:43:21 | INFO | train_inner | epoch 008:   1389 / 1474 loss=2.101, trans_loss=3.53, nll_loss=1.699, w2v_ctc_loss=1.081, task_loss=0.921, contrastive_loss=0.178, total=4141.5, n_correct=2452.68, ppl=3.25, accuracy=59.222, wps=13410.6, ups=1.08, wpb=12367.2, bsz=461.5, num_updates=11700, lr=0.000130744, gnorm=0.404, clip=0, loss_scale=16, train_wall=92, gb_free=16.2, wall=10825
2023-08-06 13:44:39 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 13:45:03 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 4.294 | trans_loss 5.652 | nll_loss 2.95 | w2v_ctc_loss 1.352 | task_loss 4.583 | contrastive_loss 0.253 | total 4003.4 | n_correct 2415.7 | ppl 7.73 | accuracy 60.341 | uer 18.822 | wer 20.76 | raw_wer 20.76 | bleu 18.84 | wps 2140.3 | wpb 4003.4 | bsz 141.8 | num_updates 11785 | best_bleu 18.84
2023-08-06 13:45:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11785 updates
2023-08-06 13:45:03 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 13:45:16 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 13:45:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt (epoch 8 @ 11785 updates, score 18.84) (writing took 23.49414099752903 seconds)
2023-08-06 13:45:27 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-08-06 13:45:27 | INFO | train | epoch 008 | loss 2.099 | trans_loss 3.525 | nll_loss 1.689 | w2v_ctc_loss 1.084 | task_loss 0.933 | contrastive_loss 0.173 | total 4138.65 | n_correct 2450.99 | ppl 3.22 | accuracy 59.222 | wps 12770.6 | ups 1.03 | wpb 12355.8 | bsz 458.5 | num_updates 11785 | lr 0.000130272 | gnorm 0.406 | clip 0 | loss_scale 16 | train_wall 1363 | gb_free 16.7 | wall 10951
2023-08-06 13:45:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 13:45:28 | INFO | fairseq.trainer | begin training epoch 9
2023-08-06 13:45:28 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 13:45:49 | INFO | train_inner | epoch 009:     15 / 1474 loss=2.114, trans_loss=3.523, nll_loss=1.688, w2v_ctc_loss=1.064, task_loss=0.899, contrastive_loss=0.301, total=4139.35, n_correct=2459.31, ppl=3.22, accuracy=59.413, wps=8319.6, ups=0.67, wpb=12350.9, bsz=472.9, num_updates=11800, lr=0.000130189, gnorm=0.412, clip=0, loss_scale=16, train_wall=92, gb_free=15.3, wall=10973
2023-08-06 13:47:22 | INFO | train_inner | epoch 009:    115 / 1474 loss=2.035, trans_loss=3.485, nll_loss=1.639, w2v_ctc_loss=1.038, task_loss=0.888, contrastive_loss=0.122, total=4181.9, n_correct=2529.92, ppl=3.11, accuracy=60.497, wps=13499, ups=1.08, wpb=12488.1, bsz=475.9, num_updates=11900, lr=0.000129641, gnorm=0.397, clip=0, loss_scale=16, train_wall=92, gb_free=16.1, wall=11066
2023-08-06 13:48:55 | INFO | train_inner | epoch 009:    215 / 1474 loss=2.027, trans_loss=3.494, nll_loss=1.65, w2v_ctc_loss=1.041, task_loss=1.004, contrastive_loss=0.085, total=4062.07, n_correct=2445.28, ppl=3.14, accuracy=60.198, wps=13006.6, ups=1.07, wpb=12129.1, bsz=431.6, num_updates=12000, lr=0.000129099, gnorm=0.405, clip=0, loss_scale=16, train_wall=93, gb_free=15.3, wall=11159
2023-08-06 13:48:55 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 13:49:18 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.283 | trans_loss 5.662 | nll_loss 2.965 | w2v_ctc_loss 1.291 | task_loss 4.556 | contrastive_loss 0.256 | total 4003.4 | n_correct 2410.2 | ppl 7.81 | accuracy 60.204 | uer 18.674 | wer 20.503 | raw_wer 20.503 | bleu 18.74 | wps 2283.3 | wpb 4003.4 | bsz 141.8 | num_updates 12000 | best_bleu 18.84
2023-08-06 13:49:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12000 updates
2023-08-06 13:49:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_9_12000.pt
2023-08-06 13:49:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_9_12000.pt
2023-08-06 13:49:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_9_12000.pt (epoch 9 @ 12000 updates, score 18.74) (writing took 34.41513928771019 seconds)
2023-08-06 13:51:27 | INFO | train_inner | epoch 009:    315 / 1474 loss=2.031, trans_loss=3.482, nll_loss=1.637, w2v_ctc_loss=1.028, task_loss=0.872, contrastive_loss=0.131, total=4152.1, n_correct=2512.68, ppl=3.11, accuracy=60.516, wps=8136.4, ups=0.66, wpb=12407.3, bsz=476.6, num_updates=12100, lr=0.000128565, gnorm=0.406, clip=0, loss_scale=16, train_wall=92, gb_free=16.1, wall=11311
2023-08-06 13:53:01 | INFO | train_inner | epoch 009:    415 / 1474 loss=2.029, trans_loss=3.497, nll_loss=1.655, w2v_ctc_loss=1.037, task_loss=0.913, contrastive_loss=0.1, total=4203.78, n_correct=2528.99, ppl=3.15, accuracy=60.16, wps=13420.2, ups=1.07, wpb=12551.8, bsz=469.8, num_updates=12200, lr=0.000128037, gnorm=0.399, clip=0, loss_scale=16, train_wall=93, gb_free=17, wall=11405
2023-08-06 13:54:33 | INFO | train_inner | epoch 009:    515 / 1474 loss=2.067, trans_loss=3.504, nll_loss=1.662, w2v_ctc_loss=1.063, task_loss=0.982, contrastive_loss=0.15, total=4112.78, n_correct=2462.6, ppl=3.16, accuracy=59.877, wps=13279.2, ups=1.08, wpb=12275.6, bsz=437.7, num_updates=12300, lr=0.000127515, gnorm=0.405, clip=0, loss_scale=16, train_wall=92, gb_free=16, wall=11497
2023-08-06 13:56:06 | INFO | train_inner | epoch 009:    615 / 1474 loss=2.028, trans_loss=3.494, nll_loss=1.653, w2v_ctc_loss=1.032, task_loss=0.949, contrastive_loss=0.11, total=4131.32, n_correct=2487.41, ppl=3.15, accuracy=60.209, wps=13289.8, ups=1.08, wpb=12347.4, bsz=455, num_updates=12400, lr=0.000127, gnorm=0.4, clip=0, loss_scale=16, train_wall=92, gb_free=17.7, wall=11590
2023-08-06 13:57:39 | INFO | train_inner | epoch 009:    715 / 1474 loss=2.072, trans_loss=3.505, nll_loss=1.667, w2v_ctc_loss=1.057, task_loss=0.954, contrastive_loss=0.192, total=4082.11, n_correct=2442.18, ppl=3.18, accuracy=59.826, wps=13162.8, ups=1.08, wpb=12194.8, bsz=449.7, num_updates=12500, lr=0.000126491, gnorm=0.411, clip=0, loss_scale=16, train_wall=92, gb_free=16.8, wall=11683
2023-08-06 13:58:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-06 13:59:13 | INFO | train_inner | epoch 009:    816 / 1474 loss=2.094, trans_loss=3.498, nll_loss=1.659, w2v_ctc_loss=1.05, task_loss=0.86, contrastive_loss=0.259, total=4202.99, n_correct=2525.41, ppl=3.16, accuracy=60.086, wps=13329.2, ups=1.06, wpb=12557.2, bsz=492, num_updates=12600, lr=0.000125988, gnorm=0.414, clip=0, loss_scale=16, train_wall=94, gb_free=14.2, wall=11777
2023-08-06 14:00:48 | INFO | train_inner | epoch 009:    916 / 1474 loss=2.092, trans_loss=3.504, nll_loss=1.662, w2v_ctc_loss=1.048, task_loss=0.962, contrastive_loss=0.319, total=4146.05, n_correct=2486.88, ppl=3.17, accuracy=59.982, wps=13074.7, ups=1.06, wpb=12371.5, bsz=450.3, num_updates=12700, lr=0.000125491, gnorm=0.402, clip=0, loss_scale=16, train_wall=94, gb_free=17.7, wall=11872
2023-08-06 14:02:20 | INFO | train_inner | epoch 009:   1016 / 1474 loss=2.045, trans_loss=3.513, nll_loss=1.674, w2v_ctc_loss=1.055, task_loss=1.04, contrastive_loss=0.099, total=4101.48, n_correct=2450.09, ppl=3.19, accuracy=59.737, wps=13237, ups=1.08, wpb=12241.7, bsz=424.4, num_updates=12800, lr=0.000125, gnorm=0.401, clip=0, loss_scale=16, train_wall=92, gb_free=15.7, wall=11964
2023-08-06 14:03:53 | INFO | train_inner | epoch 009:   1116 / 1474 loss=2.045, trans_loss=3.508, nll_loss=1.665, w2v_ctc_loss=1.044, task_loss=0.877, contrastive_loss=0.121, total=4179.09, n_correct=2509.88, ppl=3.17, accuracy=60.058, wps=13393.7, ups=1.08, wpb=12457.7, bsz=474.7, num_updates=12900, lr=0.000124515, gnorm=0.406, clip=0, loss_scale=16, train_wall=93, gb_free=15.1, wall=12057
2023-08-06 14:05:27 | INFO | train_inner | epoch 009:   1216 / 1474 loss=2.048, trans_loss=3.508, nll_loss=1.67, w2v_ctc_loss=1.058, task_loss=0.987, contrastive_loss=0.104, total=4140.66, n_correct=2482.24, ppl=3.18, accuracy=59.948, wps=13200.8, ups=1.07, wpb=12363.4, bsz=448.1, num_updates=13000, lr=0.000124035, gnorm=0.405, clip=0, loss_scale=16, train_wall=93, gb_free=16.9, wall=12151
2023-08-06 14:07:00 | INFO | train_inner | epoch 009:   1316 / 1474 loss=2.09, trans_loss=3.502, nll_loss=1.662, w2v_ctc_loss=1.035, task_loss=0.847, contrastive_loss=0.296, total=4204.43, n_correct=2531.42, ppl=3.16, accuracy=60.208, wps=13516.5, ups=1.08, wpb=12544.9, bsz=492.5, num_updates=13100, lr=0.00012356, gnorm=0.399, clip=0, loss_scale=16, train_wall=92, gb_free=17.6, wall=12244
2023-08-06 14:08:32 | INFO | train_inner | epoch 009:   1416 / 1474 loss=2.041, trans_loss=3.516, nll_loss=1.679, w2v_ctc_loss=1.054, task_loss=1.008, contrastive_loss=0.083, total=4069.19, n_correct=2432.86, ppl=3.2, accuracy=59.787, wps=13224.8, ups=1.09, wpb=12143.2, bsz=427.7, num_updates=13200, lr=0.000123091, gnorm=0.404, clip=0, loss_scale=16, train_wall=91, gb_free=16.3, wall=12336
2023-08-06 14:09:24 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 14:09:48 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.263 | trans_loss 5.629 | nll_loss 2.926 | w2v_ctc_loss 1.297 | task_loss 4.574 | contrastive_loss 0.261 | total 4003.4 | n_correct 2429.1 | ppl 7.6 | accuracy 60.676 | uer 18.276 | wer 20.242 | raw_wer 20.242 | bleu 19.2 | wps 2207.4 | wpb 4003.4 | bsz 141.8 | num_updates 13258 | best_bleu 19.2
2023-08-06 14:09:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 13258 updates
2023-08-06 14:09:48 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 14:10:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 14:10:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt (epoch 9 @ 13258 updates, score 19.2) (writing took 23.765021963045 seconds)
2023-08-06 14:10:12 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-08-06 14:10:12 | INFO | train | epoch 009 | loss 2.054 | trans_loss 3.501 | nll_loss 1.66 | w2v_ctc_loss 1.046 | task_loss 0.934 | contrastive_loss 0.16 | total 4137.53 | n_correct 2486.06 | ppl 3.16 | accuracy 60.085 | wps 12254.4 | ups 0.99 | wpb 12352.4 | bsz 457.9 | num_updates 13258 | lr 0.000122822 | gnorm 0.404 | clip 0 | loss_scale 16 | train_wall 1363 | gb_free 11.4 | wall 12436
2023-08-06 14:10:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 14:10:12 | INFO | fairseq.trainer | begin training epoch 10
2023-08-06 14:10:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 14:10:59 | INFO | train_inner | epoch 010:     42 / 1474 loss=2.045, trans_loss=3.494, nll_loss=1.65, w2v_ctc_loss=1.025, task_loss=0.889, contrastive_loss=0.181, total=4100.8, n_correct=2483.36, ppl=3.14, accuracy=60.558, wps=8319.8, ups=0.68, wpb=12238.2, bsz=469.4, num_updates=13300, lr=0.000122628, gnorm=0.408, clip=0, loss_scale=16, train_wall=91, gb_free=16.2, wall=12483
2023-08-06 14:12:31 | INFO | train_inner | epoch 010:    142 / 1474 loss=1.985, trans_loss=3.468, nll_loss=1.617, w2v_ctc_loss=0.995, task_loss=0.881, contrastive_loss=0.101, total=4247.35, n_correct=2599.75, ppl=3.07, accuracy=61.209, wps=13669.8, ups=1.08, wpb=12684.5, bsz=479.6, num_updates=13400, lr=0.000122169, gnorm=0.39, clip=0, loss_scale=16, train_wall=92, gb_free=11.3, wall=12576
2023-08-06 14:14:04 | INFO | train_inner | epoch 010:    242 / 1474 loss=2.029, trans_loss=3.47, nll_loss=1.618, w2v_ctc_loss=1.009, task_loss=0.923, contrastive_loss=0.225, total=4122.82, n_correct=2521.38, ppl=3.07, accuracy=61.157, wps=13277.6, ups=1.08, wpb=12303.3, bsz=461.4, num_updates=13500, lr=0.000121716, gnorm=0.397, clip=0, loss_scale=16, train_wall=92, gb_free=16.1, wall=12668
2023-08-06 14:15:37 | INFO | train_inner | epoch 010:    342 / 1474 loss=2.003, trans_loss=3.469, nll_loss=1.623, w2v_ctc_loss=1.003, task_loss=0.943, contrastive_loss=0.138, total=4138.27, n_correct=2522.22, ppl=3.08, accuracy=60.949, wps=13306.8, ups=1.08, wpb=12371, bsz=453.8, num_updates=13600, lr=0.000121268, gnorm=0.401, clip=0, loss_scale=16, train_wall=92, gb_free=16.2, wall=12761
2023-08-06 14:17:12 | INFO | train_inner | epoch 010:    442 / 1474 loss=2.039, trans_loss=3.475, nll_loss=1.628, w2v_ctc_loss=0.99, task_loss=0.897, contrastive_loss=0.309, total=4196.37, n_correct=2559.84, ppl=3.09, accuracy=61.001, wps=13229, ups=1.06, wpb=12528, bsz=481.1, num_updates=13700, lr=0.000120824, gnorm=0.397, clip=0, loss_scale=16, train_wall=94, gb_free=15.9, wall=12856
2023-08-06 14:18:45 | INFO | train_inner | epoch 010:    542 / 1474 loss=2.012, trans_loss=3.489, nll_loss=1.641, w2v_ctc_loss=1.027, task_loss=1.003, contrastive_loss=0.091, total=4102.8, n_correct=2485.56, ppl=3.12, accuracy=60.582, wps=13079.6, ups=1.07, wpb=12234.1, bsz=437.8, num_updates=13800, lr=0.000120386, gnorm=0.403, clip=0, loss_scale=16, train_wall=93, gb_free=16.8, wall=12949
2023-08-06 14:20:19 | INFO | train_inner | epoch 010:    642 / 1474 loss=2.04, trans_loss=3.485, nll_loss=1.639, w2v_ctc_loss=1.015, task_loss=0.888, contrastive_loss=0.207, total=4176.56, n_correct=2536.98, ppl=3.11, accuracy=60.743, wps=13336.3, ups=1.07, wpb=12464, bsz=477.2, num_updates=13900, lr=0.000119952, gnorm=0.403, clip=0, loss_scale=16, train_wall=93, gb_free=16, wall=13043
2023-08-06 14:21:51 | INFO | train_inner | epoch 010:    742 / 1474 loss=2.016, trans_loss=3.485, nll_loss=1.639, w2v_ctc_loss=1.032, task_loss=0.935, contrastive_loss=0.091, total=4125.87, n_correct=2501.74, ppl=3.11, accuracy=60.635, wps=13335.2, ups=1.08, wpb=12315.3, bsz=454.4, num_updates=14000, lr=0.000119523, gnorm=0.406, clip=0, loss_scale=16, train_wall=92, gb_free=14.2, wall=13135
2023-08-06 14:21:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 14:22:16 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.284 | trans_loss 5.63 | nll_loss 2.92 | w2v_ctc_loss 1.37 | task_loss 4.624 | contrastive_loss 0.258 | total 4003.4 | n_correct 2438.2 | ppl 7.57 | accuracy 60.903 | uer 18.658 | wer 20.491 | raw_wer 20.491 | bleu 19.22 | wps 1965.2 | wpb 4003.4 | bsz 141.8 | num_updates 14000 | best_bleu 19.22
2023-08-06 14:22:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14000 updates
2023-08-06 14:22:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_10_14000.pt
2023-08-06 14:22:20 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_10_14000.pt
2023-08-06 14:22:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_10_14000.pt (epoch 10 @ 14000 updates, score 19.22) (writing took 25.301929526031017 seconds)
2023-08-06 14:24:15 | INFO | train_inner | epoch 010:    842 / 1474 loss=1.992, trans_loss=3.48, nll_loss=1.634, w2v_ctc_loss=1.003, task_loss=0.924, contrastive_loss=0.092, total=4128.44, n_correct=2515.83, ppl=3.1, accuracy=60.939, wps=8538.9, ups=0.69, wpb=12327.8, bsz=456.3, num_updates=14100, lr=0.000119098, gnorm=0.398, clip=0, loss_scale=16, train_wall=92, gb_free=14.6, wall=13280
2023-08-06 14:25:48 | INFO | train_inner | epoch 010:    942 / 1474 loss=2.016, trans_loss=3.483, nll_loss=1.635, w2v_ctc_loss=1.013, task_loss=0.896, contrastive_loss=0.13, total=4160.94, n_correct=2533.89, ppl=3.11, accuracy=60.897, wps=13436, ups=1.08, wpb=12411.1, bsz=468.1, num_updates=14200, lr=0.000118678, gnorm=0.403, clip=0, loss_scale=16, train_wall=92, gb_free=15.3, wall=13372
2023-08-06 14:27:20 | INFO | train_inner | epoch 010:   1042 / 1474 loss=2.008, trans_loss=3.487, nll_loss=1.643, w2v_ctc_loss=1.018, task_loss=1.008, contrastive_loss=0.104, total=4067.53, n_correct=2462.92, ppl=3.12, accuracy=60.551, wps=13151.7, ups=1.08, wpb=12145, bsz=434.3, num_updates=14300, lr=0.000118262, gnorm=0.407, clip=0, loss_scale=16, train_wall=92, gb_free=16.8, wall=13464
2023-08-06 14:28:53 | INFO | train_inner | epoch 010:   1142 / 1474 loss=2.014, trans_loss=3.493, nll_loss=1.651, w2v_ctc_loss=1.031, task_loss=1.042, contrastive_loss=0.088, total=4044.03, n_correct=2443.15, ppl=3.14, accuracy=60.414, wps=13029.6, ups=1.08, wpb=12074.4, bsz=422.3, num_updates=14400, lr=0.000117851, gnorm=0.408, clip=0, loss_scale=16, train_wall=92, gb_free=17.2, wall=13557
2023-08-06 14:30:25 | INFO | train_inner | epoch 010:   1242 / 1474 loss=2.005, trans_loss=3.482, nll_loss=1.642, w2v_ctc_loss=1.024, task_loss=0.956, contrastive_loss=0.085, total=4110.41, n_correct=2492.75, ppl=3.12, accuracy=60.645, wps=13335.4, ups=1.08, wpb=12291.6, bsz=445.2, num_updates=14500, lr=0.000117444, gnorm=0.404, clip=0, loss_scale=16, train_wall=92, gb_free=16.3, wall=13649
2023-08-06 14:31:59 | INFO | train_inner | epoch 010:   1342 / 1474 loss=2.005, trans_loss=3.489, nll_loss=1.648, w2v_ctc_loss=1.018, task_loss=0.952, contrastive_loss=0.095, total=4121.38, n_correct=2501.9, ppl=3.13, accuracy=60.705, wps=13135.6, ups=1.07, wpb=12308.4, bsz=451, num_updates=14600, lr=0.000117041, gnorm=0.403, clip=0, loss_scale=32, train_wall=93, gb_free=13.9, wall=13743
2023-08-06 14:33:32 | INFO | train_inner | epoch 010:   1442 / 1474 loss=2.087, trans_loss=3.495, nll_loss=1.653, w2v_ctc_loss=1.005, task_loss=0.88, contrastive_loss=0.343, total=4192.39, n_correct=2537.34, ppl=3.14, accuracy=60.523, wps=13433.4, ups=1.07, wpb=12506.1, bsz=482, num_updates=14700, lr=0.000116642, gnorm=0.414, clip=0, loss_scale=32, train_wall=93, gb_free=16.9, wall=13836
2023-08-06 14:34:01 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 14:34:25 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.258 | trans_loss 5.614 | nll_loss 2.904 | w2v_ctc_loss 1.318 | task_loss 4.605 | contrastive_loss 0.262 | total 4003.4 | n_correct 2437.9 | ppl 7.48 | accuracy 60.896 | uer 17.97 | wer 19.839 | raw_wer 19.839 | bleu 19.23 | wps 2115.2 | wpb 4003.4 | bsz 141.8 | num_updates 14732 | best_bleu 19.23
2023-08-06 14:34:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14732 updates
2023-08-06 14:34:25 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 14:34:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 14:34:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt (epoch 10 @ 14732 updates, score 19.23) (writing took 29.123216772451997 seconds)
2023-08-06 14:34:55 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-08-06 14:34:55 | INFO | train | epoch 010 | loss 2.019 | trans_loss 3.482 | nll_loss 1.636 | w2v_ctc_loss 1.011 | task_loss 0.933 | contrastive_loss 0.161 | total 4138.65 | n_correct 2516.05 | ppl 3.11 | accuracy 60.794 | wps 12284.1 | ups 0.99 | wpb 12355.8 | bsz 458.5 | num_updates 14732 | lr 0.000116516 | gnorm 0.403 | clip 0 | loss_scale 32 | train_wall 1363 | gb_free 17.2 | wall 13919
2023-08-06 14:34:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 14:34:55 | INFO | fairseq.trainer | begin training epoch 11
2023-08-06 14:34:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 14:36:06 | INFO | train_inner | epoch 011:     68 / 1474 loss=1.986, trans_loss=3.457, nll_loss=1.605, w2v_ctc_loss=0.981, task_loss=0.864, contrastive_loss=0.168, total=4175.24, n_correct=2572.08, ppl=3.04, accuracy=61.603, wps=8108.2, ups=0.65, wpb=12463.5, bsz=478.8, num_updates=14800, lr=0.000116248, gnorm=0.392, clip=0, loss_scale=32, train_wall=91, gb_free=16.6, wall=13990
2023-08-06 14:37:38 | INFO | train_inner | epoch 011:    168 / 1474 loss=1.968, trans_loss=3.458, nll_loss=1.607, w2v_ctc_loss=0.988, task_loss=0.962, contrastive_loss=0.088, total=4087.78, n_correct=2515.56, ppl=3.05, accuracy=61.539, wps=13272.8, ups=1.09, wpb=12214.2, bsz=445.9, num_updates=14900, lr=0.000115857, gnorm=0.401, clip=0, loss_scale=32, train_wall=92, gb_free=16.2, wall=14082
2023-08-06 14:39:10 | INFO | train_inner | epoch 011:    268 / 1474 loss=1.958, trans_loss=3.457, nll_loss=1.605, w2v_ctc_loss=0.979, task_loss=0.963, contrastive_loss=0.084, total=4118.77, n_correct=2539.95, ppl=3.04, accuracy=61.668, wps=13269.4, ups=1.08, wpb=12299.1, bsz=446.5, num_updates=15000, lr=0.00011547, gnorm=0.394, clip=0, loss_scale=32, train_wall=92, gb_free=12.1, wall=14174
Mixup rate:0.5, token after shrink shape:torch.Size([8, 98]), X shape:torch.Size([8, 98, 512])
CTC Tokens:tensor([ 0,  0, 86, 86,  0], device='cuda:0'), Shrink Mask:tensor([ True, False,  True, False,  True], device='cuda:0'), New Tokens:tensor([  0,  86,   0, 116,   0], device='cuda:0')
Mixup Sent Mask:tensor([[2],
        [7]], device='cuda:0'), 2,  Mixup Mask:tensor([False,  True, False, False, False], device='cuda:0'), 
                    Org X:tensor([[-0.1859, -1.3486,  0.7246,  ..., -1.4238,  0.0863,  0.2399],
        [ 0.2891,  0.8101, -0.7598,  ...,  0.0928, -1.5742,  0.4199],
        [ 0.4900,  1.1133, -0.4910,  ..., -0.9272, -0.0061,  0.4070],
        [ 0.7441,  0.9912, -0.6812,  ...,  0.5078,  0.0592,  1.4316],
        [ 1.0469,  0.7080,  0.8149,  ...,  0.5342,  0.4663,  0.5317]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.1859, -1.3486,  0.7246,  ..., -1.4238,  0.0863,  0.2399],
        [-0.7368, -0.0795, -0.1138,  ...,  0.6924, -1.3311, -0.3035],
        [ 0.4900,  1.1133, -0.4910,  ..., -0.9272, -0.0061,  0.4070],
        [ 0.7441,  0.9912, -0.6812,  ...,  0.5078,  0.0592,  1.4316],
        [ 1.0469,  0.7080,  0.8149,  ...,  0.5342,  0.4663,  0.5317]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [-0.7368, -0.0795, -0.1138,  ...,  0.6924, -1.3311, -0.3035],
        [-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [ 0.1644, -0.3569,  0.3127,  ...,  2.7109, -1.2090, -1.1055],
        [-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:0')
2023-08-06 14:40:19 | INFO | train_inner | epoch 011:    368 / 1474 loss=2.102, trans_loss=5.139, nll_loss=2.39, w2v_ctc_loss=0.735, task_loss=1.427, contrastive_loss=0.068, total=4097.83, n_correct=2519.31, ppl=5.24, accuracy=61.479, wps=11958.2, ups=1.45, wpb=8240.8, bsz=298, num_updates=15100, lr=0.000115087, gnorm=0.517, clip=0, loss_scale=32, train_wall=68, gb_free=15.4, wall=14243
2023-08-06 14:41:29 | INFO | train_inner | epoch 011:    468 / 1474 loss=2.126, trans_loss=5.178, nll_loss=2.417, w2v_ctc_loss=0.73, task_loss=1.464, contrastive_loss=0.184, total=4110.64, n_correct=2513.22, ppl=5.34, accuracy=61.139, wps=11716, ups=1.43, wpb=8221.3, bsz=300.4, num_updates=15200, lr=0.000114708, gnorm=0.523, clip=0, loss_scale=32, train_wall=70, gb_free=16.2, wall=14314
2023-08-06 14:42:39 | INFO | train_inner | epoch 011:    568 / 1474 loss=2.127, trans_loss=5.176, nll_loss=2.416, w2v_ctc_loss=0.744, task_loss=1.501, contrastive_loss=0.184, total=4071.69, n_correct=2491.38, ppl=5.34, accuracy=61.188, wps=11725.6, ups=1.44, wpb=8143.4, bsz=293.7, num_updates=15300, lr=0.000114332, gnorm=0.535, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=14383
2023-08-06 14:43:49 | INFO | train_inner | epoch 011:    668 / 1474 loss=2.134, trans_loss=5.178, nll_loss=2.418, w2v_ctc_loss=0.737, task_loss=1.372, contrastive_loss=0.238, total=4157.2, n_correct=2541.7, ppl=5.35, accuracy=61.14, wps=11937.8, ups=1.44, wpb=8314.4, bsz=309.6, num_updates=15400, lr=0.000113961, gnorm=0.528, clip=0, loss_scale=32, train_wall=69, gb_free=16.6, wall=14453
2023-08-06 14:44:58 | INFO | train_inner | epoch 011:    768 / 1474 loss=2.117, trans_loss=5.187, nll_loss=2.431, w2v_ctc_loss=0.75, task_loss=1.409, contrastive_loss=0.067, total=4174.91, n_correct=2550.28, ppl=5.39, accuracy=61.086, wps=11971, ups=1.43, wpb=8349.8, bsz=306.9, num_updates=15500, lr=0.000113592, gnorm=0.528, clip=0, loss_scale=32, train_wall=69, gb_free=16.9, wall=14522
2023-08-06 14:46:08 | INFO | train_inner | epoch 011:    868 / 1474 loss=2.117, trans_loss=5.189, nll_loss=2.433, w2v_ctc_loss=0.745, task_loss=1.465, contrastive_loss=0.057, total=4118.44, n_correct=2511.14, ppl=5.4, accuracy=60.973, wps=11880.4, ups=1.44, wpb=8236.9, bsz=293.7, num_updates=15600, lr=0.000113228, gnorm=0.521, clip=0, loss_scale=32, train_wall=69, gb_free=10.5, wall=14592
2023-08-06 14:47:17 | INFO | train_inner | epoch 011:    968 / 1474 loss=2.116, trans_loss=5.186, nll_loss=2.429, w2v_ctc_loss=0.748, task_loss=1.432, contrastive_loss=0.068, total=4140.92, n_correct=2528.63, ppl=5.38, accuracy=61.064, wps=11919.1, ups=1.44, wpb=8281.8, bsz=301.9, num_updates=15700, lr=0.000112867, gnorm=0.524, clip=0, loss_scale=32, train_wall=69, gb_free=15.5, wall=14661
2023-08-06 14:48:26 | INFO | train_inner | epoch 011:   1068 / 1474 loss=2.115, trans_loss=5.181, nll_loss=2.424, w2v_ctc_loss=0.745, task_loss=1.374, contrastive_loss=0.086, total=4136.99, n_correct=2532.69, ppl=5.37, accuracy=61.221, wps=11990, ups=1.45, wpb=8274, bsz=308.8, num_updates=15800, lr=0.000112509, gnorm=0.519, clip=0, loss_scale=32, train_wall=69, gb_free=17.5, wall=14730
2023-08-06 14:49:36 | INFO | train_inner | epoch 011:   1168 / 1474 loss=2.114, trans_loss=5.187, nll_loss=2.432, w2v_ctc_loss=0.745, task_loss=1.393, contrastive_loss=0.073, total=4185.65, n_correct=2557.14, ppl=5.4, accuracy=61.093, wps=11923, ups=1.42, wpb=8371.3, bsz=309.8, num_updates=15900, lr=0.000112154, gnorm=0.521, clip=0, loss_scale=32, train_wall=70, gb_free=13.8, wall=14800
2023-08-06 14:50:45 | INFO | train_inner | epoch 011:   1268 / 1474 loss=2.123, trans_loss=5.181, nll_loss=2.425, w2v_ctc_loss=0.745, task_loss=1.343, contrastive_loss=0.138, total=4171.89, n_correct=2553.65, ppl=5.37, accuracy=61.211, wps=12075.8, ups=1.45, wpb=8343.8, bsz=314.1, num_updates=16000, lr=0.000111803, gnorm=0.529, clip=0, loss_scale=32, train_wall=69, gb_free=15.7, wall=14870
2023-08-06 14:50:45 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([24, 55]), X shape:torch.Size([24, 55, 512])
CTC Tokens:tensor([0, 0, 0, 0, 0], device='cuda:4'), Shrink Mask:tensor([ True, False, False, False, False], device='cuda:4'), New Tokens:tensor([  0,  46,   0, 185,  12], device='cuda:4')
Mixup Sent Mask:tensor([[ 2],
        [ 7],
        [16],
        [21]], device='cuda:4'), 2,  Mixup Mask:tensor([ True, False,  True,  True, False], device='cuda:4'), 
                    Org X:tensor([[ 0.0894, -0.7109,  1.1338,  ..., -0.6167,  0.0630, -0.7520],
        [ 0.2063,  0.2299,  1.3877,  ..., -0.2059, -0.1768,  0.6592],
        [ 0.5879,  0.5386,  1.3428,  ...,  0.1388, -0.5562,  0.5327],
        [-0.0178,  1.5791, -0.0496,  ...,  0.6538,  0.3843,  0.7012],
        [ 0.9521,  0.9219,  0.4329,  ...,  0.7837, -0.0229,  0.7319]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [ 0.2063,  0.2299,  1.3877,  ..., -0.2059, -0.1768,  0.6592],
        [-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [ 0.1382,  0.0770,  0.3086,  ...,  1.6045, -1.5869, -1.9941],
        [ 0.9521,  0.9219,  0.4329,  ...,  0.7837, -0.0229,  0.7319]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [-0.4060, -0.5845, -0.2717,  ...,  0.4644, -2.5547,  0.0600],
        [-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [ 0.1382,  0.0770,  0.3086,  ...,  1.6045, -1.5869, -1.9941],
        [-0.1797, -0.0512,  0.1132,  ...,  0.8589, -3.5586, -0.6436]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 65]), X shape:torch.Size([16, 65, 512])
CTC Tokens:tensor([  55,  511, 9474,    0, 1303], device='cuda:5'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:5'), New Tokens:tensor([  55,  511, 9474,    0, 1303], device='cuda:5')
Mixup Sent Mask:tensor([[2],
        [7]], device='cuda:5'), 2,  Mixup Mask:tensor([ True,  True,  True, False,  True], device='cuda:5'), 
                    Org X:tensor([[ 0.4739,  0.1898, -0.5200,  ...,  0.5400,  0.7876,  0.2045],
        [-0.7168,  0.3943, -0.4836,  ...,  0.4031,  0.5132, -0.8062],
        [-0.3342,  0.3088,  0.2510,  ...,  0.6411,  0.3901, -0.1896],
        [ 0.5015,  0.2808,  0.6826,  ..., -0.6362,  0.6025, -0.3733],
        [ 0.4111,  0.1162, -1.1602,  ..., -1.4102,  0.8955, -0.7485]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.1484,  0.1056, -0.0237,  ..., -0.0686, -3.7480, -2.4629],
        [ 0.6460,  1.3652, -0.6646,  ...,  0.4851, -5.5234, -2.7852],
        [-0.7764,  1.6543, -0.7480,  ...,  2.3223, -0.1875, -0.3362],
        [ 0.5015,  0.2808,  0.6826,  ..., -0.6362,  0.6025, -0.3733],
        [ 0.3601, -0.3508,  0.1077,  ..., -0.5137,  0.2208, -0.6748]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[ 0.1484,  0.1056, -0.0237,  ..., -0.0686, -3.7480, -2.4629],
        [ 0.6460,  1.3652, -0.6646,  ...,  0.4851, -5.5234, -2.7852],
        [-0.7764,  1.6543, -0.7480,  ...,  2.3223, -0.1875, -0.3362],
        [-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [ 0.3601, -0.3508,  0.1077,  ..., -0.5137,  0.2208, -0.6748]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([72, 24]), X shape:torch.Size([72, 24, 512])
CTC Tokens:tensor([ 21,   0, 384,   0, 587], device='cuda:1'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:1'), New Tokens:tensor([ 21,   0, 384,   0, 587], device='cuda:1')
Mixup Sent Mask:tensor([[ 2],
        [ 7],
        [16],
        [21],
        [24],
        [25],
        [26],
        [30],
        [34],
        [36],
        [39],
        [45],
        [47],
        [52],
        [56],
        [57],
        [59],
        [64],
        [67],
        [69],
        [70],
        [71]], device='cuda:1'), 2,  Mixup Mask:tensor([ True,  True,  True, False,  True], device='cuda:1'), 
                    Org X:tensor([[ 0.2852, -1.6699, -2.8926,  ..., -1.2764,  0.8408, -1.7090],
        [ 0.3103, -0.6514, -0.7734,  ..., -0.4827,  1.6406, -1.1768],
        [-0.5859, -0.0212,  0.1646,  ..., -0.0467,  0.8550, -1.8105],
        [ 0.8164, -0.3887,  0.9673,  ..., -0.6064, -0.7222, -1.9570],
        [ 0.2700, -0.0505,  1.2588,  ...,  1.4102, -1.6279, -1.3457]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.5454, -0.3044, -0.3916,  ..., -0.9531, -3.6738, -2.1484],
        [-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [-0.7017, -0.5898,  0.1317,  ...,  1.0059,  2.9531, -0.0917],
        [ 0.8164, -0.3887,  0.9673,  ..., -0.6064, -0.7222, -1.9570],
        [-0.9087, -0.0349,  1.4795,  ...,  4.7969, -3.1152, -1.6211]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5454, -0.3044, -0.3916,  ..., -0.9531, -3.6738, -2.1484],
        [-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [-0.7017, -0.5898,  0.1317,  ...,  1.0059,  2.9531, -0.0917],
        [-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [-0.9087, -0.0349,  1.4795,  ...,  4.7969, -3.1152, -1.6211]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([24, 45]), X shape:torch.Size([24, 45, 512])
CTC Tokens:tensor([ 0, 70,  0, 24, 24], device='cuda:6'), Shrink Mask:tensor([ True,  True,  True,  True, False], device='cuda:6'), New Tokens:tensor([ 0, 70,  0, 24,  0], device='cuda:6')
Mixup Sent Mask:tensor([[ 2],
        [ 7],
        [16],
        [21]], device='cuda:6'), 2,  Mixup Mask:tensor([False, False, False, False, False], device='cuda:6'), 
                    Org X:tensor([[-1.0508, -1.3262,  0.8789,  ..., -1.3906, -0.2061, -0.3079],
        [-0.7559,  0.0571,  0.2974,  ..., -0.0589, -0.8413, -0.3420],
        [-1.5762,  0.0831,  1.5029,  ..., -1.0820,  0.5254, -1.7344],
        [-2.9863,  0.2263,  1.0479,  ..., -0.0695,  0.8486, -1.3496],
        [ 0.8750,  0.1428,  1.9062,  ...,  0.0373,  0.6665, -0.1947]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-1.0508, -1.3262,  0.8789,  ..., -1.3906, -0.2061, -0.3079],
        [-0.7559,  0.0571,  0.2974,  ..., -0.0589, -0.8413, -0.3420],
        [-1.5762,  0.0831,  1.5029,  ..., -1.0820,  0.5254, -1.7344],
        [-2.9863,  0.2263,  1.0479,  ..., -0.0695,  0.8486, -1.3496],
        [ 0.8750,  0.1428,  1.9062,  ...,  0.0373,  0.6665, -0.1947]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [-0.3516, -0.5454, -0.2377,  ...,  0.3079, -1.9082, -3.0332],
        [-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [-0.7236, -0.4719,  0.8105,  ..., -4.5820,  1.4844, -1.5479],
        [-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([24, 55]), X shape:torch.Size([24, 55, 512])
CTC Tokens:tensor([ 0,  7, 91,  9,  9], device='cuda:2'), Shrink Mask:tensor([ True,  True,  True,  True, False], device='cuda:2'), New Tokens:tensor([ 0,  7, 91,  9,  7], device='cuda:2')
Mixup Sent Mask:tensor([[ 2],
        [ 7],
        [16],
        [21]], device='cuda:2'), 2,  Mixup Mask:tensor([ True, False,  True,  True, False], device='cuda:2'), 
                    Org X:tensor([[ 0.0369, -0.4661, -0.7495,  ...,  0.4951, -0.5767, -2.1055],
        [ 0.1964, -0.2668, -1.0820,  ...,  0.1299, -0.6367, -0.6616],
        [-0.3711, -0.2952, -0.4336,  ...,  0.2397,  0.1295, -0.5117],
        [ 1.3682, -1.0674, -1.5869,  ...,  0.2236, -0.4785, -1.1543],
        [ 0.4312, -0.2023, -2.6016,  ...,  0.2219,  0.5234, -0.3245]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [ 0.1964, -0.2668, -1.0820,  ...,  0.1299, -0.6367, -0.6616],
        [-0.3486, -0.1069, -0.0699,  ..., -0.5283,  0.4275, -1.6475],
        [-0.4619, -0.3328, -0.3735,  ...,  0.3958, -1.3330,  1.6729],
        [ 0.4312, -0.2023, -2.6016,  ...,  0.2219,  0.5234, -0.3245]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [-0.5361, -0.2576, -0.2112,  ..., -0.2451, -1.8145, -1.5010],
        [-0.3486, -0.1069, -0.0699,  ..., -0.5283,  0.4275, -1.6475],
        [-0.4619, -0.3328, -0.3735,  ...,  0.3958, -1.3330,  1.6729],
        [-0.5361, -0.2576, -0.2112,  ..., -0.2451, -1.8145, -1.5010]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([6, 182]), X shape:torch.Size([6, 182, 512])
CTC Tokens:tensor([ 8,  8,  0, 70, 24], device='cuda:7'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:7'), New Tokens:tensor([ 8,  0, 70, 24, 11], device='cuda:7')
Mixup Sent Mask:tensor([[2]], device='cuda:7'), 2,  Mixup Mask:tensor([ True,  True,  True,  True, False], device='cuda:7'), 
                    Org X:tensor([[ 0.5449, -0.6689,  0.2155,  ...,  0.1669,  1.3936, -0.8604],
        [-1.1836,  0.2847,  0.4907,  ..., -0.5562, -1.2656, -0.3789],
        [-2.1426,  0.3604, -0.4758,  ...,  0.2209, -0.7593, -0.6255],
        [-1.9746,  0.2786, -0.2634,  ..., -1.7637,  0.5327, -2.3672],
        [ 0.6396,  0.6401, -1.1836,  ..., -1.3457,  0.2208, -0.7344]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.4517, -0.1854, -0.7285,  ...,  1.0078, -1.8877, -1.6943],
        [-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [-0.3516, -0.5454, -0.2377,  ...,  0.3079, -1.9082, -3.0332],
        [-0.7236, -0.4719,  0.8105,  ..., -4.5820,  1.4844, -1.5479],
        [ 0.6396,  0.6401, -1.1836,  ..., -1.3457,  0.2208, -0.7344]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4517, -0.1854, -0.7285,  ...,  1.0078, -1.8877, -1.6943],
        [-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [-0.3516, -0.5454, -0.2377,  ...,  0.3079, -1.9082, -3.0332],
        [-0.7236, -0.4719,  0.8105,  ..., -4.5820,  1.4844, -1.5479],
        [-0.5273, -0.5347, -0.3362,  ..., -2.3457, -0.4475, -4.0352]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 68]), X shape:torch.Size([16, 68, 512])
CTC Tokens:tensor([  0,   7,   0, 211,   0], device='cuda:3'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:3'), New Tokens:tensor([  0,   7,   0, 211,   0], device='cuda:3')
Mixup Sent Mask:tensor([[2],
        [7]], device='cuda:3'), 2,  Mixup Mask:tensor([ True,  True,  True, False, False], device='cuda:3'), 
                    Org X:tensor([[ 0.3394, -1.2041,  0.2242,  ..., -1.8770, -0.3040, -1.0361],
        [ 0.6484,  0.0770, -0.0848,  ..., -0.0209, -0.2708, -0.7715],
        [ 0.9668,  0.1249,  2.2930,  ..., -0.6558, -0.2825, -0.4937],
        [ 0.3093, -0.0914,  0.7373,  ...,  0.2524,  0.4309, -1.2422],
        [ 0.5332, -0.6108,  1.6172,  ..., -0.3872, -0.0113, -1.0557]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [-0.5361, -0.2576, -0.2112,  ..., -0.2451, -1.8145, -1.5010],
        [-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [ 0.3093, -0.0914,  0.7373,  ...,  0.2524,  0.4309, -1.2422],
        [ 0.5332, -0.6108,  1.6172,  ..., -0.3872, -0.0113, -1.0557]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [-0.5361, -0.2576, -0.2112,  ..., -0.2451, -1.8145, -1.5010],
        [-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531],
        [-0.0569, -0.0963,  0.2661,  ...,  0.9771, -1.1758, -2.5762],
        [-0.3599, -1.0879, -0.0182,  ..., -2.4824, -1.8848, -1.9531]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:3')
2023-08-06 14:51:08 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.261 | trans_loss 5.616 | nll_loss 2.907 | w2v_ctc_loss 1.336 | task_loss 4.623 | contrastive_loss 0.245 | total 4003.4 | n_correct 2447.4 | ppl 7.5 | accuracy 61.133 | uer 17.92 | wer 19.757 | raw_wer 19.757 | bleu 19.25 | wps 2313.2 | wpb 4003.4 | bsz 141.8 | num_updates 16000 | best_bleu 19.25
2023-08-06 14:51:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16000 updates
2023-08-06 14:51:08 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_11_16000.pt
2023-08-06 14:51:12 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_11_16000.pt
2023-08-06 14:51:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_11_16000.pt (epoch 11 @ 16000 updates, score 19.25) (writing took 25.202054178342223 seconds)
2023-08-06 14:52:45 | INFO | train_inner | epoch 011:   1368 / 1474 loss=2.139, trans_loss=5.183, nll_loss=2.429, w2v_ctc_loss=0.734, task_loss=1.293, contrastive_loss=0.301, total=4190.34, n_correct=2561.4, ppl=5.38, accuracy=61.126, wps=7026.9, ups=0.84, wpb=8380.7, bsz=327.9, num_updates=16100, lr=0.000111456, gnorm=0.526, clip=0, loss_scale=32, train_wall=70, gb_free=16.9, wall=14989
2023-08-06 14:53:54 | INFO | train_inner | epoch 011:   1468 / 1474 loss=2.112, trans_loss=5.186, nll_loss=2.432, w2v_ctc_loss=0.74, task_loss=1.355, contrastive_loss=0.076, total=4158.39, n_correct=2540.33, ppl=5.4, accuracy=61.089, wps=12031.5, ups=1.45, wpb=8316.8, bsz=312, num_updates=16200, lr=0.000111111, gnorm=0.519, clip=0, loss_scale=32, train_wall=69, gb_free=16.7, wall=15058
2023-08-06 14:53:58 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 14:54:21 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.246 | trans_loss 5.605 | nll_loss 2.894 | w2v_ctc_loss 1.301 | task_loss 4.612 | contrastive_loss 0.25 | total 4003.4 | n_correct 2440.3 | ppl 7.43 | accuracy 60.956 | uer 18.146 | wer 19.973 | raw_wer 19.973 | bleu 19.17 | wps 2206.9 | wpb 4003.4 | bsz 141.8 | num_updates 16206 | best_bleu 19.25
2023-08-06 14:54:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16206 updates
2023-08-06 14:54:21 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.1707.pt
2023-08-06 14:54:24 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.1707.pt
2023-08-06 14:54:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.1707.pt (epoch 11 @ 16206 updates, score 19.17) (writing took 18.355786431580782 seconds)
2023-08-06 14:54:40 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-08-06 14:54:40 | INFO | train | epoch 011 | loss 2.081 | trans_loss 4.751 | nll_loss 2.219 | w2v_ctc_loss 0.801 | task_loss 1.284 | contrastive_loss 0.118 | total 4138.65 | n_correct 2534.66 | ppl 4.66 | accuracy 61.244 | wps 11217.7 | ups 1.24 | wpb 9021 | bsz 333.4 | num_updates 16206 | lr 0.000111091 | gnorm 0.501 | clip 0 | loss_scale 32 | train_wall 1079 | gb_free 17.1 | wall 15104
2023-08-06 14:54:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 14:54:40 | INFO | fairseq.trainer | begin training epoch 12
2023-08-06 14:54:40 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 14:55:54 | INFO | train_inner | epoch 012:     94 / 1474 loss=2.088, trans_loss=5.127, nll_loss=2.353, w2v_ctc_loss=0.724, task_loss=1.338, contrastive_loss=0.108, total=4146.82, n_correct=2583.78, ppl=5.11, accuracy=62.308, wps=6885.8, ups=0.83, wpb=8293.6, bsz=313.9, num_updates=16300, lr=0.00011077, gnorm=0.519, clip=0, loss_scale=32, train_wall=69, gb_free=15.6, wall=15178
2023-08-06 14:57:04 | INFO | train_inner | epoch 012:    194 / 1474 loss=2.092, trans_loss=5.139, nll_loss=2.368, w2v_ctc_loss=0.732, task_loss=1.446, contrastive_loss=0.061, total=4120.68, n_correct=2552.51, ppl=5.16, accuracy=61.944, wps=11878.1, ups=1.44, wpb=8241.4, bsz=294.7, num_updates=16400, lr=0.000110432, gnorm=0.523, clip=0, loss_scale=32, train_wall=69, gb_free=15.5, wall=15248
2023-08-06 14:58:14 | INFO | train_inner | epoch 012:    294 / 1474 loss=2.087, trans_loss=5.138, nll_loss=2.368, w2v_ctc_loss=0.718, task_loss=1.318, contrastive_loss=0.088, total=4199.46, n_correct=2605.15, ppl=5.16, accuracy=62.035, wps=11927.4, ups=1.42, wpb=8398.9, bsz=320.3, num_updates=16500, lr=0.000110096, gnorm=0.517, clip=0, loss_scale=32, train_wall=70, gb_free=16.4, wall=15318
2023-08-06 14:59:24 | INFO | train_inner | epoch 012:    394 / 1474 loss=2.093, trans_loss=5.148, nll_loss=2.38, w2v_ctc_loss=0.729, task_loss=1.37, contrastive_loss=0.074, total=4151.14, n_correct=2567.86, ppl=5.21, accuracy=61.859, wps=11871.8, ups=1.43, wpb=8302.3, bsz=307.8, num_updates=16600, lr=0.000109764, gnorm=0.521, clip=0, loss_scale=32, train_wall=70, gb_free=17, wall=15388
2023-08-06 15:00:34 | INFO | train_inner | epoch 012:    494 / 1474 loss=2.103, trans_loss=5.161, nll_loss=2.398, w2v_ctc_loss=0.736, task_loss=1.405, contrastive_loss=0.08, total=4110.49, n_correct=2535.16, ppl=5.27, accuracy=61.675, wps=11742.2, ups=1.43, wpb=8221, bsz=302.2, num_updates=16700, lr=0.000109435, gnorm=0.522, clip=0, loss_scale=64, train_wall=70, gb_free=13.6, wall=15458
2023-08-06 15:01:45 | INFO | train_inner | epoch 012:    594 / 1474 loss=2.103, trans_loss=5.151, nll_loss=2.385, w2v_ctc_loss=0.729, task_loss=1.343, contrastive_loss=0.144, total=4189.92, n_correct=2590.85, ppl=5.22, accuracy=61.835, wps=11872.4, ups=1.42, wpb=8379.8, bsz=315.1, num_updates=16800, lr=0.000109109, gnorm=0.526, clip=0, loss_scale=64, train_wall=70, gb_free=14.7, wall=15529
2023-08-06 15:01:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-06 15:02:55 | INFO | train_inner | epoch 012:    695 / 1474 loss=2.09, trans_loss=5.15, nll_loss=2.384, w2v_ctc_loss=0.722, task_loss=1.337, contrastive_loss=0.076, total=4176.04, n_correct=2585.69, ppl=5.22, accuracy=61.917, wps=11803.8, ups=1.41, wpb=8352.1, bsz=313.5, num_updates=16900, lr=0.000108786, gnorm=0.518, clip=0, loss_scale=32, train_wall=70, gb_free=17.3, wall=15600
2023-08-06 15:04:05 | INFO | train_inner | epoch 012:    795 / 1474 loss=2.094, trans_loss=5.148, nll_loss=2.381, w2v_ctc_loss=0.733, task_loss=1.416, contrastive_loss=0.071, total=4095.72, n_correct=2536.76, ppl=5.21, accuracy=61.937, wps=11748.4, ups=1.43, wpb=8191.4, bsz=298.9, num_updates=17000, lr=0.000108465, gnorm=0.527, clip=0, loss_scale=32, train_wall=69, gb_free=17.1, wall=15669
2023-08-06 15:05:16 | INFO | train_inner | epoch 012:    895 / 1474 loss=2.106, trans_loss=5.156, nll_loss=2.392, w2v_ctc_loss=0.733, task_loss=1.437, contrastive_loss=0.121, total=4162.82, n_correct=2569.35, ppl=5.25, accuracy=61.721, wps=11745.8, ups=1.41, wpb=8325.6, bsz=305.4, num_updates=17100, lr=0.000108148, gnorm=0.526, clip=0, loss_scale=32, train_wall=70, gb_free=16.1, wall=15740
2023-08-06 15:06:26 | INFO | train_inner | epoch 012:    995 / 1474 loss=2.109, trans_loss=5.167, nll_loss=2.407, w2v_ctc_loss=0.737, task_loss=1.425, contrastive_loss=0.131, total=4117.63, n_correct=2534.35, ppl=5.3, accuracy=61.549, wps=11814.9, ups=1.43, wpb=8235.3, bsz=301.6, num_updates=17200, lr=0.000107833, gnorm=0.538, clip=0, loss_scale=32, train_wall=69, gb_free=16.7, wall=15810
2023-08-06 15:07:36 | INFO | train_inner | epoch 012:   1095 / 1474 loss=2.122, trans_loss=5.17, nll_loss=2.41, w2v_ctc_loss=0.739, task_loss=1.469, contrastive_loss=0.177, total=4046.48, n_correct=2488.11, ppl=5.32, accuracy=61.488, wps=11570.5, ups=1.43, wpb=8093, bsz=289.6, num_updates=17300, lr=0.000107521, gnorm=0.537, clip=0, loss_scale=32, train_wall=70, gb_free=15.8, wall=15880
2023-08-06 15:08:46 | INFO | train_inner | epoch 012:   1195 / 1474 loss=2.121, trans_loss=5.179, nll_loss=2.423, w2v_ctc_loss=0.746, task_loss=1.366, contrastive_loss=0.144, total=4201.13, n_correct=2573.63, ppl=5.36, accuracy=61.26, wps=11925.1, ups=1.42, wpb=8402.3, bsz=319.2, num_updates=17400, lr=0.000107211, gnorm=0.53, clip=0, loss_scale=32, train_wall=70, gb_free=17.1, wall=15950
2023-08-06 15:09:56 | INFO | train_inner | epoch 012:   1295 / 1474 loss=2.11, trans_loss=5.17, nll_loss=2.411, w2v_ctc_loss=0.751, task_loss=1.555, contrastive_loss=0.06, total=4070.27, n_correct=2502.42, ppl=5.32, accuracy=61.48, wps=11637.5, ups=1.43, wpb=8140.5, bsz=286.1, num_updates=17500, lr=0.000106904, gnorm=0.528, clip=0, loss_scale=32, train_wall=70, gb_free=15.8, wall=16020
2023-08-06 15:11:06 | INFO | train_inner | epoch 012:   1395 / 1474 loss=2.109, trans_loss=5.17, nll_loss=2.412, w2v_ctc_loss=0.725, task_loss=1.408, contrastive_loss=0.161, total=4139.63, n_correct=2547.59, ppl=5.32, accuracy=61.541, wps=11847.3, ups=1.43, wpb=8279.3, bsz=305.8, num_updates=17600, lr=0.0001066, gnorm=0.522, clip=0, loss_scale=32, train_wall=69, gb_free=16.9, wall=16090
2023-08-06 15:12:01 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 15:12:26 | INFO | dev_st | epoch 012 | valid on 'dev_st' subset | loss 4.255 | trans_loss 5.594 | nll_loss 2.879 | w2v_ctc_loss 1.356 | task_loss 4.58 | contrastive_loss 0.254 | total 4003.4 | n_correct 2456.3 | ppl 7.36 | accuracy 61.355 | uer 18.177 | wer 19.779 | raw_wer 19.779 | bleu 19.61 | wps 2114.2 | wpb 4003.4 | bsz 141.8 | num_updates 17679 | best_bleu 19.61
2023-08-06 15:12:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 17679 updates
2023-08-06 15:12:26 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 15:12:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 15:12:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt (epoch 12 @ 17679 updates, score 19.61) (writing took 24.440685784444213 seconds)
2023-08-06 15:12:50 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-08-06 15:12:50 | INFO | train | epoch 012 | loss 2.102 | trans_loss 5.156 | nll_loss 2.392 | w2v_ctc_loss 0.733 | task_loss 1.403 | contrastive_loss 0.105 | total 4136.79 | n_correct 2554.25 | ppl 5.25 | accuracy 61.745 | wps 11177.8 | ups 1.35 | wpb 8273.6 | bsz 304.9 | num_updates 17679 | lr 0.000106362 | gnorm 0.526 | clip 0 | loss_scale 32 | train_wall 1027 | gb_free 12.6 | wall 16195
2023-08-06 15:12:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 15:12:51 | INFO | fairseq.trainer | begin training epoch 13
2023-08-06 15:12:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 15:13:12 | INFO | train_inner | epoch 013:     21 / 1474 loss=2.105, trans_loss=5.17, nll_loss=2.411, w2v_ctc_loss=0.742, task_loss=1.454, contrastive_loss=0.068, total=4096.49, n_correct=2520.91, ppl=5.32, accuracy=61.538, wps=6492.6, ups=0.79, wpb=8193, bsz=295.4, num_updates=17700, lr=0.000106299, gnorm=0.53, clip=0, loss_scale=32, train_wall=70, gb_free=14.4, wall=16216
2023-08-06 15:14:23 | INFO | train_inner | epoch 013:    121 / 1474 loss=2.078, trans_loss=5.12, nll_loss=2.345, w2v_ctc_loss=0.718, task_loss=1.404, contrastive_loss=0.078, total=4160.97, n_correct=2596.19, ppl=5.08, accuracy=62.394, wps=11801.2, ups=1.42, wpb=8321.9, bsz=302.9, num_updates=17800, lr=0.000106, gnorm=0.523, clip=0, loss_scale=32, train_wall=70, gb_free=16.1, wall=16287
2023-08-06 15:15:33 | INFO | train_inner | epoch 013:    221 / 1474 loss=2.105, trans_loss=5.129, nll_loss=2.358, w2v_ctc_loss=0.712, task_loss=1.291, contrastive_loss=0.293, total=4212.08, n_correct=2622.47, ppl=5.13, accuracy=62.261, wps=12015.4, ups=1.43, wpb=8424.2, bsz=329.7, num_updates=17900, lr=0.000105703, gnorm=0.523, clip=0, loss_scale=32, train_wall=70, gb_free=14.5, wall=16357
2023-08-06 15:16:43 | INFO | train_inner | epoch 013:    321 / 1474 loss=2.074, trans_loss=5.119, nll_loss=2.343, w2v_ctc_loss=0.713, task_loss=1.452, contrastive_loss=0.063, total=4102.3, n_correct=2567.43, ppl=5.07, accuracy=62.585, wps=11712.9, ups=1.43, wpb=8204.6, bsz=294.1, num_updates=18000, lr=0.000105409, gnorm=0.522, clip=0, loss_scale=32, train_wall=70, gb_free=17.2, wall=16427
2023-08-06 15:16:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 15:17:06 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.264 | trans_loss 5.607 | nll_loss 2.893 | w2v_ctc_loss 1.349 | task_loss 4.603 | contrastive_loss 0.261 | total 4003.4 | n_correct 2450.9 | ppl 7.43 | accuracy 61.22 | uer 18.193 | wer 19.921 | raw_wer 19.921 | bleu 19.44 | wps 1981.1 | wpb 4003.4 | bsz 141.8 | num_updates 18000 | best_bleu 19.61
2023-08-06 15:17:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 18000 updates
2023-08-06 15:17:06 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_13_18000.pt
2023-08-06 15:17:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_13_18000.pt
2023-08-06 15:17:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_13_18000.pt (epoch 13 @ 18000 updates, score 19.44) (writing took 33.45729517005384 seconds)
2023-08-06 15:18:50 | INFO | train_inner | epoch 013:    421 / 1474 loss=2.084, trans_loss=5.127, nll_loss=2.355, w2v_ctc_loss=0.721, task_loss=1.311, contrastive_loss=0.109, total=4177.29, n_correct=2608.16, ppl=5.11, accuracy=62.437, wps=6572.6, ups=0.79, wpb=8354.6, bsz=318.4, num_updates=18100, lr=0.000105118, gnorm=0.523, clip=0, loss_scale=32, train_wall=69, gb_free=17.4, wall=16554
2023-08-06 15:20:01 | INFO | train_inner | epoch 013:    521 / 1474 loss=2.092, trans_loss=5.135, nll_loss=2.366, w2v_ctc_loss=0.721, task_loss=1.358, contrastive_loss=0.145, total=4201.22, n_correct=2609.91, ppl=5.15, accuracy=62.123, wps=11890.6, ups=1.42, wpb=8402.4, bsz=319, num_updates=18200, lr=0.000104828, gnorm=0.524, clip=0, loss_scale=32, train_wall=70, gb_free=12.7, wall=16625
2023-08-06 15:21:10 | INFO | train_inner | epoch 013:    621 / 1474 loss=2.074, trans_loss=5.128, nll_loss=2.357, w2v_ctc_loss=0.717, task_loss=1.359, contrastive_loss=0.061, total=4161.98, n_correct=2602.36, ppl=5.12, accuracy=62.527, wps=11993, ups=1.44, wpb=8324, bsz=308.3, num_updates=18300, lr=0.000104542, gnorm=0.52, clip=0, loss_scale=32, train_wall=69, gb_free=15.7, wall=16694
2023-08-06 15:22:20 | INFO | train_inner | epoch 013:    721 / 1474 loss=2.096, trans_loss=5.144, nll_loss=2.376, w2v_ctc_loss=0.744, task_loss=1.558, contrastive_loss=0.06, total=4096.76, n_correct=2538.17, ppl=5.19, accuracy=61.956, wps=11649.9, ups=1.42, wpb=8193.5, bsz=284.6, num_updates=18400, lr=0.000104257, gnorm=0.529, clip=0, loss_scale=32, train_wall=70, gb_free=16.5, wall=16765
2023-08-06 15:23:30 | INFO | train_inner | epoch 013:    821 / 1474 loss=2.092, trans_loss=5.142, nll_loss=2.375, w2v_ctc_loss=0.723, task_loss=1.417, contrastive_loss=0.105, total=4121.73, n_correct=2554.8, ppl=5.19, accuracy=61.984, wps=11763.5, ups=1.43, wpb=8243.5, bsz=306.7, num_updates=18500, lr=0.000103975, gnorm=0.535, clip=0, loss_scale=32, train_wall=70, gb_free=14.7, wall=16835
2023-08-06 15:24:40 | INFO | train_inner | epoch 013:    921 / 1474 loss=2.089, trans_loss=5.146, nll_loss=2.38, w2v_ctc_loss=0.728, task_loss=1.432, contrastive_loss=0.069, total=4107.01, n_correct=2550.57, ppl=5.21, accuracy=62.103, wps=11769.7, ups=1.43, wpb=8214, bsz=296.5, num_updates=18600, lr=0.000103695, gnorm=0.528, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=16904
2023-08-06 15:25:49 | INFO | train_inner | epoch 013:   1021 / 1474 loss=2.1, trans_loss=5.148, nll_loss=2.383, w2v_ctc_loss=0.731, task_loss=1.478, contrastive_loss=0.119, total=4081.02, n_correct=2524.7, ppl=5.22, accuracy=61.864, wps=11855.1, ups=1.45, wpb=8162, bsz=293.4, num_updates=18700, lr=0.000103418, gnorm=0.531, clip=0, loss_scale=32, train_wall=68, gb_free=16.1, wall=16973
2023-08-06 15:26:59 | INFO | train_inner | epoch 013:   1121 / 1474 loss=2.086, trans_loss=5.136, nll_loss=2.368, w2v_ctc_loss=0.718, task_loss=1.381, contrastive_loss=0.104, total=4105.62, n_correct=2554.78, ppl=5.16, accuracy=62.226, wps=11770.4, ups=1.43, wpb=8211.2, bsz=305.9, num_updates=18800, lr=0.000103142, gnorm=0.524, clip=0, loss_scale=32, train_wall=69, gb_free=16.6, wall=17043
2023-08-06 15:28:08 | INFO | train_inner | epoch 013:   1221 / 1474 loss=2.091, trans_loss=5.151, nll_loss=2.387, w2v_ctc_loss=0.731, task_loss=1.491, contrastive_loss=0.061, total=4110.35, n_correct=2549.07, ppl=5.23, accuracy=62.016, wps=11803.4, ups=1.44, wpb=8220.7, bsz=295.1, num_updates=18900, lr=0.000102869, gnorm=0.529, clip=0, loss_scale=64, train_wall=69, gb_free=14.7, wall=17113
2023-08-06 15:29:18 | INFO | train_inner | epoch 013:   1321 / 1474 loss=2.094, trans_loss=5.137, nll_loss=2.37, w2v_ctc_loss=0.723, task_loss=1.383, contrastive_loss=0.158, total=4112.2, n_correct=2561.24, ppl=5.17, accuracy=62.284, wps=11798.6, ups=1.43, wpb=8224.4, bsz=308.2, num_updates=19000, lr=0.000102598, gnorm=0.53, clip=0, loss_scale=64, train_wall=69, gb_free=17.5, wall=17182
2023-08-06 15:30:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-06 15:30:28 | INFO | train_inner | epoch 013:   1422 / 1474 loss=2.087, trans_loss=5.152, nll_loss=2.39, w2v_ctc_loss=0.724, task_loss=1.419, contrastive_loss=0.059, total=4156.59, n_correct=2577.11, ppl=5.24, accuracy=62.001, wps=11848, ups=1.43, wpb=8313.2, bsz=303.4, num_updates=19100, lr=0.000102329, gnorm=0.524, clip=0, loss_scale=32, train_wall=70, gb_free=15.9, wall=17253
2023-08-06 15:31:04 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 15:31:30 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.243 | trans_loss 5.596 | nll_loss 2.882 | w2v_ctc_loss 1.312 | task_loss 4.616 | contrastive_loss 0.25 | total 4003.4 | n_correct 2455.3 | ppl 7.37 | accuracy 61.33 | uer 18.172 | wer 19.802 | raw_wer 19.802 | bleu 19.32 | wps 1946.1 | wpb 4003.4 | bsz 141.8 | num_updates 19152 | best_bleu 19.61
2023-08-06 15:31:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 19152 updates
2023-08-06 15:31:30 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.3208.pt
2023-08-06 15:31:33 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.3208.pt
2023-08-06 15:31:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.3208.pt (epoch 13 @ 19152 updates, score 19.32) (writing took 17.585545206442475 seconds)
2023-08-06 15:31:48 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-08-06 15:31:48 | INFO | train | epoch 013 | loss 2.088 | trans_loss 5.136 | nll_loss 2.367 | w2v_ctc_loss 0.723 | task_loss 1.403 | contrastive_loss 0.106 | total 4137.23 | n_correct 2573.62 | ppl 5.16 | accuracy 62.206 | wps 10715 | ups 1.29 | wpb 8274.5 | bsz 305.1 | num_updates 19152 | lr 0.00010219 | gnorm 0.526 | clip 0 | loss_scale 32 | train_wall 1023 | gb_free 17.6 | wall 17332
2023-08-06 15:31:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 15:31:48 | INFO | fairseq.trainer | begin training epoch 14
2023-08-06 15:31:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 15:32:28 | INFO | train_inner | epoch 014:     48 / 1474 loss=2.065, trans_loss=5.105, nll_loss=2.33, w2v_ctc_loss=0.711, task_loss=1.288, contrastive_loss=0.074, total=4179.66, n_correct=2630.39, ppl=5.03, accuracy=62.933, wps=6960.1, ups=0.83, wpb=8359.3, bsz=322, num_updates=19200, lr=0.000102062, gnorm=0.523, clip=0, loss_scale=32, train_wall=69, gb_free=10.2, wall=17373
2023-08-06 15:33:38 | INFO | train_inner | epoch 014:    148 / 1474 loss=2.06, trans_loss=5.093, nll_loss=2.311, w2v_ctc_loss=0.711, task_loss=1.4, contrastive_loss=0.059, total=4081.01, n_correct=2577.23, ppl=4.96, accuracy=63.152, wps=11786.4, ups=1.44, wpb=8162, bsz=300.6, num_updates=19300, lr=0.000101797, gnorm=0.518, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=17442
2023-08-06 15:34:47 | INFO | train_inner | epoch 014:    248 / 1474 loss=2.083, trans_loss=5.114, nll_loss=2.338, w2v_ctc_loss=0.713, task_loss=1.461, contrastive_loss=0.157, total=4109.83, n_correct=2574.19, ppl=5.05, accuracy=62.635, wps=11869.3, ups=1.44, wpb=8219.7, bsz=295.2, num_updates=19400, lr=0.000101535, gnorm=0.522, clip=0, loss_scale=32, train_wall=69, gb_free=15.5, wall=17511
2023-08-06 15:35:57 | INFO | train_inner | epoch 014:    348 / 1474 loss=2.066, trans_loss=5.106, nll_loss=2.329, w2v_ctc_loss=0.708, task_loss=1.305, contrastive_loss=0.09, total=4171.83, n_correct=2622.55, ppl=5.03, accuracy=62.863, wps=11928, ups=1.43, wpb=8343.7, bsz=319.7, num_updates=19500, lr=0.000101274, gnorm=0.524, clip=0, loss_scale=32, train_wall=70, gb_free=16.6, wall=17581
2023-08-06 15:37:06 | INFO | train_inner | epoch 014:    448 / 1474 loss=2.067, trans_loss=5.115, nll_loss=2.34, w2v_ctc_loss=0.706, task_loss=1.406, contrastive_loss=0.067, total=4142.75, n_correct=2593.91, ppl=5.06, accuracy=62.613, wps=11995.9, ups=1.45, wpb=8285.5, bsz=302.2, num_updates=19600, lr=0.000101015, gnorm=0.519, clip=0, loss_scale=32, train_wall=69, gb_free=16.5, wall=17650
2023-08-06 15:38:16 | INFO | train_inner | epoch 014:    548 / 1474 loss=2.079, trans_loss=5.119, nll_loss=2.345, w2v_ctc_loss=0.726, task_loss=1.506, contrastive_loss=0.074, total=4073.76, n_correct=2543.45, ppl=5.08, accuracy=62.435, wps=11592.7, ups=1.42, wpb=8147.5, bsz=290.9, num_updates=19700, lr=0.000100759, gnorm=0.532, clip=0, loss_scale=32, train_wall=70, gb_free=15.3, wall=17720
2023-08-06 15:39:26 | INFO | train_inner | epoch 014:    648 / 1474 loss=2.081, trans_loss=5.12, nll_loss=2.347, w2v_ctc_loss=0.712, task_loss=1.401, contrastive_loss=0.132, total=4158.79, n_correct=2599.18, ppl=5.09, accuracy=62.498, wps=11890.9, ups=1.43, wpb=8317.6, bsz=306.8, num_updates=19800, lr=0.000100504, gnorm=0.529, clip=0, loss_scale=32, train_wall=70, gb_free=17.1, wall=17790
2023-08-06 15:40:36 | INFO | train_inner | epoch 014:    748 / 1474 loss=2.064, trans_loss=5.107, nll_loss=2.331, w2v_ctc_loss=0.708, task_loss=1.363, contrastive_loss=0.065, total=4145.47, n_correct=2604.65, ppl=5.03, accuracy=62.831, wps=11933.1, ups=1.44, wpb=8290.9, bsz=309.6, num_updates=19900, lr=0.000100251, gnorm=0.514, clip=0, loss_scale=32, train_wall=69, gb_free=16, wall=17860
2023-08-06 15:41:46 | INFO | train_inner | epoch 014:    848 / 1474 loss=2.082, trans_loss=5.113, nll_loss=2.339, w2v_ctc_loss=0.708, task_loss=1.331, contrastive_loss=0.174, total=4171.1, n_correct=2614.33, ppl=5.06, accuracy=62.677, wps=11910.6, ups=1.43, wpb=8342.2, bsz=319.7, num_updates=20000, lr=0.0001, gnorm=0.524, clip=0, loss_scale=32, train_wall=70, gb_free=16.5, wall=17930
2023-08-06 15:41:46 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 15:42:09 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.241 | trans_loss 5.589 | nll_loss 2.872 | w2v_ctc_loss 1.327 | task_loss 4.63 | contrastive_loss 0.249 | total 4003.4 | n_correct 2462.4 | ppl 7.32 | accuracy 61.508 | uer 17.625 | wer 19.653 | raw_wer 19.653 | bleu 19.86 | wps 2252.5 | wpb 4003.4 | bsz 141.8 | num_updates 20000 | best_bleu 19.86
2023-08-06 15:42:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20000 updates
2023-08-06 15:42:09 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_14_20000.pt
2023-08-06 15:42:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_14_20000.pt
2023-08-06 15:42:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_14_20000.pt (epoch 14 @ 20000 updates, score 19.86) (writing took 44.327562956139445 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:0')
2023-08-06 15:44:05 | INFO | train_inner | epoch 014:    948 / 1474 loss=2.077, trans_loss=5.121, nll_loss=2.349, w2v_ctc_loss=0.712, task_loss=1.405, contrastive_loss=0.109, total=4167.75, n_correct=2607.3, ppl=5.09, accuracy=62.559, wps=6006.1, ups=0.72, wpb=8335.5, bsz=310.1, num_updates=20100, lr=9.97509e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=70, gb_free=16.1, wall=18069
2023-08-06 15:45:14 | INFO | train_inner | epoch 014:   1048 / 1474 loss=2.076, trans_loss=5.127, nll_loss=2.357, w2v_ctc_loss=0.71, task_loss=1.426, contrastive_loss=0.085, total=4143.92, n_correct=2588.53, ppl=5.12, accuracy=62.466, wps=11866.7, ups=1.43, wpb=8287.8, bsz=300.7, num_updates=20200, lr=9.95037e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=18139
2023-08-06 15:46:25 | INFO | train_inner | epoch 014:   1148 / 1474 loss=2.111, trans_loss=5.124, nll_loss=2.355, w2v_ctc_loss=0.718, task_loss=1.312, contrastive_loss=0.355, total=4228.69, n_correct=2637.91, ppl=5.11, accuracy=62.381, wps=11929.8, ups=1.41, wpb=8457.4, bsz=327.2, num_updates=20300, lr=9.92583e-05, gnorm=0.522, clip=0, loss_scale=32, train_wall=70, gb_free=15.6, wall=18209
2023-08-06 15:47:35 | INFO | train_inner | epoch 014:   1248 / 1474 loss=2.088, trans_loss=5.144, nll_loss=2.377, w2v_ctc_loss=0.732, task_loss=1.648, contrastive_loss=0.049, total=4021.19, n_correct=2499.33, ppl=5.19, accuracy=62.154, wps=11551.1, ups=1.44, wpb=8042.4, bsz=271.7, num_updates=20400, lr=9.90148e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=69, gb_free=16.2, wall=18279
2023-08-06 15:48:45 | INFO | train_inner | epoch 014:   1348 / 1474 loss=2.07, trans_loss=5.13, nll_loss=2.362, w2v_ctc_loss=0.706, task_loss=1.318, contrastive_loss=0.065, total=4213.9, n_correct=2634.23, ppl=5.14, accuracy=62.513, wps=12055.3, ups=1.43, wpb=8427.8, bsz=319.4, num_updates=20500, lr=9.8773e-05, gnorm=0.514, clip=0, loss_scale=32, train_wall=69, gb_free=16, wall=18349
2023-08-06 15:49:54 | INFO | train_inner | epoch 014:   1448 / 1474 loss=2.082, trans_loss=5.136, nll_loss=2.37, w2v_ctc_loss=0.714, task_loss=1.4, contrastive_loss=0.103, total=4130.28, n_correct=2576.69, ppl=5.17, accuracy=62.385, wps=11915.6, ups=1.44, wpb=8260.6, bsz=304.1, num_updates=20600, lr=9.85329e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=69, gb_free=15.1, wall=18418
2023-08-06 15:50:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:6')
2023-08-06 15:50:36 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.243 | trans_loss 5.589 | nll_loss 2.877 | w2v_ctc_loss 1.329 | task_loss 4.642 | contrastive_loss 0.255 | total 4003.4 | n_correct 2457.7 | ppl 7.34 | accuracy 61.39 | uer 17.97 | wer 19.746 | raw_wer 19.746 | bleu 19.53 | wps 2194.6 | wpb 4003.4 | bsz 141.8 | num_updates 20626 | best_bleu 19.86
2023-08-06 15:50:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20626 updates
2023-08-06 15:50:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.5300.pt
2023-08-06 15:50:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.5300.pt
2023-08-06 15:50:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.5300.pt (epoch 14 @ 20626 updates, score 19.53) (writing took 13.39912755601108 seconds)
2023-08-06 15:50:50 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-08-06 15:50:50 | INFO | train | epoch 014 | loss 2.077 | trans_loss 5.119 | nll_loss 2.346 | w2v_ctc_loss 0.714 | task_loss 1.401 | contrastive_loss 0.113 | total 4138.65 | n_correct 2590.5 | ppl 5.08 | accuracy 62.593 | wps 10687.4 | ups 1.29 | wpb 8277.3 | bsz 305.7 | num_updates 20626 | lr 9.84708e-05 | gnorm 0.524 | clip 0 | loss_scale 32 | train_wall 1022 | gb_free 16.2 | wall 18474
2023-08-06 15:50:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 15:50:50 | INFO | fairseq.trainer | begin training epoch 15
2023-08-06 15:50:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 15:51:48 | INFO | train_inner | epoch 015:     74 / 1474 loss=2.073, trans_loss=5.105, nll_loss=2.328, w2v_ctc_loss=0.705, task_loss=1.405, contrastive_loss=0.154, total=4083.88, n_correct=2567.1, ppl=5.02, accuracy=62.859, wps=7183.7, ups=0.88, wpb=8167.8, bsz=300.2, num_updates=20700, lr=9.82946e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=18532
2023-08-06 15:52:57 | INFO | train_inner | epoch 015:    174 / 1474 loss=2.061, trans_loss=5.096, nll_loss=2.315, w2v_ctc_loss=0.711, task_loss=1.456, contrastive_loss=0.062, total=4115.73, n_correct=2592.21, ppl=4.97, accuracy=62.983, wps=11913.9, ups=1.45, wpb=8231.5, bsz=297.8, num_updates=20800, lr=9.80581e-05, gnorm=0.52, clip=0, loss_scale=32, train_wall=69, gb_free=16.7, wall=18601
2023-08-06 15:54:07 | INFO | train_inner | epoch 015:    274 / 1474 loss=2.051, trans_loss=5.095, nll_loss=2.315, w2v_ctc_loss=0.697, task_loss=1.344, contrastive_loss=0.054, total=4193.15, n_correct=2652.7, ppl=4.98, accuracy=63.263, wps=12033.3, ups=1.43, wpb=8386.3, bsz=312.6, num_updates=20900, lr=9.78232e-05, gnorm=0.515, clip=0, loss_scale=32, train_wall=69, gb_free=12.6, wall=18671
2023-08-06 15:55:16 | INFO | train_inner | epoch 015:    374 / 1474 loss=2.057, trans_loss=5.089, nll_loss=2.306, w2v_ctc_loss=0.702, task_loss=1.41, contrastive_loss=0.077, total=4167.66, n_correct=2630.78, ppl=4.95, accuracy=63.124, wps=12057.5, ups=1.45, wpb=8335.3, bsz=306, num_updates=21000, lr=9.759e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=69, gb_free=15.9, wall=18740
2023-08-06 15:55:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-06 15:56:26 | INFO | train_inner | epoch 015:    475 / 1474 loss=2.055, trans_loss=5.098, nll_loss=2.318, w2v_ctc_loss=0.696, task_loss=1.507, contrastive_loss=0.049, total=4045.97, n_correct=2551.79, ppl=4.99, accuracy=63.07, wps=11453.3, ups=1.42, wpb=8091.9, bsz=285.2, num_updates=21100, lr=9.73585e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=70, gb_free=16.6, wall=18811
2023-08-06 15:57:36 | INFO | train_inner | epoch 015:    575 / 1474 loss=2.061, trans_loss=5.098, nll_loss=2.319, w2v_ctc_loss=0.705, task_loss=1.431, contrastive_loss=0.08, total=4156.05, n_correct=2620.51, ppl=4.99, accuracy=63.053, wps=11895.6, ups=1.43, wpb=8312.1, bsz=302.8, num_updates=21200, lr=9.71286e-05, gnorm=0.523, clip=0, loss_scale=16, train_wall=69, gb_free=11.5, wall=18881
2023-08-06 15:58:46 | INFO | train_inner | epoch 015:    675 / 1474 loss=2.07, trans_loss=5.098, nll_loss=2.319, w2v_ctc_loss=0.708, task_loss=1.434, contrastive_loss=0.121, total=4118.87, n_correct=2599.32, ppl=4.99, accuracy=63.108, wps=11890.3, ups=1.44, wpb=8237.7, bsz=303, num_updates=21300, lr=9.69003e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=69, gb_free=16.8, wall=18950
2023-08-06 15:59:56 | INFO | train_inner | epoch 015:    775 / 1474 loss=2.062, trans_loss=5.106, nll_loss=2.33, w2v_ctc_loss=0.708, task_loss=1.409, contrastive_loss=0.064, total=4176.64, n_correct=2629.19, ppl=5.03, accuracy=62.95, wps=11935.4, ups=1.43, wpb=8353.3, bsz=305.3, num_updates=21400, lr=9.66736e-05, gnorm=0.522, clip=0, loss_scale=16, train_wall=70, gb_free=13.8, wall=19020
2023-08-06 16:01:05 | INFO | train_inner | epoch 015:    875 / 1474 loss=2.069, trans_loss=5.114, nll_loss=2.34, w2v_ctc_loss=0.716, task_loss=1.512, contrastive_loss=0.06, total=4056.99, n_correct=2545.82, ppl=5.06, accuracy=62.751, wps=11769.9, ups=1.45, wpb=8114, bsz=288, num_updates=21500, lr=9.64486e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=68, gb_free=17.2, wall=19089
2023-08-06 16:02:14 | INFO | train_inner | epoch 015:    975 / 1474 loss=2.07, trans_loss=5.107, nll_loss=2.332, w2v_ctc_loss=0.703, task_loss=1.391, contrastive_loss=0.14, total=4134.44, n_correct=2599.76, ppl=5.03, accuracy=62.881, wps=11949, ups=1.45, wpb=8268.9, bsz=304.8, num_updates=21600, lr=9.6225e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=69, gb_free=15.9, wall=19158
2023-08-06 16:03:24 | INFO | train_inner | epoch 015:   1075 / 1474 loss=2.094, trans_loss=5.114, nll_loss=2.342, w2v_ctc_loss=0.707, task_loss=1.317, contrastive_loss=0.299, total=4185.02, n_correct=2622.04, ppl=5.07, accuracy=62.653, wps=11936.3, ups=1.43, wpb=8370, bsz=324.2, num_updates=21700, lr=9.60031e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=70, gb_free=16.6, wall=19228
2023-08-06 16:04:33 | INFO | train_inner | epoch 015:   1175 / 1474 loss=2.052, trans_loss=5.1, nll_loss=2.325, w2v_ctc_loss=0.684, task_loss=1.25, contrastive_loss=0.107, total=4187.68, n_correct=2648.54, ppl=5.01, accuracy=63.246, wps=12081.2, ups=1.44, wpb=8375.4, bsz=329.7, num_updates=21800, lr=9.57826e-05, gnorm=0.514, clip=0, loss_scale=16, train_wall=69, gb_free=16.5, wall=19297
2023-08-06 16:05:43 | INFO | train_inner | epoch 015:   1275 / 1474 loss=2.068, trans_loss=5.111, nll_loss=2.337, w2v_ctc_loss=0.717, task_loss=1.442, contrastive_loss=0.063, total=4141.6, n_correct=2601.31, ppl=5.05, accuracy=62.809, wps=11946.5, ups=1.44, wpb=8283.2, bsz=301.1, num_updates=21900, lr=9.55637e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=69, gb_free=13.3, wall=19367
2023-08-06 16:06:52 | INFO | train_inner | epoch 015:   1375 / 1474 loss=2.06, trans_loss=5.11, nll_loss=2.335, w2v_ctc_loss=0.703, task_loss=1.45, contrastive_loss=0.05, total=4099.6, n_correct=2580.86, ppl=5.05, accuracy=62.954, wps=11820.2, ups=1.44, wpb=8199.2, bsz=293.2, num_updates=22000, lr=9.53463e-05, gnorm=0.524, clip=0, loss_scale=16, train_wall=69, gb_free=14.2, wall=19436
2023-08-06 16:06:52 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 16:07:17 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 4.211 | trans_loss 5.577 | nll_loss 2.855 | w2v_ctc_loss 1.253 | task_loss 4.619 | contrastive_loss 0.247 | total 4003.4 | n_correct 2467 | ppl 7.24 | accuracy 61.623 | uer 17.628 | wer 19.492 | raw_wer 19.492 | bleu 19.82 | wps 1911 | wpb 4003.4 | bsz 141.8 | num_updates 22000 | best_bleu 19.86
2023-08-06 16:07:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22000 updates
2023-08-06 16:07:17 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_15_22000.pt
2023-08-06 16:07:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_15_22000.pt
2023-08-06 16:07:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_15_22000.pt (epoch 15 @ 22000 updates, score 19.82) (writing took 20.82278029061854 seconds)
2023-08-06 16:08:48 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 16:09:11 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 4.23 | trans_loss 5.574 | nll_loss 2.855 | w2v_ctc_loss 1.317 | task_loss 4.625 | contrastive_loss 0.255 | total 4003.4 | n_correct 2469 | ppl 7.23 | accuracy 61.673 | uer 17.79 | wer 19.526 | raw_wer 19.526 | bleu 19.82 | wps 2312.1 | wpb 4003.4 | bsz 141.8 | num_updates 22099 | best_bleu 19.86
2023-08-06 16:09:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22099 updates
2023-08-06 16:09:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.8200.pt
2023-08-06 16:09:14 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.8200.pt
2023-08-06 16:09:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.8200.pt (epoch 15 @ 22099 updates, score 19.82) (writing took 13.860309870913625 seconds)
2023-08-06 16:09:25 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-08-06 16:09:25 | INFO | train | epoch 015 | loss 2.065 | trans_loss 5.103 | nll_loss 2.326 | w2v_ctc_loss 0.704 | task_loss 1.402 | contrastive_loss 0.103 | total 4136.86 | n_correct 2605.83 | ppl 5.01 | accuracy 62.99 | wps 10921.6 | ups 1.32 | wpb 8273.7 | bsz 305.1 | num_updates 22099 | lr 9.51325e-05 | gnorm 0.523 | clip 0 | loss_scale 16 | train_wall 1018 | gb_free 16.8 | wall 19590
2023-08-06 16:09:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 16:09:26 | INFO | fairseq.trainer | begin training epoch 16
2023-08-06 16:09:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 16:09:34 | INFO | train_inner | epoch 016:      1 / 1474 loss=2.075, trans_loss=5.115, nll_loss=2.345, w2v_ctc_loss=0.708, task_loss=1.341, contrastive_loss=0.135, total=4149.9, n_correct=2607.59, ppl=5.08, accuracy=62.835, wps=5136.6, ups=0.62, wpb=8299.8, bsz=316.3, num_updates=22100, lr=9.51303e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=70, gb_free=17.4, wall=19598
2023-08-06 16:10:43 | INFO | train_inner | epoch 016:    101 / 1474 loss=2.044, trans_loss=5.073, nll_loss=2.287, w2v_ctc_loss=0.694, task_loss=1.345, contrastive_loss=0.079, total=4118.73, n_correct=2619.18, ppl=4.88, accuracy=63.592, wps=11885.1, ups=1.44, wpb=8237.5, bsz=314, num_updates=22200, lr=9.49158e-05, gnorm=0.524, clip=0, loss_scale=16, train_wall=69, gb_free=16.1, wall=19667
2023-08-06 16:11:52 | INFO | train_inner | epoch 016:    201 / 1474 loss=2.038, trans_loss=5.07, nll_loss=2.282, w2v_ctc_loss=0.686, task_loss=1.435, contrastive_loss=0.056, total=4106.45, n_correct=2615.14, ppl=4.86, accuracy=63.684, wps=11886.2, ups=1.45, wpb=8212.9, bsz=297.4, num_updates=22300, lr=9.47027e-05, gnorm=0.517, clip=0, loss_scale=16, train_wall=69, gb_free=16.3, wall=19736
2023-08-06 16:13:01 | INFO | train_inner | epoch 016:    301 / 1474 loss=2.058, trans_loss=5.082, nll_loss=2.299, w2v_ctc_loss=0.698, task_loss=1.382, contrastive_loss=0.128, total=4169.65, n_correct=2643.37, ppl=4.92, accuracy=63.395, wps=12008.6, ups=1.44, wpb=8339.3, bsz=309.9, num_updates=22400, lr=9.44911e-05, gnorm=0.524, clip=0, loss_scale=16, train_wall=69, gb_free=10.5, wall=19806
2023-08-06 16:14:11 | INFO | train_inner | epoch 016:    401 / 1474 loss=2.065, trans_loss=5.085, nll_loss=2.301, w2v_ctc_loss=0.705, task_loss=1.5, contrastive_loss=0.142, total=4063.79, n_correct=2569.92, ppl=4.93, accuracy=63.239, wps=11739.4, ups=1.44, wpb=8127.6, bsz=286.5, num_updates=22500, lr=9.42809e-05, gnorm=0.539, clip=0, loss_scale=16, train_wall=69, gb_free=12.5, wall=19875
2023-08-06 16:15:21 | INFO | train_inner | epoch 016:    501 / 1474 loss=2.046, trans_loss=5.08, nll_loss=2.297, w2v_ctc_loss=0.691, task_loss=1.344, contrastive_loss=0.085, total=4179.53, n_correct=2659.01, ppl=4.91, accuracy=63.62, wps=11926.8, ups=1.43, wpb=8359.1, bsz=319.5, num_updates=22600, lr=9.40721e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=70, gb_free=17.7, wall=19945
2023-08-06 16:16:30 | INFO | train_inner | epoch 016:    601 / 1474 loss=2.046, trans_loss=5.083, nll_loss=2.3, w2v_ctc_loss=0.692, task_loss=1.41, contrastive_loss=0.051, total=4121.37, n_correct=2612.73, ppl=4.93, accuracy=63.395, wps=11938.4, ups=1.45, wpb=8242.7, bsz=297.5, num_updates=22700, lr=9.38647e-05, gnorm=0.523, clip=0, loss_scale=16, train_wall=69, gb_free=17.8, wall=20014
2023-08-06 16:17:39 | INFO | train_inner | epoch 016:    701 / 1474 loss=2.048, trans_loss=5.089, nll_loss=2.308, w2v_ctc_loss=0.696, task_loss=1.431, contrastive_loss=0.054, total=4099.17, n_correct=2596.86, ppl=4.95, accuracy=63.351, wps=11872.6, ups=1.45, wpb=8198.3, bsz=297.5, num_updates=22800, lr=9.36586e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=69, gb_free=16.6, wall=20083
2023-08-06 16:18:48 | INFO | train_inner | epoch 016:    801 / 1474 loss=2.053, trans_loss=5.089, nll_loss=2.308, w2v_ctc_loss=0.686, task_loss=1.337, contrastive_loss=0.113, total=4184.53, n_correct=2650.45, ppl=4.95, accuracy=63.339, wps=12109.3, ups=1.45, wpb=8369.1, bsz=313, num_updates=22900, lr=9.34539e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=69, gb_free=13.2, wall=20152
2023-08-06 16:19:58 | INFO | train_inner | epoch 016:    901 / 1474 loss=2.053, trans_loss=5.088, nll_loss=2.308, w2v_ctc_loss=0.691, task_loss=1.378, contrastive_loss=0.104, total=4151.84, n_correct=2633.79, ppl=4.95, accuracy=63.437, wps=11898.2, ups=1.43, wpb=8303.7, bsz=307, num_updates=23000, lr=9.32505e-05, gnorm=0.531, clip=0, loss_scale=16, train_wall=69, gb_free=16.7, wall=20222
2023-08-06 16:21:07 | INFO | train_inner | epoch 016:   1001 / 1474 loss=2.066, trans_loss=5.102, nll_loss=2.325, w2v_ctc_loss=0.709, task_loss=1.448, contrastive_loss=0.102, total=4112.79, n_correct=2590.01, ppl=5.01, accuracy=62.975, wps=11831.1, ups=1.44, wpb=8225.6, bsz=299.5, num_updates=23100, lr=9.30484e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=69, gb_free=14.7, wall=20291
2023-08-06 16:22:17 | INFO | train_inner | epoch 016:   1101 / 1474 loss=2.064, trans_loss=5.104, nll_loss=2.328, w2v_ctc_loss=0.709, task_loss=1.486, contrastive_loss=0.079, total=4111.6, n_correct=2588.68, ppl=5.02, accuracy=62.96, wps=11794.6, ups=1.43, wpb=8223.2, bsz=295.6, num_updates=23200, lr=9.28477e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=69, gb_free=14.5, wall=20361
2023-08-06 16:23:27 | INFO | train_inner | epoch 016:   1201 / 1474 loss=2.066, trans_loss=5.098, nll_loss=2.321, w2v_ctc_loss=0.687, task_loss=1.428, contrastive_loss=0.175, total=4157.51, n_correct=2623.05, ppl=5, accuracy=63.092, wps=11816.2, ups=1.42, wpb=8315, bsz=306.6, num_updates=23300, lr=9.26482e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=70, gb_free=14.7, wall=20432
2023-08-06 16:24:37 | INFO | train_inner | epoch 016:   1301 / 1474 loss=2.066, trans_loss=5.099, nll_loss=2.322, w2v_ctc_loss=0.703, task_loss=1.36, contrastive_loss=0.153, total=4151.03, n_correct=2622.22, ppl=5, accuracy=63.17, wps=11888.6, ups=1.43, wpb=8302.1, bsz=314.5, num_updates=23400, lr=9.245e-05, gnorm=0.522, clip=0, loss_scale=32, train_wall=69, gb_free=15.9, wall=20501
2023-08-06 16:25:47 | INFO | train_inner | epoch 016:   1401 / 1474 loss=2.056, trans_loss=5.1, nll_loss=2.324, w2v_ctc_loss=0.7, task_loss=1.337, contrastive_loss=0.082, total=4201.47, n_correct=2653.58, ppl=5.01, accuracy=63.158, wps=12030.1, ups=1.43, wpb=8402.9, bsz=320.7, num_updates=23500, lr=9.22531e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=20571
2023-08-06 16:26:38 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 16:27:02 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 4.233 | trans_loss 5.573 | nll_loss 2.853 | w2v_ctc_loss 1.334 | task_loss 4.651 | contrastive_loss 0.252 | total 4003.4 | n_correct 2470.5 | ppl 7.23 | accuracy 61.71 | uer 17.575 | wer 19.261 | raw_wer 19.261 | bleu 20.02 | wps 2153.4 | wpb 4003.4 | bsz 141.8 | num_updates 23573 | best_bleu 20.02
2023-08-06 16:27:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 23573 updates
2023-08-06 16:27:02 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 16:27:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 16:27:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt (epoch 16 @ 23573 updates, score 20.02) (writing took 28.780569361522794 seconds)
2023-08-06 16:27:31 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-08-06 16:27:31 | INFO | train | epoch 016 | loss 2.056 | trans_loss 5.089 | nll_loss 2.309 | w2v_ctc_loss 0.696 | task_loss 1.4 | contrastive_loss 0.109 | total 4138.65 | n_correct 2619.98 | ppl 4.95 | accuracy 63.305 | wps 11237.3 | ups 1.36 | wpb 8277.3 | bsz 305.7 | num_updates 23573 | lr 9.21102e-05 | gnorm 0.526 | clip 0 | loss_scale 32 | train_wall 1019 | gb_free 15.3 | wall 20675
2023-08-06 16:27:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 16:27:31 | INFO | fairseq.trainer | begin training epoch 17
2023-08-06 16:27:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 16:27:57 | INFO | train_inner | epoch 017:     27 / 1474 loss=2.064, trans_loss=5.083, nll_loss=2.301, w2v_ctc_loss=0.69, task_loss=1.432, contrastive_loss=0.22, total=4145.04, n_correct=2626.2, ppl=4.93, accuracy=63.358, wps=6366.1, ups=0.77, wpb=8290.1, bsz=302.4, num_updates=23600, lr=9.20575e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=70, gb_free=15.5, wall=20701
2023-08-06 16:29:07 | INFO | train_inner | epoch 017:    127 / 1474 loss=2.038, trans_loss=5.06, nll_loss=2.27, w2v_ctc_loss=0.695, task_loss=1.441, contrastive_loss=0.057, total=4117.27, n_correct=2630.03, ppl=4.82, accuracy=63.878, wps=11820.4, ups=1.44, wpb=8234.5, bsz=296.2, num_updates=23700, lr=9.1863e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=69, gb_free=17.7, wall=20771
2023-08-06 16:30:16 | INFO | train_inner | epoch 017:    227 / 1474 loss=2.053, trans_loss=5.06, nll_loss=2.272, w2v_ctc_loss=0.675, task_loss=1.327, contrastive_loss=0.219, total=4159.6, n_correct=2656.63, ppl=4.83, accuracy=63.867, wps=12002.7, ups=1.44, wpb=8319.2, bsz=317.6, num_updates=23800, lr=9.16698e-05, gnorm=0.526, clip=0, loss_scale=32, train_wall=69, gb_free=16.8, wall=20840
2023-08-06 16:31:26 | INFO | train_inner | epoch 017:    327 / 1474 loss=2.055, trans_loss=5.067, nll_loss=2.28, w2v_ctc_loss=0.684, task_loss=1.393, contrastive_loss=0.225, total=4156.91, n_correct=2651.44, ppl=4.86, accuracy=63.784, wps=11941.8, ups=1.44, wpb=8313.8, bsz=305.7, num_updates=23900, lr=9.14779e-05, gnorm=0.526, clip=0, loss_scale=32, train_wall=69, gb_free=17.4, wall=20910
2023-08-06 16:32:36 | INFO | train_inner | epoch 017:    427 / 1474 loss=2.034, trans_loss=5.069, nll_loss=2.283, w2v_ctc_loss=0.682, task_loss=1.388, contrastive_loss=0.055, total=4146.43, n_correct=2647.04, ppl=4.87, accuracy=63.839, wps=11861.1, ups=1.43, wpb=8292.9, bsz=308, num_updates=24000, lr=9.12871e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=69, gb_free=16.1, wall=20980
2023-08-06 16:32:36 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 16:32:59 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 4.232 | trans_loss 5.577 | nll_loss 2.851 | w2v_ctc_loss 1.328 | task_loss 4.635 | contrastive_loss 0.243 | total 4003.4 | n_correct 2467.3 | ppl 7.21 | accuracy 61.63 | uer 17.384 | wer 19.22 | raw_wer 19.22 | bleu 19.46 | wps 2224.1 | wpb 4003.4 | bsz 141.8 | num_updates 24000 | best_bleu 20.02
2023-08-06 16:32:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 24000 updates
2023-08-06 16:32:59 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_17_24000.pt
2023-08-06 16:33:02 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_17_24000.pt
2023-08-06 16:33:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_17_24000.pt (epoch 17 @ 24000 updates, score 19.46) (writing took 37.38657685555518 seconds)
2023-08-06 16:34:48 | INFO | train_inner | epoch 017:    527 / 1474 loss=2.046, trans_loss=5.073, nll_loss=2.288, w2v_ctc_loss=0.692, task_loss=1.453, contrastive_loss=0.098, total=4182.1, n_correct=2661.58, ppl=4.88, accuracy=63.642, wps=6336.3, ups=0.76, wpb=8364.2, bsz=307.9, num_updates=24100, lr=9.10975e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=71, gb_free=16.6, wall=21112
2023-08-06 16:35:57 | INFO | train_inner | epoch 017:    627 / 1474 loss=2.038, trans_loss=5.076, nll_loss=2.292, w2v_ctc_loss=0.686, task_loss=1.409, contrastive_loss=0.051, total=4167.27, n_correct=2655.55, ppl=4.9, accuracy=63.724, wps=11988.4, ups=1.44, wpb=8334.5, bsz=302.2, num_updates=24200, lr=9.09091e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=69, gb_free=10.5, wall=21181
2023-08-06 16:37:07 | INFO | train_inner | epoch 017:    727 / 1474 loss=2.053, trans_loss=5.081, nll_loss=2.299, w2v_ctc_loss=0.699, task_loss=1.384, contrastive_loss=0.095, total=4166.12, n_correct=2643.73, ppl=4.92, accuracy=63.458, wps=12040.4, ups=1.45, wpb=8332.2, bsz=308.1, num_updates=24300, lr=9.07218e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=69, gb_free=16.2, wall=21251
2023-08-06 16:38:16 | INFO | train_inner | epoch 017:    827 / 1474 loss=2.042, trans_loss=5.079, nll_loss=2.295, w2v_ctc_loss=0.69, task_loss=1.419, contrastive_loss=0.062, total=4091.64, n_correct=2605.49, ppl=4.91, accuracy=63.678, wps=11852.7, ups=1.45, wpb=8183.3, bsz=295.3, num_updates=24400, lr=9.05357e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=69, gb_free=17.2, wall=21320
2023-08-06 16:39:24 | INFO | train_inner | epoch 017:    927 / 1474 loss=2.037, trans_loss=5.077, nll_loss=2.294, w2v_ctc_loss=0.682, task_loss=1.381, contrastive_loss=0.059, total=4106.83, n_correct=2611.47, ppl=4.9, accuracy=63.588, wps=11919.6, ups=1.45, wpb=8213.7, bsz=304.6, num_updates=24500, lr=9.03508e-05, gnorm=0.524, clip=0, loss_scale=32, train_wall=68, gb_free=15.7, wall=21389
2023-08-06 16:40:34 | INFO | train_inner | epoch 017:   1027 / 1474 loss=2.04, trans_loss=5.077, nll_loss=2.294, w2v_ctc_loss=0.689, task_loss=1.386, contrastive_loss=0.064, total=4115.49, n_correct=2621.32, ppl=4.91, accuracy=63.694, wps=11918.6, ups=1.45, wpb=8231, bsz=305.8, num_updates=24600, lr=9.0167e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=69, gb_free=16.5, wall=21458
2023-08-06 16:41:44 | INFO | train_inner | epoch 017:   1127 / 1474 loss=2.038, trans_loss=5.079, nll_loss=2.296, w2v_ctc_loss=0.685, task_loss=1.456, contrastive_loss=0.051, total=4078.39, n_correct=2596.83, ppl=4.91, accuracy=63.673, wps=11649.5, ups=1.43, wpb=8156.8, bsz=293.7, num_updates=24700, lr=8.99843e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=70, gb_free=15.4, wall=21528
2023-08-06 16:42:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-06 16:42:54 | INFO | train_inner | epoch 017:   1228 / 1474 loss=2.066, trans_loss=5.087, nll_loss=2.308, w2v_ctc_loss=0.685, task_loss=1.395, contrastive_loss=0.24, total=4148.2, n_correct=2625.08, ppl=4.95, accuracy=63.282, wps=11728.1, ups=1.41, wpb=8296.4, bsz=315.7, num_updates=24800, lr=8.98027e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=70, gb_free=15.9, wall=21598
2023-08-06 16:44:04 | INFO | train_inner | epoch 017:   1328 / 1474 loss=2.051, trans_loss=5.087, nll_loss=2.308, w2v_ctc_loss=0.679, task_loss=1.393, contrastive_loss=0.133, total=4149.03, n_correct=2631.78, ppl=4.95, accuracy=63.431, wps=11856.2, ups=1.43, wpb=8298.1, bsz=306.7, num_updates=24900, lr=8.96221e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=70, gb_free=16.7, wall=21668
2023-08-06 16:45:14 | INFO | train_inner | epoch 017:   1428 / 1474 loss=2.042, trans_loss=5.087, nll_loss=2.308, w2v_ctc_loss=0.684, task_loss=1.408, contrastive_loss=0.056, total=4117.13, n_correct=2610.8, ppl=4.95, accuracy=63.413, wps=11848.1, ups=1.44, wpb=8234.3, bsz=303.7, num_updates=25000, lr=8.94427e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=69, gb_free=17.4, wall=21738
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:0')
2023-08-06 16:45:46 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:3')
2023-08-06 16:46:09 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 4.229 | trans_loss 5.565 | nll_loss 2.845 | w2v_ctc_loss 1.333 | task_loss 4.633 | contrastive_loss 0.258 | total 4003.4 | n_correct 2468 | ppl 7.18 | accuracy 61.648 | uer 17.604 | wer 19.369 | raw_wer 19.369 | bleu 19.77 | wps 2213.4 | wpb 4003.4 | bsz 141.8 | num_updates 25046 | best_bleu 20.02
2023-08-06 16:46:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 25046 updates
2023-08-06 16:46:09 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.7707.pt
2023-08-06 16:46:12 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.7707.pt
2023-08-06 16:46:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.7707.pt (epoch 17 @ 25046 updates, score 19.77) (writing took 16.886574558913708 seconds)
2023-08-06 16:46:27 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-08-06 16:46:27 | INFO | train | epoch 017 | loss 2.045 | trans_loss 5.075 | nll_loss 2.292 | w2v_ctc_loss 0.687 | task_loss 1.403 | contrastive_loss 0.103 | total 4136.92 | n_correct 2632.68 | ppl 4.9 | accuracy 63.639 | wps 10733.3 | ups 1.3 | wpb 8273.8 | bsz 305.1 | num_updates 25046 | lr 8.93605e-05 | gnorm 0.526 | clip 0 | loss_scale 16 | train_wall 1020 | gb_free 16.2 | wall 21811
2023-08-06 16:46:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 16:46:27 | INFO | fairseq.trainer | begin training epoch 18
2023-08-06 16:46:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 16:47:13 | INFO | train_inner | epoch 018:     54 / 1474 loss=2.039, trans_loss=5.07, nll_loss=2.284, w2v_ctc_loss=0.691, task_loss=1.425, contrastive_loss=0.064, total=4138.21, n_correct=2636.14, ppl=4.87, accuracy=63.702, wps=6956.4, ups=0.84, wpb=8276.4, bsz=303.2, num_updates=25100, lr=8.92644e-05, gnorm=0.533, clip=0, loss_scale=16, train_wall=70, gb_free=17.7, wall=21857
2023-08-06 16:48:22 | INFO | train_inner | epoch 018:    154 / 1474 loss=2.036, trans_loss=5.045, nll_loss=2.251, w2v_ctc_loss=0.662, task_loss=1.33, contrastive_loss=0.185, total=4158.88, n_correct=2670.69, ppl=4.76, accuracy=64.217, wps=12051.3, ups=1.45, wpb=8317.8, bsz=314, num_updates=25200, lr=8.90871e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=69, gb_free=16.6, wall=21926
2023-08-06 16:49:32 | INFO | train_inner | epoch 018:    254 / 1474 loss=2.017, trans_loss=5.043, nll_loss=2.249, w2v_ctc_loss=0.67, task_loss=1.36, contrastive_loss=0.055, total=4164.11, n_correct=2683.67, ppl=4.75, accuracy=64.448, wps=11925.3, ups=1.43, wpb=8328.2, bsz=312.5, num_updates=25300, lr=8.89108e-05, gnorm=0.519, clip=0, loss_scale=16, train_wall=69, gb_free=14.8, wall=21996
2023-08-06 16:50:42 | INFO | train_inner | epoch 018:    354 / 1474 loss=2.03, trans_loss=5.055, nll_loss=2.264, w2v_ctc_loss=0.677, task_loss=1.419, contrastive_loss=0.07, total=4163.13, n_correct=2664.07, ppl=4.8, accuracy=63.992, wps=11899.1, ups=1.43, wpb=8326.3, bsz=301.5, num_updates=25400, lr=8.87357e-05, gnorm=0.541, clip=0, loss_scale=16, train_wall=70, gb_free=17.5, wall=22066
2023-08-06 16:51:52 | INFO | train_inner | epoch 018:    454 / 1474 loss=2.043, trans_loss=5.061, nll_loss=2.272, w2v_ctc_loss=0.677, task_loss=1.493, contrastive_loss=0.158, total=4087.83, n_correct=2612.19, ppl=4.83, accuracy=63.902, wps=11598, ups=1.42, wpb=8175.7, bsz=295.3, num_updates=25500, lr=8.85615e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=70, gb_free=16.2, wall=22136
2023-08-06 16:53:02 | INFO | train_inner | epoch 018:    554 / 1474 loss=2.02, trans_loss=5.047, nll_loss=2.255, w2v_ctc_loss=0.671, task_loss=1.258, contrastive_loss=0.069, total=4204.41, n_correct=2703.61, ppl=4.77, accuracy=64.304, wps=11992.5, ups=1.43, wpb=8408.8, bsz=328, num_updates=25600, lr=8.83883e-05, gnorm=0.524, clip=0, loss_scale=16, train_wall=70, gb_free=17.3, wall=22206
2023-08-06 16:54:12 | INFO | train_inner | epoch 018:    654 / 1474 loss=2.045, trans_loss=5.071, nll_loss=2.286, w2v_ctc_loss=0.682, task_loss=1.446, contrastive_loss=0.136, total=4096.81, n_correct=2614.42, ppl=4.88, accuracy=63.816, wps=11738.1, ups=1.43, wpb=8193.6, bsz=298.9, num_updates=25700, lr=8.82162e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=69, gb_free=16.5, wall=22276
2023-08-06 16:55:22 | INFO | train_inner | epoch 018:    754 / 1474 loss=2.055, trans_loss=5.068, nll_loss=2.283, w2v_ctc_loss=0.685, task_loss=1.336, contrastive_loss=0.228, total=4208.29, n_correct=2683.92, ppl=4.87, accuracy=63.777, wps=12031.4, ups=1.43, wpb=8416.6, bsz=322.8, num_updates=25800, lr=8.80451e-05, gnorm=0.518, clip=0, loss_scale=16, train_wall=70, gb_free=17.7, wall=22346
2023-08-06 16:56:32 | INFO | train_inner | epoch 018:    854 / 1474 loss=2.03, trans_loss=5.067, nll_loss=2.281, w2v_ctc_loss=0.678, task_loss=1.424, contrastive_loss=0.047, total=4166.81, n_correct=2662.25, ppl=4.86, accuracy=63.892, wps=11948.2, ups=1.43, wpb=8333.6, bsz=301.9, num_updates=25900, lr=8.7875e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=69, gb_free=12.4, wall=22416
2023-08-06 16:57:41 | INFO | train_inner | epoch 018:    954 / 1474 loss=2.025, trans_loss=5.06, nll_loss=2.273, w2v_ctc_loss=0.669, task_loss=1.302, contrastive_loss=0.068, total=4142.65, n_correct=2654.13, ppl=4.83, accuracy=64.068, wps=11934.7, ups=1.44, wpb=8285.3, bsz=316.1, num_updates=26000, lr=8.77058e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=69, gb_free=14.7, wall=22485
2023-08-06 16:57:41 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 16:58:07 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 4.235 | trans_loss 5.576 | nll_loss 2.855 | w2v_ctc_loss 1.333 | task_loss 4.625 | contrastive_loss 0.251 | total 4003.4 | n_correct 2473.1 | ppl 7.23 | accuracy 61.775 | uer 17.591 | wer 19.343 | raw_wer 19.343 | bleu 19.93 | wps 1883.6 | wpb 4003.4 | bsz 141.8 | num_updates 26000 | best_bleu 20.02
2023-08-06 16:58:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26000 updates
2023-08-06 16:58:07 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_18_26000.pt
2023-08-06 16:58:11 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_18_26000.pt
2023-08-06 16:58:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_18_26000.pt (epoch 18 @ 26000 updates, score 19.93) (writing took 37.079458102583885 seconds)
2023-08-06 16:59:55 | INFO | train_inner | epoch 018:   1054 / 1474 loss=2.029, trans_loss=5.067, nll_loss=2.281, w2v_ctc_loss=0.67, task_loss=1.458, contrastive_loss=0.058, total=4137.77, n_correct=2644.19, ppl=4.86, accuracy=63.904, wps=6186.6, ups=0.75, wpb=8275.5, bsz=300.5, num_updates=26100, lr=8.75376e-05, gnorm=0.523, clip=0, loss_scale=16, train_wall=69, gb_free=17.2, wall=22619
2023-08-06 17:01:05 | INFO | train_inner | epoch 018:   1154 / 1474 loss=2.04, trans_loss=5.056, nll_loss=2.268, w2v_ctc_loss=0.675, task_loss=1.326, contrastive_loss=0.164, total=4153.69, n_correct=2660.01, ppl=4.82, accuracy=64.04, wps=11863.7, ups=1.43, wpb=8307.4, bsz=314.9, num_updates=26200, lr=8.73704e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=70, gb_free=14.9, wall=22689
2023-08-06 17:02:14 | INFO | train_inner | epoch 018:   1254 / 1474 loss=2.036, trans_loss=5.078, nll_loss=2.296, w2v_ctc_loss=0.68, task_loss=1.505, contrastive_loss=0.052, total=4087.62, n_correct=2601.34, ppl=4.91, accuracy=63.639, wps=11899.2, ups=1.46, wpb=8175.2, bsz=287.1, num_updates=26300, lr=8.72041e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=68, gb_free=16.5, wall=22758
2023-08-06 17:03:23 | INFO | train_inner | epoch 018:   1354 / 1474 loss=2.048, trans_loss=5.085, nll_loss=2.305, w2v_ctc_loss=0.694, task_loss=1.495, contrastive_loss=0.075, total=4070.69, n_correct=2583.33, ppl=4.94, accuracy=63.462, wps=11693.9, ups=1.44, wpb=8141.4, bsz=291.7, num_updates=26400, lr=8.70388e-05, gnorm=0.535, clip=0, loss_scale=16, train_wall=69, gb_free=17.3, wall=22827
2023-08-06 17:04:33 | INFO | train_inner | epoch 018:   1454 / 1474 loss=2.038, trans_loss=5.077, nll_loss=2.295, w2v_ctc_loss=0.685, task_loss=1.476, contrastive_loss=0.062, total=4113.2, n_correct=2618.81, ppl=4.91, accuracy=63.668, wps=11860.5, ups=1.44, wpb=8226.4, bsz=297.5, num_updates=26500, lr=8.68744e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=69, gb_free=16.3, wall=22897
2023-08-06 17:04:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 17:05:10 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 4.238 | trans_loss 5.566 | nll_loss 2.846 | w2v_ctc_loss 1.372 | task_loss 4.614 | contrastive_loss 0.245 | total 4003.4 | n_correct 2476.7 | ppl 7.19 | accuracy 61.865 | uer 17.724 | wer 19.444 | raw_wer 19.444 | bleu 20.13 | wps 2104.6 | wpb 4003.4 | bsz 141.8 | num_updates 26520 | best_bleu 20.13
2023-08-06 17:05:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26520 updates
2023-08-06 17:05:10 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 17:05:23 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 17:05:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt (epoch 18 @ 26520 updates, score 20.13) (writing took 23.58875730447471 seconds)
2023-08-06 17:05:35 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-08-06 17:05:35 | INFO | train | epoch 018 | loss 2.035 | trans_loss 5.063 | nll_loss 2.275 | w2v_ctc_loss 0.677 | task_loss 1.4 | contrastive_loss 0.105 | total 4138.65 | n_correct 2646.17 | ppl 4.84 | accuracy 63.938 | wps 10628.2 | ups 1.28 | wpb 8277.3 | bsz 305.7 | num_updates 26520 | lr 8.68417e-05 | gnorm 0.528 | clip 0 | loss_scale 16 | train_wall 1021 | gb_free 15.8 | wall 22959
2023-08-06 17:05:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 17:05:35 | INFO | fairseq.trainer | begin training epoch 19
2023-08-06 17:05:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 17:06:38 | INFO | train_inner | epoch 019:     80 / 1474 loss=2.025, trans_loss=5.039, nll_loss=2.244, w2v_ctc_loss=0.669, task_loss=1.404, contrastive_loss=0.114, total=4102.06, n_correct=2639.67, ppl=4.74, accuracy=64.35, wps=6563.6, ups=0.8, wpb=8204.1, bsz=296.9, num_updates=26600, lr=8.6711e-05, gnorm=0.531, clip=0, loss_scale=16, train_wall=69, gb_free=17.4, wall=23022
2023-08-06 17:07:47 | INFO | train_inner | epoch 019:    180 / 1474 loss=2.022, trans_loss=5.032, nll_loss=2.236, w2v_ctc_loss=0.678, task_loss=1.303, contrastive_loss=0.099, total=4227.7, n_correct=2731.2, ppl=4.71, accuracy=64.603, wps=12129.7, ups=1.43, wpb=8455.4, bsz=324.8, num_updates=26700, lr=8.65485e-05, gnorm=0.517, clip=0, loss_scale=16, train_wall=69, gb_free=16.9, wall=23092
2023-08-06 17:08:57 | INFO | train_inner | epoch 019:    280 / 1474 loss=2.013, trans_loss=5.033, nll_loss=2.236, w2v_ctc_loss=0.67, task_loss=1.381, contrastive_loss=0.048, total=4187.34, n_correct=2703.28, ppl=4.71, accuracy=64.558, wps=12025.2, ups=1.44, wpb=8374.7, bsz=306.4, num_updates=26800, lr=8.63868e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=23161
2023-08-06 17:10:06 | INFO | train_inner | epoch 019:    380 / 1474 loss=2.029, trans_loss=5.041, nll_loss=2.248, w2v_ctc_loss=0.663, task_loss=1.379, contrastive_loss=0.154, total=4170.52, n_correct=2685.01, ppl=4.75, accuracy=64.381, wps=12034.1, ups=1.44, wpb=8341, bsz=311, num_updates=26900, lr=8.62261e-05, gnorm=0.521, clip=0, loss_scale=32, train_wall=69, gb_free=16.2, wall=23231
2023-08-06 17:11:15 | INFO | train_inner | epoch 019:    480 / 1474 loss=2.024, trans_loss=5.048, nll_loss=2.256, w2v_ctc_loss=0.676, task_loss=1.435, contrastive_loss=0.063, total=4113.89, n_correct=2643.64, ppl=4.78, accuracy=64.261, wps=11918.7, ups=1.45, wpb=8227.8, bsz=301.5, num_updates=27000, lr=8.60663e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=69, gb_free=17.1, wall=23300
2023-08-06 17:12:25 | INFO | train_inner | epoch 019:    580 / 1474 loss=2.024, trans_loss=5.043, nll_loss=2.251, w2v_ctc_loss=0.666, task_loss=1.371, contrastive_loss=0.125, total=4128.58, n_correct=2657.68, ppl=4.76, accuracy=64.373, wps=11937.6, ups=1.45, wpb=8257.2, bsz=306.2, num_updates=27100, lr=8.59074e-05, gnorm=0.526, clip=0, loss_scale=32, train_wall=69, gb_free=16.5, wall=23369
2023-08-06 17:13:34 | INFO | train_inner | epoch 019:    680 / 1474 loss=2.007, trans_loss=5.044, nll_loss=2.252, w2v_ctc_loss=0.652, task_loss=1.274, contrastive_loss=0.054, total=4201.56, n_correct=2710.05, ppl=4.76, accuracy=64.501, wps=12120.6, ups=1.44, wpb=8403.1, bsz=321.5, num_updates=27200, lr=8.57493e-05, gnorm=0.514, clip=0, loss_scale=32, train_wall=69, gb_free=17.7, wall=23438
2023-08-06 17:14:44 | INFO | train_inner | epoch 019:    780 / 1474 loss=2.023, trans_loss=5.049, nll_loss=2.258, w2v_ctc_loss=0.676, task_loss=1.442, contrastive_loss=0.059, total=4124.03, n_correct=2652.45, ppl=4.78, accuracy=64.317, wps=11831.4, ups=1.43, wpb=8248.1, bsz=299, num_updates=27300, lr=8.55921e-05, gnorm=0.519, clip=0, loss_scale=32, train_wall=69, gb_free=17.4, wall=23508
2023-08-06 17:15:53 | INFO | train_inner | epoch 019:    880 / 1474 loss=2.024, trans_loss=5.058, nll_loss=2.27, w2v_ctc_loss=0.674, task_loss=1.394, contrastive_loss=0.056, total=4177.8, n_correct=2677.96, ppl=4.82, accuracy=64.1, wps=11994.2, ups=1.44, wpb=8355.6, bsz=309.6, num_updates=27400, lr=8.54358e-05, gnorm=0.521, clip=0, loss_scale=32, train_wall=69, gb_free=14.7, wall=23577
2023-08-06 17:17:03 | INFO | train_inner | epoch 019:    980 / 1474 loss=2.057, trans_loss=5.069, nll_loss=2.285, w2v_ctc_loss=0.672, task_loss=1.42, contrastive_loss=0.286, total=4084.26, n_correct=2606.37, ppl=4.87, accuracy=63.815, wps=11669.8, ups=1.43, wpb=8168.5, bsz=305.8, num_updates=27500, lr=8.52803e-05, gnorm=0.536, clip=0, loss_scale=32, train_wall=70, gb_free=16, wall=23647
2023-08-06 17:18:12 | INFO | train_inner | epoch 019:   1080 / 1474 loss=2.032, trans_loss=5.067, nll_loss=2.281, w2v_ctc_loss=0.671, task_loss=1.473, contrastive_loss=0.093, total=4042.73, n_correct=2584.26, ppl=4.86, accuracy=63.924, wps=11753.6, ups=1.45, wpb=8085.5, bsz=294, num_updates=27600, lr=8.51257e-05, gnorm=0.537, clip=0, loss_scale=32, train_wall=68, gb_free=17.2, wall=23716
2023-08-06 17:19:23 | INFO | train_inner | epoch 019:   1180 / 1474 loss=2.049, trans_loss=5.067, nll_loss=2.282, w2v_ctc_loss=0.679, task_loss=1.428, contrastive_loss=0.178, total=4140.95, n_correct=2640.38, ppl=4.86, accuracy=63.763, wps=11760.2, ups=1.42, wpb=8281.9, bsz=307.9, num_updates=27700, lr=8.49719e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=70, gb_free=12.6, wall=23787
2023-08-06 17:20:31 | INFO | train_inner | epoch 019:   1280 / 1474 loss=2.028, trans_loss=5.068, nll_loss=2.283, w2v_ctc_loss=0.665, task_loss=1.417, contrastive_loss=0.074, total=4135.79, n_correct=2645.21, ppl=4.87, accuracy=63.959, wps=12041.3, ups=1.46, wpb=8271.6, bsz=299.5, num_updates=27800, lr=8.48189e-05, gnorm=0.521, clip=0, loss_scale=32, train_wall=68, gb_free=17.8, wall=23855
2023-08-06 17:21:41 | INFO | train_inner | epoch 019:   1380 / 1474 loss=2.027, trans_loss=5.061, nll_loss=2.275, w2v_ctc_loss=0.674, task_loss=1.43, contrastive_loss=0.06, total=4138.67, n_correct=2650.99, ppl=4.84, accuracy=64.054, wps=11895.6, ups=1.44, wpb=8277.3, bsz=301.6, num_updates=27900, lr=8.46668e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=69, gb_free=16.4, wall=23925
2023-08-06 17:22:46 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 17:23:11 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 4.221 | trans_loss 5.558 | nll_loss 2.834 | w2v_ctc_loss 1.332 | task_loss 4.651 | contrastive_loss 0.249 | total 4003.4 | n_correct 2483.9 | ppl 7.13 | accuracy 62.045 | uer 17.132 | wer 18.773 | raw_wer 18.773 | bleu 19.99 | wps 1965.4 | wpb 4003.4 | bsz 141.8 | num_updates 27994 | best_bleu 20.13
2023-08-06 17:23:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 27994 updates
2023-08-06 17:23:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.9909.pt
2023-08-06 17:23:14 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.9909.pt
2023-08-06 17:23:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.9909.pt (epoch 19 @ 27994 updates, score 19.99) (writing took 33.500575510784984 seconds)
2023-08-06 17:23:45 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-08-06 17:23:45 | INFO | train | epoch 019 | loss 2.027 | trans_loss 5.051 | nll_loss 2.261 | w2v_ctc_loss 0.67 | task_loss 1.399 | contrastive_loss 0.104 | total 4138.65 | n_correct 2657.75 | ppl 4.79 | accuracy 64.218 | wps 11191.9 | ups 1.35 | wpb 8277.3 | bsz 305.7 | num_updates 27994 | lr 8.45245e-05 | gnorm 0.527 | clip 0 | loss_scale 32 | train_wall 1017 | gb_free 17.3 | wall 24049
2023-08-06 17:23:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 17:23:45 | INFO | fairseq.trainer | begin training epoch 20
2023-08-06 17:23:45 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 17:23:57 | INFO | train_inner | epoch 020:      6 / 1474 loss=2.031, trans_loss=5.055, nll_loss=2.267, w2v_ctc_loss=0.665, task_loss=1.421, contrastive_loss=0.146, total=4117.61, n_correct=2644.56, ppl=4.81, accuracy=64.226, wps=6065.3, ups=0.74, wpb=8235.2, bsz=303, num_updates=28000, lr=8.45154e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=69, gb_free=16.3, wall=24061
2023-08-06 17:23:57 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 17:24:21 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 4.221 | trans_loss 5.558 | nll_loss 2.831 | w2v_ctc_loss 1.333 | task_loss 4.653 | contrastive_loss 0.247 | total 4003.4 | n_correct 2484.1 | ppl 7.12 | accuracy 62.05 | uer 17.206 | wer 18.881 | raw_wer 18.881 | bleu 20.27 | wps 2096.2 | wpb 4003.4 | bsz 141.8 | num_updates 28000 | best_bleu 20.27
2023-08-06 17:24:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 28000 updates
2023-08-06 17:24:21 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_20_28000.pt
2023-08-06 17:24:25 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_20_28000.pt
2023-08-06 17:25:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_20_28000.pt (epoch 20 @ 28000 updates, score 20.27) (writing took 39.98596560023725 seconds)
2023-08-06 17:26:12 | INFO | train_inner | epoch 020:    106 / 1474 loss=2.003, trans_loss=5.018, nll_loss=2.217, w2v_ctc_loss=0.654, task_loss=1.358, contrastive_loss=0.065, total=4192.82, n_correct=2720.1, ppl=4.65, accuracy=64.875, wps=6209.1, ups=0.74, wpb=8385.6, bsz=312.8, num_updates=28100, lr=8.43649e-05, gnorm=0.524, clip=0, loss_scale=32, train_wall=70, gb_free=16.1, wall=24196
2023-08-06 17:27:21 | INFO | train_inner | epoch 020:    206 / 1474 loss=2.014, trans_loss=5.027, nll_loss=2.229, w2v_ctc_loss=0.659, task_loss=1.449, contrastive_loss=0.119, total=4155.9, n_correct=2690.47, ppl=4.69, accuracy=64.739, wps=11976.5, ups=1.44, wpb=8311.8, bsz=302.3, num_updates=28200, lr=8.42152e-05, gnorm=0.52, clip=0, loss_scale=32, train_wall=69, gb_free=11.5, wall=24265
2023-08-06 17:28:30 | INFO | train_inner | epoch 020:    306 / 1474 loss=2.002, trans_loss=5.021, nll_loss=2.222, w2v_ctc_loss=0.662, task_loss=1.254, contrastive_loss=0.056, total=4192.69, n_correct=2721.96, ppl=4.67, accuracy=64.922, wps=12107.6, ups=1.44, wpb=8385.4, bsz=327.6, num_updates=28300, lr=8.40663e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=69, gb_free=16.9, wall=24334
2023-08-06 17:29:40 | INFO | train_inner | epoch 020:    406 / 1474 loss=2.006, trans_loss=5.027, nll_loss=2.229, w2v_ctc_loss=0.654, task_loss=1.421, contrastive_loss=0.057, total=4116.96, n_correct=2667.2, ppl=4.69, accuracy=64.786, wps=11870.2, ups=1.44, wpb=8233.9, bsz=296.8, num_updates=28400, lr=8.39181e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=69, gb_free=12.5, wall=24404
2023-08-06 17:30:49 | INFO | train_inner | epoch 020:    506 / 1474 loss=2.024, trans_loss=5.043, nll_loss=2.25, w2v_ctc_loss=0.659, task_loss=1.442, contrastive_loss=0.144, total=4100.73, n_correct=2644.85, ppl=4.76, accuracy=64.497, wps=11830.3, ups=1.44, wpb=8201.5, bsz=298.4, num_updates=28500, lr=8.37708e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=69, gb_free=16.1, wall=24473
2023-08-06 17:31:58 | INFO | train_inner | epoch 020:    606 / 1474 loss=2.029, trans_loss=5.042, nll_loss=2.249, w2v_ctc_loss=0.665, task_loss=1.457, contrastive_loss=0.141, total=4101.99, n_correct=2639.22, ppl=4.76, accuracy=64.34, wps=11844.4, ups=1.44, wpb=8204, bsz=298.3, num_updates=28600, lr=8.36242e-05, gnorm=0.536, clip=0, loss_scale=32, train_wall=69, gb_free=13.3, wall=24542
2023-08-06 17:33:07 | INFO | train_inner | epoch 020:    706 / 1474 loss=2.016, trans_loss=5.043, nll_loss=2.25, w2v_ctc_loss=0.668, task_loss=1.422, contrastive_loss=0.049, total=4124.25, n_correct=2656.49, ppl=4.76, accuracy=64.411, wps=11940.3, ups=1.45, wpb=8248.5, bsz=297.2, num_updates=28700, lr=8.34784e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=69, gb_free=16.6, wall=24612
2023-08-06 17:34:17 | INFO | train_inner | epoch 020:    806 / 1474 loss=2.014, trans_loss=5.04, nll_loss=2.247, w2v_ctc_loss=0.669, task_loss=1.373, contrastive_loss=0.053, total=4153.23, n_correct=2680.2, ppl=4.75, accuracy=64.533, wps=11987.2, ups=1.44, wpb=8306.5, bsz=308.5, num_updates=28800, lr=8.33333e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=69, gb_free=17.6, wall=24681
2023-08-06 17:35:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-06 17:35:28 | INFO | train_inner | epoch 020:    907 / 1474 loss=2.042, trans_loss=5.055, nll_loss=2.267, w2v_ctc_loss=0.671, task_loss=1.391, contrastive_loss=0.187, total=4132.98, n_correct=2649.58, ppl=4.81, accuracy=64.108, wps=11643.6, ups=1.41, wpb=8266, bsz=311.4, num_updates=28900, lr=8.3189e-05, gnorm=0.538, clip=0, loss_scale=32, train_wall=71, gb_free=17.6, wall=24752
2023-08-06 17:36:38 | INFO | train_inner | epoch 020:   1007 / 1474 loss=2.011, trans_loss=5.041, nll_loss=2.249, w2v_ctc_loss=0.656, task_loss=1.38, contrastive_loss=0.058, total=4171.86, n_correct=2690.64, ppl=4.75, accuracy=64.495, wps=11884.7, ups=1.42, wpb=8343.7, bsz=308.6, num_updates=29000, lr=8.30455e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=70, gb_free=15.5, wall=24822
2023-08-06 17:37:48 | INFO | train_inner | epoch 020:   1107 / 1474 loss=2.035, trans_loss=5.049, nll_loss=2.26, w2v_ctc_loss=0.663, task_loss=1.353, contrastive_loss=0.191, total=4162.96, n_correct=2674.91, ppl=4.79, accuracy=64.255, wps=11954.5, ups=1.44, wpb=8325.9, bsz=314.9, num_updates=29100, lr=8.29027e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=69, gb_free=16.9, wall=24892
2023-08-06 17:38:57 | INFO | train_inner | epoch 020:   1207 / 1474 loss=2.018, trans_loss=5.042, nll_loss=2.249, w2v_ctc_loss=0.674, task_loss=1.529, contrastive_loss=0.047, total=4033.74, n_correct=2595.47, ppl=4.75, accuracy=64.344, wps=11629.9, ups=1.44, wpb=8067.5, bsz=285.2, num_updates=29200, lr=8.27606e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=69, gb_free=17.1, wall=24961
2023-08-06 17:40:07 | INFO | train_inner | epoch 020:   1307 / 1474 loss=2.017, trans_loss=5.053, nll_loss=2.264, w2v_ctc_loss=0.663, task_loss=1.479, contrastive_loss=0.052, total=4124.42, n_correct=2652.12, ppl=4.8, accuracy=64.303, wps=11798.1, ups=1.43, wpb=8248.8, bsz=297, num_updates=29300, lr=8.26192e-05, gnorm=0.526, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=25031
2023-08-06 17:41:17 | INFO | train_inner | epoch 020:   1407 / 1474 loss=2.018, trans_loss=5.051, nll_loss=2.262, w2v_ctc_loss=0.664, task_loss=1.482, contrastive_loss=0.051, total=4114.1, n_correct=2643.34, ppl=4.8, accuracy=64.251, wps=11775.7, ups=1.43, wpb=8228.2, bsz=293.7, num_updates=29400, lr=8.24786e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=69, gb_free=14.3, wall=25101
2023-08-06 17:42:03 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 17:42:28 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 4.217 | trans_loss 5.56 | nll_loss 2.835 | w2v_ctc_loss 1.315 | task_loss 4.63 | contrastive_loss 0.247 | total 4003.4 | n_correct 2476.2 | ppl 7.13 | accuracy 61.852 | uer 17.171 | wer 18.911 | raw_wer 18.911 | bleu 20.06 | wps 1967.3 | wpb 4003.4 | bsz 141.8 | num_updates 29467 | best_bleu 20.27
2023-08-06 17:42:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 29467 updates
2023-08-06 17:42:28 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.0603.pt
2023-08-06 17:42:31 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.0603.pt
2023-08-06 17:42:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.0603.pt (epoch 20 @ 29467 updates, score 20.06) (writing took 13.566748216748238 seconds)
2023-08-06 17:42:42 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-08-06 17:42:42 | INFO | train | epoch 020 | loss 2.018 | trans_loss 5.04 | nll_loss 2.247 | w2v_ctc_loss 0.663 | task_loss 1.403 | contrastive_loss 0.091 | total 4137.15 | n_correct 2667.89 | ppl 4.75 | accuracy 64.486 | wps 10721.5 | ups 1.3 | wpb 8274.3 | bsz 305 | num_updates 29467 | lr 8.23848e-05 | gnorm 0.53 | clip 0 | loss_scale 32 | train_wall 1018 | gb_free 16 | wall 25186
2023-08-06 17:42:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 17:42:42 | INFO | fairseq.trainer | begin training epoch 21
2023-08-06 17:42:42 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 17:43:12 | INFO | train_inner | epoch 021:     33 / 1474 loss=2.027, trans_loss=5.043, nll_loss=2.253, w2v_ctc_loss=0.66, task_loss=1.324, contrastive_loss=0.168, total=4155.01, n_correct=2675.95, ppl=4.77, accuracy=64.403, wps=7177.3, ups=0.86, wpb=8310, bsz=317.6, num_updates=29500, lr=8.23387e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=69, gb_free=16.1, wall=25217
2023-08-06 17:44:22 | INFO | train_inner | epoch 021:    133 / 1474 loss=2.01, trans_loss=5.01, nll_loss=2.207, w2v_ctc_loss=0.65, task_loss=1.323, contrastive_loss=0.16, total=4186.67, n_correct=2720.77, ppl=4.62, accuracy=64.986, wps=12040.1, ups=1.44, wpb=8373.3, bsz=317.4, num_updates=29600, lr=8.21995e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=69, gb_free=12.8, wall=25286
2023-08-06 17:45:31 | INFO | train_inner | epoch 021:    233 / 1474 loss=2, trans_loss=5.017, nll_loss=2.217, w2v_ctc_loss=0.642, task_loss=1.319, contrastive_loss=0.116, total=4166.37, n_correct=2707.66, ppl=4.65, accuracy=64.988, wps=12122.6, ups=1.45, wpb=8332.7, bsz=315.2, num_updates=29700, lr=8.2061e-05, gnorm=0.524, clip=0, loss_scale=32, train_wall=68, gb_free=13.9, wall=25355
2023-08-06 17:46:41 | INFO | train_inner | epoch 021:    333 / 1474 loss=2.009, trans_loss=5.018, nll_loss=2.219, w2v_ctc_loss=0.656, task_loss=1.417, contrastive_loss=0.117, total=4132.25, n_correct=2682.32, ppl=4.65, accuracy=64.912, wps=11699.5, ups=1.42, wpb=8264.5, bsz=305, num_updates=29800, lr=8.19232e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=70, gb_free=16.5, wall=25426
2023-08-06 17:47:50 | INFO | train_inner | epoch 021:    433 / 1474 loss=1.995, trans_loss=5.016, nll_loss=2.216, w2v_ctc_loss=0.644, task_loss=1.333, contrastive_loss=0.051, total=4195.53, n_correct=2729.95, ppl=4.65, accuracy=65.068, wps=12155, ups=1.45, wpb=8391.1, bsz=311.6, num_updates=29900, lr=8.17861e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=69, gb_free=16.2, wall=25495
2023-08-06 17:49:00 | INFO | train_inner | epoch 021:    533 / 1474 loss=1.999, trans_loss=5.017, nll_loss=2.216, w2v_ctc_loss=0.656, task_loss=1.445, contrastive_loss=0.045, total=4085.05, n_correct=2654.43, ppl=4.65, accuracy=64.979, wps=11745.1, ups=1.44, wpb=8170.1, bsz=296.1, num_updates=30000, lr=8.16497e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=69, gb_free=16.1, wall=25564
2023-08-06 17:49:00 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 17:49:24 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 4.207 | trans_loss 5.569 | nll_loss 2.844 | w2v_ctc_loss 1.266 | task_loss 4.635 | contrastive_loss 0.244 | total 4003.4 | n_correct 2476 | ppl 7.18 | accuracy 61.847 | uer 17.129 | wer 18.97 | raw_wer 18.97 | bleu 19.97 | wps 2066.3 | wpb 4003.4 | bsz 141.8 | num_updates 30000 | best_bleu 20.27
2023-08-06 17:49:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30000 updates
2023-08-06 17:49:24 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_21_30000.pt
2023-08-06 17:49:27 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_21_30000.pt
2023-08-06 17:49:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_21_30000.pt (epoch 21 @ 30000 updates, score 19.97) (writing took 16.14483362995088 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:0')
2023-08-06 17:50:51 | INFO | train_inner | epoch 021:    633 / 1474 loss=2.021, trans_loss=5.026, nll_loss=2.229, w2v_ctc_loss=0.647, task_loss=1.383, contrastive_loss=0.216, total=4220.3, n_correct=2732.91, ppl=4.69, accuracy=64.756, wps=7622.8, ups=0.9, wpb=8440.6, bsz=315.8, num_updates=30100, lr=8.15139e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=70, gb_free=15.6, wall=25675
2023-08-06 17:52:01 | INFO | train_inner | epoch 021:    733 / 1474 loss=2.01, trans_loss=5.036, nll_loss=2.242, w2v_ctc_loss=0.654, task_loss=1.4, contrastive_loss=0.075, total=4148.18, n_correct=2680.27, ppl=4.73, accuracy=64.613, wps=11893.4, ups=1.43, wpb=8296.4, bsz=308.3, num_updates=30200, lr=8.13788e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=69, gb_free=11.7, wall=25745
2023-08-06 17:53:11 | INFO | train_inner | epoch 021:    833 / 1474 loss=2.014, trans_loss=5.038, nll_loss=2.244, w2v_ctc_loss=0.654, task_loss=1.49, contrastive_loss=0.088, total=4062.56, n_correct=2624.36, ppl=4.74, accuracy=64.599, wps=11587.3, ups=1.43, wpb=8125.1, bsz=293, num_updates=30300, lr=8.12444e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=70, gb_free=16, wall=25815
2023-08-06 17:54:20 | INFO | train_inner | epoch 021:    933 / 1474 loss=2.004, trans_loss=5.028, nll_loss=2.232, w2v_ctc_loss=0.654, task_loss=1.396, contrastive_loss=0.062, total=4103.66, n_correct=2657.12, ppl=4.7, accuracy=64.75, wps=11904.4, ups=1.45, wpb=8207.3, bsz=301.5, num_updates=30400, lr=8.11107e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=68, gb_free=17.1, wall=25884
2023-08-06 17:55:29 | INFO | train_inner | epoch 021:   1033 / 1474 loss=2.012, trans_loss=5.046, nll_loss=2.255, w2v_ctc_loss=0.657, task_loss=1.432, contrastive_loss=0.059, total=4100.54, n_correct=2641.72, ppl=4.77, accuracy=64.424, wps=11872.3, ups=1.45, wpb=8201.1, bsz=298.2, num_updates=30500, lr=8.09776e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=69, gb_free=17.8, wall=25953
2023-08-06 17:56:38 | INFO | train_inner | epoch 021:   1133 / 1474 loss=2.01, trans_loss=5.035, nll_loss=2.239, w2v_ctc_loss=0.657, task_loss=1.505, contrastive_loss=0.062, total=4119.98, n_correct=2662.62, ppl=4.72, accuracy=64.627, wps=11903, ups=1.44, wpb=8240, bsz=294, num_updates=30600, lr=8.08452e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=69, gb_free=17.8, wall=26022
2023-08-06 17:57:47 | INFO | train_inner | epoch 021:   1233 / 1474 loss=2.014, trans_loss=5.036, nll_loss=2.244, w2v_ctc_loss=0.653, task_loss=1.322, contrastive_loss=0.114, total=4161.49, n_correct=2690.48, ppl=4.74, accuracy=64.652, wps=12037.6, ups=1.45, wpb=8323, bsz=313, num_updates=30700, lr=8.07134e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=69, gb_free=16.6, wall=26091
2023-08-06 17:58:56 | INFO | train_inner | epoch 021:   1333 / 1474 loss=2.011, trans_loss=5.037, nll_loss=2.245, w2v_ctc_loss=0.658, task_loss=1.353, contrastive_loss=0.074, total=4141.76, n_correct=2676.13, ppl=4.74, accuracy=64.613, wps=11943.7, ups=1.44, wpb=8283.5, bsz=311.7, num_updates=30800, lr=8.05823e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=69, gb_free=17.2, wall=26161
2023-08-06 18:00:06 | INFO | train_inner | epoch 021:   1433 / 1474 loss=2.031, trans_loss=5.049, nll_loss=2.259, w2v_ctc_loss=0.675, task_loss=1.477, contrastive_loss=0.122, total=4127.02, n_correct=2652.97, ppl=4.79, accuracy=64.283, wps=11825, ups=1.43, wpb=8254, bsz=302.1, num_updates=30900, lr=8.04518e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=69, gb_free=16.3, wall=26230
2023-08-06 18:00:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:6')
2023-08-06 18:01:00 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 4.213 | trans_loss 5.563 | nll_loss 2.839 | w2v_ctc_loss 1.296 | task_loss 4.615 | contrastive_loss 0.245 | total 4003.4 | n_correct 2482.5 | ppl 7.15 | accuracy 62.01 | uer 17.325 | wer 19.302 | raw_wer 19.302 | bleu 19.95 | wps 1939.2 | wpb 4003.4 | bsz 141.8 | num_updates 30941 | best_bleu 20.27
2023-08-06 18:01:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30941 updates
2023-08-06 18:01:00 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.9505.pt
2023-08-06 18:01:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.9505.pt
2023-08-06 18:01:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_19.9505.pt (epoch 21 @ 30941 updates, score 19.95) (writing took 15.423419505357742 seconds)
2023-08-06 18:01:16 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-08-06 18:01:16 | INFO | train | epoch 021 | loss 2.011 | trans_loss 5.029 | nll_loss 2.233 | w2v_ctc_loss 0.654 | task_loss 1.4 | contrastive_loss 0.102 | total 4138.65 | n_correct 2678.79 | ppl 4.7 | accuracy 64.726 | wps 10949.2 | ups 1.32 | wpb 8277.3 | bsz 305.7 | num_updates 30941 | lr 8.03985e-05 | gnorm 0.53 | clip 0 | loss_scale 64 | train_wall 1018 | gb_free 15.3 | wall 26300
2023-08-06 18:01:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 18:01:16 | INFO | fairseq.trainer | begin training epoch 22
2023-08-06 18:01:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 18:02:04 | INFO | train_inner | epoch 022:     59 / 1474 loss=1.999, trans_loss=5.019, nll_loss=2.22, w2v_ctc_loss=0.655, task_loss=1.41, contrastive_loss=0.045, total=4140.16, n_correct=2690.66, ppl=4.66, accuracy=64.989, wps=7020.7, ups=0.85, wpb=8280.3, bsz=300.1, num_updates=31000, lr=8.03219e-05, gnorm=0.527, clip=0, loss_scale=64, train_wall=69, gb_free=17.6, wall=26348
2023-08-06 18:03:14 | INFO | train_inner | epoch 022:    159 / 1474 loss=2.001, trans_loss=5.005, nll_loss=2.202, w2v_ctc_loss=0.646, task_loss=1.408, contrastive_loss=0.124, total=4115.86, n_correct=2678.4, ppl=4.6, accuracy=65.075, wps=11796.6, ups=1.43, wpb=8231.7, bsz=309.4, num_updates=31100, lr=8.01927e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=69, gb_free=17.2, wall=26418
2023-08-06 18:04:23 | INFO | train_inner | epoch 022:    259 / 1474 loss=1.984, trans_loss=5.001, nll_loss=2.198, w2v_ctc_loss=0.634, task_loss=1.267, contrastive_loss=0.056, total=4247.73, n_correct=2775.88, ppl=4.59, accuracy=65.35, wps=12235.9, ups=1.44, wpb=8495.5, bsz=323.2, num_updates=31200, lr=8.00641e-05, gnorm=0.518, clip=0, loss_scale=64, train_wall=69, gb_free=13.7, wall=26488
2023-08-06 18:05:34 | INFO | train_inner | epoch 022:    359 / 1474 loss=2.021, trans_loss=5.014, nll_loss=2.213, w2v_ctc_loss=0.648, task_loss=1.377, contrastive_loss=0.223, total=4212.22, n_correct=2737.46, ppl=4.64, accuracy=64.989, wps=11944.7, ups=1.42, wpb=8424.4, bsz=317.9, num_updates=31300, lr=7.99361e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=70, gb_free=15.4, wall=26558
2023-08-06 18:06:43 | INFO | train_inner | epoch 022:    459 / 1474 loss=2.009, trans_loss=5.022, nll_loss=2.222, w2v_ctc_loss=0.652, task_loss=1.472, contrastive_loss=0.104, total=4131.12, n_correct=2680.08, ppl=4.67, accuracy=64.875, wps=11920.9, ups=1.44, wpb=8262.2, bsz=297.3, num_updates=31400, lr=7.98087e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=16.3, wall=26627
2023-08-06 18:07:53 | INFO | train_inner | epoch 022:    559 / 1474 loss=1.997, trans_loss=5.015, nll_loss=2.214, w2v_ctc_loss=0.65, task_loss=1.406, contrastive_loss=0.055, total=4153.54, n_correct=2702.4, ppl=4.64, accuracy=65.063, wps=11840.3, ups=1.43, wpb=8307.1, bsz=307.1, num_updates=31500, lr=7.96819e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=70, gb_free=14.8, wall=26698
2023-08-06 18:09:02 | INFO | train_inner | epoch 022:    659 / 1474 loss=1.997, trans_loss=5.008, nll_loss=2.206, w2v_ctc_loss=0.633, task_loss=1.326, contrastive_loss=0.139, total=4143.91, n_correct=2705.17, ppl=4.61, accuracy=65.281, wps=12126.5, ups=1.46, wpb=8287.8, bsz=313.1, num_updates=31600, lr=7.95557e-05, gnorm=0.521, clip=0, loss_scale=64, train_wall=68, gb_free=15.7, wall=26766
2023-08-06 18:10:12 | INFO | train_inner | epoch 022:    759 / 1474 loss=1.998, trans_loss=5.014, nll_loss=2.214, w2v_ctc_loss=0.651, task_loss=1.44, contrastive_loss=0.058, total=4168.91, n_correct=2713.5, ppl=4.64, accuracy=65.089, wps=11943.8, ups=1.43, wpb=8337.8, bsz=303.5, num_updates=31700, lr=7.94301e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=69, gb_free=17.4, wall=26836
2023-08-06 18:11:21 | INFO | train_inner | epoch 022:    859 / 1474 loss=2.005, trans_loss=5.029, nll_loss=2.234, w2v_ctc_loss=0.655, task_loss=1.521, contrastive_loss=0.046, total=4079.59, n_correct=2638.38, ppl=4.7, accuracy=64.673, wps=11669.1, ups=1.43, wpb=8159.2, bsz=288.7, num_updates=31800, lr=7.93052e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=17.2, wall=26906
2023-08-06 18:12:31 | INFO | train_inner | epoch 022:    959 / 1474 loss=1.992, trans_loss=5.019, nll_loss=2.221, w2v_ctc_loss=0.641, task_loss=1.404, contrastive_loss=0.046, total=4129.75, n_correct=2687.36, ppl=4.66, accuracy=65.073, wps=11842.1, ups=1.43, wpb=8259.5, bsz=303.9, num_updates=31900, lr=7.91808e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=69, gb_free=17.7, wall=26975
2023-08-06 18:13:40 | INFO | train_inner | epoch 022:   1059 / 1474 loss=2.012, trans_loss=5.019, nll_loss=2.221, w2v_ctc_loss=0.641, task_loss=1.332, contrastive_loss=0.214, total=4155.56, n_correct=2700.98, ppl=4.66, accuracy=64.997, wps=12017.5, ups=1.45, wpb=8311.1, bsz=315.3, num_updates=32000, lr=7.90569e-05, gnorm=0.526, clip=0, loss_scale=64, train_wall=69, gb_free=16.9, wall=27045
2023-08-06 18:13:40 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 18:14:05 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 4.209 | trans_loss 5.561 | nll_loss 2.836 | w2v_ctc_loss 1.289 | task_loss 4.601 | contrastive_loss 0.242 | total 4003.4 | n_correct 2478.2 | ppl 7.14 | accuracy 61.902 | uer 17.225 | wer 19.049 | raw_wer 19.049 | bleu 20.05 | wps 2045.7 | wpb 4003.4 | bsz 141.8 | num_updates 32000 | best_bleu 20.27
2023-08-06 18:14:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32000 updates
2023-08-06 18:14:05 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_22_32000.pt
2023-08-06 18:14:09 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_22_32000.pt
2023-08-06 18:14:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_22_32000.pt (epoch 22 @ 32000 updates, score 20.05) (writing took 31.929878508672118 seconds)
2023-08-06 18:15:48 | INFO | train_inner | epoch 022:   1159 / 1474 loss=2.018, trans_loss=5.042, nll_loss=2.25, w2v_ctc_loss=0.662, task_loss=1.47, contrastive_loss=0.093, total=4089.92, n_correct=2638.75, ppl=4.76, accuracy=64.518, wps=6406.1, ups=0.78, wpb=8179.8, bsz=292.2, num_updates=32100, lr=7.89337e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=69, gb_free=16, wall=27172
2023-08-06 18:16:57 | INFO | train_inner | epoch 022:   1259 / 1474 loss=2.008, trans_loss=5.035, nll_loss=2.243, w2v_ctc_loss=0.653, task_loss=1.306, contrastive_loss=0.088, total=4179.82, n_correct=2702.85, ppl=4.73, accuracy=64.664, wps=12103.9, ups=1.45, wpb=8359.6, bsz=322.5, num_updates=32200, lr=7.8811e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=15.7, wall=27241
2023-08-06 18:18:07 | INFO | train_inner | epoch 022:   1359 / 1474 loss=2.002, trans_loss=5.023, nll_loss=2.227, w2v_ctc_loss=0.64, task_loss=1.378, contrastive_loss=0.112, total=4076.98, n_correct=2646.46, ppl=4.68, accuracy=64.912, wps=11754.3, ups=1.44, wpb=8154, bsz=303.1, num_updates=32300, lr=7.86889e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=69, gb_free=10.4, wall=27311
2023-08-06 18:19:15 | INFO | train_inner | epoch 022:   1459 / 1474 loss=2.013, trans_loss=5.042, nll_loss=2.25, w2v_ctc_loss=0.662, task_loss=1.508, contrastive_loss=0.059, total=4070.93, n_correct=2624.18, ppl=4.76, accuracy=64.461, wps=11829.9, ups=1.45, wpb=8141.9, bsz=286.5, num_updates=32400, lr=7.85674e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=68, gb_free=16.6, wall=27380
2023-08-06 18:19:26 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 18:19:50 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 4.2 | trans_loss 5.552 | nll_loss 2.822 | w2v_ctc_loss 1.281 | task_loss 4.583 | contrastive_loss 0.24 | total 4003.4 | n_correct 2483.7 | ppl 7.07 | accuracy 62.04 | uer 17.041 | wer 18.836 | raw_wer 18.836 | bleu 20.17 | wps 1872.1 | wpb 4003.4 | bsz 141.8 | num_updates 32415 | best_bleu 20.27
2023-08-06 18:19:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32415 updates
2023-08-06 18:19:50 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.1706.pt
2023-08-06 18:19:54 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.1706.pt
2023-08-06 18:20:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.1706.pt (epoch 22 @ 32415 updates, score 20.17) (writing took 17.37335736490786 seconds)
2023-08-06 18:20:08 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-08-06 18:20:08 | INFO | train | epoch 022 | loss 2.004 | trans_loss 5.02 | nll_loss 2.222 | w2v_ctc_loss 0.648 | task_loss 1.4 | contrastive_loss 0.099 | total 4138.65 | n_correct 2687.67 | ppl 4.66 | accuracy 64.941 | wps 10774.9 | ups 1.3 | wpb 8277.3 | bsz 305.7 | num_updates 32415 | lr 7.85492e-05 | gnorm 0.531 | clip 0 | loss_scale 64 | train_wall 1018 | gb_free 11.4 | wall 27432
2023-08-06 18:20:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 18:20:09 | INFO | fairseq.trainer | begin training epoch 23
2023-08-06 18:20:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 18:21:16 | INFO | train_inner | epoch 023:     85 / 1474 loss=1.989, trans_loss=4.997, nll_loss=2.192, w2v_ctc_loss=0.649, task_loss=1.439, contrastive_loss=0.051, total=4094.01, n_correct=2675.46, ppl=4.57, accuracy=65.351, wps=6810.3, ups=0.83, wpb=8188, bsz=301, num_updates=32500, lr=7.84465e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=69, gb_free=16.1, wall=27500
2023-08-06 18:22:25 | INFO | train_inner | epoch 023:    185 / 1474 loss=1.984, trans_loss=4.993, nll_loss=2.186, w2v_ctc_loss=0.636, task_loss=1.467, contrastive_loss=0.049, total=4118.15, n_correct=2698.08, ppl=4.55, accuracy=65.517, wps=11864.6, ups=1.44, wpb=8236.3, bsz=296.2, num_updates=32600, lr=7.8326e-05, gnorm=0.528, clip=0, loss_scale=64, train_wall=69, gb_free=15.3, wall=27569
2023-08-06 18:23:35 | INFO | train_inner | epoch 023:    285 / 1474 loss=1.994, trans_loss=5.003, nll_loss=2.199, w2v_ctc_loss=0.632, task_loss=1.412, contrastive_loss=0.123, total=4156.76, n_correct=2716.62, ppl=4.59, accuracy=65.354, wps=11910.7, ups=1.43, wpb=8313.5, bsz=305.6, num_updates=32700, lr=7.82062e-05, gnorm=0.527, clip=0, loss_scale=64, train_wall=69, gb_free=15.9, wall=27639
2023-08-06 18:24:44 | INFO | train_inner | epoch 023:    385 / 1474 loss=1.985, trans_loss=5.001, nll_loss=2.196, w2v_ctc_loss=0.637, task_loss=1.447, contrastive_loss=0.042, total=4114.42, n_correct=2692.74, ppl=4.58, accuracy=65.446, wps=11953.1, ups=1.45, wpb=8228.8, bsz=295.1, num_updates=32800, lr=7.80869e-05, gnorm=0.526, clip=0, loss_scale=64, train_wall=68, gb_free=12.9, wall=27708
2023-08-06 18:25:53 | INFO | train_inner | epoch 023:    485 / 1474 loss=1.998, trans_loss=5.009, nll_loss=2.207, w2v_ctc_loss=0.644, task_loss=1.361, contrastive_loss=0.098, total=4156.07, n_correct=2706.79, ppl=4.62, accuracy=65.129, wps=11917, ups=1.43, wpb=8312.1, bsz=312.6, num_updates=32900, lr=7.79681e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=69, gb_free=15.4, wall=27778
2023-08-06 18:27:02 | INFO | train_inner | epoch 023:    585 / 1474 loss=1.979, trans_loss=4.995, nll_loss=2.189, w2v_ctc_loss=0.633, task_loss=1.331, contrastive_loss=0.047, total=4169.74, n_correct=2731.92, ppl=4.56, accuracy=65.518, wps=12114.6, ups=1.45, wpb=8339.5, bsz=314.9, num_updates=33000, lr=7.78499e-05, gnorm=0.525, clip=0, loss_scale=128, train_wall=68, gb_free=16.4, wall=27846
2023-08-06 18:27:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-06 18:28:12 | INFO | train_inner | epoch 023:    686 / 1474 loss=1.985, trans_loss=5.003, nll_loss=2.199, w2v_ctc_loss=0.636, task_loss=1.429, contrastive_loss=0.046, total=4123.62, n_correct=2698.64, ppl=4.59, accuracy=65.443, wps=11763.7, ups=1.43, wpb=8247.2, bsz=297.2, num_updates=33100, lr=7.77322e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=70, gb_free=17.6, wall=27917
2023-08-06 18:29:22 | INFO | train_inner | epoch 023:    786 / 1474 loss=1.994, trans_loss=5.013, nll_loss=2.212, w2v_ctc_loss=0.645, task_loss=1.413, contrastive_loss=0.063, total=4147.22, n_correct=2702.32, ppl=4.63, accuracy=65.16, wps=11863.1, ups=1.43, wpb=8294.4, bsz=305.1, num_updates=33200, lr=7.76151e-05, gnorm=0.531, clip=0, loss_scale=64, train_wall=69, gb_free=17.2, wall=27986
2023-08-06 18:30:31 | INFO | train_inner | epoch 023:    886 / 1474 loss=1.998, trans_loss=5.009, nll_loss=2.209, w2v_ctc_loss=0.637, task_loss=1.267, contrastive_loss=0.14, total=4193.16, n_correct=2737.6, ppl=4.62, accuracy=65.287, wps=12124.1, ups=1.45, wpb=8386.3, bsz=327.3, num_updates=33300, lr=7.74984e-05, gnorm=0.526, clip=0, loss_scale=64, train_wall=69, gb_free=15.8, wall=28056
2023-08-06 18:31:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-06 18:31:42 | INFO | train_inner | epoch 023:    987 / 1474 loss=2.018, trans_loss=5.013, nll_loss=2.213, w2v_ctc_loss=0.631, task_loss=1.399, contrastive_loss=0.306, total=4165.31, n_correct=2712.12, ppl=4.64, accuracy=65.112, wps=11791.8, ups=1.42, wpb=8330.6, bsz=309.9, num_updates=33400, lr=7.73823e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=70, gb_free=10.6, wall=28126
2023-08-06 18:32:52 | INFO | train_inner | epoch 023:   1087 / 1474 loss=2, trans_loss=5.018, nll_loss=2.22, w2v_ctc_loss=0.653, task_loss=1.494, contrastive_loss=0.053, total=4088.49, n_correct=2656.13, ppl=4.66, accuracy=64.966, wps=11703.1, ups=1.43, wpb=8177, bsz=290, num_updates=33500, lr=7.72667e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=69, gb_free=15.7, wall=28196
2023-08-06 18:34:02 | INFO | train_inner | epoch 023:   1187 / 1474 loss=1.992, trans_loss=5.019, nll_loss=2.221, w2v_ctc_loss=0.644, task_loss=1.391, contrastive_loss=0.047, total=4162.7, n_correct=2707.34, ppl=4.66, accuracy=65.038, wps=11871.9, ups=1.43, wpb=8325.4, bsz=309.3, num_updates=33600, lr=7.71517e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=70, gb_free=15.9, wall=28266
2023-08-06 18:35:11 | INFO | train_inner | epoch 023:   1287 / 1474 loss=1.989, trans_loss=5.017, nll_loss=2.219, w2v_ctc_loss=0.636, task_loss=1.36, contrastive_loss=0.057, total=4135.53, n_correct=2698.18, ppl=4.66, accuracy=65.244, wps=11955.4, ups=1.45, wpb=8271.1, bsz=308.9, num_updates=33700, lr=7.70371e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=69, gb_free=16.7, wall=28335
2023-08-06 18:36:21 | INFO | train_inner | epoch 023:   1387 / 1474 loss=2.012, trans_loss=5.037, nll_loss=2.245, w2v_ctc_loss=0.646, task_loss=1.415, contrastive_loss=0.113, total=4143.98, n_correct=2680.79, ppl=4.74, accuracy=64.691, wps=11855.2, ups=1.43, wpb=8288, bsz=305.2, num_updates=33800, lr=7.69231e-05, gnorm=0.537, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=28405
2023-08-06 18:37:21 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 18:37:44 | INFO | dev_st | epoch 023 | valid on 'dev_st' subset | loss 4.215 | trans_loss 5.561 | nll_loss 2.837 | w2v_ctc_loss 1.305 | task_loss 4.612 | contrastive_loss 0.247 | total 4003.4 | n_correct 2481.1 | ppl 7.15 | accuracy 61.975 | uer 16.771 | wer 18.579 | raw_wer 18.579 | bleu 20.17 | wps 2249.5 | wpb 4003.4 | bsz 141.8 | num_updates 33887 | best_bleu 20.27
2023-08-06 18:37:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 33887 updates
2023-08-06 18:37:44 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.1700.pt
2023-08-06 18:37:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.1700.pt
2023-08-06 18:38:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.1700.pt (epoch 23 @ 33887 updates, score 20.17) (writing took 27.15985315106809 seconds)
2023-08-06 18:38:14 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-08-06 18:38:14 | INFO | train | epoch 023 | loss 1.996 | trans_loss 5.01 | nll_loss 2.209 | w2v_ctc_loss 0.64 | task_loss 1.401 | contrastive_loss 0.097 | total 4137.78 | n_correct 2698.16 | ppl 4.62 | accuracy 65.208 | wps 11219.3 | ups 1.36 | wpb 8275.6 | bsz 305.4 | num_updates 33887 | lr 7.68243e-05 | gnorm 0.53 | clip 0 | loss_scale 32 | train_wall 1019 | gb_free 13.5 | wall 28518
2023-08-06 18:38:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 18:38:14 | INFO | fairseq.trainer | begin training epoch 24
2023-08-06 18:38:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 18:38:31 | INFO | train_inner | epoch 024:     13 / 1474 loss=2.016, trans_loss=5.026, nll_loss=2.231, w2v_ctc_loss=0.639, task_loss=1.406, contrastive_loss=0.188, total=4085.11, n_correct=2649.59, ppl=4.7, accuracy=64.86, wps=6292.5, ups=0.77, wpb=8170.2, bsz=304.2, num_updates=33900, lr=7.68095e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=69, gb_free=12.4, wall=28535
2023-08-06 18:39:41 | INFO | train_inner | epoch 024:    113 / 1474 loss=1.995, trans_loss=4.982, nll_loss=2.172, w2v_ctc_loss=0.628, task_loss=1.294, contrastive_loss=0.209, total=4171.44, n_correct=2741.11, ppl=4.51, accuracy=65.711, wps=11980.6, ups=1.44, wpb=8342.9, bsz=324.6, num_updates=34000, lr=7.66965e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=69, gb_free=16.2, wall=28605
2023-08-06 18:39:41 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 18:40:05 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 4.208 | trans_loss 5.563 | nll_loss 2.838 | w2v_ctc_loss 1.283 | task_loss 4.6 | contrastive_loss 0.245 | total 4003.4 | n_correct 2481.6 | ppl 7.15 | accuracy 61.987 | uer 17.007 | wer 18.855 | raw_wer 18.855 | bleu 19.92 | wps 2027.1 | wpb 4003.4 | bsz 141.8 | num_updates 34000 | best_bleu 20.27
2023-08-06 18:40:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 34000 updates
2023-08-06 18:40:05 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_24_34000.pt
2023-08-06 18:40:09 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_24_34000.pt
2023-08-06 18:40:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_24_34000.pt (epoch 24 @ 34000 updates, score 19.92) (writing took 18.840071715414524 seconds)
2023-08-06 18:41:35 | INFO | train_inner | epoch 024:    213 / 1474 loss=1.997, trans_loss=4.986, nll_loss=2.179, w2v_ctc_loss=0.618, task_loss=1.226, contrastive_loss=0.258, total=4251.29, n_correct=2793.5, ppl=4.53, accuracy=65.709, wps=7467.3, ups=0.88, wpb=8502.6, bsz=340.8, num_updates=34100, lr=7.6584e-05, gnorm=0.519, clip=0, loss_scale=32, train_wall=70, gb_free=16.6, wall=28719
2023-08-06 18:42:44 | INFO | train_inner | epoch 024:    313 / 1474 loss=1.977, trans_loss=4.99, nll_loss=2.183, w2v_ctc_loss=0.632, task_loss=1.371, contrastive_loss=0.044, total=4128.18, n_correct=2710.36, ppl=4.54, accuracy=65.655, wps=11853, ups=1.44, wpb=8256.4, bsz=305.5, num_updates=34200, lr=7.64719e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=69, gb_free=16, wall=28788
2023-08-06 18:43:54 | INFO | train_inner | epoch 024:    413 / 1474 loss=2.008, trans_loss=4.998, nll_loss=2.193, w2v_ctc_loss=0.642, task_loss=1.464, contrastive_loss=0.182, total=4158.92, n_correct=2712.94, ppl=4.57, accuracy=65.232, wps=11928, ups=1.43, wpb=8317.8, bsz=299.7, num_updates=34300, lr=7.63604e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=69, gb_free=16.4, wall=28858
2023-08-06 18:45:04 | INFO | train_inner | epoch 024:    513 / 1474 loss=1.989, trans_loss=4.994, nll_loss=2.188, w2v_ctc_loss=0.637, task_loss=1.425, contrastive_loss=0.111, total=4144.91, n_correct=2718.46, ppl=4.56, accuracy=65.586, wps=11823, ups=1.43, wpb=8289.8, bsz=303.3, num_updates=34400, lr=7.62493e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=70, gb_free=16.8, wall=28928
2023-08-06 18:46:14 | INFO | train_inner | epoch 024:    613 / 1474 loss=1.98, trans_loss=4.994, nll_loss=2.188, w2v_ctc_loss=0.624, task_loss=1.409, contrastive_loss=0.071, total=4165.3, n_correct=2731.79, ppl=4.56, accuracy=65.584, wps=11980.9, ups=1.44, wpb=8330.6, bsz=307.7, num_updates=34500, lr=7.61387e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=69, gb_free=15.5, wall=28998
2023-08-06 18:47:23 | INFO | train_inner | epoch 024:    713 / 1474 loss=1.993, trans_loss=5.008, nll_loss=2.206, w2v_ctc_loss=0.637, task_loss=1.445, contrastive_loss=0.087, total=4102.21, n_correct=2681.16, ppl=4.61, accuracy=65.359, wps=11817.2, ups=1.44, wpb=8204.4, bsz=295.1, num_updates=34600, lr=7.60286e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=69, gb_free=16.7, wall=29067
2023-08-06 18:48:32 | INFO | train_inner | epoch 024:    813 / 1474 loss=1.987, trans_loss=5.009, nll_loss=2.209, w2v_ctc_loss=0.632, task_loss=1.409, contrastive_loss=0.063, total=4110.6, n_correct=2683.19, ppl=4.62, accuracy=65.275, wps=11853, ups=1.44, wpb=8221.2, bsz=305.3, num_updates=34700, lr=7.5919e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=69, gb_free=16.3, wall=29137
2023-08-06 18:49:42 | INFO | train_inner | epoch 024:    913 / 1474 loss=1.994, trans_loss=5.015, nll_loss=2.214, w2v_ctc_loss=0.646, task_loss=1.554, contrastive_loss=0.039, total=4043.03, n_correct=2628.18, ppl=4.64, accuracy=65.005, wps=11697.4, ups=1.45, wpb=8086.1, bsz=281, num_updates=34800, lr=7.58098e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=69, gb_free=11, wall=29206
2023-08-06 18:50:52 | INFO | train_inner | epoch 024:   1013 / 1474 loss=1.986, trans_loss=5.012, nll_loss=2.212, w2v_ctc_loss=0.633, task_loss=1.445, contrastive_loss=0.044, total=4136.81, n_correct=2700.68, ppl=4.63, accuracy=65.284, wps=11795.5, ups=1.43, wpb=8273.6, bsz=298.4, num_updates=34900, lr=7.57011e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=70, gb_free=16.7, wall=29276
2023-08-06 18:52:01 | INFO | train_inner | epoch 024:   1113 / 1474 loss=1.989, trans_loss=4.999, nll_loss=2.195, w2v_ctc_loss=0.639, task_loss=1.351, contrastive_loss=0.084, total=4135.73, n_correct=2704.47, ppl=4.58, accuracy=65.393, wps=11906.6, ups=1.44, wpb=8271.5, bsz=308.9, num_updates=35000, lr=7.55929e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=69, gb_free=16.9, wall=29345
Mixup rate:0.5, token after shrink shape:torch.Size([16, 64]), X shape:torch.Size([16, 64, 512])
CTC Tokens:tensor([ 0, 84, 63,  0, 94], device='cuda:0'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:0'), New Tokens:tensor([ 0, 84, 63,  0, 94], device='cuda:0')
Mixup Sent Mask:tensor([[ 2],
        [ 7],
        [10],
        [12]], device='cuda:0'), 2,  Mixup Mask:tensor([ True, False,  True,  True, False], device='cuda:0'), 
                    Org X:tensor([[-1.3213, -0.3762, -0.1692,  ..., -1.7568, -0.7720, -0.3250],
        [-1.6006,  0.7441, -0.2332,  ..., -0.0142, -1.3447,  0.0706],
        [-0.8501,  1.2812, -0.1421,  ..., -0.0369, -0.3870, -0.2625],
        [-0.0045,  0.9692, -0.4680,  ..., -0.6611,  0.7495, -0.6387],
        [-0.8516,  0.2832, -1.3320,  ..., -1.2256,  2.3574,  0.0340]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [-1.6006,  0.7441, -0.2332,  ..., -0.0142, -1.3447,  0.0706],
        [-0.2607,  0.0882,  0.0286,  ..., -1.3779, -4.0117, -2.8535],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [-0.8516,  0.2832, -1.3320,  ..., -1.2256,  2.3574,  0.0340]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [-0.1453, -0.1560, -0.7505,  ..., -0.7456, -1.0195, -1.5127],
        [-0.2607,  0.0882,  0.0286,  ..., -1.3779, -4.0117, -2.8535],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [-0.1112,  0.3008, -0.4368,  ..., -0.4778,  2.2695, -2.7070]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:0')
2023-08-06 18:53:11 | INFO | train_inner | epoch 024:   1213 / 1474 loss=1.989, trans_loss=5.009, nll_loss=2.209, w2v_ctc_loss=0.633, task_loss=1.391, contrastive_loss=0.073, total=4148.3, n_correct=2709.08, ppl=4.62, accuracy=65.306, wps=11862.8, ups=1.43, wpb=8296.6, bsz=310.8, num_updates=35100, lr=7.54851e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=70, gb_free=16.5, wall=29415
2023-08-06 18:54:21 | INFO | train_inner | epoch 024:   1313 / 1474 loss=1.995, trans_loss=5.016, nll_loss=2.217, w2v_ctc_loss=0.649, task_loss=1.491, contrastive_loss=0.048, total=4110.05, n_correct=2673.46, ppl=4.65, accuracy=65.047, wps=11767.5, ups=1.43, wpb=8220.1, bsz=294.3, num_updates=35200, lr=7.53778e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=69, gb_free=17.1, wall=29485
2023-08-06 18:55:30 | INFO | train_inner | epoch 024:   1413 / 1474 loss=1.997, trans_loss=5.021, nll_loss=2.224, w2v_ctc_loss=0.651, task_loss=1.466, contrastive_loss=0.047, total=4090.91, n_correct=2661.69, ppl=4.67, accuracy=65.064, wps=11792.7, ups=1.44, wpb=8181.8, bsz=292.7, num_updates=35300, lr=7.5271e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=69, gb_free=15.9, wall=29555
2023-08-06 18:56:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([8, 87]), X shape:torch.Size([8, 87, 512])
CTC Tokens:tensor([  0,   9,   0, 108,   0], device='cuda:7'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:7'), New Tokens:tensor([  0,   9,   0, 108,   0], device='cuda:7')
Mixup Sent Mask:tensor([[2],
        [7]], device='cuda:7'), 2,  Mixup Mask:tensor([ True, False, False, False,  True], device='cuda:7'), 
                    Org X:tensor([[ 2.0469, -2.7539,  0.1976,  ..., -0.4573, -0.3628,  0.0453],
        [ 1.9131, -0.8867, -1.5020,  ...,  0.1320, -0.5552,  0.5684],
        [ 0.1733, -0.0610,  0.7100,  ..., -1.1328, -1.4736,  1.5605],
        [-2.4121,  1.5830, -0.1205,  ..., -1.1104, -0.4336,  0.6221],
        [ 0.4690,  1.3320,  1.8145,  ..., -2.2930,  0.3088,  0.4080]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [ 1.9131, -0.8867, -1.5020,  ...,  0.1320, -0.5552,  0.5684],
        [ 0.1733, -0.0610,  0.7100,  ..., -1.1328, -1.4736,  1.5605],
        [-2.4121,  1.5830, -0.1205,  ..., -1.1104, -0.4336,  0.6221],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [-0.2034, -0.4558, -0.3625,  ...,  0.6079, -1.0635,  1.8203],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [-0.2986, -0.0324,  0.4612,  ..., -1.5762, -0.8008, -0.8896],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 101]), X shape:torch.Size([8, 101, 512])
CTC Tokens:tensor([67, 67,  0,  0,  7], device='cuda:4'), Shrink Mask:tensor([ True, False,  True, False,  True], device='cuda:4'), New Tokens:tensor([ 67,   0,   7,   0, 524], device='cuda:4')
Mixup Sent Mask:tensor([[2],
        [7]], device='cuda:4'), 2,  Mixup Mask:tensor([ True, False, False, False, False], device='cuda:4'), 
                    Org X:tensor([[ 0.0740, -0.4092,  0.3870,  ..., -0.4438, -0.1420, -2.0059],
        [-0.1724,  0.2017,  0.6733,  ..., -0.5293, -0.0100, -2.5430],
        [-0.1169,  0.2983, -1.1953,  ..., -0.4539,  0.8340, -1.9424],
        [ 0.1370,  0.6963,  1.4951,  ..., -1.5908,  1.7656, -2.3809],
        [-1.1445,  1.9141,  0.9829,  ..., -0.0081, -0.3125, -0.7617]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.0105, -0.1223, -0.8916,  ...,  0.8506, -1.6875, -3.7090],
        [-0.1724,  0.2017,  0.6733,  ..., -0.5293, -0.0100, -2.5430],
        [-0.1169,  0.2983, -1.1953,  ..., -0.4539,  0.8340, -1.9424],
        [ 0.1370,  0.6963,  1.4951,  ..., -1.5908,  1.7656, -2.3809],
        [-1.1445,  1.9141,  0.9829,  ..., -0.0081, -0.3125, -0.7617]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.0105, -0.1223, -0.8916,  ...,  0.8506, -1.6875, -3.7090],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [-0.2856, -0.2043, -0.2878,  ..., -0.0890, -1.8379, -1.4814],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [-1.4238,  0.6543, -0.6289,  ...,  2.5820,  1.7676,  0.0992]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 56]), X shape:torch.Size([16, 56, 512])
CTC Tokens:tensor([ 0, 21,  0,  0,  0], device='cuda:5'), Shrink Mask:tensor([ True,  True,  True, False, False], device='cuda:5'), New Tokens:tensor([  0,  21,   0, 991,   0], device='cuda:5')
Mixup Sent Mask:tensor([[ 2],
        [ 7],
        [10],
        [12]], device='cuda:5'), 2,  Mixup Mask:tensor([False,  True, False,  True, False], device='cuda:5'), 
                    Org X:tensor([[-0.2063, -1.8760, -0.8145,  ..., -1.3389,  0.6812, -1.0088],
        [-0.3955, -0.5347, -0.5542,  ..., -0.7554,  1.7490, -1.3740],
        [-0.1643, -0.4617,  2.5527,  ..., -2.2852,  1.8154, -2.3340],
        [ 0.7832,  0.1418,  0.8320,  ..., -0.0975, -0.1985, -0.4111],
        [ 1.4902,  0.3809,  1.4834,  ..., -0.8262, -1.1787,  0.1613]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.2063, -1.8760, -0.8145,  ..., -1.3389,  0.6812, -1.0088],
        [-0.4651, -0.2239, -0.2598,  ..., -0.9785, -3.2520, -2.2129],
        [-0.1643, -0.4617,  2.5527,  ..., -2.2852,  1.8154, -2.3340],
        [ 0.4233,  0.5225, -0.8652,  ..., -0.2291,  0.2122, -0.1094],
        [ 1.4902,  0.3809,  1.4834,  ..., -0.8262, -1.1787,  0.1613]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [-0.4651, -0.2239, -0.2598,  ..., -0.9785, -3.2520, -2.2129],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [ 0.4233,  0.5225, -0.8652,  ..., -0.2291,  0.2122, -0.1094],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 78]), X shape:torch.Size([8, 78, 512])
CTC Tokens:tensor([  19,    0,    0,    0, 1252], device='cuda:1'), Shrink Mask:tensor([ True,  True, False, False,  True], device='cuda:1'), New Tokens:tensor([  19,    0, 1252,    0,  556], device='cuda:1')
Mixup Sent Mask:tensor([[2],
        [7]], device='cuda:1'), 2,  Mixup Mask:tensor([ True,  True, False,  True,  True], device='cuda:1'), 
                    Org X:tensor([[ 0.4443,  0.4072,  0.7510,  ..., -0.2581, -1.7383,  0.3469],
        [ 0.4458,  1.4961,  2.2676,  ..., -0.3064, -1.6260,  0.0924],
        [ 0.5474,  1.1934,  2.9668,  ...,  0.0649,  0.4941, -0.4309],
        [ 0.1677,  1.5342,  3.1309,  ...,  0.0675,  0.6724, -0.0677],
        [-0.0718,  1.8369,  3.1348,  ..., -0.0397,  0.5264, -0.0771]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.1685, -0.5142, -0.1763,  ..., -0.8252,  0.0367, -1.3643],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [ 0.5474,  1.1934,  2.9668,  ...,  0.0649,  0.4941, -0.4309],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [-0.0959, -0.4004,  0.3235,  ...,  1.8076,  0.7085, -3.5059]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[ 0.1685, -0.5142, -0.1763,  ..., -0.8252,  0.0367, -1.3643],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [ 0.7886,  1.7412, -0.0304,  ..., -0.6265, -3.6484, -4.9453],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [-0.0959, -0.4004,  0.3235,  ...,  1.8076,  0.7085, -3.5059]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 82]), X shape:torch.Size([8, 82, 512])
CTC Tokens:tensor([ 0,  0, 25,  0, 19], device='cuda:6'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:6'), New Tokens:tensor([ 0, 25,  0, 19,  0], device='cuda:6')
Mixup Sent Mask:tensor([[2],
        [7]], device='cuda:6'), 2,  Mixup Mask:tensor([False,  True, False,  True,  True], device='cuda:6'), 
                    Org X:tensor([[ 0.4392, -0.1322,  1.8613,  ..., -0.4932, -0.7324, -0.1680],
        [-0.2031,  0.5205,  3.0840,  ..., -0.4846,  0.5361, -0.3733],
        [ 0.5337,  0.2469,  1.9990,  ..., -1.1592,  0.1238, -1.0742],
        [ 0.4478,  1.0049,  0.1656,  ..., -0.9233,  1.5820, -0.8657],
        [ 1.4707,  1.4453,  0.8857,  ..., -2.5859,  2.0195, -0.4910]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.4392, -0.1322,  1.8613,  ..., -0.4932, -0.7324, -0.1680],
        [ 0.2791, -0.5088, -0.0689,  ..., -1.0928,  0.4119, -2.4922],
        [ 0.5337,  0.2469,  1.9990,  ..., -1.1592,  0.1238, -1.0742],
        [ 0.1685, -0.5142, -0.1763,  ..., -0.8252,  0.0367, -1.3643],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [ 0.2791, -0.5088, -0.0689,  ..., -1.0928,  0.4119, -2.4922],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [ 0.1685, -0.5142, -0.1763,  ..., -0.8252,  0.0367, -1.3643],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([40, 36]), X shape:torch.Size([40, 36, 512])
CTC Tokens:tensor([  8,   0,   7, 207,   0], device='cuda:2'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:2'), New Tokens:tensor([  8,   0,   7, 207,   0], device='cuda:2')
Mixup Sent Mask:tensor([[ 2],
        [ 7],
        [10],
        [12],
        [17],
        [24],
        [26],
        [27],
        [30],
        [31],
        [32],
        [37]], device='cuda:2'), 2,  Mixup Mask:tensor([ True,  True,  True, False,  True], device='cuda:2'), 
                    Org X:tensor([[ 0.5532, -0.4507, -0.3137,  ...,  0.7563,  0.8799, -0.8262],
        [ 0.3503, -0.6689, -1.6348,  ..., -0.1375,  0.4626, -1.1680],
        [ 0.2949, -0.5034, -0.4475,  ...,  0.1637,  1.9219, -1.4219],
        [-0.0060, -0.7451,  0.8862,  ...,  0.2871,  3.1289, -1.2900],
        [ 0.5396, -0.2766,  1.2773,  ..., -0.2122,  1.4678, -2.3691]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3420, -0.1058, -0.6289,  ...,  1.0098, -1.6348, -1.5479],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [-0.2856, -0.2043, -0.2878,  ..., -0.0890, -1.8379, -1.4814],
        [-0.0060, -0.7451,  0.8862,  ...,  0.2871,  3.1289, -1.2900],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3420, -0.1058, -0.6289,  ...,  1.0098, -1.6348, -1.5479],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [-0.2856, -0.2043, -0.2878,  ..., -0.0890, -1.8379, -1.4814],
        [ 0.0263, -0.0284,  0.1976,  ...,  0.0614, -0.3743, -0.8950],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 86]), X shape:torch.Size([8, 86, 512])
CTC Tokens:tensor([ 0,  7,  0,  0, 94], device='cuda:3'), Shrink Mask:tensor([ True,  True,  True, False,  True], device='cuda:3'), New Tokens:tensor([ 0,  7,  0, 94,  0], device='cuda:3')
Mixup Sent Mask:tensor([[2],
        [7]], device='cuda:3'), 2,  Mixup Mask:tensor([False,  True,  True, False, False], device='cuda:3'), 
                    Org X:tensor([[ 0.2402, -0.6689,  0.6113,  ..., -0.7544, -2.0586, -0.6396],
        [-0.8726,  0.0558,  0.1211,  ...,  0.2568, -1.3105, -0.6934],
        [-1.0850,  0.1313,  0.9907,  ..., -0.1466, -0.7007, -1.3574],
        [-0.7798,  0.2076, -0.4485,  ..., -0.8486,  1.8340, -0.0399],
        [ 0.4248,  0.5010,  0.5483,  ..., -1.9150,  1.5713,  0.2708]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.2402, -0.6689,  0.6113,  ..., -0.7544, -2.0586, -0.6396],
        [-0.2856, -0.2043, -0.2878,  ..., -0.0890, -1.8379, -1.4814],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [-0.7798,  0.2076, -0.4485,  ..., -0.8486,  1.8340, -0.0399],
        [ 0.4248,  0.5010,  0.5483,  ..., -1.9150,  1.5713,  0.2708]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [-0.2856, -0.2043, -0.2878,  ..., -0.0890, -1.8379, -1.4814],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682],
        [-0.1112,  0.3008, -0.4368,  ..., -0.4778,  2.2695, -2.7070],
        [-0.4346, -0.8516,  0.2375,  ..., -2.4219, -1.7080, -1.8682]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:3')
2023-08-06 18:56:36 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 4.196 | trans_loss 5.555 | nll_loss 2.827 | w2v_ctc_loss 1.258 | task_loss 4.611 | contrastive_loss 0.244 | total 4003.4 | n_correct 2482.2 | ppl 7.09 | accuracy 62.002 | uer 16.733 | wer 18.679 | raw_wer 18.679 | bleu 20.03 | wps 2257.5 | wpb 4003.4 | bsz 141.8 | num_updates 35361 | best_bleu 20.27
2023-08-06 18:56:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 35361 updates
2023-08-06 18:56:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.0307.pt
2023-08-06 18:56:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.0307.pt
2023-08-06 18:56:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.0307.pt (epoch 24 @ 35361 updates, score 20.03) (writing took 14.819362254813313 seconds)
2023-08-06 18:56:51 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-08-06 18:56:51 | INFO | train | epoch 024 | loss 1.99 | trans_loss 5.002 | nll_loss 2.199 | w2v_ctc_loss 0.635 | task_loss 1.401 | contrastive_loss 0.097 | total 4138.65 | n_correct 2705.87 | ppl 4.59 | accuracy 65.38 | wps 10925.3 | ups 1.32 | wpb 8277.3 | bsz 305.7 | num_updates 35361 | lr 7.5206e-05 | gnorm 0.532 | clip 0 | loss_scale 32 | train_wall 1020 | gb_free 16 | wall 29635
2023-08-06 18:56:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 18:56:51 | INFO | fairseq.trainer | begin training epoch 25
2023-08-06 18:56:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 18:57:25 | INFO | train_inner | epoch 025:     39 / 1474 loss=1.973, trans_loss=4.99, nll_loss=2.184, w2v_ctc_loss=0.627, task_loss=1.339, contrastive_loss=0.052, total=4166.95, n_correct=2738.25, ppl=4.54, accuracy=65.714, wps=7246.7, ups=0.87, wpb=8333.9, bsz=312, num_updates=35400, lr=7.51646e-05, gnorm=0.52, clip=0, loss_scale=32, train_wall=69, gb_free=16.3, wall=29670
2023-08-06 18:58:35 | INFO | train_inner | epoch 025:    139 / 1474 loss=1.966, trans_loss=4.972, nll_loss=2.159, w2v_ctc_loss=0.622, task_loss=1.368, contrastive_loss=0.051, total=4133.64, n_correct=2729.6, ppl=4.47, accuracy=66.034, wps=11902.3, ups=1.44, wpb=8267.3, bsz=307.7, num_updates=35500, lr=7.50587e-05, gnorm=0.526, clip=0, loss_scale=64, train_wall=69, gb_free=15.6, wall=29739
2023-08-06 18:59:45 | INFO | train_inner | epoch 025:    239 / 1474 loss=1.971, trans_loss=4.978, nll_loss=2.167, w2v_ctc_loss=0.626, task_loss=1.439, contrastive_loss=0.054, total=4114.53, n_correct=2713.4, ppl=4.49, accuracy=65.947, wps=11773.1, ups=1.43, wpb=8229.1, bsz=302.7, num_updates=35600, lr=7.49532e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=69, gb_free=16.9, wall=29809
2023-08-06 19:00:54 | INFO | train_inner | epoch 025:    339 / 1474 loss=1.983, trans_loss=4.985, nll_loss=2.174, w2v_ctc_loss=0.631, task_loss=1.492, contrastive_loss=0.085, total=4148.7, n_correct=2721.79, ppl=4.51, accuracy=65.606, wps=11893.5, ups=1.43, wpb=8297.4, bsz=295.1, num_updates=35700, lr=7.48481e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=69, gb_free=16.1, wall=29879
2023-08-06 19:01:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-06 19:02:05 | INFO | train_inner | epoch 025:    440 / 1474 loss=1.986, trans_loss=4.989, nll_loss=2.181, w2v_ctc_loss=0.648, task_loss=1.508, contrastive_loss=0.045, total=4142.33, n_correct=2717.77, ppl=4.53, accuracy=65.61, wps=11801.5, ups=1.42, wpb=8284.7, bsz=289.3, num_updates=35800, lr=7.47435e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=70, gb_free=15.6, wall=29949
2023-08-06 19:03:14 | INFO | train_inner | epoch 025:    540 / 1474 loss=1.98, trans_loss=4.995, nll_loss=2.191, w2v_ctc_loss=0.632, task_loss=1.366, contrastive_loss=0.053, total=4160.61, n_correct=2729.73, ppl=4.57, accuracy=65.609, wps=11934.5, ups=1.43, wpb=8321.2, bsz=313.9, num_updates=35900, lr=7.46393e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=69, gb_free=17.5, wall=30019
2023-08-06 19:04:24 | INFO | train_inner | epoch 025:    640 / 1474 loss=1.986, trans_loss=4.987, nll_loss=2.18, w2v_ctc_loss=0.634, task_loss=1.386, contrastive_loss=0.121, total=4153.68, n_correct=2728.18, ppl=4.53, accuracy=65.681, wps=12018, ups=1.45, wpb=8307.4, bsz=309.6, num_updates=36000, lr=7.45356e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=69, gb_free=16.3, wall=30088
2023-08-06 19:04:24 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 19:04:47 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 4.214 | trans_loss 5.558 | nll_loss 2.831 | w2v_ctc_loss 1.319 | task_loss 4.611 | contrastive_loss 0.237 | total 4003.4 | n_correct 2487.2 | ppl 7.12 | accuracy 62.127 | uer 17.047 | wer 18.903 | raw_wer 18.903 | bleu 20.23 | wps 2212.6 | wpb 4003.4 | bsz 141.8 | num_updates 36000 | best_bleu 20.27
2023-08-06 19:04:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36000 updates
2023-08-06 19:04:47 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_25_36000.pt
2023-08-06 19:04:51 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_25_36000.pt
2023-08-06 19:05:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_25_36000.pt (epoch 25 @ 36000 updates, score 20.23) (writing took 34.38335909694433 seconds)
2023-08-06 19:06:33 | INFO | train_inner | epoch 025:    740 / 1474 loss=1.987, trans_loss=4.989, nll_loss=2.182, w2v_ctc_loss=0.632, task_loss=1.42, contrastive_loss=0.112, total=4128.34, n_correct=2707.5, ppl=4.54, accuracy=65.583, wps=6378.2, ups=0.77, wpb=8256.7, bsz=301.3, num_updates=36100, lr=7.44323e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=70, gb_free=17, wall=30217
2023-08-06 19:07:42 | INFO | train_inner | epoch 025:    840 / 1474 loss=1.974, trans_loss=4.992, nll_loss=2.187, w2v_ctc_loss=0.625, task_loss=1.291, contrastive_loss=0.062, total=4182.4, n_correct=2745.87, ppl=4.55, accuracy=65.653, wps=12148.6, ups=1.45, wpb=8364.8, bsz=326, num_updates=36200, lr=7.43294e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=68, gb_free=17.6, wall=30286
2023-08-06 19:08:52 | INFO | train_inner | epoch 025:    940 / 1474 loss=1.988, trans_loss=4.998, nll_loss=2.195, w2v_ctc_loss=0.632, task_loss=1.326, contrastive_loss=0.119, total=4155.21, n_correct=2723.62, ppl=4.58, accuracy=65.547, wps=11920.2, ups=1.43, wpb=8310.4, bsz=317, num_updates=36300, lr=7.4227e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=69, gb_free=13.9, wall=30356
2023-08-06 19:10:01 | INFO | train_inner | epoch 025:   1040 / 1474 loss=2.002, trans_loss=5.004, nll_loss=2.202, w2v_ctc_loss=0.624, task_loss=1.392, contrastive_loss=0.229, total=4177.7, n_correct=2729.01, ppl=4.6, accuracy=65.323, wps=12058, ups=1.44, wpb=8355.4, bsz=309.8, num_updates=36400, lr=7.41249e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=69, gb_free=15.5, wall=30425
2023-08-06 19:11:10 | INFO | train_inner | epoch 025:   1140 / 1474 loss=1.978, trans_loss=5.001, nll_loss=2.197, w2v_ctc_loss=0.624, task_loss=1.513, contrastive_loss=0.039, total=4039.24, n_correct=2642.75, ppl=4.58, accuracy=65.427, wps=11636.1, ups=1.44, wpb=8078.5, bsz=285.2, num_updates=36500, lr=7.40233e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=69, gb_free=16.3, wall=30494
2023-08-06 19:12:19 | INFO | train_inner | epoch 025:   1240 / 1474 loss=1.98, trans_loss=5.004, nll_loss=2.201, w2v_ctc_loss=0.628, task_loss=1.416, contrastive_loss=0.046, total=4090.59, n_correct=2677.69, ppl=4.6, accuracy=65.46, wps=11909.9, ups=1.46, wpb=8181.2, bsz=295.7, num_updates=36600, lr=7.39221e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=68, gb_free=17.1, wall=30563
2023-08-06 19:13:29 | INFO | train_inner | epoch 025:   1340 / 1474 loss=1.993, trans_loss=5.001, nll_loss=2.199, w2v_ctc_loss=0.633, task_loss=1.364, contrastive_loss=0.14, total=4164.34, n_correct=2723.85, ppl=4.59, accuracy=65.409, wps=11950.2, ups=1.43, wpb=8328.7, bsz=310.1, num_updates=36700, lr=7.38213e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=69, gb_free=16.7, wall=30633
2023-08-06 19:14:39 | INFO | train_inner | epoch 025:   1440 / 1474 loss=1.997, trans_loss=5.018, nll_loss=2.22, w2v_ctc_loss=0.64, task_loss=1.458, contrastive_loss=0.088, total=4099.11, n_correct=2666.85, ppl=4.66, accuracy=65.059, wps=11726.4, ups=1.43, wpb=8198.2, bsz=299.3, num_updates=36800, lr=7.3721e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=69, gb_free=12, wall=30703
2023-08-06 19:15:02 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 19:15:25 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 4.215 | trans_loss 5.555 | nll_loss 2.83 | w2v_ctc_loss 1.322 | task_loss 4.623 | contrastive_loss 0.248 | total 4003.4 | n_correct 2486.9 | ppl 7.11 | accuracy 62.12 | uer 17.057 | wer 18.914 | raw_wer 18.914 | bleu 20.06 | wps 2243.7 | wpb 4003.4 | bsz 141.8 | num_updates 36834 | best_bleu 20.27
2023-08-06 19:15:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36834 updates
2023-08-06 19:15:25 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.0608.pt
2023-08-06 19:15:29 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.0608.pt
2023-08-06 19:15:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.0608.pt (epoch 25 @ 36834 updates, score 20.06) (writing took 15.762960081920028 seconds)
2023-08-06 19:15:42 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-08-06 19:15:42 | INFO | train | epoch 025 | loss 1.983 | trans_loss 4.994 | nll_loss 2.188 | w2v_ctc_loss 0.631 | task_loss 1.403 | contrastive_loss 0.088 | total 4137.25 | n_correct 2712.92 | ppl 4.56 | accuracy 65.573 | wps 10778.5 | ups 1.3 | wpb 8274.5 | bsz 305.1 | num_updates 36834 | lr 7.36869e-05 | gnorm 0.533 | clip 0 | loss_scale 32 | train_wall 1018 | gb_free 14.1 | wall 30766
2023-08-06 19:15:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 19:15:42 | INFO | fairseq.trainer | begin training epoch 26
2023-08-06 19:15:42 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 19:16:35 | INFO | train_inner | epoch 026:     66 / 1474 loss=1.968, trans_loss=4.973, nll_loss=2.162, w2v_ctc_loss=0.619, task_loss=1.316, contrastive_loss=0.074, total=4180.21, n_correct=2757.56, ppl=4.48, accuracy=65.967, wps=7155.7, ups=0.86, wpb=8360.4, bsz=318.3, num_updates=36900, lr=7.3621e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=69, gb_free=16.8, wall=30820
2023-08-06 19:17:46 | INFO | train_inner | epoch 026:    166 / 1474 loss=1.986, trans_loss=4.97, nll_loss=2.159, w2v_ctc_loss=0.609, task_loss=1.23, contrastive_loss=0.256, total=4270.78, n_correct=2823.76, ppl=4.47, accuracy=66.118, wps=12181.7, ups=1.43, wpb=8541.6, bsz=340.4, num_updates=37000, lr=7.35215e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=70, gb_free=15.1, wall=30890
2023-08-06 19:18:55 | INFO | train_inner | epoch 026:    266 / 1474 loss=1.982, trans_loss=4.975, nll_loss=2.164, w2v_ctc_loss=0.63, task_loss=1.388, contrastive_loss=0.132, total=4125.04, n_correct=2717.87, ppl=4.48, accuracy=65.887, wps=11846.4, ups=1.44, wpb=8250.1, bsz=307.1, num_updates=37100, lr=7.34223e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=69, gb_free=15, wall=30959
2023-08-06 19:20:05 | INFO | train_inner | epoch 026:    366 / 1474 loss=1.973, trans_loss=4.975, nll_loss=2.164, w2v_ctc_loss=0.624, task_loss=1.341, contrastive_loss=0.089, total=4165.74, n_correct=2750.82, ppl=4.48, accuracy=66.034, wps=11953.8, ups=1.43, wpb=8331.5, bsz=314.7, num_updates=37200, lr=7.33236e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=69, gb_free=16.7, wall=31029
2023-08-06 19:21:14 | INFO | train_inner | epoch 026:    466 / 1474 loss=1.976, trans_loss=4.97, nll_loss=2.157, w2v_ctc_loss=0.621, task_loss=1.336, contrastive_loss=0.138, total=4170.23, n_correct=2757.11, ppl=4.46, accuracy=66.114, wps=12023.8, ups=1.44, wpb=8340.5, bsz=315.4, num_updates=37300, lr=7.32252e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=69, gb_free=17.7, wall=31098
2023-08-06 19:22:25 | INFO | train_inner | epoch 026:    566 / 1474 loss=1.979, trans_loss=4.985, nll_loss=2.177, w2v_ctc_loss=0.638, task_loss=1.413, contrastive_loss=0.059, total=4155.02, n_correct=2732.45, ppl=4.52, accuracy=65.763, wps=11784.3, ups=1.42, wpb=8310, bsz=303.9, num_updates=37400, lr=7.31272e-05, gnorm=0.538, clip=0, loss_scale=32, train_wall=70, gb_free=17.7, wall=31169
2023-08-06 19:23:35 | INFO | train_inner | epoch 026:    666 / 1474 loss=1.968, trans_loss=4.981, nll_loss=2.172, w2v_ctc_loss=0.618, task_loss=1.431, contrastive_loss=0.044, total=4136.96, n_correct=2723.19, ppl=4.51, accuracy=65.826, wps=11859.9, ups=1.43, wpb=8273.9, bsz=299.2, num_updates=37500, lr=7.30297e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=69, gb_free=15.2, wall=31239
2023-08-06 19:24:44 | INFO | train_inner | epoch 026:    766 / 1474 loss=1.99, trans_loss=4.989, nll_loss=2.182, w2v_ctc_loss=0.624, task_loss=1.423, contrastive_loss=0.158, total=4086.28, n_correct=2686.54, ppl=4.54, accuracy=65.745, wps=11733.9, ups=1.44, wpb=8172.6, bsz=298.5, num_updates=37600, lr=7.29325e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=69, gb_free=14.9, wall=31308
2023-08-06 19:25:53 | INFO | train_inner | epoch 026:    866 / 1474 loss=1.978, trans_loss=4.988, nll_loss=2.181, w2v_ctc_loss=0.631, task_loss=1.385, contrastive_loss=0.058, total=4183.26, n_correct=2745.08, ppl=4.53, accuracy=65.621, wps=12158.6, ups=1.45, wpb=8366.5, bsz=308.1, num_updates=37700, lr=7.28357e-05, gnorm=0.524, clip=0, loss_scale=32, train_wall=68, gb_free=17.2, wall=31377
2023-08-06 19:27:03 | INFO | train_inner | epoch 026:    966 / 1474 loss=1.982, trans_loss=4.998, nll_loss=2.193, w2v_ctc_loss=0.616, task_loss=1.447, contrastive_loss=0.11, total=4137.96, n_correct=2712.96, ppl=4.57, accuracy=65.563, wps=11835.7, ups=1.43, wpb=8275.9, bsz=299, num_updates=37800, lr=7.27393e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=69, gb_free=16.8, wall=31447
2023-08-06 19:28:12 | INFO | train_inner | epoch 026:   1066 / 1474 loss=1.974, trans_loss=4.993, nll_loss=2.188, w2v_ctc_loss=0.625, task_loss=1.468, contrastive_loss=0.044, total=4120.53, n_correct=2708.09, ppl=4.56, accuracy=65.722, wps=11863.8, ups=1.44, wpb=8241.1, bsz=294.3, num_updates=37900, lr=7.26433e-05, gnorm=0.528, clip=0, loss_scale=64, train_wall=69, gb_free=16.5, wall=31517
2023-08-06 19:29:22 | INFO | train_inner | epoch 026:   1166 / 1474 loss=1.983, trans_loss=4.999, nll_loss=2.195, w2v_ctc_loss=0.627, task_loss=1.464, contrastive_loss=0.083, total=4113.86, n_correct=2695.47, ppl=4.58, accuracy=65.522, wps=11762.7, ups=1.43, wpb=8227.7, bsz=298.5, num_updates=38000, lr=7.25476e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=69, gb_free=16.4, wall=31587
2023-08-06 19:29:22 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 19:29:49 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 4.212 | trans_loss 5.557 | nll_loss 2.831 | w2v_ctc_loss 1.31 | task_loss 4.606 | contrastive_loss 0.243 | total 4003.4 | n_correct 2485.8 | ppl 7.12 | accuracy 62.092 | uer 17.14 | wer 18.981 | raw_wer 18.981 | bleu 20.25 | wps 1880.6 | wpb 4003.4 | bsz 141.8 | num_updates 38000 | best_bleu 20.27
2023-08-06 19:29:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38000 updates
2023-08-06 19:29:49 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_26_38000.pt
2023-08-06 19:29:53 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_26_38000.pt
2023-08-06 19:30:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_26_38000.pt (epoch 26 @ 38000 updates, score 20.25) (writing took 18.847614036872983 seconds)
2023-08-06 19:31:17 | INFO | train_inner | epoch 026:   1266 / 1474 loss=1.99, trans_loss=5.01, nll_loss=2.209, w2v_ctc_loss=0.645, task_loss=1.564, contrastive_loss=0.047, total=3996.19, n_correct=2604.46, ppl=4.62, accuracy=65.174, wps=6955.6, ups=0.87, wpb=7992.4, bsz=279.3, num_updates=38100, lr=7.24524e-05, gnorm=0.543, clip=0, loss_scale=64, train_wall=69, gb_free=17.7, wall=31701
2023-08-06 19:32:28 | INFO | train_inner | epoch 026:   1366 / 1474 loss=1.975, trans_loss=5, nll_loss=2.197, w2v_ctc_loss=0.619, task_loss=1.393, contrastive_loss=0.058, total=4159.74, n_correct=2733.09, ppl=4.58, accuracy=65.703, wps=11820.2, ups=1.42, wpb=8319.5, bsz=311.4, num_updates=38200, lr=7.23575e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=70, gb_free=17.1, wall=31772
2023-08-06 19:33:36 | INFO | train_inner | epoch 026:   1466 / 1474 loss=1.969, trans_loss=4.994, nll_loss=2.19, w2v_ctc_loss=0.615, task_loss=1.325, contrastive_loss=0.053, total=4165.66, n_correct=2742.75, ppl=4.56, accuracy=65.842, wps=12145.7, ups=1.46, wpb=8331.3, bsz=317.5, num_updates=38300, lr=7.22629e-05, gnorm=0.531, clip=0, loss_scale=64, train_wall=68, gb_free=16.1, wall=31840
2023-08-06 19:33:42 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 19:34:05 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 4.209 | trans_loss 5.556 | nll_loss 2.831 | w2v_ctc_loss 1.302 | task_loss 4.611 | contrastive_loss 0.243 | total 4003.4 | n_correct 2480.5 | ppl 7.12 | accuracy 61.96 | uer 16.93 | wer 18.728 | raw_wer 18.728 | bleu 20.01 | wps 2139.2 | wpb 4003.4 | bsz 141.8 | num_updates 38308 | best_bleu 20.27
2023-08-06 19:34:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38308 updates
2023-08-06 19:34:05 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_last.pt
2023-08-06 19:34:18 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_last.pt
2023-08-06 19:34:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_last.pt (epoch 26 @ 38308 updates, score 20.01) (writing took 12.482235193252563 seconds)
2023-08-06 19:34:18 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-08-06 19:34:18 | INFO | train | epoch 026 | loss 1.978 | trans_loss 4.986 | nll_loss 2.178 | w2v_ctc_loss 0.624 | task_loss 1.4 | contrastive_loss 0.095 | total 4138.65 | n_correct 2722.69 | ppl 4.53 | accuracy 65.787 | wps 10929.8 | ups 1.32 | wpb 8277.3 | bsz 305.7 | num_updates 38308 | lr 7.22554e-05 | gnorm 0.533 | clip 0 | loss_scale 64 | train_wall 1020 | gb_free 15.9 | wall 31882
2023-08-06 19:34:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 19:34:18 | INFO | fairseq.trainer | begin training epoch 27
2023-08-06 19:34:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 19:35:29 | INFO | train_inner | epoch 027:     92 / 1474 loss=1.954, trans_loss=4.953, nll_loss=2.133, w2v_ctc_loss=0.608, task_loss=1.505, contrastive_loss=0.035, total=4054.57, n_correct=2693.45, ppl=4.39, accuracy=66.43, wps=7191.6, ups=0.89, wpb=8109.1, bsz=282.4, num_updates=38400, lr=7.21688e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=69, gb_free=16.1, wall=31953
2023-08-06 19:36:38 | INFO | train_inner | epoch 027:    192 / 1474 loss=1.957, trans_loss=4.957, nll_loss=2.14, w2v_ctc_loss=0.617, task_loss=1.331, contrastive_loss=0.06, total=4195.2, n_correct=2783.49, ppl=4.41, accuracy=66.349, wps=12142.1, ups=1.45, wpb=8390.4, bsz=323.2, num_updates=38500, lr=7.2075e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=17, wall=32022
2023-08-06 19:37:48 | INFO | train_inner | epoch 027:    292 / 1474 loss=1.96, trans_loss=4.965, nll_loss=2.15, w2v_ctc_loss=0.617, task_loss=1.403, contrastive_loss=0.046, total=4162.23, n_correct=2756.57, ppl=4.44, accuracy=66.228, wps=11880.5, ups=1.43, wpb=8324.5, bsz=305.5, num_updates=38600, lr=7.19816e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=70, gb_free=17, wall=32092
2023-08-06 19:37:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-06 19:38:59 | INFO | train_inner | epoch 027:    393 / 1474 loss=1.971, trans_loss=4.971, nll_loss=2.158, w2v_ctc_loss=0.616, task_loss=1.511, contrastive_loss=0.111, total=4052.96, n_correct=2676.62, ppl=4.46, accuracy=66.041, wps=11521.6, ups=1.42, wpb=8105.9, bsz=288.5, num_updates=38700, lr=7.18885e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=70, gb_free=17.6, wall=32163
2023-08-06 19:40:09 | INFO | train_inner | epoch 027:    493 / 1474 loss=1.98, trans_loss=4.98, nll_loss=2.172, w2v_ctc_loss=0.615, task_loss=1.283, contrastive_loss=0.162, total=4249.35, n_correct=2801.64, ppl=4.51, accuracy=65.931, wps=12056.2, ups=1.42, wpb=8498.7, bsz=331.9, num_updates=38800, lr=7.17958e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=70, gb_free=11.8, wall=32233
2023-08-06 19:41:18 | INFO | train_inner | epoch 027:    593 / 1474 loss=1.975, trans_loss=4.975, nll_loss=2.164, w2v_ctc_loss=0.623, task_loss=1.371, contrastive_loss=0.102, total=4133.39, n_correct=2728, ppl=4.48, accuracy=65.999, wps=11948.9, ups=1.45, wpb=8266.8, bsz=312, num_updates=38900, lr=7.17035e-05, gnorm=0.537, clip=0, loss_scale=32, train_wall=69, gb_free=11.9, wall=32302
2023-08-06 19:42:28 | INFO | train_inner | epoch 027:    693 / 1474 loss=1.975, trans_loss=4.981, nll_loss=2.172, w2v_ctc_loss=0.624, task_loss=1.396, contrastive_loss=0.081, total=4162.71, n_correct=2744.56, ppl=4.51, accuracy=65.932, wps=12002.2, ups=1.44, wpb=8325.4, bsz=305.4, num_updates=39000, lr=7.16115e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=69, gb_free=16.2, wall=32372
2023-08-06 19:43:37 | INFO | train_inner | epoch 027:    793 / 1474 loss=1.971, trans_loss=4.982, nll_loss=2.173, w2v_ctc_loss=0.625, task_loss=1.473, contrastive_loss=0.047, total=4103.81, n_correct=2703.71, ppl=4.51, accuracy=65.883, wps=11857.6, ups=1.44, wpb=8207.6, bsz=294.2, num_updates=39100, lr=7.15199e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=69, gb_free=16.1, wall=32441
2023-08-06 19:44:46 | INFO | train_inner | epoch 027:    893 / 1474 loss=1.965, trans_loss=4.987, nll_loss=2.179, w2v_ctc_loss=0.611, task_loss=1.459, contrastive_loss=0.039, total=4101.56, n_correct=2703.81, ppl=4.53, accuracy=65.922, wps=11828.5, ups=1.44, wpb=8203.1, bsz=292.1, num_updates=39200, lr=7.14286e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=69, gb_free=17.6, wall=32510
2023-08-06 19:45:57 | INFO | train_inner | epoch 027:    993 / 1474 loss=1.986, trans_loss=4.981, nll_loss=2.173, w2v_ctc_loss=0.616, task_loss=1.352, contrastive_loss=0.222, total=4199.56, n_correct=2769.14, ppl=4.51, accuracy=65.939, wps=11934.1, ups=1.42, wpb=8399.1, bsz=316.8, num_updates=39300, lr=7.13376e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=70, gb_free=11.4, wall=32581
2023-08-06 19:47:06 | INFO | train_inner | epoch 027:   1093 / 1474 loss=1.964, trans_loss=4.979, nll_loss=2.169, w2v_ctc_loss=0.611, task_loss=1.411, contrastive_loss=0.056, total=4150.97, n_correct=2738.67, ppl=4.5, accuracy=65.977, wps=11961.1, ups=1.44, wpb=8301.9, bsz=305, num_updates=39400, lr=7.1247e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=69, gb_free=11.8, wall=32650
2023-08-06 19:48:15 | INFO | train_inner | epoch 027:   1193 / 1474 loss=1.977, trans_loss=4.989, nll_loss=2.183, w2v_ctc_loss=0.63, task_loss=1.462, contrastive_loss=0.058, total=4103.06, n_correct=2700.97, ppl=4.54, accuracy=65.828, wps=11873, ups=1.45, wpb=8206.1, bsz=297.7, num_updates=39500, lr=7.11568e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=69, gb_free=16.4, wall=32719
2023-08-06 19:49:24 | INFO | train_inner | epoch 027:   1293 / 1474 loss=1.987, trans_loss=4.995, nll_loss=2.19, w2v_ctc_loss=0.627, task_loss=1.498, contrastive_loss=0.111, total=4062.52, n_correct=2663.6, ppl=4.56, accuracy=65.565, wps=11738.8, ups=1.44, wpb=8125, bsz=292.2, num_updates=39600, lr=7.10669e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=69, gb_free=16.5, wall=32789
2023-08-06 19:50:33 | INFO | train_inner | epoch 027:   1393 / 1474 loss=1.972, trans_loss=4.986, nll_loss=2.18, w2v_ctc_loss=0.613, task_loss=1.323, contrastive_loss=0.093, total=4152, n_correct=2734.99, ppl=4.53, accuracy=65.872, wps=12045.4, ups=1.45, wpb=8304, bsz=312.5, num_updates=39700, lr=7.09773e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=68, gb_free=16.5, wall=32857
2023-08-06 19:51:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 19:51:52 | INFO | dev_st | epoch 027 | valid on 'dev_st' subset | loss 4.21 | trans_loss 5.553 | nll_loss 2.826 | w2v_ctc_loss 1.323 | task_loss 4.617 | contrastive_loss 0.231 | total 4003.4 | n_correct 2488.1 | ppl 7.09 | accuracy 62.15 | uer 16.842 | wer 18.773 | raw_wer 18.773 | bleu 20.47 | wps 2186 | wpb 4003.4 | bsz 141.8 | num_updates 39781 | best_bleu 20.47
2023-08-06 19:51:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 39781 updates
2023-08-06 19:51:52 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 19:52:05 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 19:52:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt (epoch 27 @ 39781 updates, score 20.47) (writing took 23.35585194453597 seconds)
2023-08-06 19:52:16 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-08-06 19:52:16 | INFO | train | epoch 027 | loss 1.97 | trans_loss 4.977 | nll_loss 2.167 | w2v_ctc_loss 0.618 | task_loss 1.403 | contrastive_loss 0.087 | total 4136.97 | n_correct 2730.5 | ppl 4.49 | accuracy 66.002 | wps 11303.4 | ups 1.37 | wpb 8273.9 | bsz 305.1 | num_updates 39781 | lr 7.0905e-05 | gnorm 0.535 | clip 0 | loss_scale 32 | train_wall 1017 | gb_free 17.7 | wall 32960
2023-08-06 19:52:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 19:52:16 | INFO | fairseq.trainer | begin training epoch 28
2023-08-06 19:52:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 19:52:36 | INFO | train_inner | epoch 028:     19 / 1474 loss=1.961, trans_loss=4.978, nll_loss=2.169, w2v_ctc_loss=0.61, task_loss=1.353, contrastive_loss=0.047, total=4108.43, n_correct=2717.24, ppl=4.5, accuracy=66.138, wps=6677.3, ups=0.81, wpb=8216.9, bsz=305.1, num_updates=39800, lr=7.08881e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=68, gb_free=16.2, wall=32981
2023-08-06 19:53:46 | INFO | train_inner | epoch 028:    119 / 1474 loss=1.95, trans_loss=4.947, nll_loss=2.126, w2v_ctc_loss=0.606, task_loss=1.46, contrastive_loss=0.042, total=4113.41, n_correct=2741.16, ppl=4.37, accuracy=66.64, wps=11808.3, ups=1.44, wpb=8226.8, bsz=293.9, num_updates=39900, lr=7.07992e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=69, gb_free=16.7, wall=33050
2023-08-06 19:54:55 | INFO | train_inner | epoch 028:    219 / 1474 loss=1.952, trans_loss=4.956, nll_loss=2.14, w2v_ctc_loss=0.606, task_loss=1.326, contrastive_loss=0.049, total=4191.56, n_correct=2785.06, ppl=4.41, accuracy=66.444, wps=12128.5, ups=1.45, wpb=8383.1, bsz=315.2, num_updates=40000, lr=7.07107e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=69, gb_free=15, wall=33119
2023-08-06 19:54:55 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 19:55:20 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 4.227 | trans_loss 5.564 | nll_loss 2.837 | w2v_ctc_loss 1.343 | task_loss 4.608 | contrastive_loss 0.244 | total 4003.4 | n_correct 2486.5 | ppl 7.15 | accuracy 62.11 | uer 16.826 | wer 18.735 | raw_wer 18.735 | bleu 20.52 | wps 1912.8 | wpb 4003.4 | bsz 141.8 | num_updates 40000 | best_bleu 20.52
2023-08-06 19:55:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 40000 updates
2023-08-06 19:55:20 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_28_40000.pt
2023-08-06 19:55:24 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_28_40000.pt
2023-08-06 19:56:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_28_40000.pt (epoch 28 @ 40000 updates, score 20.52) (writing took 49.22932400368154 seconds)
Mixup rate:0.5, token after shrink shape:torch.Size([16, 64]), X shape:torch.Size([16, 64, 512])
CTC Tokens:tensor([0, 0, 0, 0, 0], device='cuda:0'), Shrink Mask:tensor([ True, False, False, False, False], device='cuda:0'), New Tokens:tensor([  0, 339,  11,   6, 150], device='cuda:0')
Mixup Sent Mask:tensor([[ 1],
        [ 3],
        [ 9],
        [12],
        [14]], device='cuda:0'), 1,  Mixup Mask:tensor([ True, False,  True,  True,  True], device='cuda:0'), 
                    Org X:tensor([[ 0.1807, -0.6562,  0.6143,  ..., -0.4160,  1.7275, -0.4453],
        [ 0.4253,  0.0743,  0.5181,  ...,  0.2111, -0.1790,  0.3591],
        [ 1.4229,  0.1083,  0.8311,  ...,  0.1824,  0.0187,  0.5396],
        [ 2.2578,  0.5967,  1.1465,  ..., -0.0264, -0.4209,  0.2212],
        [ 0.3564,  1.3008,  1.1230,  ...,  0.0304, -0.5278, -0.3647]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [ 0.4253,  0.0743,  0.5181,  ...,  0.2111, -0.1790,  0.3591],
        [-0.4353, -0.3445, -0.3542,  ..., -2.5488, -0.4307, -3.8125],
        [ 0.1631, -0.4360, -0.1758,  ..., -2.9746, -3.2598,  0.8901],
        [-0.0938, -0.5078, -0.5913,  ...,  0.6479, -1.4893, -2.9004]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [ 0.5884, -0.6685,  0.0985,  ...,  1.4863, -1.8994, -0.8921],
        [-0.4353, -0.3445, -0.3542,  ..., -2.5488, -0.4307, -3.8125],
        [ 0.1631, -0.4360, -0.1758,  ..., -2.9746, -3.2598,  0.8901],
        [-0.0938, -0.5078, -0.5913,  ...,  0.6479, -1.4893, -2.9004]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:0')
2023-08-06 19:57:20 | INFO | train_inner | epoch 028:    319 / 1474 loss=1.999, trans_loss=4.965, nll_loss=2.152, w2v_ctc_loss=0.603, task_loss=1.39, contrastive_loss=0.379, total=4145.32, n_correct=2739.3, ppl=4.44, accuracy=66.082, wps=5722.3, ups=0.69, wpb=8290.6, bsz=316.1, num_updates=40100, lr=7.06225e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=69, gb_free=15.6, wall=33264
2023-08-06 19:58:29 | INFO | train_inner | epoch 028:    419 / 1474 loss=1.959, trans_loss=4.964, nll_loss=2.149, w2v_ctc_loss=0.617, task_loss=1.447, contrastive_loss=0.038, total=4092.14, n_correct=2713.04, ppl=4.43, accuracy=66.299, wps=11900.6, ups=1.45, wpb=8184.3, bsz=295.7, num_updates=40200, lr=7.05346e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=68, gb_free=16.1, wall=33333
2023-08-06 19:59:38 | INFO | train_inner | epoch 028:    519 / 1474 loss=1.959, trans_loss=4.965, nll_loss=2.15, w2v_ctc_loss=0.611, task_loss=1.461, contrastive_loss=0.05, total=4096.35, n_correct=2712.8, ppl=4.44, accuracy=66.225, wps=11780.9, ups=1.44, wpb=8192.7, bsz=295.5, num_updates=40300, lr=7.0447e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=69, gb_free=15.9, wall=33403
2023-08-06 20:00:47 | INFO | train_inner | epoch 028:    619 / 1474 loss=1.964, trans_loss=4.976, nll_loss=2.166, w2v_ctc_loss=0.616, task_loss=1.412, contrastive_loss=0.05, total=4178.12, n_correct=2758.72, ppl=4.49, accuracy=66.028, wps=12110.7, ups=1.45, wpb=8356.2, bsz=305.3, num_updates=40400, lr=7.03598e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=69, gb_free=15.6, wall=33472
2023-08-06 20:01:57 | INFO | train_inner | epoch 028:    719 / 1474 loss=1.972, trans_loss=4.972, nll_loss=2.161, w2v_ctc_loss=0.609, task_loss=1.272, contrastive_loss=0.158, total=4185.82, n_correct=2771.27, ppl=4.47, accuracy=66.206, wps=12033.3, ups=1.44, wpb=8371.6, bsz=326.4, num_updates=40500, lr=7.02728e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=33541
2023-08-06 20:03:06 | INFO | train_inner | epoch 028:    819 / 1474 loss=1.955, trans_loss=4.97, nll_loss=2.158, w2v_ctc_loss=0.607, task_loss=1.371, contrastive_loss=0.043, total=4096.2, n_correct=2715.26, ppl=4.46, accuracy=66.287, wps=11894.6, ups=1.45, wpb=8192.4, bsz=307, num_updates=40600, lr=7.01862e-05, gnorm=0.537, clip=0, loss_scale=32, train_wall=68, gb_free=15.6, wall=33610
2023-08-06 20:04:16 | INFO | train_inner | epoch 028:    919 / 1474 loss=1.977, trans_loss=4.983, nll_loss=2.175, w2v_ctc_loss=0.619, task_loss=1.437, contrastive_loss=0.101, total=4120.27, n_correct=2709.93, ppl=4.52, accuracy=65.771, wps=11739.6, ups=1.42, wpb=8240.5, bsz=300.8, num_updates=40700, lr=7.01e-05, gnorm=0.543, clip=0, loss_scale=64, train_wall=70, gb_free=17.3, wall=33680
2023-08-06 20:05:26 | INFO | train_inner | epoch 028:   1019 / 1474 loss=1.982, trans_loss=4.979, nll_loss=2.169, w2v_ctc_loss=0.622, task_loss=1.369, contrastive_loss=0.151, total=4177.86, n_correct=2751.58, ppl=4.5, accuracy=65.861, wps=11994.8, ups=1.44, wpb=8355.7, bsz=311.1, num_updates=40800, lr=7.0014e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=69, gb_free=16.3, wall=33750
2023-08-06 20:06:35 | INFO | train_inner | epoch 028:   1119 / 1474 loss=1.959, trans_loss=4.968, nll_loss=2.156, w2v_ctc_loss=0.61, task_loss=1.36, contrastive_loss=0.06, total=4210.86, n_correct=2789.98, ppl=4.46, accuracy=66.257, wps=12075, ups=1.43, wpb=8421.7, bsz=318.9, num_updates=40900, lr=6.99284e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=69, gb_free=17.4, wall=33820
2023-08-06 20:07:45 | INFO | train_inner | epoch 028:   1219 / 1474 loss=1.961, trans_loss=4.979, nll_loss=2.17, w2v_ctc_loss=0.608, task_loss=1.382, contrastive_loss=0.049, total=4104.61, n_correct=2708.56, ppl=4.5, accuracy=65.988, wps=11809.1, ups=1.44, wpb=8209.2, bsz=305.6, num_updates=41000, lr=6.9843e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=69, gb_free=16.4, wall=33889
2023-08-06 20:08:55 | INFO | train_inner | epoch 028:   1319 / 1474 loss=1.977, trans_loss=4.986, nll_loss=2.178, w2v_ctc_loss=0.628, task_loss=1.534, contrastive_loss=0.063, total=4087.78, n_correct=2688.34, ppl=4.52, accuracy=65.765, wps=11722.9, ups=1.43, wpb=8175.6, bsz=285.1, num_updates=41100, lr=6.9758e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=69, gb_free=14.9, wall=33959
2023-08-06 20:10:05 | INFO | train_inner | epoch 028:   1419 / 1474 loss=1.972, trans_loss=4.982, nll_loss=2.174, w2v_ctc_loss=0.614, task_loss=1.466, contrastive_loss=0.084, total=4145.03, n_correct=2729.37, ppl=4.51, accuracy=65.847, wps=11854.9, ups=1.43, wpb=8290.1, bsz=297.6, num_updates=41200, lr=6.96733e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=69, gb_free=17.6, wall=34029
2023-08-06 20:10:42 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([24, 54]), X shape:torch.Size([24, 54, 512])
CTC Tokens:tensor([ 0, 29,  0,  4, 71], device='cuda:7'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:7'), New Tokens:tensor([ 0, 29,  0,  4, 71], device='cuda:7')
Mixup Sent Mask:tensor([[ 1],
        [ 3],
        [ 9],
        [12],
        [14],
        [19],
        [21],
        [23]], device='cuda:7'), 1,  Mixup Mask:tensor([ True,  True,  True, False,  True], device='cuda:7'), 
                    Org X:tensor([[-0.0961, -1.2979,  0.3210,  ...,  0.1871, -1.0029,  0.8213],
        [-0.6719,  0.2903, -0.3047,  ...,  0.5171, -0.2404,  0.3481],
        [-0.0550,  0.2515,  1.1572,  ...,  0.0254, -0.3831, -0.3254],
        [ 0.1736,  0.5347,  0.9395,  ..., -0.3860,  0.8408,  0.4429],
        [-0.4150,  0.0824, -0.7598,  ...,  0.2469,  1.7285,  0.6294]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [-1.0967, -0.0187, -0.2224,  ..., -0.0938, -2.1953,  3.4453],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [ 0.1736,  0.5347,  0.9395,  ..., -0.3860,  0.8408,  0.4429],
        [ 0.0291,  0.1262, -0.3069,  ..., -0.7964, -0.3704, -1.6211]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [-1.0967, -0.0187, -0.2224,  ..., -0.0938, -2.1953,  3.4453],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [-0.2139, -0.3711, -0.2256,  ..., -1.4980, -1.0596, -2.1523],
        [ 0.0291,  0.1262, -0.3069,  ..., -0.7964, -0.3704, -1.6211]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 100]), X shape:torch.Size([8, 100, 512])
CTC Tokens:tensor([  29,   29,    0,    0, 1042], device='cuda:5'), Shrink Mask:tensor([ True, False,  True, False,  True], device='cuda:5'), New Tokens:tensor([  29,    0, 1042,    0,   25], device='cuda:5')
Mixup Sent Mask:tensor([[1],
        [3]], device='cuda:5'), 1,  Mixup Mask:tensor([False, False,  True,  True, False], device='cuda:5'), 
                    Org X:tensor([[ 0.2156, -1.5049,  0.2905,  ..., -0.0557, -1.3857,  0.4446],
        [-0.1311,  0.0746,  1.4375,  ..., -0.4111, -0.9087,  0.0421],
        [ 0.4780, -0.6226,  0.9990,  ...,  0.2778,  1.2383,  0.7744],
        [ 1.1416, -0.6294,  1.6729,  ...,  0.9067,  1.6562,  0.3923],
        [ 1.1318, -1.0264, -2.3770,  ...,  0.2681,  1.8311, -1.1172]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.2156, -1.5049,  0.2905,  ..., -0.0557, -1.3857,  0.4446],
        [-0.1311,  0.0746,  1.4375,  ..., -0.4111, -0.9087,  0.0421],
        [-0.4424, -0.0481,  0.7090,  ...,  2.3340, -1.1816, -1.6865],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [ 1.1318, -1.0264, -2.3770,  ...,  0.2681,  1.8311, -1.1172]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-1.0967, -0.0187, -0.2224,  ..., -0.0938, -2.1953,  3.4453],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [-0.4424, -0.0481,  0.7090,  ...,  2.3340, -1.1816, -1.6865],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [ 0.3181, -0.4902, -0.1173,  ..., -1.1230,  0.4060, -2.4883]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 81]), X shape:torch.Size([8, 81, 512])
CTC Tokens:tensor([  0,   0, 419,   0,   0], device='cuda:2'), Shrink Mask:tensor([ True, False,  True,  True, False], device='cuda:2'), New Tokens:tensor([   0,  419,    0, 5186,    0], device='cuda:2')
Mixup Sent Mask:tensor([[1],
        [3]], device='cuda:2'), 1,  Mixup Mask:tensor([False, False,  True, False, False], device='cuda:2'), 
                    Org X:tensor([[-0.3450,  0.0775, -0.2363,  ..., -0.3896,  0.8003,  0.7998],
        [-0.6182,  1.0469, -2.0156,  ...,  0.1403,  1.2539,  0.8242],
        [-0.9937,  0.8291, -0.3413,  ..., -0.2168,  1.6396,  0.8179],
        [ 2.4492,  0.1996,  1.6719,  ..., -1.4434, -0.1857, -0.1238],
        [ 2.3926,  0.2391,  2.9629,  ..., -2.5566,  0.8364, -1.0479]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3450,  0.0775, -0.2363,  ..., -0.3896,  0.8003,  0.7998],
        [-0.6182,  1.0469, -2.0156,  ...,  0.1403,  1.2539,  0.8242],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [ 2.4492,  0.1996,  1.6719,  ..., -1.4434, -0.1857, -0.1238],
        [ 2.3926,  0.2391,  2.9629,  ..., -2.5566,  0.8364, -1.0479]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [-0.4861,  0.4409, -0.2844,  ...,  0.9531,  0.8374, -1.8945],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [ 1.1914,  0.6055,  0.4724,  ...,  0.0049, -0.3652, -1.1553],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([6, 168]), X shape:torch.Size([6, 168, 512])
CTC Tokens:tensor([ 0,  0, 12,  0,  7], device='cuda:3'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:3'), New Tokens:tensor([ 0, 12,  0,  7,  0], device='cuda:3')
Mixup Sent Mask:tensor([[1],
        [3]], device='cuda:3'), 1,  Mixup Mask:tensor([ True, False,  True,  True, False], device='cuda:3'), 
                    Org X:tensor([[-0.3931, -0.6465,  0.7031,  ..., -0.0093, -1.2031,  0.2607],
        [-0.6787,  0.3105,  2.4570,  ...,  0.2817, -2.4824, -0.0732],
        [-0.9966,  0.8418,  2.0801,  ...,  0.2747, -1.3877, -0.4902],
        [-1.0977,  0.0873, -0.4001,  ...,  0.1044,  0.1316, -0.3447],
        [-1.1240,  0.7227,  2.0352,  ..., -0.4885,  1.1963,  0.1719]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [-0.6787,  0.3105,  2.4570,  ...,  0.2817, -2.4824, -0.0732],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [-0.3169, -0.1808, -0.2052,  ..., -0.1072, -1.7871, -1.5049],
        [-1.1240,  0.7227,  2.0352,  ..., -0.4885,  1.1963,  0.1719]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [-0.1664,  0.0335,  0.1084,  ...,  0.5356, -3.2793, -0.6289],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [-0.3169, -0.1808, -0.2052,  ..., -0.1072, -1.7871, -1.5049],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([128, 14]), X shape:torch.Size([128, 14, 512])
CTC Tokens:tensor([  8,   0, 568, 568,   0], device='cuda:4'), Shrink Mask:tensor([ True,  True,  True, False,  True], device='cuda:4'), New Tokens:tensor([  8,   0, 568,   0,  17], device='cuda:4')
Mixup Sent Mask:tensor([[  1],
        [  3],
        [  9],
        [ 12],
        [ 14],
        [ 19],
        [ 21],
        [ 23],
        [ 24],
        [ 29],
        [ 35],
        [ 36],
        [ 37],
        [ 46],
        [ 47],
        [ 49],
        [ 52],
        [ 55],
        [ 57],
        [ 60],
        [ 62],
        [ 65],
        [ 70],
        [ 75],
        [ 77],
        [ 80],
        [ 83],
        [ 84],
        [ 91],
        [ 94],
        [ 97],
        [ 99],
        [101],
        [104],
        [105],
        [106],
        [111],
        [114],
        [117],
        [124],
        [125]], device='cuda:4'), 1,  Mixup Mask:tensor([ True,  True,  True,  True, False], device='cuda:4'), 
                    Org X:tensor([[ 2.2852e-01, -4.6606e-01, -4.4250e-04,  ..., -2.3120e-01,
          1.5967e+00, -3.5858e-02],
        [-7.4805e-01, -4.5288e-01,  1.4033e+00,  ..., -7.1240e-01,
          2.9297e-01,  8.9941e-01],
        [-9.8779e-01, -8.3069e-02,  6.7969e-01,  ..., -1.1169e-01,
          4.2651e-01, -1.1243e-01],
        [-5.4395e-01,  2.0093e-01, -1.1660e+00,  ..., -1.5967e+00,
          1.2441e+00, -5.8887e-01],
        [-2.2441e+00,  5.4004e-01, -1.9072e+00,  ..., -4.3164e-01,
          6.2451e-01, -5.3125e-01]], device='cuda:4', dtype=torch.float16,
       grad_fn=<SliceBackward0>), New X:tensor([[-0.3674, -0.0518, -0.5205,  ...,  0.9941, -1.5205, -1.6162],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [-0.1826,  0.4287,  0.6455,  ..., -0.0771, -0.8223, -2.0156],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [-2.2441,  0.5400, -1.9072,  ..., -0.4316,  0.6245, -0.5312]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3674, -0.0518, -0.5205,  ...,  0.9941, -1.5205, -1.6162],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [-0.1826,  0.4287,  0.6455,  ..., -0.0771, -0.8223, -2.0156],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [-0.2052, -0.1902, -0.3811,  ...,  0.3516, -1.4814, -2.1641]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 79]), X shape:torch.Size([16, 79, 512])
CTC Tokens:tensor([ 21, 169, 169,  51,  51], device='cuda:1'), Shrink Mask:tensor([ True,  True, False,  True, False], device='cuda:1'), New Tokens:tensor([ 21, 169,  51,  13,   0], device='cuda:1')
Mixup Sent Mask:tensor([[ 1],
        [ 3],
        [ 9],
        [12],
        [14]], device='cuda:1'), 1,  Mixup Mask:tensor([ True, False,  True, False,  True], device='cuda:1'), 
                    Org X:tensor([[-0.4673, -0.9541, -2.4785,  ..., -0.3398,  1.4873, -0.4573],
        [ 1.0176, -1.9795, -0.4668,  ..., -0.9253,  2.9043,  0.1729],
        [-0.1382, -0.1799, -0.8555,  ...,  0.1217,  0.9043, -0.7827],
        [-0.6670,  0.7310, -3.2227,  ...,  0.1248,  1.6553,  0.2842],
        [-0.3792,  0.2673, -1.4902,  ...,  0.4448,  1.8174,  0.0515]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.4346, -0.2035, -0.2573,  ..., -0.8823, -3.0957, -2.0977],
        [ 1.0176, -1.9795, -0.4668,  ..., -0.9253,  2.9043,  0.1729],
        [-1.6768,  0.7354,  0.5205,  ..., -1.7432,  2.1094,  0.4019],
        [-0.6670,  0.7310, -3.2227,  ...,  0.1248,  1.6553,  0.2842],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4346, -0.2035, -0.2573,  ..., -0.8823, -3.0957, -2.0977],
        [ 0.0737, -0.3591,  0.6011,  ..., -0.8037,  2.5254, -1.8574],
        [-1.6768,  0.7354,  0.5205,  ..., -1.7432,  2.1094,  0.4019],
        [-0.4844, -0.2250,  0.0880,  ..., -1.5596, -0.3823,  0.6221],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([24, 51]), X shape:torch.Size([24, 51, 512])
CTC Tokens:tensor([101, 101,  11,   6,   6], device='cuda:6'), Shrink Mask:tensor([ True, False,  True,  True, False], device='cuda:6'), New Tokens:tensor([101,  11,   6,   0,  80], device='cuda:6')
Mixup Sent Mask:tensor([[ 1],
        [ 3],
        [ 9],
        [12],
        [14],
        [19],
        [21],
        [23]], device='cuda:6'), 1,  Mixup Mask:tensor([ True, False,  True,  True,  True], device='cuda:6'), 
                    Org X:tensor([[ 0.1558, -0.2886, -1.7451,  ..., -0.3542, -0.3333,  1.5000],
        [ 1.7432,  0.2443, -1.1943,  ..., -0.3652,  0.1832,  0.3403],
        [ 1.8320, -0.1592, -1.8076,  ..., -1.4121,  0.1677,  0.3982],
        [-1.1484,  1.7627, -0.3867,  ..., -1.8125,  0.2703, -0.4438],
        [-2.3750,  1.9062, -0.5312,  ..., -0.7764,  0.0562, -0.6948]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.5156,  0.0415,  0.4294,  ...,  0.3521, -1.6807,  1.6562],
        [ 1.7432,  0.2443, -1.1943,  ..., -0.3652,  0.1832,  0.3403],
        [ 0.1631, -0.4360, -0.1758,  ..., -2.9746, -3.2598,  0.8901],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [-0.4011,  0.3625, -0.3184,  ...,  1.2930, -1.2314, -1.6396]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5156,  0.0415,  0.4294,  ...,  0.3521, -1.6807,  1.6562],
        [-0.4353, -0.3445, -0.3542,  ..., -2.5488, -0.4307, -3.8125],
        [ 0.1631, -0.4360, -0.1758,  ..., -2.9746, -3.2598,  0.8901],
        [-0.4417, -0.8320,  0.2920,  ..., -2.4570, -1.6436, -1.8076],
        [-0.4011,  0.3625, -0.3184,  ...,  1.2930, -1.2314, -1.6396]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:6')
2023-08-06 20:11:07 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 4.217 | trans_loss 5.555 | nll_loss 2.827 | w2v_ctc_loss 1.333 | task_loss 4.633 | contrastive_loss 0.243 | total 4003.4 | n_correct 2492.2 | ppl 7.1 | accuracy 62.252 | uer 16.869 | wer 18.758 | raw_wer 18.758 | bleu 20.37 | wps 1997.8 | wpb 4003.4 | bsz 141.8 | num_updates 41255 | best_bleu 20.52
2023-08-06 20:11:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 41255 updates
2023-08-06 20:11:07 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.3706.pt
2023-08-06 20:11:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.3706.pt
2023-08-06 20:11:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.3706.pt (epoch 28 @ 41255 updates, score 20.37) (writing took 14.062115143984556 seconds)
2023-08-06 20:11:22 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-08-06 20:11:22 | INFO | train | epoch 028 | loss 1.967 | trans_loss 4.97 | nll_loss 2.159 | w2v_ctc_loss 0.613 | task_loss 1.4 | contrastive_loss 0.093 | total 4138.65 | n_correct 2736.91 | ppl 4.46 | accuracy 66.13 | wps 10650.4 | ups 1.29 | wpb 8277.3 | bsz 305.7 | num_updates 41255 | lr 6.96268e-05 | gnorm 0.536 | clip 0 | loss_scale 64 | train_wall 1017 | gb_free 16.3 | wall 34106
2023-08-06 20:11:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 20:11:22 | INFO | fairseq.trainer | begin training epoch 29
2023-08-06 20:11:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 20:12:00 | INFO | train_inner | epoch 029:     45 / 1474 loss=1.954, trans_loss=4.954, nll_loss=2.138, w2v_ctc_loss=0.612, task_loss=1.352, contrastive_loss=0.053, total=4163.06, n_correct=2767.17, ppl=4.4, accuracy=66.47, wps=7195.8, ups=0.86, wpb=8326.1, bsz=314, num_updates=41300, lr=6.95889e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=69, gb_free=16, wall=34145
2023-08-06 20:13:10 | INFO | train_inner | epoch 029:    145 / 1474 loss=1.958, trans_loss=4.953, nll_loss=2.136, w2v_ctc_loss=0.61, task_loss=1.372, contrastive_loss=0.08, total=4116.29, n_correct=2737.45, ppl=4.39, accuracy=66.503, wps=11791.7, ups=1.43, wpb=8232.6, bsz=308.5, num_updates=41400, lr=6.95048e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=69, gb_free=13.8, wall=34214
2023-08-06 20:14:20 | INFO | train_inner | epoch 029:    245 / 1474 loss=1.959, trans_loss=4.946, nll_loss=2.127, w2v_ctc_loss=0.595, task_loss=1.277, contrastive_loss=0.159, total=4197.24, n_correct=2796.8, ppl=4.37, accuracy=66.634, wps=12077.7, ups=1.44, wpb=8394.5, bsz=329.9, num_updates=41500, lr=6.9421e-05, gnorm=0.527, clip=0, loss_scale=64, train_wall=69, gb_free=17, wall=34284
2023-08-06 20:15:30 | INFO | train_inner | epoch 029:    345 / 1474 loss=1.962, trans_loss=4.966, nll_loss=2.153, w2v_ctc_loss=0.619, task_loss=1.501, contrastive_loss=0.046, total=4092.21, n_correct=2713.98, ppl=4.45, accuracy=66.321, wps=11675.8, ups=1.43, wpb=8184.4, bsz=290.6, num_updates=41600, lr=6.93375e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=70, gb_free=15.5, wall=34354
2023-08-06 20:16:39 | INFO | train_inner | epoch 029:    445 / 1474 loss=1.943, trans_loss=4.941, nll_loss=2.12, w2v_ctc_loss=0.602, task_loss=1.351, contrastive_loss=0.039, total=4161.27, n_correct=2779.23, ppl=4.35, accuracy=66.788, wps=12051.8, ups=1.45, wpb=8322.5, bsz=307.8, num_updates=41700, lr=6.92543e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=15.8, wall=34423
2023-08-06 20:17:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-06 20:17:49 | INFO | train_inner | epoch 029:    546 / 1474 loss=1.963, trans_loss=4.966, nll_loss=2.152, w2v_ctc_loss=0.608, task_loss=1.51, contrastive_loss=0.074, total=4137.09, n_correct=2739.67, ppl=4.45, accuracy=66.222, wps=11775.1, ups=1.42, wpb=8274.2, bsz=290.3, num_updates=41800, lr=6.91714e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=70, gb_free=15.6, wall=34493
2023-08-06 20:18:58 | INFO | train_inner | epoch 029:    646 / 1474 loss=1.967, trans_loss=4.956, nll_loss=2.141, w2v_ctc_loss=0.601, task_loss=1.323, contrastive_loss=0.208, total=4143.02, n_correct=2749.77, ppl=4.41, accuracy=66.371, wps=11989.9, ups=1.45, wpb=8286, bsz=318.6, num_updates=41900, lr=6.90889e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=69, gb_free=16.9, wall=34562
2023-08-06 20:20:08 | INFO | train_inner | epoch 029:    746 / 1474 loss=1.958, trans_loss=4.956, nll_loss=2.14, w2v_ctc_loss=0.6, task_loss=1.292, contrastive_loss=0.118, total=4249.79, n_correct=2824.73, ppl=4.41, accuracy=66.468, wps=12108.4, ups=1.42, wpb=8499.6, bsz=330, num_updates=42000, lr=6.90066e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=70, gb_free=16.5, wall=34633
2023-08-06 20:20:08 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 20:20:33 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 4.22 | trans_loss 5.557 | nll_loss 2.83 | w2v_ctc_loss 1.334 | task_loss 4.611 | contrastive_loss 0.246 | total 4003.4 | n_correct 2489.6 | ppl 7.11 | accuracy 62.187 | uer 17.302 | wer 19.142 | raw_wer 19.142 | bleu 20.07 | wps 2056.9 | wpb 4003.4 | bsz 141.8 | num_updates 42000 | best_bleu 20.52
2023-08-06 20:20:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42000 updates
2023-08-06 20:20:33 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_29_42000.pt
2023-08-06 20:20:36 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_29_42000.pt
2023-08-06 20:20:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_29_42000.pt (epoch 29 @ 42000 updates, score 20.07) (writing took 21.2796419095248 seconds)
2023-08-06 20:22:04 | INFO | train_inner | epoch 029:    846 / 1474 loss=1.966, trans_loss=4.98, nll_loss=2.17, w2v_ctc_loss=0.613, task_loss=1.555, contrastive_loss=0.04, total=4027.19, n_correct=2654.47, ppl=4.5, accuracy=65.914, wps=6996.3, ups=0.87, wpb=8054.4, bsz=280.6, num_updates=42100, lr=6.89246e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=69, gb_free=17.1, wall=34748
2023-08-06 20:23:13 | INFO | train_inner | epoch 029:    946 / 1474 loss=1.963, trans_loss=4.977, nll_loss=2.167, w2v_ctc_loss=0.614, task_loss=1.427, contrastive_loss=0.049, total=4082.14, n_correct=2702.35, ppl=4.49, accuracy=66.199, wps=11802.4, ups=1.45, wpb=8164.3, bsz=296.3, num_updates=42200, lr=6.88428e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=69, gb_free=15.2, wall=34817
2023-08-06 20:24:22 | INFO | train_inner | epoch 029:   1046 / 1474 loss=1.961, trans_loss=4.963, nll_loss=2.149, w2v_ctc_loss=0.603, task_loss=1.392, contrastive_loss=0.12, total=4148.18, n_correct=2753.43, ppl=4.43, accuracy=66.377, wps=11961.6, ups=1.44, wpb=8296.4, bsz=308.2, num_updates=42300, lr=6.87614e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=34886
2023-08-06 20:25:31 | INFO | train_inner | epoch 029:   1146 / 1474 loss=1.963, trans_loss=4.979, nll_loss=2.169, w2v_ctc_loss=0.616, task_loss=1.536, contrastive_loss=0.035, total=4063.95, n_correct=2684.71, ppl=4.5, accuracy=66.062, wps=11741.5, ups=1.44, wpb=8127.9, bsz=283, num_updates=42400, lr=6.86803e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=69, gb_free=13.1, wall=34955
2023-08-06 20:26:41 | INFO | train_inner | epoch 029:   1246 / 1474 loss=1.962, trans_loss=4.978, nll_loss=2.169, w2v_ctc_loss=0.615, task_loss=1.422, contrastive_loss=0.042, total=4158.81, n_correct=2748.4, ppl=4.5, accuracy=66.086, wps=11994.3, ups=1.44, wpb=8317.6, bsz=301.2, num_updates=42500, lr=6.85994e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=69, gb_free=15.5, wall=35025
2023-08-06 20:27:50 | INFO | train_inner | epoch 029:   1346 / 1474 loss=1.966, trans_loss=4.971, nll_loss=2.16, w2v_ctc_loss=0.606, task_loss=1.378, contrastive_loss=0.105, total=4166.34, n_correct=2756.1, ppl=4.47, accuracy=66.152, wps=12007.4, ups=1.44, wpb=8332.7, bsz=310.7, num_updates=42600, lr=6.85189e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=69, gb_free=17.7, wall=35094
2023-08-06 20:28:59 | INFO | train_inner | epoch 029:   1446 / 1474 loss=1.969, trans_loss=4.97, nll_loss=2.16, w2v_ctc_loss=0.611, task_loss=1.373, contrastive_loss=0.134, total=4162.2, n_correct=2753.05, ppl=4.47, accuracy=66.144, wps=12081.8, ups=1.45, wpb=8324.4, bsz=311.5, num_updates=42700, lr=6.84386e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=68, gb_free=17.2, wall=35163
2023-08-06 20:29:18 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 20:29:41 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 4.204 | trans_loss 5.55 | nll_loss 2.821 | w2v_ctc_loss 1.299 | task_loss 4.638 | contrastive_loss 0.243 | total 4003.4 | n_correct 2489.5 | ppl 7.07 | accuracy 62.185 | uer 16.71 | wer 18.497 | raw_wer 18.497 | bleu 19.95 | wps 2215.1 | wpb 4003.4 | bsz 141.8 | num_updates 42728 | best_bleu 20.52
2023-08-06 20:29:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42728 updates
2023-08-06 20:29:41 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_last.pt
2023-08-06 20:29:54 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_last.pt
2023-08-06 20:29:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_last.pt (epoch 29 @ 42728 updates, score 19.95) (writing took 12.525178957730532 seconds)
2023-08-06 20:29:54 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-08-06 20:29:54 | INFO | train | epoch 029 | loss 1.961 | trans_loss 4.964 | nll_loss 2.15 | w2v_ctc_loss 0.607 | task_loss 1.401 | contrastive_loss 0.089 | total 4137.69 | n_correct 2744.11 | ppl 4.44 | accuracy 66.32 | wps 10961.6 | ups 1.32 | wpb 8275.4 | bsz 305.3 | num_updates 42728 | lr 6.84162e-05 | gnorm 0.536 | clip 0 | loss_scale 32 | train_wall 1016 | gb_free 15.9 | wall 35218
2023-08-06 20:29:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 20:29:54 | INFO | fairseq.trainer | begin training epoch 30
2023-08-06 20:29:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 20:30:51 | INFO | train_inner | epoch 030:     72 / 1474 loss=1.955, trans_loss=4.946, nll_loss=2.127, w2v_ctc_loss=0.593, task_loss=1.327, contrastive_loss=0.147, total=4182.65, n_correct=2789.14, ppl=4.37, accuracy=66.684, wps=7471, ups=0.89, wpb=8365.3, bsz=320.5, num_updates=42800, lr=6.83586e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=69, gb_free=13.3, wall=35275
2023-08-06 20:32:00 | INFO | train_inner | epoch 030:    172 / 1474 loss=1.944, trans_loss=4.93, nll_loss=2.106, w2v_ctc_loss=0.6, task_loss=1.314, contrastive_loss=0.08, total=4203.05, n_correct=2818.26, ppl=4.31, accuracy=67.053, wps=12122, ups=1.44, wpb=8406.1, bsz=318.3, num_updates=42900, lr=6.82789e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=69, gb_free=17.4, wall=35344
2023-08-06 20:33:10 | INFO | train_inner | epoch 030:    272 / 1474 loss=1.949, trans_loss=4.945, nll_loss=2.125, w2v_ctc_loss=0.611, task_loss=1.444, contrastive_loss=0.037, total=4116.93, n_correct=2744.65, ppl=4.36, accuracy=66.667, wps=11887, ups=1.44, wpb=8233.9, bsz=295.1, num_updates=43000, lr=6.81994e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=69, gb_free=16.9, wall=35414
2023-08-06 20:34:19 | INFO | train_inner | epoch 030:    372 / 1474 loss=1.941, trans_loss=4.94, nll_loss=2.119, w2v_ctc_loss=0.596, task_loss=1.406, contrastive_loss=0.041, total=4173.13, n_correct=2790.32, ppl=4.34, accuracy=66.864, wps=11941.5, ups=1.43, wpb=8346.3, bsz=305.6, num_updates=43100, lr=6.81203e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=69, gb_free=11.5, wall=35484
2023-08-06 20:35:29 | INFO | train_inner | epoch 030:    472 / 1474 loss=1.95, trans_loss=4.946, nll_loss=2.127, w2v_ctc_loss=0.594, task_loss=1.334, contrastive_loss=0.103, total=4135.2, n_correct=2757.7, ppl=4.37, accuracy=66.688, wps=11968.9, ups=1.45, wpb=8270.4, bsz=314.8, num_updates=43200, lr=6.80414e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=69, gb_free=16.8, wall=35553
2023-08-06 20:36:38 | INFO | train_inner | epoch 030:    572 / 1474 loss=1.951, trans_loss=4.956, nll_loss=2.14, w2v_ctc_loss=0.601, task_loss=1.365, contrastive_loss=0.064, total=4168.65, n_correct=2772.3, ppl=4.41, accuracy=66.504, wps=12013, ups=1.44, wpb=8337.3, bsz=312.1, num_updates=43300, lr=6.79628e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=69, gb_free=15.7, wall=35622
2023-08-06 20:37:47 | INFO | train_inner | epoch 030:    672 / 1474 loss=1.957, trans_loss=4.955, nll_loss=2.139, w2v_ctc_loss=0.611, task_loss=1.383, contrastive_loss=0.077, total=4183.65, n_correct=2779.76, ppl=4.41, accuracy=66.443, wps=12099.8, ups=1.45, wpb=8367.3, bsz=314.5, num_updates=43400, lr=6.78844e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=69, gb_free=16.7, wall=35691
2023-08-06 20:38:57 | INFO | train_inner | epoch 030:    772 / 1474 loss=1.977, trans_loss=4.968, nll_loss=2.156, w2v_ctc_loss=0.618, task_loss=1.432, contrastive_loss=0.157, total=4106.9, n_correct=2718.95, ppl=4.46, accuracy=66.204, wps=11711, ups=1.43, wpb=8213.8, bsz=302.9, num_updates=43500, lr=6.78064e-05, gnorm=0.549, clip=0, loss_scale=32, train_wall=70, gb_free=11.7, wall=35761
2023-08-06 20:40:07 | INFO | train_inner | epoch 030:    872 / 1474 loss=1.953, trans_loss=4.962, nll_loss=2.147, w2v_ctc_loss=0.603, task_loss=1.467, contrastive_loss=0.043, total=4089.18, n_correct=2714.61, ppl=4.43, accuracy=66.385, wps=11809.3, ups=1.44, wpb=8178.4, bsz=291.4, num_updates=43600, lr=6.77285e-05, gnorm=0.537, clip=0, loss_scale=32, train_wall=69, gb_free=16.3, wall=35831
2023-08-06 20:41:16 | INFO | train_inner | epoch 030:    972 / 1474 loss=1.957, trans_loss=4.966, nll_loss=2.153, w2v_ctc_loss=0.608, task_loss=1.412, contrastive_loss=0.059, total=4140.03, n_correct=2747.41, ppl=4.45, accuracy=66.362, wps=11950.4, ups=1.44, wpb=8280.1, bsz=303.9, num_updates=43700, lr=6.7651e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=69, gb_free=13.7, wall=35900
2023-08-06 20:42:26 | INFO | train_inner | epoch 030:   1072 / 1474 loss=1.974, trans_loss=4.975, nll_loss=2.163, w2v_ctc_loss=0.613, task_loss=1.567, contrastive_loss=0.133, total=4101.12, n_correct=2709.98, ppl=4.48, accuracy=66.079, wps=11746.3, ups=1.43, wpb=8202.2, bsz=282.8, num_updates=43800, lr=6.75737e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=69, gb_free=16.4, wall=35970
2023-08-06 20:43:35 | INFO | train_inner | epoch 030:   1172 / 1474 loss=1.955, trans_loss=4.961, nll_loss=2.148, w2v_ctc_loss=0.592, task_loss=1.343, contrastive_loss=0.111, total=4168.22, n_correct=2771, ppl=4.43, accuracy=66.479, wps=11957.1, ups=1.43, wpb=8336.4, bsz=314.5, num_updates=43900, lr=6.74967e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=69, gb_free=16.6, wall=36040
2023-08-06 20:44:45 | INFO | train_inner | epoch 030:   1272 / 1474 loss=1.962, trans_loss=4.971, nll_loss=2.159, w2v_ctc_loss=0.613, task_loss=1.565, contrastive_loss=0.045, total=4032.74, n_correct=2671.34, ppl=4.47, accuracy=66.241, wps=11551.5, ups=1.43, wpb=8065.5, bsz=281.8, num_updates=44000, lr=6.742e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=69, gb_free=16.7, wall=36109
2023-08-06 20:44:45 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 20:45:08 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 4.211 | trans_loss 5.553 | nll_loss 2.824 | w2v_ctc_loss 1.319 | task_loss 4.599 | contrastive_loss 0.243 | total 4003.4 | n_correct 2491.4 | ppl 7.08 | accuracy 62.232 | uer 16.832 | wer 18.657 | raw_wer 18.657 | bleu 20.26 | wps 2200.6 | wpb 4003.4 | bsz 141.8 | num_updates 44000 | best_bleu 20.52
2023-08-06 20:45:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44000 updates
2023-08-06 20:45:08 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_30_44000.pt
2023-08-06 20:45:12 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_30_44000.pt
2023-08-06 20:45:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_30_44000.pt (epoch 30 @ 44000 updates, score 20.26) (writing took 20.626276295632124 seconds)
2023-08-06 20:46:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-06 20:46:39 | INFO | train_inner | epoch 030:   1373 / 1474 loss=1.948, trans_loss=4.959, nll_loss=2.147, w2v_ctc_loss=0.6, task_loss=1.322, contrastive_loss=0.054, total=4167.55, n_correct=2771.12, ppl=4.43, accuracy=66.493, wps=7293.2, ups=0.88, wpb=8335.1, bsz=321.9, num_updates=44100, lr=6.73435e-05, gnorm=0.536, clip=0, loss_scale=32, train_wall=69, gb_free=16.5, wall=36224
2023-08-06 20:47:48 | INFO | train_inner | epoch 030:   1473 / 1474 loss=1.967, trans_loss=4.966, nll_loss=2.155, w2v_ctc_loss=0.594, task_loss=1.316, contrastive_loss=0.202, total=4141.76, n_correct=2748.57, ppl=4.45, accuracy=66.362, wps=12095.5, ups=1.46, wpb=8283.5, bsz=314.3, num_updates=44200, lr=6.72673e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=68, gb_free=16.2, wall=36292
2023-08-06 20:47:49 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 20:48:14 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 4.214 | trans_loss 5.554 | nll_loss 2.825 | w2v_ctc_loss 1.32 | task_loss 4.635 | contrastive_loss 0.246 | total 4003.4 | n_correct 2489.6 | ppl 7.09 | accuracy 62.187 | uer 16.885 | wer 18.881 | raw_wer 18.881 | bleu 20.27 | wps 1772.7 | wpb 4003.4 | bsz 141.8 | num_updates 44201 | best_bleu 20.52
2023-08-06 20:48:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44201 updates
2023-08-06 20:48:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.2706.pt
2023-08-06 20:48:27 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.2706.pt
2023-08-06 20:48:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.2706.pt (epoch 30 @ 44201 updates, score 20.27) (writing took 22.65880947932601 seconds)
2023-08-06 20:48:37 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-08-06 20:48:37 | INFO | train | epoch 030 | loss 1.956 | trans_loss 4.956 | nll_loss 2.14 | w2v_ctc_loss 0.603 | task_loss 1.401 | contrastive_loss 0.092 | total 4138.52 | n_correct 2752.25 | ppl 4.41 | accuracy 66.503 | wps 10858 | ups 1.31 | wpb 8277 | bsz 305.6 | num_updates 44201 | lr 6.72665e-05 | gnorm 0.537 | clip 0 | loss_scale 32 | train_wall 1017 | gb_free 17 | wall 36341
2023-08-06 20:48:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 20:48:37 | INFO | fairseq.trainer | begin training epoch 31
2023-08-06 20:48:37 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 20:49:53 | INFO | train_inner | epoch 031:     99 / 1474 loss=1.945, trans_loss=4.939, nll_loss=2.116, w2v_ctc_loss=0.606, task_loss=1.497, contrastive_loss=0.04, total=4054.44, n_correct=2709.09, ppl=4.33, accuracy=66.818, wps=6497.7, ups=0.8, wpb=8108.9, bsz=288.2, num_updates=44300, lr=6.71913e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=69, gb_free=16.3, wall=36417
2023-08-06 20:51:02 | INFO | train_inner | epoch 031:    199 / 1474 loss=1.947, trans_loss=4.942, nll_loss=2.12, w2v_ctc_loss=0.602, task_loss=1.431, contrastive_loss=0.066, total=4147.4, n_correct=2767.75, ppl=4.35, accuracy=66.735, wps=11912.2, ups=1.44, wpb=8294.8, bsz=302.2, num_updates=44400, lr=6.71156e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=69, gb_free=16.7, wall=36487
2023-08-06 20:52:12 | INFO | train_inner | epoch 031:    299 / 1474 loss=1.95, trans_loss=4.939, nll_loss=2.118, w2v_ctc_loss=0.597, task_loss=1.428, contrastive_loss=0.104, total=4149.21, n_correct=2771.93, ppl=4.34, accuracy=66.806, wps=11866.5, ups=1.43, wpb=8298.4, bsz=301.6, num_updates=44500, lr=6.70402e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=69, gb_free=16, wall=36557
2023-08-06 20:53:22 | INFO | train_inner | epoch 031:    399 / 1474 loss=1.949, trans_loss=4.95, nll_loss=2.13, w2v_ctc_loss=0.602, task_loss=1.529, contrastive_loss=0.043, total=4092.62, n_correct=2728.7, ppl=4.38, accuracy=66.674, wps=11813.2, ups=1.44, wpb=8185.2, bsz=285.6, num_updates=44600, lr=6.6965e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=69, gb_free=17.1, wall=36626
2023-08-06 20:54:31 | INFO | train_inner | epoch 031:    499 / 1474 loss=1.95, trans_loss=4.946, nll_loss=2.127, w2v_ctc_loss=0.61, task_loss=1.463, contrastive_loss=0.051, total=4111.85, n_correct=2740.51, ppl=4.37, accuracy=66.649, wps=11802.7, ups=1.44, wpb=8223.7, bsz=300.3, num_updates=44700, lr=6.689e-05, gnorm=0.538, clip=0, loss_scale=32, train_wall=69, gb_free=10.7, wall=36695
2023-08-06 20:55:41 | INFO | train_inner | epoch 031:    599 / 1474 loss=1.942, trans_loss=4.945, nll_loss=2.125, w2v_ctc_loss=0.594, task_loss=1.461, contrastive_loss=0.043, total=4083.44, n_correct=2722.3, ppl=4.36, accuracy=66.667, wps=11647.3, ups=1.43, wpb=8166.9, bsz=294.5, num_updates=44800, lr=6.68153e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=70, gb_free=16.7, wall=36766
2023-08-06 20:56:51 | INFO | train_inner | epoch 031:    699 / 1474 loss=1.937, trans_loss=4.942, nll_loss=2.122, w2v_ctc_loss=0.59, task_loss=1.331, contrastive_loss=0.043, total=4213.98, n_correct=2815.24, ppl=4.35, accuracy=66.807, wps=12147.8, ups=1.44, wpb=8428, bsz=315.7, num_updates=44900, lr=6.67409e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=69, gb_free=16.1, wall=36835
2023-08-06 20:58:00 | INFO | train_inner | epoch 031:    799 / 1474 loss=1.961, trans_loss=4.959, nll_loss=2.143, w2v_ctc_loss=0.603, task_loss=1.466, contrastive_loss=0.111, total=4097.37, n_correct=2719.03, ppl=4.42, accuracy=66.36, wps=11798.2, ups=1.44, wpb=8194.7, bsz=295.8, num_updates=45000, lr=6.66667e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=69, gb_free=12.8, wall=36904
Mixup rate:0.5, token after shrink shape:torch.Size([16, 77]), X shape:torch.Size([16, 77, 512])
CTC Tokens:tensor([168,  11,   6,   6,   7], device='cuda:0'), Shrink Mask:tensor([ True,  True,  True, False,  True], device='cuda:0'), New Tokens:tensor([168,  11,   6,   7, 279], device='cuda:0')
Mixup Sent Mask:tensor([[ 2],
        [ 3],
        [ 8],
        [11],
        [12],
        [13],
        [15]], device='cuda:0'), 2,  Mixup Mask:tensor([False,  True, False,  True,  True], device='cuda:0'), 
                    Org X:tensor([[-1.0449,  0.8281, -0.7866,  ...,  0.0900,  0.2498,  1.0225],
        [ 0.0872,  1.0430, -0.8374,  ..., -0.0192,  0.0661,  0.9551],
        [ 0.7524,  0.9243, -0.9272,  ...,  0.2668, -0.0685,  0.2209],
        [ 0.5117, -0.2278, -1.7002,  ..., -0.1032, -1.4033, -0.1537],
        [ 0.3945,  0.5820, -0.8486,  ..., -0.2190,  1.6934,  0.0822]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-1.0449,  0.8281, -0.7866,  ...,  0.0900,  0.2498,  1.0225],
        [-0.5146, -0.3208, -0.2798,  ..., -2.5820, -0.5508, -3.7070],
        [ 0.7524,  0.9243, -0.9272,  ...,  0.2668, -0.0685,  0.2209],
        [-0.2874, -0.2274, -0.1625,  ..., -0.1914, -1.7529, -1.5635],
        [ 0.0115,  0.9780, -0.5669,  ...,  1.1904,  1.9580, -0.6807]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[ 0.5132,  0.3127, -1.0459,  ..., -0.5532, -2.8398, -0.1641],
        [-0.5146, -0.3208, -0.2798,  ..., -2.5820, -0.5508, -3.7070],
        [ 0.1375, -0.4153, -0.1128,  ..., -2.9785, -3.1484,  0.8008],
        [-0.2874, -0.2274, -0.1625,  ..., -0.1914, -1.7529, -1.5635],
        [ 0.0115,  0.9780, -0.5669,  ...,  1.1904,  1.9580, -0.6807]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:0')
2023-08-06 20:59:10 | INFO | train_inner | epoch 031:    899 / 1474 loss=1.949, trans_loss=4.948, nll_loss=2.129, w2v_ctc_loss=0.603, task_loss=1.456, contrastive_loss=0.057, total=4096.72, n_correct=2729.65, ppl=4.38, accuracy=66.63, wps=11736.4, ups=1.43, wpb=8193.4, bsz=296.1, num_updates=45100, lr=6.65927e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=69, gb_free=17.1, wall=36974
2023-08-06 21:00:20 | INFO | train_inner | epoch 031:    999 / 1474 loss=1.958, trans_loss=4.956, nll_loss=2.142, w2v_ctc_loss=0.597, task_loss=1.323, contrastive_loss=0.14, total=4187.84, n_correct=2786.9, ppl=4.41, accuracy=66.547, wps=12065.3, ups=1.44, wpb=8375.7, bsz=319.5, num_updates=45200, lr=6.6519e-05, gnorm=0.538, clip=0, loss_scale=32, train_wall=69, gb_free=17.2, wall=37044
2023-08-06 21:01:29 | INFO | train_inner | epoch 031:   1099 / 1474 loss=1.952, trans_loss=4.954, nll_loss=2.138, w2v_ctc_loss=0.596, task_loss=1.368, contrastive_loss=0.085, total=4149.44, n_correct=2763.5, ppl=4.4, accuracy=66.599, wps=11968.7, ups=1.44, wpb=8298.9, bsz=315, num_updates=45300, lr=6.64455e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=69, gb_free=17.5, wall=37113
2023-08-06 21:02:38 | INFO | train_inner | epoch 031:   1199 / 1474 loss=1.964, trans_loss=4.957, nll_loss=2.143, w2v_ctc_loss=0.596, task_loss=1.31, contrastive_loss=0.203, total=4189.76, n_correct=2786.05, ppl=4.42, accuracy=66.497, wps=12151.1, ups=1.45, wpb=8379.5, bsz=321.3, num_updates=45400, lr=6.63723e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=69, gb_free=13, wall=37182
2023-08-06 21:03:47 | INFO | train_inner | epoch 031:   1299 / 1474 loss=1.945, trans_loss=4.956, nll_loss=2.142, w2v_ctc_loss=0.599, task_loss=1.258, contrastive_loss=0.048, total=4227.44, n_correct=2816.99, ppl=4.41, accuracy=66.636, wps=12193.5, ups=1.44, wpb=8454.9, bsz=326.3, num_updates=45500, lr=6.62994e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=69, gb_free=16.2, wall=37251
2023-08-06 21:04:57 | INFO | train_inner | epoch 031:   1399 / 1474 loss=1.977, trans_loss=4.959, nll_loss=2.146, w2v_ctc_loss=0.598, task_loss=1.286, contrastive_loss=0.251, total=4186.05, n_correct=2779.85, ppl=4.43, accuracy=66.407, wps=11947.8, ups=1.43, wpb=8372.1, bsz=326.6, num_updates=45600, lr=6.62266e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=70, gb_free=17.4, wall=37321
2023-08-06 21:05:49 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([8, 96]), X shape:torch.Size([8, 96, 512])
CTC Tokens:tensor([  8,  19,  19,   0, 226], device='cuda:4'), Shrink Mask:tensor([ True,  True, False,  True,  True], device='cuda:4'), New Tokens:tensor([   8,   19,    0,  226, 2432], device='cuda:4')
Mixup Sent Mask:tensor([[2],
        [3]], device='cuda:4'), 2,  Mixup Mask:tensor([False,  True,  True,  True,  True], device='cuda:4'), 
                    Org X:tensor([[ 0.5859, -0.1544, -0.0397,  ...,  0.9702,  0.5420, -1.4395],
        [-0.0438, -0.6401, -1.0361,  ...,  0.1416,  0.7944, -0.8433],
        [ 1.2227,  0.8115,  0.5249,  ..., -0.4617,  0.4473, -0.8179],
        [-0.1969,  1.0254,  1.6348,  ..., -0.2576,  0.4907, -1.1406],
        [ 2.4219,  0.4839,  0.1038,  ..., -1.1016,  0.4314,  1.2354]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.5859, -0.1544, -0.0397,  ...,  0.9702,  0.5420, -1.4395],
        [ 0.1849, -0.4653, -0.1660,  ..., -0.7119,  0.1937, -1.3457],
        [-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [ 1.4736, -0.8965,  0.1754,  ..., -1.0508,  0.0411,  0.5127],
        [ 1.3311,  0.8086, -0.6353,  ..., -1.6914,  0.6831,  0.3745]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4287, -0.1092, -0.5439,  ...,  1.0391, -1.4609, -1.5078],
        [ 0.1849, -0.4653, -0.1660,  ..., -0.7119,  0.1937, -1.3457],
        [-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [ 1.4736, -0.8965,  0.1754,  ..., -1.0508,  0.0411,  0.5127],
        [ 1.3311,  0.8086, -0.6353,  ..., -1.6914,  0.6831,  0.3745]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([80, 21]), X shape:torch.Size([80, 21, 512])
CTC Tokens:tensor([33, 33, 26, 39,  9], device='cuda:6'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:6'), New Tokens:tensor([  33,   26,   39,    9, 7261], device='cuda:6')
Mixup Sent Mask:tensor([[ 2],
        [ 3],
        [ 8],
        [11],
        [12],
        [13],
        [15],
        [16],
        [20],
        [31],
        [32],
        [34],
        [40],
        [43],
        [45],
        [51],
        [58],
        [59],
        [66],
        [69],
        [71],
        [75],
        [78],
        [79]], device='cuda:6'), 2,  Mixup Mask:tensor([ True, False, False, False, False], device='cuda:6'), 
                    Org X:tensor([[-1.1680,  0.0896, -0.0100,  ..., -0.3855, -0.3188,  1.1699],
        [ 0.4312, -0.3794, -0.7236,  ..., -1.9795, -0.5620,  0.4551],
        [ 0.0544, -0.1070, -3.1562,  ..., -1.2930, -1.3584,  0.9302],
        [ 0.2189, -0.4875, -2.6543,  ..., -0.0841,  0.2344,  0.5791],
        [ 0.5015, -1.5215, -0.0853,  ...,  0.3428,  0.4302,  0.1367]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3198, -0.1565,  0.0680,  ..., -1.9580, -2.8555, -0.8267],
        [ 0.4312, -0.3794, -0.7236,  ..., -1.9795, -0.5620,  0.4551],
        [ 0.0544, -0.1070, -3.1562,  ..., -1.2930, -1.3584,  0.9302],
        [ 0.2189, -0.4875, -2.6543,  ..., -0.0841,  0.2344,  0.5791],
        [ 0.5015, -1.5215, -0.0853,  ...,  0.3428,  0.4302,  0.1367]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3198, -0.1565,  0.0680,  ..., -1.9580, -2.8555, -0.8267],
        [-0.3149, -0.0664,  0.1693,  ..., -1.8828, -1.1543, -4.0039],
        [-1.1152, -0.6011,  1.2158,  ..., -1.2861, -2.2012,  0.5479],
        [-0.1941, -0.4124, -0.2798,  ...,  0.6265, -0.9438,  1.8877],
        [-0.4893,  1.1904, -0.5454,  ...,  2.0527, -0.3472,  1.0117]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([24, 52]), X shape:torch.Size([24, 52, 512])
CTC Tokens:tensor([19, 19,  0,  0,  0], device='cuda:7'), Shrink Mask:tensor([ True, False,  True, False, False], device='cuda:7'), New Tokens:tensor([  19,    0, 4600,    0,  134], device='cuda:7')
Mixup Sent Mask:tensor([[ 2],
        [ 3],
        [ 8],
        [11],
        [12],
        [13],
        [15],
        [16],
        [20]], device='cuda:7'), 2,  Mixup Mask:tensor([False, False,  True,  True,  True], device='cuda:7'), 
                    Org X:tensor([[ 0.7021, -0.3154, -0.6479,  ..., -0.0668,  0.8745, -1.0713],
        [ 1.7656,  1.5811,  2.5625,  ..., -0.5630,  1.0645, -1.3389],
        [ 2.5645,  1.1631,  0.8984,  ...,  0.7856, -1.1377,  0.4973],
        [ 2.2520,  1.3643,  1.7666,  ...,  0.6392, -1.2969, -0.1279],
        [ 0.6748,  1.5645, -1.0645,  ..., -0.9922,  0.2617,  0.3972]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.7021, -0.3154, -0.6479,  ..., -0.0668,  0.8745, -1.0713],
        [ 1.7656,  1.5811,  2.5625,  ..., -0.5630,  1.0645, -1.3389],
        [ 0.9839,  1.2705, -0.2267,  ...,  1.8848, -0.5234, -0.7236],
        [-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [-0.0461,  0.5669,  0.2000,  ..., -0.6982, -0.4412, -0.7407]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[ 0.1849, -0.4653, -0.1660,  ..., -0.7119,  0.1937, -1.3457],
        [-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [ 0.9839,  1.2705, -0.2267,  ...,  1.8848, -0.5234, -0.7236],
        [-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [-0.0461,  0.5669,  0.2000,  ..., -0.6982, -0.4412, -0.7407]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 58]), X shape:torch.Size([16, 58, 512])
CTC Tokens:tensor([  0,   0,   0, 401,   0], device='cuda:1'), Shrink Mask:tensor([ True, False, False,  True,  True], device='cuda:1'), New Tokens:tensor([  0, 401,   0,  21,   0], device='cuda:1')
Mixup Sent Mask:tensor([[ 2],
        [ 3],
        [ 8],
        [11],
        [12],
        [13],
        [15]], device='cuda:1'), 2,  Mixup Mask:tensor([ True, False, False,  True, False], device='cuda:1'), 
                    Org X:tensor([[ 0.3169, -0.4458,  0.8682,  ..., -1.0430,  1.3008,  0.1871],
        [-0.1509,  0.7197, -1.3672,  ...,  0.5537,  1.3125,  0.1116],
        [ 0.6978,  0.4695, -0.5562,  ..., -0.3484,  1.5293, -0.0947],
        [ 0.6826,  0.7192, -1.0957,  ..., -0.7188,  1.0869, -0.2773],
        [ 0.8018,  0.4517,  0.1628,  ..., -1.8340,  1.3008, -0.5835]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [-0.1509,  0.7197, -1.3672,  ...,  0.5537,  1.3125,  0.1116],
        [ 0.6978,  0.4695, -0.5562,  ..., -0.3484,  1.5293, -0.0947],
        [-0.4434, -0.1169, -0.2786,  ..., -0.8813, -3.0293, -2.0449],
        [ 0.8018,  0.4517,  0.1628,  ..., -1.8340,  1.3008, -0.5835]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [ 0.1735,  0.5728, -0.4651,  ...,  2.0527,  1.0576, -0.9453],
        [-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [-0.4434, -0.1169, -0.2786,  ..., -0.8813, -3.0293, -2.0449],
        [-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 85]), X shape:torch.Size([8, 85, 512])
CTC Tokens:tensor([0, 8, 0, 7, 0], device='cuda:3'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:3'), New Tokens:tensor([0, 8, 0, 7, 0], device='cuda:3')
Mixup Sent Mask:tensor([[2],
        [3]], device='cuda:3'), 2,  Mixup Mask:tensor([ True, False,  True, False, False], device='cuda:3'), 
                    Org X:tensor([[ 0.5410, -0.7554,  1.1133,  ..., -0.1235,  0.0684, -0.4653],
        [ 0.8901,  0.7979,  1.2803,  ...,  0.4734, -0.1606, -0.1669],
        [ 0.4526,  0.2274,  0.7285,  ..., -0.5654, -2.2285, -1.4033],
        [-0.2057,  0.6460, -1.0615,  ..., -0.0248,  0.2900, -0.8726],
        [-0.1395,  0.3970,  1.4482,  ..., -1.2715,  0.9946, -0.4919]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [ 0.8901,  0.7979,  1.2803,  ...,  0.4734, -0.1606, -0.1669],
        [-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [-0.2057,  0.6460, -1.0615,  ..., -0.0248,  0.2900, -0.8726],
        [-0.1395,  0.3970,  1.4482,  ..., -1.2715,  0.9946, -0.4919]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [-0.4287, -0.1092, -0.5439,  ...,  1.0391, -1.4609, -1.5078],
        [-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [-0.2874, -0.2274, -0.1625,  ..., -0.1914, -1.7529, -1.5635],
        [-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([24, 49]), X shape:torch.Size([24, 49, 512])
CTC Tokens:tensor([0, 0, 0, 0, 0], device='cuda:5'), Shrink Mask:tensor([ True, False, False, False, False], device='cuda:5'), New Tokens:tensor([ 0, 21, 11,  6,  0], device='cuda:5')
Mixup Sent Mask:tensor([[ 2],
        [ 3],
        [ 8],
        [11],
        [12],
        [13],
        [15],
        [16],
        [20]], device='cuda:5'), 2,  Mixup Mask:tensor([ True, False, False, False, False], device='cuda:5'), 
                    Org X:tensor([[ 0.6562,  0.5605,  0.3936,  ..., -0.2349, -0.3245,  0.4617],
        [ 0.4673,  0.0770, -0.3867,  ...,  0.1370, -1.3301, -0.6875],
        [ 1.4170, -0.5591,  0.1057,  ..., -0.1255, -0.2114, -0.1084],
        [ 1.3896,  0.2291, -0.2849,  ..., -0.9658,  0.2729,  0.3582],
        [ 0.3145,  0.9380,  0.5635,  ..., -1.0127,  0.3652, -0.6475]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [ 0.4673,  0.0770, -0.3867,  ...,  0.1370, -1.3301, -0.6875],
        [ 1.4170, -0.5591,  0.1057,  ..., -0.1255, -0.2114, -0.1084],
        [ 1.3896,  0.2291, -0.2849,  ..., -0.9658,  0.2729,  0.3582],
        [ 0.3145,  0.9380,  0.5635,  ..., -1.0127,  0.3652, -0.6475]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [-0.4434, -0.1169, -0.2786,  ..., -0.8813, -3.0293, -2.0449],
        [-0.5146, -0.3208, -0.2798,  ..., -2.5820, -0.5508, -3.7070],
        [ 0.1375, -0.4153, -0.1128,  ..., -2.9785, -3.1484,  0.8008],
        [-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 119]), X shape:torch.Size([8, 119, 512])
CTC Tokens:tensor([  0, 432, 432,   0,  89], device='cuda:2'), Shrink Mask:tensor([ True,  True, False,  True,  True], device='cuda:2'), New Tokens:tensor([  0, 432,   0,  89,   0], device='cuda:2')
Mixup Sent Mask:tensor([[2],
        [3]], device='cuda:2'), 2,  Mixup Mask:tensor([ True,  True,  True, False,  True], device='cuda:2'), 
                    Org X:tensor([[ 0.3872, -1.4121,  1.0537,  ..., -1.2227,  0.9482,  0.0988],
        [ 0.2303, -0.0061,  0.6973,  ..., -0.0431,  0.8896,  0.3240],
        [ 0.8813,  1.7666,  0.3682,  ..., -0.1078,  0.0959,  0.2908],
        [ 0.1317,  1.8564, -1.6309,  ..., -0.4294,  1.0986, -0.1312],
        [ 0.4697,  1.3896, -1.0869,  ..., -1.0635,  1.4453, -0.3748]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [ 0.0923, -0.2932, -0.4617,  ..., -0.1864, -1.9639, -3.5684],
        [-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [ 0.1317,  1.8564, -1.6309,  ..., -0.4294,  1.0986, -0.1312],
        [-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [ 0.0923, -0.2932, -0.4617,  ..., -0.1864, -1.9639, -3.5684],
        [-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311],
        [ 0.5747, -0.0329, -0.2668,  ...,  0.6011, -0.5527, -1.3271],
        [-0.3999, -0.8389,  0.2764,  ..., -2.4395, -1.5635, -1.8311]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2509, device='cuda:2')
2023-08-06 21:06:13 | INFO | dev_st | epoch 031 | valid on 'dev_st' subset | loss 4.204 | trans_loss 5.553 | nll_loss 2.822 | w2v_ctc_loss 1.294 | task_loss 4.646 | contrastive_loss 0.244 | total 4003.4 | n_correct 2492 | ppl 7.07 | accuracy 62.247 | uer 16.593 | wer 18.62 | raw_wer 18.62 | bleu 20.11 | wps 2105.4 | wpb 4003.4 | bsz 141.8 | num_updates 45675 | best_bleu 20.52
2023-08-06 21:06:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 45675 updates
2023-08-06 21:06:13 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_last.pt
2023-08-06 21:06:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_last.pt
2023-08-06 21:06:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_last.pt (epoch 31 @ 45675 updates, score 20.11) (writing took 12.962628593668342 seconds)
2023-08-06 21:06:26 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-08-06 21:06:26 | INFO | train | epoch 031 | loss 1.952 | trans_loss 4.95 | nll_loss 2.133 | w2v_ctc_loss 0.6 | task_loss 1.4 | contrastive_loss 0.091 | total 4138.65 | n_correct 2757.19 | ppl 4.38 | accuracy 66.62 | wps 11412.6 | ups 1.38 | wpb 8277.3 | bsz 305.7 | num_updates 45675 | lr 6.61722e-05 | gnorm 0.538 | clip 0 | loss_scale 32 | train_wall 1018 | gb_free 11.9 | wall 37410
2023-08-06 21:06:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 21:06:26 | INFO | fairseq.trainer | begin training epoch 32
2023-08-06 21:06:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 21:06:50 | INFO | train_inner | epoch 032:     25 / 1474 loss=1.946, trans_loss=4.951, nll_loss=2.133, w2v_ctc_loss=0.601, task_loss=1.478, contrastive_loss=0.039, total=4042.6, n_correct=2695.17, ppl=4.39, accuracy=66.669, wps=7143, ups=0.88, wpb=8085.2, bsz=288.7, num_updates=45700, lr=6.61541e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=69, gb_free=16.2, wall=37435
2023-08-06 21:08:00 | INFO | train_inner | epoch 032:    125 / 1474 loss=1.92, trans_loss=4.913, nll_loss=2.084, w2v_ctc_loss=0.576, task_loss=1.294, contrastive_loss=0.048, total=4227.68, n_correct=2850.83, ppl=4.24, accuracy=67.432, wps=12201.5, ups=1.44, wpb=8455.4, bsz=323.3, num_updates=45800, lr=6.60819e-05, gnorm=0.519, clip=0, loss_scale=32, train_wall=69, gb_free=16, wall=37504
2023-08-06 21:09:10 | INFO | train_inner | epoch 032:    225 / 1474 loss=1.933, trans_loss=4.931, nll_loss=2.107, w2v_ctc_loss=0.587, task_loss=1.332, contrastive_loss=0.057, total=4157.32, n_correct=2789.61, ppl=4.31, accuracy=67.101, wps=11876, ups=1.43, wpb=8314.6, bsz=320.6, num_updates=45900, lr=6.60098e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=70, gb_free=17.1, wall=37574
2023-08-06 21:10:19 | INFO | train_inner | epoch 032:    325 / 1474 loss=1.927, trans_loss=4.922, nll_loss=2.096, w2v_ctc_loss=0.58, task_loss=1.321, contrastive_loss=0.05, total=4183.45, n_correct=2814.94, ppl=4.28, accuracy=67.288, wps=12124.1, ups=1.45, wpb=8366.9, bsz=314.4, num_updates=46000, lr=6.5938e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=69, gb_free=17.5, wall=37643
2023-08-06 21:10:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 21:10:43 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 4.22 | trans_loss 5.562 | nll_loss 2.834 | w2v_ctc_loss 1.33 | task_loss 4.623 | contrastive_loss 0.24 | total 4003.4 | n_correct 2487.4 | ppl 7.13 | accuracy 62.132 | uer 16.999 | wer 18.907 | raw_wer 18.907 | bleu 20.1 | wps 2114.3 | wpb 4003.4 | bsz 141.8 | num_updates 46000 | best_bleu 20.52
2023-08-06 21:10:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 46000 updates
2023-08-06 21:10:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_32_46000.pt
2023-08-06 21:10:46 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_32_46000.pt
2023-08-06 21:11:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_32_46000.pt (epoch 32 @ 46000 updates, score 20.1) (writing took 36.66940204426646 seconds)
2023-08-06 21:12:29 | INFO | train_inner | epoch 032:    425 / 1474 loss=1.935, trans_loss=4.931, nll_loss=2.107, w2v_ctc_loss=0.593, task_loss=1.393, contrastive_loss=0.047, total=4157.28, n_correct=2789.04, ppl=4.31, accuracy=67.088, wps=6371.5, ups=0.77, wpb=8314.6, bsz=305.9, num_updates=46100, lr=6.58665e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=69, gb_free=14.7, wall=37773
2023-08-06 21:13:40 | INFO | train_inner | epoch 032:    525 / 1474 loss=1.951, trans_loss=4.941, nll_loss=2.121, w2v_ctc_loss=0.595, task_loss=1.354, contrastive_loss=0.127, total=4198.93, n_correct=2808.67, ppl=4.35, accuracy=66.89, wps=11893.1, ups=1.42, wpb=8397.9, bsz=317.6, num_updates=46200, lr=6.57952e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=70, gb_free=17.2, wall=37844
2023-08-06 21:14:49 | INFO | train_inner | epoch 032:    625 / 1474 loss=1.945, trans_loss=4.948, nll_loss=2.129, w2v_ctc_loss=0.596, task_loss=1.445, contrastive_loss=0.055, total=4142.69, n_correct=2762.24, ppl=4.37, accuracy=66.677, wps=11915.7, ups=1.44, wpb=8285.4, bsz=301.6, num_updates=46300, lr=6.57241e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=69, gb_free=17.3, wall=37914
2023-08-06 21:16:00 | INFO | train_inner | epoch 032:    725 / 1474 loss=1.946, trans_loss=4.947, nll_loss=2.129, w2v_ctc_loss=0.605, task_loss=1.429, contrastive_loss=0.04, total=4154.59, n_correct=2774.18, ppl=4.37, accuracy=66.774, wps=11792.5, ups=1.42, wpb=8309.2, bsz=301.8, num_updates=46400, lr=6.56532e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=70, gb_free=15.7, wall=37984
2023-08-06 21:17:08 | INFO | train_inner | epoch 032:    825 / 1474 loss=1.938, trans_loss=4.943, nll_loss=2.122, w2v_ctc_loss=0.589, task_loss=1.442, contrastive_loss=0.038, total=4114.54, n_correct=2749.36, ppl=4.35, accuracy=66.821, wps=12048.8, ups=1.46, wpb=8229.1, bsz=294.9, num_updates=46500, lr=6.55826e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=68, gb_free=17, wall=38052
2023-08-06 21:18:18 | INFO | train_inner | epoch 032:    925 / 1474 loss=1.939, trans_loss=4.945, nll_loss=2.126, w2v_ctc_loss=0.589, task_loss=1.45, contrastive_loss=0.037, total=4139.67, n_correct=2764.36, ppl=4.37, accuracy=66.777, wps=11782.5, ups=1.42, wpb=8279.3, bsz=298.3, num_updates=46600, lr=6.55122e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=70, gb_free=16.8, wall=38123
2023-08-06 21:19:27 | INFO | train_inner | epoch 032:   1025 / 1474 loss=1.956, trans_loss=4.954, nll_loss=2.137, w2v_ctc_loss=0.597, task_loss=1.388, contrastive_loss=0.131, total=4119.15, n_correct=2741.93, ppl=4.4, accuracy=66.565, wps=11967, ups=1.45, wpb=8238.3, bsz=304.5, num_updates=46700, lr=6.5442e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=68, gb_free=16.3, wall=38192
2023-08-06 21:20:37 | INFO | train_inner | epoch 032:   1125 / 1474 loss=1.958, trans_loss=4.959, nll_loss=2.143, w2v_ctc_loss=0.601, task_loss=1.648, contrastive_loss=0.072, total=4019.61, n_correct=2669.51, ppl=4.42, accuracy=66.412, wps=11586.3, ups=1.44, wpb=8039.2, bsz=271.4, num_updates=46800, lr=6.5372e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=69, gb_free=17.6, wall=38261
2023-08-06 21:21:46 | INFO | train_inner | epoch 032:   1225 / 1474 loss=1.969, trans_loss=4.963, nll_loss=2.151, w2v_ctc_loss=0.597, task_loss=1.387, contrastive_loss=0.174, total=4149.28, n_correct=2754.38, ppl=4.44, accuracy=66.382, wps=11965.4, ups=1.44, wpb=8298.6, bsz=310.3, num_updates=46900, lr=6.53023e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=69, gb_free=15.8, wall=38330
2023-08-06 21:22:55 | INFO | train_inner | epoch 032:   1325 / 1474 loss=1.946, trans_loss=4.954, nll_loss=2.138, w2v_ctc_loss=0.599, task_loss=1.439, contrastive_loss=0.038, total=4079.22, n_correct=2716.58, ppl=4.4, accuracy=66.596, wps=11890.9, ups=1.46, wpb=8158.4, bsz=296.2, num_updates=47000, lr=6.52328e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=68, gb_free=14.9, wall=38399
2023-08-06 21:24:04 | INFO | train_inner | epoch 032:   1425 / 1474 loss=1.978, trans_loss=4.959, nll_loss=2.145, w2v_ctc_loss=0.604, task_loss=1.405, contrastive_loss=0.27, total=4111.41, n_correct=2731.85, ppl=4.42, accuracy=66.446, wps=11901.6, ups=1.45, wpb=8222.8, bsz=306.1, num_updates=47100, lr=6.51635e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=69, gb_free=15.9, wall=38468
2023-08-06 21:24:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-06 21:24:37 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 21:25:01 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 4.216 | trans_loss 5.555 | nll_loss 2.829 | w2v_ctc_loss 1.329 | task_loss 4.631 | contrastive_loss 0.245 | total 4003.4 | n_correct 2492.3 | ppl 7.11 | accuracy 62.255 | uer 16.885 | wer 18.728 | raw_wer 18.728 | bleu 20.22 | wps 2151.7 | wpb 4003.4 | bsz 141.8 | num_updates 47148 | best_bleu 20.52
2023-08-06 21:25:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 47148 updates
2023-08-06 21:25:01 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.2209.pt
2023-08-06 21:25:05 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.2209.pt
2023-08-06 21:25:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint.best_bleu_20.2209.pt (epoch 32 @ 47148 updates, score 20.22) (writing took 18.17101758904755 seconds)
2023-08-06 21:25:19 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-08-06 21:25:19 | INFO | train | epoch 032 | loss 1.946 | trans_loss 4.943 | nll_loss 2.124 | w2v_ctc_loss 0.593 | task_loss 1.401 | contrastive_loss 0.087 | total 4138.18 | n_correct 2764.73 | ppl 4.36 | accuracy 66.81 | wps 10755.9 | ups 1.3 | wpb 8276.4 | bsz 305.4 | num_updates 47148 | lr 6.51303e-05 | gnorm 0.538 | clip 0 | loss_scale 32 | train_wall 1016 | gb_free 16.3 | wall 38544
2023-08-06 21:25:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 21:25:20 | INFO | fairseq.trainer | begin training epoch 33
2023-08-06 21:25:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 21:26:03 | INFO | train_inner | epoch 033:     52 / 1474 loss=1.944, trans_loss=4.939, nll_loss=2.119, w2v_ctc_loss=0.588, task_loss=1.336, contrastive_loss=0.1, total=4144.32, n_correct=2774.9, ppl=4.34, accuracy=66.957, wps=6951, ups=0.84, wpb=8288.6, bsz=317.5, num_updates=47200, lr=6.50945e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=69, gb_free=16.9, wall=38587
2023-08-06 21:27:12 | INFO | train_inner | epoch 033:    152 / 1474 loss=1.927, trans_loss=4.92, nll_loss=2.092, w2v_ctc_loss=0.578, task_loss=1.504, contrastive_loss=0.031, total=4073.9, n_correct=2741.45, ppl=4.26, accuracy=67.293, wps=11801.3, ups=1.45, wpb=8147.8, bsz=284.8, num_updates=47300, lr=6.50256e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=69, gb_free=15.1, wall=38656
2023-08-06 21:28:22 | INFO | train_inner | epoch 033:    252 / 1474 loss=1.95, trans_loss=4.921, nll_loss=2.097, w2v_ctc_loss=0.583, task_loss=1.195, contrastive_loss=0.196, total=4280.14, n_correct=2876.71, ppl=4.28, accuracy=67.211, wps=12309.4, ups=1.44, wpb=8560.3, bsz=346.5, num_updates=47400, lr=6.4957e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=69, gb_free=16.3, wall=38726
2023-08-06 21:29:31 | INFO | train_inner | epoch 033:    352 / 1474 loss=1.937, trans_loss=4.933, nll_loss=2.11, w2v_ctc_loss=0.591, task_loss=1.427, contrastive_loss=0.055, total=4120.27, n_correct=2760.08, ppl=4.32, accuracy=66.988, wps=11881.9, ups=1.44, wpb=8240.5, bsz=300.7, num_updates=47500, lr=6.48886e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=69, gb_free=17.2, wall=38795
2023-08-06 21:30:40 | INFO | train_inner | epoch 033:    452 / 1474 loss=1.923, trans_loss=4.919, nll_loss=2.092, w2v_ctc_loss=0.581, task_loss=1.327, contrastive_loss=0.037, total=4141.22, n_correct=2787.2, ppl=4.26, accuracy=67.304, wps=12024.5, ups=1.45, wpb=8282.4, bsz=310.8, num_updates=47600, lr=6.48204e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=68, gb_free=16.3, wall=38864
2023-08-06 21:31:49 | INFO | train_inner | epoch 033:    552 / 1474 loss=1.943, trans_loss=4.938, nll_loss=2.116, w2v_ctc_loss=0.597, task_loss=1.454, contrastive_loss=0.057, total=4133.59, n_correct=2761.72, ppl=4.33, accuracy=66.812, wps=11943.4, ups=1.44, wpb=8267.2, bsz=294, num_updates=47700, lr=6.47524e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=69, gb_free=15.1, wall=38933
2023-08-06 21:32:59 | INFO | train_inner | epoch 033:    652 / 1474 loss=1.949, trans_loss=4.95, nll_loss=2.132, w2v_ctc_loss=0.591, task_loss=1.43, contrastive_loss=0.09, total=4157.63, n_correct=2771, ppl=4.38, accuracy=66.649, wps=11937.8, ups=1.44, wpb=8315.3, bsz=301.6, num_updates=47800, lr=6.46846e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=69, gb_free=17.7, wall=39003
2023-08-06 21:34:08 | INFO | train_inner | epoch 033:    752 / 1474 loss=1.947, trans_loss=4.947, nll_loss=2.127, w2v_ctc_loss=0.606, task_loss=1.519, contrastive_loss=0.038, total=4070.75, n_correct=2713.15, ppl=4.37, accuracy=66.65, wps=11689.3, ups=1.44, wpb=8141.5, bsz=287.2, num_updates=47900, lr=6.46171e-05, gnorm=0.554, clip=0, loss_scale=32, train_wall=69, gb_free=16.3, wall=39073
2023-08-06 21:35:17 | INFO | train_inner | epoch 033:    852 / 1474 loss=1.933, trans_loss=4.934, nll_loss=2.113, w2v_ctc_loss=0.573, task_loss=1.327, contrastive_loss=0.105, total=4130.24, n_correct=2775.15, ppl=4.33, accuracy=67.191, wps=11981.3, ups=1.45, wpb=8260.5, bsz=316.3, num_updates=48000, lr=6.45497e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=69, gb_free=16.6, wall=39142
2023-08-06 21:35:17 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 21:35:40 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 4.222 | trans_loss 5.554 | nll_loss 2.825 | w2v_ctc_loss 1.355 | task_loss 4.626 | contrastive_loss 0.235 | total 4003.4 | n_correct 2493.3 | ppl 7.09 | accuracy 62.28 | uer 16.749 | wer 18.668 | raw_wer 18.668 | bleu 20.32 | wps 2228.3 | wpb 4003.4 | bsz 141.8 | num_updates 48000 | best_bleu 20.52
2023-08-06 21:35:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48000 updates
2023-08-06 21:35:40 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_33_48000.pt
2023-08-06 21:35:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_33_48000.pt
2023-08-06 21:36:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_33_48000.pt (epoch 33 @ 48000 updates, score 20.32) (writing took 38.21243127062917 seconds)
2023-08-06 21:37:32 | INFO | train_inner | epoch 033:    952 / 1474 loss=1.944, trans_loss=4.945, nll_loss=2.127, w2v_ctc_loss=0.601, task_loss=1.399, contrastive_loss=0.049, total=4151.18, n_correct=2769.81, ppl=4.37, accuracy=66.723, wps=6172.7, ups=0.74, wpb=8302.4, bsz=308.3, num_updates=48100, lr=6.44826e-05, gnorm=0.549, clip=0, loss_scale=32, train_wall=69, gb_free=11, wall=39276
2023-08-06 21:38:42 | INFO | train_inner | epoch 033:   1052 / 1474 loss=1.955, trans_loss=4.942, nll_loss=2.122, w2v_ctc_loss=0.59, task_loss=1.405, contrastive_loss=0.154, total=4140.1, n_correct=2764.06, ppl=4.35, accuracy=66.763, wps=11813.4, ups=1.43, wpb=8280.2, bsz=307.6, num_updates=48200, lr=6.44157e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=70, gb_free=11.4, wall=39346
2023-08-06 21:39:52 | INFO | train_inner | epoch 033:   1152 / 1474 loss=1.952, trans_loss=4.948, nll_loss=2.131, w2v_ctc_loss=0.586, task_loss=1.404, contrastive_loss=0.143, total=4182.67, n_correct=2787.17, ppl=4.38, accuracy=66.636, wps=11904.5, ups=1.42, wpb=8365.3, bsz=309.5, num_updates=48300, lr=6.43489e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=70, gb_free=17.4, wall=39416
2023-08-06 21:41:01 | INFO | train_inner | epoch 033:   1252 / 1474 loss=1.943, trans_loss=4.944, nll_loss=2.124, w2v_ctc_loss=0.6, task_loss=1.471, contrastive_loss=0.042, total=4110.02, n_correct=2749.01, ppl=4.36, accuracy=66.886, wps=11886.5, ups=1.45, wpb=8220, bsz=294.4, num_updates=48400, lr=6.42824e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=69, gb_free=16.6, wall=39486
2023-08-06 21:42:12 | INFO | train_inner | epoch 033:   1352 / 1474 loss=1.94, trans_loss=4.946, nll_loss=2.129, w2v_ctc_loss=0.591, task_loss=1.373, contrastive_loss=0.058, total=4128.82, n_correct=2759.64, ppl=4.37, accuracy=66.838, wps=11777.7, ups=1.43, wpb=8257.6, bsz=312.4, num_updates=48500, lr=6.42161e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=70, gb_free=15.9, wall=39556
2023-08-06 21:43:21 | INFO | train_inner | epoch 033:   1452 / 1474 loss=1.957, trans_loss=4.946, nll_loss=2.129, w2v_ctc_loss=0.587, task_loss=1.396, contrastive_loss=0.206, total=4123.47, n_correct=2754.05, ppl=4.37, accuracy=66.79, wps=11947.9, ups=1.45, wpb=8246.9, bsz=308.9, num_updates=48600, lr=6.415e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=69, gb_free=16.4, wall=39625
2023-08-06 21:43:36 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 21:43:59 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 4.207 | trans_loss 5.551 | nll_loss 2.819 | w2v_ctc_loss 1.316 | task_loss 4.652 | contrastive_loss 0.234 | total 4003.4 | n_correct 2493.6 | ppl 7.06 | accuracy 62.287 | uer 16.664 | wer 18.582 | raw_wer 18.582 | bleu 20.52 | wps 2153.6 | wpb 4003.4 | bsz 141.8 | num_updates 48622 | best_bleu 20.52
2023-08-06 21:43:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48622 updates
2023-08-06 21:43:59 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 21:44:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt
2023-08-06 21:44:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_best.pt (epoch 33 @ 48622 updates, score 20.52) (writing took 24.77017636783421 seconds)
2023-08-06 21:44:25 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-08-06 21:44:25 | INFO | train | epoch 033 | loss 1.942 | trans_loss 4.938 | nll_loss 2.117 | w2v_ctc_loss 0.59 | task_loss 1.399 | contrastive_loss 0.089 | total 4138.65 | n_correct 2769.71 | ppl 4.34 | accuracy 66.923 | wps 10652.8 | ups 1.29 | wpb 8277.3 | bsz 305.7 | num_updates 48622 | lr 6.41355e-05 | gnorm 0.542 | clip 0 | loss_scale 32 | train_wall 1017 | gb_free 17.8 | wall 39689
2023-08-06 21:44:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-06 21:44:25 | INFO | fairseq.trainer | begin training epoch 34
2023-08-06 21:44:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-06 21:45:26 | INFO | train_inner | epoch 034:     78 / 1474 loss=1.928, trans_loss=4.918, nll_loss=2.091, w2v_ctc_loss=0.586, task_loss=1.382, contrastive_loss=0.044, total=4128.94, n_correct=2779.33, ppl=4.26, accuracy=67.313, wps=6562.9, ups=0.79, wpb=8257.9, bsz=302.1, num_updates=48700, lr=6.40841e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=69, gb_free=15.1, wall=39751
2023-08-06 21:46:36 | INFO | train_inner | epoch 034:    178 / 1474 loss=1.926, trans_loss=4.914, nll_loss=2.085, w2v_ctc_loss=0.582, task_loss=1.461, contrastive_loss=0.045, total=4071.22, n_correct=2743.72, ppl=4.24, accuracy=67.393, wps=11693.8, ups=1.44, wpb=8142.4, bsz=295.1, num_updates=48800, lr=6.40184e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=69, gb_free=15.4, wall=39820
2023-08-06 21:47:46 | INFO | train_inner | epoch 034:    278 / 1474 loss=1.96, trans_loss=4.931, nll_loss=2.109, w2v_ctc_loss=0.583, task_loss=1.321, contrastive_loss=0.246, total=4237.89, n_correct=2837.01, ppl=4.31, accuracy=66.944, wps=12113.5, ups=1.43, wpb=8475.8, bsz=326.9, num_updates=48900, lr=6.39529e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=70, gb_free=10.2, wall=39890
2023-08-06 21:48:55 | INFO | train_inner | epoch 034:    378 / 1474 loss=1.932, trans_loss=4.915, nll_loss=2.087, w2v_ctc_loss=0.576, task_loss=1.321, contrastive_loss=0.138, total=4167, n_correct=2813.41, ppl=4.25, accuracy=67.516, wps=12073.8, ups=1.45, wpb=8334, bsz=319, num_updates=49000, lr=6.38877e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=69, gb_free=17.2, wall=39959
2023-08-06 21:50:04 | INFO | train_inner | epoch 034:    478 / 1474 loss=1.938, trans_loss=4.933, nll_loss=2.109, w2v_ctc_loss=0.597, task_loss=1.533, contrastive_loss=0.039, total=4071.65, n_correct=2726.33, ppl=4.31, accuracy=66.959, wps=11741.6, ups=1.44, wpb=8143.3, bsz=284.8, num_updates=49100, lr=6.38226e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=69, gb_free=11.3, wall=40029
2023-08-06 21:51:13 | INFO | train_inner | epoch 034:    578 / 1474 loss=1.928, trans_loss=4.92, nll_loss=2.093, w2v_ctc_loss=0.586, task_loss=1.422, contrastive_loss=0.041, total=4110.13, n_correct=2765.46, ppl=4.27, accuracy=67.284, wps=11971.2, ups=1.46, wpb=8220.3, bsz=299, num_updates=49200, lr=6.37577e-05, gnorm=0.543, clip=0, loss_scale=64, train_wall=68, gb_free=16.5, wall=40097
2023-08-06 21:52:22 | INFO | train_inner | epoch 034:    678 / 1474 loss=1.927, trans_loss=4.927, nll_loss=2.103, w2v_ctc_loss=0.582, task_loss=1.419, contrastive_loss=0.036, total=4128.65, n_correct=2776.23, ppl=4.3, accuracy=67.243, wps=11953.4, ups=1.45, wpb=8257.3, bsz=300.7, num_updates=49300, lr=6.3693e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=69, gb_free=17.8, wall=40166
2023-08-06 21:53:32 | INFO | train_inner | epoch 034:    778 / 1474 loss=1.944, trans_loss=4.947, nll_loss=2.128, w2v_ctc_loss=0.579, task_loss=1.469, contrastive_loss=0.103, total=4075.69, n_correct=2720.53, ppl=4.37, accuracy=66.75, wps=11652, ups=1.43, wpb=8151.4, bsz=294.5, num_updates=49400, lr=6.36285e-05, gnorm=0.544, clip=0, loss_scale=64, train_wall=70, gb_free=17.2, wall=40236
2023-08-06 21:54:41 | INFO | train_inner | epoch 034:    878 / 1474 loss=1.938, trans_loss=4.938, nll_loss=2.118, w2v_ctc_loss=0.586, task_loss=1.48, contrastive_loss=0.06, total=4104.97, n_correct=2751.82, ppl=4.34, accuracy=67.036, wps=11844.6, ups=1.44, wpb=8209.9, bsz=296.3, num_updates=49500, lr=6.35642e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=69, gb_free=17.7, wall=40306
2023-08-06 21:55:51 | INFO | train_inner | epoch 034:    978 / 1474 loss=1.939, trans_loss=4.94, nll_loss=2.12, w2v_ctc_loss=0.592, task_loss=1.368, contrastive_loss=0.057, total=4168.94, n_correct=2788.48, ppl=4.35, accuracy=66.887, wps=11945.3, ups=1.43, wpb=8337.9, bsz=312.8, num_updates=49600, lr=6.35001e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=69, gb_free=14.8, wall=40375
2023-08-06 21:57:00 | INFO | train_inner | epoch 034:   1078 / 1474 loss=1.937, trans_loss=4.94, nll_loss=2.12, w2v_ctc_loss=0.594, task_loss=1.352, contrastive_loss=0.041, total=4155.12, n_correct=2780.26, ppl=4.35, accuracy=66.912, wps=12013.6, ups=1.45, wpb=8310.2, bsz=309.1, num_updates=49700, lr=6.34361e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=16.6, wall=40445
2023-08-06 21:58:10 | INFO | train_inner | epoch 034:   1178 / 1474 loss=1.937, trans_loss=4.94, nll_loss=2.12, w2v_ctc_loss=0.587, task_loss=1.447, contrastive_loss=0.052, total=4096.48, n_correct=2742.82, ppl=4.35, accuracy=66.956, wps=11802.4, ups=1.44, wpb=8193, bsz=297.2, num_updates=49800, lr=6.33724e-05, gnorm=0.553, clip=0, loss_scale=64, train_wall=69, gb_free=16.5, wall=40514
2023-08-06 21:59:19 | INFO | train_inner | epoch 034:   1278 / 1474 loss=1.934, trans_loss=4.937, nll_loss=2.116, w2v_ctc_loss=0.587, task_loss=1.42, contrastive_loss=0.038, total=4149.03, n_correct=2780.01, ppl=4.33, accuracy=67.004, wps=12006.7, ups=1.45, wpb=8298.1, bsz=299.7, num_updates=49900, lr=6.33089e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=69, gb_free=16.3, wall=40583
2023-08-06 22:00:29 | INFO | train_inner | epoch 034:   1378 / 1474 loss=1.951, trans_loss=4.946, nll_loss=2.129, w2v_ctc_loss=0.6, task_loss=1.342, contrastive_loss=0.101, total=4200.34, n_correct=2799.14, ppl=4.37, accuracy=66.641, wps=12027.6, ups=1.43, wpb=8400.7, bsz=321.9, num_updates=50000, lr=6.32456e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=69, gb_free=14.8, wall=40653
2023-08-06 22:00:29 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2023-08-06 22:00:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-06 22:00:52 | INFO | dev_st | epoch 034 | valid on 'dev_st' subset | loss 4.208 | trans_loss 5.55 | nll_loss 2.819 | w2v_ctc_loss 1.318 | task_loss 4.638 | contrastive_loss 0.238 | total 4003.4 | n_correct 2498 | ppl 7.06 | accuracy 62.397 | uer 16.548 | wer 18.478 | raw_wer 18.478 | bleu 20.28 | wps 2126.5 | wpb 4003.4 | bsz 141.8 | num_updates 50000 | best_bleu 20.52
2023-08-06 22:00:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 50000 updates
2023-08-06 22:00:52 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_34_50000.pt
2023-08-06 22:00:56 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_34_50000.pt
2023-08-06 22:01:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0805_mixup_at/checkpoint_34_50000.pt (epoch 34 @ 50000 updates, score 20.28) (writing took 18.382712479680777 seconds)
2023-08-06 22:01:12 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2023-08-06 22:01:12 | INFO | train | epoch 034 | loss 1.937 | trans_loss 4.932 | nll_loss 2.109 | w2v_ctc_loss 0.587 | task_loss 1.409 | contrastive_loss 0.077 | total 4133.33 | n_correct 2771.88 | ppl 4.31 | accuracy 67.062 | wps 11312.8 | ups 1.37 | wpb 8266.7 | bsz 304.3 | num_updates 50000 | lr 6.32456e-05 | gnorm 0.542 | clip 0 | loss_scale 64 | train_wall 950 | gb_free 14.8 | wall 40696
2023-08-06 22:01:12 | INFO | fairseq_cli.train | done training in 40645.5 seconds
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-7:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-6:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    raise EOFError
EOFError
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    raise EOFError
EOFError
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1472 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
