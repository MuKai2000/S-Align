2023-07-24 20:27:43 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:16901
2023-07-24 20:27:44 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:16901
2023-07-24 20:27:44 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:16901
2023-07-24 20:27:44 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:16901
2023-07-24 20:27:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-24 20:27:44 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:16901
2023-07-24 20:27:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-24 20:27:44 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:16901
2023-07-24 20:27:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-24 20:27:44 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:16901
2023-07-24 20:27:44 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:16901
2023-07-24 20:27:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-24 20:27:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-24 20:27:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-24 20:27:45 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-24 20:27:45 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-24 20:27:45 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:27:45 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-07-24 20:27:45 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:27:45 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-07-24 20:27:45 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:27:45 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:27:45 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:27:45 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-07-24 20:27:45 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-07-24 20:27:45 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-07-24 20:27:45 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:27:45 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:27:45 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:27:45 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-07-24 20:27:45 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-07-24 20:27:45 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-07-24 20:27:49 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16901', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-07-24 20:27:49 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-07-24 20:27:49 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-07-24 20:27:49 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-24 20:27:49 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 1.0
2023-07-24 20:27:49 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-24 20:27:54 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-07-24 20:27:54 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-24 20:27:54 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-24 20:27:57 | INFO | root | load pretrained hubert
2023-07-24 20:27:58 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-24 20:27:59 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-24 20:28:02 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-24 20:28:02 | INFO | root | share the sematic adapter and textual encoder
2023-07-24 20:28:02 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-24 20:28:02 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-07-24 20:28:02 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-24 20:28:02 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-07-24 20:28:02 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-24 20:28:02 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-24 20:28:02 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-24 20:28:02 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-24 20:28:02 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-24 20:28:02 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-24 20:28:05 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-24 20:28:05 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-24 20:28:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-24 20:28:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-24 20:28:05 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:28:05 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:28:05 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:28:05 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:28:05 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:28:05 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:28:05 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:28:05 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:28:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-24 20:28:05 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-24 20:28:05 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-24 20:28:05 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_last.pt
2023-07-24 20:28:05 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_last.pt
2023-07-24 20:28:05 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-24 20:28:05 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-24 20:28:05 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-24 20:28:05 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-24 20:28:07 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-24 20:28:08 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-24 20:28:10 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-24 20:29:21 | INFO | fairseq.optim.adam | using FusedAdam
2023-07-24 20:29:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-24 20:29:21 | INFO | fairseq.trainer | begin training epoch 1
2023-07-24 20:29:21 | INFO | fairseq_cli.train | Start iterating over samples
at:None	 st:5599.3466796875	 mt:1279.093017578125
-- Cal ASR Loss asr
at:None	 st:5928.474609375	 mt:1313.819091796875
-- Cal ASR Loss asr
at:None	 st:8041.96875	 mt:2051.02880859375
-- Cal ASR Loss asr
at:None	 st:7695.8056640625	 mt:1752.3310546875
-- Cal ASR Loss asr
at:None	 st:7417.826171875	 mt:1636.32470703125
-- Cal ASR Loss asr
at:None	 st:4906.9404296875	 mt:1137.467529296875
-- Cal ASR Loss asr
at:None	 st:8587.578125	 mt:2191.44189453125
-- Cal ASR Loss asr
at:None	 st:8766.13671875	 mt:2016.6324462890625
-- Cal ASR Loss asr
at:None	 st:8992.6640625	 mt:2183.70068359375
-- Cal ASR Loss asr
at:None	 st:7907.9462890625	 mt:1949.138916015625
-- Cal ASR Loss asr
at:None	 st:8956.763671875	 mt:2209.9521484375
-- Cal ASR Loss asr
at:None	 st:7801.4462890625	 mt:1806.141357421875
-- Cal ASR Loss asr
at:None	 st:5290.01025390625	 mt:1299.14990234375
-- Cal ASR Loss asr
at:None	 st:9769.564453125	 mt:2521.431396484375
-- Cal ASR Loss asr
at:None	 st:8015.564453125	 mt:1932.401123046875
-- Cal ASR Loss asr
at:None	 st:7203.18359375	 mt:1787.0272216796875
-- Cal ASR Loss asr
at:None	 st:4702.38037109375	 mt:1082.370849609375
-- Cal ASR Loss asr
at:None	 st:9015.939453125	 mt:2284.86669921875
-- Cal ASR Loss asr
at:None	 st:13744.794921875	 mt:4031.813720703125
-- Cal ASR Loss asr
at:None	 st:9391.298828125	 mt:2382.161376953125
-- Cal ASR Loss asr
at:None	 st:7130.2939453125	 mt:1631.06591796875
-- Cal ASR Loss asr
at:None	 st:7404.93701171875	 mt:1391.1959228515625
-- Cal ASR Loss asr
at:None	 st:4294.91015625	 mt:842.9528198242188
-- Cal ASR Loss asr
at:None	 st:5507.9072265625	 mt:1406.7646484375
-- Cal ASR Loss asr
at:None	 st:4655.35302734375	 mt:1147.570556640625
-- Cal ASR Loss asr
at:None	 st:8673.0615234375	 mt:2088.67822265625
-- Cal ASR Loss asr
at:None	 st:4543.69189453125	 mt:1082.122802734375
-- Cal ASR Loss asr
at:None	 st:9284.984375	 mt:2265.0234375
-- Cal ASR Loss asr
at:None	 st:8456.611328125	 mt:1902.128662109375
-- Cal ASR Loss asr
at:None	 st:8363.4150390625	 mt:2201.43115234375
-- Cal ASR Loss asr
at:None	 st:9383.701171875	 mt:2339.83447265625
-- Cal ASR Loss asr
at:None	 st:4505.2890625	 mt:955.2584228515625
-- Cal ASR Loss asr
at:None	 st:4972.04638671875	 mt:1142.40771484375
-- Cal ASR Loss asr
at:None	 st:9045.2802734375	 mt:2331.662109375
-- Cal ASR Loss asr
at:None	 st:4607.931640625	 mt:1245.650390625
-- Cal ASR Loss asr
at:None	 st:5089.24609375	 mt:1101.3680419921875
-- Cal ASR Loss asr
at:None	 st:4301.19384765625	 mt:961.7819213867188
-- Cal ASR Loss asr
at:None	 st:5469.4033203125	 mt:1340.6422119140625
-- Cal ASR Loss asr
at:None	 st:7475.6279296875	 mt:1831.0440673828125
-- Cal ASR Loss asr
at:None	 st:9860.025390625	 mt:2575.125
-- Cal ASR Loss asr
at:None	 st:7124.6357421875	 mt:1813.3538818359375
-- Cal ASR Loss asr
at:None	 st:10523.341796875	 mt:2951.757568359375
-- Cal ASR Loss asr
at:None	 st:9654.49609375	 mt:2382.947265625
-- Cal ASR Loss asr
at:None	 st:8182.80419921875	 mt:2059.49755859375
-- Cal ASR Loss asr
at:None	 st:6466.43017578125	 mt:1378.6158447265625
-- Cal ASR Loss asr
at:None	 st:7203.0224609375	 mt:1583.78662109375
-- Cal ASR Loss asr
at:None	 st:11768.4599609375	 mt:3202.95166015625
-- Cal ASR Loss asr
at:None	 st:6631.1318359375	 mt:1691.610107421875
-- Cal ASR Loss asr
at:None	 st:5600.5634765625	 mt:1314.9041748046875
-- Cal ASR Loss asr
at:None	 st:6965.73681640625	 mt:1645.74462890625
-- Cal ASR Loss asr
at:None	 st:6161.6875	 mt:1502.723876953125
-- Cal ASR Loss asr
at:None	 st:7344.8076171875	 mt:1807.987548828125
-- Cal ASR Loss asr
at:None	 st:8660.498046875	 mt:2185.4931640625
-- Cal ASR Loss asr
at:None	 st:6257.88232421875	 mt:1777.5216064453125
-- Cal ASR Loss asr
at:None	 st:5550.8857421875	 mt:1260.2081298828125
-- Cal ASR Loss asr
at:None	 st:7585.9111328125	 mt:1677.6734619140625
-- Cal ASR Loss asr
at:None	 st:8849.05078125	 mt:2131.494140625
-- Cal ASR Loss asr
at:None	 st:5180.1015625	 mt:1243.2117919921875
-- Cal ASR Loss asr
at:None	 st:4722.7041015625	 mt:1072.0057373046875
-- Cal ASR Loss asr
at:None	 st:8272.88671875	 mt:1808.304931640625
-- Cal ASR Loss asr
at:None	 st:5381.90771484375	 mt:1294.8939208984375
-- Cal ASR Loss asr
at:None	 st:4603.189453125	 mt:1048.18408203125
-- Cal ASR Loss asr
at:None	 st:8975.0205078125	 mt:2364.481201171875
-- Cal ASR Loss asr
at:None	 st:7784.81201171875	 mt:1810.857177734375
-- Cal ASR Loss asr
at:None	 st:8534.5712890625	 mt:2174.65283203125
-- Cal ASR Loss asr
at:None	 st:9349.86328125	 mt:2357.07666015625
-- Cal ASR Loss asr
at:None	 st:6897.2646484375	 mt:1338.43798828125
-- Cal ASR Loss asr
at:None	 st:4673.50537109375	 mt:1218.670654296875
-- Cal ASR Loss asr
at:None	 st:7633.021484375	 mt:1792.119140625
-- Cal ASR Loss asr
at:None	 st:4623.4521484375	 mt:1246.930419921875
-- Cal ASR Loss asr
at:None	 st:13826.123046875	 mt:4550.7431640625
-- Cal ASR Loss asr
at:None	 st:4770.4453125	 mt:1087.2781982421875
-- Cal ASR Loss asr
at:None	 st:9158.044921875	 mt:2392.327880859375
-- Cal ASR Loss asr
at:None	 st:7666.68896484375	 mt:1717.008544921875
-- Cal ASR Loss asr
at:None	 st:8133.21240234375	 mt:2115.30322265625
-- Cal ASR Loss asr
at:None	 st:9501.01171875	 mt:2424.245849609375
-- Cal ASR Loss asr
at:None	 st:8630.611328125	 mt:2145.71337890625
-- Cal ASR Loss asr
at:None	 st:7656.9921875	 mt:1906.2705078125
-- Cal ASR Loss asr
at:None	 st:7681.75341796875	 mt:1845.782958984375
-- Cal ASR Loss asr
at:None	 st:5716.1787109375	 mt:1370.002197265625
-- Cal ASR Loss asr
at:None	 st:6223.1767578125	 mt:1448.4266357421875
-- Cal ASR Loss asr
at:None	 st:7907.8828125	 mt:1847.780517578125
-- Cal ASR Loss asr
at:None	 st:8306.392578125	 mt:1960.87890625
-- Cal ASR Loss asr
at:None	 st:8888.474609375	 mt:2267.669189453125
-- Cal ASR Loss asr
at:None	 st:6048.8291015625	 mt:1656.70458984375
-- Cal ASR Loss asr
at:None	 st:6743.4375	 mt:1625.981689453125
-- Cal ASR Loss asr
at:None	 st:6236.48095703125	 mt:1440.0792236328125
-- Cal ASR Loss asr
at:None	 st:7225.759765625	 mt:1731.252197265625
-- Cal ASR Loss asr
at:None	 st:6882.8544921875	 mt:1623.9833984375
-- Cal ASR Loss asr
at:None	 st:6043.984375	 mt:1453.5875244140625
-- Cal ASR Loss asr
at:None	 st:8950.5654296875	 mt:2255.1826171875
-- Cal ASR Loss asr
at:None	 st:7395.4091796875	 mt:1982.504638671875
-- Cal ASR Loss asr
at:None	 st:4221.12646484375	 mt:931.0015258789062
-- Cal ASR Loss asr
at:None	 st:4918.927734375	 mt:1122.6336669921875
-- Cal ASR Loss asr
at:None	 st:6551.3095703125	 mt:1441.994140625
-- Cal ASR Loss asr
at:None	 st:8132.625	 mt:1895.5885009765625
-- Cal ASR Loss asr
at:None	 st:4683.13671875	 mt:1092.4471435546875
-- Cal ASR Loss asr
at:None	 st:6235.251953125	 mt:1592.1732177734375
-- Cal ASR Loss asr
at:None	 st:9568.8125	 mt:2557.020263671875
-- Cal ASR Loss asr
at:None	 st:6070.9619140625	 mt:1517.04931640625
-- Cal ASR Loss asr
2023-07-24 20:30:38 | INFO | train_inner | epoch 001:    100 / 1474 loss=19.616, trans_loss=5.599, nll_loss=4.164, w2v_ctc_loss=23.048, task_loss=0, contrastive_loss=3.325, total=4207.04, n_correct=209.27, ppl=17.93, accuracy=4.974, wps=20104.9, ups=1.6, wpb=12551.1, bsz=471.4, num_updates=100, lr=4.098e-06, gnorm=0.913, clip=0, loss_scale=128, train_wall=68, gb_free=19.5, wall=153
at:None	 st:4405.4599609375	 mt:1022.8797607421875
-- Cal ASR Loss asr
at:None	 st:8041.345703125	 mt:2039.24462890625
-- Cal ASR Loss asr
at:None	 st:5503.8671875	 mt:1335.2503662109375
-- Cal ASR Loss asr
at:None	 st:11549.626953125	 mt:3179.5390625
-- Cal ASR Loss asr
at:None	 st:7338.1142578125	 mt:1626.697509765625
-- Cal ASR Loss asr
at:None	 st:4747.17431640625	 mt:1120.3822021484375
-- Cal ASR Loss asr
at:None	 st:7513.1416015625	 mt:1763.225830078125
-- Cal ASR Loss asr
at:None	 st:5723.3759765625	 mt:1280.4451904296875
-- Cal ASR Loss asr
at:None	 st:7076.173828125	 mt:1543.1990966796875
-- Cal ASR Loss asr
at:None	 st:4618.5751953125	 mt:1058.551025390625
-- Cal ASR Loss asr
at:None	 st:6490.48046875	 mt:1456.373046875
-- Cal ASR Loss asr
at:None	 st:8973.455078125	 mt:2298.26806640625
-- Cal ASR Loss asr
at:None	 st:4661.3330078125	 mt:1153.66064453125
-- Cal ASR Loss asr
at:None	 st:7320.72021484375	 mt:1798.15087890625
-- Cal ASR Loss asr
at:None	 st:4689.248046875	 mt:1146.962158203125
-- Cal ASR Loss asr
at:None	 st:9289.201171875	 mt:2308.97900390625
-- Cal ASR Loss asr
at:None	 st:9520.3212890625	 mt:2385.513671875
-- Cal ASR Loss asr
at:None	 st:11235.0927734375	 mt:3115.377685546875
-- Cal ASR Loss asr
at:None	 st:4381.92822265625	 mt:941.6658325195312
-- Cal ASR Loss asr
at:None	 st:4819.67431640625	 mt:1207.233154296875
-- Cal ASR Loss asr
at:None	 st:7174.9033203125	 mt:1672.6317138671875
-- Cal ASR Loss asr
at:None	 st:4317.5888671875	 mt:980.702880859375
-- Cal ASR Loss asr
at:None	 st:9297.9033203125	 mt:2352.30224609375
-- Cal ASR Loss asr
at:None	 st:4627.87109375	 mt:1223.4080810546875
-- Cal ASR Loss asr
at:None	 st:6453.0205078125	 mt:1494.82666015625
-- Cal ASR Loss asr
at:None	 st:8781.80078125	 mt:2059.455810546875
-- Cal ASR Loss asr
at:None	 st:7811.97607421875	 mt:1788.5830078125
-- Cal ASR Loss asr
at:None	 st:5483.5322265625	 mt:1270.6468505859375
-- Cal ASR Loss asr
at:None	 st:6672.2529296875	 mt:1549.522705078125
-- Cal ASR Loss asr
at:None	 st:5837.8837890625	 mt:1437.89306640625
-- Cal ASR Loss asr
at:None	 st:9642.5009765625	 mt:2658.439697265625
-- Cal ASR Loss asr
at:None	 st:7047.96435546875	 mt:1596.477294921875
-- Cal ASR Loss asr
at:None	 st:5307.2587890625	 mt:1342.501220703125
-- Cal ASR Loss asr
at:None	 st:4486.384765625	 mt:1010.9688110351562
-- Cal ASR Loss asr
at:None	 st:5580.833984375	 mt:1265.626953125
-- Cal ASR Loss asr
at:None	 st:4649.9130859375	 mt:1171.468994140625
-- Cal ASR Loss asr
at:None	 st:8359.5078125	 mt:2123.2822265625
-- Cal ASR Loss asr
at:None	 st:9639.134765625	 mt:2493.281494140625
-- Cal ASR Loss asr
at:None	 st:8348.8388671875	 mt:1990.748291015625
-- Cal ASR Loss asr
at:None	 st:6893.71875	 mt:1449.9215087890625
-- Cal ASR Loss asr
at:None	 st:5255.15234375	 mt:1183.6312255859375
-- Cal ASR Loss asr
at:None	 st:7503.8212890625	 mt:1695.7003173828125
-- Cal ASR Loss asr
at:None	 st:5243.9208984375	 mt:1284.40380859375
-- Cal ASR Loss asr
at:None	 st:5306.10595703125	 mt:1230.7645263671875
-- Cal ASR Loss asr
at:None	 st:7587.1962890625	 mt:1845.5355224609375
-- Cal ASR Loss asr
at:None	 st:6873.66796875	 mt:1393.49560546875
-- Cal ASR Loss asr
at:None	 st:4538.517578125	 mt:1028.0269775390625
-- Cal ASR Loss asr
at:None	 st:7493.451171875	 mt:1857.2384033203125
-- Cal ASR Loss asr
at:None	 st:7309.2861328125	 mt:1838.1907958984375
-- Cal ASR Loss asr
at:None	 st:6920.52734375	 mt:1732.6881103515625
-- Cal ASR Loss asr
at:None	 st:8081.490234375	 mt:1787.8531494140625
-- Cal ASR Loss asr
at:None	 st:8155.1337890625	 mt:2081.096923828125
-- Cal ASR Loss asr
at:None	 st:7103.5703125	 mt:1688.545654296875
-- Cal ASR Loss asr
at:None	 st:7039.5986328125	 mt:1418.9764404296875
-- Cal ASR Loss asr
at:None	 st:5457.74609375	 mt:1373.701171875
-- Cal ASR Loss asr
at:None	 st:8381.740234375	 mt:2077.3818359375
-- Cal ASR Loss asr
at:None	 st:4756.10205078125	 mt:1073.0880126953125
-- Cal ASR Loss asr
at:None	 st:7654.6318359375	 mt:2025.46240234375
-- Cal ASR Loss asr
at:None	 st:6499.37841796875	 mt:1659.9708251953125
-- Cal ASR Loss asr
at:None	 st:9202.8564453125	 mt:2299.396728515625
-- Cal ASR Loss asr
at:None	 st:9543.5224609375	 mt:2545.0966796875
-- Cal ASR Loss asr
at:None	 st:7320.42626953125	 mt:1831.85595703125
-- Cal ASR Loss asr
at:None	 st:6646.421875	 mt:1496.14599609375
-- Cal ASR Loss asr
at:None	 st:5963.2763671875	 mt:1305.4583740234375
-- Cal ASR Loss asr
at:None	 st:7996.3291015625	 mt:1941.1363525390625
-- Cal ASR Loss asr
at:None	 st:5417.04296875	 mt:1292.54931640625
-- Cal ASR Loss asr
at:None	 st:8828.4716796875	 mt:2169.26953125
-- Cal ASR Loss asr
at:None	 st:7436.44775390625	 mt:1813.563232421875
-- Cal ASR Loss asr
at:None	 st:4633.8291015625	 mt:1209.2939453125
-- Cal ASR Loss asr
at:None	 st:6477.748046875	 mt:1754.6572265625
-- Cal ASR Loss asr
at:None	 st:11102.55859375	 mt:3085.601318359375
-- Cal ASR Loss asr
at:None	 st:11999.234375	 mt:3482.313232421875
-- Cal ASR Loss asr
at:None	 st:7997.5830078125	 mt:1935.01708984375
-- Cal ASR Loss asr
at:None	 st:6490.044921875	 mt:1417.9127197265625
-- Cal ASR Loss asr
at:None	 st:5547.59326171875	 mt:1435.932861328125
-- Cal ASR Loss asr
at:None	 st:8434.142578125	 mt:2141.37109375
-- Cal ASR Loss asr
at:None	 st:7128.1826171875	 mt:1776.7215576171875
-- Cal ASR Loss asr
at:None	 st:7074.9501953125	 mt:1563.974365234375
-- Cal ASR Loss asr
at:None	 st:7567.8916015625	 mt:1721.262939453125
-- Cal ASR Loss asr
at:None	 st:7449.015625	 mt:1677.9208984375
-- Cal ASR Loss asr
at:None	 st:8683.64453125	 mt:2207.268798828125
-- Cal ASR Loss asr
at:None	 st:5190.42919921875	 mt:1186.43359375
-- Cal ASR Loss asr
at:None	 st:6619.5810546875	 mt:1647.4730224609375
-- Cal ASR Loss asr
at:None	 st:10326.841796875	 mt:2934.83984375
-- Cal ASR Loss asr
at:None	 st:7001.66796875	 mt:1719.594482421875
-- Cal ASR Loss asr
at:None	 st:6633.36865234375	 mt:1407.6011962890625
-- Cal ASR Loss asr
at:None	 st:8381.1767578125	 mt:2268.211669921875
-- Cal ASR Loss asr
at:None	 st:4620.96435546875	 mt:1169.882080078125
-- Cal ASR Loss asr
at:None	 st:6996.39404296875	 mt:1568.971923828125
-- Cal ASR Loss asr
at:None	 st:9543.634765625	 mt:2466.2802734375
-- Cal ASR Loss asr
at:None	 st:5677.15478515625	 mt:1280.5634765625
-- Cal ASR Loss asr
at:None	 st:7653.67822265625	 mt:1911.40869140625
-- Cal ASR Loss asr
at:None	 st:5068.31982421875	 mt:1192.57666015625
-- Cal ASR Loss asr
at:None	 st:8605.171875	 mt:2044.3062744140625
-- Cal ASR Loss asr
at:None	 st:4846.482421875	 mt:1119.05712890625
-- Cal ASR Loss asr
at:None	 st:5260.6494140625	 mt:1258.6494140625
-- Cal ASR Loss asr
at:None	 st:8426.5751953125	 mt:2133.55224609375
-- Cal ASR Loss asr
at:None	 st:4582.759765625	 mt:1148.494384765625
-- Cal ASR Loss asr
at:None	 st:9040.0107421875	 mt:2372.7119140625
-- Cal ASR Loss asr
at:None	 st:5433.99169921875	 mt:1354.0738525390625
-- Cal ASR Loss asr
at:None	 st:9179.21484375	 mt:2422.50830078125
-- Cal ASR Loss asr
at:None	 st:9216.66015625	 mt:2297.911865234375
-- Cal ASR Loss asr
at:None	 st:7871.9931640625	 mt:1865.472412109375
-- Cal ASR Loss asr
at:None	 st:7556.107421875	 mt:1980.90478515625
-- Cal ASR Loss asr
at:None	 st:6077.2724609375	 mt:1410.5989990234375
-- Cal ASR Loss asr
at:None	 st:5104.333984375	 mt:1377.7008056640625
-- Cal ASR Loss asr
at:None	 st:6800.4296875	 mt:1575.7078857421875
-- Cal ASR Loss asr
at:None	 st:5443.166015625	 mt:1288.6898193359375
-- Cal ASR Loss asr
at:None	 st:5406.51025390625	 mt:1303.9093017578125
-- Cal ASR Loss asr
at:None	 st:7956.8291015625	 mt:1842.5921630859375
-- Cal ASR Loss asr
at:None	 st:6695.5703125	 mt:1414.886962890625
-- Cal ASR Loss asr
at:None	 st:8109.6064453125	 mt:1924.2894287109375
-- Cal ASR Loss asr
at:None	 st:6228.44921875	 mt:1482.2303466796875
-- Cal ASR Loss asr
at:None	 st:8193.794921875	 mt:2116.010986328125
-- Cal ASR Loss asr
at:None	 st:7009.93798828125	 mt:1729.318359375
-- Cal ASR Loss asr
at:None	 st:7743.2197265625	 mt:1880.338623046875
-- Cal ASR Loss asr
at:None	 st:7725.42578125	 mt:1911.89501953125
-- Cal ASR Loss asr
at:None	 st:7850.5556640625	 mt:1912.394775390625
-- Cal ASR Loss asr
at:None	 st:7276.2421875	 mt:1541.72216796875
-- Cal ASR Loss asr
at:None	 st:9962.400390625	 mt:2569.04443359375
-- Cal ASR Loss asr
at:None	 st:6705.337890625	 mt:1645.9339599609375
-- Cal ASR Loss asr
at:None	 st:9021.423828125	 mt:2351.642822265625
-- Cal ASR Loss asr
at:None	 st:5734.29833984375	 mt:1380.417724609375
-- Cal ASR Loss asr
at:None	 st:9530.560546875	 mt:2267.614013671875
-- Cal ASR Loss asr
at:None	 st:5421.3408203125	 mt:1115.08203125
-- Cal ASR Loss asr
at:None	 st:8700.4736328125	 mt:2001.989990234375
-- Cal ASR Loss asr
at:None	 st:5798.357421875	 mt:1485.44287109375
-- Cal ASR Loss asr
at:None	 st:6014.09326171875	 mt:1505.11865234375
-- Cal ASR Loss asr
at:None	 st:9004.720703125	 mt:2325.328857421875
-- Cal ASR Loss asr
at:None	 st:6899.7109375	 mt:1397.116943359375
-- Cal ASR Loss asr
at:None	 st:4641.72021484375	 mt:1068.1922607421875
-- Cal ASR Loss asr
at:None	 st:5929.38330078125	 mt:1365.9630126953125
-- Cal ASR Loss asr
at:None	 st:10612.3681640625	 mt:2896.203857421875
-- Cal ASR Loss asr
at:None	 st:9309.541015625	 mt:2474.772216796875
-- Cal ASR Loss asr
at:None	 st:7590.8623046875	 mt:1779.0892333984375
-- Cal ASR Loss asr
at:None	 st:4201.66552734375	 mt:909.9036254882812
-- Cal ASR Loss asr
at:None	 st:7436.478515625	 mt:1819.15478515625
-- Cal ASR Loss asr
at:None	 st:7182.884765625	 mt:1278.47216796875
-- Cal ASR Loss asr
at:None	 st:8195.45703125	 mt:1984.4249267578125
-- Cal ASR Loss asr
at:None	 st:7745.79296875	 mt:1855.3077392578125
-- Cal ASR Loss asr
at:None	 st:4705.3779296875	 mt:1278.856689453125
-- Cal ASR Loss asr
at:None	 st:10898.2529296875	 mt:2988.20068359375
-- Cal ASR Loss asr
at:None	 st:7866.28125	 mt:1918.392578125
-- Cal ASR Loss asr
at:None	 st:4722.142578125	 mt:1247.5159912109375
-- Cal ASR Loss asr
at:None	 st:9347.884765625	 mt:2348.80517578125
-- Cal ASR Loss asr
at:None	 st:7693.509765625	 mt:1748.987548828125
-- Cal ASR Loss asr
at:None	 st:5144.78173828125	 mt:1193.9873046875
-- Cal ASR Loss asr
at:None	 st:7033.4453125	 mt:1746.35400390625
-- Cal ASR Loss asr
at:None	 st:6175.12744140625	 mt:1484.4287109375
-- Cal ASR Loss asr
at:None	 st:4984.22900390625	 mt:1070.943115234375
-- Cal ASR Loss asr
at:None	 st:8421.3134765625	 mt:2188.777587890625
-- Cal ASR Loss asr
at:None	 st:4680.1171875	 mt:1009.3494873046875
-- Cal ASR Loss asr
at:None	 st:7473.5908203125	 mt:1566.537109375
-- Cal ASR Loss asr
at:None	 st:7488.18408203125	 mt:1682.2061767578125
-- Cal ASR Loss asr
at:None	 st:8269.302734375	 mt:2137.8173828125
-- Cal ASR Loss asr
at:None	 st:6826.298828125	 mt:1641.2388916015625
-- Cal ASR Loss asr
at:None	 st:6542.71142578125	 mt:1680.8734130859375
-- Cal ASR Loss asr
at:None	 st:9206.103515625	 mt:2343.313232421875
-- Cal ASR Loss asr
at:None	 st:8477.685546875	 mt:2090.271484375
-- Cal ASR Loss asr
at:None	 st:11964.7294921875	 mt:3431.14892578125
-- Cal ASR Loss asr
at:None	 st:8390.888671875	 mt:1868.742431640625
-- Cal ASR Loss asr
at:None	 st:6667.55224609375	 mt:1464.0955810546875
-- Cal ASR Loss asr
at:None	 st:4857.693359375	 mt:1184.935546875
-- Cal ASR Loss asr
at:None	 st:4608.5771484375	 mt:1130.296875
-- Cal ASR Loss asr
at:None	 st:5812.75146484375	 mt:1280.5345458984375
-- Cal ASR Loss asr
at:None	 st:7277.3212890625	 mt:1534.8802490234375
-- Cal ASR Loss asr
at:None	 st:5979.41455078125	 mt:1409.709228515625
-- Cal ASR Loss asr
at:None	 st:8309.2783203125	 mt:2022.614990234375
-- Cal ASR Loss asr
at:None	 st:4855.55517578125	 mt:1232.219482421875
-- Cal ASR Loss asr
at:None	 st:8927.478515625	 mt:2245.20654296875
-- Cal ASR Loss asr
at:None	 st:8208.759765625	 mt:2083.35595703125
-- Cal ASR Loss asr
at:None	 st:7169.744140625	 mt:1797.594482421875
-- Cal ASR Loss asr
at:None	 st:6235.7626953125	 mt:1485.43115234375
-- Cal ASR Loss asr
at:None	 st:7775.1416015625	 mt:1891.51171875
-- Cal ASR Loss asr
at:None	 st:7435.77392578125	 mt:1645.35595703125
-- Cal ASR Loss asr
at:None	 st:9715.865234375	 mt:2504.624267578125
-- Cal ASR Loss asr
at:None	 st:5914.83203125	 mt:1427.353271484375
-- Cal ASR Loss asr
at:None	 st:6076.6005859375	 mt:1489.0322265625
-- Cal ASR Loss asr
at:None	 st:6948.603515625	 mt:1663.543212890625
-- Cal ASR Loss asr
at:None	 st:7315.140625	 mt:1730.326416015625
-- Cal ASR Loss asr
at:None	 st:5607.330078125	 mt:1398.8990478515625
-- Cal ASR Loss asr
at:None	 st:6624.611328125	 mt:1632.993408203125
-- Cal ASR Loss asr
at:None	 st:6845.6591796875	 mt:1503.759765625
-- Cal ASR Loss asr
at:None	 st:6935.42724609375	 mt:1581.66064453125
-- Cal ASR Loss asr
at:None	 st:7667.55078125	 mt:1740.634521484375
-- Cal ASR Loss asr
at:None	 st:4713.7138671875	 mt:1222.934814453125
-- Cal ASR Loss asr
at:None	 st:7942.2412109375	 mt:1908.599365234375
-- Cal ASR Loss asr
at:None	 st:6535.7734375	 mt:1259.617431640625
-- Cal ASR Loss asr
at:None	 st:6574.81298828125	 mt:1572.2391357421875
-- Cal ASR Loss asr
at:None	 st:8163.0556640625	 mt:1932.8665771484375
-- Cal ASR Loss asr
at:None	 st:8527.529296875	 mt:2132.805908203125
-- Cal ASR Loss asr
at:None	 st:5008.59033203125	 mt:1211.607421875
-- Cal ASR Loss asr
at:None	 st:5053.1083984375	 mt:1127.058349609375
-- Cal ASR Loss asr
at:None	 st:8914.9716796875	 mt:2267.483154296875
-- Cal ASR Loss asr
at:None	 st:9234.1884765625	 mt:2412.207763671875
-- Cal ASR Loss asr
at:None	 st:7654.2060546875	 mt:1926.1943359375
-- Cal ASR Loss asr
at:None	 st:7878.24609375	 mt:1933.492431640625
-- Cal ASR Loss asr
at:None	 st:5141.2275390625	 mt:1247.0538330078125
-- Cal ASR Loss asr
at:None	 st:8371.7333984375	 mt:2101.611572265625
-- Cal ASR Loss asr
at:None	 st:7809.8564453125	 mt:1661.048095703125
-- Cal ASR Loss asr
at:None	 st:5067.6513671875	 mt:1185.52587890625
-- Cal ASR Loss asr
at:None	 st:4557.37109375	 mt:1149.6119384765625
-- Cal ASR Loss asr
at:None	 st:8076.298828125	 mt:1966.75634765625
-- Cal ASR Loss asr
at:None	 st:8895.4541015625	 mt:2137.80126953125
-- Cal ASR Loss asr
at:None	 st:5189.5048828125	 mt:1275.116943359375
-- Cal ASR Loss asr
at:None	 st:9098.248046875	 mt:2329.781982421875
-- Cal ASR Loss asr
at:None	 st:7230.97265625	 mt:1601.94384765625
-- Cal ASR Loss asr
at:None	 st:7399.111328125	 mt:1661.166259765625
-- Cal ASR Loss asr
at:None	 st:4580.64404296875	 mt:1082.4205322265625
-- Cal ASR Loss asr
at:None	 st:5221.43701171875	 mt:1217.29931640625
-- Cal ASR Loss asr
at:None	 st:6122.2783203125	 mt:1410.6805419921875
-- Cal ASR Loss asr
at:None	 st:7427.4111328125	 mt:1721.266357421875
-- Cal ASR Loss asr
at:None	 st:7691.0224609375	 mt:1890.8001708984375
-- Cal ASR Loss asr
at:None	 st:7480.51904296875	 mt:1778.66064453125
-- Cal ASR Loss asr
at:None	 st:8905.03515625	 mt:2192.1123046875
-- Cal ASR Loss asr
at:None	 st:7390.00634765625	 mt:1820.340087890625
-- Cal ASR Loss asr
at:None	 st:4462.06787109375	 mt:1103.98974609375
-- Cal ASR Loss asr
at:None	 st:7412.2724609375	 mt:1874.295654296875
-- Cal ASR Loss asr
at:None	 st:8743.37890625	 mt:2190.1669921875
-- Cal ASR Loss asr
at:None	 st:7551.7978515625	 mt:1898.7691650390625
-- Cal ASR Loss asr
at:None	 st:9699.560546875	 mt:2588.0126953125
-- Cal ASR Loss asr
at:None	 st:6815.1630859375	 mt:1613.38671875
-- Cal ASR Loss asr
at:None	 st:5268.2861328125	 mt:1426.3590087890625
-- Cal ASR Loss asr
at:None	 st:4773.4248046875	 mt:1165.9259033203125
-- Cal ASR Loss asr
at:None	 st:7628.02734375	 mt:1795.199951171875
-- Cal ASR Loss asr
at:None	 st:8972.5546875	 mt:2288.822998046875
-- Cal ASR Loss asr
at:None	 st:7198.71484375	 mt:1778.096923828125
-- Cal ASR Loss asr
at:None	 st:5519.3369140625	 mt:1318.678466796875
-- Cal ASR Loss asr
at:None	 st:5003.3232421875	 mt:1212.258544921875
-- Cal ASR Loss asr
at:None	 st:7207.7197265625	 mt:1623.7939453125
-- Cal ASR Loss asr
at:None	 st:6983.29736328125	 mt:1661.793701171875
-- Cal ASR Loss asr
at:None	 st:7143.287109375	 mt:1511.7908935546875
-- Cal ASR Loss asr
at:None	 st:5956.287109375	 mt:1348.623046875
-- Cal ASR Loss asr
at:None	 st:9713.5634765625	 mt:2634.64990234375
-- Cal ASR Loss asr
at:None	 st:5814.04052734375	 mt:1270.05322265625
-- Cal ASR Loss asr
at:None	 st:7123.166015625	 mt:1663.078125
-- Cal ASR Loss asr
at:None	 st:7104.42822265625	 mt:1650.5572509765625
-- Cal ASR Loss asr
at:None	 st:10613.2900390625	 mt:2993.5185546875
-- Cal ASR Loss asr
at:None	 st:6628.5556640625	 mt:1477.7421875
-- Cal ASR Loss asr
at:None	 st:6905.2685546875	 mt:1577.4630126953125
-- Cal ASR Loss asr
at:None	 st:9122.7890625	 mt:2408.7412109375
-- Cal ASR Loss asr
at:None	 st:4326.76953125	 mt:990.85205078125
-- Cal ASR Loss asr
at:None	 st:8032.4658203125	 mt:1798.3836669921875
-- Cal ASR Loss asr
at:None	 st:6250.44921875	 mt:1410.330078125
-- Cal ASR Loss asr
at:None	 st:6663.044921875	 mt:1578.795166015625
-- Cal ASR Loss asr
at:None	 st:9641.892578125	 mt:2641.26123046875
-- Cal ASR Loss asr
at:None	 st:5762.296875	 mt:1330.8631591796875
-- Cal ASR Loss asr
at:None	 st:7638.4716796875	 mt:1863.653076171875
-- Cal ASR Loss asr
at:None	 st:7903.751953125	 mt:2026.9910888671875
-- Cal ASR Loss asr
at:None	 st:4913.8486328125	 mt:1058.755859375
-- Cal ASR Loss asr
at:None	 st:4754.05078125	 mt:997.9496459960938
-- Cal ASR Loss asr
at:None	 st:8574.3642578125	 mt:2010.859375
-- Cal ASR Loss asr
at:None	 st:6456.1142578125	 mt:1234.24560546875
-- Cal ASR Loss asr
at:None	 st:8386.2822265625	 mt:2010.212890625
-- Cal ASR Loss asr
at:None	 st:7140.12890625	 mt:1608.6536865234375
-- Cal ASR Loss asr
at:None	 st:5035.8955078125	 mt:1209.445556640625
-- Cal ASR Loss asr
at:None	 st:9026.826171875	 mt:2236.403076171875
-- Cal ASR Loss asr
at:None	 st:7685.84375	 mt:1813.9229736328125
-- Cal ASR Loss asr
at:None	 st:5719.8525390625	 mt:1510.5531005859375
-- Cal ASR Loss asr
at:None	 st:5031.8310546875	 mt:1274.0570068359375
-- Cal ASR Loss asr
at:None	 st:6202.40087890625	 mt:1388.1962890625
-- Cal ASR Loss asr
at:None	 st:7584.5927734375	 mt:1799.4830322265625
-- Cal ASR Loss asr
at:None	 st:9205.392578125	 mt:2300.91064453125
-- Cal ASR Loss asr
at:None	 st:4894.12890625	 mt:1111.432373046875
-- Cal ASR Loss asr
at:None	 st:8243.5244140625	 mt:1982.163818359375
-- Cal ASR Loss asr
at:None	 st:8247.388671875	 mt:1809.9730224609375
-- Cal ASR Loss asr
at:None	 st:4758.8173828125	 mt:1076.4456787109375
-- Cal ASR Loss asr
at:None	 st:8681.388671875	 mt:2054.06396484375
-- Cal ASR Loss asr
at:None	 st:8555.8466796875	 mt:2029.235595703125
-- Cal ASR Loss asr
at:None	 st:12986.46484375	 mt:3864.90283203125
-- Cal ASR Loss asr
at:None	 st:7615.12158203125	 mt:1865.1326904296875
-- Cal ASR Loss asr
at:None	 st:5744.6201171875	 mt:1278.97021484375
-- Cal ASR Loss asr
at:None	 st:5922.1259765625	 mt:1353.8218994140625
-- Cal ASR Loss asr
at:None	 st:7083.65576171875	 mt:1645.3388671875
-- Cal ASR Loss asr
at:None	 st:7499.357421875	 mt:1849.4234619140625
-- Cal ASR Loss asr
at:None	 st:7614.60595703125	 mt:1757.7178955078125
-- Cal ASR Loss asr
at:None	 st:8568.1015625	 mt:2218.91552734375
-- Cal ASR Loss asr
at:None	 st:6245.0810546875	 mt:1485.9442138671875
-- Cal ASR Loss asr
at:None	 st:8228.47265625	 mt:2020.561279296875
-- Cal ASR Loss asr
at:None	 st:7764.34765625	 mt:1721.761474609375
-- Cal ASR Loss asr
at:None	 st:4394.310546875	 mt:990.0640869140625
-- Cal ASR Loss asr
at:None	 st:7628.87890625	 mt:1791.841796875
-- Cal ASR Loss asr
at:None	 st:4516.93603515625	 mt:1122.1021728515625
-- Cal ASR Loss asr
at:None	 st:5727.2587890625	 mt:1181.6514892578125
-- Cal ASR Loss asr
at:None	 st:6003.951171875	 mt:1331.8358154296875
-- Cal ASR Loss asr
at:None	 st:7039.6669921875	 mt:1542.5179443359375
-- Cal ASR Loss asr
at:None	 st:9758.8701171875	 mt:2414.56396484375
-- Cal ASR Loss asr
at:None	 st:4666.099609375	 mt:1126.66552734375
-- Cal ASR Loss asr
at:None	 st:5942.259765625	 mt:1412.384521484375
-- Cal ASR Loss asr
at:None	 st:7671.74755859375	 mt:1869.562255859375
-- Cal ASR Loss asr
at:None	 st:6519.322265625	 mt:1758.2115478515625
-- Cal ASR Loss asr
at:None	 st:6694.32470703125	 mt:1575.601806640625
-- Cal ASR Loss asr
at:None	 st:8904.6806640625	 mt:2194.9873046875
-- Cal ASR Loss asr
at:None	 st:9131.1279296875	 mt:2221.075439453125
-- Cal ASR Loss asr
at:None	 st:7005.58203125	 mt:1715.673095703125
-- Cal ASR Loss asr
at:None	 st:5635.279296875	 mt:1341.48046875
-- Cal ASR Loss asr
at:None	 st:5408.2392578125	 mt:1211.1915283203125
-- Cal ASR Loss asr
at:None	 st:6969.8056640625	 mt:1719.745849609375
-- Cal ASR Loss asr
at:None	 st:5937.958984375	 mt:1651.6929931640625
-- Cal ASR Loss asr
at:None	 st:7742.048828125	 mt:1776.435791015625
-- Cal ASR Loss asr
at:None	 st:7269.43359375	 mt:1708.66015625
-- Cal ASR Loss asr
at:None	 st:10075.197265625	 mt:2804.182861328125
-- Cal ASR Loss asr
at:None	 st:9569.8994140625	 mt:2388.255126953125
-- Cal ASR Loss asr
at:None	 st:7498.79248046875	 mt:1922.001708984375
-- Cal ASR Loss asr
at:None	 st:8545.142578125	 mt:2227.79443359375
-- Cal ASR Loss asr
at:None	 st:7771.1533203125	 mt:1839.0723876953125
-- Cal ASR Loss asr
at:None	 st:4827.017578125	 mt:1126.3199462890625
-- Cal ASR Loss asr
at:None	 st:7762.9169921875	 mt:1953.383056640625
-- Cal ASR Loss asr
at:None	 st:8818.9296875	 mt:2195.783935546875
-- Cal ASR Loss asr
at:None	 st:4847.66064453125	 mt:1098.415771484375
-- Cal ASR Loss asr
at:None	 st:7673.279296875	 mt:1744.49951171875
-- Cal ASR Loss asr
at:None	 st:9046.4794921875	 mt:2400.614501953125
-- Cal ASR Loss asr
at:None	 st:7334.9892578125	 mt:1452.437744140625
-- Cal ASR Loss asr
at:None	 st:7584.47607421875	 mt:1851.275634765625
-- Cal ASR Loss asr
at:None	 st:7929.8955078125	 mt:1901.757080078125
-- Cal ASR Loss asr
at:None	 st:4614.5830078125	 mt:957.3858032226562
-- Cal ASR Loss asr
at:None	 st:5224.4755859375	 mt:1324.992431640625
-- Cal ASR Loss asr
at:None	 st:6562.1689453125	 mt:1551.906982421875
-- Cal ASR Loss asr
at:None	 st:4237.74365234375	 mt:924.2194213867188
-- Cal ASR Loss asr
at:None	 st:4402.8408203125	 mt:992.41943359375
-- Cal ASR Loss asr
at:None	 st:6241.5654296875	 mt:1632.9591064453125
-- Cal ASR Loss asr
at:None	 st:4393.341796875	 mt:1044.78369140625
-- Cal ASR Loss asr
at:None	 st:7179.55078125	 mt:1731.6661376953125
-- Cal ASR Loss asr
at:None	 st:5890.732421875	 mt:1305.36962890625
-- Cal ASR Loss asr
at:None	 st:7505.072265625	 mt:1818.284912109375
-- Cal ASR Loss asr
at:None	 st:7922.279296875	 mt:1782.604736328125
-- Cal ASR Loss asr
at:None	 st:8030.78857421875	 mt:2038.54833984375
-- Cal ASR Loss asr
at:None	 st:9381.3173828125	 mt:2288.91845703125
-- Cal ASR Loss asr
at:None	 st:6988.583984375	 mt:1860.8876953125
-- Cal ASR Loss asr
at:None	 st:6782.81640625	 mt:1555.466796875
-- Cal ASR Loss asr
at:None	 st:4544.9296875	 mt:1018.615234375
-- Cal ASR Loss asr
at:None	 st:6082.4052734375	 mt:1450.9410400390625
-- Cal ASR Loss asr
at:None	 st:10576.4326171875	 mt:2817.26904296875
-- Cal ASR Loss asr
at:None	 st:6572.8359375	 mt:1440.10546875
-- Cal ASR Loss asr
at:None	 st:9034.640625	 mt:2375.81884765625
-- Cal ASR Loss asr
at:None	 st:7865.884765625	 mt:1906.76953125
-- Cal ASR Loss asr
at:None	 st:5681.4208984375	 mt:1159.754638671875
-- Cal ASR Loss asr
at:None	 st:8894.5390625	 mt:2112.355712890625
-- Cal ASR Loss asr
at:None	 st:8163.16357421875	 mt:2051.808349609375
-- Cal ASR Loss asr
at:None	 st:8927.7470703125	 mt:2427.960205078125
-- Cal ASR Loss asr
at:None	 st:6651.7958984375	 mt:1723.3026123046875
-- Cal ASR Loss asr
at:None	 st:9324.1591796875	 mt:2341.725830078125
-- Cal ASR Loss asr
at:None	 st:4500.80517578125	 mt:1167.7733154296875
-- Cal ASR Loss asr
at:None	 st:7794.32421875	 mt:1925.2757568359375
-- Cal ASR Loss asr
at:None	 st:7393.5849609375	 mt:1722.8621826171875
-- Cal ASR Loss asr
at:None	 st:5543.9560546875	 mt:1542.455810546875
-- Cal ASR Loss asr
at:None	 st:6863.43798828125	 mt:1543.91650390625
-- Cal ASR Loss asr
at:None	 st:9474.2958984375	 mt:2532.865234375
-- Cal ASR Loss asr
at:None	 st:5189.587890625	 mt:1224.0330810546875
-- Cal ASR Loss asr
at:None	 st:7563.0283203125	 mt:1799.466796875
-- Cal ASR Loss asr
at:None	 st:5090.341796875	 mt:1140.3587646484375
-- Cal ASR Loss asr
at:None	 st:4929.20263671875	 mt:1210.438232421875
-- Cal ASR Loss asr
at:None	 st:7216.791015625	 mt:1802.081787109375
-- Cal ASR Loss asr
at:None	 st:7512.1396484375	 mt:1838.6365966796875
-- Cal ASR Loss asr
at:None	 st:7238.9443359375	 mt:1724.610595703125
-- Cal ASR Loss asr
at:None	 st:9388.533203125	 mt:2423.63720703125
-- Cal ASR Loss asr
at:None	 st:4455.6201171875	 mt:939.9674072265625
-- Cal ASR Loss asr
at:None	 st:4809.50390625	 mt:1161.9053955078125
-- Cal ASR Loss asr
at:None	 st:9611.341796875	 mt:2412.988525390625
-- Cal ASR Loss asr
at:None	 st:8719.494140625	 mt:2075.002685546875
-- Cal ASR Loss asr
at:None	 st:4770.10888671875	 mt:1086.2593994140625
-- Cal ASR Loss asr
at:None	 st:4751.1171875	 mt:1013.302490234375
-- Cal ASR Loss asr
at:None	 st:8211.6162109375	 mt:2063.330078125
-- Cal ASR Loss asr
at:None	 st:8174.873046875	 mt:2061.515380859375
-- Cal ASR Loss asr
at:None	 st:9125.474609375	 mt:2451.29541015625
-- Cal ASR Loss asr
at:None	 st:9775.98046875	 mt:2517.104736328125
-- Cal ASR Loss asr
at:None	 st:6278.1455078125	 mt:1492.3514404296875
-- Cal ASR Loss asr
at:None	 st:4530.935546875	 mt:1072.1162109375
-- Cal ASR Loss asr
at:None	 st:6600.12451171875	 mt:1435.528564453125
-- Cal ASR Loss asr
at:None	 st:7600.69677734375	 mt:1603.856689453125
-- Cal ASR Loss asr
at:None	 st:9721.6943359375	 mt:2454.276123046875
-- Cal ASR Loss asr
at:None	 st:4203.642578125	 mt:859.765625
-- Cal ASR Loss asr
at:None	 st:4822.36181640625	 mt:1140.944580078125
-- Cal ASR Loss asr
at:None	 st:7951.466796875	 mt:1771.8033447265625
-- Cal ASR Loss asr
at:None	 st:7612.98095703125	 mt:1815.940673828125
-- Cal ASR Loss asr
at:None	 st:7567.35595703125	 mt:1846.8077392578125
-- Cal ASR Loss asr
at:None	 st:7802.8447265625	 mt:1771.78369140625
-- Cal ASR Loss asr
at:None	 st:6816.77880859375	 mt:1735.9254150390625
-- Cal ASR Loss asr
at:None	 st:6509.849609375	 mt:1637.3846435546875
-- Cal ASR Loss asr
at:None	 st:7692.8701171875	 mt:1753.4676513671875
-- Cal ASR Loss asr
at:None	 st:6099.7451171875	 mt:1427.56005859375
-- Cal ASR Loss asr
at:None	 st:8503.12890625	 mt:1989.75439453125
-- Cal ASR Loss asr
at:None	 st:8657.58203125	 mt:2069.7490234375
-- Cal ASR Loss asr
at:None	 st:8633.6943359375	 mt:2064.962646484375
-- Cal ASR Loss asr
at:None	 st:4295.95947265625	 mt:953.5301513671875
-- Cal ASR Loss asr
at:None	 st:8431.3271484375	 mt:1959.386962890625
-- Cal ASR Loss asr
at:None	 st:4732.6611328125	 mt:1024.84423828125
-- Cal ASR Loss asr
at:None	 st:4891.55419921875	 mt:1126.869873046875
-- Cal ASR Loss asr
at:None	 st:5164.03271484375	 mt:1298.20947265625
-- Cal ASR Loss asr
at:None	 st:8702.279296875	 mt:2300.842041015625
-- Cal ASR Loss asr
at:None	 st:4982.9697265625	 mt:1295.5985107421875
-- Cal ASR Loss asr
at:None	 st:4177.4130859375	 mt:912.5791015625
-- Cal ASR Loss asr
at:None	 st:4582.9013671875	 mt:938.827880859375
-- Cal ASR Loss asr
at:None	 st:5466.109375	 mt:1255.537109375
-- Cal ASR Loss asr
at:None	 st:8512.23046875	 mt:2151.423583984375
-- Cal ASR Loss asr
at:None	 st:5923.04296875	 mt:1342.36572265625
-- Cal ASR Loss asr
at:None	 st:7251.6044921875	 mt:1741.8031005859375
-- Cal ASR Loss asr
at:None	 st:8244.3564453125	 mt:2057.2666015625
-- Cal ASR Loss asr
at:None	 st:5899.1015625	 mt:1318.0592041015625
-- Cal ASR Loss asr
at:None	 st:9256.5556640625	 mt:2429.24853515625
-- Cal ASR Loss asr
at:None	 st:7463.6220703125	 mt:1797.827880859375
-- Cal ASR Loss asr
at:None	 st:7525.31640625	 mt:1689.0902099609375
-- Cal ASR Loss asr
at:None	 st:8408.4033203125	 mt:1982.8033447265625
-- Cal ASR Loss asr
at:None	 st:7737.966796875	 mt:1983.674560546875
-- Cal ASR Loss asr
at:None	 st:4550.58056640625	 mt:1056.192138671875
-- Cal ASR Loss asr
at:None	 st:7238.2109375	 mt:1662.6044921875
-- Cal ASR Loss asr
at:None	 st:9472.875	 mt:2348.6474609375
-- Cal ASR Loss asr
at:None	 st:6661.93359375	 mt:1766.699462890625
-- Cal ASR Loss asr
at:None	 st:9208.0166015625	 mt:2322.75439453125
-- Cal ASR Loss asr
at:None	 st:7060.27392578125	 mt:1751.29150390625
-- Cal ASR Loss asr
at:None	 st:5225.1220703125	 mt:1232.71337890625
-- Cal ASR Loss asr
at:None	 st:6582.3125	 mt:1562.556640625
-- Cal ASR Loss asr
at:None	 st:7210.2099609375	 mt:1692.042724609375
-- Cal ASR Loss asr
at:None	 st:6590.6015625	 mt:1566.72119140625
-- Cal ASR Loss asr
at:None	 st:5404.337890625	 mt:1338.260009765625
-- Cal ASR Loss asr
at:None	 st:8061.31640625	 mt:2067.40234375
-- Cal ASR Loss asr
at:None	 st:8520.5517578125	 mt:2304.517578125
-- Cal ASR Loss asr
at:None	 st:6513.9228515625	 mt:1551.714111328125
-- Cal ASR Loss asr
at:None	 st:7987.974609375	 mt:2019.2972412109375
-- Cal ASR Loss asr
at:None	 st:4397.15185546875	 mt:982.5687255859375
-- Cal ASR Loss asr
at:None	 st:6123.40771484375	 mt:1418.567626953125
-- Cal ASR Loss asr
at:None	 st:7607.2060546875	 mt:1867.82421875
-- Cal ASR Loss asr
at:None	 st:9543.9833984375	 mt:2393.8466796875
-- Cal ASR Loss asr
at:None	 st:8070.3193359375	 mt:1840.394775390625
-- Cal ASR Loss asr
at:None	 st:7452.65673828125	 mt:1673.98388671875
-- Cal ASR Loss asr
at:None	 st:6602.421875	 mt:1748.9404296875
-- Cal ASR Loss asr
at:None	 st:6061.662109375	 mt:1430.3756103515625
-- Cal ASR Loss asr
at:None	 st:7702.7529296875	 mt:1821.2841796875
-- Cal ASR Loss asr
at:None	 st:8760.26953125	 mt:2297.280029296875
-- Cal ASR Loss asr
at:None	 st:8211.521484375	 mt:1950.4918212890625
-- Cal ASR Loss asr
at:None	 st:7715.9921875	 mt:1727.93505859375
-- Cal ASR Loss asr
at:None	 st:7981.3857421875	 mt:2003.677734375
-- Cal ASR Loss asr
at:None	 st:6863.1064453125	 mt:1544.548095703125
-- Cal ASR Loss asr
at:None	 st:10807.9677734375	 mt:3058.130126953125
-- Cal ASR Loss asr
at:None	 st:8558.677734375	 mt:2130.37744140625
-- Cal ASR Loss asr
at:None	 st:8384.236328125	 mt:2110.182861328125
-- Cal ASR Loss asr
at:None	 st:9366.8427734375	 mt:2589.03759765625
-- Cal ASR Loss asr
at:None	 st:6799.6494140625	 mt:1624.6556396484375
-- Cal ASR Loss asr
at:None	 st:5786.97265625	 mt:1262.143798828125
-- Cal ASR Loss asr
at:None	 st:5245.99560546875	 mt:1205.30224609375
-- Cal ASR Loss asr
at:None	 st:7516.8310546875	 mt:1570.6158447265625
-- Cal ASR Loss asr
at:None	 st:7821.447265625	 mt:1762.646728515625
-- Cal ASR Loss asr
at:None	 st:7729.7099609375	 mt:1894.951904296875
-- Cal ASR Loss asr
at:None	 st:4534.10302734375	 mt:1095.050537109375
-- Cal ASR Loss asr
at:None	 st:9158.6162109375	 mt:2375.763671875
-- Cal ASR Loss asr
at:None	 st:8967.052734375	 mt:2439.29443359375
-- Cal ASR Loss asr
at:None	 st:6514.322265625	 mt:1628.739013671875
-- Cal ASR Loss asr
at:None	 st:9606.7216796875	 mt:2460.25634765625
-- Cal ASR Loss asr
at:None	 st:7712.591796875	 mt:1826.3111572265625
-- Cal ASR Loss asr
at:None	 st:4936.3310546875	 mt:1182.836669921875
-- Cal ASR Loss asr
at:None	 st:6977.056640625	 mt:1549.010009765625
-- Cal ASR Loss asr
at:None	 st:7712.173828125	 mt:1821.13623046875
-- Cal ASR Loss asr
at:None	 st:4895.4658203125	 mt:1143.8358154296875
-- Cal ASR Loss asr
at:None	 st:7670.857421875	 mt:2031.791015625
-- Cal ASR Loss asr
at:None	 st:4391.54248046875	 mt:990.3331909179688
-- Cal ASR Loss asr
at:None	 st:4623.28857421875	 mt:1200.41748046875
-- Cal ASR Loss asr
at:None	 st:4593.5732421875	 mt:1018.4307250976562
-- Cal ASR Loss asr
at:None	 st:7980.9169921875	 mt:1877.06591796875
-- Cal ASR Loss asr
at:None	 st:7375.9677734375	 mt:1794.950927734375
-- Cal ASR Loss asr
at:None	 st:7504.6181640625	 mt:1776.962646484375
-- Cal ASR Loss asr
at:None	 st:6430.1083984375	 mt:1674.254150390625
-- Cal ASR Loss asr
at:None	 st:5489.953125	 mt:1427.5340576171875
-- Cal ASR Loss asr
at:None	 st:7447.2392578125	 mt:1867.58154296875
-- Cal ASR Loss asr
at:None	 st:7286.4560546875	 mt:1810.6424560546875
-- Cal ASR Loss asr
at:None	 st:8219.974609375	 mt:2015.5250244140625
-- Cal ASR Loss asr
at:None	 st:7844.4765625	 mt:1796.922119140625
-- Cal ASR Loss asr
at:None	 st:8503.306640625	 mt:2278.87890625
-- Cal ASR Loss asr
at:None	 st:4636.6767578125	 mt:1206.4605712890625
-- Cal ASR Loss asr
at:None	 st:10838.306640625	 mt:3078.885498046875
-- Cal ASR Loss asr
at:None	 st:4396.57275390625	 mt:1096.2236328125
-- Cal ASR Loss asr
at:None	 st:5359.23046875	 mt:1234.3084716796875
-- Cal ASR Loss asr
at:None	 st:7700.7373046875	 mt:1904.0380859375
-- Cal ASR Loss asr
at:None	 st:8588.767578125	 mt:2120.388671875
-- Cal ASR Loss asr
at:None	 st:7099.46630859375	 mt:1638.7139892578125
-- Cal ASR Loss asr
at:None	 st:6366.6767578125	 mt:1545.8485107421875
-- Cal ASR Loss asr
at:None	 st:8273.3720703125	 mt:1891.7772216796875
-- Cal ASR Loss asr
at:None	 st:10629.583984375	 mt:2978.998046875
-- Cal ASR Loss asr
at:None	 st:7658.07763671875	 mt:1862.920654296875
-- Cal ASR Loss asr
at:None	 st:7259.384765625	 mt:1677.434326171875
-- Cal ASR Loss asr
at:None	 st:7020.4130859375	 mt:1316.382080078125
-- Cal ASR Loss asr
at:None	 st:7807.248046875	 mt:1799.9658203125
-- Cal ASR Loss asr
at:None	 st:7584.705078125	 mt:1878.38525390625
-- Cal ASR Loss asr
at:None	 st:5646.6083984375	 mt:1333.135986328125
-- Cal ASR Loss asr
at:None	 st:4668.18798828125	 mt:1100.1953125
-- Cal ASR Loss asr
at:None	 st:5239.8662109375	 mt:1317.259521484375
-- Cal ASR Loss asr
at:None	 st:7067.44287109375	 mt:1550.8275146484375
-- Cal ASR Loss asr
at:None	 st:6190.626953125	 mt:1413.3853759765625
-- Cal ASR Loss asr
at:None	 st:10087.59765625	 mt:2554.60546875
-- Cal ASR Loss asr
at:None	 st:8350.4833984375	 mt:2046.52197265625
-- Cal ASR Loss asr
at:None	 st:8323.0703125	 mt:2327.079345703125
-- Cal ASR Loss asr
at:None	 st:9965.4052734375	 mt:2520.150390625
-- Cal ASR Loss asr
at:None	 st:9316.34375	 mt:2379.833984375
-- Cal ASR Loss asr
at:None	 st:6952.69140625	 mt:1757.02978515625
-- Cal ASR Loss asr
at:None	 st:6328.455078125	 mt:1571.3226318359375
-- Cal ASR Loss asr
at:None	 st:4298.521484375	 mt:1028.9964599609375
-- Cal ASR Loss asr
at:None	 st:7094.8310546875	 mt:1915.8695068359375
-- Cal ASR Loss asr
at:None	 st:7594.15283203125	 mt:1778.4620361328125
-- Cal ASR Loss asr
at:None	 st:5338.4658203125	 mt:1428.0810546875
-- Cal ASR Loss asr
at:None	 st:7825.57470703125	 mt:1949.125732421875
-- Cal ASR Loss asr
at:None	 st:9669.18359375	 mt:2578.75830078125
-- Cal ASR Loss asr
at:None	 st:5983.7626953125	 mt:1215.2431640625
-- Cal ASR Loss asr
at:None	 st:7945.05419921875	 mt:1945.82470703125
-- Cal ASR Loss asr
at:None	 st:7416.12255859375	 mt:1788.52978515625
-- Cal ASR Loss asr
at:None	 st:6249.8720703125	 mt:1479.58447265625
-- Cal ASR Loss asr
at:None	 st:5590.11865234375	 mt:1516.453125
-- Cal ASR Loss asr
at:None	 st:5677.9267578125	 mt:1278.6103515625
-- Cal ASR Loss asr
at:None	 st:8754.41796875	 mt:2337.48876953125
-- Cal ASR Loss asr
at:None	 st:8354.4482421875	 mt:2029.2637939453125
-- Cal ASR Loss asr
at:None	 st:7005.52734375	 mt:1420.8563232421875
-- Cal ASR Loss asr
at:None	 st:6570.6943359375	 mt:1540.3150634765625
-- Cal ASR Loss asr
at:None	 st:9310.755859375	 mt:2363.26220703125
-- Cal ASR Loss asr
at:None	 st:4181.806640625	 mt:812.3568115234375
-- Cal ASR Loss asr
at:None	 st:6482.9814453125	 mt:1588.4688720703125
-- Cal ASR Loss asr
at:None	 st:8851.65234375	 mt:2180.8623046875
-- Cal ASR Loss asr
at:None	 st:7529.18896484375	 mt:1771.5528564453125
-- Cal ASR Loss asr
at:None	 st:7065.8662109375	 mt:1485.4188232421875
-- Cal ASR Loss asr
at:None	 st:5379.357421875	 mt:1335.8590087890625
-- Cal ASR Loss asr
at:None	 st:5216.59228515625	 mt:1309.63916015625
-- Cal ASR Loss asr
at:None	 st:6700.3515625	 mt:1364.1346435546875
-- Cal ASR Loss asr
at:None	 st:8005.2333984375	 mt:1860.879150390625
-- Cal ASR Loss asr
at:None	 st:6682.7294921875	 mt:1554.526611328125
-- Cal ASR Loss asr
at:None	 st:7878.0517578125	 mt:1870.240478515625
-- Cal ASR Loss asr
at:None	 st:7962.92529296875	 mt:1791.0301513671875
-- Cal ASR Loss asr
at:None	 st:9107.9033203125	 mt:2342.150634765625
-- Cal ASR Loss asr
at:None	 st:8198.6875	 mt:2051.5458984375
-- Cal ASR Loss asr
at:None	 st:6640.8076171875	 mt:1685.3956298828125
-- Cal ASR Loss asr
at:None	 st:8364.4794921875	 mt:1921.3221435546875
-- Cal ASR Loss asr
at:None	 st:6913.65625	 mt:1337.025146484375
-- Cal ASR Loss asr
at:None	 st:7910.267578125	 mt:1784.1849365234375
-- Cal ASR Loss asr
at:None	 st:8719.181640625	 mt:2275.523681640625
-- Cal ASR Loss asr
at:None	 st:8114.5263671875	 mt:1911.704833984375
-- Cal ASR Loss asr
at:None	 st:5500.4658203125	 mt:1334.900390625
-- Cal ASR Loss asr
at:None	 st:7002.177734375	 mt:1557.69970703125
-- Cal ASR Loss asr
at:None	 st:7619.96044921875	 mt:1805.062255859375
-- Cal ASR Loss asr
at:None	 st:11468.14453125	 mt:3188.3427734375
-- Cal ASR Loss asr
at:None	 st:7743.2265625	 mt:1899.42822265625
-- Cal ASR Loss asr
at:None	 st:7451.2099609375	 mt:1831.6953125
-- Cal ASR Loss asr
at:None	 st:8287.94921875	 mt:2187.7138671875
-- Cal ASR Loss asr
at:None	 st:7838.4677734375	 mt:1940.859619140625
-- Cal ASR Loss asr
at:None	 st:6252.54296875	 mt:1261.4248046875
-- Cal ASR Loss asr
at:None	 st:9356.3974609375	 mt:2369.98388671875
-- Cal ASR Loss asr
at:None	 st:7405.7158203125	 mt:1842.7130126953125
-- Cal ASR Loss asr
at:None	 st:4643.99609375	 mt:1095.6947021484375
-- Cal ASR Loss asr
at:None	 st:4788.61865234375	 mt:1049.25927734375
-- Cal ASR Loss asr
at:None	 st:8202.521484375	 mt:2001.5804443359375
-- Cal ASR Loss asr
at:None	 st:6587.9833984375	 mt:1552.521484375
-- Cal ASR Loss asr
at:None	 st:8079.9208984375	 mt:1880.139892578125
-- Cal ASR Loss asr
at:None	 st:8353.1484375	 mt:2114.917724609375
-- Cal ASR Loss asr
at:None	 st:11311.3779296875	 mt:3157.05322265625
-- Cal ASR Loss asr
at:None	 st:7280.4736328125	 mt:1727.5712890625
-- Cal ASR Loss asr
at:None	 st:7213.095703125	 mt:1791.565673828125
-- Cal ASR Loss asr
at:None	 st:11210.744140625	 mt:3156.798828125
-- Cal ASR Loss asr
at:None	 st:9068.3056640625	 mt:2331.806640625
-- Cal ASR Loss asr
at:None	 st:8088.02978515625	 mt:1934.34521484375
-- Cal ASR Loss asr
at:None	 st:6629.216796875	 mt:1487.684326171875
-- Cal ASR Loss asr
at:None	 st:6915.84326171875	 mt:1601.123779296875
-- Cal ASR Loss asr
at:None	 st:7277.3134765625	 mt:1907.600830078125
-- Cal ASR Loss asr
at:None	 st:7469.091796875	 mt:1696.48388671875
-- Cal ASR Loss asr
at:None	 st:7004.4658203125	 mt:1702.7802734375
-- Cal ASR Loss asr
at:None	 st:8245.130859375	 mt:2023.6156005859375
-- Cal ASR Loss asr
at:None	 st:5516.021484375	 mt:1378.5262451171875
-- Cal ASR Loss asr
at:None	 st:7632.4326171875	 mt:1848.22705078125
-- Cal ASR Loss asr
at:None	 st:5193.31396484375	 mt:1065.2218017578125
-- Cal ASR Loss asr
at:None	 st:8124.4013671875	 mt:2236.0439453125
-- Cal ASR Loss asr
at:None	 st:8493.0615234375	 mt:2173.707763671875
-- Cal ASR Loss asr
at:None	 st:6659.95947265625	 mt:1093.8291015625
-- Cal ASR Loss asr
at:None	 st:4778.671875	 mt:1261.318603515625
-- Cal ASR Loss asr
at:None	 st:8244.884765625	 mt:1880.4305419921875
-- Cal ASR Loss asr
at:None	 st:9212.41015625	 mt:2286.3017578125
-- Cal ASR Loss asr
at:None	 st:7615.8623046875	 mt:1953.89501953125
-- Cal ASR Loss asr
at:None	 st:7822.1826171875	 mt:1940.047607421875
-- Cal ASR Loss asr
at:None	 st:10299.4140625	 mt:2850.765625
-- Cal ASR Loss asr
at:None	 st:5037.44970703125	 mt:1167.317138671875
-- Cal ASR Loss asr
at:None	 st:5199.17724609375	 mt:1372.4508056640625
-- Cal ASR Loss asr
at:None	 st:4831.9931640625	 mt:1165.048828125
-- Cal ASR Loss asr
at:None	 st:5160.98828125	 mt:1262.8861083984375
-- Cal ASR Loss asr
at:None	 st:8129.12548828125	 mt:2040.4459228515625
-- Cal ASR Loss asr
at:None	 st:6107.2373046875	 mt:1484.0987548828125
-- Cal ASR Loss asr
at:None	 st:8736.759765625	 mt:2355.132080078125
-- Cal ASR Loss asr
at:None	 st:7523.7626953125	 mt:1774.6837158203125
-- Cal ASR Loss asr
at:None	 st:7721.654296875	 mt:1722.667724609375
-- Cal ASR Loss asr
at:None	 st:6238.89990234375	 mt:1359.10888671875
-- Cal ASR Loss asr
at:None	 st:7363.140625	 mt:1828.072265625
-- Cal ASR Loss asr
at:None	 st:8431.2197265625	 mt:2187.751953125
-- Cal ASR Loss asr
at:None	 st:4391.359375	 mt:1025.9884033203125
-- Cal ASR Loss asr
at:None	 st:7455.9716796875	 mt:1755.2967529296875
-- Cal ASR Loss asr
at:None	 st:7525.5205078125	 mt:1988.4415283203125
-- Cal ASR Loss asr
at:None	 st:7979.1767578125	 mt:2053.655517578125
-- Cal ASR Loss asr
at:None	 st:7473.03076171875	 mt:1625.38818359375
-- Cal ASR Loss asr
at:None	 st:8396.70703125	 mt:1977.23486328125
-- Cal ASR Loss asr
at:None	 st:10901.4130859375	 mt:3059.9462890625
-- Cal ASR Loss asr
at:None	 st:5788.3798828125	 mt:1179.61962890625
-- Cal ASR Loss at:None	 st:6278.8330078125	 mt:1545.4344482421875
-- Cal ASR Loss asr
at:None	 st:8781.9248046875	 mt:2023.764892578125
-- Cal ASR Loss asr
at:None	 st:7394.263671875	 mt:1636.1334228515625
-- Cal ASR Loss asr
at:None	 st:7751.9033203125	 mt:1740.698974609375
-- Cal ASR Loss asr
at:None	 st:9074.6875	 mt:2306.85595703125
-- Cal ASR Loss asr
at:None	 st:6742.7197265625	 mt:1486.5216064453125
-- Cal ASR Loss asr
at:None	 st:5424.51708984375	 mt:1189.677001953125
-- Cal ASR Loss asr
at:None	 st:9856.923828125	 mt:2520.0478515625
-- Cal ASR Loss asr
at:None	 st:7315.2490234375	 mt:1702.6544189453125
-- Cal ASR Loss asr
at:None	 st:6186.3291015625	 mt:1428.1181640625
-- Cal ASR Loss asr
at:None	 st:6296.1298828125	 mt:1537.964599609375
-- Cal ASR Loss asr
at:None	 st:8515.0263671875	 mt:2141.104736328125
-- Cal ASR Loss asr
at:None	 st:7791.49169921875	 mt:1933.579345703125
-- Cal ASR Loss asr
at:None	 st:6427.576171875	 mt:1623.52001953125
-- Cal ASR Loss asr
at:None	 st:8305.74609375	 mt:2240.82861328125
-- Cal ASR Loss asr
at:None	 st:7783.958984375	 mt:1821.7763671875
-- Cal ASR Loss asr
at:None	 st:5710.66455078125	 mt:1207.0147705078125
-- Cal ASR Loss asr
at:None	 st:8988.171875	 mt:2190.647216796875
-- Cal ASR Loss asr
at:None	 st:5466.13818359375	 mt:1295.005859375
-- Cal ASR Loss asr
at:None	 st:8463.96875	 mt:2161.16259765625
-- Cal ASR Loss asr
at:None	 st:7436.9130859375	 mt:1790.4580078125
-- Cal ASR Loss asr
at:None	 st:6363.861328125	 mt:1505.8076171875
-- Cal ASR Loss asr
at:None	 st:4548.16748046875	 mt:1065.3675537109375
-- Cal ASR Loss asr
at:None	 st:7599.80859375	 mt:1783.976806640625
-- Cal ASR Loss asr
at:None	 st:5616.353515625	 mt:1324.8067626953125
-- Cal ASR Loss asr
at:None	 st:8236.7958984375	 mt:1821.873046875
-- Cal ASR Loss asr
at:None	 st:6858.90234375	 mt:1615.9796142578125
-- Cal ASR Loss asr
at:None	 st:4544.677734375	 mt:1046.3013916015625
-- Cal ASR Loss asr
at:None	 st:4947.58984375	 mt:1189.580078125
-- Cal ASR Loss asr
at:None	 st:4750.6708984375	 mt:1131.4666748046875
-- Cal ASR Loss asr
at:None	 st:4862.26220703125	 mt:1093.082275390625
-- Cal ASR Loss asr
at:None	 st:5039.0576171875	 mt:1211.129638671875
-- Cal ASR Loss asr
at:None	 st:8452.6083984375	 mt:2124.10546875
-- Cal ASR Loss asr
at:None	 st:7058.4521484375	 mt:1584.30615234375
-- Cal ASR Loss asr
at:None	 st:8261.8955078125	 mt:2008.5369873046875
-- Cal ASR Loss asr
at:None	 st:9525.095703125	 mt:2372.985595703125
-- Cal ASR Loss asr
at:None	 st:9197.53515625	 mt:2425.5166015625
-- Cal ASR Loss asr
at:None	 st:6361.849609375	 mt:1634.3251953125
-- Cal ASR Loss asr
at:None	 st:9427.2880859375	 mt:2449.76025390625
-- Cal ASR Loss asr
at:None	 st:11022.369140625	 mt:3017.5517578125
-- Cal ASR Loss asr
at:None	 st:5098.9462890625	 mt:1215.3350830078125
-- Cal ASR Loss asr
at:None	 st:5868.546875	 mt:1531.6402587890625
-- Cal ASR Loss asr
at:None	 st:6241.31689453125	 mt:1480.7216796875
-- Cal ASR Loss asr
at:None	 st:10587.873046875	 mt:3000.300537109375
-- Cal ASR Loss asr
at:None	 st:5970.3193359375	 mt:1415.569580078125
-- Cal ASR Loss asr
at:None	 st:7702.787109375	 mt:1855.82421875
-- Cal ASR Loss asr
at:None	 st:6344.3828125	 mt:1446.234375
-- Cal ASR Loss asr
at:None	 st:5623.4775390625	 mt:1489.3125
-- Cal ASR Loss asr
at:None	 st:6960.31640625	 mt:1346.83740234375
-- Cal ASR Loss asr
at:None	 st:4562.03173828125	 mt:1091.85302734375
-- Cal ASR Loss asr
at:None	 st:7526.6650390625	 mt:1640.508544921875
-- Cal ASR Loss asr
at:None	 st:5300.73583984375	 mt:1098.5496826171875
-- Cal ASR Loss asr
at:None	 st:8988.181640625	 mt:2390.095458984375
-- Cal ASR Loss asr
at:None	 st:7425.20751953125	 mt:1755.322265625
-- Cal ASR Loss asr
at:None	 st:11958.8984375	 mt:3315.05126953125
-- Cal ASR Loss asr
at:None	 st:7290.4208984375	 mt:1724.283935546875
-- Cal ASR Loss asr
at:None	 st:6441.8876953125	 mt:1502.3795166015625
-- Cal ASR Loss asr
at:None	 st:8815.5625	 mt:2234.71435546875
-- Cal ASR Loss asr
at:None	 st:8055.09326171875	 mt:1730.39599609375
-- Cal ASR Loss asr
at:None	 st:8745.88671875	 mt:2177.81689453125
-- Cal ASR Loss asr
at:None	 st:7639.896484375	 mt:1990.361572265625
-- Cal ASR Loss asr
at:None	 st:7270.97265625	 mt:1830.1414794921875
-- Cal ASR Loss asr
at:None	 st:5011.70263671875	 mt:1063.8018798828125
-- Cal ASR Loss asr
at:None	 st:6818.923828125	 mt:1844.2587890625
-- Cal ASR Loss asr
at:None	 st:8935.5400390625	 mt:2331.82080078125
-- Cal ASR Loss asr
at:None	 st:9689.6240234375	 mt:2677.8046875
-- Cal ASR Loss asr
at:None	 st:7806.4482421875	 mt:1971.6785888671875
-- Cal ASR Loss asr
at:None	 st:8977.623046875	 mt:2189.52734375
-- Cal ASR Loss asr
at:None	 st:7109.31591796875	 mt:1756.4058837890625
-- Cal ASR Loss asr
at:None	 st:8485.4306640625	 mt:2043.160888671875
-- Cal ASR Loss asr
at:None	 st:5722.60400390625	 mt:1384.37158203125
-- Cal ASR Loss asr
at:None	 st:7182.85546875	 mt:1815.5302734375
-- Cal ASR Loss asr
at:None	 st:5568.9375	 mt:1426.6361083984375
-- Cal ASR Loss asr
at:None	 st:6352.138671875	 mt:1451.876953125
-- Cal ASR Loss asr
at:None	 st:8940.73046875	 mt:2227.5546875
-- Cal ASR Loss asr
at:None	 st:7670.9326171875	 mt:1849.59326171875
-- Cal ASR Loss asr
at:None	 st:4386.98583984375	 mt:1036.5032958984375
-- Cal ASR Loss asr
at:None	 st:8500.841796875	 mt:2242.339111328125
-- Cal ASR Loss asr
at:None	 st:7686.9873046875	 mt:1736.636474609375
-- Cal ASR Loss asr
at:None	 st:11331.7958984375	 mt:3226.5947265625
-- Cal ASR Loss asr
at:None	 st:8350.412109375	 mt:2050.54443359375
-- Cal ASR Loss asr
at:None	 st:9176.296875	 mt:2157.180419921875
-- Cal ASR Loss asr
at:None	 st:5683.89013671875	 mt:1307.8173828125
-- Cal ASR Loss asr
at:None	 st:11075.294921875	 mt:3101.912109375
-- Cal ASR Loss asr
at:None	 st:4885.884765625	 mt:1155.962890625
-- Cal ASR Loss asr
at:None	 st:11458.994140625	 mt:3426.80615234375
-- Cal ASR Loss asr
at:None	 st:4889.6083984375	 mt:1343.279052734375
-- Cal ASR Loss asr
at:None	 st:7607.8271484375	 mt:1859.673583984375
-- Cal ASR Loss asr
at:None	 st:5779.107421875	 mt:1538.497802734375
-- Cal ASR Loss asr
at:None	 st:8051.47314453125	 mt:1918.3963623046875
-- Cal ASR Loss asr
at:None	 st:6747.83203125	 mt:1629.4993896484375
-- Cal ASR Loss asr
at:None	 st:14519.041015625	 mt:4654.6806640625
-- Cal ASR Loss asr
at:None	 st:7320.68359375	 mt:1832.048583984375
-- Cal ASR Loss asr
at:None	 st:6976.65869140625	 mt:1671.825439453125
-- Cal ASR Loss asr
at:None	 st:4874.31982421875	 mt:1204.6646728515625
-- Cal ASR Loss asr
at:None	 st:6631.3349609375	 mt:1619.6597900390625
-- Cal ASR Loss asr
at:None	 st:6574.708984375	 mt:1465.019287109375
-- Cal ASR Loss asr
at:None	 st:8821.421875	 mt:2118.730224609375
-- Cal ASR Loss asr
at:None	 st:6594.49853515625	 mt:1615.2626953125
-- Cal ASR Loss asr
at:None	 st:6202.0390625	 mt:1537.5928955078125
-- Cal ASR Loss asr
at:None	 st:7860.8017578125	 mt:1987.73486328125
-- Cal ASR Loss asr
at:None	 st:7873.9365234375	 mt:1798.8974609375
-- Cal ASR Loss asr
at:None	 st:8658.6064453125	 mt:2104.7216796875
-- Cal ASR Loss asr
at:None	 st:6373.69140625	 mt:1471.2061767578125
-- Cal ASR Loss asr
at:None	 st:8135.1123046875	 mt:1886.001953125
-- Cal ASR Loss asr
at:None	 st:8010.654296875	 mt:1898.4500732421875
-- Cal ASR Loss asr
at:None	 st:7321.666015625	 mt:1872.0501708984375
-- Cal ASR Loss asr
at:None	 st:7896.6640625	 mt:2006.6939697265625
-- Cal ASR Loss asr
at:None	 st:6622.060546875	 mt:1512.7255859375
-- Cal ASR Loss asr
at:None	 st:9223.87890625	 mt:2449.500244140625
-- Cal ASR Loss asr
at:None	 st:5056.11279296875	 mt:1292.0076904296875
-- Cal ASR Loss asr
at:None	 st:6132.2041015625	 mt:1631.349609375
-- Cal ASR Loss asr
at:None	 st:5058.4541015625	 mt:1162.7801513671875
-- Cal ASR Loss asr
at:None	 st:8626.0703125	 mt:2210.83544921875
-- Cal ASR Loss asr
at:None	 st:8198.86328125	 mt:2020.813720703125
-- Cal ASR Loss asr
at:None	 st:5650.00537109375	 mt:1583.772705078125
-- Cal ASR Loss asr
at:None	 st:7364.9609375	 mt:1574.0572509765625
-- Cal ASR Loss asr
at:None	 st:6652.5625	 mt:1486.4788818359375
-- Cal ASR Loss asr
at:None	 st:6021.74169921875	 mt:1433.82421875
-- Cal ASR Loss asr
at:None	 st:4872.9306640625	 mt:1129.0535888671875
-- Cal ASR Loss asr
at:None	 st:9936.916015625	 mt:2702.06005859375
-- Cal ASR Loss asr
at:None	 st:5390.67626953125	 mt:1220.478271484375
-- Cal ASR Loss asr
at:None	 st:5437.126953125	 mt:1378.9951171875
-- Cal ASR Loss asr
at:None	 st:8519.375	 mt:2089.604736328125
-- Cal ASR Loss asr
at:None	 st:8792.3935546875	 mt:2099.743408203125
-- Cal ASR Loss asr
at:None	 st:9557.2724609375	 mt:2610.489013671875
-- Cal ASR Loss asr
at:None	 st:8114.271484375	 mt:1993.2318115234375
-- Cal ASR Loss asr
at:None	 st:7592.2734375	 mt:1782.6602783203125
-- Cal ASR Loss asr
at:None	 st:5134.595703125	 mt:1183.19921875
-- Cal ASR Loss asr
at:None	 st:4514.98193359375	 mt:1048.679443359375
-- Cal ASR Loss asr
at:None	 st:10639.07421875	 mt:2942.078369140625
-- Cal ASR Loss asr
at:None	 st:8689.29296875	 mt:1942.2244873046875
-- Cal ASR Loss asr
at:None	 st:7822.4326171875	 mt:2035.427490234375
-- Cal ASR Loss asr
at:None	 st:9189.24609375	 mt:2316.78271484375
-- Cal ASR Loss asr
at:None	 st:5652.02978515625	 mt:1240.4052734375
-- Cal ASR Loss asr
at:None	 st:6833.369140625	 mt:1544.355712890625
-- Cal ASR Loss asr
at:None	 st:7797.6103515625	 mt:1793.5384521484375
-- Cal ASR Loss asr
at:None	 st:9126.5498046875	 mt:2281.529541015625
-- Cal ASR Loss asr
at:None	 st:7630.8251953125	 mt:1729.7283935546875
-- Cal ASR Loss asr
at:None	 st:6272.9052734375	 mt:1379.3963623046875
-- Cal ASR Loss asr
at:None	 st:7256.626953125	 mt:1608.3380126953125
-- Cal ASR Loss asr
at:None	 st:6974.32080078125	 mt:1629.44140625
-- Cal ASR Loss asr
at:None	 st:12053.5546875	 mt:3435.2138671875
-- Cal ASR Loss asr
at:None	 st:5473.5712890625	 mt:1150.88134765625
-- Cal ASR Loss asr
at:None	 st:10357.5625	 mt:2791.546142578125
-- Cal ASR Loss asr
at:None	 st:8489.6943359375	 mt:2038.6357421875
-- Cal ASR Loss asr
at:None	 st:7684.162109375	 mt:1634.80224609375
-- Cal ASR Loss asr
at:None	 st:4919.59912109375	 mt:1153.748046875
-- Cal ASR Loss asr
at:None	 st:8938.9033203125	 mt:2321.45166015625
-- Cal ASR Loss asr
at:None	 st:8643.345703125	 mt:2179.16796875
-- Cal ASR Loss asr
at:None	 st:7386.12890625	 mt:1585.23291015625
-- Cal ASR Loss asr
at:None	 st:7478.486328125	 mt:1813.265625
-- Cal ASR Loss asr
at:None	 st:7563.365234375	 mt:1790.3291015625
-- Cal ASR Loss asr
at:None	 st:8340.1103515625	 mt:2062.724609375
-- Cal ASR Loss asr
at:None	 st:6933.50439453125	 mt:1673.878662109375
-- Cal ASR Loss asr
at:None	 st:7164.2978515625	 mt:1682.912353515625
-- Cal ASR Loss asr
at:None	 st:8325.873046875	 mt:2009.6690673828125
-- Cal ASR Loss asr
at:None	 st:6966.837890625	 mt:1655.042236328125
-- Cal ASR Loss asr
at:None	 st:4596.431640625	 mt:1150.828369140625
-- Cal ASR Loss asr
at:None	 st:6967.65625	 mt:1660.89990234375
-- Cal ASR Loss asr
at:None	 st:4269.203125	 mt:1016.5728759765625
-- Cal ASR Loss asr
at:None	 st:4818.244140625	 mt:1218.1165771484375
-- Cal ASR Loss asr
at:None	 st:6877.83984375	 mt:1325.3349609375
-- Cal ASR Loss asr
at:None	 st:12882.662109375	 mt:3842.3486328125
-- Cal ASR Loss asr
at:None	 st:11726.103515625	 mt:3245.815185546875
-- Cal ASR Loss asr
at:None	 st:7424.5146484375	 mt:1796.7672119140625
-- Cal ASR Loss asr
at:None	 st:8283.92578125	 mt:2000.4703369140625
-- Cal ASR Loss asr
at:None	 st:6182.04052734375	 mt:1426.4556884765625
-- Cal ASR Loss asr
at:None	 st:6346.37255859375	 mt:1476.966552734375
-- Cal ASR Loss asr
at:None	 st:8047.46484375	 mt:2001.1092529296875
-- Cal ASR Loss asr
at:None	 st:7026.02294921875	 mt:1533.7149658203125
-- Cal ASR Loss asr
at:None	 st:8708.7890625	 mt:2223.5673828125
-- Cal ASR Loss asr
at:None	 st:7457.439453125	 mt:1682.9730224609375
-- Cal ASR Loss asr
at:None	 st:7268.8583984375	 mt:1763.4996337890625
-- Cal ASR Loss asr
at:None	 st:8602.5380859375	 mt:2193.67529296875
-- Cal ASR Loss asr
at:None	 st:7320.6083984375	 mt:1583.1806640625
-- Cal ASR Loss asr
at:None	 st:8533.9560546875	 mt:2129.68017578125
-- Cal ASR Loss asr
at:None	 st:8886.8984375	 mt:2207.42041015625
-- Cal ASR Loss asr
at:None	 st:4459.025390625	 mt:1113.4669189453125
-- Cal ASR Loss asr
at:None	 st:9408.099609375	 mt:2382.061279296875
-- Cal ASR Loss asr
at:None	 st:4919.1044921875	 mt:1192.5306396484375
-- Cal ASR Loss asr
at:None	 st:6155.91796875	 mt:1528.018310546875
-- Cal ASR Loss asr
at:None	 st:4190.125	 mt:933.7854614257812
-- Cal ASR Loss asr
at:None	 st:10541.447265625	 mt:2923.46435546875
-- Cal ASR Loss asr
at:None	 st:8813.6806640625	 mt:2213.8515625
-- Cal ASR Loss asr
at:None	 st:7955.5048828125	 mt:2132.544921875
-- Cal ASR Loss asr
at:None	 st:8981.892578125	 mt:2209.05615234375
-- Cal ASR Loss asr
at:None	 st:7772.26025390625	 mt:1815.999755859375
-- Cal ASR Loss asr
at:None	 st:8420.9150390625	 mt:2231.698486328125
-- Cal ASR Loss asr
at:None	 st:6741.9931640625	 mt:1586.453125
-- Cal ASR Loss asr
at:None	 st:8195.775390625	 mt:1900.979248046875
-- Cal ASR Loss asr
at:None	 st:7788.31494140625	 mt:1793.903564453125
-- Cal ASR Loss asr
at:None	 st:8100.52197265625	 mt:2144.1083984375
-- Cal ASR Loss asr
at:None	 st:10521.32421875	 mt:2924.380859375
-- Cal ASR Loss asr
at:None	 st:7831.60595703125	 mt:2073.943359375
-- Cal ASR Loss asr
at:None	 st:6301.072265625	 mt:1133.115478515625
-- Cal ASR Loss asr
at:None	 st:9079.158203125	 mt:2489.129150390625
-- Cal ASR Loss asr
at:None	 st:9490.326171875	 mt:2495.1318359375
-- Cal ASR Loss asr
at:None	 st:5900.294921875	 mt:1473.596923828125
-- Cal ASR Loss asr
at:None	 st:7745.3671875	 mt:1837.349853515625
-- Cal ASR Loss asr
at:None	 st:9364.552734375	 mt:2380.5478515625
-- Cal ASR Loss asr
at:None	 st:7423.3310546875	 mt:1615.2298583984375
-- Cal ASR Loss asr
at:None	 st:6488.23486328125	 mt:1597.1654052734375
-- Cal ASR Loss asr
at:None	 st:8976.6142578125	 mt:2304.927490234375
-- Cal ASR Loss asr
at:None	 st:8893.861328125	 mt:2397.16015625
-- Cal ASR Loss asr
at:None	 st:8250.201171875	 mt:2060.6513671875
-- Cal ASR Loss asr
at:None	 st:7754.87109375	 mt:1786.9393310546875
-- Cal ASR Loss asr
at:None	 st:4959.951171875	 mt:1200.885986328125
-- Cal ASR Loss asr
at:None	 st:4617.216796875	 mt:1130.3907470703125
-- Cal ASR Loss asr
at:None	 st:8206.666015625	 mt:1996.8699951171875
-- Cal ASR Loss asr
at:None	 st:4814.9443359375	 mt:1214.33154296875
-- Cal ASR Loss asr
at:None	 st:7266.82958984375	 mt:1854.924072265625
-- Cal ASR Loss asr
at:None	 st:8050.26953125	 mt:1941.406982421875
-- Cal ASR Loss asr
at:None	 st:7126.2275390625	 mt:1539.08935546875
-- Cal ASR Loss asr
at:None	 st:4926.8310546875	 mt:1009.7083740234375
-- Cal ASR Loss asr
at:None	 st:4384.55224609375	 mt:1019.469482421875
-- Cal ASR Loss asr
at:None	 st:9879.07421875	 mt:2571.87646484375
-- Cal ASR Loss asr
at:None	 st:4666.28173828125	 mt:1053.2734375
-- Cal ASR Loss asr
at:None	 st:5849.8837890625	 mt:1409.0301513671875
-- Cal ASR Loss asr
at:None	 st:4628.87353515625	 mt:1175.8525390625
-- Cal ASR Loss asr
at:None	 st:7585.8427734375	 mt:1770.5306396484375
-- Cal ASR Loss asr
at:None	 st:10004.291015625	 mt:2594.6796875
-- Cal ASR Loss asr
at:None	 st:7030.19384765625	 mt:1734.523193359375
-- Cal ASR Loss asr
at:None	 st:8527.8486328125	 mt:1954.1611328125
-- Cal ASR Loss asr
at:None	 st:8264.6376953125	 mt:2193.1640625
-- Cal ASR Loss asr
at:None	 st:5591.16015625	 mt:1445.88671875
-- Cal ASR Loss asr
at:None	 st:5615.87451171875	 mt:1394.4732666015625
-- Cal ASR Loss asr
at:None	 st:8552.859375	 mt:2192.3046875
-- Cal ASR Loss asr
at:None	 st:5888.53564453125	 mt:1363.3887939453125
-- Cal ASR Loss asr
at:None	 st:7917.2958984375	 mt:2024.1715087890625
-- Cal ASR Loss asr
at:None	 st:6682.96484375	 mt:1587.261962890625
-- Cal ASR Loss asr
at:None	 st:6942.9443359375	 mt:1527.5438232421875
-- Cal ASR Loss asr
at:None	 st:6930.853515625	 mt:1821.988037109375
-- Cal ASR Loss asr
at:None	 st:5225.8232421875	 mt:1307.0225830078125
-- Cal ASR Loss asr
at:None	 st:6427.59912109375	 mt:1395.780029296875
-- Cal ASR Loss asr
at:None	 st:7448.2822265625	 mt:1957.026611328125
-- Cal ASR Loss asr
at:None	 st:6921.20751953125	 mt:1838.7357177734375
-- Cal ASR Loss asr
at:None	 st:5814.38427734375	 mt:1463.611572265625
-- Cal ASR Loss asr
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 2 terminated with signal SIGKILL
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 131 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-07-24 20:31:42 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:18415
2023-07-24 20:31:42 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:18415
2023-07-24 20:31:42 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:18415
2023-07-24 20:31:42 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:18415
2023-07-24 20:31:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-24 20:31:42 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:18415
2023-07-24 20:31:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-24 20:31:42 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:18415
2023-07-24 20:31:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-24 20:31:42 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:18415
2023-07-24 20:31:42 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:18415
2023-07-24 20:31:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-24 20:31:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-24 20:31:43 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-24 20:31:43 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-24 20:31:43 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-24 20:31:43 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:31:43 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-07-24 20:31:43 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:31:43 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-07-24 20:31:43 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:31:43 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-07-24 20:31:43 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:31:43 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-07-24 20:31:43 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:31:43 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-07-24 20:31:43 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:31:43 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:31:43 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:31:43 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-07-24 20:31:43 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-07-24 20:31:43 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-07-24 20:31:47 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18415', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-07-24 20:31:47 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-07-24 20:31:47 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-07-24 20:31:47 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-24 20:31:47 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 1.0
2023-07-24 20:31:47 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-24 20:31:52 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-07-24 20:31:52 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-24 20:31:52 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-24 20:31:53 | INFO | root | load pretrained hubert
2023-07-24 20:31:54 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-24 20:31:56 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-24 20:31:57 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-24 20:31:57 | INFO | root | share the sematic adapter and textual encoder
2023-07-24 20:31:57 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-24 20:31:57 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-07-24 20:31:57 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-24 20:31:57 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-07-24 20:31:57 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-24 20:31:57 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-24 20:31:57 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-24 20:31:57 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-24 20:31:57 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-24 20:31:57 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-24 20:31:57 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-24 20:32:05 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-24 20:32:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-24 20:32:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-24 20:32:05 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:32:05 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:32:05 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:32:05 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:32:05 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:32:05 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:32:05 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:32:05 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:32:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-24 20:32:05 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-24 20:32:05 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-24 20:32:05 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_last.pt
2023-07-24 20:32:05 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_last.pt
2023-07-24 20:32:05 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-24 20:32:05 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-24 20:32:05 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-24 20:32:06 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-24 20:32:07 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-24 20:32:08 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-24 20:32:10 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-24 20:33:18 | INFO | fairseq.optim.adam | using FusedAdam
2023-07-24 20:33:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-24 20:33:18 | INFO | fairseq.trainer | begin training epoch 1
2023-07-24 20:33:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-24 20:34:37 | INFO | train_inner | epoch 001:    100 / 1474 loss=19.616, trans_loss=5.599, nll_loss=4.164, w2v_ctc_loss=23.048, task_loss=0, contrastive_loss=3.325, total=4207.04, n_correct=209.16, ppl=17.93, accuracy=4.972, wps=20104.4, ups=1.6, wpb=12551.1, bsz=471.4, num_updates=100, lr=4.098e-06, gnorm=0.913, clip=0, loss_scale=128, train_wall=70, gb_free=19.5, wall=151
2023-07-24 20:35:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-24 20:35:40 | INFO | train_inner | epoch 001:    201 / 1474 loss=17.415, trans_loss=5.473, nll_loss=4.06, w2v_ctc_loss=19.847, task_loss=0, contrastive_loss=3.283, total=4121.36, n_correct=224.4, ppl=16.67, accuracy=5.445, wps=19515.7, ups=1.59, wpb=12305.5, bsz=461.4, num_updates=200, lr=8.096e-06, gnorm=3.718, clip=0, loss_scale=64, train_wall=63, gb_free=19.2, wall=214
2023-07-24 20:36:41 | INFO | train_inner | epoch 001:    301 / 1474 loss=10.342, trans_loss=5.501, nll_loss=4.147, w2v_ctc_loss=8.993, task_loss=0, contrastive_loss=3.204, total=4079.62, n_correct=201.83, ppl=17.72, accuracy=4.947, wps=19951.2, ups=1.64, wpb=12186.7, bsz=438.2, num_updates=300, lr=1.2094e-05, gnorm=4.711, clip=0, loss_scale=64, train_wall=61, gb_free=19.9, wall=276
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 131 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-07-24 20:37:17 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:10016
2023-07-24 20:37:17 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:10016
2023-07-24 20:37:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-24 20:37:17 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:10016
2023-07-24 20:37:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-24 20:37:17 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:10016
2023-07-24 20:37:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-24 20:37:17 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:10016
2023-07-24 20:37:17 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:10016
2023-07-24 20:37:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-24 20:37:17 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:10016
2023-07-24 20:37:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-24 20:37:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-24 20:37:17 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:10016
2023-07-24 20:37:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-24 20:37:17 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:37:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-24 20:37:17 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-07-24 20:37:17 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:37:17 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-07-24 20:37:17 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:37:17 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:37:17 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-07-24 20:37:17 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-07-24 20:37:17 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:37:17 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:37:17 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-07-24 20:37:17 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-07-24 20:37:17 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:37:17 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-07-24 20:37:17 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-24 20:37:17 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-07-24 20:37:21 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10016', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-07-24 20:37:21 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-07-24 20:37:21 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-07-24 20:37:21 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-24 20:37:21 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 1.0
2023-07-24 20:37:21 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-24 20:37:26 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-07-24 20:37:26 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-24 20:37:26 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-24 20:37:28 | INFO | root | load pretrained hubert
2023-07-24 20:37:31 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-24 20:37:31 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-24 20:37:32 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-24 20:37:32 | INFO | root | share the sematic adapter and textual encoder
2023-07-24 20:37:32 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-24 20:37:32 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-07-24 20:37:32 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-24 20:37:32 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-07-24 20:37:32 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-24 20:37:32 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-24 20:37:32 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-24 20:37:32 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-24 20:37:32 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-24 20:37:32 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-24 20:37:33 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-24 20:37:33 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-24 20:37:33 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-24 20:37:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-24 20:37:33 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:37:33 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:37:33 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:37:33 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:37:33 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:37:33 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:37:33 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:37:33 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-24 20:37:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-24 20:37:33 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-24 20:37:33 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-24 20:37:33 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_last.pt
2023-07-24 20:37:33 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_last.pt
2023-07-24 20:37:33 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-24 20:37:33 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-24 20:37:33 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-24 20:37:33 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-24 20:37:34 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-24 20:37:36 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-24 20:37:38 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-24 20:38:47 | INFO | fairseq.optim.adam | using FusedAdam
2023-07-24 20:38:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-24 20:38:47 | INFO | fairseq.trainer | begin training epoch 1
2023-07-24 20:38:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-24 20:40:06 | INFO | train_inner | epoch 001:    100 / 1474 loss=19.616, trans_loss=5.599, nll_loss=4.164, w2v_ctc_loss=23.048, task_loss=0, contrastive_loss=3.325, total=4207.04, n_correct=209.21, ppl=17.93, accuracy=4.973, wps=20195.7, ups=1.61, wpb=12551.1, bsz=471.4, num_updates=100, lr=4.098e-06, gnorm=0.913, clip=0, loss_scale=128, train_wall=70, gb_free=19.5, wall=153
2023-07-24 20:41:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-24 20:41:09 | INFO | train_inner | epoch 001:    201 / 1474 loss=17.416, trans_loss=5.474, nll_loss=4.061, w2v_ctc_loss=19.847, task_loss=0, contrastive_loss=3.283, total=4121.36, n_correct=224.26, ppl=16.69, accuracy=5.441, wps=19535.4, ups=1.59, wpb=12305.5, bsz=461.4, num_updates=200, lr=8.096e-06, gnorm=3.718, clip=0, loss_scale=64, train_wall=63, gb_free=19.2, wall=216
2023-07-24 20:42:10 | INFO | train_inner | epoch 001:    301 / 1474 loss=10.343, trans_loss=5.501, nll_loss=4.148, w2v_ctc_loss=8.993, task_loss=0, contrastive_loss=3.204, total=4079.62, n_correct=201.54, ppl=17.73, accuracy=4.94, wps=19898, ups=1.63, wpb=12186.7, bsz=438.2, num_updates=300, lr=1.2094e-05, gnorm=4.711, clip=0, loss_scale=64, train_wall=61, gb_free=19.9, wall=277
2023-07-24 20:43:11 | INFO | train_inner | epoch 001:    401 / 1474 loss=9.07, trans_loss=5.524, nll_loss=4.199, w2v_ctc_loss=6.983, task_loss=0, contrastive_loss=3.236, total=4174.14, n_correct=191.5, ppl=18.36, accuracy=4.588, wps=20261.1, ups=1.63, wpb=12463.5, bsz=460.4, num_updates=400, lr=1.6092e-05, gnorm=3.015, clip=0, loss_scale=64, train_wall=61, gb_free=18.9, wall=338
2023-07-24 20:44:13 | INFO | train_inner | epoch 001:    501 / 1474 loss=8.63, trans_loss=5.497, nll_loss=4.181, w2v_ctc_loss=6.334, task_loss=0, contrastive_loss=3.231, total=4176.18, n_correct=185.15, ppl=18.13, accuracy=4.433, wps=20411.2, ups=1.64, wpb=12479.7, bsz=477.4, num_updates=500, lr=2.009e-05, gnorm=1.448, clip=0, loss_scale=64, train_wall=61, gb_free=19.2, wall=400
2023-07-24 20:45:16 | INFO | train_inner | epoch 001:    601 / 1474 loss=8.377, trans_loss=5.525, nll_loss=4.216, w2v_ctc_loss=5.959, task_loss=0, contrastive_loss=3.287, total=4147.79, n_correct=182.22, ppl=18.59, accuracy=4.393, wps=19581.1, ups=1.58, wpb=12371.6, bsz=484.2, num_updates=600, lr=2.4088e-05, gnorm=0.751, clip=0, loss_scale=64, train_wall=63, gb_free=19, wall=463
2023-07-24 20:46:17 | INFO | train_inner | epoch 001:    701 / 1474 loss=8.202, trans_loss=5.521, nll_loss=4.216, w2v_ctc_loss=5.831, task_loss=0, contrastive_loss=3.037, total=4152.1, n_correct=196.42, ppl=18.59, accuracy=4.731, wps=20241.8, ups=1.63, wpb=12395.5, bsz=456.2, num_updates=700, lr=2.8086e-05, gnorm=0.564, clip=0, loss_scale=64, train_wall=61, gb_free=19.5, wall=524
2023-07-24 20:47:18 | INFO | train_inner | epoch 001:    801 / 1474 loss=7.926, trans_loss=5.463, nll_loss=4.155, w2v_ctc_loss=5.602, task_loss=0, contrastive_loss=2.945, total=4123.83, n_correct=238.97, ppl=17.81, accuracy=5.795, wps=20067.8, ups=1.63, wpb=12306.1, bsz=464.1, num_updates=800, lr=3.2084e-05, gnorm=0.816, clip=0, loss_scale=64, train_wall=61, gb_free=19.2, wall=585
2023-07-24 20:48:19 | INFO | train_inner | epoch 001:    901 / 1474 loss=7.651, trans_loss=5.425, nll_loss=4.12, w2v_ctc_loss=5.407, task_loss=0, contrastive_loss=2.704, total=4163.61, n_correct=264.27, ppl=17.39, accuracy=6.347, wps=20367.4, ups=1.64, wpb=12433.9, bsz=457.1, num_updates=900, lr=3.6082e-05, gnorm=1.33, clip=0, loss_scale=64, train_wall=61, gb_free=18.9, wall=646
2023-07-24 20:49:22 | INFO | train_inner | epoch 001:   1001 / 1474 loss=7.384, trans_loss=5.406, nll_loss=4.106, w2v_ctc_loss=5.19, task_loss=0, contrastive_loss=2.55, total=4135.34, n_correct=286.17, ppl=17.22, accuracy=6.92, wps=19804.5, ups=1.6, wpb=12353.2, bsz=456.8, num_updates=1000, lr=4.008e-05, gnorm=1.464, clip=0, loss_scale=64, train_wall=62, gb_free=19, wall=709
2023-07-24 20:50:23 | INFO | train_inner | epoch 001:   1101 / 1474 loss=7.108, trans_loss=5.395, nll_loss=4.096, w2v_ctc_loss=4.986, task_loss=0, contrastive_loss=2.328, total=4147.38, n_correct=309.55, ppl=17.1, accuracy=7.464, wps=20137.3, ups=1.63, wpb=12367, bsz=454.9, num_updates=1100, lr=4.4078e-05, gnorm=1.7, clip=0, loss_scale=64, train_wall=61, gb_free=18.9, wall=770
2023-07-24 20:51:24 | INFO | train_inner | epoch 001:   1201 / 1474 loss=6.87, trans_loss=5.371, nll_loss=4.073, w2v_ctc_loss=4.812, task_loss=0, contrastive_loss=2.118, total=4139.9, n_correct=316.7, ppl=16.83, accuracy=7.65, wps=20331, ups=1.64, wpb=12366.5, bsz=440.1, num_updates=1200, lr=4.8076e-05, gnorm=1.745, clip=0, loss_scale=64, train_wall=60, gb_free=19, wall=831
2023-07-24 20:52:25 | INFO | train_inner | epoch 001:   1301 / 1474 loss=6.655, trans_loss=5.371, nll_loss=4.075, w2v_ctc_loss=4.614, task_loss=0, contrastive_loss=1.939, total=4046.58, n_correct=320.63, ppl=16.85, accuracy=7.923, wps=19882.9, ups=1.65, wpb=12081.6, bsz=439.3, num_updates=1300, lr=5.2074e-05, gnorm=1.844, clip=0, loss_scale=64, train_wall=60, gb_free=19.7, wall=892
2023-07-24 20:53:26 | INFO | train_inner | epoch 001:   1401 / 1474 loss=6.453, trans_loss=5.365, nll_loss=4.07, w2v_ctc_loss=4.413, task_loss=0, contrastive_loss=2.009, total=4133.18, n_correct=329.91, ppl=16.8, accuracy=7.982, wps=20122.2, ups=1.63, wpb=12350, bsz=455, num_updates=1400, lr=5.6072e-05, gnorm=1.756, clip=0, loss_scale=64, train_wall=61, gb_free=19.9, wall=953
2023-07-24 20:54:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-24 20:54:51 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 9.961 | trans_loss 11.004 | nll_loss 10.007 | w2v_ctc_loss 5.796 | task_loss 0 | contrastive_loss 2.358 | total 4003.4 | n_correct 359.1 | ppl 1028.78 | accuracy 8.97 | uer 71.744 | wer 69.666 | raw_wer 69.666 | bleu 0.02 | wps 1201.9 | wpb 4003.4 | bsz 141.8 | num_updates 1473
2023-07-24 20:54:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1473 updates
2023-07-24 20:54:51 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt
2023-07-24 20:54:54 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt
2023-07-24 20:54:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt (epoch 1 @ 1473 updates, score 0.02) (writing took 5.860378734767437 seconds)
2023-07-24 20:54:57 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-07-24 20:54:57 | INFO | train | epoch 001 | loss 9.264 | trans_loss 5.455 | nll_loss 4.131 | w2v_ctc_loss 7.831 | task_loss 0 | contrastive_loss 2.759 | total 4138.36 | n_correct 251.164 | ppl 17.52 | accuracy 6.069 | wps 19094 | ups 1.55 | wpb 12355 | bsz 458.4 | num_updates 1473 | lr 5.89905e-05 | gnorm 1.834 | clip 0 | loss_scale 64 | train_wall 910 | gb_free 19.2 | wall 1044
2023-07-24 20:54:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-24 20:54:57 | INFO | fairseq.trainer | begin training epoch 2
2023-07-24 20:54:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-24 20:55:22 | INFO | train_inner | epoch 002:     27 / 1474 loss=6.267, trans_loss=5.355, nll_loss=4.054, w2v_ctc_loss=4.222, task_loss=0, contrastive_loss=1.858, total=4162.95, n_correct=334.8, ppl=16.61, accuracy=8.042, wps=10716.6, ups=0.86, wpb=12416.8, bsz=470.8, num_updates=1500, lr=6.007e-05, gnorm=1.699, clip=0, loss_scale=64, train_wall=62, gb_free=19.7, wall=1069
2023-07-24 20:56:23 | INFO | train_inner | epoch 002:    127 / 1474 loss=6.089, trans_loss=5.349, nll_loss=4.046, w2v_ctc_loss=4.098, task_loss=0, contrastive_loss=1.649, total=4155.98, n_correct=337.08, ppl=16.51, accuracy=8.111, wps=20324.6, ups=1.64, wpb=12394.8, bsz=451.6, num_updates=1600, lr=6.4068e-05, gnorm=1.676, clip=0, loss_scale=64, train_wall=61, gb_free=18.9, wall=1130
2023-07-24 20:57:24 | INFO | train_inner | epoch 002:    227 / 1474 loss=5.93, trans_loss=5.327, nll_loss=4.023, w2v_ctc_loss=3.902, task_loss=0, contrastive_loss=1.679, total=4179.21, n_correct=347.23, ppl=16.26, accuracy=8.309, wps=20582.7, ups=1.65, wpb=12484.6, bsz=488.9, num_updates=1700, lr=6.8066e-05, gnorm=1.487, clip=0, loss_scale=64, train_wall=60, gb_free=19, wall=1191
2023-07-24 20:58:25 | INFO | train_inner | epoch 002:    327 / 1474 loss=5.758, trans_loss=5.329, nll_loss=4.022, w2v_ctc_loss=3.8, task_loss=0, contrastive_loss=1.391, total=4146.1, n_correct=349.14, ppl=16.25, accuracy=8.421, wps=20216.8, ups=1.63, wpb=12374.1, bsz=447.8, num_updates=1800, lr=7.2064e-05, gnorm=1.467, clip=0, loss_scale=64, train_wall=61, gb_free=18.8, wall=1252
2023-07-24 20:59:26 | INFO | train_inner | epoch 002:    427 / 1474 loss=5.606, trans_loss=5.321, nll_loss=4.017, w2v_ctc_loss=3.699, task_loss=0, contrastive_loss=1.208, total=4037.99, n_correct=342.98, ppl=16.19, accuracy=8.494, wps=19656.4, ups=1.63, wpb=12069.2, bsz=415.4, num_updates=1900, lr=7.6062e-05, gnorm=1.436, clip=0, loss_scale=64, train_wall=61, gb_free=19, wall=1313
2023-07-24 21:00:27 | INFO | train_inner | epoch 002:    527 / 1474 loss=5.505, trans_loss=5.306, nll_loss=3.994, w2v_ctc_loss=3.54, task_loss=0, contrastive_loss=1.309, total=4176.97, n_correct=363.62, ppl=15.94, accuracy=8.705, wps=20418.5, ups=1.64, wpb=12463.5, bsz=468.6, num_updates=2000, lr=8.006e-05, gnorm=1.351, clip=0, loss_scale=64, train_wall=61, gb_free=19.6, wall=1374
2023-07-24 21:00:27 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-24 21:01:06 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 9.298 | trans_loss 10.792 | nll_loss 9.733 | w2v_ctc_loss 4.675 | task_loss 0 | contrastive_loss 1.657 | total 4003.4 | n_correct 407.2 | ppl 850.97 | accuracy 10.171 | uer 61.349 | wer 59.181 | raw_wer 59.181 | bleu 0.05 | wps 1204.6 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_bleu 0.05
2023-07-24 21:01:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-07-24 21:01:06 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_2_2000.pt
2023-07-24 21:01:09 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_2_2000.pt
2023-07-24 21:01:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.05) (writing took 20.258215373381972 seconds)
2023-07-24 21:02:26 | INFO | train_inner | epoch 002:    627 / 1474 loss=5.363, trans_loss=5.294, nll_loss=3.977, w2v_ctc_loss=3.431, task_loss=0, contrastive_loss=1.111, total=4126.49, n_correct=369.61, ppl=15.75, accuracy=8.957, wps=10339.9, ups=0.84, wpb=12314.9, bsz=445.5, num_updates=2100, lr=8.4058e-05, gnorm=1.2, clip=0, loss_scale=64, train_wall=60, gb_free=19.2, wall=1493
2023-07-24 21:03:27 | INFO | train_inner | epoch 002:    727 / 1474 loss=5.295, trans_loss=5.282, nll_loss=3.967, w2v_ctc_loss=3.347, task_loss=0, contrastive_loss=1.21, total=4149.06, n_correct=374.41, ppl=15.64, accuracy=9.024, wps=20401.5, ups=1.65, wpb=12386.4, bsz=465.4, num_updates=2200, lr=8.8056e-05, gnorm=1.144, clip=0, loss_scale=64, train_wall=60, gb_free=19.2, wall=1554
2023-07-24 21:04:29 | INFO | train_inner | epoch 002:    827 / 1474 loss=5.206, trans_loss=5.271, nll_loss=3.955, w2v_ctc_loss=3.275, task_loss=0, contrastive_loss=1.159, total=4175.4, n_correct=384.72, ppl=15.51, accuracy=9.214, wps=20110.4, ups=1.61, wpb=12471.9, bsz=460.9, num_updates=2300, lr=9.2054e-05, gnorm=1.064, clip=0, loss_scale=128, train_wall=61, gb_free=19.8, wall=1616
2023-07-24 21:05:31 | INFO | train_inner | epoch 002:    927 / 1474 loss=5.106, trans_loss=5.254, nll_loss=3.932, w2v_ctc_loss=3.178, task_loss=0, contrastive_loss=1.142, total=4104.2, n_correct=382.15, ppl=15.27, accuracy=9.311, wps=19910.3, ups=1.62, wpb=12253.1, bsz=445.9, num_updates=2400, lr=9.6052e-05, gnorm=1.062, clip=0, loss_scale=128, train_wall=61, gb_free=19, wall=1678
2023-07-24 21:06:31 | INFO | train_inner | epoch 002:   1027 / 1474 loss=5.022, trans_loss=5.247, nll_loss=3.926, w2v_ctc_loss=3.106, task_loss=0, contrastive_loss=0.995, total=4102.5, n_correct=386.51, ppl=15.2, accuracy=9.421, wps=20257.2, ups=1.65, wpb=12251.2, bsz=456.3, num_updates=2500, lr=0.00010005, gnorm=0.916, clip=0, loss_scale=128, train_wall=60, gb_free=19.3, wall=1738
2023-07-24 21:07:32 | INFO | train_inner | epoch 002:   1127 / 1474 loss=4.985, trans_loss=5.241, nll_loss=3.918, w2v_ctc_loss=3.019, task_loss=0, contrastive_loss=1.208, total=4187.61, n_correct=399.82, ppl=15.11, accuracy=9.548, wps=20430.1, ups=1.63, wpb=12495.7, bsz=487.1, num_updates=2600, lr=0.000104048, gnorm=0.93, clip=0, loss_scale=128, train_wall=61, gb_free=19.5, wall=1799
2023-07-24 21:08:34 | INFO | train_inner | epoch 002:   1227 / 1474 loss=4.921, trans_loss=5.226, nll_loss=3.899, w2v_ctc_loss=2.973, task_loss=0, contrastive_loss=1.128, total=4221.06, n_correct=418.52, ppl=14.92, accuracy=9.915, wps=20503.9, ups=1.63, wpb=12596.1, bsz=492.8, num_updates=2700, lr=0.000108046, gnorm=0.839, clip=0, loss_scale=128, train_wall=61, gb_free=19.5, wall=1861
2023-07-24 21:09:36 | INFO | train_inner | epoch 002:   1327 / 1474 loss=4.819, trans_loss=5.214, nll_loss=3.889, w2v_ctc_loss=2.934, task_loss=0, contrastive_loss=0.838, total=4157.86, n_correct=413.83, ppl=14.81, accuracy=9.953, wps=20124.2, ups=1.62, wpb=12425.5, bsz=460.7, num_updates=2800, lr=0.000112044, gnorm=0.795, clip=0, loss_scale=128, train_wall=61, gb_free=19.5, wall=1923
2023-07-24 21:10:37 | INFO | train_inner | epoch 002:   1427 / 1474 loss=4.779, trans_loss=5.222, nll_loss=3.897, w2v_ctc_loss=2.891, task_loss=0, contrastive_loss=0.925, total=4054.34, n_correct=402.27, ppl=14.9, accuracy=9.922, wps=19772.3, ups=1.63, wpb=12107, bsz=438.8, num_updates=2900, lr=0.000116042, gnorm=0.761, clip=0, loss_scale=128, train_wall=61, gb_free=19.4, wall=1984
2023-07-24 21:11:05 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-24 21:11:43 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 8.526 | trans_loss 10.31 | nll_loss 9.138 | w2v_ctc_loss 3.742 | task_loss 0 | contrastive_loss 1.003 | total 4003.4 | n_correct 497.1 | ppl 563.32 | accuracy 12.417 | uer 52.019 | wer 50.617 | raw_wer 50.617 | bleu 0.13 | wps 1212.5 | wpb 4003.4 | bsz 141.8 | num_updates 2947 | best_bleu 0.13
2023-07-24 21:11:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2947 updates
2023-07-24 21:11:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt
2023-07-24 21:11:54 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt
2023-07-24 21:12:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt (epoch 2 @ 2947 updates, score 0.13) (writing took 19.414213517680764 seconds)
2023-07-24 21:12:03 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-07-24 21:12:03 | INFO | train | epoch 002 | loss 5.312 | trans_loss 5.277 | nll_loss 3.961 | w2v_ctc_loss 3.369 | task_loss 0 | contrastive_loss 1.213 | total 4138.65 | n_correct 376.869 | ppl 15.57 | accuracy 9.106 | wps 17748.8 | ups 1.44 | wpb 12355.8 | bsz 458.5 | num_updates 2947 | lr 0.000117921 | gnorm 1.15 | clip 0 | loss_scale 128 | train_wall 893 | gb_free 19.3 | wall 2070
2023-07-24 21:12:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-24 21:12:03 | INFO | fairseq.trainer | begin training epoch 3
2023-07-24 21:12:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-24 21:12:44 | INFO | train_inner | epoch 003:     53 / 1474 loss=4.704, trans_loss=5.194, nll_loss=3.862, w2v_ctc_loss=2.828, task_loss=0, contrastive_loss=0.828, total=4071.2, n_correct=421.95, ppl=14.54, accuracy=10.364, wps=9571.3, ups=0.79, wpb=12154.1, bsz=442.6, num_updates=3000, lr=0.00012004, gnorm=0.75, clip=0, loss_scale=128, train_wall=60, gb_free=19.1, wall=2111
2023-07-24 21:12:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-24 21:12:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-07-24 21:12:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-07-24 21:12:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-07-24 21:12:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-07-24 21:14:07 | INFO | train_inner | epoch 003:    158 / 1474 loss=3.898, trans_loss=4.387, nll_loss=2.806, w2v_ctc_loss=2.493, task_loss=0, contrastive_loss=0.758, total=4144.18, n_correct=1140.83, ppl=6.99, accuracy=27.528, wps=14856.5, ups=1.2, wpb=12374.6, bsz=458.6, num_updates=3100, lr=0.000124038, gnorm=2.085, clip=1, loss_scale=4, train_wall=83, gb_free=16.8, wall=2194
2023-07-24 21:15:28 | INFO | train_inner | epoch 003:    258 / 1474 loss=3.499, trans_loss=4.141, nll_loss=2.487, w2v_ctc_loss=2.27, task_loss=0, contrastive_loss=0.647, total=4161.13, n_correct=1426.35, ppl=5.61, accuracy=34.278, wps=15350.1, ups=1.23, wpb=12431.5, bsz=467, num_updates=3200, lr=0.000128036, gnorm=1.704, clip=0, loss_scale=4, train_wall=80, gb_free=17.3, wall=2275
2023-07-24 21:16:48 | INFO | train_inner | epoch 003:    358 / 1474 loss=3.366, trans_loss=4.094, nll_loss=2.421, w2v_ctc_loss=2.155, task_loss=0, contrastive_loss=0.669, total=4150.02, n_correct=1501.5, ppl=5.35, accuracy=36.181, wps=15543.5, ups=1.26, wpb=12384.9, bsz=461.6, num_updates=3300, lr=0.000132034, gnorm=1.368, clip=0, loss_scale=4, train_wall=79, gb_free=17.3, wall=2355
2023-07-24 21:18:08 | INFO | train_inner | epoch 003:    458 / 1474 loss=3.264, trans_loss=4.051, nll_loss=2.364, w2v_ctc_loss=2.083, task_loss=0, contrastive_loss=0.535, total=4209.57, n_correct=1592.14, ppl=5.15, accuracy=37.822, wps=15723.7, ups=1.25, wpb=12566, bsz=476.8, num_updates=3400, lr=0.000136032, gnorm=1.541, clip=0, loss_scale=4, train_wall=79, gb_free=16.3, wall=2435
2023-07-24 21:19:28 | INFO | train_inner | epoch 003:    558 / 1474 loss=3.17, trans_loss=4.021, nll_loss=2.328, w2v_ctc_loss=2.019, task_loss=0, contrastive_loss=0.496, total=4088.48, n_correct=1587.28, ppl=5.02, accuracy=38.823, wps=15270, ups=1.25, wpb=12212.5, bsz=439.7, num_updates=3500, lr=0.00014003, gnorm=1.446, clip=0, loss_scale=4, train_wall=79, gb_free=17.8, wall=2515
2023-07-24 21:20:49 | INFO | train_inner | epoch 003:    658 / 1474 loss=3.119, trans_loss=3.989, nll_loss=2.28, w2v_ctc_loss=1.952, task_loss=0, contrastive_loss=0.61, total=4221.58, n_correct=1692.64, ppl=4.86, accuracy=40.095, wps=15524.6, ups=1.23, wpb=12587.8, bsz=481.9, num_updates=3600, lr=0.000144028, gnorm=1.323, clip=0, loss_scale=4, train_wall=80, gb_free=16.6, wall=2596
2023-07-24 21:22:08 | INFO | train_inner | epoch 003:    758 / 1474 loss=3.032, trans_loss=3.955, nll_loss=2.24, w2v_ctc_loss=1.915, task_loss=0, contrastive_loss=0.375, total=4167.41, n_correct=1720.75, ppl=4.72, accuracy=41.291, wps=15675.2, ups=1.26, wpb=12447.6, bsz=472.6, num_updates=3700, lr=0.000148026, gnorm=1.235, clip=0, loss_scale=4, train_wall=79, gb_free=16.6, wall=2675
2023-07-24 21:23:27 | INFO | train_inner | epoch 003:    858 / 1474 loss=2.974, trans_loss=3.942, nll_loss=2.221, w2v_ctc_loss=1.868, task_loss=0, contrastive_loss=0.336, total=4165.53, n_correct=1748.7, ppl=4.66, accuracy=41.98, wps=15749.2, ups=1.27, wpb=12437.8, bsz=456.1, num_updates=3800, lr=0.000152024, gnorm=1.174, clip=0, loss_scale=4, train_wall=78, gb_free=17.3, wall=2754
2023-07-24 21:24:48 | INFO | train_inner | epoch 003:    958 / 1474 loss=2.945, trans_loss=3.922, nll_loss=2.194, w2v_ctc_loss=1.84, task_loss=0, contrastive_loss=0.365, total=4162.3, n_correct=1791.57, ppl=4.58, accuracy=43.043, wps=15413.1, ups=1.24, wpb=12417, bsz=469.2, num_updates=3900, lr=0.000156022, gnorm=1.223, clip=0, loss_scale=4, train_wall=80, gb_free=17, wall=2835
2023-07-24 21:26:07 | INFO | train_inner | epoch 003:   1058 / 1474 loss=2.924, trans_loss=3.908, nll_loss=2.178, w2v_ctc_loss=1.836, task_loss=0, contrastive_loss=0.329, total=4069.95, n_correct=1760.37, ppl=4.52, accuracy=43.253, wps=15302.8, ups=1.26, wpb=12153.7, bsz=443.6, num_updates=4000, lr=0.00016002, gnorm=1.181, clip=0, loss_scale=4, train_wall=79, gb_free=16.5, wall=2914
2023-07-24 21:26:07 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-24 21:26:32 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.252 | trans_loss 6.463 | nll_loss 4.027 | w2v_ctc_loss 2.187 | task_loss 0 | contrastive_loss 0.438 | total 4003.4 | n_correct 1943.4 | ppl 16.31 | accuracy 48.544 | uer 30.736 | wer 31.382 | raw_wer 31.382 | bleu 10.79 | wps 1979.6 | wpb 4003.4 | bsz 141.8 | num_updates 4000 | best_bleu 10.79
2023-07-24 21:26:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-07-24 21:26:32 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_3_4000.pt
2023-07-24 21:26:35 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_3_4000.pt
2023-07-24 21:26:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 10.79) (writing took 20.20723008736968 seconds)
2023-07-24 21:28:11 | INFO | train_inner | epoch 003:   1158 / 1474 loss=2.866, trans_loss=3.896, nll_loss=2.161, w2v_ctc_loss=1.782, task_loss=0, contrastive_loss=0.3, total=4038.49, n_correct=1774.43, ppl=4.47, accuracy=43.938, wps=9751, ups=0.81, wpb=12054.8, bsz=432.5, num_updates=4100, lr=0.000164018, gnorm=1.113, clip=0, loss_scale=4, train_wall=78, gb_free=16.6, wall=3038
2023-07-24 21:29:30 | INFO | train_inner | epoch 003:   1258 / 1474 loss=2.816, trans_loss=3.874, nll_loss=2.134, w2v_ctc_loss=1.741, task_loss=0, contrastive_loss=0.282, total=4064.31, n_correct=1820.13, ppl=4.39, accuracy=44.783, wps=15336.8, ups=1.26, wpb=12136.8, bsz=433.9, num_updates=4200, lr=0.000168016, gnorm=1.034, clip=0, loss_scale=4, train_wall=79, gb_free=17.5, wall=3117
2023-07-24 21:30:51 | INFO | train_inner | epoch 003:   1358 / 1474 loss=2.803, trans_loss=3.858, nll_loss=2.112, w2v_ctc_loss=1.706, task_loss=0, contrastive_loss=0.39, total=4134.58, n_correct=1876.51, ppl=4.32, accuracy=45.386, wps=15236.5, ups=1.23, wpb=12343.8, bsz=460.7, num_updates=4300, lr=0.000172014, gnorm=1.052, clip=0, loss_scale=4, train_wall=80, gb_free=17.9, wall=3198
2023-07-24 21:32:11 | INFO | train_inner | epoch 003:   1458 / 1474 loss=2.77, trans_loss=3.843, nll_loss=2.095, w2v_ctc_loss=1.683, task_loss=0, contrastive_loss=0.374, total=4209.94, n_correct=1936.01, ppl=4.27, accuracy=45.987, wps=15666.7, ups=1.25, wpb=12573.5, bsz=477.4, num_updates=4400, lr=0.000176012, gnorm=0.992, clip=0, loss_scale=4, train_wall=80, gb_free=17.2, wall=3278
2023-07-24 21:32:24 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-24 21:32:49 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.106 | trans_loss 6.336 | nll_loss 3.859 | w2v_ctc_loss 2.015 | task_loss 0 | contrastive_loss 0.414 | total 4003.4 | n_correct 2027.8 | ppl 14.51 | accuracy 50.652 | uer 29.868 | wer 30.182 | raw_wer 30.182 | bleu 12.19 | wps 1984.7 | wpb 4003.4 | bsz 141.8 | num_updates 4416 | best_bleu 12.19
2023-07-24 21:32:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4416 updates
2023-07-24 21:32:49 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt
2023-07-24 21:32:59 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt
2023-07-24 21:33:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt (epoch 3 @ 4416 updates, score 12.19) (writing took 20.23523311316967 seconds)
2023-07-24 21:33:09 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-07-24 21:33:09 | INFO | train | epoch 003 | loss 3.158 | trans_loss 4.033 | nll_loss 2.342 | w2v_ctc_loss 1.982 | task_loss 0 | contrastive_loss 0.478 | total 4140.05 | n_correct 1626.31 | ppl 5.07 | accuracy 39.283 | wps 14340.5 | ups 1.16 | wpb 12360.1 | bsz 459 | num_updates 4416 | lr 0.000176652 | gnorm 1.294 | clip 0.1 | loss_scale 4 | train_wall 1158 | gb_free 16.6 | wall 3336
2023-07-24 21:33:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-24 21:33:09 | INFO | fairseq.trainer | begin training epoch 4
2023-07-24 21:33:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-24 21:34:24 | INFO | train_inner | epoch 004:     84 / 1474 loss=2.689, trans_loss=3.812, nll_loss=2.051, w2v_ctc_loss=1.636, task_loss=0, contrastive_loss=0.223, total=4099.41, n_correct=1917.79, ppl=4.14, accuracy=46.782, wps=9247.8, ups=0.76, wpb=12237, bsz=439.5, num_updates=4500, lr=0.00018001, gnorm=0.971, clip=0, loss_scale=4, train_wall=78, gb_free=16.5, wall=3410
2023-07-24 21:35:43 | INFO | train_inner | epoch 004:    184 / 1474 loss=2.668, trans_loss=3.789, nll_loss=2.022, w2v_ctc_loss=1.612, task_loss=0, contrastive_loss=0.252, total=4175.15, n_correct=1995.02, ppl=4.06, accuracy=47.783, wps=15657.2, ups=1.26, wpb=12464.9, bsz=468.3, num_updates=4600, lr=0.000184008, gnorm=0.962, clip=0, loss_scale=4, train_wall=79, gb_free=16.9, wall=3490
2023-07-24 21:37:03 | INFO | train_inner | epoch 004:    284 / 1474 loss=2.683, trans_loss=3.793, nll_loss=2.03, w2v_ctc_loss=1.612, task_loss=0, contrastive_loss=0.377, total=4145.23, n_correct=1972.8, ppl=4.08, accuracy=47.592, wps=15458.8, ups=1.25, wpb=12382.4, bsz=463, num_updates=4700, lr=0.000188006, gnorm=0.953, clip=0, loss_scale=4, train_wall=80, gb_free=16.2, wall=3570
2023-07-24 21:38:22 | INFO | train_inner | epoch 004:    384 / 1474 loss=2.638, trans_loss=3.793, nll_loss=2.025, w2v_ctc_loss=1.589, task_loss=0, contrastive_loss=0.218, total=4127.66, n_correct=1979.22, ppl=4.07, accuracy=47.95, wps=15633.6, ups=1.27, wpb=12314.6, bsz=443.5, num_updates=4800, lr=0.000192004, gnorm=0.95, clip=0, loss_scale=4, train_wall=78, gb_free=17.6, wall=3649
2023-07-24 21:39:42 | INFO | train_inner | epoch 004:    484 / 1474 loss=2.685, trans_loss=3.779, nll_loss=2.01, w2v_ctc_loss=1.564, task_loss=0, contrastive_loss=0.625, total=4218.78, n_correct=2042.25, ppl=4.03, accuracy=48.409, wps=15652.4, ups=1.24, wpb=12592.4, bsz=497.8, num_updates=4900, lr=0.000196002, gnorm=1.021, clip=0, loss_scale=4, train_wall=80, gb_free=16.7, wall=3729
2023-07-24 21:41:03 | INFO | train_inner | epoch 004:    584 / 1474 loss=2.633, trans_loss=3.769, nll_loss=1.998, w2v_ctc_loss=1.579, task_loss=0, contrastive_loss=0.297, total=4217.52, n_correct=2061.86, ppl=3.99, accuracy=48.888, wps=15729, ups=1.25, wpb=12591.1, bsz=485.9, num_updates=5000, lr=0.0002, gnorm=0.937, clip=0, loss_scale=4, train_wall=79, gb_free=16.3, wall=3809
mt_weight tensor(1.)
asr_weight tensor(2.0444, device='cuda:0')
2023-07-24 21:42:24 | INFO | train_inner | epoch 004:    684 / 1474 loss=2.624, trans_loss=3.781, nll_loss=2.009, w2v_ctc_loss=1.564, task_loss=0, contrastive_loss=0.343, total=4176.39, n_correct=2037.51, ppl=4.02, accuracy=48.786, wps=15332.2, ups=1.23, wpb=12448.9, bsz=455.8, num_updates=5100, lr=0.00019803, gnorm=1.53, clip=0, loss_scale=8, train_wall=81, gb_free=17.3, wall=3891
2023-07-24 21:43:43 | INFO | train_inner | epoch 004:    784 / 1474 loss=2.583, trans_loss=3.77, nll_loss=1.999, w2v_ctc_loss=1.553, task_loss=0, contrastive_loss=0.202, total=4026.63, n_correct=1976.52, ppl=4, accuracy=49.086, wps=15154.9, ups=1.26, wpb=12025, bsz=420.6, num_updates=5200, lr=0.000196116, gnorm=1.516, clip=0, loss_scale=8, train_wall=79, gb_free=13.6, wall=3970
2023-07-24 21:44:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-07-24 21:45:04 | INFO | train_inner | epoch 004:    885 / 1474 loss=2.571, trans_loss=3.755, nll_loss=1.981, w2v_ctc_loss=1.539, task_loss=0, contrastive_loss=0.214, total=4157.57, n_correct=2057.74, ppl=3.95, accuracy=49.494, wps=15332.5, ups=1.23, wpb=12416, bsz=454, num_updates=5300, lr=0.000194257, gnorm=1.526, clip=0, loss_scale=4, train_wall=80, gb_free=15.7, wall=4051
2023-07-24 21:46:25 | INFO | train_inner | epoch 004:    985 / 1474 loss=2.548, trans_loss=3.748, nll_loss=1.973, w2v_ctc_loss=1.506, task_loss=0, contrastive_loss=0.243, total=4128.78, n_correct=2062.08, ppl=3.93, accuracy=49.944, wps=15268.8, ups=1.24, wpb=12332, bsz=456.8, num_updates=5400, lr=0.00019245, gnorm=1.492, clip=0, loss_scale=4, train_wall=80, gb_free=16.2, wall=4132
2023-07-24 21:47:45 | INFO | train_inner | epoch 004:   1085 / 1474 loss=2.544, trans_loss=3.757, nll_loss=1.983, w2v_ctc_loss=1.506, task_loss=0, contrastive_loss=0.219, total=4080.2, n_correct=2032.12, ppl=3.95, accuracy=49.804, wps=15282.2, ups=1.25, wpb=12179.1, bsz=437.7, num_updates=5500, lr=0.000190693, gnorm=1.482, clip=0, loss_scale=4, train_wall=79, gb_free=16.4, wall=4211
2023-07-24 21:49:04 | INFO | train_inner | epoch 004:   1185 / 1474 loss=2.553, trans_loss=3.746, nll_loss=1.971, w2v_ctc_loss=1.495, task_loss=0, contrastive_loss=0.332, total=4163.45, n_correct=2087.62, ppl=3.92, accuracy=50.142, wps=15659.2, ups=1.26, wpb=12436, bsz=485.6, num_updates=5600, lr=0.000188982, gnorm=1.484, clip=0, loss_scale=4, train_wall=79, gb_free=15.4, wall=4291
2023-07-24 21:50:24 | INFO | train_inner | epoch 004:   1285 / 1474 loss=2.524, trans_loss=3.738, nll_loss=1.961, w2v_ctc_loss=1.476, task_loss=0, contrastive_loss=0.288, total=4152.41, n_correct=2091.82, ppl=3.89, accuracy=50.376, wps=15557.8, ups=1.25, wpb=12401.3, bsz=471.4, num_updates=5700, lr=0.000187317, gnorm=1.457, clip=0, loss_scale=4, train_wall=79, gb_free=13.2, wall=4371
2023-07-24 21:51:43 | INFO | train_inner | epoch 004:   1385 / 1474 loss=2.49, trans_loss=3.736, nll_loss=1.958, w2v_ctc_loss=1.467, task_loss=0, contrastive_loss=0.168, total=4103.57, n_correct=2074.49, ppl=3.88, accuracy=50.553, wps=15356.7, ups=1.25, wpb=12255.2, bsz=437.9, num_updates=5800, lr=0.000185695, gnorm=1.427, clip=0, loss_scale=4, train_wall=79, gb_free=16.9, wall=4450
2023-07-24 21:52:54 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(1.)
asr_weight tensor(2.0444, device='cuda:5')
mt_weight tensor(1.)
asr_weight tensor(2.0444, device='cuda:4')
mt_weight tensor(1.)
asr_weight tensor(2.0444, device='cuda:1')
mt_weight tensor(1.)
asr_weight tensor(2.0444, device='cuda:6')
mt_weight tensor(1.)
asr_weight tensor(2.0444, device='cuda:3')
mt_weight tensor(1.)
asr_weight tensor(2.0444, device='cuda:2')
mt_weight tensor(1.)
asr_weight tensor(2.0444, device='cuda:7')
2023-07-24 21:53:17 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 4.749 | trans_loss 6.019 | nll_loss 3.43 | w2v_ctc_loss 1.626 | task_loss 0 | contrastive_loss 0.323 | total 4003.4 | n_correct 2199.9 | ppl 10.78 | accuracy 54.951 | uer 24.161 | wer 25.7 | raw_wer 25.7 | bleu 15.58 | wps 2141.5 | wpb 4003.4 | bsz 141.8 | num_updates 5889 | best_bleu 15.58
2023-07-24 21:53:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5889 updates
2023-07-24 21:53:17 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt
2023-07-24 21:53:28 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt
2023-07-24 21:53:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt (epoch 4 @ 5889 updates, score 15.58) (writing took 19.912880428135395 seconds)
2023-07-24 21:53:38 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-07-24 21:53:38 | INFO | train | epoch 004 | loss 2.594 | trans_loss 3.766 | nll_loss 1.994 | w2v_ctc_loss 1.542 | task_loss 0 | contrastive_loss 0.285 | total 4137.18 | n_correct 2031.66 | ppl 3.98 | accuracy 49.107 | wps 14808.2 | ups 1.2 | wpb 12351.5 | bsz 457.8 | num_updates 5889 | lr 0.000184287 | gnorm 1.279 | clip 0 | loss_scale 4 | train_wall 1169 | gb_free 15.1 | wall 4565
2023-07-24 21:53:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-24 21:53:38 | INFO | fairseq.trainer | begin training epoch 5
2023-07-24 21:53:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-24 21:53:55 | INFO | train_inner | epoch 005:     11 / 1474 loss=2.469, trans_loss=3.726, nll_loss=1.943, w2v_ctc_loss=1.438, task_loss=0, contrastive_loss=0.187, total=4031.51, n_correct=2053.29, ppl=3.84, accuracy=50.931, wps=9127.9, ups=0.76, wpb=12037.5, bsz=436.7, num_updates=5900, lr=0.000184115, gnorm=1.438, clip=0, loss_scale=4, train_wall=79, gb_free=14.5, wall=4582
2023-07-24 21:55:15 | INFO | train_inner | epoch 005:    111 / 1474 loss=2.404, trans_loss=3.674, nll_loss=1.875, w2v_ctc_loss=1.363, task_loss=0, contrastive_loss=0.207, total=4256.63, n_correct=2234.23, ppl=3.67, accuracy=52.488, wps=15952.6, ups=1.26, wpb=12710, bsz=500.1, num_updates=6000, lr=0.000182574, gnorm=1.386, clip=0, loss_scale=4, train_wall=79, gb_free=16.4, wall=4662
2023-07-24 21:55:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-24 21:55:40 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.734 | trans_loss 5.999 | nll_loss 3.403 | w2v_ctc_loss 1.628 | task_loss 0 | contrastive_loss 0.314 | total 4003.4 | n_correct 2210.2 | ppl 10.58 | accuracy 55.208 | uer 23.717 | wer 25.279 | raw_wer 25.279 | bleu 15.57 | wps 2031.2 | wpb 4003.4 | bsz 141.8 | num_updates 6000 | best_bleu 15.58
2023-07-24 21:55:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6000 updates
2023-07-24 21:55:40 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_5_6000.pt
2023-07-24 21:55:43 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_5_6000.pt
2023-07-24 21:55:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_5_6000.pt (epoch 5 @ 6000 updates, score 15.57) (writing took 13.70785615593195 seconds)
2023-07-24 21:57:14 | INFO | train_inner | epoch 005:    211 / 1474 loss=2.44, trans_loss=3.686, nll_loss=1.889, w2v_ctc_loss=1.379, task_loss=0, contrastive_loss=0.403, total=4186.83, n_correct=2184.06, ppl=3.7, accuracy=52.165, wps=10512.4, ups=0.84, wpb=12492.2, bsz=485.3, num_updates=6100, lr=0.000181071, gnorm=1.418, clip=0, loss_scale=4, train_wall=79, gb_free=16.2, wall=4781
2023-07-24 21:58:33 | INFO | train_inner | epoch 005:    311 / 1474 loss=2.423, trans_loss=3.681, nll_loss=1.887, w2v_ctc_loss=1.392, task_loss=0, contrastive_loss=0.263, total=4094.07, n_correct=2131.89, ppl=3.7, accuracy=52.073, wps=15459.3, ups=1.26, wpb=12241.2, bsz=446, num_updates=6200, lr=0.000179605, gnorm=1.412, clip=0, loss_scale=4, train_wall=79, gb_free=16.4, wall=4860
2023-07-24 21:59:53 | INFO | train_inner | epoch 005:    411 / 1474 loss=2.419, trans_loss=3.675, nll_loss=1.88, w2v_ctc_loss=1.36, task_loss=0, contrastive_loss=0.351, total=4140.39, n_correct=2171.99, ppl=3.68, accuracy=52.459, wps=15499.9, ups=1.25, wpb=12372.8, bsz=467.9, num_updates=6300, lr=0.000178174, gnorm=1.434, clip=0, loss_scale=4, train_wall=79, gb_free=16.3, wall=4940
2023-07-24 22:01:12 | INFO | train_inner | epoch 005:    511 / 1474 loss=2.38, trans_loss=3.682, nll_loss=1.887, w2v_ctc_loss=1.362, task_loss=0, contrastive_loss=0.139, total=4026.21, n_correct=2106.48, ppl=3.7, accuracy=52.319, wps=15251.6, ups=1.27, wpb=12028.8, bsz=418.1, num_updates=6400, lr=0.000176777, gnorm=1.387, clip=0, loss_scale=4, train_wall=78, gb_free=17.2, wall=5019
2023-07-24 22:02:33 | INFO | train_inner | epoch 005:    611 / 1474 loss=2.404, trans_loss=3.69, nll_loss=1.894, w2v_ctc_loss=1.353, task_loss=0, contrastive_loss=0.306, total=4109.94, n_correct=2151.78, ppl=3.72, accuracy=52.356, wps=15157.2, ups=1.24, wpb=12260.2, bsz=449.8, num_updates=6500, lr=0.000175412, gnorm=1.409, clip=0, loss_scale=4, train_wall=80, gb_free=15.6, wall=5100
2023-07-24 22:03:53 | INFO | train_inner | epoch 005:    711 / 1474 loss=2.399, trans_loss=3.68, nll_loss=1.885, w2v_ctc_loss=1.35, task_loss=0, contrastive_loss=0.286, total=4176.83, n_correct=2204.31, ppl=3.69, accuracy=52.775, wps=15580.3, ups=1.25, wpb=12467.5, bsz=483.6, num_updates=6600, lr=0.000174078, gnorm=1.38, clip=0, loss_scale=4, train_wall=80, gb_free=17.2, wall=5180
2023-07-24 22:05:14 | INFO | train_inner | epoch 005:    811 / 1474 loss=2.381, trans_loss=3.682, nll_loss=1.885, w2v_ctc_loss=1.346, task_loss=0, contrastive_loss=0.213, total=4127.9, n_correct=2172.47, ppl=3.69, accuracy=52.629, wps=15210.1, ups=1.23, wpb=12321.2, bsz=447.5, num_updates=6700, lr=0.000172774, gnorm=1.376, clip=0, loss_scale=4, train_wall=80, gb_free=16, wall=5261
2023-07-24 22:06:34 | INFO | train_inner | epoch 005:    911 / 1474 loss=2.358, trans_loss=3.672, nll_loss=1.875, w2v_ctc_loss=1.333, task_loss=0, contrastive_loss=0.175, total=4101.19, n_correct=2171.34, ppl=3.67, accuracy=52.944, wps=15243.1, ups=1.24, wpb=12245.9, bsz=447.9, num_updates=6800, lr=0.000171499, gnorm=1.386, clip=0, loss_scale=4, train_wall=80, gb_free=17.4, wall=5341
2023-07-24 22:07:55 | INFO | train_inner | epoch 005:   1011 / 1474 loss=2.37, trans_loss=3.675, nll_loss=1.878, w2v_ctc_loss=1.335, task_loss=0, contrastive_loss=0.252, total=4164.27, n_correct=2205.97, ppl=3.68, accuracy=52.974, wps=15425.2, ups=1.24, wpb=12430.2, bsz=462, num_updates=6900, lr=0.000170251, gnorm=1.347, clip=0, loss_scale=4, train_wall=80, gb_free=15.2, wall=5422
2023-07-24 22:09:15 | INFO | train_inner | epoch 005:   1111 / 1474 loss=2.383, trans_loss=3.676, nll_loss=1.878, w2v_ctc_loss=1.343, task_loss=0, contrastive_loss=0.257, total=4168.94, n_correct=2212.1, ppl=3.68, accuracy=53.061, wps=15415.7, ups=1.24, wpb=12436.1, bsz=464.1, num_updates=7000, lr=0.000169031, gnorm=1.367, clip=0, loss_scale=4, train_wall=80, gb_free=16.8, wall=5502
2023-07-24 22:10:36 | INFO | train_inner | epoch 005:   1211 / 1474 loss=2.345, trans_loss=3.673, nll_loss=1.874, w2v_ctc_loss=1.318, task_loss=0, contrastive_loss=0.163, total=4171.16, n_correct=2218.39, ppl=3.67, accuracy=53.184, wps=15493.2, ups=1.25, wpb=12443.2, bsz=456.3, num_updates=7100, lr=0.000167836, gnorm=1.377, clip=0, loss_scale=4, train_wall=80, gb_free=16.1, wall=5583
2023-07-24 22:11:56 | INFO | train_inner | epoch 005:   1311 / 1474 loss=2.325, trans_loss=3.67, nll_loss=1.871, w2v_ctc_loss=1.304, task_loss=0, contrastive_loss=0.129, total=4126.97, n_correct=2199.79, ppl=3.66, accuracy=53.303, wps=15361.5, ups=1.25, wpb=12317.9, bsz=443.4, num_updates=7200, lr=0.000166667, gnorm=1.33, clip=0, loss_scale=4, train_wall=80, gb_free=15.7, wall=5663
2023-07-24 22:13:16 | INFO | train_inner | epoch 005:   1411 / 1474 loss=2.336, trans_loss=3.669, nll_loss=1.873, w2v_ctc_loss=1.302, task_loss=0, contrastive_loss=0.191, total=4138.54, n_correct=2209, ppl=3.66, accuracy=53.376, wps=15348.3, ups=1.24, wpb=12359.3, bsz=459, num_updates=7300, lr=0.000165521, gnorm=1.353, clip=0, loss_scale=4, train_wall=80, gb_free=17, wall=5743
2023-07-24 22:14:06 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-24 22:14:30 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.624 | trans_loss 5.904 | nll_loss 3.284 | w2v_ctc_loss 1.488 | task_loss 0 | contrastive_loss 0.309 | total 4003.4 | n_correct 2266.3 | ppl 9.74 | accuracy 56.609 | uer 22.119 | wer 23.657 | raw_wer 23.657 | bleu 16.35 | wps 2122.9 | wpb 4003.4 | bsz 141.8 | num_updates 7363 | best_bleu 16.35
2023-07-24 22:14:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7363 updates
2023-07-24 22:14:30 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt
2023-07-24 22:14:41 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt
2023-07-24 22:14:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt (epoch 5 @ 7363 updates, score 16.35) (writing took 20.270757475867867 seconds)
2023-07-24 22:14:50 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-07-24 22:14:50 | INFO | train | epoch 005 | loss 2.382 | trans_loss 3.677 | nll_loss 1.88 | w2v_ctc_loss 1.345 | task_loss 0 | contrastive_loss 0.238 | total 4138.65 | n_correct 2182.94 | ppl 3.68 | accuracy 52.745 | wps 14310.3 | ups 1.16 | wpb 12355.8 | bsz 458.5 | num_updates 7363 | lr 0.000164812 | gnorm 1.385 | clip 0 | loss_scale 8 | train_wall 1173 | gb_free 16.5 | wall 5837
2023-07-24 22:14:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-24 22:14:51 | INFO | fairseq.trainer | begin training epoch 6
2023-07-24 22:14:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-24 22:15:29 | INFO | train_inner | epoch 006:     37 / 1474 loss=2.323, trans_loss=3.647, nll_loss=1.842, w2v_ctc_loss=1.298, task_loss=0, contrastive_loss=0.189, total=4113.87, n_correct=2215.53, ppl=3.58, accuracy=53.855, wps=9260.6, ups=0.75, wpb=12276.7, bsz=447.4, num_updates=7400, lr=0.000164399, gnorm=1.385, clip=0, loss_scale=8, train_wall=79, gb_free=18, wall=5876
2023-07-24 22:16:49 | INFO | train_inner | epoch 006:    137 / 1474 loss=2.276, trans_loss=3.616, nll_loss=1.802, w2v_ctc_loss=1.243, task_loss=0, contrastive_loss=0.231, total=4161.2, n_correct=2271.02, ppl=3.49, accuracy=54.576, wps=15600.1, ups=1.26, wpb=12428.5, bsz=458.1, num_updates=7500, lr=0.000163299, gnorm=1.343, clip=0, loss_scale=8, train_wall=79, gb_free=17.1, wall=5956
2023-07-24 22:18:08 | INFO | train_inner | epoch 006:    237 / 1474 loss=2.285, trans_loss=3.627, nll_loss=1.817, w2v_ctc_loss=1.273, task_loss=0, contrastive_loss=0.141, total=4110.12, n_correct=2233.17, ppl=3.52, accuracy=54.333, wps=15367.4, ups=1.25, wpb=12279.2, bsz=437.3, num_updates=7600, lr=0.000162221, gnorm=1.332, clip=0, loss_scale=8, train_wall=79, gb_free=17.3, wall=6035
2023-07-24 22:19:29 | INFO | train_inner | epoch 006:    337 / 1474 loss=2.303, trans_loss=3.617, nll_loss=1.804, w2v_ctc_loss=1.229, task_loss=0, contrastive_loss=0.443, total=4170.52, n_correct=2281.8, ppl=3.49, accuracy=54.713, wps=15451.1, ups=1.24, wpb=12453.1, bsz=488.4, num_updates=7700, lr=0.000161165, gnorm=1.361, clip=0, loss_scale=8, train_wall=80, gb_free=15.9, wall=6116
2023-07-24 22:20:48 | INFO | train_inner | epoch 006:    437 / 1474 loss=2.258, trans_loss=3.619, nll_loss=1.806, w2v_ctc_loss=1.235, task_loss=0, contrastive_loss=0.155, total=4154.89, n_correct=2279.98, ppl=3.5, accuracy=54.875, wps=15699.5, ups=1.27, wpb=12405.6, bsz=470.4, num_updates=7800, lr=0.000160128, gnorm=1.323, clip=0, loss_scale=8, train_wall=79, gb_free=16.6, wall=6195
2023-07-24 22:22:08 | INFO | train_inner | epoch 006:    537 / 1474 loss=2.261, trans_loss=3.624, nll_loss=1.812, w2v_ctc_loss=1.244, task_loss=0, contrastive_loss=0.143, total=4174.46, n_correct=2291.37, ppl=3.51, accuracy=54.89, wps=15649.9, ups=1.26, wpb=12460.1, bsz=458.3, num_updates=7900, lr=0.000159111, gnorm=1.332, clip=0, loss_scale=8, train_wall=79, gb_free=17.3, wall=6275
2023-07-24 22:23:27 | INFO | train_inner | epoch 006:    637 / 1474 loss=2.266, trans_loss=3.626, nll_loss=1.816, w2v_ctc_loss=1.232, task_loss=0, contrastive_loss=0.2, total=4145.19, n_correct=2274.02, ppl=3.52, accuracy=54.859, wps=15578.3, ups=1.26, wpb=12372.7, bsz=470.9, num_updates=8000, lr=0.000158114, gnorm=1.345, clip=0, loss_scale=8, train_wall=79, gb_free=16, wall=6354
2023-07-24 22:23:27 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-24 22:23:50 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.59 | trans_loss 5.865 | nll_loss 3.23 | w2v_ctc_loss 1.477 | task_loss 0 | contrastive_loss 0.282 | total 4003.4 | n_correct 2286.3 | ppl 9.38 | accuracy 57.109 | uer 21.655 | wer 23.362 | raw_wer 23.362 | bleu 16.53 | wps 2304.4 | wpb 4003.4 | bsz 141.8 | num_updates 8000 | best_bleu 16.53
2023-07-24 22:23:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8000 updates
2023-07-24 22:23:50 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_6_8000.pt
2023-07-24 22:23:53 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_6_8000.pt
2023-07-24 22:24:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_6_8000.pt (epoch 6 @ 8000 updates, score 16.53) (writing took 32.13523314334452 seconds)
2023-07-24 22:25:41 | INFO | train_inner | epoch 006:    737 / 1474 loss=2.264, trans_loss=3.629, nll_loss=1.82, w2v_ctc_loss=1.245, task_loss=0, contrastive_loss=0.151, total=4151.01, n_correct=2271.84, ppl=3.53, accuracy=54.73, wps=9238.6, ups=0.75, wpb=12393.7, bsz=454.2, num_updates=8100, lr=0.000157135, gnorm=1.321, clip=0, loss_scale=8, train_wall=79, gb_free=13.3, wall=6488
2023-07-24 22:27:01 | INFO | train_inner | epoch 006:    837 / 1474 loss=2.261, trans_loss=3.638, nll_loss=1.831, w2v_ctc_loss=1.241, task_loss=0, contrastive_loss=0.134, total=4108.83, n_correct=2240.8, ppl=3.56, accuracy=54.536, wps=15441.2, ups=1.26, wpb=12267.1, bsz=439.4, num_updates=8200, lr=0.000156174, gnorm=1.355, clip=0, loss_scale=8, train_wall=79, gb_free=17.2, wall=6568
2023-07-24 22:28:20 | INFO | train_inner | epoch 006:    937 / 1474 loss=2.278, trans_loss=3.637, nll_loss=1.83, w2v_ctc_loss=1.242, task_loss=0, contrastive_loss=0.231, total=4076.46, n_correct=2224.01, ppl=3.55, accuracy=54.557, wps=15295.7, ups=1.26, wpb=12166, bsz=443, num_updates=8300, lr=0.00015523, gnorm=1.357, clip=0, loss_scale=8, train_wall=79, gb_free=12.9, wall=6647
2023-07-24 22:29:40 | INFO | train_inner | epoch 006:   1037 / 1474 loss=2.269, trans_loss=3.621, nll_loss=1.811, w2v_ctc_loss=1.222, task_loss=0, contrastive_loss=0.306, total=4175.9, n_correct=2296.3, ppl=3.51, accuracy=54.989, wps=15638.6, ups=1.25, wpb=12465, bsz=480.4, num_updates=8400, lr=0.000154303, gnorm=1.354, clip=0, loss_scale=8, train_wall=79, gb_free=14.4, wall=6727
2023-07-24 22:31:00 | INFO | train_inner | epoch 006:   1137 / 1474 loss=2.254, trans_loss=3.628, nll_loss=1.819, w2v_ctc_loss=1.235, task_loss=0, contrastive_loss=0.139, total=4077.2, n_correct=2235.39, ppl=3.53, accuracy=54.827, wps=15310, ups=1.26, wpb=12172.7, bsz=430.6, num_updates=8500, lr=0.000153393, gnorm=1.332, clip=0, loss_scale=8, train_wall=79, gb_free=16.5, wall=6806
2023-07-24 22:32:20 | INFO | train_inner | epoch 006:   1237 / 1474 loss=2.288, trans_loss=3.62, nll_loss=1.811, w2v_ctc_loss=1.219, task_loss=0, contrastive_loss=0.454, total=4133.46, n_correct=2272.59, ppl=3.51, accuracy=54.98, wps=15433.7, ups=1.25, wpb=12346.4, bsz=470.3, num_updates=8600, lr=0.000152499, gnorm=1.356, clip=0, loss_scale=8, train_wall=80, gb_free=12.7, wall=6886
2023-07-24 22:33:39 | INFO | train_inner | epoch 006:   1337 / 1474 loss=2.237, trans_loss=3.627, nll_loss=1.816, w2v_ctc_loss=1.218, task_loss=0, contrastive_loss=0.125, total=4127.77, n_correct=2273.76, ppl=3.52, accuracy=55.084, wps=15537.3, ups=1.26, wpb=12312.7, bsz=454.1, num_updates=8700, lr=0.00015162, gnorm=1.322, clip=0, loss_scale=8, train_wall=79, gb_free=17.2, wall=6966
2023-07-24 22:34:59 | INFO | train_inner | epoch 006:   1437 / 1474 loss=2.233, trans_loss=3.618, nll_loss=1.806, w2v_ctc_loss=1.216, task_loss=0, contrastive_loss=0.13, total=4190.32, n_correct=2321.44, ppl=3.5, accuracy=55.4, wps=15636.1, ups=1.25, wpb=12507.6, bsz=460.5, num_updates=8800, lr=0.000150756, gnorm=1.32, clip=0, loss_scale=8, train_wall=79, gb_free=17, wall=7046
2023-07-24 22:35:28 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-24 22:35:52 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.539 | trans_loss 5.814 | nll_loss 3.164 | w2v_ctc_loss 1.431 | task_loss 0 | contrastive_loss 0.281 | total 4003.4 | n_correct 2317.1 | ppl 8.96 | accuracy 57.878 | uer 20.471 | wer 22.285 | raw_wer 22.285 | bleu 17.39 | wps 2009.2 | wpb 4003.4 | bsz 141.8 | num_updates 8837 | best_bleu 17.39
2023-07-24 22:35:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8837 updates
2023-07-24 22:35:52 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt
2023-07-24 22:36:04 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt
2023-07-24 22:36:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_baseline_mt_0724/checkpoint_best.pt (epoch 6 @ 8837 updates, score 17.39) (writing took 21.539975820109248 seconds)
2023-07-24 22:36:14 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-07-24 22:36:14 | INFO | train | epoch 006 | loss 2.265 | trans_loss 3.624 | nll_loss 1.814 | w2v_ctc_loss 1.235 | task_loss 0 | contrastive_loss 0.212 | total 4138.65 | n_correct 2268.9 | ppl 3.52 | accuracy 54.822 | wps 14189.9 | ups 1.15 | wpb 12355.8 | bsz 458.5 | num_updates 8837 | lr 0.00015044 | gnorm 1.339 | clip 0 | loss_scale 8 | train_wall 1166 | gb_free 15.4 | wall 7121
2023-07-24 22:36:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-24 22:36:14 | INFO | fairseq.trainer | begin training epoch 7
2023-07-24 22:36:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-24 22:37:12 | INFO | train_inner | epoch 007:     63 / 1474 loss=2.203, trans_loss=3.596, nll_loss=1.779, w2v_ctc_loss=1.184, task_loss=0, contrastive_loss=0.146, total=4110.43, n_correct=2292.41, ppl=3.43, accuracy=55.771, wps=9187.1, ups=0.75, wpb=12272.9, bsz=462.8, num_updates=8900, lr=0.000149906, gnorm=1.317, clip=0, loss_scale=8, train_wall=79, gb_free=17.5, wall=7179
2023-07-24 22:38:32 | INFO | train_inner | epoch 007:    163 / 1474 loss=2.199, trans_loss=3.587, nll_loss=1.765, w2v_ctc_loss=1.168, task_loss=0, contrastive_loss=0.216, total=4109.53, n_correct=2299.4, ppl=3.4, accuracy=55.953, wps=15468.1, ups=1.26, wpb=12269.6, bsz=454.1, num_updates=9000, lr=0.000149071, gnorm=1.324, clip=0, loss_scale=8, train_wall=79, gb_free=13.9, wall=7259
2023-07-24 22:39:51 | INFO | train_inner | epoch 007:    263 / 1474 loss=2.184, trans_loss=3.582, nll_loss=1.757, w2v_ctc_loss=1.173, task_loss=0, contrastive_loss=0.125, total=4133.29, n_correct=2326.17, ppl=3.38, accuracy=56.279, wps=15625.7, ups=1.27, wpb=12335.8, bsz=455.5, num_updates=9100, lr=0.00014825, gnorm=1.324, clip=0, loss_scale=8, train_wall=78, gb_free=15.6, wall=7338
2023-07-24 22:41:11 | INFO | train_inner | epoch 007:    363 / 1474 loss=2.22, trans_loss=3.59, nll_loss=1.768, w2v_ctc_loss=1.164, task_loss=0, contrastive_loss=0.387, total=4194.76, n_correct=2348.04, ppl=3.41, accuracy=55.976, wps=15677.9, ups=1.25, wpb=12518.2, bsz=477.6, num_updates=9200, lr=0.000147442, gnorm=1.314, clip=0, loss_scale=8, train_wall=79, gb_free=13.3, wall=7417
2023-07-24 22:42:30 | INFO | train_inner | epoch 007:    463 / 1474 loss=2.206, trans_loss=3.59, nll_loss=1.772, w2v_ctc_loss=1.161, task_loss=0, contrastive_loss=0.306, total=4153.22, n_correct=2325.56, ppl=3.41, accuracy=55.994, wps=15582.1, ups=1.26, wpb=12403.3, bsz=463, num_updates=9300, lr=0.000146647, gnorm=1.317, clip=0, loss_scale=8, train_wall=79, gb_free=17, wall=7497
2023-07-24 22:43:50 | INFO | train_inner | epoch 007:    563 / 1474 loss=2.183, trans_loss=3.59, nll_loss=1.767, w2v_ctc_loss=1.165, task_loss=0, contrastive_loss=0.131, total=4168.14, n_correct=2344.87, ppl=3.4, accuracy=56.257, wps=15614.4, ups=1.26, wpb=12434.7, bsz=459.8, num_updates=9400, lr=0.000145865, gnorm=1.329, clip=0, loss_scale=16, train_wall=79, gb_free=17, wall=7577
2023-07-24 22:45:09 | INFO | train_inner | epoch 007:    663 / 1474 loss=2.174, trans_loss=3.588, nll_loss=1.766, w2v_ctc_loss=1.159, task_loss=0, contrastive_loss=0.12, total=4157.82, n_correct=2341.19, ppl=3.4, accuracy=56.308, wps=15559.4, ups=1.25, wpb=12406.4, bsz=455.4, num_updates=9500, lr=0.000145095, gnorm=1.315, clip=0, loss_scale=16, train_wall=79, gb_free=15.8, wall=7656
2023-07-24 22:46:29 | INFO | train_inner | epoch 007:    763 / 1474 loss=2.173, trans_loss=3.584, nll_loss=1.762, w2v_ctc_loss=1.159, task_loss=0, contrastive_loss=0.116, total=4122.1, n_correct=2318.15, ppl=3.39, accuracy=56.237, wps=15490, ups=1.26, wpb=12308.4, bsz=446.2, num_updates=9600, lr=0.000144338, gnorm=1.316, clip=0, loss_scale=16, train_wall=79, gb_free=15.8, wall=7736
2023-07-24 22:47:49 | INFO | train_inner | epoch 007:    863 / 1474 loss=2.179, trans_loss=3.592, nll_loss=1.773, w2v_ctc_loss=1.159, task_loss=0, contrastive_loss=0.137, total=4147.23, n_correct=2329.31, ppl=3.42, accuracy=56.165, wps=15412.6, ups=1.25, wpb=12377.6, bsz=460.9, num_updates=9700, lr=0.000143592, gnorm=1.323, clip=0, loss_scale=16, train_wall=80, gb_free=17.6, wall=7816
2023-07-24 22:49:09 | INFO | train_inner | epoch 007:    963 / 1474 loss=2.183, trans_loss=3.587, nll_loss=1.768, w2v_ctc_loss=1.146, task_loss=0, contrastive_loss=0.229, total=4140.14, n_correct=2333.62, ppl=3.41, accuracy=56.366, wps=15428.1, ups=1.25, wpb=12359.1, bsz=474.6, num_updates=9800, lr=0.000142857, gnorm=1.325, clip=0, loss_scale=16, train_wall=80, gb_free=16.2, wall=7896
2023-07-24 22:50:29 | INFO | train_inner | epoch 007:   1063 / 1474 loss=2.174, trans_loss=3.599, nll_loss=1.782, w2v_ctc_loss=1.161, task_loss=0, contrastive_loss=0.103, total=4103.51, n_correct=2299.46, ppl=3.44, accuracy=56.036, wps=15397, ups=1.26, wpb=12251.1, bsz=437.5, num_updates=9900, lr=0.000142134, gnorm=1.32, clip=0, loss_scale=16, train_wall=79, gb_free=17, wall=7976
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 393 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
