2023-07-27 10:18:26 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:13815
2023-07-27 10:18:26 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:13815
2023-07-27 10:18:26 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:13815
2023-07-27 10:18:27 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:13815
2023-07-27 10:18:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-27 10:18:27 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:13815
2023-07-27 10:18:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-27 10:18:27 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:13815
2023-07-27 10:18:27 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:13815
2023-07-27 10:18:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-27 10:18:27 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:13815
2023-07-27 10:18:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-27 10:18:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-27 10:18:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-27 10:18:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-27 10:18:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-27 10:18:27 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 10:18:27 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-07-27 10:18:27 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 10:18:27 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 10:18:27 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 10:18:27 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-07-27 10:18:27 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-07-27 10:18:27 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-07-27 10:18:27 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 10:18:27 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-07-27 10:18:27 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 10:18:27 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 10:18:27 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 10:18:27 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-07-27 10:18:27 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-07-27 10:18:27 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-07-27 10:18:32 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13815', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_rate=0.5, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_rate=0.5, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_rate=0.5, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-07-27 10:18:32 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-07-27 10:18:32 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-07-27 10:18:32 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-27 10:18:32 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 1.0
2023-07-27 10:18:32 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-27 10:18:36 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-07-27 10:18:36 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-27 10:18:36 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-27 10:18:38 | INFO | root | load pretrained hubert
2023-07-27 10:18:41 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-27 10:18:43 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-27 10:18:46 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-27 10:18:46 | INFO | root | share the sematic adapter and textual encoder
2023-07-27 10:18:46 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-27 10:18:46 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-07-27 10:18:46 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-27 10:18:46 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-07-27 10:18:46 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-27 10:18:46 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-27 10:18:46 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-27 10:18:46 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-27 10:18:46 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-27 10:18:46 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-27 10:18:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-27 10:18:51 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-27 10:18:51 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-27 10:18:52 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-27 10:18:52 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 10:18:52 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 10:18:52 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 10:18:52 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 10:18:52 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 10:18:52 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 10:18:52 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 10:18:52 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 10:18:52 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-27 10:18:52 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-27 10:18:52 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-27 10:18:52 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_last.pt
2023-07-27 10:18:52 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_last.pt
2023-07-27 10:18:52 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-27 10:18:52 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-27 10:18:52 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-27 10:18:52 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-27 10:18:53 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-27 10:18:55 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-27 10:18:56 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-27 10:20:06 | INFO | fairseq.optim.adam | using FusedAdam
2023-07-27 10:20:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 10:20:06 | INFO | fairseq.trainer | begin training epoch 1
2023-07-27 10:20:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 10:21:29 | INFO | train_inner | epoch 001:    100 / 1474 loss=19.137, trans_loss=5.598, nll_loss=4.163, w2v_ctc_loss=22.485, task_loss=1.749, contrastive_loss=3.325, total=4207.04, n_correct=209.14, ppl=17.91, accuracy=4.971, wps=19212.8, ups=1.53, wpb=12551.1, bsz=471.4, num_updates=100, lr=4.098e-06, gnorm=0.891, clip=0, loss_scale=128, train_wall=73, gb_free=19.5, wall=157
2023-07-27 10:22:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-27 10:22:34 | INFO | train_inner | epoch 001:    201 / 1474 loss=16.991, trans_loss=5.478, nll_loss=4.066, w2v_ctc_loss=19.358, task_loss=1.706, contrastive_loss=3.278, total=4124.14, n_correct=223.42, ppl=16.75, accuracy=5.417, wps=18875.9, ups=1.53, wpb=12313.4, bsz=461, num_updates=200, lr=8.096e-06, gnorm=3.625, clip=0, loss_scale=64, train_wall=65, gb_free=19.2, wall=222
2023-07-27 10:23:38 | INFO | train_inner | epoch 001:    301 / 1474 loss=10.092, trans_loss=5.489, nll_loss=4.133, w2v_ctc_loss=8.784, task_loss=1.706, contrastive_loss=3.203, total=4079.62, n_correct=205.92, ppl=17.55, accuracy=5.048, wps=19048.8, ups=1.56, wpb=12186.7, bsz=438.2, num_updates=300, lr=1.2094e-05, gnorm=4.607, clip=0, loss_scale=64, train_wall=63, gb_free=19.9, wall=286
2023-07-27 10:24:42 | INFO | train_inner | epoch 001:    401 / 1474 loss=8.848, trans_loss=5.517, nll_loss=4.19, w2v_ctc_loss=6.815, task_loss=1.496, contrastive_loss=3.237, total=4174.14, n_correct=193.96, ppl=18.25, accuracy=4.647, wps=19453.4, ups=1.56, wpb=12463.5, bsz=460.4, num_updates=400, lr=1.6092e-05, gnorm=2.947, clip=0, loss_scale=64, train_wall=64, gb_free=18.9, wall=351
2023-07-27 10:25:48 | INFO | train_inner | epoch 001:    501 / 1474 loss=8.414, trans_loss=5.494, nll_loss=4.178, w2v_ctc_loss=6.177, task_loss=1.368, contrastive_loss=3.233, total=4176.18, n_correct=188.99, ppl=18.1, accuracy=4.525, wps=19131.6, ups=1.53, wpb=12479.7, bsz=477.4, num_updates=500, lr=2.009e-05, gnorm=1.417, clip=0, loss_scale=64, train_wall=65, gb_free=19.2, wall=416
2023-07-27 10:26:52 | INFO | train_inner | epoch 001:    601 / 1474 loss=8.165, trans_loss=5.523, nll_loss=4.214, w2v_ctc_loss=5.809, task_loss=1.274, contrastive_loss=3.288, total=4147.79, n_correct=184.35, ppl=18.55, accuracy=4.445, wps=19086.6, ups=1.54, wpb=12371.6, bsz=484.2, num_updates=600, lr=2.4088e-05, gnorm=0.728, clip=0, loss_scale=64, train_wall=64, gb_free=18.9, wall=481
2023-07-27 10:27:56 | INFO | train_inner | epoch 001:    701 / 1474 loss=8.008, trans_loss=5.524, nll_loss=4.22, w2v_ctc_loss=5.69, task_loss=1.325, contrastive_loss=3.039, total=4152.1, n_correct=193.85, ppl=18.63, accuracy=4.669, wps=19385.7, ups=1.56, wpb=12395.5, bsz=456.2, num_updates=700, lr=2.8086e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=63, gb_free=19.5, wall=545
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 131 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-07-27 10:40:49 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:14900
2023-07-27 10:40:49 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:14900
2023-07-27 10:40:49 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:14900
2023-07-27 10:40:49 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:14900
2023-07-27 10:40:49 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-27 10:40:49 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:14900
2023-07-27 10:40:49 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-27 10:40:49 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:14900
2023-07-27 10:40:49 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-27 10:40:50 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:14900
2023-07-27 10:40:50 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:14900
2023-07-27 10:40:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-27 10:40:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-27 10:40:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-27 10:40:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-27 10:40:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-27 10:40:50 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 10:40:50 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-07-27 10:40:50 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 10:40:50 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 10:40:50 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 10:40:50 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 10:40:50 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-07-27 10:40:50 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-07-27 10:40:50 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-07-27 10:40:50 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-07-27 10:40:50 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 10:40:50 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-07-27 10:40:50 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 10:40:50 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 10:40:50 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-07-27 10:40:50 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-07-27 10:40:54 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14900', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_rate=0.5, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_rate=0.5, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_rate=0.5, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-07-27 10:40:54 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-07-27 10:40:54 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-07-27 10:40:54 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-27 10:40:54 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 1.0
2023-07-27 10:40:54 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-27 10:40:59 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-07-27 10:40:59 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-27 10:40:59 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-27 10:41:01 | INFO | root | load pretrained hubert
2023-07-27 10:41:04 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-27 10:41:04 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-27 10:41:07 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-27 10:41:07 | INFO | root | share the sematic adapter and textual encoder
2023-07-27 10:41:07 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-27 10:41:07 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-07-27 10:41:07 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-27 10:41:07 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-07-27 10:41:07 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-27 10:41:07 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-27 10:41:07 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-27 10:41:07 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-27 10:41:07 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-27 10:41:07 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-27 10:41:09 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-27 10:41:09 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-27 10:41:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-27 10:41:10 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-27 10:41:10 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 10:41:10 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 10:41:10 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 10:41:10 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 10:41:10 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 10:41:10 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 10:41:10 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 10:41:10 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 10:41:10 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-27 10:41:10 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-27 10:41:10 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-27 10:41:10 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_last.pt
2023-07-27 10:41:10 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_last.pt
2023-07-27 10:41:10 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-27 10:41:10 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-27 10:41:10 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-27 10:41:10 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-27 10:41:11 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-27 10:41:13 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-27 10:41:15 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-27 10:42:29 | INFO | fairseq.optim.adam | using FusedAdam
2023-07-27 10:42:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 10:42:29 | INFO | fairseq.trainer | begin training epoch 1
2023-07-27 10:42:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 10:43:49 | INFO | train_inner | epoch 001:    100 / 1474 loss=19.137, trans_loss=5.598, nll_loss=4.163, w2v_ctc_loss=22.485, task_loss=1.749, contrastive_loss=3.325, total=4207.04, n_correct=209.33, ppl=17.91, accuracy=4.976, wps=19065.1, ups=1.52, wpb=12551.1, bsz=471.4, num_updates=100, lr=4.098e-06, gnorm=0.891, clip=0, loss_scale=128, train_wall=71, gb_free=19.5, wall=159
2023-07-27 10:44:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-27 10:44:55 | INFO | train_inner | epoch 001:    201 / 1474 loss=16.991, trans_loss=5.478, nll_loss=4.066, w2v_ctc_loss=19.358, task_loss=1.706, contrastive_loss=3.278, total=4124.14, n_correct=223.37, ppl=16.75, accuracy=5.416, wps=18597.5, ups=1.51, wpb=12313.4, bsz=461, num_updates=200, lr=8.096e-06, gnorm=3.625, clip=0, loss_scale=64, train_wall=66, gb_free=19.2, wall=225
2023-07-27 10:45:59 | INFO | train_inner | epoch 001:    301 / 1474 loss=10.093, trans_loss=5.49, nll_loss=4.135, w2v_ctc_loss=8.784, task_loss=1.706, contrastive_loss=3.203, total=4079.62, n_correct=205.45, ppl=17.57, accuracy=5.036, wps=19010.8, ups=1.56, wpb=12186.7, bsz=438.2, num_updates=300, lr=1.2094e-05, gnorm=4.607, clip=0, loss_scale=64, train_wall=63, gb_free=19.9, wall=289
2023-07-27 10:47:03 | INFO | train_inner | epoch 001:    401 / 1474 loss=8.848, trans_loss=5.516, nll_loss=4.19, w2v_ctc_loss=6.815, task_loss=1.496, contrastive_loss=3.237, total=4174.14, n_correct=193.56, ppl=18.25, accuracy=4.637, wps=19429.1, ups=1.56, wpb=12463.5, bsz=460.4, num_updates=400, lr=1.6092e-05, gnorm=2.947, clip=0, loss_scale=64, train_wall=64, gb_free=18.9, wall=353
2023-07-27 10:48:08 | INFO | train_inner | epoch 001:    501 / 1474 loss=8.415, trans_loss=5.494, nll_loss=4.178, w2v_ctc_loss=6.177, task_loss=1.368, contrastive_loss=3.233, total=4176.18, n_correct=188.92, ppl=18.1, accuracy=4.524, wps=19281.8, ups=1.55, wpb=12479.7, bsz=477.4, num_updates=500, lr=2.009e-05, gnorm=1.417, clip=0, loss_scale=64, train_wall=64, gb_free=19.2, wall=418
2023-07-27 10:49:13 | INFO | train_inner | epoch 001:    601 / 1474 loss=8.164, trans_loss=5.523, nll_loss=4.213, w2v_ctc_loss=5.809, task_loss=1.274, contrastive_loss=3.287, total=4147.79, n_correct=184.54, ppl=18.54, accuracy=4.449, wps=19076.5, ups=1.54, wpb=12371.6, bsz=484.2, num_updates=600, lr=2.4088e-05, gnorm=0.727, clip=0, loss_scale=64, train_wall=64, gb_free=18.9, wall=483
2023-07-27 10:50:16 | INFO | train_inner | epoch 001:    701 / 1474 loss=8.007, trans_loss=5.523, nll_loss=4.218, w2v_ctc_loss=5.69, task_loss=1.325, contrastive_loss=3.038, total=4152.1, n_correct=195.11, ppl=18.61, accuracy=4.699, wps=19451.8, ups=1.57, wpb=12395.5, bsz=456.2, num_updates=700, lr=2.8086e-05, gnorm=0.551, clip=0, loss_scale=64, train_wall=63, gb_free=19.5, wall=547
2023-07-27 10:51:20 | INFO | train_inner | epoch 001:    801 / 1474 loss=7.732, trans_loss=5.46, nll_loss=4.15, w2v_ctc_loss=5.465, task_loss=1.281, contrastive_loss=2.946, total=4123.83, n_correct=237.19, ppl=17.76, accuracy=5.752, wps=19326, ups=1.57, wpb=12306.1, bsz=464.1, num_updates=800, lr=3.2084e-05, gnorm=0.813, clip=0, loss_scale=64, train_wall=63, gb_free=19.2, wall=610
2023-07-27 10:52:25 | INFO | train_inner | epoch 001:    901 / 1474 loss=7.475, trans_loss=5.429, nll_loss=4.124, w2v_ctc_loss=5.283, task_loss=1.302, contrastive_loss=2.707, total=4163.61, n_correct=263.37, ppl=17.43, accuracy=6.326, wps=19267.8, ups=1.55, wpb=12433.9, bsz=457.1, num_updates=900, lr=3.6082e-05, gnorm=1.338, clip=0, loss_scale=64, train_wall=64, gb_free=18.9, wall=675
2023-07-27 10:53:29 | INFO | train_inner | epoch 001:   1001 / 1474 loss=7.215, trans_loss=5.404, nll_loss=4.102, w2v_ctc_loss=5.072, task_loss=1.311, contrastive_loss=2.558, total=4135.34, n_correct=284.9, ppl=17.17, accuracy=6.889, wps=19102.5, ups=1.55, wpb=12353.2, bsz=456.8, num_updates=1000, lr=4.008e-05, gnorm=1.42, clip=0, loss_scale=64, train_wall=64, gb_free=19, wall=739
2023-07-27 10:54:33 | INFO | train_inner | epoch 001:   1101 / 1474 loss=6.945, trans_loss=5.39, nll_loss=4.088, w2v_ctc_loss=4.874, task_loss=1.322, contrastive_loss=2.336, total=4147.38, n_correct=308.9, ppl=17, accuracy=7.448, wps=19397.5, ups=1.57, wpb=12367, bsz=454.9, num_updates=1100, lr=4.4078e-05, gnorm=1.659, clip=0, loss_scale=64, train_wall=63, gb_free=18.9, wall=803
2023-07-27 10:55:37 | INFO | train_inner | epoch 001:   1201 / 1474 loss=6.725, trans_loss=5.37, nll_loss=4.071, w2v_ctc_loss=4.708, task_loss=1.377, contrastive_loss=2.133, total=4139.9, n_correct=315.79, ppl=16.8, accuracy=7.628, wps=19424.4, ups=1.57, wpb=12366.5, bsz=440.1, num_updates=1200, lr=4.8076e-05, gnorm=1.759, clip=0, loss_scale=64, train_wall=63, gb_free=19, wall=867
2023-07-27 10:56:41 | INFO | train_inner | epoch 001:   1301 / 1474 loss=6.503, trans_loss=5.368, nll_loss=4.07, w2v_ctc_loss=4.513, task_loss=1.324, contrastive_loss=1.942, total=4046.58, n_correct=319.38, ppl=16.8, accuracy=7.893, wps=18719.7, ups=1.55, wpb=12081.6, bsz=439.3, num_updates=1300, lr=5.2074e-05, gnorm=1.73, clip=0, loss_scale=64, train_wall=64, gb_free=19.7, wall=931
2023-07-27 10:57:46 | INFO | train_inner | epoch 001:   1401 / 1474 loss=6.297, trans_loss=5.357, nll_loss=4.061, w2v_ctc_loss=4.312, task_loss=1.308, contrastive_loss=2.01, total=4133.18, n_correct=333.32, ppl=16.69, accuracy=8.064, wps=19020.1, ups=1.54, wpb=12350, bsz=455, num_updates=1400, lr=5.6072e-05, gnorm=1.627, clip=0, loss_scale=64, train_wall=64, gb_free=19.9, wall=996
2023-07-27 10:58:33 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 10:59:14 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 9.571 | trans_loss 10.922 | nll_loss 9.907 | w2v_ctc_loss 5.608 | task_loss 7.547 | contrastive_loss 2.369 | total 4003.4 | n_correct 384.3 | ppl 960.39 | accuracy 9.599 | uer 71.953 | wer 69.882 | raw_wer 69.882 | bleu 0.02 | wps 1120.7 | wpb 4003.4 | bsz 141.8 | num_updates 1473
2023-07-27 10:59:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1473 updates
2023-07-27 10:59:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt
2023-07-27 10:59:17 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt
2023-07-27 10:59:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt (epoch 1 @ 1473 updates, score 0.02) (writing took 5.8740518391132355 seconds)
2023-07-27 10:59:20 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-07-27 10:59:20 | INFO | train | epoch 001 | loss 9.043 | trans_loss 5.452 | nll_loss 4.127 | w2v_ctc_loss 7.645 | task_loss 1.41 | contrastive_loss 2.762 | total 4138.55 | n_correct 251.745 | ppl 17.47 | accuracy 6.083 | wps 18254.6 | ups 1.48 | wpb 12355.5 | bsz 458.4 | num_updates 1473 | lr 5.89905e-05 | gnorm 1.789 | clip 0 | loss_scale 64 | train_wall 947 | gb_free 19.2 | wall 1090
2023-07-27 10:59:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 10:59:20 | INFO | fairseq.trainer | begin training epoch 2
2023-07-27 10:59:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 10:59:47 | INFO | train_inner | epoch 002:     27 / 1474 loss=6.109, trans_loss=5.35, nll_loss=4.048, w2v_ctc_loss=4.119, task_loss=1.246, contrastive_loss=1.856, total=4162.95, n_correct=338.49, ppl=16.54, accuracy=8.131, wps=10287.5, ups=0.83, wpb=12416.8, bsz=470.8, num_updates=1500, lr=6.007e-05, gnorm=1.664, clip=0, loss_scale=64, train_wall=64, gb_free=19.6, wall=1117
2023-07-27 11:00:51 | INFO | train_inner | epoch 002:    127 / 1474 loss=5.949, trans_loss=5.347, nll_loss=4.044, w2v_ctc_loss=4.003, task_loss=1.33, contrastive_loss=1.653, total=4155.98, n_correct=338.88, ppl=16.49, accuracy=8.154, wps=19280.8, ups=1.56, wpb=12394.8, bsz=451.6, num_updates=1600, lr=6.4068e-05, gnorm=1.728, clip=0, loss_scale=64, train_wall=64, gb_free=18.9, wall=1181
2023-07-27 11:01:56 | INFO | train_inner | epoch 002:    227 / 1474 loss=5.784, trans_loss=5.326, nll_loss=4.022, w2v_ctc_loss=3.807, task_loss=1.153, contrastive_loss=1.683, total=4179.21, n_correct=348.09, ppl=16.25, accuracy=8.329, wps=19284.8, ups=1.54, wpb=12484.6, bsz=488.9, num_updates=1700, lr=6.8066e-05, gnorm=1.51, clip=0, loss_scale=64, train_wall=64, gb_free=19, wall=1246
2023-07-27 11:03:00 | INFO | train_inner | epoch 002:    327 / 1474 loss=5.623, trans_loss=5.324, nll_loss=4.016, w2v_ctc_loss=3.717, task_loss=1.325, contrastive_loss=1.392, total=4146.1, n_correct=353.48, ppl=16.18, accuracy=8.526, wps=19250.9, ups=1.56, wpb=12374.1, bsz=447.8, num_updates=1800, lr=7.2064e-05, gnorm=1.392, clip=0, loss_scale=64, train_wall=64, gb_free=18.8, wall=1310
2023-07-27 11:04:04 | INFO | train_inner | epoch 002:    427 / 1474 loss=5.484, trans_loss=5.316, nll_loss=4.01, w2v_ctc_loss=3.619, task_loss=1.456, contrastive_loss=1.215, total=4037.99, n_correct=343.41, ppl=16.12, accuracy=8.504, wps=19046.4, ups=1.58, wpb=12069.2, bsz=415.4, num_updates=1900, lr=7.6062e-05, gnorm=1.444, clip=0, loss_scale=64, train_wall=63, gb_free=19, wall=1374
2023-07-27 11:05:07 | INFO | train_inner | epoch 002:    527 / 1474 loss=5.373, trans_loss=5.305, nll_loss=3.993, w2v_ctc_loss=3.458, task_loss=1.266, contrastive_loss=1.311, total=4176.97, n_correct=360.02, ppl=15.92, accuracy=8.619, wps=19544.4, ups=1.57, wpb=12463.5, bsz=468.6, num_updates=2000, lr=8.006e-05, gnorm=1.283, clip=0, loss_scale=64, train_wall=63, gb_free=19.6, wall=1437
2023-07-27 11:05:07 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 11:05:47 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 8.955 | trans_loss 10.757 | nll_loss 9.683 | w2v_ctc_loss 4.516 | task_loss 7.546 | contrastive_loss 1.646 | total 4003.4 | n_correct 412.9 | ppl 822.19 | accuracy 10.314 | uer 61.423 | wer 59.315 | raw_wer 59.315 | bleu 0.04 | wps 1159.6 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_bleu 0.04
2023-07-27 11:05:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-07-27 11:05:47 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_2_2000.pt
2023-07-27 11:05:50 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_2_2000.pt
2023-07-27 11:06:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.04) (writing took 19.866381529718637 seconds)
2023-07-27 11:07:13 | INFO | train_inner | epoch 002:    627 / 1474 loss=5.24, trans_loss=5.297, nll_loss=3.983, w2v_ctc_loss=3.353, task_loss=1.309, contrastive_loss=1.111, total=4126.49, n_correct=366.42, ppl=15.82, accuracy=8.88, wps=9844.9, ups=0.8, wpb=12314.9, bsz=445.5, num_updates=2100, lr=8.4058e-05, gnorm=1.17, clip=0, loss_scale=64, train_wall=65, gb_free=19.2, wall=1563
2023-07-27 11:08:16 | INFO | train_inner | epoch 002:    727 / 1474 loss=5.168, trans_loss=5.281, nll_loss=3.967, w2v_ctc_loss=3.268, task_loss=1.283, contrastive_loss=1.212, total=4149.06, n_correct=373, ppl=15.63, accuracy=8.99, wps=19570.3, ups=1.58, wpb=12386.4, bsz=465.4, num_updates=2200, lr=8.8056e-05, gnorm=1.14, clip=0, loss_scale=64, train_wall=63, gb_free=19.2, wall=1626
2023-07-27 11:09:20 | INFO | train_inner | epoch 002:    827 / 1474 loss=5.078, trans_loss=5.265, nll_loss=3.948, w2v_ctc_loss=3.198, task_loss=1.317, contrastive_loss=1.161, total=4175.4, n_correct=385.64, ppl=15.43, accuracy=9.236, wps=19465.7, ups=1.56, wpb=12471.9, bsz=460.9, num_updates=2300, lr=9.2054e-05, gnorm=1.031, clip=0, loss_scale=128, train_wall=64, gb_free=19.8, wall=1690
2023-07-27 11:10:24 | INFO | train_inner | epoch 002:    927 / 1474 loss=4.987, trans_loss=5.254, nll_loss=3.932, w2v_ctc_loss=3.105, task_loss=1.344, contrastive_loss=1.143, total=4104.2, n_correct=379.76, ppl=15.26, accuracy=9.253, wps=19167.5, ups=1.56, wpb=12253.1, bsz=445.9, num_updates=2400, lr=9.6052e-05, gnorm=1.027, clip=0, loss_scale=128, train_wall=63, gb_free=19, wall=1754
2023-07-27 11:11:29 | INFO | train_inner | epoch 002:   1027 / 1474 loss=4.901, trans_loss=5.246, nll_loss=3.925, w2v_ctc_loss=3.034, task_loss=1.305, contrastive_loss=0.996, total=4102.5, n_correct=387.01, ppl=15.19, accuracy=9.434, wps=18883.2, ups=1.54, wpb=12251.2, bsz=456.3, num_updates=2500, lr=0.00010005, gnorm=0.894, clip=0, loss_scale=128, train_wall=64, gb_free=19.2, wall=1819
2023-07-27 11:12:33 | INFO | train_inner | epoch 002:   1127 / 1474 loss=4.858, trans_loss=5.24, nll_loss=3.916, w2v_ctc_loss=2.943, task_loss=1.187, contrastive_loss=1.207, total=4187.61, n_correct=400.59, ppl=15.1, accuracy=9.566, wps=19516.5, ups=1.56, wpb=12495.7, bsz=487.1, num_updates=2600, lr=0.000104048, gnorm=0.906, clip=0, loss_scale=128, train_wall=64, gb_free=19.5, wall=1883
2023-07-27 11:13:37 | INFO | train_inner | epoch 002:   1227 / 1474 loss=4.8, trans_loss=5.227, nll_loss=3.901, w2v_ctc_loss=2.9, task_loss=1.194, contrastive_loss=1.13, total=4221.06, n_correct=416.95, ppl=14.94, accuracy=9.878, wps=19573.2, ups=1.55, wpb=12596.1, bsz=492.8, num_updates=2700, lr=0.000108046, gnorm=0.816, clip=0, loss_scale=128, train_wall=64, gb_free=19.5, wall=1947
2023-07-27 11:14:41 | INFO | train_inner | epoch 002:   1327 / 1474 loss=4.707, trans_loss=5.219, nll_loss=3.895, w2v_ctc_loss=2.865, task_loss=1.259, contrastive_loss=0.838, total=4157.86, n_correct=415.41, ppl=14.88, accuracy=9.991, wps=19559.5, ups=1.57, wpb=12425.5, bsz=460.7, num_updates=2800, lr=0.000112044, gnorm=0.78, clip=0, loss_scale=128, train_wall=63, gb_free=19.5, wall=2011
2023-07-27 11:15:46 | INFO | train_inner | epoch 002:   1427 / 1474 loss=4.67, trans_loss=5.225, nll_loss=3.902, w2v_ctc_loss=2.823, task_loss=1.414, contrastive_loss=0.927, total=4054.34, n_correct=400.57, ppl=14.95, accuracy=9.88, wps=18478.5, ups=1.53, wpb=12107, bsz=438.8, num_updates=2900, lr=0.000116042, gnorm=0.73, clip=0, loss_scale=128, train_wall=65, gb_free=19.4, wall=2076
2023-07-27 11:16:16 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 11:16:56 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 8.186 | trans_loss 10.244 | nll_loss 9.058 | w2v_ctc_loss 3.616 | task_loss 7.547 | contrastive_loss 0.989 | total 4003.4 | n_correct 505.6 | ppl 533.14 | accuracy 12.629 | uer 51.952 | wer 50.725 | raw_wer 50.725 | bleu 0.12 | wps 1135.3 | wpb 4003.4 | bsz 141.8 | num_updates 2947 | best_bleu 0.12
2023-07-27 11:16:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2947 updates
2023-07-27 11:16:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt
2023-07-27 11:17:06 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt
2023-07-27 11:17:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt (epoch 2 @ 2947 updates, score 0.12) (writing took 19.397851856425405 seconds)
2023-07-27 11:17:15 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-07-27 11:17:15 | INFO | train | epoch 002 | loss 5.186 | trans_loss 5.276 | nll_loss 3.96 | w2v_ctc_loss 3.291 | task_loss 1.292 | contrastive_loss 1.215 | total 4138.65 | n_correct 376.685 | ppl 15.56 | accuracy 9.102 | wps 16931.6 | ups 1.37 | wpb 12355.8 | bsz 458.5 | num_updates 2947 | lr 0.000117921 | gnorm 1.127 | clip 0 | loss_scale 128 | train_wall 939 | gb_free 19.3 | wall 2166
2023-07-27 11:17:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 11:17:16 | INFO | fairseq.trainer | begin training epoch 3
2023-07-27 11:17:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 11:17:58 | INFO | train_inner | epoch 003:     53 / 1474 loss=4.595, trans_loss=5.197, nll_loss=3.867, w2v_ctc_loss=2.762, task_loss=1.324, contrastive_loss=0.827, total=4071.2, n_correct=417.3, ppl=14.59, accuracy=10.25, wps=9190.8, ups=0.76, wpb=12154.1, bsz=442.6, num_updates=3000, lr=0.00012004, gnorm=0.72, clip=0, loss_scale=128, train_wall=63, gb_free=19.1, wall=2208
2023-07-27 11:17:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-27 11:18:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-07-27 11:18:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-07-27 11:18:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-07-27 11:18:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-07-27 11:19:34 | INFO | train_inner | epoch 003:    158 / 1474 loss=3.784, trans_loss=4.335, nll_loss=2.739, w2v_ctc_loss=2.43, task_loss=0.91, contrastive_loss=0.842, total=4140.74, n_correct=1196.47, ppl=6.68, accuracy=28.895, wps=12868.3, ups=1.04, wpb=12365, bsz=456.9, num_updates=3100, lr=0.000124038, gnorm=2.159, clip=1, loss_scale=4, train_wall=96, gb_free=16.5, wall=2305
2023-07-27 11:21:08 | INFO | train_inner | epoch 003:    258 / 1474 loss=3.359, trans_loss=4.12, nll_loss=2.461, w2v_ctc_loss=2.155, task_loss=0.915, contrastive_loss=0.687, total=4161.13, n_correct=1463.13, ppl=5.51, accuracy=35.162, wps=13267.5, ups=1.07, wpb=12431.5, bsz=467, num_updates=3200, lr=0.000128036, gnorm=1.42, clip=0, loss_scale=4, train_wall=93, gb_free=17.1, wall=2398
2023-07-27 11:22:41 | INFO | train_inner | epoch 003:    358 / 1474 loss=3.232, trans_loss=4.071, nll_loss=2.392, w2v_ctc_loss=2.046, task_loss=0.918, contrastive_loss=0.698, total=4150.02, n_correct=1547.82, ppl=5.25, accuracy=37.297, wps=13345.5, ups=1.08, wpb=12384.9, bsz=461.6, num_updates=3300, lr=0.000132034, gnorm=1.297, clip=0, loss_scale=4, train_wall=92, gb_free=17.1, wall=2491
2023-07-27 11:24:14 | INFO | train_inner | epoch 003:    458 / 1474 loss=3.102, trans_loss=4.02, nll_loss=2.326, w2v_ctc_loss=1.956, task_loss=0.89, contrastive_loss=0.538, total=4209.57, n_correct=1647.7, ppl=5.01, accuracy=39.142, wps=13481.3, ups=1.07, wpb=12566, bsz=476.8, num_updates=3400, lr=0.000136032, gnorm=1.173, clip=0, loss_scale=4, train_wall=93, gb_free=16.1, wall=2584
2023-07-27 11:25:46 | INFO | train_inner | epoch 003:    558 / 1474 loss=3.011, trans_loss=3.995, nll_loss=2.294, w2v_ctc_loss=1.883, task_loss=0.981, contrastive_loss=0.499, total=4088.48, n_correct=1638.07, ppl=4.91, accuracy=40.066, wps=13281.2, ups=1.09, wpb=12212.5, bsz=439.7, num_updates=3500, lr=0.00014003, gnorm=1.125, clip=0, loss_scale=4, train_wall=92, gb_free=17.7, wall=2676
2023-07-27 11:27:21 | INFO | train_inner | epoch 003:    658 / 1474 loss=2.951, trans_loss=3.961, nll_loss=2.246, w2v_ctc_loss=1.806, task_loss=0.882, contrastive_loss=0.612, total=4221.58, n_correct=1751.75, ppl=4.74, accuracy=41.495, wps=13319.4, ups=1.06, wpb=12587.8, bsz=481.9, num_updates=3600, lr=0.000144028, gnorm=1.01, clip=0, loss_scale=4, train_wall=94, gb_free=16.4, wall=2771
2023-07-27 11:28:53 | INFO | train_inner | epoch 003:    758 / 1474 loss=2.877, trans_loss=3.93, nll_loss=2.21, w2v_ctc_loss=1.781, task_loss=0.879, contrastive_loss=0.377, total=4167.41, n_correct=1773.31, ppl=4.63, accuracy=42.552, wps=13498.9, ups=1.08, wpb=12447.6, bsz=472.6, num_updates=3700, lr=0.000148026, gnorm=1.068, clip=0, loss_scale=4, train_wall=92, gb_free=16.3, wall=2863
2023-07-27 11:30:26 | INFO | train_inner | epoch 003:    858 / 1474 loss=2.822, trans_loss=3.918, nll_loss=2.192, w2v_ctc_loss=1.735, task_loss=0.931, contrastive_loss=0.335, total=4165.53, n_correct=1797.71, ppl=4.57, accuracy=43.157, wps=13386.6, ups=1.08, wpb=12437.8, bsz=456.1, num_updates=3800, lr=0.000152024, gnorm=0.973, clip=0, loss_scale=4, train_wall=92, gb_free=17.1, wall=2956
2023-07-27 11:31:59 | INFO | train_inner | epoch 003:    958 / 1474 loss=2.794, trans_loss=3.9, nll_loss=2.167, w2v_ctc_loss=1.709, task_loss=0.895, contrastive_loss=0.364, total=4162.3, n_correct=1838.39, ppl=4.49, accuracy=44.168, wps=13258.7, ups=1.07, wpb=12417, bsz=469.2, num_updates=3900, lr=0.000156022, gnorm=0.982, clip=0, loss_scale=4, train_wall=93, gb_free=16.8, wall=3050
2023-07-27 11:33:31 | INFO | train_inner | epoch 003:   1058 / 1474 loss=2.774, trans_loss=3.884, nll_loss=2.149, w2v_ctc_loss=1.707, task_loss=0.983, contrastive_loss=0.324, total=4069.95, n_correct=1811.47, ppl=4.43, accuracy=44.508, wps=13226.7, ups=1.09, wpb=12153.7, bsz=443.6, num_updates=4000, lr=0.00016002, gnorm=1.028, clip=0, loss_scale=4, train_wall=91, gb_free=16.3, wall=3141
2023-07-27 11:33:31 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 11:34:10 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.272 | trans_loss 6.813 | nll_loss 4.526 | w2v_ctc_loss 2 | task_loss 4.353 | contrastive_loss 0.426 | total 4003.4 | n_correct 1760.1 | ppl 23.03 | accuracy 43.965 | uer 29.411 | wer 30.368 | raw_wer 30.368 | bleu 5.25 | wps 1203.9 | wpb 4003.4 | bsz 141.8 | num_updates 4000 | best_bleu 5.25
2023-07-27 11:34:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-07-27 11:34:10 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_3_4000.pt
2023-07-27 11:34:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_3_4000.pt
2023-07-27 11:34:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 5.25) (writing took 20.077934173867106 seconds)
2023-07-27 11:36:01 | INFO | train_inner | epoch 003:   1158 / 1474 loss=2.725, trans_loss=3.876, nll_loss=2.137, w2v_ctc_loss=1.66, task_loss=0.998, contrastive_loss=0.296, total=4038.49, n_correct=1816.35, ppl=4.4, accuracy=44.976, wps=8055.5, ups=0.67, wpb=12054.8, bsz=432.5, num_updates=4100, lr=0.000164018, gnorm=0.948, clip=0, loss_scale=4, train_wall=91, gb_free=16.4, wall=3291
2023-07-27 11:37:34 | INFO | train_inner | epoch 003:   1258 / 1474 loss=2.688, trans_loss=3.858, nll_loss=2.114, w2v_ctc_loss=1.629, task_loss=0.977, contrastive_loss=0.279, total=4064.31, n_correct=1851.61, ppl=4.33, accuracy=45.558, wps=13081.5, ups=1.08, wpb=12136.8, bsz=433.9, num_updates=4200, lr=0.000168016, gnorm=0.897, clip=0, loss_scale=4, train_wall=92, gb_free=17.3, wall=3384
2023-07-27 11:39:07 | INFO | train_inner | epoch 003:   1358 / 1474 loss=2.675, trans_loss=3.844, nll_loss=2.096, w2v_ctc_loss=1.596, task_loss=0.932, contrastive_loss=0.39, total=4134.58, n_correct=1908.12, ppl=4.28, accuracy=46.15, wps=13193.1, ups=1.07, wpb=12343.8, bsz=460.7, num_updates=4300, lr=0.000172014, gnorm=0.929, clip=0, loss_scale=4, train_wall=93, gb_free=17.8, wall=3477
2023-07-27 11:40:40 | INFO | train_inner | epoch 003:   1458 / 1474 loss=2.65, trans_loss=3.835, nll_loss=2.086, w2v_ctc_loss=1.577, task_loss=0.88, contrastive_loss=0.371, total=4209.94, n_correct=1959.6, ppl=4.25, accuracy=46.547, wps=13517.3, ups=1.08, wpb=12573.5, bsz=477.4, num_updates=4400, lr=0.000176012, gnorm=0.872, clip=0, loss_scale=4, train_wall=93, gb_free=17, wall=3570
2023-07-27 11:40:55 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 11:41:33 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.032 | trans_loss 6.536 | nll_loss 4.153 | w2v_ctc_loss 1.84 | task_loss 4.192 | contrastive_loss 0.406 | total 4003.4 | n_correct 1914.2 | ppl 17.79 | accuracy 47.814 | uer 29.13 | wer 29.443 | raw_wer 29.443 | bleu 9.06 | wps 1135.2 | wpb 4003.4 | bsz 141.8 | num_updates 4416 | best_bleu 9.06
2023-07-27 11:41:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4416 updates
2023-07-27 11:41:33 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt
2023-07-27 11:41:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt
2023-07-27 11:41:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt (epoch 3 @ 4416 updates, score 9.06) (writing took 20.31464970111847 seconds)
2023-07-27 11:41:53 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-07-27 11:41:53 | INFO | train | epoch 003 | loss 3.016 | trans_loss 4.011 | nll_loss 2.314 | w2v_ctc_loss 1.864 | task_loss 0.939 | contrastive_loss 0.488 | total 4139.81 | n_correct 1669.52 | ppl 4.97 | accuracy 40.328 | wps 12287.7 | ups 0.99 | wpb 12359.4 | bsz 458.9 | num_updates 4416 | lr 0.000176652 | gnorm 1.117 | clip 0.1 | loss_scale 4 | train_wall 1345 | gb_free 16.4 | wall 3643
2023-07-27 11:41:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 11:41:53 | INFO | fairseq.trainer | begin training epoch 4
2023-07-27 11:41:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 11:43:19 | INFO | train_inner | epoch 004:     84 / 1474 loss=2.57, trans_loss=3.801, nll_loss=2.039, w2v_ctc_loss=1.527, task_loss=0.956, contrastive_loss=0.221, total=4099.41, n_correct=1947.98, ppl=4.11, accuracy=47.519, wps=7705.5, ups=0.63, wpb=12237, bsz=439.5, num_updates=4500, lr=0.00018001, gnorm=0.844, clip=0, loss_scale=4, train_wall=92, gb_free=16.3, wall=3729
2023-07-27 11:44:51 | INFO | train_inner | epoch 004:    184 / 1474 loss=2.553, trans_loss=3.782, nll_loss=2.014, w2v_ctc_loss=1.51, task_loss=0.882, contrastive_loss=0.247, total=4175.15, n_correct=2011.49, ppl=4.04, accuracy=48.178, wps=13512.5, ups=1.08, wpb=12464.9, bsz=468.3, num_updates=4600, lr=0.000184008, gnorm=0.851, clip=0, loss_scale=4, train_wall=92, gb_free=16.6, wall=3822
2023-07-27 11:46:24 | INFO | train_inner | epoch 004:    284 / 1474 loss=2.57, trans_loss=3.789, nll_loss=2.027, w2v_ctc_loss=1.508, task_loss=0.926, contrastive_loss=0.373, total=4145.23, n_correct=1987.97, ppl=4.07, accuracy=47.958, wps=13391.9, ups=1.08, wpb=12382.4, bsz=463, num_updates=4700, lr=0.000188006, gnorm=0.831, clip=0, loss_scale=4, train_wall=92, gb_free=15.9, wall=3914
2023-07-27 11:47:56 | INFO | train_inner | epoch 004:    384 / 1474 loss=2.531, trans_loss=3.787, nll_loss=2.02, w2v_ctc_loss=1.492, task_loss=0.966, contrastive_loss=0.214, total=4127.66, n_correct=1992.59, ppl=4.06, accuracy=48.274, wps=13372.5, ups=1.09, wpb=12314.6, bsz=443.5, num_updates=4800, lr=0.000192004, gnorm=0.832, clip=0, loss_scale=4, train_wall=92, gb_free=17.4, wall=4006
2023-07-27 11:49:30 | INFO | train_inner | epoch 004:    484 / 1474 loss=2.562, trans_loss=3.774, nll_loss=2.005, w2v_ctc_loss=1.455, task_loss=0.84, contrastive_loss=0.616, total=4218.78, n_correct=2055.53, ppl=4.01, accuracy=48.723, wps=13448.3, ups=1.07, wpb=12592.4, bsz=497.8, num_updates=4900, lr=0.000196002, gnorm=0.83, clip=0, loss_scale=4, train_wall=93, gb_free=16.5, wall=4100
2023-07-27 11:51:03 | INFO | train_inner | epoch 004:    584 / 1474 loss=2.522, trans_loss=3.765, nll_loss=1.994, w2v_ctc_loss=1.479, task_loss=0.874, contrastive_loss=0.293, total=4217.52, n_correct=2073.08, ppl=3.98, accuracy=49.154, wps=13451.5, ups=1.07, wpb=12591.1, bsz=485.9, num_updates=5000, lr=0.0002, gnorm=0.824, clip=0, loss_scale=4, train_wall=93, gb_free=16, wall=4193
Mixup rate:0.5, token after shrink shape:torch.Size([24, 55]), X shape:torch.Size([24, 55, 512])
CTC Tokens:tensor([  0,   0,   0, 150,   0], device='cuda:0'), Shrink Mask:tensor([ True, False, False,  True,  True], device='cuda:0'), New Tokens:tensor([  0, 150,   0, 115,   0], device='cuda:0'), Org X:tensor([[ 1.2666, -0.2341,  0.4304,  ...,  0.2939,  0.7573, -0.5049],
        [-0.0263,  0.3164, -0.4146,  ..., -0.6719, -0.4536, -1.8203],
        [ 1.2148,  1.4600,  1.5400,  ..., -2.7578, -0.2498, -1.7656],
        [ 0.3345,  1.5459,  0.3105,  ..., -0.4812, -1.3154,  0.7686],
        [ 0.9707,  1.3838,  0.9395,  ..., -1.3340, -1.2666,  0.6909]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
New X:tensor([[ 1.2666, -0.2341,  0.4304,  ...,  0.2939,  0.7573, -0.5049],
        [-0.6685, -0.4343, -0.8218,  ..., -0.3999, -1.5840, -2.6621],
        [-0.6279, -1.1045, -0.4160,  ..., -2.0996, -1.4814, -1.7881],
        [ 0.3345,  1.5459,  0.3105,  ..., -0.4812, -1.3154,  0.7686],
        [ 0.9707,  1.3838,  0.9395,  ..., -1.3340, -1.2666,  0.6909]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.3714, device='cuda:0')
2023-07-27 11:52:38 | INFO | train_inner | epoch 004:    684 / 1474 loss=2.49, trans_loss=3.768, nll_loss=1.993, w2v_ctc_loss=1.437, task_loss=0.954, contrastive_loss=0.336, total=4176.39, n_correct=2060.14, ppl=3.98, accuracy=49.328, wps=13193.4, ups=1.06, wpb=12448.9, bsz=455.8, num_updates=5100, lr=0.00019803, gnorm=0.487, clip=0, loss_scale=8, train_wall=94, gb_free=17.1, wall=4288
2023-07-27 11:54:11 | INFO | train_inner | epoch 004:    784 / 1474 loss=2.473, trans_loss=3.756, nll_loss=1.983, w2v_ctc_loss=1.45, task_loss=1.024, contrastive_loss=0.203, total=4026.63, n_correct=1999.89, ppl=3.95, accuracy=49.667, wps=12910.7, ups=1.07, wpb=12025, bsz=420.6, num_updates=5200, lr=0.000196116, gnorm=0.474, clip=0, loss_scale=8, train_wall=93, gb_free=13.1, wall=4381
2023-07-27 11:55:44 | INFO | train_inner | epoch 004:    884 / 1474 loss=2.497, trans_loss=3.747, nll_loss=1.973, w2v_ctc_loss=1.447, task_loss=0.928, contrastive_loss=0.386, total=4186.04, n_correct=2087.88, ppl=3.93, accuracy=49.877, wps=13465.2, ups=1.08, wpb=12501.4, bsz=466.3, num_updates=5300, lr=0.000194257, gnorm=0.475, clip=0, loss_scale=8, train_wall=92, gb_free=17.7, wall=4474
2023-07-27 11:57:17 | INFO | train_inner | epoch 004:    984 / 1474 loss=2.455, trans_loss=3.737, nll_loss=1.96, w2v_ctc_loss=1.427, task_loss=0.942, contrastive_loss=0.255, total=4125.02, n_correct=2074.76, ppl=3.89, accuracy=50.297, wps=13260, ups=1.08, wpb=12321, bsz=457.1, num_updates=5400, lr=0.00019245, gnorm=0.461, clip=0, loss_scale=8, train_wall=92, gb_free=12.7, wall=4567
2023-07-27 11:58:50 | INFO | train_inner | epoch 004:   1084 / 1474 loss=2.458, trans_loss=3.748, nll_loss=1.972, w2v_ctc_loss=1.434, task_loss=1.005, contrastive_loss=0.227, total=4075.6, n_correct=2047.04, ppl=3.92, accuracy=50.227, wps=13028.8, ups=1.07, wpb=12163.4, bsz=435.7, num_updates=5500, lr=0.000190693, gnorm=0.477, clip=0, loss_scale=8, train_wall=93, gb_free=16, wall=4660
2023-07-27 12:00:22 | INFO | train_inner | epoch 004:   1184 / 1474 loss=2.463, trans_loss=3.735, nll_loss=1.96, w2v_ctc_loss=1.422, task_loss=0.874, contrastive_loss=0.345, total=4161.18, n_correct=2102.34, ppl=3.89, accuracy=50.523, wps=13527.2, ups=1.09, wpb=12431.8, bsz=483.4, num_updates=5600, lr=0.000188982, gnorm=0.459, clip=0, loss_scale=8, train_wall=91, gb_free=16.8, wall=4752
2023-07-27 12:01:54 | INFO | train_inner | epoch 004:   1284 / 1474 loss=2.437, trans_loss=3.726, nll_loss=1.946, w2v_ctc_loss=1.403, task_loss=0.887, contrastive_loss=0.306, total=4156.53, n_correct=2115.37, ppl=3.85, accuracy=50.893, wps=13422.3, ups=1.08, wpb=12411.4, bsz=472.7, num_updates=5700, lr=0.000187317, gnorm=0.457, clip=0, loss_scale=8, train_wall=92, gb_free=15.8, wall=4844
2023-07-27 12:03:26 | INFO | train_inner | epoch 004:   1384 / 1474 loss=2.412, trans_loss=3.724, nll_loss=1.944, w2v_ctc_loss=1.405, task_loss=0.955, contrastive_loss=0.18, total=4101.23, n_correct=2094.49, ppl=3.85, accuracy=51.07, wps=13421.7, ups=1.1, wpb=12249, bsz=437.6, num_updates=5800, lr=0.000185695, gnorm=0.451, clip=0, loss_scale=8, train_wall=91, gb_free=15.6, wall=4936
2023-07-27 12:04:49 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([8, 110]), X shape:torch.Size([8, 110, 512])
CTC Tokens:tensor([25, 25, 73, 73,  0], device='cuda:7'), Shrink Mask:tensor([ True, False,  True, False,  True], device='cuda:7'), New Tokens:tensor([ 25,  73,   0, 598,   0], device='cuda:7'), Org X:tensor([[ 0.5967, -0.0851, -2.2949,  ..., -0.3252,  1.8984, -0.5234],
        [ 0.2202, -0.1929,  0.2355,  ...,  0.4490,  1.2021,  0.1925],
        [ 1.0811,  1.1123,  0.8267,  ...,  0.4858,  1.1211,  0.6714],
        [ 1.4648,  1.6484,  0.6616,  ...,  0.2231,  0.8867,  0.2361],
        [ 2.4004,  1.5889,  1.9482,  ..., -0.6226, -0.1277,  0.4719]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
New X:tensor([[ 0.5967, -0.0851, -2.2949,  ..., -0.3252,  1.8984, -0.5234],
        [-0.5615, -0.6143, -0.4375,  ...,  3.6680, -1.8994, -1.8115],
        [-0.6279, -1.1045, -0.4160,  ..., -2.0996, -1.4814, -1.7881],
        [ 1.4648,  1.6484,  0.6616,  ...,  0.2231,  0.8867,  0.2361],
        [ 2.4004,  1.5889,  1.9482,  ..., -0.6226, -0.1277,  0.4719]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.3714, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([24, 58]), X shape:torch.Size([24, 58, 512])
CTC Tokens:tensor([   0,  120,   13,    6, 8396], device='cuda:6'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:6'), New Tokens:tensor([   0,  120,   13,    6, 8396], device='cuda:6'), Org X:tensor([[ 0.2942, -0.1193,  0.8359,  ...,  0.2454, -0.4497,  0.1848],
        [-0.2625, -0.2012, -1.1543,  ...,  0.3855, -0.6963, -1.2354],
        [-0.5063, -0.7314, -1.3252,  ..., -0.2368,  0.7148, -0.7520],
        [-1.1338, -0.6870,  1.1611,  ..., -1.2100,  1.8398, -0.3015],
        [-1.2295, -0.3621,  1.8496,  ..., -0.1924,  0.1093, -0.4521]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
New X:tensor([[ 0.2942, -0.1193,  0.8359,  ...,  0.2454, -0.4497,  0.1848],
        [-0.2625, -0.2012, -1.1543,  ...,  0.3855, -0.6963, -1.2354],
        [-0.5063, -0.7314, -1.3252,  ..., -0.2368,  0.7148, -0.7520],
        [-1.1338, -0.6870,  1.1611,  ..., -1.2100,  1.8398, -0.3015],
        [-1.2295, -0.3621,  1.8496,  ..., -0.1924,  0.1093, -0.4521]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.3714, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 61]), X shape:torch.Size([16, 61, 512])
CTC Tokens:tensor([   0,    0, 6101,    0,    0], device='cuda:2'), Shrink Mask:tensor([ True, False,  True,  True, False], device='cuda:2'), New Tokens:tensor([   0, 6101,    0, 1313,    0], device='cuda:2'), Org X:tensor([[ 0.8491,  0.1903,  1.6426,  ..., -1.2393,  2.3398, -0.7783],
        [-0.9956,  0.0497,  1.9277,  ..., -1.0137,  0.6807, -0.5854],
        [ 0.0822,  0.5210,  2.0586,  ..., -1.5596,  0.7324, -0.9292],
        [ 1.1260, -1.3945, -0.3572,  ...,  0.1321, -2.9336,  0.8193],
        [ 1.0889,  0.3037,  0.5723,  ..., -0.2402, -0.2151,  0.5005]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
New X:tensor([[ 0.8491,  0.1903,  1.6426,  ..., -1.2393,  2.3398, -0.7783],
        [-0.9956,  0.0497,  1.9277,  ..., -1.0137,  0.6807, -0.5854],
        [ 0.0822,  0.5210,  2.0586,  ..., -1.5596,  0.7324, -0.9292],
        [ 1.6104, -0.9341, -0.2563,  ...,  3.4863, -0.1211, -2.7305],
        [-0.6279, -1.1045, -0.4160,  ..., -2.0996, -1.4814, -1.7881]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.3714, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 88]), X shape:torch.Size([8, 88, 512])
CTC Tokens:tensor([ 67, 103, 103,  25,  25], device='cuda:1'), Shrink Mask:tensor([ True,  True, False,  True, False], device='cuda:1'), New Tokens:tensor([ 67, 103,  25,   0, 305], device='cuda:1'), Org X:tensor([[-0.2004,  0.1365, -1.1133,  ...,  0.4189, -0.0875, -2.7793],
        [-1.4268, -0.4336, -1.0137,  ...,  0.4526,  2.2070, -2.5977],
        [ 0.3567, -0.4487, -2.5645,  ..., -0.3062,  2.8711, -1.3076],
        [ 0.7739,  0.0908,  0.2488,  ...,  0.2126,  0.6602, -0.7827],
        [-0.1008,  0.2207, -0.2839,  ...,  0.1851,  0.1426, -1.2754]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
New X:tensor([[-0.0232, -0.4785, -1.0879,  ...,  1.4912, -1.5244, -4.4961],
        [ 0.5918, -1.2861, -0.4636,  ...,  1.4307, -0.0098, -0.9404],
        [ 0.3567, -0.4487, -2.5645,  ..., -0.3062,  2.8711, -1.3076],
        [-0.6279, -1.1045, -0.4160,  ..., -2.0996, -1.4814, -1.7881],
        [-0.1008,  0.2207, -0.2839,  ...,  0.1851,  0.1426, -1.2754]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.3714, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([40, 34]), X shape:torch.Size([40, 34, 512])
CTC Tokens:tensor([ 24,  24,   0, 135,   0], device='cuda:3'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:3'), New Tokens:tensor([ 24,   0, 135,   0, 117], device='cuda:3'), Org X:tensor([[-0.4822,  0.5322, -0.3193,  ...,  0.0880,  1.7373, -0.8979],
        [ 0.5269,  0.8643,  0.2184,  ...,  0.3340,  0.2969, -0.0867],
        [ 0.6187,  0.8506,  0.3132,  ...,  0.1218, -0.6245, -0.3157],
        [ 0.7671,  1.5303,  0.1569,  ..., -1.2178, -1.1719, -0.5293],
        [-1.1631,  1.2607, -1.4980,  ..., -1.1289, -0.1951, -0.0091]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
New X:tensor([[-0.7373, -0.4250,  0.5723,  ..., -4.5156,  1.5996, -1.4570],
        [-0.6279, -1.1045, -0.4160,  ..., -2.0996, -1.4814, -1.7881],
        [ 0.6187,  0.8506,  0.3132,  ...,  0.1218, -0.6245, -0.3157],
        [ 0.7671,  1.5303,  0.1569,  ..., -1.2178, -1.1719, -0.5293],
        [-1.1631,  1.2607, -1.4980,  ..., -1.1289, -0.1951, -0.0091]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.3714, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([64, 28]), X shape:torch.Size([64, 28, 512])
CTC Tokens:tensor([ 8,  8,  0,  0, 19], device='cuda:5'), Shrink Mask:tensor([ True, False,  True, False,  True], device='cuda:5'), New Tokens:tensor([  8,   0,  19,   0, 192], device='cuda:5'), Org X:tensor([[ 0.4622, -0.0485, -0.1755,  ...,  0.3577,  0.1279, -0.9009],
        [ 0.4409,  0.7080,  1.4258,  ..., -0.5254,  1.6035, -0.7983],
        [ 0.3025,  1.1641, -0.1051,  ..., -0.0501,  0.4792, -1.5039],
        [ 0.7051,  1.3291,  1.3223,  ..., -0.3970,  1.5820, -1.2959],
        [ 0.2539, -0.2910, -0.1497,  ...,  0.3574, -1.1533, -0.8237]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
New X:tensor([[ 0.4622, -0.0485, -0.1755,  ...,  0.3577,  0.1279, -0.9009],
        [-0.6279, -1.1045, -0.4160,  ..., -2.0996, -1.4814, -1.7881],
        [ 0.3025,  1.1641, -0.1051,  ..., -0.0501,  0.4792, -1.5039],
        [ 0.7051,  1.3291,  1.3223,  ..., -0.3970,  1.5820, -1.2959],
        [-0.0161, -0.6030,  1.3125,  ..., -0.9121, -0.0493, -3.6855]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.3714, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([32, 48]), X shape:torch.Size([32, 48, 512])
CTC Tokens:tensor([   0,   29,    0,    0, 3296], device='cuda:4'), Shrink Mask:tensor([ True,  True,  True, False,  True], device='cuda:4'), New Tokens:tensor([   0,   29,    0, 3296,    0], device='cuda:4'), Org X:tensor([[ 0.3027,  0.3191,  0.0298,  ...,  0.6387, -1.5635,  0.1534],
        [-1.2158,  0.6855, -0.0674,  ...,  0.2559, -1.3838, -0.7222],
        [ 0.2443,  0.4824,  0.5181,  ..., -0.2180,  0.3860, -0.8564],
        [-0.3259,  0.4412, -0.0947,  ..., -0.0986,  1.2090,  1.0439],
        [ 0.3979,  0.9619,  0.8037,  ..., -0.4480,  2.1680,  0.5981]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
New X:tensor([[-0.6279, -1.1045, -0.4160,  ..., -2.0996, -1.4814, -1.7881],
        [-1.0439,  0.1560, -0.4644,  ..., -0.2225, -2.1992,  3.5742],
        [ 0.2443,  0.4824,  0.5181,  ..., -0.2180,  0.3860, -0.8564],
        [ 0.8789,  0.1852, -0.4702,  ...,  2.0898, -0.0845,  3.3320],
        [ 0.3979,  0.9619,  0.8037,  ..., -0.4480,  2.1680,  0.5981]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.3714, device='cuda:4')
2023-07-27 12:05:22 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 4.635 | trans_loss 6.094 | nll_loss 3.546 | w2v_ctc_loss 1.561 | task_loss 4.474 | contrastive_loss 0.329 | total 4003.4 | n_correct 2147.5 | ppl 11.68 | accuracy 53.642 | uer 23.587 | wer 25.197 | raw_wer 25.197 | bleu 14.72 | wps 1321.7 | wpb 4003.4 | bsz 141.8 | num_updates 5890 | best_bleu 14.72
2023-07-27 12:05:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5890 updates
2023-07-27 12:05:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt
2023-07-27 12:05:33 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt
2023-07-27 12:05:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt (epoch 4 @ 5890 updates, score 14.72) (writing took 20.585046896710992 seconds)
2023-07-27 12:05:43 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-07-27 12:05:43 | INFO | train | epoch 004 | loss 2.492 | trans_loss 3.757 | nll_loss 1.984 | w2v_ctc_loss 1.451 | task_loss 0.929 | contrastive_loss 0.299 | total 4138.65 | n_correct 2050.34 | ppl 3.96 | accuracy 49.541 | wps 12737.5 | ups 1.03 | wpb 12355.8 | bsz 458.5 | num_updates 5890 | lr 0.000184271 | gnorm 0.612 | clip 0 | loss_scale 8 | train_wall 1359 | gb_free 14.8 | wall 5073
2023-07-27 12:05:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 12:05:43 | INFO | fairseq.trainer | begin training epoch 5
2023-07-27 12:05:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 12:06:00 | INFO | train_inner | epoch 005:     10 / 1474 loss=2.391, trans_loss=3.714, nll_loss=1.931, w2v_ctc_loss=1.376, task_loss=0.966, contrastive_loss=0.201, total=4037.7, n_correct=2075.09, ppl=3.81, accuracy=51.393, wps=7786.4, ups=0.65, wpb=12055.9, bsz=439.3, num_updates=5900, lr=0.000184115, gnorm=0.454, clip=0, loss_scale=8, train_wall=91, gb_free=16.9, wall=5090
2023-07-27 12:07:33 | INFO | train_inner | epoch 005:    110 / 1474 loss=2.317, trans_loss=3.656, nll_loss=1.856, w2v_ctc_loss=1.297, task_loss=0.841, contrastive_loss=0.211, total=4247.37, n_correct=2253.26, ppl=3.62, accuracy=53.051, wps=13685.7, ups=1.08, wpb=12683.4, bsz=495.1, num_updates=6000, lr=0.000182574, gnorm=0.434, clip=0, loss_scale=8, train_wall=92, gb_free=16.7, wall=5183
2023-07-27 12:07:33 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 12:08:07 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.632 | trans_loss 6.094 | nll_loss 3.545 | w2v_ctc_loss 1.543 | task_loss 4.461 | contrastive_loss 0.34 | total 4003.4 | n_correct 2152.7 | ppl 11.67 | accuracy 53.772 | uer 23.707 | wer 25.335 | raw_wer 25.335 | bleu 14.29 | wps 1309.1 | wpb 4003.4 | bsz 141.8 | num_updates 6000 | best_bleu 14.72
2023-07-27 12:08:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6000 updates
2023-07-27 12:08:07 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_5_6000.pt
2023-07-27 12:08:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_5_6000.pt
2023-07-27 12:08:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_5_6000.pt (epoch 5 @ 6000 updates, score 14.29) (writing took 15.649940064176917 seconds)
2023-07-27 12:09:55 | INFO | train_inner | epoch 005:    210 / 1474 loss=2.362, trans_loss=3.67, nll_loss=1.871, w2v_ctc_loss=1.316, task_loss=0.861, contrastive_loss=0.438, total=4189.85, n_correct=2208.3, ppl=3.66, accuracy=52.706, wps=8784.1, ups=0.7, wpb=12500.5, bsz=488.2, num_updates=6100, lr=0.000181071, gnorm=0.445, clip=0, loss_scale=8, train_wall=92, gb_free=17.8, wall=5325
2023-07-27 12:11:28 | INFO | train_inner | epoch 005:    310 / 1474 loss=2.344, trans_loss=3.664, nll_loss=1.868, w2v_ctc_loss=1.33, task_loss=0.957, contrastive_loss=0.274, total=4090.1, n_correct=2150.63, ppl=3.65, accuracy=52.581, wps=13255.8, ups=1.08, wpb=12228.1, bsz=443.9, num_updates=6200, lr=0.000179605, gnorm=0.444, clip=0, loss_scale=8, train_wall=92, gb_free=16.2, wall=5418
2023-07-27 12:13:00 | INFO | train_inner | epoch 005:    410 / 1474 loss=2.337, trans_loss=3.656, nll_loss=1.859, w2v_ctc_loss=1.298, task_loss=0.899, contrastive_loss=0.369, total=4147.17, n_correct=2201.21, ppl=3.63, accuracy=53.077, wps=13380.2, ups=1.08, wpb=12395.1, bsz=472.5, num_updates=6300, lr=0.000178174, gnorm=0.446, clip=0, loss_scale=8, train_wall=92, gb_free=14.8, wall=5510
2023-07-27 12:14:32 | INFO | train_inner | epoch 005:    510 / 1474 loss=2.307, trans_loss=3.666, nll_loss=1.868, w2v_ctc_loss=1.308, task_loss=1.047, contrastive_loss=0.149, total=4026.81, n_correct=2130.4, ppl=3.65, accuracy=52.905, wps=13064.2, ups=1.09, wpb=12029.7, bsz=416.6, num_updates=6400, lr=0.000176777, gnorm=0.434, clip=0, loss_scale=8, train_wall=92, gb_free=17.4, wall=5602
2023-07-27 12:16:05 | INFO | train_inner | epoch 005:    610 / 1474 loss=2.328, trans_loss=3.671, nll_loss=1.873, w2v_ctc_loss=1.298, task_loss=0.955, contrastive_loss=0.324, total=4107.75, n_correct=2175.09, ppl=3.66, accuracy=52.951, wps=13176.8, ups=1.08, wpb=12253.8, bsz=451.2, num_updates=6500, lr=0.000175412, gnorm=0.446, clip=0, loss_scale=8, train_wall=92, gb_free=16.1, wall=5695
2023-07-27 12:17:38 | INFO | train_inner | epoch 005:    710 / 1474 loss=2.329, trans_loss=3.665, nll_loss=1.868, w2v_ctc_loss=1.297, task_loss=0.883, contrastive_loss=0.311, total=4178.85, n_correct=2221.35, ppl=3.65, accuracy=53.157, wps=13489.1, ups=1.08, wpb=12473.1, bsz=480.9, num_updates=6600, lr=0.000174078, gnorm=0.434, clip=0, loss_scale=8, train_wall=92, gb_free=17.7, wall=5788
2023-07-27 12:19:11 | INFO | train_inner | epoch 005:    810 / 1474 loss=2.308, trans_loss=3.665, nll_loss=1.866, w2v_ctc_loss=1.291, task_loss=0.957, contrastive_loss=0.229, total=4127.73, n_correct=2192.5, ppl=3.65, accuracy=53.116, wps=13232.1, ups=1.07, wpb=12320.4, bsz=449.2, num_updates=6700, lr=0.000172774, gnorm=0.432, clip=0, loss_scale=8, train_wall=93, gb_free=15.1, wall=5881
2023-07-27 12:20:44 | INFO | train_inner | epoch 005:    910 / 1474 loss=2.287, trans_loss=3.655, nll_loss=1.855, w2v_ctc_loss=1.282, task_loss=0.962, contrastive_loss=0.19, total=4095.48, n_correct=2191.39, ppl=3.62, accuracy=53.508, wps=13196, ups=1.08, wpb=12229.5, bsz=445.3, num_updates=6800, lr=0.000171499, gnorm=0.432, clip=0, loss_scale=8, train_wall=92, gb_free=15.6, wall=5974
2023-07-27 12:22:16 | INFO | train_inner | epoch 005:   1010 / 1474 loss=2.299, trans_loss=3.659, nll_loss=1.86, w2v_ctc_loss=1.281, task_loss=0.92, contrastive_loss=0.272, total=4165.12, n_correct=2225.54, ppl=3.63, accuracy=53.433, wps=13507.7, ups=1.09, wpb=12433.6, bsz=463.5, num_updates=6900, lr=0.000170251, gnorm=0.425, clip=0, loss_scale=8, train_wall=92, gb_free=15.6, wall=6066
2023-07-27 12:23:49 | INFO | train_inner | epoch 005:   1110 / 1474 loss=2.313, trans_loss=3.659, nll_loss=1.859, w2v_ctc_loss=1.292, task_loss=0.921, contrastive_loss=0.277, total=4176.72, n_correct=2242.93, ppl=3.63, accuracy=53.701, wps=13392.5, ups=1.07, wpb=12459.2, bsz=466.1, num_updates=7000, lr=0.000169031, gnorm=0.43, clip=0, loss_scale=8, train_wall=93, gb_free=16.7, wall=6159
2023-07-27 12:25:22 | INFO | train_inner | epoch 005:   1210 / 1474 loss=2.274, trans_loss=3.655, nll_loss=1.853, w2v_ctc_loss=1.267, task_loss=0.949, contrastive_loss=0.178, total=4164.13, n_correct=2240.88, ppl=3.61, accuracy=53.814, wps=13355.3, ups=1.08, wpb=12420.9, bsz=453.8, num_updates=7100, lr=0.000167836, gnorm=0.429, clip=0, loss_scale=8, train_wall=93, gb_free=16.9, wall=6252
2023-07-27 12:26:54 | INFO | train_inner | epoch 005:   1310 / 1474 loss=2.262, trans_loss=3.653, nll_loss=1.853, w2v_ctc_loss=1.258, task_loss=0.947, contrastive_loss=0.144, total=4134.91, n_correct=2224.88, ppl=3.61, accuracy=53.807, wps=13318.5, ups=1.08, wpb=12341.4, bsz=445.6, num_updates=7200, lr=0.000166667, gnorm=0.424, clip=0, loss_scale=16, train_wall=92, gb_free=16.3, wall=6344
2023-07-27 12:28:26 | INFO | train_inner | epoch 005:   1410 / 1474 loss=2.268, trans_loss=3.652, nll_loss=1.853, w2v_ctc_loss=1.252, task_loss=0.941, contrastive_loss=0.213, total=4134.37, n_correct=2224.83, ppl=3.61, accuracy=53.813, wps=13452.6, ups=1.09, wpb=12347.5, bsz=458.5, num_updates=7300, lr=0.000165521, gnorm=0.428, clip=0, loss_scale=16, train_wall=91, gb_free=17.8, wall=6436
2023-07-27 12:29:25 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 12:29:58 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.524 | trans_loss 5.98 | nll_loss 3.4 | w2v_ctc_loss 1.441 | task_loss 4.477 | contrastive_loss 0.332 | total 4003.4 | n_correct 2227.4 | ppl 10.56 | accuracy 55.638 | uer 23.054 | wer 24.664 | raw_wer 24.664 | bleu 15.68 | wps 1356.9 | wpb 4003.4 | bsz 141.8 | num_updates 7364 | best_bleu 15.68
2023-07-27 12:29:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7364 updates
2023-07-27 12:29:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt
2023-07-27 12:30:08 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt
2023-07-27 12:30:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt (epoch 5 @ 7364 updates, score 15.68) (writing took 19.6862960960716 seconds)
2023-07-27 12:30:18 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-07-27 12:30:18 | INFO | train | epoch 005 | loss 2.309 | trans_loss 3.66 | nll_loss 1.861 | w2v_ctc_loss 1.29 | task_loss 0.931 | contrastive_loss 0.256 | total 4138.65 | n_correct 2205.08 | ppl 3.63 | accuracy 53.28 | wps 12349.7 | ups 1 | wpb 12355.8 | bsz 458.5 | num_updates 7364 | lr 0.0001648 | gnorm 0.435 | clip 0 | loss_scale 16 | train_wall 1357 | gb_free 16.2 | wall 6548
2023-07-27 12:30:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 12:30:18 | INFO | fairseq.trainer | begin training epoch 6
2023-07-27 12:30:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 12:31:01 | INFO | train_inner | epoch 006:     36 / 1474 loss=2.258, trans_loss=3.63, nll_loss=1.822, w2v_ctc_loss=1.251, task_loss=0.956, contrastive_loss=0.209, total=4115.45, n_correct=2237.53, ppl=3.54, accuracy=54.369, wps=7915, ups=0.64, wpb=12281.2, bsz=447.9, num_updates=7400, lr=0.000164399, gnorm=0.439, clip=0, loss_scale=16, train_wall=92, gb_free=16.4, wall=6591
2023-07-27 12:32:34 | INFO | train_inner | epoch 006:    136 / 1474 loss=2.212, trans_loss=3.595, nll_loss=1.777, w2v_ctc_loss=1.202, task_loss=0.929, contrastive_loss=0.252, total=4154.25, n_correct=2292.03, ppl=3.43, accuracy=55.173, wps=13320.7, ups=1.07, wpb=12407.4, bsz=456.1, num_updates=7500, lr=0.000163299, gnorm=0.427, clip=0, loss_scale=16, train_wall=93, gb_free=15.5, wall=6685
2023-07-27 12:34:06 | INFO | train_inner | epoch 006:    236 / 1474 loss=2.224, trans_loss=3.608, nll_loss=1.796, w2v_ctc_loss=1.23, task_loss=0.999, contrastive_loss=0.158, total=4112.66, n_correct=2254.02, ppl=3.47, accuracy=54.807, wps=13352.3, ups=1.09, wpb=12287.2, bsz=437.3, num_updates=7600, lr=0.000162221, gnorm=0.423, clip=0, loss_scale=16, train_wall=92, gb_free=16.1, wall=6777
2023-07-27 12:35:40 | INFO | train_inner | epoch 006:    336 / 1474 loss=2.234, trans_loss=3.596, nll_loss=1.78, w2v_ctc_loss=1.179, task_loss=0.867, contrastive_loss=0.468, total=4177.51, n_correct=2310.38, ppl=3.44, accuracy=55.305, wps=13310.9, ups=1.07, wpb=12473.8, bsz=491.3, num_updates=7700, lr=0.000161165, gnorm=0.429, clip=0, loss_scale=16, train_wall=93, gb_free=16, wall=6870
2023-07-27 12:37:12 | INFO | train_inner | epoch 006:    436 / 1474 loss=2.195, trans_loss=3.599, nll_loss=1.784, w2v_ctc_loss=1.192, task_loss=0.895, contrastive_loss=0.176, total=4154.57, n_correct=2303.02, ppl=3.44, accuracy=55.433, wps=13480.9, ups=1.09, wpb=12405.5, bsz=470.2, num_updates=7800, lr=0.000160128, gnorm=0.424, clip=0, loss_scale=16, train_wall=92, gb_free=16.1, wall=6962
2023-07-27 12:38:45 | INFO | train_inner | epoch 006:    536 / 1474 loss=2.203, trans_loss=3.605, nll_loss=1.79, w2v_ctc_loss=1.207, task_loss=0.937, contrastive_loss=0.162, total=4167.79, n_correct=2308.52, ppl=3.46, accuracy=55.39, wps=13472.7, ups=1.08, wpb=12438.5, bsz=455.2, num_updates=7900, lr=0.000159111, gnorm=0.421, clip=0, loss_scale=16, train_wall=92, gb_free=15.7, wall=7055
2023-07-27 12:40:16 | INFO | train_inner | epoch 006:    636 / 1474 loss=2.199, trans_loss=3.608, nll_loss=1.795, w2v_ctc_loss=1.185, task_loss=0.883, contrastive_loss=0.221, total=4146.17, n_correct=2292.54, ppl=3.47, accuracy=55.293, wps=13518.4, ups=1.09, wpb=12376.6, bsz=471.6, num_updates=8000, lr=0.000158114, gnorm=0.431, clip=0, loss_scale=16, train_wall=91, gb_free=16.3, wall=7146
2023-07-27 12:40:16 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 12:40:43 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.463 | trans_loss 5.898 | nll_loss 3.281 | w2v_ctc_loss 1.441 | task_loss 4.57 | contrastive_loss 0.307 | total 4003.4 | n_correct 2270.7 | ppl 9.72 | accuracy 56.719 | uer 20.787 | wer 22.449 | raw_wer 22.449 | bleu 17 | wps 1764.2 | wpb 4003.4 | bsz 141.8 | num_updates 8000 | best_bleu 17
2023-07-27 12:40:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8000 updates
2023-07-27 12:40:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_6_8000.pt
2023-07-27 12:40:46 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_6_8000.pt
2023-07-27 12:41:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_6_8000.pt (epoch 6 @ 8000 updates, score 17.0) (writing took 20.398962074890733 seconds)
2023-07-27 12:42:37 | INFO | train_inner | epoch 006:    736 / 1474 loss=2.208, trans_loss=3.611, nll_loss=1.8, w2v_ctc_loss=1.207, task_loss=0.954, contrastive_loss=0.175, total=4148.65, n_correct=2285.06, ppl=3.48, accuracy=55.08, wps=8803.4, ups=0.71, wpb=12388, bsz=453.7, num_updates=8100, lr=0.000157135, gnorm=0.429, clip=0, loss_scale=16, train_wall=93, gb_free=15.5, wall=7287
2023-07-27 12:44:09 | INFO | train_inner | epoch 006:    836 / 1474 loss=2.201, trans_loss=3.618, nll_loss=1.807, w2v_ctc_loss=1.198, task_loss=0.979, contrastive_loss=0.153, total=4114.34, n_correct=2264.8, ppl=3.5, accuracy=55.046, wps=13307.1, ups=1.08, wpb=12282.2, bsz=441.6, num_updates=8200, lr=0.000156174, gnorm=0.421, clip=0, loss_scale=16, train_wall=92, gb_free=15, wall=7379
2023-07-27 12:45:42 | INFO | train_inner | epoch 006:    936 / 1474 loss=2.218, trans_loss=3.617, nll_loss=1.807, w2v_ctc_loss=1.202, task_loss=0.97, contrastive_loss=0.253, total=4081.53, n_correct=2248.95, ppl=3.5, accuracy=55.101, wps=13181.2, ups=1.08, wpb=12181.3, bsz=444.5, num_updates=8300, lr=0.00015523, gnorm=0.436, clip=0, loss_scale=16, train_wall=92, gb_free=17.8, wall=7472
2023-07-27 12:47:14 | INFO | train_inner | epoch 006:   1036 / 1474 loss=2.208, trans_loss=3.604, nll_loss=1.791, w2v_ctc_loss=1.182, task_loss=0.883, contrastive_loss=0.326, total=4165.84, n_correct=2311.59, ppl=3.46, accuracy=55.489, wps=13432.3, ups=1.08, wpb=12435.7, bsz=477.2, num_updates=8400, lr=0.000154303, gnorm=0.436, clip=0, loss_scale=16, train_wall=92, gb_free=16.8, wall=7564
2023-07-27 12:48:46 | INFO | train_inner | epoch 006:   1136 / 1474 loss=2.195, trans_loss=3.608, nll_loss=1.795, w2v_ctc_loss=1.197, task_loss=1.032, contrastive_loss=0.154, total=4072.29, n_correct=2254.12, ppl=3.47, accuracy=55.353, wps=13214.3, ups=1.09, wpb=12157.6, bsz=428, num_updates=8500, lr=0.000153393, gnorm=0.424, clip=0, loss_scale=16, train_wall=91, gb_free=16.9, wall=7656
2023-07-27 12:50:19 | INFO | train_inner | epoch 006:   1236 / 1474 loss=2.224, trans_loss=3.6, nll_loss=1.789, w2v_ctc_loss=1.177, task_loss=0.904, contrastive_loss=0.484, total=4141.55, n_correct=2300.23, ppl=3.46, accuracy=55.54, wps=13262.9, ups=1.07, wpb=12370.9, bsz=474.8, num_updates=8600, lr=0.000152499, gnorm=0.425, clip=0, loss_scale=16, train_wall=93, gb_free=13.1, wall=7750
2023-07-27 12:51:51 | INFO | train_inner | epoch 006:   1336 / 1474 loss=2.173, trans_loss=3.606, nll_loss=1.792, w2v_ctc_loss=1.176, task_loss=0.932, contrastive_loss=0.138, total=4125.31, n_correct=2298.69, ppl=3.46, accuracy=55.722, wps=13447.2, ups=1.09, wpb=12305, bsz=452.6, num_updates=8700, lr=0.00015162, gnorm=0.422, clip=0, loss_scale=16, train_wall=91, gb_free=17.8, wall=7841
2023-07-27 12:53:24 | INFO | train_inner | epoch 006:   1436 / 1474 loss=2.175, trans_loss=3.6, nll_loss=1.787, w2v_ctc_loss=1.179, task_loss=0.93, contrastive_loss=0.149, total=4196.2, n_correct=2340.53, ppl=3.45, accuracy=55.777, wps=13410, ups=1.07, wpb=12525.2, bsz=461.5, num_updates=8800, lr=0.000150756, gnorm=0.415, clip=0, loss_scale=16, train_wall=93, gb_free=11.2, wall=7934
2023-07-27 12:53:59 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 12:54:26 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.412 | trans_loss 5.85 | nll_loss 3.219 | w2v_ctc_loss 1.384 | task_loss 4.558 | contrastive_loss 0.301 | total 4003.4 | n_correct 2290.3 | ppl 9.31 | accuracy 57.209 | uer 20.057 | wer 21.841 | raw_wer 21.841 | bleu 17.05 | wps 1779.9 | wpb 4003.4 | bsz 141.8 | num_updates 8838 | best_bleu 17.05
2023-07-27 12:54:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8838 updates
2023-07-27 12:54:26 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt
2023-07-27 12:54:36 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt
2023-07-27 12:54:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt (epoch 6 @ 8838 updates, score 17.05) (writing took 19.900816030800343 seconds)
2023-07-27 12:54:46 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-07-27 12:54:46 | INFO | train | epoch 006 | loss 2.204 | trans_loss 3.605 | nll_loss 1.792 | w2v_ctc_loss 1.193 | task_loss 0.932 | contrastive_loss 0.233 | total 4138.65 | n_correct 2290.24 | ppl 3.46 | accuracy 55.338 | wps 12406.2 | ups 1 | wpb 12355.8 | bsz 458.5 | num_updates 8838 | lr 0.000150431 | gnorm 0.426 | clip 0 | loss_scale 16 | train_wall 1356 | gb_free 15.1 | wall 8016
2023-07-27 12:54:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 12:54:46 | INFO | fairseq.trainer | begin training epoch 7
2023-07-27 12:54:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 12:55:51 | INFO | train_inner | epoch 007:     62 / 1474 loss=2.143, trans_loss=3.576, nll_loss=1.756, w2v_ctc_loss=1.146, task_loss=0.908, contrastive_loss=0.167, total=4108.19, n_correct=2312.06, ppl=3.38, accuracy=56.279, wps=8347.7, ups=0.68, wpb=12266.6, bsz=461.9, num_updates=8900, lr=0.000149906, gnorm=0.421, clip=0, loss_scale=16, train_wall=91, gb_free=17.1, wall=8081
2023-07-27 12:57:24 | INFO | train_inner | epoch 007:    162 / 1474 loss=2.143, trans_loss=3.567, nll_loss=1.742, w2v_ctc_loss=1.133, task_loss=0.946, contrastive_loss=0.236, total=4106.05, n_correct=2321.5, ppl=3.35, accuracy=56.539, wps=13289.9, ups=1.08, wpb=12258.7, bsz=455.4, num_updates=9000, lr=0.000149071, gnorm=0.424, clip=0, loss_scale=16, train_wall=92, gb_free=16.7, wall=8174
2023-07-27 12:58:56 | INFO | train_inner | epoch 007:    262 / 1474 loss=2.125, trans_loss=3.56, nll_loss=1.732, w2v_ctc_loss=1.135, task_loss=0.944, contrastive_loss=0.142, total=4129.3, n_correct=2349.2, ppl=3.32, accuracy=56.891, wps=13301, ups=1.08, wpb=12322.8, bsz=451.8, num_updates=9100, lr=0.00014825, gnorm=0.419, clip=0, loss_scale=16, train_wall=92, gb_free=17.3, wall=8266
2023-07-27 13:00:30 | INFO | train_inner | epoch 007:    362 / 1474 loss=2.161, trans_loss=3.568, nll_loss=1.744, w2v_ctc_loss=1.128, task_loss=0.898, contrastive_loss=0.41, total=4201.67, n_correct=2376.02, ppl=3.35, accuracy=56.549, wps=13410.5, ups=1.07, wpb=12539.8, bsz=479.7, num_updates=9200, lr=0.000147442, gnorm=0.419, clip=0, loss_scale=32, train_wall=93, gb_free=15.4, wall=8360
2023-07-27 13:02:02 | INFO | train_inner | epoch 007:    462 / 1474 loss=2.148, trans_loss=3.569, nll_loss=1.748, w2v_ctc_loss=1.122, task_loss=0.916, contrastive_loss=0.333, total=4155.31, n_correct=2345.68, ppl=3.36, accuracy=56.45, wps=13478.5, ups=1.09, wpb=12410.9, bsz=465.5, num_updates=9300, lr=0.000146647, gnorm=0.417, clip=0, loss_scale=32, train_wall=92, gb_free=16.7, wall=8452
2023-07-27 13:03:34 | INFO | train_inner | epoch 007:    562 / 1474 loss=2.125, trans_loss=3.569, nll_loss=1.743, w2v_ctc_loss=1.128, task_loss=0.916, contrastive_loss=0.152, total=4165.88, n_correct=2361.29, ppl=3.35, accuracy=56.682, wps=13420.4, ups=1.08, wpb=12426.4, bsz=459, num_updates=9400, lr=0.000145865, gnorm=0.417, clip=0, loss_scale=32, train_wall=92, gb_free=17.2, wall=8544
2023-07-27 13:05:08 | INFO | train_inner | epoch 007:    662 / 1474 loss=2.116, trans_loss=3.567, nll_loss=1.743, w2v_ctc_loss=1.122, task_loss=0.938, contrastive_loss=0.135, total=4149.29, n_correct=2361.38, ppl=3.35, accuracy=56.91, wps=13272.4, ups=1.07, wpb=12381.3, bsz=451.6, num_updates=9500, lr=0.000145095, gnorm=0.42, clip=0, loss_scale=32, train_wall=93, gb_free=17, wall=8638
2023-07-27 13:06:41 | INFO | train_inner | epoch 007:    762 / 1474 loss=2.12, trans_loss=3.564, nll_loss=1.74, w2v_ctc_loss=1.126, task_loss=0.968, contrastive_loss=0.135, total=4134.54, n_correct=2344.94, ppl=3.34, accuracy=56.716, wps=13292.7, ups=1.08, wpb=12345.4, bsz=449.8, num_updates=9600, lr=0.000144338, gnorm=0.419, clip=0, loss_scale=32, train_wall=92, gb_free=13.7, wall=8731
2023-07-27 13:08:14 | INFO | train_inner | epoch 007:    862 / 1474 loss=2.12, trans_loss=3.573, nll_loss=1.751, w2v_ctc_loss=1.122, task_loss=0.937, contrastive_loss=0.157, total=4151.77, n_correct=2350.55, ppl=3.37, accuracy=56.616, wps=13294.5, ups=1.07, wpb=12391.6, bsz=461.9, num_updates=9700, lr=0.000143592, gnorm=0.417, clip=0, loss_scale=32, train_wall=93, gb_free=14.8, wall=8824
2023-07-27 13:09:47 | INFO | train_inner | epoch 007:    962 / 1474 loss=2.129, trans_loss=3.567, nll_loss=1.745, w2v_ctc_loss=1.114, task_loss=0.896, contrastive_loss=0.256, total=4124.8, n_correct=2343.55, ppl=3.35, accuracy=56.816, wps=13206.8, ups=1.07, wpb=12313.3, bsz=471.2, num_updates=9800, lr=0.000142857, gnorm=0.421, clip=0, loss_scale=32, train_wall=93, gb_free=16.5, wall=8917
2023-07-27 13:11:19 | INFO | train_inner | epoch 007:   1062 / 1474 loss=2.117, trans_loss=3.578, nll_loss=1.759, w2v_ctc_loss=1.125, task_loss=0.975, contrastive_loss=0.118, total=4113.08, n_correct=2324.98, ppl=3.38, accuracy=56.526, wps=13295.6, ups=1.08, wpb=12279.6, bsz=439.4, num_updates=9900, lr=0.000142134, gnorm=0.419, clip=0, loss_scale=32, train_wall=92, gb_free=14.7, wall=9009
2023-07-27 13:12:52 | INFO | train_inner | epoch 007:   1162 / 1474 loss=2.155, trans_loss=3.566, nll_loss=1.746, w2v_ctc_loss=1.116, task_loss=0.911, contrastive_loss=0.401, total=4134.15, n_correct=2343.69, ppl=3.35, accuracy=56.691, wps=13272.6, ups=1.07, wpb=12353.7, bsz=470.3, num_updates=10000, lr=0.000141421, gnorm=0.429, clip=0, loss_scale=32, train_wall=93, gb_free=16, wall=9103
2023-07-27 13:12:52 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 13:13:18 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.361 | trans_loss 5.794 | nll_loss 3.156 | w2v_ctc_loss 1.344 | task_loss 4.584 | contrastive_loss 0.294 | total 4003.4 | n_correct 2318.5 | ppl 8.91 | accuracy 57.913 | uer 19.295 | wer 21.013 | raw_wer 21.013 | bleu 17.48 | wps 1872.7 | wpb 4003.4 | bsz 141.8 | num_updates 10000 | best_bleu 17.48
2023-07-27 13:13:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10000 updates
2023-07-27 13:13:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_7_10000.pt
2023-07-27 13:13:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_7_10000.pt
2023-07-27 13:13:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_7_10000.pt (epoch 7 @ 10000 updates, score 17.48) (writing took 20.500752517953515 seconds)
Mixup rate:0.5, token after shrink shape:torch.Size([40, 40]), X shape:torch.Size([40, 40, 512])
CTC Tokens:tensor([ 8, 19,  0,  0,  0], device='cuda:0'), Shrink Mask:tensor([ True,  True,  True, False, False], device='cuda:0'), New Tokens:tensor([ 8, 19,  0, 77,  0], device='cuda:0'), Org X:tensor([[-0.4436,  0.0970,  0.7407,  ...,  0.2917,  1.3652, -1.0146],
        [-1.4199, -0.1236, -0.1072,  ...,  0.0296,  1.7041, -1.3506],
        [-1.2109, -0.3962,  0.6475,  ..., -0.2468,  0.6206,  0.2661],
        [-0.0701,  0.0116,  1.9854,  ..., -0.3210,  0.3191,  0.2042],
        [ 0.0937, -0.0616,  2.5215,  ..., -0.4824, -0.1079,  0.3242]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
New X:tensor([[-0.4436,  0.0970,  0.7407,  ...,  0.2917,  1.3652, -1.0146],
        [-1.4199, -0.1236, -0.1072,  ...,  0.0296,  1.7041, -1.3506],
        [-1.2109, -0.3962,  0.6475,  ..., -0.2468,  0.6206,  0.2661],
        [-0.0701,  0.0116,  1.9854,  ..., -0.3210,  0.3191,  0.2042],
        [ 0.0937, -0.0616,  2.5215,  ..., -0.4824, -0.1079,  0.3242]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.1939, device='cuda:0')
2023-07-27 13:15:12 | INFO | train_inner | epoch 007:   1262 / 1474 loss=2.109, trans_loss=3.571, nll_loss=1.75, w2v_ctc_loss=1.111, task_loss=0.943, contrastive_loss=0.149, total=4133.98, n_correct=2345.71, ppl=3.36, accuracy=56.742, wps=8852.8, ups=0.72, wpb=12343.8, bsz=450.9, num_updates=10100, lr=0.00014072, gnorm=0.355, clip=0, loss_scale=32, train_wall=92, gb_free=16.7, wall=9242
2023-07-27 13:16:44 | INFO | train_inner | epoch 007:   1362 / 1474 loss=2.124, trans_loss=3.565, nll_loss=1.743, w2v_ctc_loss=1.122, task_loss=0.878, contrastive_loss=0.188, total=4171.46, n_correct=2378.23, ppl=3.35, accuracy=57.012, wps=13459.9, ups=1.08, wpb=12453.1, bsz=475.5, num_updates=10200, lr=0.000140028, gnorm=0.357, clip=0, loss_scale=32, train_wall=92, gb_free=17.7, wall=9334
2023-07-27 13:18:18 | INFO | train_inner | epoch 007:   1462 / 1474 loss=2.132, trans_loss=3.571, nll_loss=1.752, w2v_ctc_loss=1.12, task_loss=1.004, contrastive_loss=0.254, total=4106.94, n_correct=2326.11, ppl=3.37, accuracy=56.639, wps=13068.5, ups=1.06, wpb=12271.2, bsz=443.8, num_updates=10300, lr=0.000139347, gnorm=0.364, clip=0, loss_scale=32, train_wall=93, gb_free=15.6, wall=9428
2023-07-27 13:18:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([8, 73]), X shape:torch.Size([8, 73, 512])
CTC Tokens:tensor([  33,    6,    6, 9283,    0], device='cuda:7'), Shrink Mask:tensor([ True,  True, False,  True,  True], device='cuda:7'), New Tokens:tensor([  33,    6, 9283,    0, 4810], device='cuda:7'), Org X:tensor([[ 0.0750, -0.2744, -1.2510,  ..., -0.0424, -1.4668,  0.0890],
        [ 0.2878, -0.9404, -2.2480,  ...,  0.0454, -1.4492,  0.2474],
        [-0.6855,  0.2297, -3.0879,  ...,  0.1543,  0.1145, -0.5986],
        [-1.1289,  0.7544, -1.7871,  ..., -0.3010,  0.1036, -0.1321],
        [-0.3997,  1.7188, -1.0508,  ...,  0.2433, -0.2413,  0.2605]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
New X:tensor([[ 0.0750, -0.2744, -1.2510,  ..., -0.0424, -1.4668,  0.0890],
        [ 0.2878, -0.9404, -2.2480,  ...,  0.0454, -1.4492,  0.2474],
        [-0.6855,  0.2297, -3.0879,  ...,  0.1543,  0.1145, -0.5986],
        [-0.7764, -1.3135, -0.1971,  ..., -2.5977, -1.5879, -1.8330],
        [ 0.0091,  1.7510, -1.5605,  ...,  1.3525,  0.8823, -0.7070]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.1939, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 98]), X shape:torch.Size([8, 98, 512])
CTC Tokens:tensor([ 0, 29, 29,  0,  0], device='cuda:6'), Shrink Mask:tensor([ True,  True, False,  True, False], device='cuda:6'), New Tokens:tensor([ 0, 29,  0,  4,  0], device='cuda:6'), Org X:tensor([[ 0.8457,  0.0143,  1.5869,  ...,  0.2571, -0.8784, -0.1067],
        [ 0.3401,  0.4441,  0.7275,  ...,  0.2944, -2.0664,  0.1299],
        [-0.5342,  0.6426,  1.6230,  ...,  0.0213, -1.2881, -0.9082],
        [ 0.6802,  0.6621,  1.4092,  ..., -0.9531,  0.2671, -0.4233],
        [ 0.6318,  0.9600,  1.5020,  ..., -0.7188, -0.2067,  0.0622]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
New X:tensor([[-0.7764, -1.3135, -0.1971,  ..., -2.5977, -1.5879, -1.8330],
        [ 0.3401,  0.4441,  0.7275,  ...,  0.2944, -2.0664,  0.1299],
        [-0.7764, -1.3135, -0.1971,  ..., -2.5977, -1.5879, -1.8330],
        [ 0.6802,  0.6621,  1.4092,  ..., -0.9531,  0.2671, -0.4233],
        [-0.7764, -1.3135, -0.1971,  ..., -2.5977, -1.5879, -1.8330]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.1939, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 85]), X shape:torch.Size([16, 85, 512])
CTC Tokens:tensor([  0,   0,   0, 440,   0], device='cuda:4'), Shrink Mask:tensor([ True, False, False,  True,  True], device='cuda:4'), New Tokens:tensor([  0, 440,   0,   4,   0], device='cuda:4'), Org X:tensor([[ 0.6680, -0.4897,  1.6963,  ..., -0.0698,  0.0234,  0.4600],
        [ 0.1536,  1.2148, -0.4243,  ..., -0.0470,  0.4446, -0.5298],
        [ 0.3420,  1.3662,  0.4277,  ..., -0.8286,  0.9219, -0.6187],
        [ 0.3418,  0.4880,  1.2734,  ..., -1.7949,  1.8252, -0.9116],
        [-0.3718,  0.6348,  1.2695,  ..., -1.3076,  1.5371, -0.6890]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
New X:tensor([[ 0.6680, -0.4897,  1.6963,  ..., -0.0698,  0.0234,  0.4600],
        [ 0.1536,  1.2148, -0.4243,  ..., -0.0470,  0.4446, -0.5298],
        [-0.7764, -1.3135, -0.1971,  ..., -2.5977, -1.5879, -1.8330],
        [ 0.3418,  0.4880,  1.2734,  ..., -1.7949,  1.8252, -0.9116],
        [-0.3718,  0.6348,  1.2695,  ..., -1.3076,  1.5371, -0.6890]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.1939, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 105]), X shape:torch.Size([8, 105, 512])
CTC Tokens:tensor([  0,  25,   0, 659, 659], device='cuda:2'), Shrink Mask:tensor([ True,  True,  True,  True, False], device='cuda:2'), New Tokens:tensor([  0,  25,   0, 659,   0], device='cuda:2'), Org X:tensor([[ 0.9546, -0.0328,  0.1840,  ..., -0.5156,  1.9531, -0.2883],
        [ 0.1766,  0.4446, -1.3535,  ..., -0.2267,  1.9053, -0.3000],
        [ 0.5171,  0.7676,  0.3943,  ..., -0.3870,  1.9229,  0.1971],
        [ 0.3469,  0.7944, -0.3171,  ..., -0.4443,  2.0527,  0.0550],
        [ 2.4707,  1.4189,  1.6602,  ..., -1.8662,  0.9424,  0.1104]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
New X:tensor([[-0.7764, -1.3135, -0.1971,  ..., -2.5977, -1.5879, -1.8330],
        [ 0.1766,  0.4446, -1.3535,  ..., -0.2267,  1.9053, -0.3000],
        [-0.7764, -1.3135, -0.1971,  ..., -2.5977, -1.5879, -1.8330],
        [-0.1725,  0.8521, -0.3279,  ...,  0.6714, -0.9531,  1.9170],
        [ 2.4707,  1.4189,  1.6602,  ..., -1.8662,  0.9424,  0.1104]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.1939, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 78]), X shape:torch.Size([8, 78, 512])
CTC Tokens:tensor([  0, 473,   0,   0,   0], device='cuda:1'), Shrink Mask:tensor([ True,  True,  True, False, False], device='cuda:1'), New Tokens:tensor([   0,  473,    0, 1822,    0], device='cuda:1'), Org X:tensor([[ 1.3975, -1.0615,  2.0684,  ...,  0.4756,  0.6035,  0.1957],
        [ 0.2305,  0.3889,  0.4824,  ...,  0.2460, -0.5249, -1.1758],
        [ 0.7061,  0.4336,  1.8613,  ...,  0.3362, -0.1943, -0.4248],
        [-1.7324,  0.3271,  0.3130,  ...,  0.2281,  0.8452, -0.1019],
        [ 0.2656,  0.3679,  2.8516,  ..., -1.2109,  1.0244,  0.0229]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
New X:tensor([[ 1.3975, -1.0615,  2.0684,  ...,  0.4756,  0.6035,  0.1957],
        [ 0.2305,  0.3889,  0.4824,  ...,  0.2460, -0.5249, -1.1758],
        [ 0.7061,  0.4336,  1.8613,  ...,  0.3362, -0.1943, -0.4248],
        [-1.7324,  0.3271,  0.3130,  ...,  0.2281,  0.8452, -0.1019],
        [-0.7764, -1.3135, -0.1971,  ..., -2.5977, -1.5879, -1.8330]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.1939, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([48, 29]), X shape:torch.Size([48, 29, 512])
CTC Tokens:tensor([   0,   19,    0, 1080,   20], device='cuda:3'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:3'), New Tokens:tensor([   0,   19,    0, 1080,   20], device='cuda:3'), Org X:tensor([[ 0.5762,  0.5547,  0.0332,  ...,  0.4702, -0.5293, -1.0986],
        [ 0.7056,  0.7822, -0.4360,  ...,  0.0991,  0.3611, -1.1191],
        [ 1.3184,  1.2119, -0.0824,  ..., -0.2795,  0.8330, -0.8770],
        [ 0.4426,  1.2705, -0.5557,  ..., -1.2041,  0.8853, -1.0342],
        [ 0.8765,  1.3086,  0.6592,  ..., -2.8008,  0.8730, -1.5469]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
New X:tensor([[ 0.5762,  0.5547,  0.0332,  ...,  0.4702, -0.5293, -1.0986],
        [ 0.7056,  0.7822, -0.4360,  ...,  0.0991,  0.3611, -1.1191],
        [ 1.3184,  1.2119, -0.0824,  ..., -0.2795,  0.8330, -0.8770],
        [ 1.6943,  0.5439,  0.0076,  ..., -2.9551, -0.2412, -0.7568],
        [-0.1813, -0.6172, -0.3467,  ..., -4.2109, -3.4121, -0.7637]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.1939, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 62]), X shape:torch.Size([16, 62, 512])
CTC Tokens:tensor([  29,  237,    0,   84, 1043], device='cuda:5'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:5'), New Tokens:tensor([  29,  237,    0,   84, 1043], device='cuda:5'), Org X:tensor([[-0.5405,  0.3562,  0.2874,  ...,  0.3806, -1.7070, -0.9106],
        [-0.1674,  0.4846,  0.2322,  ...,  0.3474, -2.1328, -1.6182],
        [-0.2664,  0.6792, -0.4451,  ..., -0.0329, -1.5381, -2.6113],
        [-1.7686,  0.3550, -1.5166,  ..., -0.0867, -0.2349, -1.4043],
        [-0.5337,  0.6338,  1.5186,  ..., -0.9619,  0.1575,  0.0443]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
New X:tensor([[-0.8540,  0.1361, -0.3850,  ..., -0.0412, -2.0371,  3.6211],
        [-0.1674,  0.4846,  0.2322,  ...,  0.3474, -2.1328, -1.6182],
        [-0.7764, -1.3135, -0.1971,  ..., -2.5977, -1.5879, -1.8330],
        [-1.7686,  0.3550, -1.5166,  ..., -0.0867, -0.2349, -1.4043],
        [-0.4800,  1.4434,  0.7485,  ..., -0.5073,  0.2603, -0.5171]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.1939, device='cuda:5')
2023-07-27 13:18:53 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.362 | trans_loss 5.792 | nll_loss 3.15 | w2v_ctc_loss 1.35 | task_loss 4.55 | contrastive_loss 0.293 | total 4003.4 | n_correct 2324.4 | ppl 8.88 | accuracy 58.061 | uer 19.606 | wer 21.289 | raw_wer 21.289 | bleu 17.68 | wps 2048.6 | wpb 4003.4 | bsz 141.8 | num_updates 10312 | best_bleu 17.68
2023-07-27 13:18:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10312 updates
2023-07-27 13:18:53 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt
2023-07-27 13:19:04 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt
2023-07-27 13:19:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt (epoch 7 @ 10312 updates, score 17.68) (writing took 19.68417390808463 seconds)
2023-07-27 13:19:14 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-07-27 13:19:14 | INFO | train | epoch 007 | loss 2.13 | trans_loss 3.568 | nll_loss 1.745 | w2v_ctc_loss 1.123 | task_loss 0.933 | contrastive_loss 0.217 | total 4138.65 | n_correct 2346.48 | ppl 3.35 | accuracy 56.697 | wps 12406.7 | ups 1 | wpb 12355.8 | bsz 458.5 | num_updates 10312 | lr 0.000139266 | gnorm 0.407 | clip 0 | loss_scale 32 | train_wall 1361 | gb_free 13.1 | wall 9484
2023-07-27 13:19:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 13:19:14 | INFO | fairseq.trainer | begin training epoch 8
2023-07-27 13:19:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 13:20:44 | INFO | train_inner | epoch 008:     88 / 1474 loss=2.079, trans_loss=3.544, nll_loss=1.71, w2v_ctc_loss=1.083, task_loss=0.98, contrastive_loss=0.144, total=4120.74, n_correct=2370.33, ppl=3.27, accuracy=57.522, wps=8422.4, ups=0.69, wpb=12287.4, bsz=444.5, num_updates=10400, lr=0.000138675, gnorm=0.362, clip=0, loss_scale=32, train_wall=93, gb_free=17.2, wall=9574
2023-07-27 13:22:17 | INFO | train_inner | epoch 008:    188 / 1474 loss=2.078, trans_loss=3.537, nll_loss=1.701, w2v_ctc_loss=1.079, task_loss=1.017, contrastive_loss=0.166, total=4028.45, n_correct=2320.82, ppl=3.25, accuracy=57.611, wps=13002.3, ups=1.08, wpb=12015, bsz=426.7, num_updates=10500, lr=0.000138013, gnorm=0.365, clip=0, loss_scale=32, train_wall=92, gb_free=16.4, wall=9667
2023-07-27 13:23:49 | INFO | train_inner | epoch 008:    288 / 1474 loss=2.072, trans_loss=3.532, nll_loss=1.697, w2v_ctc_loss=1.074, task_loss=0.87, contrastive_loss=0.166, total=4212.3, n_correct=2437.15, ppl=3.24, accuracy=57.858, wps=13588.4, ups=1.08, wpb=12569, bsz=489.5, num_updates=10600, lr=0.000137361, gnorm=0.356, clip=0, loss_scale=32, train_wall=92, gb_free=16.8, wall=9759
2023-07-27 13:25:23 | INFO | train_inner | epoch 008:    388 / 1474 loss=2.094, trans_loss=3.541, nll_loss=1.709, w2v_ctc_loss=1.098, task_loss=0.995, contrastive_loss=0.184, total=4124.65, n_correct=2370.36, ppl=3.27, accuracy=57.468, wps=13159.3, ups=1.07, wpb=12310, bsz=439.9, num_updates=10700, lr=0.000136717, gnorm=0.359, clip=0, loss_scale=32, train_wall=93, gb_free=16.3, wall=9853
2023-07-27 13:26:56 | INFO | train_inner | epoch 008:    488 / 1474 loss=2.121, trans_loss=3.537, nll_loss=1.706, w2v_ctc_loss=1.071, task_loss=0.832, contrastive_loss=0.459, total=4205.7, n_correct=2428.18, ppl=3.26, accuracy=57.735, wps=13438.4, ups=1.07, wpb=12552.8, bsz=505.9, num_updates=10800, lr=0.000136083, gnorm=0.36, clip=0, loss_scale=32, train_wall=93, gb_free=16.3, wall=9946
2023-07-27 13:28:28 | INFO | train_inner | epoch 008:    588 / 1474 loss=2.082, trans_loss=3.54, nll_loss=1.713, w2v_ctc_loss=1.095, task_loss=1.012, contrastive_loss=0.116, total=4063.27, n_correct=2334.55, ppl=3.28, accuracy=57.455, wps=13186.6, ups=1.09, wpb=12148.1, bsz=428.8, num_updates=10900, lr=0.000135457, gnorm=0.361, clip=0, loss_scale=32, train_wall=92, gb_free=16.4, wall=10038
2023-07-27 13:30:01 | INFO | train_inner | epoch 008:    688 / 1474 loss=2.077, trans_loss=3.538, nll_loss=1.706, w2v_ctc_loss=1.091, task_loss=0.967, contrastive_loss=0.129, total=4140.15, n_correct=2392.91, ppl=3.26, accuracy=57.798, wps=13308.7, ups=1.08, wpb=12356.4, bsz=447.4, num_updates=11000, lr=0.00013484, gnorm=0.355, clip=0, loss_scale=32, train_wall=92, gb_free=16.8, wall=10131
2023-07-27 13:31:34 | INFO | train_inner | epoch 008:    788 / 1474 loss=2.082, trans_loss=3.536, nll_loss=1.708, w2v_ctc_loss=1.081, task_loss=0.96, contrastive_loss=0.214, total=4121.11, n_correct=2379.18, ppl=3.27, accuracy=57.732, wps=13197.9, ups=1.07, wpb=12317.3, bsz=446.8, num_updates=11100, lr=0.000134231, gnorm=0.363, clip=0, loss_scale=32, train_wall=93, gb_free=17.4, wall=10224
2023-07-27 13:33:07 | INFO | train_inner | epoch 008:    888 / 1474 loss=2.08, trans_loss=3.537, nll_loss=1.708, w2v_ctc_loss=1.071, task_loss=0.906, contrastive_loss=0.229, total=4158.35, n_correct=2403.66, ppl=3.27, accuracy=57.803, wps=13446.6, ups=1.08, wpb=12419.9, bsz=470.3, num_updates=11200, lr=0.000133631, gnorm=0.361, clip=0, loss_scale=32, train_wall=92, gb_free=16.7, wall=10317
2023-07-27 13:34:39 | INFO | train_inner | epoch 008:    988 / 1474 loss=2.057, trans_loss=3.536, nll_loss=1.706, w2v_ctc_loss=1.065, task_loss=0.876, contrastive_loss=0.128, total=4170.95, n_correct=2419.44, ppl=3.26, accuracy=58.007, wps=13557.1, ups=1.09, wpb=12452.8, bsz=470.7, num_updates=11300, lr=0.000133038, gnorm=0.354, clip=0, loss_scale=64, train_wall=91, gb_free=15.9, wall=10409
2023-07-27 13:36:12 | INFO | train_inner | epoch 008:   1088 / 1474 loss=2.098, trans_loss=3.548, nll_loss=1.721, w2v_ctc_loss=1.075, task_loss=0.93, contrastive_loss=0.354, total=4194.19, n_correct=2412.57, ppl=3.3, accuracy=57.522, wps=13356.2, ups=1.07, wpb=12518.3, bsz=464.3, num_updates=11400, lr=0.000132453, gnorm=0.364, clip=0, loss_scale=64, train_wall=93, gb_free=17.2, wall=10502
2023-07-27 13:37:45 | INFO | train_inner | epoch 008:   1188 / 1474 loss=2.065, trans_loss=3.539, nll_loss=1.712, w2v_ctc_loss=1.072, task_loss=0.894, contrastive_loss=0.124, total=4165.68, n_correct=2406.95, ppl=3.28, accuracy=57.78, wps=13492.5, ups=1.08, wpb=12442.9, bsz=467.9, num_updates=11500, lr=0.000131876, gnorm=0.357, clip=0, loss_scale=64, train_wall=92, gb_free=17.5, wall=10595
2023-07-27 13:39:16 | INFO | train_inner | epoch 008:   1288 / 1474 loss=2.081, trans_loss=3.543, nll_loss=1.716, w2v_ctc_loss=1.085, task_loss=0.959, contrastive_loss=0.173, total=4079.12, n_correct=2346.34, ppl=3.29, accuracy=57.521, wps=13302.4, ups=1.09, wpb=12185.1, bsz=444, num_updates=11600, lr=0.000131306, gnorm=0.363, clip=0, loss_scale=64, train_wall=91, gb_free=15.6, wall=10686
2023-07-27 13:40:49 | INFO | train_inner | epoch 008:   1388 / 1474 loss=2.085, trans_loss=3.546, nll_loss=1.721, w2v_ctc_loss=1.077, task_loss=0.928, contrastive_loss=0.212, total=4136.76, n_correct=2384.25, ppl=3.3, accuracy=57.636, wps=13327.4, ups=1.08, wpb=12354.4, bsz=459.3, num_updates=11700, lr=0.000130744, gnorm=0.359, clip=0, loss_scale=64, train_wall=92, gb_free=15.9, wall=10779
2023-07-27 13:42:08 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 13:42:33 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 4.333 | trans_loss 5.75 | nll_loss 3.09 | w2v_ctc_loss 1.359 | task_loss 4.627 | contrastive_loss 0.277 | total 4003.4 | n_correct 2349.7 | ppl 8.51 | accuracy 58.693 | uer 18.615 | wer 20.234 | raw_wer 20.234 | bleu 17.76 | wps 1983.9 | wpb 4003.4 | bsz 141.8 | num_updates 11786 | best_bleu 17.76
2023-07-27 13:42:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11786 updates
2023-07-27 13:42:33 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt
2023-07-27 13:42:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt
2023-07-27 13:42:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_best.pt (epoch 8 @ 11786 updates, score 17.76) (writing took 19.692854080349207 seconds)
2023-07-27 13:42:53 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-07-27 13:42:53 | INFO | train | epoch 008 | loss 2.082 | trans_loss 3.54 | nll_loss 1.71 | w2v_ctc_loss 1.079 | task_loss 0.933 | contrastive_loss 0.207 | total 4138.65 | n_correct 2387.31 | ppl 3.27 | accuracy 57.683 | wps 12829.5 | ups 1.04 | wpb 12355.8 | bsz 458.5 | num_updates 11786 | lr 0.000130266 | gnorm 0.36 | clip 0 | loss_scale 64 | train_wall 1359 | gb_free 16.8 | wall 10903
2023-07-27 13:42:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 13:42:54 | INFO | fairseq.trainer | begin training epoch 9
2023-07-27 13:42:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 13:43:16 | INFO | train_inner | epoch 009:     14 / 1474 loss=2.082, trans_loss=3.541, nll_loss=1.712, w2v_ctc_loss=1.058, task_loss=0.911, contrastive_loss=0.337, total=4126.64, n_correct=2385.44, ppl=3.28, accuracy=57.806, wps=8354, ups=0.68, wpb=12312.4, bsz=468.3, num_updates=11800, lr=0.000130189, gnorm=0.361, clip=0, loss_scale=64, train_wall=92, gb_free=17.4, wall=10926
2023-07-27 13:44:48 | INFO | train_inner | epoch 009:    114 / 1474 loss=2.027, trans_loss=3.5, nll_loss=1.659, w2v_ctc_loss=1.031, task_loss=0.881, contrastive_loss=0.166, total=4190.32, n_correct=2469.77, ppl=3.16, accuracy=58.94, wps=13610.7, ups=1.09, wpb=12513.4, bsz=478.7, num_updates=11900, lr=0.000129641, gnorm=0.353, clip=0, loss_scale=64, train_wall=91, gb_free=16.4, wall=11018
2023-07-27 13:46:21 | INFO | train_inner | epoch 009:    214 / 1474 loss=2.024, trans_loss=3.507, nll_loss=1.668, w2v_ctc_loss=1.037, task_loss=0.997, contrastive_loss=0.114, total=4071.85, n_correct=2390.03, ppl=3.18, accuracy=58.696, wps=13059, ups=1.07, wpb=12158.6, bsz=435.4, num_updates=12000, lr=0.000129099, gnorm=0.36, clip=0, loss_scale=64, train_wall=93, gb_free=16.5, wall=11111
2023-07-27 13:46:21 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 13:46:48 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.318 | trans_loss 5.755 | nll_loss 3.099 | w2v_ctc_loss 1.295 | task_loss 4.559 | contrastive_loss 0.281 | total 4003.4 | n_correct 2348 | ppl 8.57 | accuracy 58.65 | uer 18.809 | wer 20.555 | raw_wer 20.555 | bleu 17.97 | wps 1810.1 | wpb 4003.4 | bsz 141.8 | num_updates 12000 | best_bleu 17.97
2023-07-27 13:46:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12000 updates
2023-07-27 13:46:48 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_9_12000.pt
2023-07-27 13:46:51 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_9_12000.pt
2023-07-27 13:47:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_mt_0727/checkpoint_9_12000.pt (epoch 9 @ 12000 updates, score 17.97) (writing took 20.049343772232533 seconds)
2023-07-27 13:48:41 | INFO | train_inner | epoch 009:    314 / 1474 loss=2.017, trans_loss=3.494, nll_loss=1.654, w2v_ctc_loss=1.021, task_loss=0.873, contrastive_loss=0.166, total=4145.76, n_correct=2450.33, ppl=3.15, accuracy=59.104, wps=8856.8, ups=0.71, wpb=12388.7, bsz=475.4, num_updates=12100, lr=0.000128565, gnorm=0.354, clip=0, loss_scale=64, train_wall=92, gb_free=16.5, wall=11251
2023-07-27 13:50:15 | INFO | train_inner | epoch 009:    414 / 1474 loss=2.024, trans_loss=3.512, nll_loss=1.675, w2v_ctc_loss=1.032, task_loss=0.913, contrastive_loss=0.132, total=4204.03, n_correct=2460.63, ppl=3.19, accuracy=58.53, wps=13426.1, ups=1.07, wpb=12552.3, bsz=469.6, num_updates=12200, lr=0.000128037, gnorm=0.352, clip=0, loss_scale=64, train_wall=93, gb_free=16.1, wall=11345
2023-07-27 13:51:47 | INFO | train_inner | epoch 009:    514 / 1474 loss=2.053, trans_loss=3.52, nll_loss=1.683, w2v_ctc_loss=1.055, task_loss=0.98, contrastive_loss=0.183, total=4116.9, n_correct=2403.47, ppl=3.21, accuracy=58.381, wps=13264.7, ups=1.08, wpb=12286.7, bsz=438.4, num_updates=12300, lr=0.000127515, gnorm=0.359, clip=0, loss_scale=64, train_wall=92, gb_free=15.3, wall=11437
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 489 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
