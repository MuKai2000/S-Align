2023-08-07 13:58:25 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:18287
2023-08-07 13:58:25 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:18287
2023-08-07 13:58:25 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:18287
2023-08-07 13:58:25 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:18287
2023-08-07 13:58:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-07 13:58:25 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:18287
2023-08-07 13:58:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-07 13:58:25 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:18287
2023-08-07 13:58:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-07 13:58:25 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:18287
2023-08-07 13:58:25 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:18287
2023-08-07 13:58:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-07 13:58:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-07 13:58:26 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-07 13:58:26 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-07 13:58:26 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-07 13:58:26 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-07 13:58:26 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-07 13:58:26 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-07 13:58:26 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-07 13:58:26 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-07 13:58:26 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-07 13:58:26 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-07 13:58:26 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-07 13:58:26 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-07 13:58:26 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-07 13:58:26 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-07 13:58:26 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-07 13:58:26 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-07 13:58:26 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-07 13:58:26 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-07 13:58:26 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-07 13:58:30 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_0807_st', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18287', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_0807_st', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 100, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_cluster', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=False, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st', save_interval=1, save_interval_updates=100, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_sigle_st.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_cluster', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=False, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st', save_interval=1, save_interval_updates=100, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_sigle_st.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_cluster', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_cluster', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=False, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st', save_interval=1, save_interval_updates=100, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_sigle_st.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-07 13:58:30 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-08-07 13:58:30 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-08-07 13:58:30 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-07 13:58:30 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-08-07 13:58:32 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-07 13:58:32 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-07 13:58:32 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-07 13:58:34 | INFO | root | load pretrained hubert
2023-08-07 13:58:38 | INFO | root | share the sematic adapter and textual encoder
2023-08-07 13:58:38 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-07 13:58:38 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-08-07 13:58:38 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-07 13:58:38 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointCluster
2023-08-07 13:58:38 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-08-07 13:58:38 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-07 13:58:38 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-07 13:58:38 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-07 13:58:38 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-07 13:58:38 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-07 13:58:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-07 13:58:44 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-07 13:58:44 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-07 13:58:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-07 13:58:45 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-07 13:58:45 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-07 13:58:45 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-07 13:58:45 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-07 13:58:45 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-07 13:58:45 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-07 13:58:45 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-07 13:58:45 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-07 13:58:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-07 13:58:45 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-07 13:58:45 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-07 13:58:45 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_last.pt
2023-08-07 13:58:45 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_last.pt
2023-08-07 13:58:45 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-07 13:58:45 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-07 13:58:45 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-07 13:58:45 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-07 13:58:47 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-07 13:58:48 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-07 13:58:50 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-07 14:00:03 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-07 14:00:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 14:00:03 | INFO | fairseq.trainer | begin training epoch 1
2023-08-07 14:00:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 14:00:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-07 14:01:37 | INFO | train_inner | epoch 001:    101 / 1474 loss=22.984, trans_loss=9.481, nll_loss=9.424, w2v_ctc_loss=22.95, contrastive_loss=0, total=4207.57, n_correct=11.79, ppl=687.07, accuracy=0.28, wps=15793.6, ups=1.25, wpb=12564.5, bsz=471.5, num_updates=100, lr=4.098e-06, gnorm=3.019, clip=0, loss_scale=64, train_wall=85, gb_free=12.9, wall=173
2023-08-07 14:01:37 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 5 terminated with signal SIGTERM
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 147 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-08-07 14:02:41 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:18974
2023-08-07 14:02:41 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:18974
2023-08-07 14:02:41 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:18974
2023-08-07 14:02:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-07 14:02:41 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:18974
2023-08-07 14:02:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-07 14:02:41 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:18974
2023-08-07 14:02:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-07 14:02:41 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:18974
2023-08-07 14:02:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-07 14:02:41 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:18974
2023-08-07 14:02:41 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:18974
2023-08-07 14:02:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-07 14:02:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-07 14:02:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-07 14:02:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-07 14:02:42 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-07 14:02:42 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-07 14:02:42 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-07 14:02:42 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-07 14:02:42 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-07 14:02:42 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-07 14:02:42 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-07 14:02:42 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-07 14:02:42 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-07 14:02:42 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-07 14:02:42 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-07 14:02:42 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-07 14:02:42 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-07 14:02:42 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-07 14:02:42 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-07 14:02:42 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-07 14:02:47 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_0807_st', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18974', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_0807_st', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_cluster', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=False, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_sigle_st.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_cluster', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=False, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_sigle_st.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_cluster', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_cluster', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=False, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_sigle_st.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-07 14:02:47 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-08-07 14:02:47 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-08-07 14:02:47 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-07 14:02:47 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-08-07 14:02:49 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-07 14:02:49 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-07 14:02:49 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-07 14:02:51 | INFO | root | load pretrained hubert
2023-08-07 14:02:55 | INFO | root | share the sematic adapter and textual encoder
2023-08-07 14:02:55 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-07 14:02:55 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-08-07 14:02:55 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-07 14:02:55 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointCluster
2023-08-07 14:02:55 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-08-07 14:02:55 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-07 14:02:55 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-07 14:02:55 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-07 14:02:55 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-07 14:02:55 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-07 14:02:56 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-07 14:02:56 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-07 14:02:56 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-07 14:02:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-07 14:02:57 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-07 14:02:57 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-07 14:02:57 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-07 14:02:57 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-07 14:02:57 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-07 14:02:57 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-07 14:02:57 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-07 14:02:57 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-07 14:02:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-07 14:02:57 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-07 14:02:57 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-07 14:02:57 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_last.pt
2023-08-07 14:02:57 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_last.pt
2023-08-07 14:02:57 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-07 14:02:57 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-07 14:02:57 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-07 14:02:57 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-07 14:02:58 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-07 14:03:00 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-07 14:03:01 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-07 14:04:12 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-07 14:04:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 14:04:12 | INFO | fairseq.trainer | begin training epoch 1
2023-08-07 14:04:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 14:05:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-07 14:05:54 | INFO | train_inner | epoch 001:    101 / 1474 loss=22.893, trans_loss=9.539, nll_loss=9.489, w2v_ctc_loss=22.747, contrastive_loss=0, total=4202.73, n_correct=8.02, ppl=718.72, accuracy=0.191, wps=15806, ups=1.26, wpb=12557.6, bsz=471.4, num_updates=100, lr=4.098e-06, gnorm=2.456, clip=0, loss_scale=64, train_wall=94, gb_free=12.9, wall=177
2023-08-07 14:05:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-07 14:06:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-07 14:06:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-07 14:07:15 | INFO | train_inner | epoch 001:    204 / 1474 loss=14.375, trans_loss=8.413, nll_loss=8.156, w2v_ctc_loss=11.114, contrastive_loss=0, total=4129.81, n_correct=83.07, ppl=285.23, accuracy=2.011, wps=15326.2, ups=1.24, wpb=12342.4, bsz=464.7, num_updates=200, lr=8.096e-06, gnorm=6.543, clip=27, loss_scale=8, train_wall=80, gb_free=16.1, wall=258
2023-08-07 14:08:33 | INFO | train_inner | epoch 001:    304 / 1474 loss=11.63, trans_loss=7.9, nll_loss=7.529, w2v_ctc_loss=7.562, contrastive_loss=0, total=4076.47, n_correct=195.97, ppl=184.64, accuracy=4.807, wps=15568.6, ups=1.28, wpb=12182.6, bsz=439.1, num_updates=300, lr=1.2094e-05, gnorm=1.229, clip=0, loss_scale=8, train_wall=78, gb_free=17.6, wall=336
2023-08-07 14:09:51 | INFO | train_inner | epoch 001:    404 / 1474 loss=10.651, trans_loss=7.532, nll_loss=7.065, w2v_ctc_loss=6.542, contrastive_loss=0, total=4180.9, n_correct=352.02, ppl=133.86, accuracy=8.42, wps=15994.8, ups=1.28, wpb=12477.7, bsz=462.7, num_updates=400, lr=1.6092e-05, gnorm=1.151, clip=0, loss_scale=8, train_wall=78, gb_free=16.6, wall=414
2023-08-07 14:11:10 | INFO | train_inner | epoch 001:    504 / 1474 loss=10.074, trans_loss=7.249, nll_loss=6.69, w2v_ctc_loss=6.021, contrastive_loss=0, total=4193.47, n_correct=395.22, ppl=103.22, accuracy=9.425, wps=15996.7, ups=1.27, wpb=12561, bsz=488.9, num_updates=500, lr=2.009e-05, gnorm=1.583, clip=0, loss_scale=8, train_wall=78, gb_free=17.1, wall=493
2023-08-07 14:12:27 | INFO | train_inner | epoch 001:    604 / 1474 loss=9.813, trans_loss=7.1, nll_loss=6.472, w2v_ctc_loss=5.829, contrastive_loss=0, total=4117.91, n_correct=406.26, ppl=88.76, accuracy=9.866, wps=15860.5, ups=1.29, wpb=12285.5, bsz=469.7, num_updates=600, lr=2.4088e-05, gnorm=1.617, clip=0, loss_scale=8, train_wall=77, gb_free=12.7, wall=570
2023-08-07 14:13:44 | INFO | train_inner | epoch 001:    704 / 1474 loss=9.713, trans_loss=7.022, nll_loss=6.35, w2v_ctc_loss=5.798, contrastive_loss=0, total=4156.95, n_correct=416.57, ppl=81.56, accuracy=10.021, wps=16040.7, ups=1.29, wpb=12390.8, bsz=458.6, num_updates=700, lr=2.8086e-05, gnorm=2.18, clip=0, loss_scale=8, train_wall=77, gb_free=17.3, wall=647
2023-08-07 14:15:02 | INFO | train_inner | epoch 001:    804 / 1474 loss=9.636, trans_loss=6.996, nll_loss=6.292, w2v_ctc_loss=5.722, contrastive_loss=0, total=4122.35, n_correct=408.42, ppl=78.35, accuracy=9.907, wps=15758.4, ups=1.28, wpb=12305.4, bsz=462.7, num_updates=800, lr=3.2084e-05, gnorm=2.522, clip=0, loss_scale=8, train_wall=78, gb_free=17, wall=725
2023-08-07 14:16:20 | INFO | train_inner | epoch 001:    904 / 1474 loss=9.595, trans_loss=6.959, nll_loss=6.237, w2v_ctc_loss=5.726, contrastive_loss=0, total=4168.3, n_correct=421.08, ppl=75.43, accuracy=10.102, wps=16001.5, ups=1.29, wpb=12422.8, bsz=458.4, num_updates=900, lr=3.6082e-05, gnorm=1.889, clip=0, loss_scale=8, train_wall=77, gb_free=15.7, wall=803
2023-08-07 14:17:38 | INFO | train_inner | epoch 001:   1004 / 1474 loss=9.498, trans_loss=6.923, nll_loss=6.183, w2v_ctc_loss=5.626, contrastive_loss=0, total=4135.11, n_correct=419.19, ppl=72.68, accuracy=10.137, wps=15850.2, ups=1.28, wpb=12366.1, bsz=458.1, num_updates=1000, lr=4.008e-05, gnorm=1.585, clip=0, loss_scale=8, train_wall=78, gb_free=16.4, wall=881
2023-08-07 14:18:56 | INFO | train_inner | epoch 001:   1104 / 1474 loss=9.341, trans_loss=6.887, nll_loss=6.127, w2v_ctc_loss=5.448, contrastive_loss=0, total=4152.99, n_correct=425.22, ppl=69.87, accuracy=10.239, wps=15933.5, ups=1.29, wpb=12373.1, bsz=453.2, num_updates=1100, lr=4.4078e-05, gnorm=1.805, clip=0, loss_scale=8, train_wall=77, gb_free=16.6, wall=959
2023-08-07 14:20:14 | INFO | train_inner | epoch 001:   1204 / 1474 loss=9.117, trans_loss=6.865, nll_loss=6.093, w2v_ctc_loss=5.133, contrastive_loss=0, total=4126.86, n_correct=397.68, ppl=68.24, accuracy=9.636, wps=15709.1, ups=1.27, wpb=12348.8, bsz=436.5, num_updates=1200, lr=4.8076e-05, gnorm=1.557, clip=0, loss_scale=8, train_wall=78, gb_free=17, wall=1037
2023-08-07 14:21:31 | INFO | train_inner | epoch 001:   1304 / 1474 loss=8.785, trans_loss=6.798, nll_loss=6.009, w2v_ctc_loss=4.716, contrastive_loss=0, total=4059.86, n_correct=387.96, ppl=64.4, accuracy=9.556, wps=15728.1, ups=1.3, wpb=12127.2, bsz=444.9, num_updates=1300, lr=5.2074e-05, gnorm=1.384, clip=0, loss_scale=8, train_wall=77, gb_free=16.1, wall=1115
2023-08-07 14:22:49 | INFO | train_inner | epoch 001:   1404 / 1474 loss=8.42, trans_loss=6.725, nll_loss=5.918, w2v_ctc_loss=4.264, contrastive_loss=0, total=4130.35, n_correct=384.89, ppl=60.47, accuracy=9.319, wps=15898.2, ups=1.29, wpb=12294.9, bsz=451.7, num_updates=1400, lr=5.6072e-05, gnorm=1.319, clip=0, loss_scale=8, train_wall=77, gb_free=14.8, wall=1192
2023-08-07 14:23:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 14:24:19 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 8.824 | trans_loss 10.363 | nll_loss 9.262 | w2v_ctc_loss 5.234 | contrastive_loss 0 | total 4003.4 | n_correct 450.8 | ppl 614.02 | accuracy 11.26 | uer 58.113 | wer 55.181 | raw_wer 55.181 | bleu 0.02 | wps 1236.3 | wpb 4003.4 | bsz 141.8 | num_updates 1470
2023-08-07 14:24:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1470 updates
2023-08-07 14:24:19 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 14:24:22 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 14:24:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 1 @ 1470 updates, score 0.02) (writing took 6.667655656114221 seconds)
2023-08-07 14:24:26 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-08-07 14:24:26 | INFO | train | epoch 001 | loss 10.848 | trans_loss 7.322 | nll_loss 6.719 | w2v_ctc_loss 7.16 | contrastive_loss 0 | total 4138.81 | n_correct 338.726 | ppl 105.33 | accuracy 8.184 | wps 15251.4 | ups 1.23 | wpb 12356.9 | bsz 458.9 | num_updates 1470 | lr 5.88706e-05 | gnorm 2.021 | clip 1.8 | loss_scale 8 | train_wall 1156 | gb_free 16.4 | wall 1289
2023-08-07 14:24:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 14:24:26 | INFO | fairseq.trainer | begin training epoch 2
2023-08-07 14:24:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 14:24:59 | INFO | train_inner | epoch 002:     30 / 1474 loss=8.095, trans_loss=6.673, nll_loss=5.839, w2v_ctc_loss=3.824, contrastive_loss=0, total=4156.68, n_correct=396.24, ppl=57.25, accuracy=9.533, wps=9531, ups=0.77, wpb=12392.7, bsz=469.7, num_updates=1500, lr=6.007e-05, gnorm=1.418, clip=0, loss_scale=8, train_wall=77, gb_free=14.9, wall=1322
2023-08-07 14:26:17 | INFO | train_inner | epoch 002:    130 / 1474 loss=7.831, trans_loss=6.604, nll_loss=5.742, w2v_ctc_loss=3.502, contrastive_loss=0, total=4153.21, n_correct=405.02, ppl=53.51, accuracy=9.752, wps=15748.1, ups=1.27, wpb=12390.3, bsz=453.1, num_updates=1600, lr=6.4068e-05, gnorm=1.116, clip=0, loss_scale=8, train_wall=78, gb_free=16.4, wall=1401
2023-08-07 14:27:34 | INFO | train_inner | epoch 002:    230 / 1474 loss=7.512, trans_loss=6.483, nll_loss=5.59, w2v_ctc_loss=3.165, contrastive_loss=0, total=4205.43, n_correct=462.36, ppl=48.16, accuracy=10.994, wps=16304.2, ups=1.3, wpb=12554.2, bsz=495, num_updates=1700, lr=6.8066e-05, gnorm=1.279, clip=0, loss_scale=8, train_wall=77, gb_free=16.3, wall=1478
2023-08-07 14:28:52 | INFO | train_inner | epoch 002:    330 / 1474 loss=7.357, trans_loss=6.455, nll_loss=5.545, w2v_ctc_loss=2.958, contrastive_loss=0, total=4134.76, n_correct=463.13, ppl=46.69, accuracy=11.201, wps=15871.7, ups=1.28, wpb=12354.1, bsz=445.5, num_updates=1800, lr=7.2064e-05, gnorm=1.351, clip=0, loss_scale=8, train_wall=77, gb_free=16, wall=1555
2023-08-07 14:30:10 | INFO | train_inner | epoch 002:    430 / 1474 loss=7.194, trans_loss=6.393, nll_loss=5.467, w2v_ctc_loss=2.78, contrastive_loss=0, total=4029.31, n_correct=477.98, ppl=44.22, accuracy=11.863, wps=15581.6, ups=1.29, wpb=12057.1, bsz=413.4, num_updates=1900, lr=7.6062e-05, gnorm=1.325, clip=0, loss_scale=8, train_wall=77, gb_free=15.2, wall=1633
2023-08-07 14:31:28 | INFO | train_inner | epoch 002:    530 / 1474 loss=6.988, trans_loss=6.307, nll_loss=5.349, w2v_ctc_loss=2.578, contrastive_loss=0, total=4182.35, n_correct=533.62, ppl=40.75, accuracy=12.759, wps=16027.6, ups=1.28, wpb=12484.2, bsz=468.5, num_updates=2000, lr=8.006e-05, gnorm=1.307, clip=0, loss_scale=8, train_wall=77, gb_free=13.3, wall=1711
2023-08-07 14:31:28 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 14:32:04 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 7.685 | trans_loss 9.612 | nll_loss 8.259 | w2v_ctc_loss 3.188 | contrastive_loss 0 | total 4003.4 | n_correct 584 | ppl 306.33 | accuracy 14.588 | uer 40.79 | wer 39.424 | raw_wer 39.424 | bleu 0.09 | wps 1250.4 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_bleu 0.09
2023-08-07 14:32:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-08-07 14:32:04 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_2_2000.pt
2023-08-07 14:32:08 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_2_2000.pt
2023-08-07 14:32:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.09) (writing took 29.931947069242597 seconds)
2023-08-07 14:33:52 | INFO | train_inner | epoch 002:    630 / 1474 loss=6.857, trans_loss=6.259, nll_loss=5.29, w2v_ctc_loss=2.439, contrastive_loss=0, total=4127.74, n_correct=539.26, ppl=39.12, accuracy=13.064, wps=8492.3, ups=0.69, wpb=12303.9, bsz=449.2, num_updates=2100, lr=8.4058e-05, gnorm=1.209, clip=0, loss_scale=8, train_wall=78, gb_free=16.2, wall=1856
2023-08-07 14:35:09 | INFO | train_inner | epoch 002:    730 / 1474 loss=6.753, trans_loss=6.203, nll_loss=5.218, w2v_ctc_loss=2.351, contrastive_loss=0, total=4140.87, n_correct=569.96, ppl=37.21, accuracy=13.764, wps=16036.6, ups=1.3, wpb=12348.1, bsz=458.4, num_updates=2200, lr=8.8056e-05, gnorm=1.323, clip=0, loss_scale=16, train_wall=77, gb_free=16.7, wall=1933
2023-08-07 14:36:28 | INFO | train_inner | epoch 002:    830 / 1474 loss=6.663, trans_loss=6.165, nll_loss=5.169, w2v_ctc_loss=2.26, contrastive_loss=0, total=4167.27, n_correct=581.04, ppl=35.97, accuracy=13.943, wps=15875.5, ups=1.27, wpb=12456.1, bsz=461.2, num_updates=2300, lr=9.2054e-05, gnorm=1.091, clip=0, loss_scale=16, train_wall=78, gb_free=14.9, wall=2011
2023-08-07 14:37:45 | INFO | train_inner | epoch 002:    930 / 1474 loss=6.57, trans_loss=6.125, nll_loss=5.118, w2v_ctc_loss=2.177, contrastive_loss=0, total=4104.57, n_correct=578.24, ppl=34.73, accuracy=14.088, wps=15830, ups=1.29, wpb=12244, bsz=445.4, num_updates=2400, lr=9.6052e-05, gnorm=1.102, clip=0, loss_scale=16, train_wall=77, gb_free=16.2, wall=2088
2023-08-07 14:39:03 | INFO | train_inner | epoch 002:   1030 / 1474 loss=6.494, trans_loss=6.096, nll_loss=5.075, w2v_ctc_loss=2.095, contrastive_loss=0, total=4101.39, n_correct=605.47, ppl=33.7, accuracy=14.763, wps=15750.2, ups=1.28, wpb=12263.6, bsz=455, num_updates=2500, lr=0.00010005, gnorm=1.088, clip=0, loss_scale=16, train_wall=77, gb_free=15.5, wall=2166
2023-08-07 14:40:21 | INFO | train_inner | epoch 002:   1130 / 1474 loss=6.398, trans_loss=6.035, nll_loss=4.999, w2v_ctc_loss=2.034, contrastive_loss=0, total=4197.97, n_correct=642.9, ppl=31.99, accuracy=15.315, wps=16047.3, ups=1.28, wpb=12519.4, bsz=489.8, num_updates=2600, lr=0.000104048, gnorm=1.053, clip=0, loss_scale=16, train_wall=78, gb_free=16.2, wall=2244
2023-08-07 14:41:39 | INFO | train_inner | epoch 002:   1230 / 1474 loss=6.339, trans_loss=5.997, nll_loss=4.95, w2v_ctc_loss=1.991, contrastive_loss=0, total=4224.76, n_correct=658.22, ppl=30.91, accuracy=15.58, wps=16101.5, ups=1.28, wpb=12596.7, bsz=492.8, num_updates=2700, lr=0.000108046, gnorm=1.058, clip=0, loss_scale=16, train_wall=78, gb_free=11.3, wall=2323
2023-08-07 14:42:56 | INFO | train_inner | epoch 002:   1330 / 1474 loss=6.295, trans_loss=5.991, nll_loss=4.942, w2v_ctc_loss=1.937, contrastive_loss=0, total=4149.37, n_correct=631.62, ppl=30.73, accuracy=15.222, wps=16158.5, ups=1.3, wpb=12401.3, bsz=460.4, num_updates=2800, lr=0.000112044, gnorm=0.935, clip=0, loss_scale=16, train_wall=76, gb_free=17.3, wall=2399
2023-08-07 14:44:15 | INFO | train_inner | epoch 002:   1430 / 1474 loss=6.277, trans_loss=5.977, nll_loss=4.918, w2v_ctc_loss=1.925, contrastive_loss=0, total=4055.67, n_correct=621.89, ppl=30.24, accuracy=15.334, wps=15448.8, ups=1.27, wpb=12135.9, bsz=440, num_updates=2900, lr=0.000116042, gnorm=0.979, clip=0, loss_scale=16, train_wall=78, gb_free=17, wall=2478
2023-08-07 14:44:49 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 14:45:26 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 7.161 | trans_loss 9.271 | nll_loss 7.783 | w2v_ctc_loss 2.238 | contrastive_loss 0 | total 4003.4 | n_correct 698.5 | ppl 220.21 | accuracy 17.448 | uer 31.922 | wer 31.736 | raw_wer 31.736 | bleu 0.17 | wps 1236.7 | wpb 4003.4 | bsz 141.8 | num_updates 2944 | best_bleu 0.17
2023-08-07 14:45:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2944 updates
2023-08-07 14:45:26 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 14:45:40 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 14:45:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 2 @ 2944 updates, score 0.17) (writing took 27.30141180753708 seconds)
2023-08-07 14:45:53 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-08-07 14:45:53 | INFO | train | epoch 002 | loss 6.83 | trans_loss 6.221 | nll_loss 5.242 | w2v_ctc_loss 2.453 | contrastive_loss 0 | total 4138.65 | n_correct 553.832 | ppl 37.84 | accuracy 13.382 | wps 14145.1 | ups 1.14 | wpb 12355.8 | bsz 458.5 | num_updates 2944 | lr 0.000117801 | gnorm 1.165 | clip 0 | loss_scale 16 | train_wall 1140 | gb_free 14.7 | wall 2576
2023-08-07 14:45:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 14:45:54 | INFO | fairseq.trainer | begin training epoch 3
2023-08-07 14:45:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 14:46:46 | INFO | train_inner | epoch 003:     56 / 1474 loss=6.177, trans_loss=5.92, nll_loss=4.847, w2v_ctc_loss=1.856, contrastive_loss=0, total=4065.2, n_correct=643.64, ppl=28.79, accuracy=15.833, wps=7994.3, ups=0.66, wpb=12124.7, bsz=439.6, num_updates=3000, lr=0.00012004, gnorm=0.924, clip=0, loss_scale=16, train_wall=78, gb_free=17.4, wall=2630
2023-08-07 14:48:03 | INFO | train_inner | epoch 003:    156 / 1474 loss=5.949, trans_loss=5.649, nll_loss=4.484, w2v_ctc_loss=1.808, contrastive_loss=0, total=4137.82, n_correct=947.38, ppl=22.37, accuracy=22.896, wps=16032.9, ups=1.3, wpb=12357.4, bsz=455, num_updates=3100, lr=0.000124038, gnorm=1.17, clip=0, loss_scale=16, train_wall=77, gb_free=16.1, wall=2707
2023-08-07 14:49:22 | INFO | train_inner | epoch 003:    256 / 1474 loss=5.839, trans_loss=5.55, nll_loss=4.354, w2v_ctc_loss=1.758, contrastive_loss=0, total=4148.62, n_correct=1044.96, ppl=20.45, accuracy=25.188, wps=15824.5, ups=1.27, wpb=12412.1, bsz=463.7, num_updates=3200, lr=0.000128036, gnorm=1.047, clip=0, loss_scale=16, train_wall=78, gb_free=16.4, wall=2785
2023-08-07 14:50:40 | INFO | train_inner | epoch 003:    356 / 1474 loss=5.786, trans_loss=5.498, nll_loss=4.284, w2v_ctc_loss=1.745, contrastive_loss=0, total=4171.73, n_correct=1087.72, ppl=19.49, accuracy=26.074, wps=15977.5, ups=1.29, wpb=12433.2, bsz=468.2, num_updates=3300, lr=0.000132034, gnorm=1.124, clip=0, loss_scale=16, train_wall=77, gb_free=15.3, wall=2863
2023-08-07 14:51:58 | INFO | train_inner | epoch 003:    456 / 1474 loss=5.713, trans_loss=5.432, nll_loss=4.199, w2v_ctc_loss=1.724, contrastive_loss=0, total=4197.48, n_correct=1135.15, ppl=18.36, accuracy=27.044, wps=15897.1, ups=1.27, wpb=12498.7, bsz=474.4, num_updates=3400, lr=0.000136032, gnorm=1.05, clip=0, loss_scale=16, train_wall=78, gb_free=12.5, wall=2941
2023-08-07 14:53:16 | INFO | train_inner | epoch 003:    556 / 1474 loss=5.685, trans_loss=5.421, nll_loss=4.179, w2v_ctc_loss=1.679, contrastive_loss=0, total=4095.16, n_correct=1126.73, ppl=18.12, accuracy=27.514, wps=15840.4, ups=1.29, wpb=12260.8, bsz=440.6, num_updates=3500, lr=0.00014003, gnorm=1.024, clip=0, loss_scale=16, train_wall=77, gb_free=15.7, wall=3019
2023-08-07 14:54:35 | INFO | train_inner | epoch 003:    656 / 1474 loss=5.599, trans_loss=5.346, nll_loss=4.081, w2v_ctc_loss=1.655, contrastive_loss=0, total=4223.37, n_correct=1210.73, ppl=16.92, accuracy=28.667, wps=15915.8, ups=1.27, wpb=12563.9, bsz=484, num_updates=3600, lr=0.000144028, gnorm=1.028, clip=0, loss_scale=16, train_wall=78, gb_free=16.4, wall=3098
2023-08-07 14:55:52 | INFO | train_inner | epoch 003:    756 / 1474 loss=5.564, trans_loss=5.308, nll_loss=4.029, w2v_ctc_loss=1.643, contrastive_loss=0, total=4165.36, n_correct=1213.98, ppl=16.33, accuracy=29.145, wps=16099.3, ups=1.29, wpb=12459.4, bsz=470.9, num_updates=3700, lr=0.000148026, gnorm=0.978, clip=0, loss_scale=16, train_wall=77, gb_free=15, wall=3175
2023-08-07 14:57:11 | INFO | train_inner | epoch 003:    856 / 1474 loss=5.518, trans_loss=5.279, nll_loss=3.99, w2v_ctc_loss=1.614, contrastive_loss=0, total=4164.94, n_correct=1229.35, ppl=15.88, accuracy=29.517, wps=15836.5, ups=1.27, wpb=12437, bsz=457.9, num_updates=3800, lr=0.000152024, gnorm=0.997, clip=0, loss_scale=16, train_wall=78, gb_free=15, wall=3254
2023-08-07 14:58:30 | INFO | train_inner | epoch 003:    956 / 1474 loss=5.47, trans_loss=5.225, nll_loss=3.918, w2v_ctc_loss=1.613, contrastive_loss=0, total=4156.71, n_correct=1262.22, ppl=15.11, accuracy=30.366, wps=15668.6, ups=1.26, wpb=12394, bsz=465.4, num_updates=3900, lr=0.000156022, gnorm=0.991, clip=0, loss_scale=16, train_wall=79, gb_free=16.7, wall=3333
2023-08-07 14:59:47 | INFO | train_inner | epoch 003:   1056 / 1474 loss=5.452, trans_loss=5.203, nll_loss=3.888, w2v_ctc_loss=1.612, contrastive_loss=0, total=4063.38, n_correct=1250.59, ppl=14.8, accuracy=30.777, wps=15701.1, ups=1.29, wpb=12148.1, bsz=439.1, num_updates=4000, lr=0.00016002, gnorm=1.038, clip=0, loss_scale=16, train_wall=77, gb_free=17.3, wall=3410
2023-08-07 14:59:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 15:00:10 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.88 | trans_loss 7.624 | nll_loss 5.555 | w2v_ctc_loss 1.812 | contrastive_loss 0 | total 4003.4 | n_correct 1390.6 | ppl 47 | accuracy 34.735 | uer 27.2 | wer 28.034 | raw_wer 28.034 | bleu 6.06 | wps 2206.2 | wpb 4003.4 | bsz 141.8 | num_updates 4000 | best_bleu 6.06
2023-08-07 15:00:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-08-07 15:00:10 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_3_4000.pt
2023-08-07 15:00:14 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_3_4000.pt
2023-08-07 15:00:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 6.06) (writing took 39.53540641069412 seconds)
2023-08-07 15:02:07 | INFO | train_inner | epoch 003:   1156 / 1474 loss=5.394, trans_loss=5.163, nll_loss=3.833, w2v_ctc_loss=1.575, contrastive_loss=0, total=4049.57, n_correct=1264.48, ppl=14.25, accuracy=31.225, wps=8620.9, ups=0.71, wpb=12092.3, bsz=438, num_updates=4100, lr=0.000164018, gnorm=0.941, clip=0, loss_scale=16, train_wall=77, gb_free=16.8, wall=3550
2023-08-07 15:03:24 | INFO | train_inner | epoch 003:   1256 / 1474 loss=5.36, trans_loss=5.13, nll_loss=3.79, w2v_ctc_loss=1.561, contrastive_loss=0, total=4061.36, n_correct=1290.7, ppl=13.84, accuracy=31.78, wps=15895.5, ups=1.31, wpb=12143.7, bsz=433.6, num_updates=4200, lr=0.000168016, gnorm=1.003, clip=0, loss_scale=16, train_wall=76, gb_free=16.2, wall=3627
2023-08-07 15:04:43 | INFO | train_inner | epoch 003:   1356 / 1474 loss=5.287, trans_loss=5.069, nll_loss=3.712, w2v_ctc_loss=1.539, contrastive_loss=0, total=4141.12, n_correct=1344.41, ppl=13.11, accuracy=32.465, wps=15611.2, ups=1.26, wpb=12347.7, bsz=463.2, num_updates=4300, lr=0.000172014, gnorm=0.966, clip=0, loss_scale=32, train_wall=79, gb_free=14.7, wall=3706
2023-08-07 15:06:01 | INFO | train_inner | epoch 003:   1456 / 1474 loss=5.244, trans_loss=5.025, nll_loss=3.652, w2v_ctc_loss=1.527, contrastive_loss=0, total=4211.06, n_correct=1403.88, ppl=12.57, accuracy=33.338, wps=16078.7, ups=1.28, wpb=12575.8, bsz=477.6, num_updates=4400, lr=0.000176012, gnorm=0.933, clip=0, loss_scale=32, train_wall=78, gb_free=17, wall=3784
2023-08-07 15:06:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 15:06:38 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.68 | trans_loss 7.39 | nll_loss 5.254 | w2v_ctc_loss 1.689 | contrastive_loss 0 | total 4003.4 | n_correct 1489.3 | ppl 38.15 | accuracy 37.201 | uer 25.493 | wer 26.352 | raw_wer 26.352 | bleu 6.9 | wps 2131.2 | wpb 4003.4 | bsz 141.8 | num_updates 4418 | best_bleu 6.9
2023-08-07 15:06:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4418 updates
2023-08-07 15:06:38 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 15:06:53 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 15:07:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 3 @ 4418 updates, score 6.9) (writing took 27.432457426562905 seconds)
2023-08-07 15:07:06 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-08-07 15:07:06 | INFO | train | epoch 003 | loss 5.58 | trans_loss 5.327 | nll_loss 4.054 | w2v_ctc_loss 1.652 | contrastive_loss 0 | total 4138.65 | n_correct 1181.63 | ppl 16.62 | accuracy 28.551 | wps 14309.1 | ups 1.16 | wpb 12355.8 | bsz 458.5 | num_updates 4418 | lr 0.000176732 | gnorm 1.014 | clip 0 | loss_scale 32 | train_wall 1143 | gb_free 16.4 | wall 3849
2023-08-07 15:07:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 15:07:06 | INFO | fairseq.trainer | begin training epoch 4
2023-08-07 15:07:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 15:08:18 | INFO | train_inner | epoch 004:     82 / 1474 loss=5.174, trans_loss=4.981, nll_loss=3.594, w2v_ctc_loss=1.479, contrastive_loss=0, total=4090.08, n_correct=1381.52, ppl=12.08, accuracy=33.777, wps=8916.2, ups=0.73, wpb=12189, bsz=437, num_updates=4500, lr=0.00018001, gnorm=0.933, clip=0, loss_scale=32, train_wall=77, gb_free=16.4, wall=3921
2023-08-07 15:09:35 | INFO | train_inner | epoch 004:    182 / 1474 loss=5.123, trans_loss=4.93, nll_loss=3.524, w2v_ctc_loss=1.465, contrastive_loss=0, total=4179.17, n_correct=1456.63, ppl=11.51, accuracy=34.855, wps=16209.9, ups=1.3, wpb=12477.4, bsz=468, num_updates=4600, lr=0.000184008, gnorm=0.906, clip=0, loss_scale=32, train_wall=77, gb_free=10.9, wall=3998
2023-08-07 15:10:53 | INFO | train_inner | epoch 004:    282 / 1474 loss=5.111, trans_loss=4.911, nll_loss=3.5, w2v_ctc_loss=1.47, contrastive_loss=0, total=4139.78, n_correct=1450.31, ppl=11.31, accuracy=35.034, wps=15764.7, ups=1.27, wpb=12374, bsz=462.4, num_updates=4700, lr=0.000188006, gnorm=0.921, clip=0, loss_scale=32, train_wall=78, gb_free=16.8, wall=4076
2023-08-07 15:12:11 | INFO | train_inner | epoch 004:    382 / 1474 loss=5.082, trans_loss=4.888, nll_loss=3.467, w2v_ctc_loss=1.46, contrastive_loss=0, total=4132.58, n_correct=1457.84, ppl=11.05, accuracy=35.277, wps=15856.1, ups=1.29, wpb=12311.2, bsz=443.4, num_updates=4800, lr=0.000192004, gnorm=0.884, clip=0, loss_scale=32, train_wall=77, gb_free=16.1, wall=4154
2023-08-07 15:13:29 | INFO | train_inner | epoch 004:    482 / 1474 loss=5.012, trans_loss=4.827, nll_loss=3.388, w2v_ctc_loss=1.432, contrastive_loss=0, total=4193.12, n_correct=1532.77, ppl=10.47, accuracy=36.554, wps=16028.9, ups=1.28, wpb=12504.6, bsz=487.4, num_updates=4900, lr=0.000196002, gnorm=0.872, clip=0, loss_scale=32, train_wall=78, gb_free=13.8, wall=4232
2023-08-07 15:14:47 | INFO | train_inner | epoch 004:    582 / 1474 loss=4.995, trans_loss=4.797, nll_loss=3.347, w2v_ctc_loss=1.448, contrastive_loss=0, total=4234.09, n_correct=1567.17, ppl=10.17, accuracy=37.013, wps=16132.7, ups=1.28, wpb=12624.9, bsz=495.1, num_updates=5000, lr=0.0002, gnorm=0.894, clip=0, loss_scale=32, train_wall=78, gb_free=16.4, wall=4310
mt_weight tensor(0.8922, device='cuda:0')
asr_weight tensor(0.8730, device='cuda:0')
2023-08-07 15:16:06 | INFO | train_inner | epoch 004:    682 / 1474 loss=4.984, trans_loss=4.796, nll_loss=3.341, w2v_ctc_loss=1.429, contrastive_loss=0, total=4195.52, n_correct=1551.64, ppl=10.13, accuracy=36.983, wps=15840.5, ups=1.27, wpb=12500.6, bsz=461.1, num_updates=5100, lr=0.00019803, gnorm=0.791, clip=0, loss_scale=32, train_wall=78, gb_free=13.6, wall=4389
2023-08-07 15:17:24 | INFO | train_inner | epoch 004:    782 / 1474 loss=4.974, trans_loss=4.776, nll_loss=3.315, w2v_ctc_loss=1.435, contrastive_loss=0, total=4009.24, n_correct=1497.24, ppl=9.95, accuracy=37.345, wps=15313.5, ups=1.28, wpb=11999.4, bsz=415.7, num_updates=5200, lr=0.000196116, gnorm=0.8, clip=0, loss_scale=32, train_wall=78, gb_free=16.8, wall=4468
2023-08-07 15:18:43 | INFO | train_inner | epoch 004:    882 / 1474 loss=4.924, trans_loss=4.726, nll_loss=3.253, w2v_ctc_loss=1.429, contrastive_loss=0, total=4191.91, n_correct=1597.42, ppl=9.53, accuracy=38.107, wps=15978.3, ups=1.28, wpb=12508.3, bsz=467.3, num_updates=5300, lr=0.000194257, gnorm=0.785, clip=0, loss_scale=32, train_wall=78, gb_free=14.9, wall=4546
2023-08-07 15:20:01 | INFO | train_inner | epoch 004:    982 / 1474 loss=4.882, trans_loss=4.693, nll_loss=3.21, w2v_ctc_loss=1.409, contrastive_loss=0, total=4129.3, n_correct=1597.39, ppl=9.25, accuracy=38.684, wps=15737, ups=1.28, wpb=12334.3, bsz=458.1, num_updates=5400, lr=0.00019245, gnorm=0.768, clip=0, loss_scale=32, train_wall=78, gb_free=17.6, wall=4624
2023-08-07 15:20:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-07 15:21:20 | INFO | train_inner | epoch 004:   1083 / 1474 loss=4.874, trans_loss=4.687, nll_loss=3.198, w2v_ctc_loss=1.407, contrastive_loss=0, total=4067.51, n_correct=1573.37, ppl=9.17, accuracy=38.681, wps=15418.3, ups=1.27, wpb=12143.7, bsz=434.6, num_updates=5500, lr=0.000190693, gnorm=0.78, clip=0, loss_scale=16, train_wall=78, gb_free=16.1, wall=4703
2023-08-07 15:22:38 | INFO | train_inner | epoch 004:   1183 / 1474 loss=4.843, trans_loss=4.647, nll_loss=3.147, w2v_ctc_loss=1.403, contrastive_loss=0, total=4161.85, n_correct=1647.27, ppl=8.86, accuracy=39.58, wps=15878.1, ups=1.27, wpb=12456.2, bsz=483.8, num_updates=5600, lr=0.000188982, gnorm=0.771, clip=0, loss_scale=16, train_wall=78, gb_free=12.4, wall=4781
2023-08-07 15:23:56 | INFO | train_inner | epoch 004:   1283 / 1474 loss=4.808, trans_loss=4.619, nll_loss=3.109, w2v_ctc_loss=1.385, contrastive_loss=0, total=4152.03, n_correct=1667.44, ppl=8.63, accuracy=40.16, wps=16038, ups=1.29, wpb=12413.9, bsz=471, num_updates=5700, lr=0.000187317, gnorm=0.754, clip=0, loss_scale=16, train_wall=77, gb_free=16.3, wall=4859
2023-08-07 15:25:13 | INFO | train_inner | epoch 004:   1383 / 1474 loss=4.791, trans_loss=4.603, nll_loss=3.089, w2v_ctc_loss=1.382, contrastive_loss=0, total=4099.25, n_correct=1650.73, ppl=8.51, accuracy=40.269, wps=15931, ups=1.3, wpb=12246, bsz=436.9, num_updates=5800, lr=0.000185695, gnorm=0.746, clip=0, loss_scale=16, train_wall=76, gb_free=11.7, wall=4936
2023-08-07 15:26:23 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.8922, device='cuda:7')
asr_weight tensor(0.8730, device='cuda:7')
mt_weight tensor(0.8922, device='cuda:5')
asr_weight tensor(0.8730, device='cuda:5')
mt_weight tensor(0.8922, device='cuda:2')
asr_weight tensor(0.8730, device='cuda:2')
mt_weight tensor(0.8922, device='cuda:4')
asr_weight tensor(0.8730, device='cuda:4')
mt_weight tensor(0.8922, device='cuda:1')
asr_weight tensor(0.8730, device='cuda:1')
mt_weight tensor(0.8922, device='cuda:3')
asr_weight tensor(0.8730, device='cuda:3')
mt_weight tensor(0.8922, device='cuda:6')
asr_weight tensor(0.8730, device='cuda:6')
2023-08-07 15:26:49 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 5.15 | trans_loss 6.707 | nll_loss 4.335 | w2v_ctc_loss 1.517 | contrastive_loss 0 | total 4003.4 | n_correct 1824.2 | ppl 20.18 | accuracy 45.566 | uer 21.915 | wer 23.299 | raw_wer 23.299 | bleu 10.83 | wps 1811.8 | wpb 4003.4 | bsz 141.8 | num_updates 5891 | best_bleu 10.83
2023-08-07 15:26:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5891 updates
2023-08-07 15:26:49 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 15:27:04 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 15:27:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 4 @ 5891 updates, score 10.83) (writing took 26.97866870649159 seconds)
2023-08-07 15:27:16 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-08-07 15:27:16 | INFO | train | epoch 004 | loss 4.954 | trans_loss 4.763 | nll_loss 3.301 | w2v_ctc_loss 1.426 | contrastive_loss 0 | total 4138.35 | n_correct 1554.2 | ppl 9.86 | accuracy 37.556 | wps 15037.3 | ups 1.22 | wpb 12355.1 | bsz 458.4 | num_updates 5891 | lr 0.000184256 | gnorm 0.823 | clip 0 | loss_scale 16 | train_wall 1141 | gb_free 14.7 | wall 5060
2023-08-07 15:27:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 15:27:17 | INFO | fairseq.trainer | begin training epoch 5
2023-08-07 15:27:17 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 15:27:32 | INFO | train_inner | epoch 005:      9 / 1474 loss=4.761, trans_loss=4.584, nll_loss=3.064, w2v_ctc_loss=1.363, contrastive_loss=0, total=4035.57, n_correct=1641.16, ppl=8.36, accuracy=40.667, wps=8663.3, ups=0.72, wpb=12056.3, bsz=437.3, num_updates=5900, lr=0.000184115, gnorm=0.762, clip=0, loss_scale=16, train_wall=76, gb_free=15.8, wall=5075
2023-08-07 15:28:50 | INFO | train_inner | epoch 005:    109 / 1474 loss=4.626, trans_loss=4.494, nll_loss=2.947, w2v_ctc_loss=1.278, contrastive_loss=0, total=4249.55, n_correct=1803.53, ppl=7.71, accuracy=42.44, wps=16138.3, ups=1.27, wpb=12691.5, bsz=497, num_updates=6000, lr=0.000182574, gnorm=0.726, clip=0, loss_scale=16, train_wall=78, gb_free=17.4, wall=5154
2023-08-07 15:28:50 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 15:29:14 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 5.125 | trans_loss 6.673 | nll_loss 4.286 | w2v_ctc_loss 1.511 | contrastive_loss 0 | total 4003.4 | n_correct 1846.6 | ppl 19.51 | accuracy 46.126 | uer 21.482 | wer 22.889 | raw_wer 22.889 | bleu 11.18 | wps 2129.1 | wpb 4003.4 | bsz 141.8 | num_updates 6000 | best_bleu 11.18
2023-08-07 15:29:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6000 updates
2023-08-07 15:29:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_5_6000.pt
2023-08-07 15:29:17 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_5_6000.pt
2023-08-07 15:30:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_5_6000.pt (epoch 5 @ 6000 updates, score 11.18) (writing took 48.191906813532114 seconds)
2023-08-07 15:31:19 | INFO | train_inner | epoch 005:    209 / 1474 loss=4.637, trans_loss=4.488, nll_loss=2.937, w2v_ctc_loss=1.302, contrastive_loss=0, total=4195.93, n_correct=1777.03, ppl=7.66, accuracy=42.351, wps=8397.7, ups=0.67, wpb=12507.8, bsz=490.2, num_updates=6100, lr=0.000181071, gnorm=0.742, clip=0, loss_scale=16, train_wall=77, gb_free=15.7, wall=5303
2023-08-07 15:32:37 | INFO | train_inner | epoch 005:    309 / 1474 loss=4.64, trans_loss=4.481, nll_loss=2.93, w2v_ctc_loss=1.311, contrastive_loss=0, total=4092.41, n_correct=1733.33, ppl=7.62, accuracy=42.355, wps=15865.3, ups=1.3, wpb=12249.4, bsz=445.2, num_updates=6200, lr=0.000179605, gnorm=0.747, clip=0, loss_scale=16, train_wall=77, gb_free=15.9, wall=5380
2023-08-07 15:33:54 | INFO | train_inner | epoch 005:    409 / 1474 loss=4.596, trans_loss=4.453, nll_loss=2.897, w2v_ctc_loss=1.285, contrastive_loss=0, total=4142.04, n_correct=1781.19, ppl=7.45, accuracy=43.003, wps=15869.4, ups=1.28, wpb=12363.4, bsz=471.4, num_updates=6300, lr=0.000178174, gnorm=0.744, clip=0, loss_scale=16, train_wall=77, gb_free=17, wall=5458
2023-08-07 15:35:12 | INFO | train_inner | epoch 005:    509 / 1474 loss=4.618, trans_loss=4.47, nll_loss=2.914, w2v_ctc_loss=1.29, contrastive_loss=0, total=4031.44, n_correct=1718.85, ppl=7.54, accuracy=42.636, wps=15523.2, ups=1.29, wpb=12063.4, bsz=417.2, num_updates=6400, lr=0.000176777, gnorm=0.739, clip=0, loss_scale=16, train_wall=77, gb_free=12.2, wall=5535
2023-08-07 15:36:31 | INFO | train_inner | epoch 005:    609 / 1474 loss=4.585, trans_loss=4.444, nll_loss=2.879, w2v_ctc_loss=1.283, contrastive_loss=0, total=4113.46, n_correct=1771.85, ppl=7.35, accuracy=43.074, wps=15645, ups=1.28, wpb=12259.5, bsz=452, num_updates=6500, lr=0.000175412, gnorm=0.748, clip=0, loss_scale=16, train_wall=78, gb_free=17.3, wall=5614
2023-08-07 15:37:48 | INFO | train_inner | epoch 005:    709 / 1474 loss=4.566, trans_loss=4.414, nll_loss=2.843, w2v_ctc_loss=1.29, contrastive_loss=0, total=4167.56, n_correct=1822.21, ppl=7.18, accuracy=43.724, wps=16059.9, ups=1.29, wpb=12429.2, bsz=478.3, num_updates=6600, lr=0.000174078, gnorm=0.733, clip=0, loss_scale=16, train_wall=77, gb_free=11.7, wall=5691
2023-08-07 15:39:06 | INFO | train_inner | epoch 005:    809 / 1474 loss=4.556, trans_loss=4.414, nll_loss=2.84, w2v_ctc_loss=1.274, contrastive_loss=0, total=4135.12, n_correct=1809.37, ppl=7.16, accuracy=43.756, wps=15795, ups=1.28, wpb=12344, bsz=451.3, num_updates=6700, lr=0.000172774, gnorm=0.723, clip=0, loss_scale=16, train_wall=78, gb_free=16.2, wall=5769
2023-08-07 15:40:23 | INFO | train_inner | epoch 005:    909 / 1474 loss=4.54, trans_loss=4.396, nll_loss=2.817, w2v_ctc_loss=1.268, contrastive_loss=0, total=4089.65, n_correct=1804.33, ppl=7.05, accuracy=44.119, wps=15800.3, ups=1.29, wpb=12220.6, bsz=442.2, num_updates=6800, lr=0.000171499, gnorm=0.733, clip=0, loss_scale=16, train_wall=77, gb_free=16.6, wall=5847
2023-08-07 15:41:41 | INFO | train_inner | epoch 005:   1009 / 1474 loss=4.529, trans_loss=4.383, nll_loss=2.802, w2v_ctc_loss=1.269, contrastive_loss=0, total=4164.77, n_correct=1852.41, ppl=6.98, accuracy=44.478, wps=15927.4, ups=1.28, wpb=12424.9, bsz=465.4, num_updates=6900, lr=0.000170251, gnorm=0.717, clip=0, loss_scale=16, train_wall=78, gb_free=16.8, wall=5925
2023-08-07 15:43:00 | INFO | train_inner | epoch 005:   1109 / 1474 loss=4.528, trans_loss=4.375, nll_loss=2.787, w2v_ctc_loss=1.281, contrastive_loss=0, total=4170.22, n_correct=1864.92, ppl=6.9, accuracy=44.72, wps=15948.8, ups=1.28, wpb=12451.3, bsz=464.2, num_updates=7000, lr=0.000169031, gnorm=0.722, clip=0, loss_scale=16, train_wall=78, gb_free=15.7, wall=6003
2023-08-07 15:44:17 | INFO | train_inner | epoch 005:   1209 / 1474 loss=4.499, trans_loss=4.36, nll_loss=2.769, w2v_ctc_loss=1.257, contrastive_loss=0, total=4173.69, n_correct=1878.72, ppl=6.82, accuracy=45.013, wps=15994.9, ups=1.29, wpb=12438.5, bsz=455.4, num_updates=7100, lr=0.000167836, gnorm=0.722, clip=0, loss_scale=16, train_wall=77, gb_free=16.3, wall=6080
2023-08-07 15:45:35 | INFO | train_inner | epoch 005:   1309 / 1474 loss=4.485, trans_loss=4.351, nll_loss=2.758, w2v_ctc_loss=1.244, contrastive_loss=0, total=4131.5, n_correct=1860.28, ppl=6.76, accuracy=45.027, wps=15926.6, ups=1.29, wpb=12347.9, bsz=444.3, num_updates=7200, lr=0.000166667, gnorm=0.717, clip=0, loss_scale=16, train_wall=77, gb_free=17.5, wall=6158
2023-08-07 15:46:52 | INFO | train_inner | epoch 005:   1409 / 1474 loss=4.455, trans_loss=4.323, nll_loss=2.725, w2v_ctc_loss=1.243, contrastive_loss=0, total=4134.78, n_correct=1874.01, ppl=6.61, accuracy=45.323, wps=15918.2, ups=1.29, wpb=12344.9, bsz=458.6, num_updates=7300, lr=0.000165521, gnorm=0.715, clip=0, loss_scale=16, train_wall=77, gb_free=16.4, wall=6236
2023-08-07 15:47:44 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 15:48:07 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.891 | trans_loss 6.374 | nll_loss 3.906 | w2v_ctc_loss 1.432 | contrastive_loss 0 | total 4003.4 | n_correct 2000.5 | ppl 14.99 | accuracy 49.97 | uer 21.334 | wer 22.81 | raw_wer 22.81 | bleu 13.42 | wps 2103.3 | wpb 4003.4 | bsz 141.8 | num_updates 7365 | best_bleu 13.42
2023-08-07 15:48:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7365 updates
2023-08-07 15:48:07 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 15:48:22 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 15:48:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 5 @ 7365 updates, score 13.42) (writing took 26.48933775536716 seconds)
2023-08-07 15:48:34 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-08-07 15:48:34 | INFO | train | epoch 005 | loss 4.558 | trans_loss 4.414 | nll_loss 2.841 | w2v_ctc_loss 1.276 | contrastive_loss 0 | total 4138.65 | n_correct 1812.05 | ppl 7.17 | accuracy 43.784 | wps 14254.6 | ups 1.15 | wpb 12355.8 | bsz 458.5 | num_updates 7365 | lr 0.000164789 | gnorm 0.731 | clip 0 | loss_scale 16 | train_wall 1140 | gb_free 16 | wall 6337
2023-08-07 15:48:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 15:48:34 | INFO | fairseq.trainer | begin training epoch 6
2023-08-07 15:48:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 15:49:10 | INFO | train_inner | epoch 006:     35 / 1474 loss=4.435, trans_loss=4.299, nll_loss=2.692, w2v_ctc_loss=1.238, contrastive_loss=0, total=4118.2, n_correct=1895.41, ppl=6.46, accuracy=46.025, wps=8934.8, ups=0.73, wpb=12283.7, bsz=449.9, num_updates=7400, lr=0.000164399, gnorm=0.725, clip=0, loss_scale=16, train_wall=78, gb_free=15.6, wall=6373
2023-08-07 15:50:27 | INFO | train_inner | epoch 006:    135 / 1474 loss=4.365, trans_loss=4.256, nll_loss=2.638, w2v_ctc_loss=1.185, contrastive_loss=0, total=4151.34, n_correct=1938.63, ppl=6.22, accuracy=46.699, wps=16049.1, ups=1.29, wpb=12403.4, bsz=455.9, num_updates=7500, lr=0.000163299, gnorm=0.718, clip=0, loss_scale=32, train_wall=77, gb_free=11.5, wall=6450
2023-08-07 15:51:44 | INFO | train_inner | epoch 006:    235 / 1474 loss=4.39, trans_loss=4.26, nll_loss=2.644, w2v_ctc_loss=1.221, contrastive_loss=0, total=4116.84, n_correct=1910.61, ppl=6.25, accuracy=46.41, wps=15911, ups=1.29, wpb=12293.5, bsz=437.8, num_updates=7600, lr=0.000162221, gnorm=0.721, clip=0, loss_scale=32, train_wall=77, gb_free=16.3, wall=6528
2023-08-07 15:53:03 | INFO | train_inner | epoch 006:    335 / 1474 loss=4.336, trans_loss=4.231, nll_loss=2.606, w2v_ctc_loss=1.174, contrastive_loss=0, total=4142.94, n_correct=1959.05, ppl=6.09, accuracy=47.286, wps=15760.5, ups=1.27, wpb=12367.6, bsz=474.2, num_updates=7700, lr=0.000161165, gnorm=0.714, clip=0, loss_scale=32, train_wall=78, gb_free=14.7, wall=6606
2023-08-07 15:53:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-07 15:54:22 | INFO | train_inner | epoch 006:    436 / 1474 loss=4.335, trans_loss=4.227, nll_loss=2.6, w2v_ctc_loss=1.179, contrastive_loss=0, total=4154.57, n_correct=1967.36, ppl=6.06, accuracy=47.354, wps=15752.5, ups=1.27, wpb=12407.2, bsz=470.2, num_updates=7800, lr=0.000160128, gnorm=0.707, clip=0, loss_scale=16, train_wall=78, gb_free=16, wall=6685
2023-08-07 15:55:39 | INFO | train_inner | epoch 006:    536 / 1474 loss=4.348, trans_loss=4.229, nll_loss=2.604, w2v_ctc_loss=1.197, contrastive_loss=0, total=4167.79, n_correct=1969.78, ppl=6.08, accuracy=47.262, wps=15995.8, ups=1.29, wpb=12419.2, bsz=455.2, num_updates=7900, lr=0.000159111, gnorm=0.716, clip=0, loss_scale=16, train_wall=77, gb_free=15.6, wall=6762
2023-08-07 15:56:56 | INFO | train_inner | epoch 006:    636 / 1474 loss=4.322, trans_loss=4.216, nll_loss=2.582, w2v_ctc_loss=1.174, contrastive_loss=0, total=4146.17, n_correct=1969.51, ppl=5.99, accuracy=47.502, wps=16071.9, ups=1.3, wpb=12395.5, bsz=471.6, num_updates=8000, lr=0.000158114, gnorm=0.712, clip=0, loss_scale=16, train_wall=77, gb_free=16.3, wall=6840
2023-08-07 15:56:56 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 15:57:18 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.827 | trans_loss 6.284 | nll_loss 3.767 | w2v_ctc_loss 1.428 | contrastive_loss 0 | total 4003.4 | n_correct 2042.7 | ppl 13.61 | accuracy 51.024 | uer 20.16 | wer 21.793 | raw_wer 21.793 | bleu 13.57 | wps 2382.7 | wpb 4003.4 | bsz 141.8 | num_updates 8000 | best_bleu 13.57
2023-08-07 15:57:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8000 updates
2023-08-07 15:57:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_6_8000.pt
2023-08-07 15:57:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_6_8000.pt
2023-08-07 15:57:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_6_8000.pt (epoch 6 @ 8000 updates, score 13.57) (writing took 26.205770974978805 seconds)
2023-08-07 15:59:02 | INFO | train_inner | epoch 006:    736 / 1474 loss=4.339, trans_loss=4.217, nll_loss=2.588, w2v_ctc_loss=1.196, contrastive_loss=0, total=4148.65, n_correct=1973.65, ppl=6.01, accuracy=47.573, wps=9841.6, ups=0.8, wpb=12378.4, bsz=453.7, num_updates=8100, lr=0.000157135, gnorm=0.718, clip=0, loss_scale=16, train_wall=77, gb_free=15.4, wall=6965
2023-08-07 16:00:20 | INFO | train_inner | epoch 006:    836 / 1474 loss=4.331, trans_loss=4.217, nll_loss=2.586, w2v_ctc_loss=1.185, contrastive_loss=0, total=4114.34, n_correct=1949.47, ppl=6, accuracy=47.382, wps=15714.8, ups=1.28, wpb=12292.5, bsz=441.6, num_updates=8200, lr=0.000156174, gnorm=0.707, clip=0, loss_scale=16, train_wall=78, gb_free=14.9, wall=7044
2023-08-07 16:01:38 | INFO | train_inner | epoch 006:    936 / 1474 loss=4.325, trans_loss=4.206, nll_loss=2.573, w2v_ctc_loss=1.192, contrastive_loss=0, total=4081.53, n_correct=1946.2, ppl=5.95, accuracy=47.683, wps=15635.7, ups=1.29, wpb=12166.1, bsz=444.5, num_updates=8300, lr=0.00015523, gnorm=0.717, clip=0, loss_scale=16, train_wall=77, gb_free=17.6, wall=7121
2023-08-07 16:02:56 | INFO | train_inner | epoch 006:   1036 / 1474 loss=4.298, trans_loss=4.185, nll_loss=2.546, w2v_ctc_loss=1.173, contrastive_loss=0, total=4165.84, n_correct=2008.32, ppl=5.84, accuracy=48.209, wps=16041.9, ups=1.29, wpb=12437.3, bsz=477.2, num_updates=8400, lr=0.000154303, gnorm=0.724, clip=0, loss_scale=16, train_wall=77, gb_free=16.7, wall=7199
2023-08-07 16:04:13 | INFO | train_inner | epoch 006:   1136 / 1474 loss=4.314, trans_loss=4.196, nll_loss=2.56, w2v_ctc_loss=1.188, contrastive_loss=0, total=4072.29, n_correct=1951.45, ppl=5.9, accuracy=47.92, wps=15700.9, ups=1.29, wpb=12165.1, bsz=428, num_updates=8500, lr=0.000153393, gnorm=0.717, clip=0, loss_scale=16, train_wall=77, gb_free=16.8, wall=7276
2023-08-07 16:05:31 | INFO | train_inner | epoch 006:   1236 / 1474 loss=4.284, trans_loss=4.172, nll_loss=2.531, w2v_ctc_loss=1.17, contrastive_loss=0, total=4141.55, n_correct=2004.56, ppl=5.78, accuracy=48.401, wps=15909.8, ups=1.28, wpb=12385.8, bsz=474.8, num_updates=8600, lr=0.000152499, gnorm=0.713, clip=0, loss_scale=16, train_wall=77, gb_free=13.2, wall=7354
2023-08-07 16:06:49 | INFO | train_inner | epoch 006:   1336 / 1474 loss=4.287, trans_loss=4.177, nll_loss=2.532, w2v_ctc_loss=1.167, contrastive_loss=0, total=4125.31, n_correct=2002.45, ppl=5.78, accuracy=48.541, wps=15908.8, ups=1.29, wpb=12316, bsz=452.6, num_updates=8700, lr=0.00015162, gnorm=0.718, clip=0, loss_scale=16, train_wall=77, gb_free=17.5, wall=7432
2023-08-07 16:08:06 | INFO | train_inner | epoch 006:   1436 / 1474 loss=4.269, trans_loss=4.156, nll_loss=2.51, w2v_ctc_loss=1.171, contrastive_loss=0, total=4196.2, n_correct=2055.06, ppl=5.7, accuracy=48.974, wps=16104.7, ups=1.29, wpb=12515.5, bsz=461.5, num_updates=8800, lr=0.000150756, gnorm=0.696, clip=0, loss_scale=16, train_wall=77, gb_free=11.4, wall=7509
2023-08-07 16:08:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 16:08:57 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.735 | trans_loss 6.177 | nll_loss 3.64 | w2v_ctc_loss 1.372 | contrastive_loss 0 | total 4003.4 | n_correct 2110.9 | ppl 12.47 | accuracy 52.728 | uer 19.345 | wer 21.118 | raw_wer 21.118 | bleu 14.69 | wps 2437.7 | wpb 4003.4 | bsz 141.8 | num_updates 8838 | best_bleu 14.69
2023-08-07 16:08:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8838 updates
2023-08-07 16:08:57 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 16:09:12 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 16:09:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 6 @ 8838 updates, score 14.69) (writing took 31.53505226969719 seconds)
2023-08-07 16:09:28 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-08-07 16:09:28 | INFO | train | epoch 006 | loss 4.324 | trans_loss 4.21 | nll_loss 2.578 | w2v_ctc_loss 1.183 | contrastive_loss 0 | total 4136.5 | n_correct 1972.06 | ppl 5.97 | accuracy 47.675 | wps 14503.5 | ups 1.17 | wpb 12349.9 | bsz 457.4 | num_updates 8838 | lr 0.000150431 | gnorm 0.714 | clip 0 | loss_scale 16 | train_wall 1138 | gb_free 14.9 | wall 7592
2023-08-07 16:09:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 16:09:29 | INFO | fairseq.trainer | begin training epoch 7
2023-08-07 16:09:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 16:10:25 | INFO | train_inner | epoch 007:     62 / 1474 loss=4.215, trans_loss=4.116, nll_loss=2.457, w2v_ctc_loss=1.136, contrastive_loss=0, total=4108.19, n_correct=2034.9, ppl=5.49, accuracy=49.533, wps=8866.9, ups=0.72, wpb=12269.7, bsz=461.9, num_updates=8900, lr=0.000149906, gnorm=0.708, clip=0, loss_scale=16, train_wall=77, gb_free=16.9, wall=7648
2023-08-07 16:11:42 | INFO | train_inner | epoch 007:    162 / 1474 loss=4.192, trans_loss=4.104, nll_loss=2.442, w2v_ctc_loss=1.123, contrastive_loss=0, total=4106.05, n_correct=2036.13, ppl=5.43, accuracy=49.589, wps=15837.4, ups=1.29, wpb=12254.3, bsz=455.4, num_updates=9000, lr=0.000149071, gnorm=0.713, clip=0, loss_scale=16, train_wall=77, gb_free=16.6, wall=7725
2023-08-07 16:13:00 | INFO | train_inner | epoch 007:    262 / 1474 loss=4.191, trans_loss=4.095, nll_loss=2.429, w2v_ctc_loss=1.127, contrastive_loss=0, total=4129.3, n_correct=2062.49, ppl=5.39, accuracy=49.948, wps=15841.7, ups=1.29, wpb=12322.9, bsz=451.8, num_updates=9100, lr=0.00014825, gnorm=0.706, clip=0, loss_scale=16, train_wall=77, gb_free=17.1, wall=7803
2023-08-07 16:14:18 | INFO | train_inner | epoch 007:    362 / 1474 loss=4.184, trans_loss=4.093, nll_loss=2.428, w2v_ctc_loss=1.12, contrastive_loss=0, total=4201.67, n_correct=2093.97, ppl=5.38, accuracy=49.837, wps=16005.6, ups=1.28, wpb=12533.1, bsz=479.7, num_updates=9200, lr=0.000147442, gnorm=0.706, clip=0, loss_scale=16, train_wall=78, gb_free=15.3, wall=7881
2023-08-07 16:15:36 | INFO | train_inner | epoch 007:    462 / 1474 loss=4.176, trans_loss=4.088, nll_loss=2.424, w2v_ctc_loss=1.118, contrastive_loss=0, total=4155.31, n_correct=2072.07, ppl=5.37, accuracy=49.866, wps=15964.7, ups=1.29, wpb=12394.6, bsz=465.5, num_updates=9300, lr=0.000146647, gnorm=0.709, clip=0, loss_scale=16, train_wall=77, gb_free=16.6, wall=7959
2023-08-07 16:16:53 | INFO | train_inner | epoch 007:    562 / 1474 loss=4.175, trans_loss=4.083, nll_loss=2.416, w2v_ctc_loss=1.122, contrastive_loss=0, total=4165.88, n_correct=2089.62, ppl=5.34, accuracy=50.16, wps=16019, ups=1.29, wpb=12401.8, bsz=459, num_updates=9400, lr=0.000145865, gnorm=0.704, clip=0, loss_scale=16, train_wall=77, gb_free=17.1, wall=8036
2023-08-07 16:18:12 | INFO | train_inner | epoch 007:    662 / 1474 loss=4.176, trans_loss=4.089, nll_loss=2.419, w2v_ctc_loss=1.11, contrastive_loss=0, total=4149.29, n_correct=2088.05, ppl=5.35, accuracy=50.323, wps=15823.6, ups=1.28, wpb=12393.1, bsz=451.6, num_updates=9500, lr=0.000145095, gnorm=0.703, clip=0, loss_scale=16, train_wall=78, gb_free=16.8, wall=8115
2023-08-07 16:19:30 | INFO | train_inner | epoch 007:    762 / 1474 loss=4.173, trans_loss=4.083, nll_loss=2.415, w2v_ctc_loss=1.115, contrastive_loss=0, total=4134.54, n_correct=2077.34, ppl=5.33, accuracy=50.244, wps=15729.5, ups=1.27, wpb=12358.8, bsz=449.8, num_updates=9600, lr=0.000144338, gnorm=0.708, clip=0, loss_scale=16, train_wall=78, gb_free=13.8, wall=8193
2023-08-07 16:20:48 | INFO | train_inner | epoch 007:    862 / 1474 loss=4.168, trans_loss=4.076, nll_loss=2.405, w2v_ctc_loss=1.115, contrastive_loss=0, total=4151.77, n_correct=2085.68, ppl=5.3, accuracy=50.236, wps=15932.5, ups=1.28, wpb=12405.1, bsz=461.9, num_updates=9700, lr=0.000143592, gnorm=0.712, clip=0, loss_scale=16, train_wall=77, gb_free=14.7, wall=8271
2023-08-07 16:22:06 | INFO | train_inner | epoch 007:    962 / 1474 loss=4.152, trans_loss=4.065, nll_loss=2.393, w2v_ctc_loss=1.106, contrastive_loss=0, total=4124.8, n_correct=2085.55, ppl=5.25, accuracy=50.561, wps=15832.9, ups=1.29, wpb=12316.5, bsz=471.2, num_updates=9800, lr=0.000142857, gnorm=0.702, clip=0, loss_scale=32, train_wall=77, gb_free=16.5, wall=8349
2023-08-07 16:23:23 | INFO | train_inner | epoch 007:   1062 / 1474 loss=4.169, trans_loss=4.076, nll_loss=2.405, w2v_ctc_loss=1.116, contrastive_loss=0, total=4113.08, n_correct=2080.59, ppl=5.3, accuracy=50.585, wps=15883.4, ups=1.29, wpb=12291.6, bsz=439.4, num_updates=9900, lr=0.000142134, gnorm=0.708, clip=0, loss_scale=32, train_wall=77, gb_free=14.6, wall=8426
2023-08-07 16:24:42 | INFO | train_inner | epoch 007:   1162 / 1474 loss=4.141, trans_loss=4.049, nll_loss=2.377, w2v_ctc_loss=1.113, contrastive_loss=0, total=4134.15, n_correct=2097.93, ppl=5.2, accuracy=50.746, wps=15661.9, ups=1.27, wpb=12340.8, bsz=470.3, num_updates=10000, lr=0.000141421, gnorm=0.711, clip=0, loss_scale=32, train_wall=78, gb_free=15.9, wall=8505
2023-08-07 16:24:42 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 16:25:04 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.642 | trans_loss 6.048 | nll_loss 3.474 | w2v_ctc_loss 1.36 | contrastive_loss 0 | total 4003.4 | n_correct 2184.9 | ppl 11.11 | accuracy 54.576 | uer 18.454 | wer 20.249 | raw_wer 20.249 | bleu 15.81 | wps 2351.9 | wpb 4003.4 | bsz 141.8 | num_updates 10000 | best_bleu 15.81
2023-08-07 16:25:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10000 updates
2023-08-07 16:25:04 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_7_10000.pt
2023-08-07 16:25:08 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_7_10000.pt
2023-08-07 16:25:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_7_10000.pt (epoch 7 @ 10000 updates, score 15.81) (writing took 52.7419699896127 seconds)
mt_weight tensor(0.6890, device='cuda:0')
asr_weight tensor(0.5528, device='cuda:0')
2023-08-07 16:27:15 | INFO | train_inner | epoch 007:   1262 / 1474 loss=4.131, trans_loss=4.045, nll_loss=2.369, w2v_ctc_loss=1.101, contrastive_loss=0, total=4133.98, n_correct=2108.24, ppl=5.16, accuracy=50.998, wps=8087.2, ups=0.66, wpb=12339.4, bsz=450.9, num_updates=10100, lr=0.00014072, gnorm=0.536, clip=0, loss_scale=32, train_wall=77, gb_free=16.5, wall=8658
2023-08-07 16:28:32 | INFO | train_inner | epoch 007:   1362 / 1474 loss=4.127, trans_loss=4.034, nll_loss=2.355, w2v_ctc_loss=1.108, contrastive_loss=0, total=4171.46, n_correct=2142.53, ppl=5.11, accuracy=51.362, wps=16120.5, ups=1.3, wpb=12447.6, bsz=475.5, num_updates=10200, lr=0.000140028, gnorm=0.531, clip=0, loss_scale=32, train_wall=77, gb_free=17.5, wall=8735
2023-08-07 16:29:51 | INFO | train_inner | epoch 007:   1462 / 1474 loss=4.137, trans_loss=4.052, nll_loss=2.379, w2v_ctc_loss=1.101, contrastive_loss=0, total=4106.94, n_correct=2087.95, ppl=5.2, accuracy=50.84, wps=15525.6, ups=1.26, wpb=12278.7, bsz=443.8, num_updates=10300, lr=0.000139347, gnorm=0.539, clip=0, loss_scale=32, train_wall=79, gb_free=15.5, wall=8814
2023-08-07 16:30:00 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.6890, device='cuda:6')
asr_weight tensor(0.5528, device='cuda:6')
mt_weight tensor(0.6890, device='cuda:3')
asr_weight tensor(0.5528, device='cuda:3')
mt_weight tensor(0.6890, device='cuda:4')
asr_weight tensor(0.5528, device='cuda:4')
mt_weight tensor(0.6890, device='cuda:2')
asr_weight tensor(0.5528, device='cuda:2')
mt_weight tensor(0.6890, device='cuda:5')
asr_weight tensor(0.5528, device='cuda:5')
mt_weight tensor(0.6890, device='cuda:7')
asr_weight tensor(0.5528, device='cuda:7')
mt_weight tensor(0.6890, device='cuda:1')
asr_weight tensor(0.5528, device='cuda:1')
2023-08-07 16:30:22 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.615 | trans_loss 6.026 | nll_loss 3.444 | w2v_ctc_loss 1.322 | contrastive_loss 0 | total 4003.4 | n_correct 2194.6 | ppl 10.88 | accuracy 54.818 | uer 18.392 | wer 20.178 | raw_wer 20.178 | bleu 15.99 | wps 2335 | wpb 4003.4 | bsz 141.8 | num_updates 10312 | best_bleu 15.99
2023-08-07 16:30:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10312 updates
2023-08-07 16:30:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 16:30:38 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 16:30:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 7 @ 10312 updates, score 15.99) (writing took 28.308400258421898 seconds)
2023-08-07 16:30:51 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-08-07 16:30:51 | INFO | train | epoch 007 | loss 4.165 | trans_loss 4.075 | nll_loss 2.405 | w2v_ctc_loss 1.114 | contrastive_loss 0 | total 4138.65 | n_correct 2084.34 | ppl 5.3 | accuracy 50.363 | wps 14201.2 | ups 1.15 | wpb 12355.8 | bsz 458.5 | num_updates 10312 | lr 0.000139266 | gnorm 0.671 | clip 0 | loss_scale 32 | train_wall 1141 | gb_free 13.1 | wall 8874
2023-08-07 16:30:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 16:30:51 | INFO | fairseq.trainer | begin training epoch 8
2023-08-07 16:30:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 16:32:08 | INFO | train_inner | epoch 008:     88 / 1474 loss=4.072, trans_loss=4.004, nll_loss=2.312, w2v_ctc_loss=1.067, contrastive_loss=0, total=4120.74, n_correct=2136.83, ppl=4.96, accuracy=51.855, wps=8947.1, ups=0.73, wpb=12280.5, bsz=444.5, num_updates=10400, lr=0.000138675, gnorm=0.538, clip=0, loss_scale=32, train_wall=78, gb_free=17, wall=8951
2023-08-07 16:33:26 | INFO | train_inner | epoch 008:    188 / 1474 loss=4.069, trans_loss=4.003, nll_loss=2.311, w2v_ctc_loss=1.061, contrastive_loss=0, total=4028.45, n_correct=2092.7, ppl=4.96, accuracy=51.948, wps=15442.1, ups=1.29, wpb=12007.6, bsz=426.7, num_updates=10500, lr=0.000138013, gnorm=0.544, clip=0, loss_scale=32, train_wall=77, gb_free=16.3, wall=9029
2023-08-07 16:34:43 | INFO | train_inner | epoch 008:    288 / 1474 loss=4.056, trans_loss=3.986, nll_loss=2.289, w2v_ctc_loss=1.06, contrastive_loss=0, total=4212.3, n_correct=2206.37, ppl=4.89, accuracy=52.379, wps=16303, ups=1.3, wpb=12585.6, bsz=489.5, num_updates=10600, lr=0.000137361, gnorm=0.531, clip=0, loss_scale=32, train_wall=77, gb_free=16.7, wall=9106
2023-08-07 16:36:01 | INFO | train_inner | epoch 008:    388 / 1474 loss=4.074, trans_loss=3.995, nll_loss=2.301, w2v_ctc_loss=1.082, contrastive_loss=0, total=4124.65, n_correct=2136.78, ppl=4.93, accuracy=51.805, wps=15737.3, ups=1.28, wpb=12311.8, bsz=439.9, num_updates=10700, lr=0.000136717, gnorm=0.541, clip=0, loss_scale=32, train_wall=78, gb_free=16.2, wall=9184
2023-08-07 16:37:20 | INFO | train_inner | epoch 008:    488 / 1474 loss=4.044, trans_loss=3.978, nll_loss=2.28, w2v_ctc_loss=1.054, contrastive_loss=0, total=4205.7, n_correct=2199.79, ppl=4.86, accuracy=52.305, wps=15900.5, ups=1.27, wpb=12568.6, bsz=505.9, num_updates=10800, lr=0.000136083, gnorm=0.534, clip=0, loss_scale=32, train_wall=79, gb_free=16.3, wall=9264
2023-08-07 16:38:39 | INFO | train_inner | epoch 008:    588 / 1474 loss=4.068, trans_loss=3.992, nll_loss=2.3, w2v_ctc_loss=1.073, contrastive_loss=0, total=4063.27, n_correct=2108.06, ppl=4.92, accuracy=51.881, wps=15579.9, ups=1.28, wpb=12176.4, bsz=428.8, num_updates=10900, lr=0.000135457, gnorm=0.542, clip=0, loss_scale=32, train_wall=78, gb_free=16.3, wall=9342
2023-08-07 16:39:56 | INFO | train_inner | epoch 008:    688 / 1474 loss=4.06, trans_loss=3.981, nll_loss=2.286, w2v_ctc_loss=1.074, contrastive_loss=0, total=4140.15, n_correct=2175.46, ppl=4.88, accuracy=52.545, wps=15925.6, ups=1.29, wpb=12340.7, bsz=447.4, num_updates=11000, lr=0.00013484, gnorm=0.537, clip=0, loss_scale=32, train_wall=77, gb_free=16.6, wall=9419
2023-08-07 16:41:13 | INFO | train_inner | epoch 008:    788 / 1474 loss=4.044, trans_loss=3.97, nll_loss=2.273, w2v_ctc_loss=1.062, contrastive_loss=0, total=4121.11, n_correct=2168.85, ppl=4.83, accuracy=52.628, wps=15957.7, ups=1.29, wpb=12332.7, bsz=446.8, num_updates=11100, lr=0.000134231, gnorm=0.54, clip=0, loss_scale=32, train_wall=77, gb_free=17.2, wall=9496
2023-08-07 16:42:31 | INFO | train_inner | epoch 008:    888 / 1474 loss=4.038, trans_loss=3.97, nll_loss=2.274, w2v_ctc_loss=1.055, contrastive_loss=0, total=4158.35, n_correct=2187.6, ppl=4.84, accuracy=52.607, wps=16022.7, ups=1.29, wpb=12422.7, bsz=470.3, num_updates=11200, lr=0.000133631, gnorm=0.535, clip=0, loss_scale=32, train_wall=77, gb_free=16.5, wall=9574
2023-08-07 16:43:48 | INFO | train_inner | epoch 008:    988 / 1474 loss=4.023, trans_loss=3.958, nll_loss=2.257, w2v_ctc_loss=1.05, contrastive_loss=0, total=4170.95, n_correct=2203.93, ppl=4.78, accuracy=52.84, wps=16039.1, ups=1.29, wpb=12447.1, bsz=470.7, num_updates=11300, lr=0.000133038, gnorm=0.535, clip=0, loss_scale=32, train_wall=77, gb_free=15.7, wall=9652
2023-08-07 16:45:07 | INFO | train_inner | epoch 008:   1088 / 1474 loss=4.031, trans_loss=3.963, nll_loss=2.264, w2v_ctc_loss=1.055, contrastive_loss=0, total=4194.19, n_correct=2208.39, ppl=4.8, accuracy=52.654, wps=15929.6, ups=1.27, wpb=12504.4, bsz=464.3, num_updates=11400, lr=0.000132453, gnorm=0.538, clip=0, loss_scale=32, train_wall=78, gb_free=17, wall=9730
2023-08-07 16:46:24 | INFO | train_inner | epoch 008:   1188 / 1474 loss=4.027, trans_loss=3.958, nll_loss=2.258, w2v_ctc_loss=1.055, contrastive_loss=0, total=4165.68, n_correct=2205.01, ppl=4.78, accuracy=52.933, wps=16093.1, ups=1.29, wpb=12444.3, bsz=467.9, num_updates=11500, lr=0.000131876, gnorm=0.532, clip=0, loss_scale=32, train_wall=77, gb_free=17.3, wall=9807
2023-08-07 16:47:42 | INFO | train_inner | epoch 008:   1288 / 1474 loss=4.035, trans_loss=3.959, nll_loss=2.261, w2v_ctc_loss=1.064, contrastive_loss=0, total=4079.12, n_correct=2149.84, ppl=4.79, accuracy=52.704, wps=15748.7, ups=1.29, wpb=12185.3, bsz=444, num_updates=11600, lr=0.000131306, gnorm=0.542, clip=0, loss_scale=32, train_wall=77, gb_free=15.5, wall=9885
2023-08-07 16:49:00 | INFO | train_inner | epoch 008:   1388 / 1474 loss=4.028, trans_loss=3.957, nll_loss=2.26, w2v_ctc_loss=1.059, contrastive_loss=0, total=4136.76, n_correct=2187.2, ppl=4.79, accuracy=52.872, wps=15800.6, ups=1.28, wpb=12338, bsz=459.3, num_updates=11700, lr=0.000130744, gnorm=0.533, clip=0, loss_scale=32, train_wall=78, gb_free=15.8, wall=9963
2023-08-07 16:50:06 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 16:50:28 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 4.572 | trans_loss 5.935 | nll_loss 3.332 | w2v_ctc_loss 1.389 | contrastive_loss 0 | total 4003.4 | n_correct 2247.9 | ppl 10.07 | accuracy 56.15 | uer 18.069 | wer 19.839 | raw_wer 19.839 | bleu 16.54 | wps 2396.2 | wpb 4003.4 | bsz 141.8 | num_updates 11786 | best_bleu 16.54
2023-08-07 16:50:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11786 updates
2023-08-07 16:50:28 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 16:50:43 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 16:50:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 8 @ 11786 updates, score 16.54) (writing took 26.96996302716434 seconds)
2023-08-07 16:50:56 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-08-07 16:50:56 | INFO | train | epoch 008 | loss 4.046 | trans_loss 3.975 | nll_loss 2.278 | w2v_ctc_loss 1.061 | contrastive_loss 0 | total 4138.65 | n_correct 2171.89 | ppl 4.85 | accuracy 52.478 | wps 15115.8 | ups 1.22 | wpb 12355.8 | bsz 458.5 | num_updates 11786 | lr 0.000130266 | gnorm 0.537 | clip 0 | loss_scale 32 | train_wall 1141 | gb_free 16.7 | wall 10079
2023-08-07 16:50:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 16:50:56 | INFO | fairseq.trainer | begin training epoch 9
2023-08-07 16:50:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 16:51:15 | INFO | train_inner | epoch 009:     14 / 1474 loss=4.008, trans_loss=3.944, nll_loss=2.241, w2v_ctc_loss=1.042, contrastive_loss=0, total=4126.64, n_correct=2196.73, ppl=4.73, accuracy=53.233, wps=9080.2, ups=0.74, wpb=12295.4, bsz=468.3, num_updates=11800, lr=0.000130189, gnorm=0.54, clip=0, loss_scale=64, train_wall=77, gb_free=17.2, wall=10098
2023-08-07 16:52:33 | INFO | train_inner | epoch 009:    114 / 1474 loss=3.948, trans_loss=3.899, nll_loss=2.181, w2v_ctc_loss=1.009, contrastive_loss=0, total=4190.32, n_correct=2268.62, ppl=4.54, accuracy=54.14, wps=16159.3, ups=1.29, wpb=12520.6, bsz=478.7, num_updates=11900, lr=0.000129641, gnorm=0.529, clip=0, loss_scale=64, train_wall=77, gb_free=16.3, wall=10176
2023-08-07 16:53:50 | INFO | train_inner | epoch 009:    214 / 1474 loss=3.958, trans_loss=3.911, nll_loss=2.197, w2v_ctc_loss=1.011, contrastive_loss=0, total=4071.85, n_correct=2194.74, ppl=4.58, accuracy=53.9, wps=15682.4, ups=1.29, wpb=12162.7, bsz=435.4, num_updates=12000, lr=0.000129099, gnorm=0.536, clip=0, loss_scale=64, train_wall=77, gb_free=16.4, wall=10253
2023-08-07 16:53:50 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 16:54:12 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.539 | trans_loss 5.923 | nll_loss 3.308 | w2v_ctc_loss 1.309 | contrastive_loss 0 | total 4003.4 | n_correct 2259.7 | ppl 9.9 | accuracy 56.445 | uer 17.896 | wer 19.656 | raw_wer 19.656 | bleu 16.51 | wps 2392.6 | wpb 4003.4 | bsz 141.8 | num_updates 12000 | best_bleu 16.54
2023-08-07 16:54:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12000 updates
2023-08-07 16:54:12 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_9_12000.pt
2023-08-07 16:54:16 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_9_12000.pt
2023-08-07 16:54:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_9_12000.pt (epoch 9 @ 12000 updates, score 16.51) (writing took 37.982983343303204 seconds)
2023-08-07 16:56:09 | INFO | train_inner | epoch 009:    314 / 1474 loss=3.938, trans_loss=3.895, nll_loss=2.177, w2v_ctc_loss=0.997, contrastive_loss=0, total=4145.76, n_correct=2253.97, ppl=4.52, accuracy=54.368, wps=8963.5, ups=0.72, wpb=12408.2, bsz=475.4, num_updates=12100, lr=0.000128565, gnorm=0.535, clip=0, loss_scale=64, train_wall=78, gb_free=16.4, wall=10392
2023-08-07 16:57:28 | INFO | train_inner | epoch 009:    414 / 1474 loss=3.949, trans_loss=3.901, nll_loss=2.185, w2v_ctc_loss=1.01, contrastive_loss=0, total=4204.03, n_correct=2269.84, ppl=4.55, accuracy=53.992, wps=15781.4, ups=1.26, wpb=12540.5, bsz=469.6, num_updates=12200, lr=0.000128037, gnorm=0.53, clip=0, loss_scale=64, train_wall=79, gb_free=16, wall=10471
2023-08-07 16:58:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-07 16:58:46 | INFO | train_inner | epoch 009:    515 / 1474 loss=3.972, trans_loss=3.912, nll_loss=2.199, w2v_ctc_loss=1.032, contrastive_loss=0, total=4105.67, n_correct=2212.77, ppl=4.59, accuracy=53.895, wps=15708.1, ups=1.28, wpb=12241.4, bsz=433.7, num_updates=12300, lr=0.000127515, gnorm=0.542, clip=0, loss_scale=32, train_wall=77, gb_free=16, wall=10549
2023-08-07 17:00:04 | INFO | train_inner | epoch 009:    615 / 1474 loss=3.946, trans_loss=3.897, nll_loss=2.181, w2v_ctc_loss=1.006, contrastive_loss=0, total=4131.32, n_correct=2236.48, ppl=4.54, accuracy=54.135, wps=15915.5, ups=1.29, wpb=12355.3, bsz=455, num_updates=12400, lr=0.000127, gnorm=0.535, clip=0, loss_scale=32, train_wall=77, gb_free=17.6, wall=10627
2023-08-07 17:01:21 | INFO | train_inner | epoch 009:    715 / 1474 loss=3.966, trans_loss=3.903, nll_loss=2.188, w2v_ctc_loss=1.03, contrastive_loss=0, total=4082.11, n_correct=2197.08, ppl=4.56, accuracy=53.822, wps=15732.4, ups=1.29, wpb=12206.9, bsz=449.7, num_updates=12500, lr=0.000126491, gnorm=0.548, clip=0, loss_scale=32, train_wall=77, gb_free=16.8, wall=10704
2023-08-07 17:02:41 | INFO | train_inner | epoch 009:    815 / 1474 loss=3.938, trans_loss=3.88, nll_loss=2.162, w2v_ctc_loss=1.022, contrastive_loss=0, total=4221.08, n_correct=2293.78, ppl=4.47, accuracy=54.341, wps=15862.4, ups=1.26, wpb=12600.2, bsz=501.6, num_updates=12600, lr=0.000125988, gnorm=0.53, clip=0, loss_scale=32, train_wall=79, gb_free=17.4, wall=10784
2023-08-07 17:03:59 | INFO | train_inner | epoch 009:    915 / 1474 loss=3.954, trans_loss=3.899, nll_loss=2.182, w2v_ctc_loss=1.019, contrastive_loss=0, total=4142.34, n_correct=2244.51, ppl=4.54, accuracy=54.185, wps=15695.4, ups=1.27, wpb=12345.3, bsz=448.9, num_updates=12700, lr=0.000125491, gnorm=0.537, clip=0, loss_scale=32, train_wall=78, gb_free=17.1, wall=10863
2023-08-07 17:05:17 | INFO | train_inner | epoch 009:   1015 / 1474 loss=3.969, trans_loss=3.913, nll_loss=2.199, w2v_ctc_loss=1.025, contrastive_loss=0, total=4097.15, n_correct=2200.49, ppl=4.59, accuracy=53.708, wps=15754.6, ups=1.29, wpb=12230.8, bsz=422.6, num_updates=12800, lr=0.000125, gnorm=0.542, clip=0, loss_scale=32, train_wall=77, gb_free=16.6, wall=10940
2023-08-07 17:06:35 | INFO | train_inner | epoch 009:   1115 / 1474 loss=3.942, trans_loss=3.89, nll_loss=2.169, w2v_ctc_loss=1.016, contrastive_loss=0, total=4182.29, n_correct=2278.2, ppl=4.5, accuracy=54.473, wps=16053.3, ups=1.29, wpb=12446.8, bsz=477.6, num_updates=12900, lr=0.000124515, gnorm=0.538, clip=0, loss_scale=32, train_wall=77, gb_free=17.1, wall=11018
2023-08-07 17:07:54 | INFO | train_inner | epoch 009:   1215 / 1474 loss=3.966, trans_loss=3.903, nll_loss=2.183, w2v_ctc_loss=1.026, contrastive_loss=0, total=4141.43, n_correct=2243.74, ppl=4.54, accuracy=54.178, wps=15642, ups=1.26, wpb=12406.5, bsz=446.6, num_updates=13000, lr=0.000124035, gnorm=0.536, clip=0, loss_scale=32, train_wall=79, gb_free=17.3, wall=11097
2023-08-07 17:09:12 | INFO | train_inner | epoch 009:   1315 / 1474 loss=3.934, trans_loss=3.88, nll_loss=2.158, w2v_ctc_loss=1.012, contrastive_loss=0, total=4203.91, n_correct=2298.18, ppl=4.46, accuracy=54.668, wps=16006.6, ups=1.28, wpb=12540.7, bsz=492.3, num_updates=13100, lr=0.00012356, gnorm=0.533, clip=0, loss_scale=32, train_wall=78, gb_free=17.1, wall=11175
2023-08-07 17:10:30 | INFO | train_inner | epoch 009:   1415 / 1474 loss=3.958, trans_loss=3.898, nll_loss=2.18, w2v_ctc_loss=1.025, contrastive_loss=0, total=4077.08, n_correct=2212.3, ppl=4.53, accuracy=54.262, wps=15747.9, ups=1.29, wpb=12171.4, bsz=429.1, num_updates=13200, lr=0.000123091, gnorm=0.543, clip=0, loss_scale=32, train_wall=77, gb_free=17.1, wall=11253
2023-08-07 17:11:14 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 17:11:36 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.506 | trans_loss 5.867 | nll_loss 3.239 | w2v_ctc_loss 1.331 | contrastive_loss 0 | total 4003.4 | n_correct 2292.3 | ppl 9.44 | accuracy 57.259 | uer 17.477 | wer 19.377 | raw_wer 19.377 | bleu 17.48 | wps 2481.1 | wpb 4003.4 | bsz 141.8 | num_updates 13259 | best_bleu 17.48
2023-08-07 17:11:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 13259 updates
2023-08-07 17:11:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 17:11:50 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 17:12:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 9 @ 13259 updates, score 17.48) (writing took 26.308040587231517 seconds)
2023-08-07 17:12:03 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-08-07 17:12:03 | INFO | train | epoch 009 | loss 3.952 | trans_loss 3.898 | nll_loss 2.18 | w2v_ctc_loss 1.017 | contrastive_loss 0 | total 4138.16 | n_correct 2241.27 | ppl 4.53 | accuracy 54.161 | wps 14364.4 | ups 1.16 | wpb 12354.4 | bsz 458.2 | num_updates 13259 | lr 0.000122817 | gnorm 0.537 | clip 0 | loss_scale 32 | train_wall 1143 | gb_free 11.7 | wall 11346
2023-08-07 17:12:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 17:12:03 | INFO | fairseq.trainer | begin training epoch 10
2023-08-07 17:12:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 17:12:43 | INFO | train_inner | epoch 010:     41 / 1474 loss=3.911, trans_loss=3.862, nll_loss=2.134, w2v_ctc_loss=1, contrastive_loss=0, total=4100.86, n_correct=2252.12, ppl=4.39, accuracy=54.918, wps=9180.6, ups=0.75, wpb=12244.8, bsz=470.2, num_updates=13300, lr=0.000122628, gnorm=0.536, clip=0, loss_scale=32, train_wall=77, gb_free=16.3, wall=11386
2023-08-07 17:14:01 | INFO | train_inner | epoch 010:    141 / 1474 loss=3.867, trans_loss=3.836, nll_loss=2.099, w2v_ctc_loss=0.961, contrastive_loss=0, total=4240.18, n_correct=2360.19, ppl=4.28, accuracy=55.662, wps=16271, ups=1.28, wpb=12698.4, bsz=479.1, num_updates=13400, lr=0.000122169, gnorm=0.525, clip=0, loss_scale=32, train_wall=78, gb_free=14.8, wall=11464
2023-08-07 17:15:19 | INFO | train_inner | epoch 010:    241 / 1474 loss=3.872, trans_loss=3.832, nll_loss=2.097, w2v_ctc_loss=0.981, contrastive_loss=0, total=4126.3, n_correct=2297.48, ppl=4.28, accuracy=55.679, wps=15687.2, ups=1.28, wpb=12290.2, bsz=460.9, num_updates=13500, lr=0.000121716, gnorm=0.533, clip=0, loss_scale=32, train_wall=78, gb_free=15.4, wall=11542
2023-08-07 17:16:38 | INFO | train_inner | epoch 010:    341 / 1474 loss=3.87, trans_loss=3.834, nll_loss=2.101, w2v_ctc_loss=0.971, contrastive_loss=0, total=4132.25, n_correct=2297.66, ppl=4.29, accuracy=55.603, wps=15788.9, ups=1.28, wpb=12358.2, bsz=452.8, num_updates=13600, lr=0.000121268, gnorm=0.536, clip=0, loss_scale=32, train_wall=78, gb_free=14.6, wall=11621
2023-08-07 17:17:56 | INFO | train_inner | epoch 010:    441 / 1474 loss=3.862, trans_loss=3.833, nll_loss=2.098, w2v_ctc_loss=0.962, contrastive_loss=0, total=4203.14, n_correct=2336.11, ppl=4.28, accuracy=55.58, wps=15968.3, ups=1.27, wpb=12542, bsz=481.7, num_updates=13700, lr=0.000120824, gnorm=0.531, clip=0, loss_scale=32, train_wall=78, gb_free=16.2, wall=11699
2023-08-07 17:19:14 | INFO | train_inner | epoch 010:    541 / 1474 loss=3.899, trans_loss=3.849, nll_loss=2.116, w2v_ctc_loss=0.999, contrastive_loss=0, total=4106.5, n_correct=2270.96, ppl=4.34, accuracy=55.302, wps=15766.2, ups=1.29, wpb=12236.6, bsz=440, num_updates=13800, lr=0.000120386, gnorm=0.543, clip=0, loss_scale=32, train_wall=77, gb_free=16.2, wall=11777
2023-08-07 17:20:32 | INFO | train_inner | epoch 010:    641 / 1474 loss=3.886, trans_loss=3.841, nll_loss=2.107, w2v_ctc_loss=0.99, contrastive_loss=0, total=4170.61, n_correct=2312.62, ppl=4.31, accuracy=55.45, wps=15857.7, ups=1.27, wpb=12452.8, bsz=476.1, num_updates=13900, lr=0.000119952, gnorm=0.539, clip=0, loss_scale=32, train_wall=78, gb_free=10.9, wall=11855
2023-08-07 17:21:50 | INFO | train_inner | epoch 010:    741 / 1474 loss=3.894, trans_loss=3.841, nll_loss=2.108, w2v_ctc_loss=1.001, contrastive_loss=0, total=4123.31, n_correct=2285.1, ppl=4.31, accuracy=55.419, wps=15867.6, ups=1.29, wpb=12306.3, bsz=453, num_updates=14000, lr=0.000119523, gnorm=0.543, clip=0, loss_scale=32, train_wall=77, gb_free=16.8, wall=11933
2023-08-07 17:21:50 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 17:22:11 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.498 | trans_loss 5.832 | nll_loss 3.204 | w2v_ctc_loss 1.384 | contrastive_loss 0 | total 4003.4 | n_correct 2313.5 | ppl 9.22 | accuracy 57.788 | uer 17.628 | wer 19.477 | raw_wer 19.477 | bleu 17.67 | wps 2506.1 | wpb 4003.4 | bsz 141.8 | num_updates 14000 | best_bleu 17.67
2023-08-07 17:22:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14000 updates
2023-08-07 17:22:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_10_14000.pt
2023-08-07 17:22:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_10_14000.pt
2023-08-07 17:22:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_10_14000.pt (epoch 10 @ 14000 updates, score 17.67) (writing took 46.327137334272265 seconds)
2023-08-07 17:24:16 | INFO | train_inner | epoch 010:    841 / 1474 loss=3.875, trans_loss=3.838, nll_loss=2.104, w2v_ctc_loss=0.972, contrastive_loss=0, total=4125.69, n_correct=2290.87, ppl=4.3, accuracy=55.527, wps=8456, ups=0.69, wpb=12339.2, bsz=456.1, num_updates=14100, lr=0.000119098, gnorm=0.535, clip=0, loss_scale=32, train_wall=77, gb_free=15.6, wall=12079
2023-08-07 17:25:33 | INFO | train_inner | epoch 010:    941 / 1474 loss=3.871, trans_loss=3.827, nll_loss=2.091, w2v_ctc_loss=0.987, contrastive_loss=0, total=4170.41, n_correct=2327.7, ppl=4.26, accuracy=55.815, wps=16088, ups=1.3, wpb=12406.2, bsz=470.8, num_updates=14200, lr=0.000118678, gnorm=0.535, clip=0, loss_scale=32, train_wall=77, gb_free=15.9, wall=12156
2023-08-07 17:26:51 | INFO | train_inner | epoch 010:   1041 / 1474 loss=3.882, trans_loss=3.839, nll_loss=2.107, w2v_ctc_loss=0.988, contrastive_loss=0, total=4072.57, n_correct=2253.96, ppl=4.31, accuracy=55.345, wps=15518.4, ups=1.28, wpb=12159.3, bsz=434.8, num_updates=14300, lr=0.000118262, gnorm=0.545, clip=0, loss_scale=32, train_wall=78, gb_free=16.7, wall=12234
2023-08-07 17:27:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-07 17:28:10 | INFO | train_inner | epoch 010:   1142 / 1474 loss=3.894, trans_loss=3.842, nll_loss=2.111, w2v_ctc_loss=1, contrastive_loss=0, total=4029.22, n_correct=2223.63, ppl=4.32, accuracy=55.188, wps=15341.7, ups=1.28, wpb=12022.6, bsz=416.3, num_updates=14400, lr=0.000117851, gnorm=0.545, clip=0, loss_scale=32, train_wall=78, gb_free=17.1, wall=12313
2023-08-07 17:29:27 | INFO | train_inner | epoch 010:   1242 / 1474 loss=3.88, trans_loss=3.831, nll_loss=2.099, w2v_ctc_loss=0.992, contrastive_loss=0, total=4110.41, n_correct=2281.31, ppl=4.28, accuracy=55.501, wps=15914.7, ups=1.29, wpb=12302.3, bsz=445.2, num_updates=14500, lr=0.000117444, gnorm=0.542, clip=0, loss_scale=32, train_wall=77, gb_free=16.3, wall=12390
2023-08-07 17:30:45 | INFO | train_inner | epoch 010:   1342 / 1474 loss=3.877, trans_loss=3.829, nll_loss=2.094, w2v_ctc_loss=0.989, contrastive_loss=0, total=4121.38, n_correct=2293.06, ppl=4.27, accuracy=55.638, wps=15844.7, ups=1.29, wpb=12315.5, bsz=451, num_updates=14600, lr=0.000117041, gnorm=0.543, clip=0, loss_scale=32, train_wall=77, gb_free=14, wall=12468
2023-08-07 17:32:03 | INFO | train_inner | epoch 010:   1442 / 1474 loss=3.868, trans_loss=3.828, nll_loss=2.094, w2v_ctc_loss=0.98, contrastive_loss=0, total=4192.39, n_correct=2330.97, ppl=4.27, accuracy=55.6, wps=15870.1, ups=1.27, wpb=12477.9, bsz=482, num_updates=14700, lr=0.000116642, gnorm=0.544, clip=0, loss_scale=32, train_wall=78, gb_free=16.8, wall=12546
2023-08-07 17:32:28 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 17:32:50 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.467 | trans_loss 5.817 | nll_loss 3.176 | w2v_ctc_loss 1.318 | contrastive_loss 0 | total 4003.4 | n_correct 2322.5 | ppl 9.04 | accuracy 58.013 | uer 17.357 | wer 19.052 | raw_wer 19.052 | bleu 17.87 | wps 2414.8 | wpb 4003.4 | bsz 141.8 | num_updates 14732 | best_bleu 17.87
2023-08-07 17:32:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14732 updates
2023-08-07 17:32:50 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 17:33:04 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 17:33:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 10 @ 14732 updates, score 17.87) (writing took 26.636858956888318 seconds)
2023-08-07 17:33:17 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-08-07 17:33:17 | INFO | train | epoch 010 | loss 3.877 | trans_loss 3.836 | nll_loss 2.102 | w2v_ctc_loss 0.982 | contrastive_loss 0 | total 4138.26 | n_correct 2297.82 | ppl 4.29 | accuracy 55.526 | wps 14281.2 | ups 1.16 | wpb 12354.4 | bsz 458.2 | num_updates 14732 | lr 0.000116516 | gnorm 0.538 | clip 0 | loss_scale 32 | train_wall 1142 | gb_free 17.1 | wall 12620
2023-08-07 17:33:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 17:33:17 | INFO | fairseq.trainer | begin training epoch 11
2023-08-07 17:33:17 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 17:34:18 | INFO | train_inner | epoch 011:     68 / 1474 loss=3.822, trans_loss=3.791, nll_loss=2.046, w2v_ctc_loss=0.954, contrastive_loss=0, total=4175.24, n_correct=2363.3, ppl=4.13, accuracy=56.603, wps=9240, ups=0.74, wpb=12460.8, bsz=478.8, num_updates=14800, lr=0.000116248, gnorm=0.531, clip=0, loss_scale=32, train_wall=77, gb_free=16.6, wall=12681
2023-08-07 17:35:35 | INFO | train_inner | epoch 011:    168 / 1474 loss=3.816, trans_loss=3.786, nll_loss=2.038, w2v_ctc_loss=0.952, contrastive_loss=0, total=4087.78, n_correct=2315.06, ppl=4.11, accuracy=56.634, wps=15833.7, ups=1.3, wpb=12225.8, bsz=445.9, num_updates=14900, lr=0.000115857, gnorm=0.541, clip=0, loss_scale=32, train_wall=77, gb_free=16.3, wall=12758
2023-08-07 17:36:53 | INFO | train_inner | epoch 011:    268 / 1474 loss=3.811, trans_loss=3.785, nll_loss=2.039, w2v_ctc_loss=0.947, contrastive_loss=0, total=4118.77, n_correct=2333.9, ppl=4.11, accuracy=56.665, wps=15888.1, ups=1.29, wpb=12285.5, bsz=446.5, num_updates=15000, lr=0.00011547, gnorm=0.54, clip=0, loss_scale=32, train_wall=77, gb_free=12.4, wall=12836
mt_weight tensor(0.4081, device='cuda:0')
asr_weight tensor(0.2523, device='cuda:0')
2023-08-07 17:38:10 | INFO | train_inner | epoch 011:    368 / 1474 loss=3.805, trans_loss=3.782, nll_loss=2.032, w2v_ctc_loss=0.941, contrastive_loss=0, total=4097.83, n_correct=2324.41, ppl=4.09, accuracy=56.723, wps=15838.2, ups=1.3, wpb=12202.1, bsz=444.2, num_updates=15100, lr=0.000115087, gnorm=0.41, clip=0, loss_scale=32, train_wall=77, gb_free=15.9, wall=12913
2023-08-07 17:39:28 | INFO | train_inner | epoch 011:    468 / 1474 loss=3.821, trans_loss=3.8, nll_loss=2.052, w2v_ctc_loss=0.945, contrastive_loss=0, total=4110.64, n_correct=2311.87, ppl=4.15, accuracy=56.241, wps=15631.9, ups=1.27, wpb=12265.6, bsz=450.5, num_updates=15200, lr=0.000114708, gnorm=0.412, clip=0, loss_scale=32, train_wall=78, gb_free=16.2, wall=12991
2023-08-07 17:40:47 | INFO | train_inner | epoch 011:    568 / 1474 loss=3.829, trans_loss=3.795, nll_loss=2.051, w2v_ctc_loss=0.958, contrastive_loss=0, total=4071.69, n_correct=2294.36, ppl=4.14, accuracy=56.349, wps=15498.7, ups=1.27, wpb=12176.5, bsz=440.6, num_updates=15300, lr=0.000114332, gnorm=0.419, clip=0, loss_scale=32, train_wall=78, gb_free=16.1, wall=13070
2023-08-07 17:42:04 | INFO | train_inner | epoch 011:    668 / 1474 loss=3.811, trans_loss=3.785, nll_loss=2.036, w2v_ctc_loss=0.946, contrastive_loss=0, total=4157.2, n_correct=2353.93, ppl=4.1, accuracy=56.623, wps=15963, ups=1.29, wpb=12400.2, bsz=464.4, num_updates=15400, lr=0.000113961, gnorm=0.406, clip=0, loss_scale=32, train_wall=77, gb_free=16.6, wall=13148
2023-08-07 17:43:23 | INFO | train_inner | epoch 011:    768 / 1474 loss=3.826, trans_loss=3.79, nll_loss=2.042, w2v_ctc_loss=0.96, contrastive_loss=0, total=4174.91, n_correct=2369.02, ppl=4.12, accuracy=56.744, wps=15965, ups=1.28, wpb=12472.8, bsz=460.4, num_updates=15500, lr=0.000113592, gnorm=0.409, clip=0, loss_scale=32, train_wall=78, gb_free=16.8, wall=13226
2023-08-07 17:44:40 | INFO | train_inner | epoch 011:    868 / 1474 loss=3.819, trans_loss=3.785, nll_loss=2.039, w2v_ctc_loss=0.958, contrastive_loss=0, total=4118.44, n_correct=2324.83, ppl=4.11, accuracy=56.449, wps=15831, ups=1.29, wpb=12290.5, bsz=440.6, num_updates=15600, lr=0.000113228, gnorm=0.41, clip=0, loss_scale=32, train_wall=77, gb_free=10.9, wall=13303
2023-08-07 17:45:58 | INFO | train_inner | epoch 011:    968 / 1474 loss=3.82, trans_loss=3.787, nll_loss=2.039, w2v_ctc_loss=0.957, contrastive_loss=0, total=4140.92, n_correct=2342.54, ppl=4.11, accuracy=56.571, wps=15867.2, ups=1.28, wpb=12353.7, bsz=452.9, num_updates=15700, lr=0.000112867, gnorm=0.41, clip=0, loss_scale=32, train_wall=77, gb_free=15.5, wall=13381
2023-08-07 17:47:15 | INFO | train_inner | epoch 011:   1068 / 1474 loss=3.822, trans_loss=3.783, nll_loss=2.036, w2v_ctc_loss=0.963, contrastive_loss=0, total=4136.99, n_correct=2345.39, ppl=4.1, accuracy=56.693, wps=16001.9, ups=1.29, wpb=12367, bsz=463.2, num_updates=15800, lr=0.000112509, gnorm=0.416, clip=0, loss_scale=32, train_wall=77, gb_free=17.4, wall=13458
2023-08-07 17:48:33 | INFO | train_inner | epoch 011:   1168 / 1474 loss=3.815, trans_loss=3.782, nll_loss=2.038, w2v_ctc_loss=0.957, contrastive_loss=0, total=4185.65, n_correct=2368.93, ppl=4.11, accuracy=56.596, wps=15985, ups=1.28, wpb=12477.8, bsz=464.7, num_updates=15900, lr=0.000112154, gnorm=0.409, clip=0, loss_scale=32, train_wall=78, gb_free=13.9, wall=13537
2023-08-07 17:49:52 | INFO | train_inner | epoch 011:   1268 / 1474 loss=3.818, trans_loss=3.782, nll_loss=2.034, w2v_ctc_loss=0.96, contrastive_loss=0, total=4171.89, n_correct=2364.76, ppl=4.09, accuracy=56.683, wps=15905.3, ups=1.28, wpb=12466.6, bsz=471.1, num_updates=16000, lr=0.000111803, gnorm=0.407, clip=0, loss_scale=32, train_wall=78, gb_free=15.7, wall=13615
2023-08-07 17:49:52 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.4081, device='cuda:5')
asr_weight tensor(0.2523, device='cuda:5')
mt_weight tensor(0.4081, device='cuda:6')
asr_weight tensor(0.2523, device='cuda:6')
mt_weight tensor(0.4081, device='cuda:3')
asr_weight tensor(0.2523, device='cuda:3')
mt_weight tensor(0.4081, device='cuda:4')
asr_weight tensor(0.2523, device='cuda:4')
mt_weight tensor(0.4081, device='cuda:1')
asr_weight tensor(0.2523, device='cuda:1')
mt_weight tensor(0.4081, device='cuda:7')
asr_weight tensor(0.2523, device='cuda:7')
mt_weight tensor(0.4081, device='cuda:2')
asr_weight tensor(0.2523, device='cuda:2')
2023-08-07 17:50:14 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.476 | trans_loss 5.783 | nll_loss 3.128 | w2v_ctc_loss 1.428 | contrastive_loss 0 | total 4003.4 | n_correct 2338.5 | ppl 8.74 | accuracy 58.413 | uer 17.386 | wer 19.112 | raw_wer 19.112 | bleu 18.05 | wps 2362.3 | wpb 4003.4 | bsz 141.8 | num_updates 16000 | best_bleu 18.05
2023-08-07 17:50:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16000 updates
2023-08-07 17:50:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_11_16000.pt
2023-08-07 17:50:17 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_11_16000.pt
2023-08-07 17:51:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_11_16000.pt (epoch 11 @ 16000 updates, score 18.05) (writing took 52.053308652713895 seconds)
2023-08-07 17:52:28 | INFO | train_inner | epoch 011:   1368 / 1474 loss=3.808, trans_loss=3.778, nll_loss=2.028, w2v_ctc_loss=0.949, contrastive_loss=0, total=4190.34, n_correct=2378.32, ppl=4.08, accuracy=56.757, wps=8029.5, ups=0.64, wpb=12512.8, bsz=491.8, num_updates=16100, lr=0.000111456, gnorm=0.407, clip=0, loss_scale=32, train_wall=78, gb_free=16.8, wall=13771
2023-08-07 17:53:45 | INFO | train_inner | epoch 011:   1468 / 1474 loss=3.815, trans_loss=3.78, nll_loss=2.031, w2v_ctc_loss=0.959, contrastive_loss=0, total=4158.39, n_correct=2357.06, ppl=4.09, accuracy=56.682, wps=16007.3, ups=1.29, wpb=12421.8, bsz=468, num_updates=16200, lr=0.000111111, gnorm=0.408, clip=0, loss_scale=32, train_wall=77, gb_free=16.8, wall=13848
2023-08-07 17:53:50 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 17:54:11 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.42 | trans_loss 5.769 | nll_loss 3.107 | w2v_ctc_loss 1.273 | contrastive_loss 0 | total 4003.4 | n_correct 2354.6 | ppl 8.62 | accuracy 58.815 | uer 16.818 | wer 18.65 | raw_wer 18.65 | bleu 18.32 | wps 2415.2 | wpb 4003.4 | bsz 141.8 | num_updates 16206 | best_bleu 18.32
2023-08-07 17:54:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16206 updates
2023-08-07 17:54:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 17:54:30 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 17:54:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 11 @ 16206 updates, score 18.32) (writing took 33.13087131641805 seconds)
2023-08-07 17:54:45 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-08-07 17:54:45 | INFO | train | epoch 011 | loss 3.817 | trans_loss 3.785 | nll_loss 2.037 | w2v_ctc_loss 0.954 | contrastive_loss 0 | total 4138.65 | n_correct 2343.13 | ppl 4.1 | accuracy 56.616 | wps 14141.6 | ups 1.14 | wpb 12355.8 | bsz 458.5 | num_updates 16206 | lr 0.000111091 | gnorm 0.433 | clip 0 | loss_scale 32 | train_wall 1140 | gb_free 17 | wall 13908
2023-08-07 17:54:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 17:54:45 | INFO | fairseq.trainer | begin training epoch 12
2023-08-07 17:54:45 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 17:56:06 | INFO | train_inner | epoch 012:     94 / 1474 loss=3.754, trans_loss=3.736, nll_loss=1.973, w2v_ctc_loss=0.922, contrastive_loss=0, total=4146.82, n_correct=2401.54, ppl=3.93, accuracy=57.913, wps=8782.2, ups=0.71, wpb=12379.4, bsz=470.8, num_updates=16300, lr=0.00011077, gnorm=0.407, clip=0, loss_scale=32, train_wall=76, gb_free=15.6, wall=13989
2023-08-07 17:57:24 | INFO | train_inner | epoch 012:    194 / 1474 loss=3.762, trans_loss=3.739, nll_loss=1.979, w2v_ctc_loss=0.928, contrastive_loss=0, total=4120.68, n_correct=2377.03, ppl=3.94, accuracy=57.685, wps=15907.8, ups=1.29, wpb=12345.4, bsz=442.1, num_updates=16400, lr=0.000110432, gnorm=0.407, clip=0, loss_scale=32, train_wall=77, gb_free=15.5, wall=14067
2023-08-07 17:58:42 | INFO | train_inner | epoch 012:    294 / 1474 loss=3.749, trans_loss=3.736, nll_loss=1.976, w2v_ctc_loss=0.913, contrastive_loss=0, total=4199.46, n_correct=2428.66, ppl=3.94, accuracy=57.833, wps=16048.2, ups=1.28, wpb=12539.6, bsz=480.4, num_updates=16500, lr=0.000110096, gnorm=0.406, clip=0, loss_scale=64, train_wall=78, gb_free=16.4, wall=14145
2023-08-07 18:00:00 | INFO | train_inner | epoch 012:    394 / 1474 loss=3.76, trans_loss=3.741, nll_loss=1.981, w2v_ctc_loss=0.925, contrastive_loss=0, total=4151.14, n_correct=2397.08, ppl=3.95, accuracy=57.745, wps=15849.4, ups=1.28, wpb=12410, bsz=461.6, num_updates=16600, lr=0.000109764, gnorm=0.409, clip=0, loss_scale=64, train_wall=78, gb_free=16.9, wall=14223
2023-08-07 18:01:18 | INFO | train_inner | epoch 012:    494 / 1474 loss=3.784, trans_loss=3.754, nll_loss=1.995, w2v_ctc_loss=0.943, contrastive_loss=0, total=4110.49, n_correct=2364.32, ppl=3.99, accuracy=57.519, wps=15796.8, ups=1.29, wpb=12226.9, bsz=453.3, num_updates=16700, lr=0.000109435, gnorm=0.41, clip=0, loss_scale=64, train_wall=77, gb_free=13.8, wall=14301
2023-08-07 18:02:36 | INFO | train_inner | epoch 012:    594 / 1474 loss=3.761, trans_loss=3.74, nll_loss=1.982, w2v_ctc_loss=0.929, contrastive_loss=0, total=4189.92, n_correct=2416.02, ppl=3.95, accuracy=57.663, wps=15999.6, ups=1.28, wpb=12531.1, bsz=472.7, num_updates=16800, lr=0.000109109, gnorm=0.41, clip=0, loss_scale=64, train_wall=78, gb_free=14.7, wall=14379
2023-08-07 18:03:54 | INFO | train_inner | epoch 012:    694 / 1474 loss=3.744, trans_loss=3.732, nll_loss=1.971, w2v_ctc_loss=0.915, contrastive_loss=0, total=4206.3, n_correct=2434.3, ppl=3.92, accuracy=57.873, wps=16081.1, ups=1.29, wpb=12501.9, bsz=488.5, num_updates=16900, lr=0.000108786, gnorm=0.405, clip=0, loss_scale=64, train_wall=77, gb_free=16.1, wall=14457
2023-08-07 18:05:11 | INFO | train_inner | epoch 012:    794 / 1474 loss=3.773, trans_loss=3.747, nll_loss=1.987, w2v_ctc_loss=0.934, contrastive_loss=0, total=4085.96, n_correct=2354.97, ppl=3.96, accuracy=57.636, wps=15804, ups=1.29, wpb=12212.6, bsz=445.7, num_updates=17000, lr=0.000108465, gnorm=0.413, clip=0, loss_scale=64, train_wall=77, gb_free=16.2, wall=14534
2023-08-07 18:06:29 | INFO | train_inner | epoch 012:    894 / 1474 loss=3.766, trans_loss=3.741, nll_loss=1.984, w2v_ctc_loss=0.934, contrastive_loss=0, total=4169.74, n_correct=2404.12, ppl=3.95, accuracy=57.656, wps=15942.1, ups=1.28, wpb=12453, bsz=459.6, num_updates=17100, lr=0.000108148, gnorm=0.412, clip=0, loss_scale=64, train_wall=78, gb_free=15.9, wall=14612
2023-08-07 18:07:47 | INFO | train_inner | epoch 012:    994 / 1474 loss=3.77, trans_loss=3.744, nll_loss=1.985, w2v_ctc_loss=0.935, contrastive_loss=0, total=4117.67, n_correct=2369.99, ppl=3.96, accuracy=57.557, wps=15855.5, ups=1.29, wpb=12288, bsz=452.1, num_updates=17200, lr=0.000107833, gnorm=0.412, clip=0, loss_scale=64, train_wall=77, gb_free=17.4, wall=14690
2023-08-07 18:09:04 | INFO | train_inner | epoch 012:   1094 / 1474 loss=3.774, trans_loss=3.748, nll_loss=1.99, w2v_ctc_loss=0.939, contrastive_loss=0, total=4047.61, n_correct=2327.23, ppl=3.97, accuracy=57.496, wps=15640.5, ups=1.29, wpb=12086.1, bsz=435.6, num_updates=17300, lr=0.000107521, gnorm=0.42, clip=0, loss_scale=64, train_wall=77, gb_free=16.5, wall=14767
2023-08-07 18:10:22 | INFO | train_inner | epoch 012:   1194 / 1474 loss=3.785, trans_loss=3.747, nll_loss=1.992, w2v_ctc_loss=0.956, contrastive_loss=0, total=4184.55, n_correct=2396.73, ppl=3.98, accuracy=57.276, wps=16056, ups=1.28, wpb=12497.1, bsz=471.4, num_updates=17400, lr=0.000107211, gnorm=0.41, clip=0, loss_scale=64, train_wall=77, gb_free=16.6, wall=14845
2023-08-07 18:11:40 | INFO | train_inner | epoch 012:   1294 / 1474 loss=3.785, trans_loss=3.75, nll_loss=1.996, w2v_ctc_loss=0.949, contrastive_loss=0, total=4086.33, n_correct=2349.86, ppl=3.99, accuracy=57.505, wps=15671.8, ups=1.28, wpb=12210.8, bsz=437.2, num_updates=17500, lr=0.000106904, gnorm=0.418, clip=0, loss_scale=64, train_wall=77, gb_free=16.6, wall=14923
2023-08-07 18:12:58 | INFO | train_inner | epoch 012:   1394 / 1474 loss=3.762, trans_loss=3.74, nll_loss=1.981, w2v_ctc_loss=0.93, contrastive_loss=0, total=4134.89, n_correct=2381.13, ppl=3.95, accuracy=57.586, wps=15808.1, ups=1.28, wpb=12323.9, bsz=456.6, num_updates=17600, lr=0.0001066, gnorm=0.414, clip=0, loss_scale=64, train_wall=78, gb_free=17.1, wall=15001
2023-08-07 18:13:59 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 18:14:24 | INFO | dev_st | epoch 012 | valid on 'dev_st' subset | loss 4.393 | trans_loss 5.733 | nll_loss 3.056 | w2v_ctc_loss 1.264 | contrastive_loss 0 | total 4003.4 | n_correct 2371.4 | ppl 8.32 | accuracy 59.235 | uer 16.935 | wer 18.84 | raw_wer 18.84 | bleu 18.48 | wps 2046 | wpb 4003.4 | bsz 141.8 | num_updates 17680 | best_bleu 18.48
2023-08-07 18:14:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 17680 updates
2023-08-07 18:14:24 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 18:14:38 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 18:14:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 12 @ 17680 updates, score 18.48) (writing took 26.52892683632672 seconds)
2023-08-07 18:14:51 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-08-07 18:14:51 | INFO | train | epoch 012 | loss 3.766 | trans_loss 3.742 | nll_loss 1.984 | w2v_ctc_loss 0.932 | contrastive_loss 0 | total 4138.65 | n_correct 2385.54 | ppl 3.96 | accuracy 57.641 | wps 15103.3 | ups 1.22 | wpb 12355.8 | bsz 458.5 | num_updates 17680 | lr 0.000106359 | gnorm 0.411 | clip 0 | loss_scale 64 | train_wall 1139 | gb_free 12.8 | wall 15114
2023-08-07 18:14:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 18:14:51 | INFO | fairseq.trainer | begin training epoch 13
2023-08-07 18:14:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 18:15:15 | INFO | train_inner | epoch 013:     20 / 1474 loss=3.769, trans_loss=3.736, nll_loss=1.977, w2v_ctc_loss=0.944, contrastive_loss=0, total=4104.86, n_correct=2367.9, ppl=3.94, accuracy=57.685, wps=8907.5, ups=0.73, wpb=12264.8, bsz=445.3, num_updates=17700, lr=0.000106299, gnorm=0.413, clip=0, loss_scale=64, train_wall=77, gb_free=14.6, wall=15138
2023-08-07 18:16:33 | INFO | train_inner | epoch 013:    120 / 1474 loss=3.714, trans_loss=3.698, nll_loss=1.927, w2v_ctc_loss=0.907, contrastive_loss=0, total=4161.2, n_correct=2439.98, ppl=3.8, accuracy=58.636, wps=15999.1, ups=1.29, wpb=12419, bsz=454.4, num_updates=17800, lr=0.000106, gnorm=0.408, clip=0, loss_scale=64, train_wall=77, gb_free=16, wall=15216
2023-08-07 18:17:52 | INFO | train_inner | epoch 013:    220 / 1474 loss=3.718, trans_loss=3.703, nll_loss=1.936, w2v_ctc_loss=0.912, contrastive_loss=0, total=4202.62, n_correct=2457.97, ppl=3.83, accuracy=58.487, wps=15862.1, ups=1.27, wpb=12504.4, bsz=492.4, num_updates=17900, lr=0.000105703, gnorm=0.408, clip=0, loss_scale=64, train_wall=78, gb_free=17, wall=15295
2023-08-07 18:19:09 | INFO | train_inner | epoch 013:    320 / 1474 loss=3.719, trans_loss=3.708, nll_loss=1.939, w2v_ctc_loss=0.9, contrastive_loss=0, total=4112.8, n_correct=2411.25, ppl=3.83, accuracy=58.628, wps=15862.6, ups=1.29, wpb=12262.5, bsz=444, num_updates=18000, lr=0.000105409, gnorm=0.414, clip=0, loss_scale=64, train_wall=77, gb_free=17.6, wall=15372
2023-08-07 18:19:09 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 18:19:32 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.403 | trans_loss 5.74 | nll_loss 3.068 | w2v_ctc_loss 1.284 | contrastive_loss 0 | total 4003.4 | n_correct 2367.4 | ppl 8.39 | accuracy 59.135 | uer 17.235 | wer 18.993 | raw_wer 18.993 | bleu 18.48 | wps 2241.3 | wpb 4003.4 | bsz 141.8 | num_updates 18000 | best_bleu 18.48
2023-08-07 18:19:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 18000 updates
2023-08-07 18:19:32 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_13_18000.pt
2023-08-07 18:19:35 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_13_18000.pt
2023-08-07 18:19:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_13_18000.pt (epoch 13 @ 18000 updates, score 18.48) (writing took 26.731262983754277 seconds)
2023-08-07 18:21:17 | INFO | train_inner | epoch 013:    420 / 1474 loss=3.72, trans_loss=3.706, nll_loss=1.936, w2v_ctc_loss=0.907, contrastive_loss=0, total=4176.06, n_correct=2455.01, ppl=3.83, accuracy=58.788, wps=9740.8, ups=0.78, wpb=12453.9, bsz=476.2, num_updates=18100, lr=0.000105118, gnorm=0.409, clip=0, loss_scale=64, train_wall=77, gb_free=16.3, wall=15500
2023-08-07 18:22:35 | INFO | train_inner | epoch 013:    520 / 1474 loss=3.726, trans_loss=3.708, nll_loss=1.939, w2v_ctc_loss=0.912, contrastive_loss=0, total=4197.57, n_correct=2451.15, ppl=3.83, accuracy=58.394, wps=15990.2, ups=1.28, wpb=12523.8, bsz=477.2, num_updates=18200, lr=0.000104828, gnorm=0.41, clip=0, loss_scale=64, train_wall=78, gb_free=15.2, wall=15578
2023-08-07 18:23:53 | INFO | train_inner | epoch 013:    620 / 1474 loss=3.713, trans_loss=3.699, nll_loss=1.928, w2v_ctc_loss=0.902, contrastive_loss=0, total=4160.12, n_correct=2442.71, ppl=3.81, accuracy=58.717, wps=15920.6, ups=1.28, wpb=12433.1, bsz=463, num_updates=18300, lr=0.000104542, gnorm=0.409, clip=0, loss_scale=64, train_wall=78, gb_free=16.2, wall=15657
2023-08-07 18:25:11 | INFO | train_inner | epoch 013:    720 / 1474 loss=3.743, trans_loss=3.713, nll_loss=1.946, w2v_ctc_loss=0.935, contrastive_loss=0, total=4101.54, n_correct=2389.66, ppl=3.85, accuracy=58.263, wps=15768.5, ups=1.29, wpb=12238, bsz=428.5, num_updates=18400, lr=0.000104257, gnorm=0.419, clip=0, loss_scale=64, train_wall=77, gb_free=15.5, wall=15734
2023-08-07 18:26:30 | INFO | train_inner | epoch 013:    820 / 1474 loss=3.727, trans_loss=3.706, nll_loss=1.938, w2v_ctc_loss=0.915, contrastive_loss=0, total=4126.37, n_correct=2405.98, ppl=3.83, accuracy=58.307, wps=15662.4, ups=1.27, wpb=12333.7, bsz=460.5, num_updates=18500, lr=0.000103975, gnorm=0.415, clip=0, loss_scale=128, train_wall=78, gb_free=17.5, wall=15813
2023-08-07 18:26:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-07 18:27:48 | INFO | train_inner | epoch 013:    921 / 1474 loss=3.726, trans_loss=3.71, nll_loss=1.943, w2v_ctc_loss=0.909, contrastive_loss=0, total=4090.94, n_correct=2391.41, ppl=3.85, accuracy=58.456, wps=15590.4, ups=1.28, wpb=12219.5, bsz=437.4, num_updates=18600, lr=0.000103695, gnorm=0.415, clip=0, loss_scale=64, train_wall=78, gb_free=15.8, wall=15891
2023-08-07 18:29:05 | INFO | train_inner | epoch 013:   1021 / 1474 loss=3.73, trans_loss=3.704, nll_loss=1.939, w2v_ctc_loss=0.927, contrastive_loss=0, total=4081.02, n_correct=2376.71, ppl=3.83, accuracy=58.238, wps=15830.3, ups=1.3, wpb=12208.8, bsz=440.1, num_updates=18700, lr=0.000103418, gnorm=0.415, clip=0, loss_scale=64, train_wall=77, gb_free=16, wall=15968
2023-08-07 18:30:23 | INFO | train_inner | epoch 013:   1121 / 1474 loss=3.724, trans_loss=3.705, nll_loss=1.936, w2v_ctc_loss=0.911, contrastive_loss=0, total=4105.62, n_correct=2410.19, ppl=3.83, accuracy=58.705, wps=15790.7, ups=1.29, wpb=12265.4, bsz=458.9, num_updates=18800, lr=0.000103142, gnorm=0.418, clip=0, loss_scale=64, train_wall=77, gb_free=16.6, wall=16046
2023-08-07 18:31:41 | INFO | train_inner | epoch 013:   1221 / 1474 loss=3.739, trans_loss=3.713, nll_loss=1.945, w2v_ctc_loss=0.922, contrastive_loss=0, total=4110.35, n_correct=2406.43, ppl=3.85, accuracy=58.546, wps=15758.4, ups=1.28, wpb=12287.7, bsz=442.6, num_updates=18900, lr=0.000102869, gnorm=0.42, clip=0, loss_scale=64, train_wall=78, gb_free=14.7, wall=16124
2023-08-07 18:32:59 | INFO | train_inner | epoch 013:   1321 / 1474 loss=3.715, trans_loss=3.7, nll_loss=1.932, w2v_ctc_loss=0.91, contrastive_loss=0, total=4112.2, n_correct=2417.21, ppl=3.82, accuracy=58.781, wps=15773.6, ups=1.28, wpb=12280.4, bsz=462.3, num_updates=19000, lr=0.000102598, gnorm=0.416, clip=0, loss_scale=64, train_wall=77, gb_free=17.4, wall=16202
2023-08-07 18:34:17 | INFO | train_inner | epoch 013:   1421 / 1474 loss=3.723, trans_loss=3.71, nll_loss=1.942, w2v_ctc_loss=0.906, contrastive_loss=0, total=4180.88, n_correct=2442.61, ppl=3.84, accuracy=58.423, wps=15988.9, ups=1.28, wpb=12470.5, bsz=468.3, num_updates=19100, lr=0.000102329, gnorm=0.415, clip=0, loss_scale=64, train_wall=78, gb_free=15.2, wall=16280
2023-08-07 18:34:58 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 18:35:20 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.383 | trans_loss 5.708 | nll_loss 3.034 | w2v_ctc_loss 1.291 | contrastive_loss 0 | total 4003.4 | n_correct 2382.8 | ppl 8.19 | accuracy 59.519 | uer 16.712 | wer 18.541 | raw_wer 18.541 | bleu 18.86 | wps 2197.8 | wpb 4003.4 | bsz 141.8 | num_updates 19153 | best_bleu 18.86
2023-08-07 18:35:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 19153 updates
2023-08-07 18:35:20 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 18:35:35 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 18:35:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 13 @ 19153 updates, score 18.86) (writing took 27.356288535520434 seconds)
2023-08-07 18:35:48 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-08-07 18:35:48 | INFO | train | epoch 013 | loss 3.724 | trans_loss 3.705 | nll_loss 1.937 | w2v_ctc_loss 0.913 | contrastive_loss 0 | total 4138.12 | n_correct 2422.4 | ppl 3.83 | accuracy 58.539 | wps 14469.5 | ups 1.17 | wpb 12354.3 | bsz 458.2 | num_updates 19153 | lr 0.000102187 | gnorm 0.414 | clip 0 | loss_scale 64 | train_wall 1141 | gb_free 17.4 | wall 16372
2023-08-07 18:35:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 18:35:49 | INFO | fairseq.trainer | begin training epoch 14
2023-08-07 18:35:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 18:36:33 | INFO | train_inner | epoch 014:     47 / 1474 loss=3.683, trans_loss=3.67, nll_loss=1.893, w2v_ctc_loss=0.897, contrastive_loss=0, total=4176.2, n_correct=2480.46, ppl=3.71, accuracy=59.395, wps=9142.2, ups=0.73, wpb=12486.4, bsz=483.4, num_updates=19200, lr=0.000102062, gnorm=0.41, clip=0, loss_scale=64, train_wall=77, gb_free=10.8, wall=16416
2023-08-07 18:37:51 | INFO | train_inner | epoch 014:    147 / 1474 loss=3.676, trans_loss=3.666, nll_loss=1.883, w2v_ctc_loss=0.889, contrastive_loss=0, total=4080.86, n_correct=2436.08, ppl=3.69, accuracy=59.695, wps=15790.7, ups=1.29, wpb=12203.2, bsz=449.3, num_updates=19300, lr=0.000101797, gnorm=0.412, clip=0, loss_scale=64, train_wall=77, gb_free=16.8, wall=16494
2023-08-07 18:39:08 | INFO | train_inner | epoch 014:    247 / 1474 loss=3.682, trans_loss=3.679, nll_loss=1.901, w2v_ctc_loss=0.886, contrastive_loss=0, total=4106.97, n_correct=2434.64, ppl=3.73, accuracy=59.281, wps=15804.3, ups=1.29, wpb=12224.4, bsz=440, num_updates=19400, lr=0.000101535, gnorm=0.415, clip=0, loss_scale=64, train_wall=77, gb_free=12.4, wall=16571
2023-08-07 18:40:25 | INFO | train_inner | epoch 014:    347 / 1474 loss=3.654, trans_loss=3.651, nll_loss=1.873, w2v_ctc_loss=0.879, contrastive_loss=0, total=4179.8, n_correct=2493.22, ppl=3.66, accuracy=59.649, wps=16127.2, ups=1.29, wpb=12454.9, bsz=483.8, num_updates=19500, lr=0.000101274, gnorm=0.407, clip=0, loss_scale=64, train_wall=77, gb_free=17.1, wall=16648
2023-08-07 18:41:43 | INFO | train_inner | epoch 014:    447 / 1474 loss=3.682, trans_loss=3.675, nll_loss=1.899, w2v_ctc_loss=0.889, contrastive_loss=0, total=4120.38, n_correct=2440.89, ppl=3.73, accuracy=59.239, wps=15858.9, ups=1.29, wpb=12283.1, bsz=444.5, num_updates=19600, lr=0.000101015, gnorm=0.415, clip=0, loss_scale=64, train_wall=77, gb_free=17, wall=16726
2023-08-07 18:43:01 | INFO | train_inner | epoch 014:    547 / 1474 loss=3.703, trans_loss=3.681, nll_loss=1.905, w2v_ctc_loss=0.907, contrastive_loss=0, total=4089.86, n_correct=2424.69, ppl=3.75, accuracy=59.285, wps=15609.4, ups=1.27, wpb=12262.9, bsz=443.3, num_updates=19700, lr=0.000100759, gnorm=0.416, clip=0, loss_scale=64, train_wall=78, gb_free=12.1, wall=16804
2023-08-07 18:44:19 | INFO | train_inner | epoch 014:    647 / 1474 loss=3.687, trans_loss=3.676, nll_loss=1.9, w2v_ctc_loss=0.896, contrastive_loss=0, total=4158.94, n_correct=2465.61, ppl=3.73, accuracy=59.285, wps=15901.8, ups=1.28, wpb=12415.4, bsz=460, num_updates=19800, lr=0.000100504, gnorm=0.417, clip=0, loss_scale=64, train_wall=78, gb_free=16, wall=16882
2023-08-07 18:45:36 | INFO | train_inner | epoch 014:    747 / 1474 loss=3.681, trans_loss=3.672, nll_loss=1.894, w2v_ctc_loss=0.891, contrastive_loss=0, total=4150.03, n_correct=2465.43, ppl=3.72, accuracy=59.408, wps=16078, ups=1.3, wpb=12407.3, bsz=465.5, num_updates=19900, lr=0.000100251, gnorm=0.412, clip=0, loss_scale=64, train_wall=77, gb_free=15.4, wall=16960
2023-08-07 18:46:54 | INFO | train_inner | epoch 014:    847 / 1474 loss=3.67, trans_loss=3.662, nll_loss=1.885, w2v_ctc_loss=0.887, contrastive_loss=0, total=4162.8, n_correct=2475.25, ppl=3.69, accuracy=59.461, wps=16013, ups=1.29, wpb=12422.9, bsz=475.8, num_updates=20000, lr=0.0001, gnorm=0.413, clip=0, loss_scale=64, train_wall=77, gb_free=16.9, wall=17037
2023-08-07 18:46:54 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 18:47:15 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.381 | trans_loss 5.7 | nll_loss 3.016 | w2v_ctc_loss 1.304 | contrastive_loss 0 | total 4003.4 | n_correct 2389 | ppl 8.09 | accuracy 59.674 | uer 16.675 | wer 18.553 | raw_wer 18.553 | bleu 19.04 | wps 2431.6 | wpb 4003.4 | bsz 141.8 | num_updates 20000 | best_bleu 19.04
2023-08-07 18:47:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20000 updates
2023-08-07 18:47:15 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_14_20000.pt
2023-08-07 18:47:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_14_20000.pt
2023-08-07 18:47:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_14_20000.pt (epoch 14 @ 20000 updates, score 19.04) (writing took 27.947169607505202 seconds)
mt_weight tensor(0.1062, device='cuda:0')
asr_weight tensor(0.0053, device='cuda:0')
2023-08-07 18:48:42 | INFO | train_inner | epoch 014:    947 / 1474 loss=4.825, trans_loss=5.462, nll_loss=2.827, w2v_ctc_loss=0.679, contrastive_loss=0, total=4159.46, n_correct=2455.91, ppl=7.1, accuracy=59.044, wps=7747.2, ups=0.92, wpb=8378.4, bsz=309.1, num_updates=20100, lr=9.97509e-05, gnorm=0.53, clip=0, loss_scale=64, train_wall=57, gb_free=15.4, wall=17145
2023-08-07 18:49:40 | INFO | train_inner | epoch 014:   1047 / 1474 loss=4.849, trans_loss=5.501, nll_loss=2.853, w2v_ctc_loss=0.673, contrastive_loss=0, total=4155.93, n_correct=2452.23, ppl=7.22, accuracy=59.006, wps=14372.8, ups=1.73, wpb=8310, bsz=305.9, num_updates=20200, lr=9.95037e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=57, gb_free=16.5, wall=17203
2023-08-07 18:50:38 | INFO | train_inner | epoch 014:   1147 / 1474 loss=4.843, trans_loss=5.494, nll_loss=2.847, w2v_ctc_loss=0.685, contrastive_loss=0, total=4228.09, n_correct=2492.39, ppl=7.19, accuracy=58.948, wps=14675.2, ups=1.74, wpb=8439, bsz=326.3, num_updates=20300, lr=9.92583e-05, gnorm=0.527, clip=0, loss_scale=64, train_wall=57, gb_free=17.6, wall=17261
2023-08-07 18:51:35 | INFO | train_inner | epoch 014:   1247 / 1474 loss=4.872, trans_loss=5.52, nll_loss=2.878, w2v_ctc_loss=0.695, contrastive_loss=0, total=4027.71, n_correct=2358.63, ppl=7.35, accuracy=58.56, wps=14093.2, ups=1.75, wpb=8067.4, bsz=273.6, num_updates=20400, lr=9.90148e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=57, gb_free=16.8, wall=17318
2023-08-07 18:52:32 | INFO | train_inner | epoch 014:   1347 / 1474 loss=4.839, trans_loss=5.491, nll_loss=2.842, w2v_ctc_loss=0.67, contrastive_loss=0, total=4198.71, n_correct=2484.07, ppl=7.17, accuracy=59.163, wps=14639.7, ups=1.74, wpb=8395.6, bsz=315.4, num_updates=20500, lr=9.8773e-05, gnorm=0.521, clip=0, loss_scale=64, train_wall=57, gb_free=16.7, wall=17375
2023-08-07 18:53:30 | INFO | train_inner | epoch 014:   1447 / 1474 loss=4.849, trans_loss=5.499, nll_loss=2.853, w2v_ctc_loss=0.681, contrastive_loss=0, total=4140.5, n_correct=2435.54, ppl=7.22, accuracy=58.822, wps=14401.6, ups=1.74, wpb=8288.3, bsz=307.1, num_updates=20600, lr=9.85329e-05, gnorm=0.533, clip=0, loss_scale=128, train_wall=57, gb_free=17.6, wall=17433
2023-08-07 18:53:45 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.1062, device='cuda:6')
asr_weight tensor(0.0053, device='cuda:6')
mt_weight tensor(0.1062, device='cuda:1')
asr_weight tensor(0.0053, device='cuda:1')
mt_weight tensor(0.1062, device='cuda:5')
asr_weight tensor(0.0053, device='cuda:5')
mt_weight tensor(0.1062, device='cuda:7')
asr_weight tensor(0.0053, device='cuda:7')
mt_weight tensor(0.1062, device='cuda:2')
asr_weight tensor(0.0053, device='cuda:2')
mt_weight tensor(0.1062, device='cuda:3')
asr_weight tensor(0.0053, device='cuda:3')
mt_weight tensor(0.1062, device='cuda:4')
asr_weight tensor(0.0053, device='cuda:4')
2023-08-07 18:54:06 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.38 | trans_loss 5.694 | nll_loss 3.017 | w2v_ctc_loss 1.314 | contrastive_loss 0 | total 4003.4 | n_correct 2387.9 | ppl 8.09 | accuracy 59.647 | uer 16.752 | wer 18.571 | raw_wer 18.571 | bleu 18.56 | wps 2437.6 | wpb 4003.4 | bsz 141.8 | num_updates 20627 | best_bleu 19.04
2023-08-07 18:54:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20627 updates
2023-08-07 18:54:06 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_18.5609.pt
2023-08-07 18:54:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_18.5609.pt
2023-08-07 18:54:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_18.5609.pt (epoch 14 @ 20627 updates, score 18.56) (writing took 37.83383759856224 seconds)
2023-08-07 18:54:45 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-08-07 18:54:45 | INFO | train | epoch 014 | loss 4.066 | trans_loss 4.276 | nll_loss 2.21 | w2v_ctc_loss 0.821 | contrastive_loss 0 | total 4138.65 | n_correct 2450.61 | ppl 4.63 | accuracy 59.213 | wps 13780.7 | ups 1.3 | wpb 10623.1 | bsz 393.9 | num_updates 20627 | lr 9.84684e-05 | gnorm 0.463 | clip 0 | loss_scale 128 | train_wall 1011 | gb_free 16.4 | wall 17508
2023-08-07 18:54:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 18:54:45 | INFO | fairseq.trainer | begin training epoch 15
2023-08-07 18:54:45 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 18:55:34 | INFO | train_inner | epoch 015:     73 / 1474 loss=4.813, trans_loss=5.46, nll_loss=2.803, w2v_ctc_loss=0.672, contrastive_loss=0, total=4083.93, n_correct=2434.65, ppl=6.98, accuracy=59.615, wps=6550, ups=0.8, wpb=8166.9, bsz=300.1, num_updates=20700, lr=9.82946e-05, gnorm=0.534, clip=0, loss_scale=128, train_wall=56, gb_free=15.6, wall=17558
2023-08-07 18:56:32 | INFO | train_inner | epoch 015:    173 / 1474 loss=4.81, trans_loss=5.454, nll_loss=2.794, w2v_ctc_loss=0.67, contrastive_loss=0, total=4122.67, n_correct=2470.24, ppl=6.93, accuracy=59.918, wps=14370.3, ups=1.74, wpb=8245.4, bsz=299.1, num_updates=20800, lr=9.80581e-05, gnorm=0.529, clip=0, loss_scale=128, train_wall=57, gb_free=17.2, wall=17615
2023-08-07 18:57:29 | INFO | train_inner | epoch 015:    273 / 1474 loss=4.805, trans_loss=5.451, nll_loss=2.791, w2v_ctc_loss=0.663, contrastive_loss=0, total=4190.11, n_correct=2509.81, ppl=6.92, accuracy=59.898, wps=14625.6, ups=1.75, wpb=8378.8, bsz=312.6, num_updates=20900, lr=9.78232e-05, gnorm=0.525, clip=0, loss_scale=128, train_wall=57, gb_free=17.1, wall=17672
2023-08-07 18:58:27 | INFO | train_inner | epoch 015:    373 / 1474 loss=4.808, trans_loss=5.454, nll_loss=2.795, w2v_ctc_loss=0.669, contrastive_loss=0, total=4150.33, n_correct=2482.5, ppl=6.94, accuracy=59.815, wps=14401.8, ups=1.74, wpb=8275.8, bsz=301, num_updates=21000, lr=9.759e-05, gnorm=0.53, clip=0, loss_scale=128, train_wall=57, gb_free=15.5, wall=17730
2023-08-07 18:59:24 | INFO | train_inner | epoch 015:    473 / 1474 loss=4.804, trans_loss=5.452, nll_loss=2.792, w2v_ctc_loss=0.659, contrastive_loss=0, total=4082.7, n_correct=2438.67, ppl=6.93, accuracy=59.732, wps=14221.2, ups=1.74, wpb=8165.9, bsz=298.5, num_updates=21100, lr=9.73585e-05, gnorm=0.531, clip=0, loss_scale=128, train_wall=57, gb_free=17.1, wall=17787
2023-08-07 19:00:22 | INFO | train_inner | epoch 015:    573 / 1474 loss=4.821, trans_loss=5.464, nll_loss=2.807, w2v_ctc_loss=0.677, contrastive_loss=0, total=4130.96, n_correct=2463.31, ppl=7, accuracy=59.63, wps=14359.5, ups=1.74, wpb=8272.7, bsz=293.4, num_updates=21200, lr=9.71286e-05, gnorm=0.532, clip=0, loss_scale=128, train_wall=57, gb_free=17.3, wall=17845
2023-08-07 19:01:19 | INFO | train_inner | epoch 015:    673 / 1474 loss=4.806, trans_loss=5.448, nll_loss=2.787, w2v_ctc_loss=0.672, contrastive_loss=0, total=4138.41, n_correct=2478.64, ppl=6.9, accuracy=59.894, wps=14383.9, ups=1.73, wpb=8291.4, bsz=309.4, num_updates=21300, lr=9.69003e-05, gnorm=0.533, clip=0, loss_scale=128, train_wall=57, gb_free=16.9, wall=17902
2023-08-07 19:02:17 | INFO | train_inner | epoch 015:    773 / 1474 loss=4.821, trans_loss=5.468, nll_loss=2.814, w2v_ctc_loss=0.677, contrastive_loss=0, total=4186.48, n_correct=2494.54, ppl=7.03, accuracy=59.586, wps=14386.3, ups=1.72, wpb=8351.6, bsz=307.9, num_updates=21400, lr=9.66736e-05, gnorm=0.529, clip=0, loss_scale=128, train_wall=58, gb_free=16.7, wall=17960
2023-08-07 19:03:14 | INFO | train_inner | epoch 015:    873 / 1474 loss=4.825, trans_loss=5.468, nll_loss=2.814, w2v_ctc_loss=0.673, contrastive_loss=0, total=4054.09, n_correct=2413.59, ppl=7.03, accuracy=59.535, wps=14226.1, ups=1.75, wpb=8135.5, bsz=286.2, num_updates=21500, lr=9.64486e-05, gnorm=0.533, clip=0, loss_scale=128, train_wall=57, gb_free=15.9, wall=18018
2023-08-07 19:04:11 | INFO | train_inner | epoch 015:    973 / 1474 loss=4.806, trans_loss=5.455, nll_loss=2.797, w2v_ctc_loss=0.673, contrastive_loss=0, total=4126.63, n_correct=2460.77, ppl=6.95, accuracy=59.631, wps=14423.1, ups=1.75, wpb=8225, bsz=303, num_updates=21600, lr=9.6225e-05, gnorm=0.533, clip=0, loss_scale=128, train_wall=57, gb_free=16.9, wall=18075
2023-08-07 19:04:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-07 19:05:10 | INFO | train_inner | epoch 015:   1074 / 1474 loss=4.823, trans_loss=5.468, nll_loss=2.816, w2v_ctc_loss=0.677, contrastive_loss=0, total=4163.81, n_correct=2480.65, ppl=7.04, accuracy=59.576, wps=14201.8, ups=1.71, wpb=8323.2, bsz=313.7, num_updates=21700, lr=9.60031e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=58, gb_free=17.2, wall=18133
2023-08-07 19:06:08 | INFO | train_inner | epoch 015:   1174 / 1474 loss=4.801, trans_loss=5.448, nll_loss=2.79, w2v_ctc_loss=0.664, contrastive_loss=0, total=4185, n_correct=2503.29, ppl=6.92, accuracy=59.816, wps=14531.3, ups=1.74, wpb=8369.7, bsz=329.3, num_updates=21800, lr=9.57826e-05, gnorm=0.527, clip=0, loss_scale=64, train_wall=57, gb_free=16.3, wall=18191
2023-08-07 19:07:06 | INFO | train_inner | epoch 015:   1274 / 1474 loss=4.823, trans_loss=5.464, nll_loss=2.81, w2v_ctc_loss=0.685, contrastive_loss=0, total=4152.04, n_correct=2476.42, ppl=7.01, accuracy=59.643, wps=14337.6, ups=1.73, wpb=8309.2, bsz=303.7, num_updates=21900, lr=9.55637e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=57, gb_free=16.5, wall=18249
2023-08-07 19:08:03 | INFO | train_inner | epoch 015:   1374 / 1474 loss=4.821, trans_loss=5.464, nll_loss=2.811, w2v_ctc_loss=0.676, contrastive_loss=0, total=4100.21, n_correct=2442.4, ppl=7.02, accuracy=59.568, wps=14349.7, ups=1.75, wpb=8205.1, bsz=293.6, num_updates=22000, lr=9.53463e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=57, gb_free=17.5, wall=18306
2023-08-07 19:08:03 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 19:08:25 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 4.368 | trans_loss 5.678 | nll_loss 2.991 | w2v_ctc_loss 1.31 | contrastive_loss 0 | total 4003.4 | n_correct 2401.9 | ppl 7.95 | accuracy 59.997 | uer 16.633 | wer 18.403 | raw_wer 18.403 | bleu 18.84 | wps 2307.6 | wpb 4003.4 | bsz 141.8 | num_updates 22000 | best_bleu 19.04
2023-08-07 19:08:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22000 updates
2023-08-07 19:08:25 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_15_22000.pt
2023-08-07 19:08:29 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_15_22000.pt
2023-08-07 19:09:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_15_22000.pt (epoch 15 @ 22000 updates, score 18.84) (writing took 37.63324399664998 seconds)
2023-08-07 19:10:02 | INFO | train_inner | epoch 015:   1474 / 1474 loss=4.817, trans_loss=5.459, nll_loss=2.805, w2v_ctc_loss=0.68, contrastive_loss=0, total=4141.17, n_correct=2469.39, ppl=6.99, accuracy=59.63, wps=6993.5, ups=0.84, wpb=8307.8, bsz=314.3, num_updates=22100, lr=9.51303e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=57, gb_free=17, wall=18425
2023-08-07 19:10:02 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 19:10:24 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 4.364 | trans_loss 5.673 | nll_loss 2.991 | w2v_ctc_loss 1.307 | contrastive_loss 0 | total 4003.4 | n_correct 2406.7 | ppl 7.95 | accuracy 60.116 | uer 16.816 | wer 18.538 | raw_wer 18.538 | bleu 19.24 | wps 2257.1 | wpb 4003.4 | bsz 141.8 | num_updates 22100 | best_bleu 19.24
2023-08-07 19:10:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22100 updates
2023-08-07 19:10:24 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 19:10:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 19:10:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 15 @ 22100 updates, score 19.24) (writing took 28.19098672270775 seconds)
2023-08-07 19:10:53 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-08-07 19:10:53 | INFO | train | epoch 015 | loss 4.813 | trans_loss 5.458 | nll_loss 2.8 | w2v_ctc_loss 0.672 | contrastive_loss 0 | total 4136.69 | n_correct 2470.47 | ppl 6.97 | accuracy 59.721 | wps 12588.6 | ups 1.52 | wpb 8273.8 | bsz 304.9 | num_updates 22100 | lr 9.51303e-05 | gnorm 0.531 | clip 0 | loss_scale 64 | train_wall 841 | gb_free 17 | wall 18476
2023-08-07 19:10:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 19:10:53 | INFO | fairseq.trainer | begin training epoch 16
2023-08-07 19:10:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 19:11:59 | INFO | train_inner | epoch 016:    100 / 1474 loss=4.77, trans_loss=5.408, nll_loss=2.738, w2v_ctc_loss=0.651, contrastive_loss=0, total=4126.22, n_correct=2510.74, ppl=6.67, accuracy=60.848, wps=7028.1, ups=0.85, wpb=8260.9, bsz=315.6, num_updates=22200, lr=9.49158e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=57, gb_free=16.1, wall=18542
2023-08-07 19:12:57 | INFO | train_inner | epoch 016:    200 / 1474 loss=4.774, trans_loss=5.412, nll_loss=2.742, w2v_ctc_loss=0.649, contrastive_loss=0, total=4100.6, n_correct=2490.27, ppl=6.69, accuracy=60.729, wps=14351.7, ups=1.74, wpb=8229.9, bsz=296.8, num_updates=22300, lr=9.47027e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=57, gb_free=12.8, wall=18600
2023-08-07 19:13:54 | INFO | train_inner | epoch 016:    300 / 1474 loss=4.788, trans_loss=5.429, nll_loss=2.765, w2v_ctc_loss=0.673, contrastive_loss=0, total=4166.94, n_correct=2518.99, ppl=6.8, accuracy=60.452, wps=14419.8, ups=1.74, wpb=8292.7, bsz=308.9, num_updates=22400, lr=9.44911e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=57, gb_free=17.2, wall=18657
2023-08-07 19:14:51 | INFO | train_inner | epoch 016:    400 / 1474 loss=4.794, trans_loss=5.432, nll_loss=2.767, w2v_ctc_loss=0.669, contrastive_loss=0, total=4073.3, n_correct=2455.69, ppl=6.81, accuracy=60.287, wps=14236.2, ups=1.74, wpb=8159.8, bsz=288.1, num_updates=22500, lr=9.42809e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=57, gb_free=17, wall=18715
2023-08-07 19:15:49 | INFO | train_inner | epoch 016:    500 / 1474 loss=4.775, trans_loss=5.414, nll_loss=2.745, w2v_ctc_loss=0.667, contrastive_loss=0, total=4174.67, n_correct=2533.16, ppl=6.71, accuracy=60.679, wps=14389.1, ups=1.73, wpb=8336.5, bsz=319.1, num_updates=22600, lr=9.40721e-05, gnorm=0.526, clip=0, loss_scale=64, train_wall=57, gb_free=16, wall=18772
2023-08-07 19:16:47 | INFO | train_inner | epoch 016:    600 / 1474 loss=4.785, trans_loss=5.426, nll_loss=2.761, w2v_ctc_loss=0.66, contrastive_loss=0, total=4124.65, n_correct=2495.17, ppl=6.78, accuracy=60.494, wps=14391.9, ups=1.75, wpb=8247.4, bsz=297.6, num_updates=22700, lr=9.38647e-05, gnorm=0.531, clip=0, loss_scale=64, train_wall=57, gb_free=16.3, wall=18830
2023-08-07 19:17:44 | INFO | train_inner | epoch 016:    700 / 1474 loss=4.789, trans_loss=5.429, nll_loss=2.764, w2v_ctc_loss=0.664, contrastive_loss=0, total=4095.49, n_correct=2470.64, ppl=6.8, accuracy=60.326, wps=14288.4, ups=1.74, wpb=8188.3, bsz=296.3, num_updates=22800, lr=9.36586e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=57, gb_free=16.2, wall=18887
2023-08-07 19:18:41 | INFO | train_inner | epoch 016:    800 / 1474 loss=4.783, trans_loss=5.424, nll_loss=2.759, w2v_ctc_loss=0.657, contrastive_loss=0, total=4174.94, n_correct=2524.12, ppl=6.77, accuracy=60.459, wps=14585.2, ups=1.74, wpb=8359, bsz=310.9, num_updates=22900, lr=9.34539e-05, gnorm=0.524, clip=0, loss_scale=64, train_wall=57, gb_free=16.4, wall=18944
2023-08-07 19:19:39 | INFO | train_inner | epoch 016:    900 / 1474 loss=4.785, trans_loss=5.426, nll_loss=2.762, w2v_ctc_loss=0.663, contrastive_loss=0, total=4163.19, n_correct=2511.56, ppl=6.78, accuracy=60.328, wps=14423.3, ups=1.73, wpb=8323.1, bsz=310.6, num_updates=23000, lr=9.32505e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=57, gb_free=16.9, wall=19002
2023-08-07 19:20:36 | INFO | train_inner | epoch 016:   1000 / 1474 loss=4.797, trans_loss=5.434, nll_loss=2.772, w2v_ctc_loss=0.675, contrastive_loss=0, total=4103.45, n_correct=2465.83, ppl=6.83, accuracy=60.092, wps=14323.3, ups=1.74, wpb=8222.1, bsz=298.1, num_updates=23100, lr=9.30484e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=57, gb_free=15, wall=19059
2023-08-07 19:21:34 | INFO | train_inner | epoch 016:   1100 / 1474 loss=4.81, trans_loss=5.447, nll_loss=2.789, w2v_ctc_loss=0.681, contrastive_loss=0, total=4119.27, n_correct=2469.98, ppl=6.91, accuracy=59.962, wps=14339.8, ups=1.74, wpb=8256.4, bsz=295.3, num_updates=23200, lr=9.28477e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=57, gb_free=17.8, wall=19117
2023-08-07 19:22:32 | INFO | train_inner | epoch 016:   1200 / 1474 loss=4.79, trans_loss=5.432, nll_loss=2.771, w2v_ctc_loss=0.668, contrastive_loss=0, total=4165.11, n_correct=2505.3, ppl=6.82, accuracy=60.15, wps=14339.1, ups=1.72, wpb=8328.2, bsz=308.7, num_updates=23300, lr=9.26482e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=58, gb_free=16.8, wall=19175
2023-08-07 19:23:30 | INFO | train_inner | epoch 016:   1300 / 1474 loss=4.792, trans_loss=5.43, nll_loss=2.769, w2v_ctc_loss=0.677, contrastive_loss=0, total=4134.61, n_correct=2487.53, ppl=6.81, accuracy=60.164, wps=14325.1, ups=1.73, wpb=8274.4, bsz=310.8, num_updates=23400, lr=9.245e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=57, gb_free=16.7, wall=19233
2023-08-07 19:24:28 | INFO | train_inner | epoch 016:   1400 / 1474 loss=4.782, trans_loss=5.423, nll_loss=2.76, w2v_ctc_loss=0.669, contrastive_loss=0, total=4206.33, n_correct=2539.52, ppl=6.77, accuracy=60.374, wps=14404.5, ups=1.72, wpb=8396.9, bsz=322.2, num_updates=23500, lr=9.22531e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=58, gb_free=15.6, wall=19291
2023-08-07 19:25:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 19:25:33 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 4.373 | trans_loss 5.663 | nll_loss 2.974 | w2v_ctc_loss 1.364 | contrastive_loss 0 | total 4003.4 | n_correct 2408.5 | ppl 7.86 | accuracy 60.161 | uer 16.755 | wer 18.519 | raw_wer 18.519 | bleu 19.02 | wps 2206.4 | wpb 4003.4 | bsz 141.8 | num_updates 23574 | best_bleu 19.24
2023-08-07 19:25:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 23574 updates
2023-08-07 19:25:33 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.0200.pt
2023-08-07 19:25:37 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.0200.pt
2023-08-07 19:25:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.0200.pt (epoch 16 @ 23574 updates, score 19.02) (writing took 19.294807704165578 seconds)
2023-08-07 19:25:53 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-08-07 19:25:53 | INFO | train | epoch 016 | loss 4.787 | trans_loss 5.427 | nll_loss 2.762 | w2v_ctc_loss 0.666 | contrastive_loss 0 | total 4138.65 | n_correct 2498.39 | ppl 6.78 | accuracy 60.367 | wps 13551.3 | ups 1.64 | wpb 8277.3 | bsz 305.7 | num_updates 23574 | lr 9.21082e-05 | gnorm 0.533 | clip 0 | loss_scale 64 | train_wall 842 | gb_free 15.5 | wall 19376
2023-08-07 19:25:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 19:25:54 | INFO | fairseq.trainer | begin training epoch 17
2023-08-07 19:25:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 19:26:18 | INFO | train_inner | epoch 017:     26 / 1474 loss=4.773, trans_loss=5.415, nll_loss=2.749, w2v_ctc_loss=0.659, contrastive_loss=0, total=4152.31, n_correct=2515.36, ppl=6.72, accuracy=60.577, wps=7544.1, ups=0.91, wpb=8283.1, bsz=304.6, num_updates=23600, lr=9.20575e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=57, gb_free=13.9, wall=19401
2023-08-07 19:27:15 | INFO | train_inner | epoch 017:    126 / 1474 loss=4.757, trans_loss=5.389, nll_loss=2.714, w2v_ctc_loss=0.664, contrastive_loss=0, total=4118.91, n_correct=2521.4, ppl=6.56, accuracy=61.215, wps=14418.3, ups=1.75, wpb=8228.1, bsz=295.8, num_updates=23700, lr=9.1863e-05, gnorm=0.533, clip=0, loss_scale=128, train_wall=57, gb_free=16.5, wall=19458
2023-08-07 19:28:12 | INFO | train_inner | epoch 017:    226 / 1474 loss=4.75, trans_loss=5.388, nll_loss=2.712, w2v_ctc_loss=0.647, contrastive_loss=0, total=4145.15, n_correct=2539.3, ppl=6.55, accuracy=61.26, wps=14448.7, ups=1.74, wpb=8282.7, bsz=315, num_updates=23800, lr=9.16698e-05, gnorm=0.533, clip=0, loss_scale=128, train_wall=57, gb_free=15.7, wall=19515
2023-08-07 19:28:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-07 19:29:10 | INFO | train_inner | epoch 017:    327 / 1474 loss=4.758, trans_loss=5.393, nll_loss=2.719, w2v_ctc_loss=0.661, contrastive_loss=0, total=4144.26, n_correct=2529, ppl=6.58, accuracy=61.024, wps=14313.6, ups=1.73, wpb=8273.8, bsz=297.3, num_updates=23900, lr=9.14779e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=57, gb_free=17.5, wall=19573
2023-08-07 19:30:07 | INFO | train_inner | epoch 017:    427 / 1474 loss=4.768, trans_loss=5.401, nll_loss=2.729, w2v_ctc_loss=0.656, contrastive_loss=0, total=4146.43, n_correct=2534.35, ppl=6.63, accuracy=61.121, wps=14514.6, ups=1.74, wpb=8320.5, bsz=308, num_updates=24000, lr=9.12871e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=57, gb_free=16.5, wall=19631
2023-08-07 19:30:07 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 19:30:30 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 4.353 | trans_loss 5.658 | nll_loss 2.973 | w2v_ctc_loss 1.307 | contrastive_loss 0 | total 4003.4 | n_correct 2413.1 | ppl 7.85 | accuracy 60.276 | uer 16.595 | wer 18.411 | raw_wer 18.411 | bleu 19.12 | wps 1974 | wpb 4003.4 | bsz 141.8 | num_updates 24000 | best_bleu 19.24
2023-08-07 19:30:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 24000 updates
2023-08-07 19:30:30 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_17_24000.pt
2023-08-07 19:30:34 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_17_24000.pt
2023-08-07 19:31:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_17_24000.pt (epoch 17 @ 24000 updates, score 19.12) (writing took 33.17505344375968 seconds)
2023-08-07 19:32:02 | INFO | train_inner | epoch 017:    527 / 1474 loss=4.76, trans_loss=5.396, nll_loss=2.723, w2v_ctc_loss=0.663, contrastive_loss=0, total=4182.1, n_correct=2546.48, ppl=6.6, accuracy=60.89, wps=7270.2, ups=0.87, wpb=8346.4, bsz=307.9, num_updates=24100, lr=9.10975e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=57, gb_free=16.8, wall=19745
2023-08-07 19:32:59 | INFO | train_inner | epoch 017:    627 / 1474 loss=4.765, trans_loss=5.402, nll_loss=2.731, w2v_ctc_loss=0.662, contrastive_loss=0, total=4167.27, n_correct=2537.45, ppl=6.64, accuracy=60.89, wps=14560.6, ups=1.75, wpb=8316.3, bsz=302.2, num_updates=24200, lr=9.09091e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=57, gb_free=11.1, wall=19802
2023-08-07 19:33:57 | INFO | train_inner | epoch 017:    727 / 1474 loss=4.772, trans_loss=5.405, nll_loss=2.736, w2v_ctc_loss=0.674, contrastive_loss=0, total=4166.12, n_correct=2533.63, ppl=6.66, accuracy=60.815, wps=14523.9, ups=1.74, wpb=8334.7, bsz=308.1, num_updates=24300, lr=9.07218e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=57, gb_free=16.3, wall=19860
2023-08-07 19:34:54 | INFO | train_inner | epoch 017:    827 / 1474 loss=4.767, trans_loss=5.402, nll_loss=2.731, w2v_ctc_loss=0.663, contrastive_loss=0, total=4091.64, n_correct=2491.84, ppl=6.64, accuracy=60.901, wps=14344, ups=1.75, wpb=8190.8, bsz=295.3, num_updates=24400, lr=9.05357e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=57, gb_free=17.3, wall=19917
2023-08-07 19:35:50 | INFO | train_inner | epoch 017:    927 / 1474 loss=4.768, trans_loss=5.404, nll_loss=2.735, w2v_ctc_loss=0.661, contrastive_loss=0, total=4106.83, n_correct=2498.41, ppl=6.66, accuracy=60.835, wps=14554.9, ups=1.77, wpb=8208.4, bsz=304.6, num_updates=24500, lr=9.03508e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=56, gb_free=15.8, wall=19973
2023-08-07 19:36:48 | INFO | train_inner | epoch 017:   1027 / 1474 loss=4.759, trans_loss=5.393, nll_loss=2.72, w2v_ctc_loss=0.664, contrastive_loss=0, total=4115.49, n_correct=2507.99, ppl=6.59, accuracy=60.94, wps=14339.4, ups=1.74, wpb=8229.1, bsz=305.8, num_updates=24600, lr=9.0167e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=57, gb_free=16.6, wall=20031
2023-08-07 19:37:45 | INFO | train_inner | epoch 017:   1127 / 1474 loss=4.758, trans_loss=5.396, nll_loss=2.724, w2v_ctc_loss=0.65, contrastive_loss=0, total=4078.39, n_correct=2486.86, ppl=6.61, accuracy=60.977, wps=14291.4, ups=1.75, wpb=8165.8, bsz=293.7, num_updates=24700, lr=8.99843e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=57, gb_free=15.6, wall=20088
2023-08-07 19:38:43 | INFO | train_inner | epoch 017:   1227 / 1474 loss=4.76, trans_loss=5.399, nll_loss=2.729, w2v_ctc_loss=0.66, contrastive_loss=0, total=4173.49, n_correct=2527.2, ppl=6.63, accuracy=60.554, wps=14433.6, ups=1.73, wpb=8354.6, bsz=323.7, num_updates=24800, lr=8.98027e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=57, gb_free=16.1, wall=20146
2023-08-07 19:39:40 | INFO | train_inner | epoch 017:   1327 / 1474 loss=4.762, trans_loss=5.403, nll_loss=2.733, w2v_ctc_loss=0.654, contrastive_loss=0, total=4156.28, n_correct=2527.07, ppl=6.65, accuracy=60.801, wps=14486.7, ups=1.75, wpb=8295.1, bsz=308, num_updates=24900, lr=8.96221e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=57, gb_free=17.8, wall=20203
2023-08-07 19:40:37 | INFO | train_inner | epoch 017:   1427 / 1474 loss=4.77, trans_loss=5.403, nll_loss=2.734, w2v_ctc_loss=0.657, contrastive_loss=0, total=4112.95, n_correct=2505.17, ppl=6.65, accuracy=60.909, wps=14375.9, ups=1.74, wpb=8265.7, bsz=303.2, num_updates=25000, lr=8.94427e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=57, gb_free=16.9, wall=20261
mt_weight tensor(0.0010, device='cuda:0')
asr_weight tensor(0.0053, device='cuda:0')
2023-08-07 19:41:02 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.0010, device='cuda:5')
asr_weight tensor(0.0053, device='cuda:5')
mt_weight tensor(0.0010, device='cuda:7')
asr_weight tensor(0.0053, device='cuda:7')
mt_weight tensor(0.0010, device='cuda:3')
asr_weight tensor(0.0053, device='cuda:3')
mt_weight tensor(0.0010, device='cuda:4')
asr_weight tensor(0.0053, device='cuda:4')
mt_weight tensor(0.0010, device='cuda:6')
asr_weight tensor(0.0053, device='cuda:6')
mt_weight tensor(0.0010, device='cuda:2')
asr_weight tensor(0.0053, device='cuda:2')
mt_weight tensor(0.0010, device='cuda:1')
asr_weight tensor(0.0053, device='cuda:1')
2023-08-07 19:41:26 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 4.346 | trans_loss 5.648 | nll_loss 2.975 | w2v_ctc_loss 1.306 | contrastive_loss 0 | total 4003.4 | n_correct 2414.3 | ppl 7.86 | accuracy 60.306 | uer 16.72 | wer 18.732 | raw_wer 18.732 | bleu 18.8 | wps 2092.1 | wpb 4003.4 | bsz 141.8 | num_updates 25047 | best_bleu 19.24
2023-08-07 19:41:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 25047 updates
2023-08-07 19:41:26 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_18.8004.pt
2023-08-07 19:41:29 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_18.8004.pt
2023-08-07 19:41:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_18.8004.pt (epoch 17 @ 25047 updates, score 18.8) (writing took 19.7201770786196 seconds)
2023-08-07 19:41:46 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-08-07 19:41:46 | INFO | train | epoch 017 | loss 4.755 | trans_loss 5.401 | nll_loss 2.73 | w2v_ctc_loss 0.671 | contrastive_loss 0 | total 4137.21 | n_correct 2521.28 | ppl 6.63 | accuracy 60.942 | wps 12591.1 | ups 1.55 | wpb 8143.8 | bsz 300.1 | num_updates 25047 | lr 8.93588e-05 | gnorm 0.551 | clip 0 | loss_scale 64 | train_wall 835 | gb_free 16.5 | wall 20329
2023-08-07 19:41:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 19:41:46 | INFO | fairseq.trainer | begin training epoch 18
2023-08-07 19:41:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 19:42:23 | INFO | train_inner | epoch 018:     53 / 1474 loss=4.302, trans_loss=5.553, nll_loss=2.929, w2v_ctc_loss=1.337, contrastive_loss=0, total=4139.04, n_correct=2520.8, ppl=7.61, accuracy=60.903, wps=3965.9, ups=0.95, wpb=4175.7, bsz=152.8, num_updates=25100, lr=8.92644e-05, gnorm=1.076, clip=0, loss_scale=64, train_wall=52, gb_free=17.2, wall=20366
2023-08-07 19:43:15 | INFO | train_inner | epoch 018:    153 / 1474 loss=4.245, trans_loss=5.512, nll_loss=2.875, w2v_ctc_loss=1.289, contrastive_loss=0, total=4154.85, n_correct=2560.27, ppl=7.34, accuracy=61.621, wps=7905.3, ups=1.9, wpb=4154.9, bsz=156.4, num_updates=25200, lr=8.90871e-05, gnorm=1.062, clip=0, loss_scale=64, train_wall=52, gb_free=16.8, wall=20418
2023-08-07 19:44:08 | INFO | train_inner | epoch 018:    253 / 1474 loss=4.249, trans_loss=5.509, nll_loss=2.871, w2v_ctc_loss=1.309, contrastive_loss=0, total=4162.72, n_correct=2569.98, ppl=7.32, accuracy=61.738, wps=7953.6, ups=1.91, wpb=4162.7, bsz=156.5, num_updates=25300, lr=8.89108e-05, gnorm=1.058, clip=0, loss_scale=64, train_wall=52, gb_free=16.3, wall=20471
2023-08-07 19:45:00 | INFO | train_inner | epoch 018:    353 / 1474 loss=4.248, trans_loss=5.515, nll_loss=2.878, w2v_ctc_loss=1.292, contrastive_loss=0, total=4161.22, n_correct=2564.22, ppl=7.35, accuracy=61.622, wps=7975.5, ups=1.92, wpb=4161.2, bsz=150.7, num_updates=25400, lr=8.87357e-05, gnorm=1.056, clip=0, loss_scale=64, train_wall=52, gb_free=14.6, wall=20523
2023-08-07 19:45:52 | INFO | train_inner | epoch 018:    453 / 1474 loss=4.266, trans_loss=5.534, nll_loss=2.903, w2v_ctc_loss=1.31, contrastive_loss=0, total=4092.36, n_correct=2509.92, ppl=7.48, accuracy=61.332, wps=7776.4, ups=1.9, wpb=4092.4, bsz=147.7, num_updates=25500, lr=8.85615e-05, gnorm=1.072, clip=0, loss_scale=64, train_wall=52, gb_free=16.8, wall=20576
2023-08-07 19:46:44 | INFO | train_inner | epoch 018:    553 / 1474 loss=4.244, trans_loss=5.509, nll_loss=2.872, w2v_ctc_loss=1.294, contrastive_loss=0, total=4206.45, n_correct=2595.7, ppl=7.32, accuracy=61.708, wps=8086.7, ups=1.92, wpb=4206.4, bsz=164.5, num_updates=25600, lr=8.83883e-05, gnorm=1.064, clip=0, loss_scale=64, train_wall=52, gb_free=17.8, wall=20628
2023-08-07 19:47:37 | INFO | train_inner | epoch 018:    653 / 1474 loss=4.277, trans_loss=5.546, nll_loss=2.92, w2v_ctc_loss=1.317, contrastive_loss=0, total=4097.96, n_correct=2505.03, ppl=7.57, accuracy=61.129, wps=7839.9, ups=1.91, wpb=4098, bsz=149.3, num_updates=25700, lr=8.82162e-05, gnorm=1.082, clip=0, loss_scale=64, train_wall=52, gb_free=12.5, wall=20680
2023-08-07 19:48:29 | INFO | train_inner | epoch 018:    753 / 1474 loss=4.283, trans_loss=5.547, nll_loss=2.922, w2v_ctc_loss=1.335, contrastive_loss=0, total=4208.5, n_correct=2570.91, ppl=7.58, accuracy=61.089, wps=8045.1, ups=1.91, wpb=4208.5, bsz=161.3, num_updates=25800, lr=8.80451e-05, gnorm=1.074, clip=0, loss_scale=64, train_wall=52, gb_free=16.3, wall=20732
2023-08-07 19:49:21 | INFO | train_inner | epoch 018:    853 / 1474 loss=4.262, trans_loss=5.533, nll_loss=2.903, w2v_ctc_loss=1.296, contrastive_loss=0, total=4166.07, n_correct=2554.67, ppl=7.48, accuracy=61.321, wps=8020.7, ups=1.93, wpb=4166.1, bsz=151.2, num_updates=25900, lr=8.7875e-05, gnorm=1.052, clip=0, loss_scale=128, train_wall=52, gb_free=16.3, wall=20784
2023-08-07 19:50:13 | INFO | train_inner | epoch 018:    953 / 1474 loss=4.252, trans_loss=5.524, nll_loss=2.892, w2v_ctc_loss=1.284, contrastive_loss=0, total=4141.27, n_correct=2544.65, ppl=7.42, accuracy=61.446, wps=8020.8, ups=1.94, wpb=4141.3, bsz=158, num_updates=26000, lr=8.77058e-05, gnorm=1.07, clip=0, loss_scale=128, train_wall=51, gb_free=16.2, wall=20836
2023-08-07 19:50:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 19:50:35 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 4.371 | trans_loss 5.642 | nll_loss 2.956 | w2v_ctc_loss 1.404 | contrastive_loss 0 | total 4003.4 | n_correct 2429.2 | ppl 7.76 | accuracy 60.678 | uer 16.789 | wer 18.672 | raw_wer 18.672 | bleu 19.51 | wps 2276.6 | wpb 4003.4 | bsz 141.8 | num_updates 26000 | best_bleu 19.51
2023-08-07 19:50:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26000 updates
2023-08-07 19:50:35 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_18_26000.pt
2023-08-07 19:50:38 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_18_26000.pt
2023-08-07 19:51:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_18_26000.pt (epoch 18 @ 26000 updates, score 19.51) (writing took 26.963271606713533 seconds)
2023-08-07 19:51:56 | INFO | train_inner | epoch 018:   1053 / 1474 loss=4.266, trans_loss=5.537, nll_loss=2.909, w2v_ctc_loss=1.299, contrastive_loss=0, total=4134.55, n_correct=2532.01, ppl=7.51, accuracy=61.24, wps=4004.6, ups=0.97, wpb=4134.6, bsz=150.4, num_updates=26100, lr=8.75376e-05, gnorm=1.07, clip=0, loss_scale=128, train_wall=53, gb_free=16.5, wall=20939
2023-08-07 19:52:48 | INFO | train_inner | epoch 018:   1153 / 1474 loss=4.265, trans_loss=5.528, nll_loss=2.899, w2v_ctc_loss=1.316, contrastive_loss=0, total=4157.63, n_correct=2554.97, ppl=7.46, accuracy=61.453, wps=7950.7, ups=1.91, wpb=4157.6, bsz=157, num_updates=26200, lr=8.73704e-05, gnorm=1.063, clip=0, loss_scale=128, train_wall=52, gb_free=17.2, wall=20991
2023-08-07 19:53:40 | INFO | train_inner | epoch 018:   1253 / 1474 loss=4.277, trans_loss=5.549, nll_loss=2.924, w2v_ctc_loss=1.307, contrastive_loss=0, total=4085.66, n_correct=2492.73, ppl=7.59, accuracy=61.012, wps=7861.7, ups=1.92, wpb=4085.7, bsz=143.3, num_updates=26300, lr=8.72041e-05, gnorm=1.069, clip=0, loss_scale=128, train_wall=52, gb_free=17.5, wall=21043
2023-08-07 19:54:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-07 19:54:32 | INFO | train_inner | epoch 018:   1354 / 1474 loss=4.293, trans_loss=5.56, nll_loss=2.939, w2v_ctc_loss=1.338, contrastive_loss=0, total=4065.2, n_correct=2473.67, ppl=7.67, accuracy=60.85, wps=7795, ups=1.92, wpb=4065.2, bsz=145.8, num_updates=26400, lr=8.70388e-05, gnorm=1.082, clip=0, loss_scale=64, train_wall=52, gb_free=17.5, wall=21095
2023-08-07 19:55:24 | INFO | train_inner | epoch 018:   1454 / 1474 loss=4.287, trans_loss=5.554, nll_loss=2.932, w2v_ctc_loss=1.332, contrastive_loss=0, total=4113.2, n_correct=2507.39, ppl=7.63, accuracy=60.96, wps=7907.8, ups=1.92, wpb=4113.2, bsz=148.8, num_updates=26500, lr=8.68744e-05, gnorm=1.08, clip=0, loss_scale=64, train_wall=52, gb_free=16.6, wall=21147
2023-08-07 19:55:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 19:55:57 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 4.347 | trans_loss 5.632 | nll_loss 2.948 | w2v_ctc_loss 1.35 | contrastive_loss 0 | total 4003.4 | n_correct 2430.3 | ppl 7.72 | accuracy 60.706 | uer 16.532 | wer 18.418 | raw_wer 18.418 | bleu 19.23 | wps 2325.1 | wpb 4003.4 | bsz 141.8 | num_updates 26520 | best_bleu 19.51
2023-08-07 19:55:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26520 updates
2023-08-07 19:55:57 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.2304.pt
2023-08-07 19:56:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.2304.pt
2023-08-07 19:56:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.2304.pt (epoch 18 @ 26520 updates, score 19.23) (writing took 19.178056474775076 seconds)
2023-08-07 19:56:16 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-08-07 19:56:16 | INFO | train | epoch 018 | loss 4.265 | trans_loss 5.532 | nll_loss 2.903 | w2v_ctc_loss 1.309 | contrastive_loss 0 | total 4138.34 | n_correct 2537.56 | ppl 7.48 | accuracy 61.318 | wps 7004.2 | ups 1.69 | wpb 4138.3 | bsz 152.8 | num_updates 26520 | lr 8.68417e-05 | gnorm 1.068 | clip 0 | loss_scale 64 | train_wall 763 | gb_free 16 | wall 21199
2023-08-07 19:56:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 19:56:17 | INFO | fairseq.trainer | begin training epoch 19
2023-08-07 19:56:17 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 19:57:07 | INFO | train_inner | epoch 019:     80 / 1474 loss=4.225, trans_loss=5.487, nll_loss=2.844, w2v_ctc_loss=1.281, contrastive_loss=0, total=4102.06, n_correct=2541.77, ppl=7.18, accuracy=61.963, wps=3981.7, ups=0.97, wpb=4102.1, bsz=148.5, num_updates=26600, lr=8.6711e-05, gnorm=1.069, clip=0, loss_scale=64, train_wall=52, gb_free=17.6, wall=21250
2023-08-07 19:58:00 | INFO | train_inner | epoch 019:    180 / 1474 loss=4.221, trans_loss=5.47, nll_loss=2.821, w2v_ctc_loss=1.307, contrastive_loss=0, total=4227.7, n_correct=2631.77, ppl=7.07, accuracy=62.251, wps=8041.2, ups=1.9, wpb=4227.7, bsz=162.4, num_updates=26700, lr=8.65485e-05, gnorm=1.047, clip=0, loss_scale=64, train_wall=52, gb_free=17.1, wall=21303
2023-08-07 19:58:52 | INFO | train_inner | epoch 019:    280 / 1474 loss=4.209, trans_loss=5.467, nll_loss=2.818, w2v_ctc_loss=1.275, contrastive_loss=0, total=4187.34, n_correct=2609.99, ppl=7.05, accuracy=62.331, wps=8041.4, ups=1.92, wpb=4187.3, bsz=153.2, num_updates=26800, lr=8.63868e-05, gnorm=1.058, clip=0, loss_scale=64, train_wall=52, gb_free=16, wall=21355
2023-08-07 19:59:44 | INFO | train_inner | epoch 019:    380 / 1474 loss=4.221, trans_loss=5.483, nll_loss=2.84, w2v_ctc_loss=1.275, contrastive_loss=0, total=4170.52, n_correct=2586, ppl=7.16, accuracy=62.007, wps=8039.6, ups=1.93, wpb=4170.5, bsz=155.5, num_updates=26900, lr=8.62261e-05, gnorm=1.062, clip=0, loss_scale=64, train_wall=51, gb_free=16.4, wall=21407
2023-08-07 20:00:36 | INFO | train_inner | epoch 019:    480 / 1474 loss=4.236, trans_loss=5.495, nll_loss=2.855, w2v_ctc_loss=1.297, contrastive_loss=0, total=4113.89, n_correct=2550.73, ppl=7.23, accuracy=62.003, wps=7943.1, ups=1.93, wpb=4113.9, bsz=150.8, num_updates=27000, lr=8.60663e-05, gnorm=1.071, clip=0, loss_scale=64, train_wall=51, gb_free=17.2, wall=21459
2023-08-07 20:01:28 | INFO | train_inner | epoch 019:    580 / 1474 loss=4.228, trans_loss=5.489, nll_loss=2.847, w2v_ctc_loss=1.286, contrastive_loss=0, total=4128.58, n_correct=2558.09, ppl=7.2, accuracy=61.961, wps=7849.7, ups=1.9, wpb=4128.6, bsz=153.1, num_updates=27100, lr=8.59074e-05, gnorm=1.067, clip=0, loss_scale=64, train_wall=52, gb_free=16.7, wall=21511
2023-08-07 20:02:20 | INFO | train_inner | epoch 019:    680 / 1474 loss=4.223, trans_loss=5.492, nll_loss=2.852, w2v_ctc_loss=1.263, contrastive_loss=0, total=4201.56, n_correct=2604.83, ppl=7.22, accuracy=61.997, wps=8112.9, ups=1.93, wpb=4201.6, bsz=160.7, num_updates=27200, lr=8.57493e-05, gnorm=1.061, clip=0, loss_scale=64, train_wall=51, gb_free=17.8, wall=21563
2023-08-07 20:03:12 | INFO | train_inner | epoch 019:    780 / 1474 loss=4.236, trans_loss=5.493, nll_loss=2.852, w2v_ctc_loss=1.302, contrastive_loss=0, total=4124.03, n_correct=2552.38, ppl=7.22, accuracy=61.89, wps=7927.1, ups=1.92, wpb=4124, bsz=149.5, num_updates=27300, lr=8.55921e-05, gnorm=1.069, clip=0, loss_scale=64, train_wall=52, gb_free=17.6, wall=21615
2023-08-07 20:04:04 | INFO | train_inner | epoch 019:    880 / 1474 loss=4.242, trans_loss=5.505, nll_loss=2.868, w2v_ctc_loss=1.294, contrastive_loss=0, total=4177.8, n_correct=2577.82, ppl=7.3, accuracy=61.703, wps=8045, ups=1.93, wpb=4177.8, bsz=154.8, num_updates=27400, lr=8.54358e-05, gnorm=1.063, clip=0, loss_scale=64, train_wall=51, gb_free=15, wall=21667
2023-08-07 20:04:57 | INFO | train_inner | epoch 019:    980 / 1474 loss=4.255, trans_loss=5.523, nll_loss=2.892, w2v_ctc_loss=1.296, contrastive_loss=0, total=4084.26, n_correct=2508.21, ppl=7.42, accuracy=61.412, wps=7759.8, ups=1.9, wpb=4084.3, bsz=152.9, num_updates=27500, lr=8.52803e-05, gnorm=1.079, clip=0, loss_scale=64, train_wall=52, gb_free=16.3, wall=21720
2023-08-07 20:05:49 | INFO | train_inner | epoch 019:   1080 / 1474 loss=4.248, trans_loss=5.517, nll_loss=2.884, w2v_ctc_loss=1.287, contrastive_loss=0, total=4042.73, n_correct=2488, ppl=7.38, accuracy=61.543, wps=7662.9, ups=1.9, wpb=4042.7, bsz=147, num_updates=27600, lr=8.51257e-05, gnorm=1.083, clip=0, loss_scale=64, train_wall=52, gb_free=17.3, wall=21773
2023-08-07 20:06:42 | INFO | train_inner | epoch 019:   1180 / 1474 loss=4.256, trans_loss=5.519, nll_loss=2.887, w2v_ctc_loss=1.309, contrastive_loss=0, total=4140.95, n_correct=2543.45, ppl=7.4, accuracy=61.422, wps=7938.4, ups=1.92, wpb=4140.9, bsz=154, num_updates=27700, lr=8.49719e-05, gnorm=1.07, clip=0, loss_scale=64, train_wall=52, gb_free=13, wall=21825
2023-08-07 20:07:33 | INFO | train_inner | epoch 019:   1280 / 1474 loss=4.251, trans_loss=5.52, nll_loss=2.889, w2v_ctc_loss=1.289, contrastive_loss=0, total=4135.79, n_correct=2542.61, ppl=7.41, accuracy=61.478, wps=8003.4, ups=1.94, wpb=4135.8, bsz=149.8, num_updates=27800, lr=8.48189e-05, gnorm=1.072, clip=0, loss_scale=64, train_wall=51, gb_free=17.9, wall=21876
2023-08-07 20:08:25 | INFO | train_inner | epoch 019:   1380 / 1474 loss=4.245, trans_loss=5.508, nll_loss=2.872, w2v_ctc_loss=1.297, contrastive_loss=0, total=4138.67, n_correct=2551.09, ppl=7.32, accuracy=61.64, wps=7940.6, ups=1.92, wpb=4138.7, bsz=150.8, num_updates=27900, lr=8.46668e-05, gnorm=1.065, clip=0, loss_scale=64, train_wall=52, gb_free=16.6, wall=21929
2023-08-07 20:09:14 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 20:09:37 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 4.352 | trans_loss 5.628 | nll_loss 2.937 | w2v_ctc_loss 1.375 | contrastive_loss 0 | total 4003.4 | n_correct 2429.3 | ppl 7.66 | accuracy 60.681 | uer 16.564 | wer 18.392 | raw_wer 18.392 | bleu 19.49 | wps 2213.3 | wpb 4003.4 | bsz 141.8 | num_updates 27994 | best_bleu 19.51
2023-08-07 20:09:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 27994 updates
2023-08-07 20:09:37 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.4909.pt
2023-08-07 20:09:41 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.4909.pt
2023-08-07 20:09:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.4909.pt (epoch 19 @ 27994 updates, score 19.49) (writing took 20.016695231199265 seconds)
2023-08-07 20:09:58 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-08-07 20:09:58 | INFO | train | epoch 019 | loss 4.235 | trans_loss 5.498 | nll_loss 2.859 | w2v_ctc_loss 1.29 | contrastive_loss 0 | total 4138.65 | n_correct 2559.06 | ppl 7.25 | accuracy 61.833 | wps 7426.1 | ups 1.79 | wpb 4138.6 | bsz 152.8 | num_updates 27994 | lr 8.45245e-05 | gnorm 1.068 | clip 0 | loss_scale 64 | train_wall 763 | gb_free 17.4 | wall 22021
2023-08-07 20:09:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 20:09:58 | INFO | fairseq.trainer | begin training epoch 20
2023-08-07 20:09:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 20:10:10 | INFO | train_inner | epoch 020:      6 / 1474 loss=4.242, trans_loss=5.506, nll_loss=2.871, w2v_ctc_loss=1.294, contrastive_loss=0, total=4117.61, n_correct=2543.06, ppl=7.31, accuracy=61.761, wps=3939, ups=0.96, wpb=4117.6, bsz=151.5, num_updates=28000, lr=8.45154e-05, gnorm=1.077, clip=0, loss_scale=64, train_wall=52, gb_free=16.5, wall=22033
2023-08-07 20:10:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 20:10:32 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 4.339 | trans_loss 5.631 | nll_loss 2.935 | w2v_ctc_loss 1.324 | contrastive_loss 0 | total 4003.4 | n_correct 2429.7 | ppl 7.65 | accuracy 60.691 | uer 16.495 | wer 18.366 | raw_wer 18.366 | bleu 19.44 | wps 2317.3 | wpb 4003.4 | bsz 141.8 | num_updates 28000 | best_bleu 19.51
2023-08-07 20:10:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 28000 updates
2023-08-07 20:10:32 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_20_28000.pt
2023-08-07 20:10:36 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_20_28000.pt
2023-08-07 20:11:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_20_28000.pt (epoch 20 @ 28000 updates, score 19.44) (writing took 37.17886891402304 seconds)
2023-08-07 20:12:05 | INFO | train_inner | epoch 020:    106 / 1474 loss=4.181, trans_loss=5.432, nll_loss=2.774, w2v_ctc_loss=1.263, contrastive_loss=0, total=4192.82, n_correct=2636.88, ppl=6.84, accuracy=62.89, wps=3640.4, ups=0.87, wpb=4192.8, bsz=156.4, num_updates=28100, lr=8.43649e-05, gnorm=1.054, clip=0, loss_scale=64, train_wall=52, gb_free=16.4, wall=22148
2023-08-07 20:12:57 | INFO | train_inner | epoch 020:    206 / 1474 loss=4.192, trans_loss=5.446, nll_loss=2.792, w2v_ctc_loss=1.264, contrastive_loss=0, total=4155.9, n_correct=2602.68, ppl=6.93, accuracy=62.626, wps=7956.3, ups=1.91, wpb=4155.9, bsz=151.1, num_updates=28200, lr=8.42152e-05, gnorm=1.058, clip=0, loss_scale=64, train_wall=52, gb_free=12.1, wall=22200
2023-08-07 20:13:49 | INFO | train_inner | epoch 020:    306 / 1474 loss=4.187, trans_loss=5.438, nll_loss=2.782, w2v_ctc_loss=1.27, contrastive_loss=0, total=4192.69, n_correct=2632.92, ppl=6.88, accuracy=62.798, wps=8091.1, ups=1.93, wpb=4192.7, bsz=163.8, num_updates=28300, lr=8.40663e-05, gnorm=1.06, clip=0, loss_scale=64, train_wall=51, gb_free=17.1, wall=22252
2023-08-07 20:14:41 | INFO | train_inner | epoch 020:    406 / 1474 loss=4.197, trans_loss=5.454, nll_loss=2.801, w2v_ctc_loss=1.263, contrastive_loss=0, total=4116.96, n_correct=2575.64, ppl=6.97, accuracy=62.562, wps=7897.1, ups=1.92, wpb=4117, bsz=148.4, num_updates=28400, lr=8.39181e-05, gnorm=1.065, clip=0, loss_scale=64, train_wall=52, gb_free=12.9, wall=22304
2023-08-07 20:15:34 | INFO | train_inner | epoch 020:    506 / 1474 loss=4.216, trans_loss=5.478, nll_loss=2.833, w2v_ctc_loss=1.273, contrastive_loss=0, total=4100.73, n_correct=2546.95, ppl=7.12, accuracy=62.11, wps=7787.3, ups=1.9, wpb=4100.7, bsz=149.2, num_updates=28500, lr=8.37708e-05, gnorm=1.079, clip=0, loss_scale=128, train_wall=52, gb_free=16.4, wall=22357
2023-08-07 20:16:26 | INFO | train_inner | epoch 020:    606 / 1474 loss=4.223, trans_loss=5.478, nll_loss=2.834, w2v_ctc_loss=1.295, contrastive_loss=0, total=4101.99, n_correct=2546.88, ppl=7.13, accuracy=62.089, wps=7914.3, ups=1.93, wpb=4102, bsz=149.2, num_updates=28600, lr=8.36242e-05, gnorm=1.085, clip=0, loss_scale=128, train_wall=51, gb_free=13.8, wall=22409
2023-08-07 20:17:18 | INFO | train_inner | epoch 020:    706 / 1474 loss=4.222, trans_loss=5.478, nll_loss=2.833, w2v_ctc_loss=1.29, contrastive_loss=0, total=4124.25, n_correct=2561.68, ppl=7.13, accuracy=62.113, wps=7946.4, ups=1.93, wpb=4124.2, bsz=148.6, num_updates=28700, lr=8.34784e-05, gnorm=1.079, clip=0, loss_scale=128, train_wall=51, gb_free=16.8, wall=22461
2023-08-07 20:18:09 | INFO | train_inner | epoch 020:    806 / 1474 loss=4.214, trans_loss=5.469, nll_loss=2.823, w2v_ctc_loss=1.288, contrastive_loss=0, total=4153.23, n_correct=2587.66, ppl=7.08, accuracy=62.305, wps=8016.3, ups=1.93, wpb=4153.2, bsz=154.3, num_updates=28800, lr=8.33333e-05, gnorm=1.066, clip=0, loss_scale=128, train_wall=51, gb_free=17.7, wall=22513
2023-08-07 20:18:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-07 20:19:03 | INFO | train_inner | epoch 020:    907 / 1474 loss=4.227, trans_loss=5.486, nll_loss=2.844, w2v_ctc_loss=1.291, contrastive_loss=0, total=4132.98, n_correct=2562.99, ppl=7.18, accuracy=62.013, wps=7735.1, ups=1.87, wpb=4133, bsz=155.7, num_updates=28900, lr=8.3189e-05, gnorm=1.077, clip=0, loss_scale=64, train_wall=53, gb_free=17.8, wall=22566
2023-08-07 20:19:56 | INFO | train_inner | epoch 020:   1007 / 1474 loss=4.208, trans_loss=5.471, nll_loss=2.825, w2v_ctc_loss=1.263, contrastive_loss=0, total=4171.86, n_correct=2598.62, ppl=7.09, accuracy=62.289, wps=7889.7, ups=1.89, wpb=4171.9, bsz=154.3, num_updates=29000, lr=8.30455e-05, gnorm=1.06, clip=0, loss_scale=64, train_wall=52, gb_free=16, wall=22619
2023-08-07 20:20:48 | INFO | train_inner | epoch 020:   1107 / 1474 loss=4.225, trans_loss=5.485, nll_loss=2.844, w2v_ctc_loss=1.283, contrastive_loss=0, total=4162.96, n_correct=2582.56, ppl=7.18, accuracy=62.037, wps=7990.7, ups=1.92, wpb=4163, bsz=157.5, num_updates=29100, lr=8.29027e-05, gnorm=1.071, clip=0, loss_scale=64, train_wall=52, gb_free=17.1, wall=22671
2023-08-07 20:21:40 | INFO | train_inner | epoch 020:   1207 / 1474 loss=4.222, trans_loss=5.475, nll_loss=2.83, w2v_ctc_loss=1.3, contrastive_loss=0, total=4033.74, n_correct=2504.56, ppl=7.11, accuracy=62.09, wps=7786.6, ups=1.93, wpb=4033.7, bsz=142.6, num_updates=29200, lr=8.27606e-05, gnorm=1.099, clip=0, loss_scale=64, train_wall=51, gb_free=17.3, wall=22723
2023-08-07 20:22:32 | INFO | train_inner | epoch 020:   1307 / 1474 loss=4.228, trans_loss=5.489, nll_loss=2.849, w2v_ctc_loss=1.286, contrastive_loss=0, total=4124.42, n_correct=2553.36, ppl=7.21, accuracy=61.908, wps=7922, ups=1.92, wpb=4124.4, bsz=148.5, num_updates=29300, lr=8.26192e-05, gnorm=1.078, clip=0, loss_scale=64, train_wall=52, gb_free=16.1, wall=22775
2023-08-07 20:23:24 | INFO | train_inner | epoch 020:   1407 / 1474 loss=4.23, trans_loss=5.488, nll_loss=2.848, w2v_ctc_loss=1.293, contrastive_loss=0, total=4114.1, n_correct=2547.45, ppl=7.2, accuracy=61.92, wps=7870.6, ups=1.91, wpb=4114.1, bsz=146.8, num_updates=29400, lr=8.24786e-05, gnorm=1.091, clip=0, loss_scale=64, train_wall=52, gb_free=14.6, wall=22827
2023-08-07 20:23:59 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 20:24:22 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 4.327 | trans_loss 5.615 | nll_loss 2.919 | w2v_ctc_loss 1.322 | contrastive_loss 0 | total 4003.4 | n_correct 2446 | ppl 7.57 | accuracy 61.098 | uer 16.569 | wer 18.437 | raw_wer 18.437 | bleu 19.53 | wps 2148.7 | wpb 4003.4 | bsz 141.8 | num_updates 29467 | best_bleu 19.53
2023-08-07 20:24:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 29467 updates
2023-08-07 20:24:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 20:24:37 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 20:24:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 20 @ 29467 updates, score 19.53) (writing took 26.5545126888901 seconds)
2023-08-07 20:24:50 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-08-07 20:24:50 | INFO | train | epoch 020 | loss 4.213 | trans_loss 5.469 | nll_loss 2.823 | w2v_ctc_loss 1.28 | contrastive_loss 0 | total 4137.15 | n_correct 2575.92 | ppl 7.08 | accuracy 62.263 | wps 6834.1 | ups 1.65 | wpb 4137.2 | bsz 152.5 | num_updates 29467 | lr 8.23848e-05 | gnorm 1.072 | clip 0 | loss_scale 64 | train_wall 763 | gb_free 16.6 | wall 22913
2023-08-07 20:24:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 20:24:50 | INFO | fairseq.trainer | begin training epoch 21
2023-08-07 20:24:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 20:25:16 | INFO | train_inner | epoch 021:     33 / 1474 loss=4.213, trans_loss=5.471, nll_loss=2.827, w2v_ctc_loss=1.277, contrastive_loss=0, total=4155.01, n_correct=2586.06, ppl=7.09, accuracy=62.24, wps=3720.2, ups=0.9, wpb=4155, bsz=158.8, num_updates=29500, lr=8.23387e-05, gnorm=1.064, clip=0, loss_scale=64, train_wall=52, gb_free=16.4, wall=22939
2023-08-07 20:26:08 | INFO | train_inner | epoch 021:    133 / 1474 loss=4.167, trans_loss=5.415, nll_loss=2.753, w2v_ctc_loss=1.255, contrastive_loss=0, total=4186.67, n_correct=2639.98, ppl=6.74, accuracy=63.057, wps=7990.1, ups=1.91, wpb=4186.7, bsz=158.7, num_updates=29600, lr=8.21995e-05, gnorm=1.065, clip=0, loss_scale=64, train_wall=52, gb_free=13.3, wall=22991
2023-08-07 20:27:00 | INFO | train_inner | epoch 021:    233 / 1474 loss=4.159, trans_loss=5.413, nll_loss=2.75, w2v_ctc_loss=1.234, contrastive_loss=0, total=4166.37, n_correct=2631.48, ppl=6.73, accuracy=63.16, wps=8062.7, ups=1.94, wpb=4166.4, bsz=157.6, num_updates=29700, lr=8.2061e-05, gnorm=1.057, clip=0, loss_scale=64, train_wall=51, gb_free=14.3, wall=23043
2023-08-07 20:27:52 | INFO | train_inner | epoch 021:    333 / 1474 loss=4.179, trans_loss=5.427, nll_loss=2.768, w2v_ctc_loss=1.267, contrastive_loss=0, total=4132.25, n_correct=2595.85, ppl=6.81, accuracy=62.819, wps=7932.5, ups=1.92, wpb=4132.2, bsz=152.5, num_updates=29800, lr=8.19232e-05, gnorm=1.073, clip=0, loss_scale=64, train_wall=52, gb_free=16.8, wall=23095
2023-08-07 20:28:44 | INFO | train_inner | epoch 021:    433 / 1474 loss=4.173, trans_loss=5.423, nll_loss=2.762, w2v_ctc_loss=1.256, contrastive_loss=0, total=4195.53, n_correct=2643.76, ppl=6.78, accuracy=63.014, wps=8050.5, ups=1.92, wpb=4195.5, bsz=155.8, num_updates=29900, lr=8.17861e-05, gnorm=1.068, clip=0, loss_scale=64, train_wall=52, gb_free=16.6, wall=23147
2023-08-07 20:29:36 | INFO | train_inner | epoch 021:    533 / 1474 loss=4.172, trans_loss=5.421, nll_loss=2.76, w2v_ctc_loss=1.256, contrastive_loss=0, total=4085.05, n_correct=2573.37, ppl=6.77, accuracy=62.995, wps=7831.7, ups=1.92, wpb=4085.1, bsz=148, num_updates=30000, lr=8.16497e-05, gnorm=1.07, clip=0, loss_scale=64, train_wall=52, gb_free=16.3, wall=23199
2023-08-07 20:29:36 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 20:30:00 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 4.335 | trans_loss 5.625 | nll_loss 2.932 | w2v_ctc_loss 1.325 | contrastive_loss 0 | total 4003.4 | n_correct 2444.9 | ppl 7.63 | accuracy 61.071 | uer 16.396 | wer 18.172 | raw_wer 18.172 | bleu 19.28 | wps 2164.7 | wpb 4003.4 | bsz 141.8 | num_updates 30000 | best_bleu 19.53
2023-08-07 20:30:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30000 updates
2023-08-07 20:30:00 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_21_30000.pt
2023-08-07 20:30:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_21_30000.pt
2023-08-07 20:30:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_21_30000.pt (epoch 21 @ 30000 updates, score 19.28) (writing took 21.69697448797524 seconds)
mt_weight tensor(0.0010, device='cuda:0')
asr_weight tensor(0.0053, device='cuda:0')
2023-08-07 20:31:15 | INFO | train_inner | epoch 021:    633 / 1474 loss=4.183, trans_loss=5.436, nll_loss=2.78, w2v_ctc_loss=1.259, contrastive_loss=0, total=4220.3, n_correct=2651.48, ppl=6.87, accuracy=62.827, wps=4288, ups=1.02, wpb=4220.3, bsz=157.9, num_updates=30100, lr=8.15139e-05, gnorm=1.072, clip=0, loss_scale=64, train_wall=52, gb_free=15.8, wall=23298
2023-08-07 20:32:07 | INFO | train_inner | epoch 021:    733 / 1474 loss=4.193, trans_loss=5.449, nll_loss=2.797, w2v_ctc_loss=1.261, contrastive_loss=0, total=4148.18, n_correct=2596.2, ppl=6.95, accuracy=62.586, wps=7914.2, ups=1.91, wpb=4148.2, bsz=154.2, num_updates=30200, lr=8.13788e-05, gnorm=1.071, clip=0, loss_scale=64, train_wall=52, gb_free=12.2, wall=23350
2023-08-07 20:32:59 | INFO | train_inner | epoch 021:    833 / 1474 loss=4.205, trans_loss=5.459, nll_loss=2.81, w2v_ctc_loss=1.279, contrastive_loss=0, total=4062.56, n_correct=2536.59, ppl=7.01, accuracy=62.438, wps=7781.9, ups=1.92, wpb=4062.6, bsz=146.5, num_updates=30300, lr=8.12444e-05, gnorm=1.088, clip=0, loss_scale=64, train_wall=52, gb_free=16.4, wall=23402
2023-08-07 20:33:52 | INFO | train_inner | epoch 021:    933 / 1474 loss=4.192, trans_loss=5.444, nll_loss=2.791, w2v_ctc_loss=1.269, contrastive_loss=0, total=4103.66, n_correct=2569.45, ppl=6.92, accuracy=62.614, wps=7814.6, ups=1.9, wpb=4103.7, bsz=150.7, num_updates=30400, lr=8.11107e-05, gnorm=1.075, clip=0, loss_scale=64, train_wall=52, gb_free=17.2, wall=23455
2023-08-07 20:34:44 | INFO | train_inner | epoch 021:   1033 / 1474 loss=4.21, trans_loss=5.465, nll_loss=2.818, w2v_ctc_loss=1.282, contrastive_loss=0, total=4100.54, n_correct=2556.68, ppl=7.05, accuracy=62.35, wps=7897.7, ups=1.93, wpb=4100.5, bsz=149.1, num_updates=30500, lr=8.09776e-05, gnorm=1.091, clip=0, loss_scale=64, train_wall=51, gb_free=17.9, wall=23507
2023-08-07 20:35:36 | INFO | train_inner | epoch 021:   1133 / 1474 loss=4.201, trans_loss=5.456, nll_loss=2.805, w2v_ctc_loss=1.275, contrastive_loss=0, total=4119.98, n_correct=2574.01, ppl=6.99, accuracy=62.476, wps=7939.4, ups=1.93, wpb=4120, bsz=147, num_updates=30600, lr=8.08452e-05, gnorm=1.08, clip=0, loss_scale=64, train_wall=51, gb_free=17.9, wall=23559
2023-08-07 20:36:27 | INFO | train_inner | epoch 021:   1233 / 1474 loss=4.187, trans_loss=5.443, nll_loss=2.791, w2v_ctc_loss=1.254, contrastive_loss=0, total=4161.49, n_correct=2607.03, ppl=6.92, accuracy=62.647, wps=8016.8, ups=1.93, wpb=4161.5, bsz=156.5, num_updates=30700, lr=8.07134e-05, gnorm=1.059, clip=0, loss_scale=64, train_wall=51, gb_free=16.9, wall=23611
2023-08-07 20:37:19 | INFO | train_inner | epoch 021:   1333 / 1474 loss=4.196, trans_loss=5.451, nll_loss=2.801, w2v_ctc_loss=1.268, contrastive_loss=0, total=4141.76, n_correct=2593.87, ppl=6.97, accuracy=62.627, wps=7974.6, ups=1.93, wpb=4141.8, bsz=155.8, num_updates=30800, lr=8.05823e-05, gnorm=1.079, clip=0, loss_scale=64, train_wall=51, gb_free=17.4, wall=23663
2023-08-07 20:38:12 | INFO | train_inner | epoch 021:   1433 / 1474 loss=4.227, trans_loss=5.475, nll_loss=2.832, w2v_ctc_loss=1.315, contrastive_loss=0, total=4127.02, n_correct=2565.35, ppl=7.12, accuracy=62.16, wps=7818.7, ups=1.89, wpb=4127, bsz=151.1, num_updates=30900, lr=8.04518e-05, gnorm=1.089, clip=0, loss_scale=64, train_wall=52, gb_free=16.6, wall=23715
2023-08-07 20:38:34 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.0010, device='cuda:3')
asr_weight tensor(0.0053, device='cuda:3')
mt_weight tensor(0.0010, device='cuda:4')
asr_weight tensor(0.0053, device='cuda:4')
mt_weight tensor(0.0010, device='cuda:6')
asr_weight tensor(0.0053, device='cuda:6')
mt_weight tensor(0.0010, device='cuda:5')
asr_weight tensor(0.0053, device='cuda:5')
mt_weight tensor(0.0010, device='cuda:7')
asr_weight tensor(0.0053, device='cuda:7')
mt_weight tensor(0.0010, device='cuda:1')
asr_weight tensor(0.0053, device='cuda:1')
mt_weight tensor(0.0010, device='cuda:2')
asr_weight tensor(0.0053, device='cuda:2')
2023-08-07 20:38:57 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 4.342 | trans_loss 5.617 | nll_loss 2.911 | w2v_ctc_loss 1.368 | contrastive_loss 0 | total 4003.4 | n_correct 2448.7 | ppl 7.52 | accuracy 61.166 | uer 16.439 | wer 18.385 | raw_wer 18.385 | bleu 19.61 | wps 2190.8 | wpb 4003.4 | bsz 141.8 | num_updates 30941 | best_bleu 19.61
2023-08-07 20:38:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30941 updates
2023-08-07 20:38:57 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 20:39:11 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 20:39:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 21 @ 30941 updates, score 19.61) (writing took 28.850526355206966 seconds)
2023-08-07 20:39:27 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-08-07 20:39:27 | INFO | train | epoch 021 | loss 4.189 | trans_loss 5.441 | nll_loss 2.787 | w2v_ctc_loss 1.266 | contrastive_loss 0 | total 4138.65 | n_correct 2594.59 | ppl 6.9 | accuracy 62.692 | wps 6956 | ups 1.68 | wpb 4138.6 | bsz 152.8 | num_updates 30941 | lr 8.03985e-05 | gnorm 1.074 | clip 0 | loss_scale 128 | train_wall 763 | gb_free 15.5 | wall 23790
2023-08-07 20:39:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 20:39:27 | INFO | fairseq.trainer | begin training epoch 22
2023-08-07 20:39:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 20:40:07 | INFO | train_inner | epoch 022:     59 / 1474 loss=4.166, trans_loss=5.41, nll_loss=2.747, w2v_ctc_loss=1.262, contrastive_loss=0, total=4140.16, n_correct=2619.39, ppl=6.71, accuracy=63.268, wps=3621.7, ups=0.87, wpb=4140.2, bsz=150.1, num_updates=31000, lr=8.03219e-05, gnorm=1.079, clip=0, loss_scale=128, train_wall=52, gb_free=17.8, wall=23830
2023-08-07 20:40:58 | INFO | train_inner | epoch 022:    159 / 1474 loss=4.154, trans_loss=5.396, nll_loss=2.729, w2v_ctc_loss=1.257, contrastive_loss=0, total=4115.86, n_correct=2607.65, ppl=6.63, accuracy=63.356, wps=7952.6, ups=1.93, wpb=4115.9, bsz=154.7, num_updates=31100, lr=8.01927e-05, gnorm=1.078, clip=0, loss_scale=128, train_wall=51, gb_free=17.4, wall=23881
2023-08-07 20:41:51 | INFO | train_inner | epoch 022:    259 / 1474 loss=4.133, trans_loss=5.381, nll_loss=2.709, w2v_ctc_loss=1.222, contrastive_loss=0, total=4247.73, n_correct=2700.37, ppl=6.54, accuracy=63.572, wps=8135, ups=1.92, wpb=4247.7, bsz=161.6, num_updates=31200, lr=8.00641e-05, gnorm=1.056, clip=0, loss_scale=128, train_wall=52, gb_free=14.1, wall=23934
2023-08-07 20:42:44 | INFO | train_inner | epoch 022:    359 / 1474 loss=4.166, trans_loss=5.415, nll_loss=2.753, w2v_ctc_loss=1.254, contrastive_loss=0, total=4212.22, n_correct=2660.78, ppl=6.74, accuracy=63.168, wps=7921.5, ups=1.88, wpb=4212.2, bsz=159, num_updates=31300, lr=7.99361e-05, gnorm=1.076, clip=0, loss_scale=128, train_wall=53, gb_free=15.7, wall=23987
2023-08-07 20:43:37 | INFO | train_inner | epoch 022:    459 / 1474 loss=4.173, trans_loss=5.42, nll_loss=2.758, w2v_ctc_loss=1.264, contrastive_loss=0, total=4131.12, n_correct=2606.45, ppl=6.76, accuracy=63.093, wps=7789.9, ups=1.89, wpb=4131.1, bsz=148.6, num_updates=31400, lr=7.98087e-05, gnorm=1.085, clip=0, loss_scale=128, train_wall=53, gb_free=16.6, wall=24040
2023-08-07 20:44:29 | INFO | train_inner | epoch 022:    559 / 1474 loss=4.167, trans_loss=5.412, nll_loss=2.75, w2v_ctc_loss=1.263, contrastive_loss=0, total=4153.54, n_correct=2621.3, ppl=6.72, accuracy=63.11, wps=7878.7, ups=1.9, wpb=4153.5, bsz=153.6, num_updates=31500, lr=7.96819e-05, gnorm=1.079, clip=0, loss_scale=128, train_wall=52, gb_free=15.1, wall=24093
2023-08-07 20:45:21 | INFO | train_inner | epoch 022:    659 / 1474 loss=4.145, trans_loss=5.397, nll_loss=2.731, w2v_ctc_loss=1.224, contrastive_loss=0, total=4143.91, n_correct=2624.94, ppl=6.64, accuracy=63.345, wps=7985.5, ups=1.93, wpb=4143.9, bsz=156.6, num_updates=31600, lr=7.95557e-05, gnorm=1.081, clip=0, loss_scale=128, train_wall=51, gb_free=15.9, wall=24145
2023-08-07 20:46:13 | INFO | train_inner | epoch 022:    759 / 1474 loss=4.17, trans_loss=5.415, nll_loss=2.752, w2v_ctc_loss=1.265, contrastive_loss=0, total=4168.91, n_correct=2626.48, ppl=6.74, accuracy=63.002, wps=8040.7, ups=1.93, wpb=4168.9, bsz=151.8, num_updates=31700, lr=7.94301e-05, gnorm=1.073, clip=0, loss_scale=128, train_wall=51, gb_free=17.6, wall=24196
2023-08-07 20:47:06 | INFO | train_inner | epoch 022:    859 / 1474 loss=4.181, trans_loss=5.428, nll_loss=2.77, w2v_ctc_loss=1.271, contrastive_loss=0, total=4079.59, n_correct=2562.09, ppl=6.82, accuracy=62.803, wps=7752.2, ups=1.9, wpb=4079.6, bsz=144.3, num_updates=31800, lr=7.93052e-05, gnorm=1.095, clip=0, loss_scale=128, train_wall=52, gb_free=17.4, wall=24249
2023-08-07 20:47:58 | INFO | train_inner | epoch 022:    959 / 1474 loss=4.167, trans_loss=5.419, nll_loss=2.759, w2v_ctc_loss=1.245, contrastive_loss=0, total=4129.75, n_correct=2600.87, ppl=6.77, accuracy=62.979, wps=7920.3, ups=1.92, wpb=4129.8, bsz=151.9, num_updates=31900, lr=7.91808e-05, gnorm=1.08, clip=0, loss_scale=128, train_wall=52, gb_free=17.8, wall=24301
2023-08-07 20:48:50 | INFO | train_inner | epoch 022:   1059 / 1474 loss=4.163, trans_loss=5.415, nll_loss=2.754, w2v_ctc_loss=1.241, contrastive_loss=0, total=4155.56, n_correct=2621.98, ppl=6.75, accuracy=63.096, wps=7932.7, ups=1.91, wpb=4155.6, bsz=157.7, num_updates=32000, lr=7.90569e-05, gnorm=1.071, clip=0, loss_scale=128, train_wall=52, gb_free=17.1, wall=24354
2023-08-07 20:48:50 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 20:49:14 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 4.331 | trans_loss 5.608 | nll_loss 2.911 | w2v_ctc_loss 1.352 | contrastive_loss 0 | total 4003.4 | n_correct 2449.2 | ppl 7.52 | accuracy 61.178 | uer 16.587 | wer 18.556 | raw_wer 18.556 | bleu 19.77 | wps 2126.5 | wpb 4003.4 | bsz 141.8 | num_updates 32000 | best_bleu 19.77
2023-08-07 20:49:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32000 updates
2023-08-07 20:49:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_22_32000.pt
2023-08-07 20:49:17 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_22_32000.pt
2023-08-07 20:50:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_22_32000.pt (epoch 22 @ 32000 updates, score 19.77) (writing took 50.72739477828145 seconds)
2023-08-07 20:50:57 | INFO | train_inner | epoch 022:   1159 / 1474 loss=4.194, trans_loss=5.445, nll_loss=2.792, w2v_ctc_loss=1.277, contrastive_loss=0, total=4089.92, n_correct=2564.81, ppl=6.93, accuracy=62.711, wps=3217.3, ups=0.79, wpb=4089.9, bsz=146.1, num_updates=32100, lr=7.89337e-05, gnorm=1.094, clip=0, loss_scale=128, train_wall=51, gb_free=16.3, wall=24481
2023-08-07 20:51:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-07 20:51:50 | INFO | train_inner | epoch 022:   1260 / 1474 loss=4.184, trans_loss=5.433, nll_loss=2.778, w2v_ctc_loss=1.272, contrastive_loss=0, total=4188.36, n_correct=2631.84, ppl=6.86, accuracy=62.837, wps=7952.7, ups=1.9, wpb=4188.4, bsz=162.7, num_updates=32200, lr=7.8811e-05, gnorm=1.073, clip=0, loss_scale=64, train_wall=52, gb_free=16.6, wall=24533
2023-08-07 20:52:42 | INFO | train_inner | epoch 022:   1360 / 1474 loss=4.167, trans_loss=5.422, nll_loss=2.763, w2v_ctc_loss=1.24, contrastive_loss=0, total=4071.58, n_correct=2565.57, ppl=6.79, accuracy=63.012, wps=7908.6, ups=1.94, wpb=4071.6, bsz=150.3, num_updates=32300, lr=7.86889e-05, gnorm=1.081, clip=0, loss_scale=64, train_wall=51, gb_free=16.8, wall=24585
2023-08-07 20:53:34 | INFO | train_inner | epoch 022:   1460 / 1474 loss=4.194, trans_loss=5.443, nll_loss=2.789, w2v_ctc_loss=1.278, contrastive_loss=0, total=4077.83, n_correct=2553.79, ppl=6.91, accuracy=62.626, wps=7824.2, ups=1.92, wpb=4077.8, bsz=144, num_updates=32400, lr=7.85674e-05, gnorm=1.092, clip=0, loss_scale=64, train_wall=52, gb_free=16.3, wall=24637
2023-08-07 20:53:41 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 20:54:03 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 4.318 | trans_loss 5.61 | nll_loss 2.918 | w2v_ctc_loss 1.305 | contrastive_loss 0 | total 4003.4 | n_correct 2451.7 | ppl 7.56 | accuracy 61.24 | uer 16.447 | wer 18.329 | raw_wer 18.329 | bleu 19.74 | wps 2336.3 | wpb 4003.4 | bsz 141.8 | num_updates 32414 | best_bleu 19.77
2023-08-07 20:54:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32414 updates
2023-08-07 20:54:03 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.7402.pt
2023-08-07 20:54:07 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.7402.pt
2023-08-07 20:54:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.7402.pt (epoch 22 @ 32414 updates, score 19.74) (writing took 36.96148612909019 seconds)
2023-08-07 20:54:40 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-08-07 20:54:40 | INFO | train | epoch 022 | loss 4.168 | trans_loss 5.416 | nll_loss 2.754 | w2v_ctc_loss 1.256 | contrastive_loss 0 | total 4138.86 | n_correct 2610.62 | ppl 6.75 | accuracy 63.076 | wps 6671.6 | ups 1.61 | wpb 4138.9 | bsz 152.8 | num_updates 32414 | lr 7.85505e-05 | gnorm 1.08 | clip 0 | loss_scale 64 | train_wall 763 | gb_free 12 | wall 24704
2023-08-07 20:54:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 20:54:41 | INFO | fairseq.trainer | begin training epoch 23
2023-08-07 20:54:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 20:55:34 | INFO | train_inner | epoch 023:     86 / 1474 loss=4.14, trans_loss=5.376, nll_loss=2.702, w2v_ctc_loss=1.256, contrastive_loss=0, total=4089.8, n_correct=2604.18, ppl=6.51, accuracy=63.675, wps=3413.3, ups=0.83, wpb=4089.8, bsz=149.7, num_updates=32500, lr=7.84465e-05, gnorm=1.095, clip=0, loss_scale=64, train_wall=51, gb_free=16.7, wall=24757
2023-08-07 20:56:26 | INFO | train_inner | epoch 023:    186 / 1474 loss=4.12, trans_loss=5.362, nll_loss=2.683, w2v_ctc_loss=1.223, contrastive_loss=0, total=4117.76, n_correct=2633.09, ppl=6.42, accuracy=63.945, wps=7893.2, ups=1.92, wpb=4117.8, bsz=148, num_updates=32600, lr=7.8326e-05, gnorm=1.068, clip=0, loss_scale=64, train_wall=52, gb_free=15.6, wall=24809
2023-08-07 20:57:18 | INFO | train_inner | epoch 023:    286 / 1474 loss=4.142, trans_loss=5.388, nll_loss=2.718, w2v_ctc_loss=1.235, contrastive_loss=0, total=4144.73, n_correct=2631.73, ppl=6.58, accuracy=63.496, wps=7893.7, ups=1.9, wpb=4144.7, bsz=152, num_updates=32700, lr=7.82062e-05, gnorm=1.094, clip=0, loss_scale=64, train_wall=52, gb_free=17.6, wall=24861
2023-08-07 20:58:11 | INFO | train_inner | epoch 023:    386 / 1474 loss=4.128, trans_loss=5.373, nll_loss=2.699, w2v_ctc_loss=1.223, contrastive_loss=0, total=4126.79, n_correct=2632.5, ppl=6.49, accuracy=63.791, wps=7883.6, ups=1.91, wpb=4126.8, bsz=148.2, num_updates=32800, lr=7.80869e-05, gnorm=1.075, clip=0, loss_scale=64, train_wall=52, gb_free=17.2, wall=24914
2023-08-07 20:59:03 | INFO | train_inner | epoch 023:    486 / 1474 loss=4.14, trans_loss=5.385, nll_loss=2.715, w2v_ctc_loss=1.233, contrastive_loss=0, total=4150.15, n_correct=2640.62, ppl=6.57, accuracy=63.627, wps=7941.9, ups=1.91, wpb=4150.1, bsz=156, num_updates=32900, lr=7.79681e-05, gnorm=1.076, clip=0, loss_scale=64, train_wall=52, gb_free=16.3, wall=24966
2023-08-07 20:59:55 | INFO | train_inner | epoch 023:    586 / 1474 loss=4.133, trans_loss=5.372, nll_loss=2.698, w2v_ctc_loss=1.243, contrastive_loss=0, total=4174.6, n_correct=2663.5, ppl=6.49, accuracy=63.803, wps=8063, ups=1.93, wpb=4174.6, bsz=158.2, num_updates=33000, lr=7.78499e-05, gnorm=1.081, clip=0, loss_scale=64, train_wall=51, gb_free=16.6, wall=25018
2023-08-07 21:00:46 | INFO | train_inner | epoch 023:    686 / 1474 loss=4.15, trans_loss=5.392, nll_loss=2.723, w2v_ctc_loss=1.251, contrastive_loss=0, total=4136.6, n_correct=2626.11, ppl=6.6, accuracy=63.485, wps=7989.6, ups=1.93, wpb=4136.6, bsz=150.6, num_updates=33100, lr=7.77322e-05, gnorm=1.092, clip=0, loss_scale=64, train_wall=51, gb_free=17.8, wall=25070
2023-08-07 21:01:39 | INFO | train_inner | epoch 023:    786 / 1474 loss=4.155, trans_loss=5.397, nll_loss=2.73, w2v_ctc_loss=1.257, contrastive_loss=0, total=4147.22, n_correct=2627.85, ppl=6.64, accuracy=63.364, wps=7926.3, ups=1.91, wpb=4147.2, bsz=152.6, num_updates=33200, lr=7.76151e-05, gnorm=1.081, clip=0, loss_scale=64, train_wall=52, gb_free=17.4, wall=25122
2023-08-07 21:02:31 | INFO | train_inner | epoch 023:    886 / 1474 loss=4.147, trans_loss=5.388, nll_loss=2.72, w2v_ctc_loss=1.252, contrastive_loss=0, total=4193.16, n_correct=2663.81, ppl=6.59, accuracy=63.528, wps=8009.9, ups=1.91, wpb=4193.2, bsz=163.6, num_updates=33300, lr=7.74984e-05, gnorm=1.079, clip=0, loss_scale=64, train_wall=52, gb_free=16.1, wall=25174
2023-08-07 21:03:23 | INFO | train_inner | epoch 023:    986 / 1474 loss=4.156, trans_loss=5.408, nll_loss=2.745, w2v_ctc_loss=1.236, contrastive_loss=0, total=4164.33, n_correct=2629.23, ppl=6.7, accuracy=63.137, wps=7960.9, ups=1.91, wpb=4164.3, bsz=155.1, num_updates=33400, lr=7.73823e-05, gnorm=1.096, clip=0, loss_scale=64, train_wall=52, gb_free=17.7, wall=25227
2023-08-07 21:04:16 | INFO | train_inner | epoch 023:   1086 / 1474 loss=4.163, trans_loss=5.404, nll_loss=2.739, w2v_ctc_loss=1.267, contrastive_loss=0, total=4088.37, n_correct=2584.62, ppl=6.68, accuracy=63.219, wps=7822.6, ups=1.91, wpb=4088.4, bsz=144.8, num_updates=33500, lr=7.72667e-05, gnorm=1.096, clip=0, loss_scale=64, train_wall=52, gb_free=15.7, wall=25279
2023-08-07 21:05:08 | INFO | train_inner | epoch 023:   1186 / 1474 loss=4.159, trans_loss=5.403, nll_loss=2.739, w2v_ctc_loss=1.255, contrastive_loss=0, total=4162.3, n_correct=2631.48, ppl=6.68, accuracy=63.222, wps=7955, ups=1.91, wpb=4162.3, bsz=154.5, num_updates=33600, lr=7.71517e-05, gnorm=1.09, clip=0, loss_scale=64, train_wall=52, gb_free=15.8, wall=25331
2023-08-07 21:06:00 | INFO | train_inner | epoch 023:   1286 / 1474 loss=4.148, trans_loss=5.396, nll_loss=2.729, w2v_ctc_loss=1.237, contrastive_loss=0, total=4131.74, n_correct=2616.31, ppl=6.63, accuracy=63.322, wps=7953.3, ups=1.92, wpb=4131.7, bsz=154.4, num_updates=33700, lr=7.70371e-05, gnorm=1.084, clip=0, loss_scale=64, train_wall=51, gb_free=17.2, wall=25383
2023-08-07 21:06:53 | INFO | train_inner | epoch 023:   1386 / 1474 loss=4.18, trans_loss=5.43, nll_loss=2.773, w2v_ctc_loss=1.265, contrastive_loss=0, total=4141.25, n_correct=2602.68, ppl=6.83, accuracy=62.848, wps=7795.1, ups=1.88, wpb=4141.2, bsz=152.4, num_updates=33800, lr=7.69231e-05, gnorm=1.082, clip=0, loss_scale=64, train_wall=53, gb_free=16.9, wall=25436
2023-08-07 21:07:39 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 21:08:01 | INFO | dev_st | epoch 023 | valid on 'dev_st' subset | loss 4.327 | trans_loss 5.605 | nll_loss 2.904 | w2v_ctc_loss 1.343 | contrastive_loss 0 | total 4003.4 | n_correct 2459.9 | ppl 7.48 | accuracy 61.445 | uer 16.351 | wer 18.143 | raw_wer 18.143 | bleu 19.96 | wps 2310.5 | wpb 4003.4 | bsz 141.8 | num_updates 33888 | best_bleu 19.96
2023-08-07 21:08:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 33888 updates
2023-08-07 21:08:01 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 21:08:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 21:08:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 23 @ 33888 updates, score 19.96) (writing took 27.927931934595108 seconds)
2023-08-07 21:08:30 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-08-07 21:08:30 | INFO | train | epoch 023 | loss 4.149 | trans_loss 5.393 | nll_loss 2.725 | w2v_ctc_loss 1.245 | contrastive_loss 0 | total 4138.65 | n_correct 2625.34 | ppl 6.61 | accuracy 63.435 | wps 7356.7 | ups 1.78 | wpb 4138.6 | bsz 152.8 | num_updates 33888 | lr 7.68231e-05 | gnorm 1.086 | clip 0 | loss_scale 64 | train_wall 763 | gb_free 13.9 | wall 25533
2023-08-07 21:08:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 21:08:30 | INFO | fairseq.trainer | begin training epoch 24
2023-08-07 21:08:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 21:08:44 | INFO | train_inner | epoch 024:     12 / 1474 loss=4.169, trans_loss=5.419, nll_loss=2.76, w2v_ctc_loss=1.253, contrastive_loss=0, total=4095.53, n_correct=2581.72, ppl=6.78, accuracy=63.038, wps=3680.3, ups=0.9, wpb=4095.5, bsz=153.1, num_updates=33900, lr=7.68095e-05, gnorm=1.108, clip=0, loss_scale=64, train_wall=51, gb_free=17.4, wall=25548
2023-08-07 21:09:36 | INFO | train_inner | epoch 024:    112 / 1474 loss=4.1, trans_loss=5.336, nll_loss=2.651, w2v_ctc_loss=1.215, contrastive_loss=0, total=4167.42, n_correct=2680.41, ppl=6.28, accuracy=64.318, wps=7998.8, ups=1.92, wpb=4167.4, bsz=161.5, num_updates=34000, lr=7.66965e-05, gnorm=1.075, clip=0, loss_scale=64, train_wall=52, gb_free=17.8, wall=25600
2023-08-07 21:09:36 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 21:09:59 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 4.326 | trans_loss 5.615 | nll_loss 2.911 | w2v_ctc_loss 1.317 | contrastive_loss 0 | total 4003.4 | n_correct 2445.4 | ppl 7.52 | accuracy 61.083 | uer 16.309 | wer 18.221 | raw_wer 18.221 | bleu 19.43 | wps 2364.9 | wpb 4003.4 | bsz 141.8 | num_updates 34000 | best_bleu 19.96
2023-08-07 21:09:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 34000 updates
2023-08-07 21:09:59 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_24_34000.pt
2023-08-07 21:10:02 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_24_34000.pt
2023-08-07 21:10:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_24_34000.pt (epoch 24 @ 34000 updates, score 19.43) (writing took 18.123503047972918 seconds)
2023-08-07 21:11:10 | INFO | train_inner | epoch 024:    212 / 1474 loss=4.109, trans_loss=5.35, nll_loss=2.67, w2v_ctc_loss=1.212, contrastive_loss=0, total=4247.08, n_correct=2725.68, ppl=6.36, accuracy=64.178, wps=4528.4, ups=1.07, wpb=4247.1, bsz=169.8, num_updates=34100, lr=7.6584e-05, gnorm=1.063, clip=0, loss_scale=64, train_wall=52, gb_free=16.7, wall=25693
2023-08-07 21:12:03 | INFO | train_inner | epoch 024:    312 / 1474 loss=4.104, trans_loss=5.342, nll_loss=2.658, w2v_ctc_loss=1.217, contrastive_loss=0, total=4139.31, n_correct=2657.5, ppl=6.31, accuracy=64.202, wps=7913.3, ups=1.91, wpb=4139.3, bsz=154.2, num_updates=34200, lr=7.64719e-05, gnorm=1.084, clip=0, loss_scale=128, train_wall=52, gb_free=16.5, wall=25746
2023-08-07 21:12:55 | INFO | train_inner | epoch 024:    412 / 1474 loss=4.129, trans_loss=5.367, nll_loss=2.691, w2v_ctc_loss=1.241, contrastive_loss=0, total=4157.07, n_correct=2655.14, ppl=6.46, accuracy=63.87, wps=7870.3, ups=1.89, wpb=4157.1, bsz=150, num_updates=34300, lr=7.63604e-05, gnorm=1.094, clip=0, loss_scale=128, train_wall=52, gb_free=16.3, wall=25799
2023-08-07 21:13:47 | INFO | train_inner | epoch 024:    512 / 1474 loss=4.123, trans_loss=5.359, nll_loss=2.681, w2v_ctc_loss=1.239, contrastive_loss=0, total=4138.82, n_correct=2646.21, ppl=6.41, accuracy=63.936, wps=7972.1, ups=1.93, wpb=4138.8, bsz=150.8, num_updates=34400, lr=7.62493e-05, gnorm=1.091, clip=0, loss_scale=128, train_wall=51, gb_free=17, wall=25850
2023-08-07 21:14:40 | INFO | train_inner | epoch 024:    612 / 1474 loss=4.11, trans_loss=5.352, nll_loss=2.672, w2v_ctc_loss=1.211, contrastive_loss=0, total=4164.75, n_correct=2668.59, ppl=6.37, accuracy=64.076, wps=7947.7, ups=1.91, wpb=4164.8, bsz=154.5, num_updates=34500, lr=7.61387e-05, gnorm=1.082, clip=0, loss_scale=128, train_wall=52, gb_free=15.7, wall=25903
2023-08-07 21:15:32 | INFO | train_inner | epoch 024:    712 / 1474 loss=4.138, trans_loss=5.38, nll_loss=2.709, w2v_ctc_loss=1.24, contrastive_loss=0, total=4103.92, n_correct=2609.83, ppl=6.54, accuracy=63.594, wps=7896.5, ups=1.92, wpb=4103.9, bsz=147, num_updates=34600, lr=7.60286e-05, gnorm=1.089, clip=0, loss_scale=128, train_wall=52, gb_free=12.4, wall=25955
2023-08-07 21:16:25 | INFO | train_inner | epoch 024:    812 / 1474 loss=4.136, trans_loss=5.377, nll_loss=2.705, w2v_ctc_loss=1.241, contrastive_loss=0, total=4109.8, n_correct=2614.49, ppl=6.52, accuracy=63.616, wps=7770.8, ups=1.89, wpb=4109.8, bsz=152.7, num_updates=34700, lr=7.5919e-05, gnorm=1.09, clip=0, loss_scale=128, train_wall=52, gb_free=17.3, wall=26008
2023-08-07 21:17:17 | INFO | train_inner | epoch 024:    912 / 1474 loss=4.146, trans_loss=5.385, nll_loss=2.714, w2v_ctc_loss=1.255, contrastive_loss=0, total=4042.08, n_correct=2566.21, ppl=6.56, accuracy=63.487, wps=7786.6, ups=1.93, wpb=4042.1, bsz=140.3, num_updates=34800, lr=7.58098e-05, gnorm=1.104, clip=0, loss_scale=128, train_wall=51, gb_free=16.8, wall=26060
2023-08-07 21:18:08 | INFO | train_inner | epoch 024:   1012 / 1474 loss=4.136, trans_loss=5.381, nll_loss=2.71, w2v_ctc_loss=1.231, contrastive_loss=0, total=4140.44, n_correct=2637.54, ppl=6.54, accuracy=63.702, wps=7971.6, ups=1.93, wpb=4140.4, bsz=149.3, num_updates=34900, lr=7.57011e-05, gnorm=1.093, clip=0, loss_scale=128, train_wall=51, gb_free=12.1, wall=26112
2023-08-07 21:19:00 | INFO | train_inner | epoch 024:   1112 / 1474 loss=4.123, trans_loss=5.363, nll_loss=2.687, w2v_ctc_loss=1.23, contrastive_loss=0, total=4134.93, n_correct=2640.06, ppl=6.44, accuracy=63.848, wps=7957.7, ups=1.92, wpb=4134.9, bsz=154.7, num_updates=35000, lr=7.55929e-05, gnorm=1.086, clip=0, loss_scale=128, train_wall=52, gb_free=16.5, wall=26164
mt_weight tensor(0.0010, device='cuda:0')
asr_weight tensor(0.0053, device='cuda:0')
2023-08-07 21:19:53 | INFO | train_inner | epoch 024:   1212 / 1474 loss=4.132, trans_loss=5.378, nll_loss=2.707, w2v_ctc_loss=1.225, contrastive_loss=0, total=4144.49, n_correct=2641.44, ppl=6.53, accuracy=63.734, wps=7955.6, ups=1.92, wpb=4144.5, bsz=154.9, num_updates=35100, lr=7.54851e-05, gnorm=1.089, clip=0, loss_scale=128, train_wall=52, gb_free=16.6, wall=26216
2023-08-07 21:20:45 | INFO | train_inner | epoch 024:   1312 / 1474 loss=4.15, trans_loss=5.389, nll_loss=2.72, w2v_ctc_loss=1.259, contrastive_loss=0, total=4110.93, n_correct=2610.15, ppl=6.59, accuracy=63.493, wps=7789.7, ups=1.89, wpb=4110.9, bsz=146.5, num_updates=35200, lr=7.53778e-05, gnorm=1.11, clip=0, loss_scale=128, train_wall=52, gb_free=13.9, wall=26268
2023-08-07 21:21:37 | INFO | train_inner | epoch 024:   1412 / 1474 loss=4.152, trans_loss=5.391, nll_loss=2.722, w2v_ctc_loss=1.26, contrastive_loss=0, total=4088.73, n_correct=2592.3, ppl=6.6, accuracy=63.401, wps=7894.7, ups=1.93, wpb=4088.7, bsz=147.1, num_updates=35300, lr=7.5271e-05, gnorm=1.106, clip=0, loss_scale=128, train_wall=51, gb_free=17.8, wall=26320
2023-08-07 21:22:09 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.0010, device='cuda:6')
asr_weight tensor(0.0053, device='cuda:6')
mt_weight tensor(0.0010, device='cuda:4')
asr_weight tensor(0.0053, device='cuda:4')
mt_weight tensor(0.0010, device='cuda:7')
asr_weight tensor(0.0053, device='cuda:7')
mt_weight tensor(0.0010, device='cuda:5')
asr_weight tensor(0.0053, device='cuda:5')
mt_weight tensor(0.0010, device='cuda:3')
asr_weight tensor(0.0053, device='cuda:3')
mt_weight tensor(0.0010, device='cuda:1')
asr_weight tensor(0.0053, device='cuda:1')
mt_weight tensor(0.0010, device='cuda:2')
asr_weight tensor(0.0053, device='cuda:2')
2023-08-07 21:22:32 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 4.331 | trans_loss 5.598 | nll_loss 2.896 | w2v_ctc_loss 1.377 | contrastive_loss 0 | total 4003.4 | n_correct 2461.2 | ppl 7.44 | accuracy 61.478 | uer 16.266 | wer 17.971 | raw_wer 17.971 | bleu 19.98 | wps 2309.5 | wpb 4003.4 | bsz 141.8 | num_updates 35362 | best_bleu 19.98
2023-08-07 21:22:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 35362 updates
2023-08-07 21:22:32 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 21:22:45 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 21:22:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 24 @ 35362 updates, score 19.98) (writing took 26.183644592761993 seconds)
2023-08-07 21:22:58 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-08-07 21:22:58 | INFO | train | epoch 024 | loss 4.128 | trans_loss 5.368 | nll_loss 2.693 | w2v_ctc_loss 1.233 | contrastive_loss 0 | total 4138.65 | n_correct 2641.32 | ppl 6.47 | accuracy 63.821 | wps 7023 | ups 1.7 | wpb 4138.6 | bsz 152.8 | num_updates 35362 | lr 7.5205e-05 | gnorm 1.089 | clip 0 | loss_scale 128 | train_wall 763 | gb_free 16.2 | wall 26401
2023-08-07 21:22:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 21:22:58 | INFO | fairseq.trainer | begin training epoch 25
2023-08-07 21:22:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 21:23:26 | INFO | train_inner | epoch 025:     38 / 1474 loss=4.111, trans_loss=5.349, nll_loss=2.67, w2v_ctc_loss=1.222, contrastive_loss=0, total=4170.36, n_correct=2675.06, ppl=6.36, accuracy=64.145, wps=3829.1, ups=0.92, wpb=4170.4, bsz=156.5, num_updates=35400, lr=7.51646e-05, gnorm=1.084, clip=0, loss_scale=128, train_wall=51, gb_free=16.3, wall=26429
2023-08-07 21:23:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-07 21:24:19 | INFO | train_inner | epoch 025:    139 / 1474 loss=4.078, trans_loss=5.31, nll_loss=2.619, w2v_ctc_loss=1.203, contrastive_loss=0, total=4131.92, n_correct=2677.17, ppl=6.14, accuracy=64.792, wps=7842.9, ups=1.9, wpb=4131.9, bsz=153.9, num_updates=35500, lr=7.50587e-05, gnorm=1.072, clip=0, loss_scale=64, train_wall=52, gb_free=15.9, wall=26482
2023-08-07 21:25:11 | INFO | train_inner | epoch 025:    239 / 1474 loss=4.101, trans_loss=5.33, nll_loss=2.644, w2v_ctc_loss=1.232, contrastive_loss=0, total=4114.53, n_correct=2649.32, ppl=6.25, accuracy=64.389, wps=7905.9, ups=1.92, wpb=4114.5, bsz=151.4, num_updates=35600, lr=7.49532e-05, gnorm=1.095, clip=0, loss_scale=64, train_wall=52, gb_free=17.1, wall=26534
2023-08-07 21:26:04 | INFO | train_inner | epoch 025:    339 / 1474 loss=4.1, trans_loss=5.334, nll_loss=2.648, w2v_ctc_loss=1.22, contrastive_loss=0, total=4148.7, n_correct=2665.53, ppl=6.27, accuracy=64.25, wps=7823, ups=1.89, wpb=4148.7, bsz=147.6, num_updates=35700, lr=7.48481e-05, gnorm=1.085, clip=0, loss_scale=64, train_wall=53, gb_free=16.6, wall=26587
2023-08-07 21:26:56 | INFO | train_inner | epoch 025:    439 / 1474 loss=4.111, trans_loss=5.343, nll_loss=2.66, w2v_ctc_loss=1.237, contrastive_loss=0, total=4167.03, n_correct=2677.53, ppl=6.32, accuracy=64.255, wps=7996.7, ups=1.92, wpb=4167, bsz=149.2, num_updates=35800, lr=7.47435e-05, gnorm=1.09, clip=0, loss_scale=64, train_wall=52, gb_free=12.6, wall=26639
2023-08-07 21:27:48 | INFO | train_inner | epoch 025:    539 / 1474 loss=4.114, trans_loss=5.349, nll_loss=2.669, w2v_ctc_loss=1.233, contrastive_loss=0, total=4156.93, n_correct=2665.45, ppl=6.36, accuracy=64.121, wps=8014.3, ups=1.93, wpb=4156.9, bsz=156.4, num_updates=35900, lr=7.46393e-05, gnorm=1.083, clip=0, loss_scale=64, train_wall=51, gb_free=17.1, wall=26691
2023-08-07 21:28:40 | INFO | train_inner | epoch 025:    639 / 1474 loss=4.106, trans_loss=5.339, nll_loss=2.656, w2v_ctc_loss=1.227, contrastive_loss=0, total=4153.23, n_correct=2667.71, ppl=6.3, accuracy=64.232, wps=7949.7, ups=1.91, wpb=4153.2, bsz=154.8, num_updates=36000, lr=7.45356e-05, gnorm=1.091, clip=0, loss_scale=64, train_wall=52, gb_free=15, wall=26743
2023-08-07 21:28:40 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 21:29:02 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 4.334 | trans_loss 5.608 | nll_loss 2.913 | w2v_ctc_loss 1.362 | contrastive_loss 0 | total 4003.4 | n_correct 2453.5 | ppl 7.53 | accuracy 61.285 | uer 16.128 | wer 17.878 | raw_wer 17.878 | bleu 19.67 | wps 2383.1 | wpb 4003.4 | bsz 141.8 | num_updates 36000 | best_bleu 19.98
2023-08-07 21:29:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36000 updates
2023-08-07 21:29:02 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_25_36000.pt
2023-08-07 21:29:06 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_25_36000.pt
2023-08-07 21:29:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_25_36000.pt (epoch 25 @ 36000 updates, score 19.67) (writing took 24.76639256812632 seconds)
2023-08-07 21:30:20 | INFO | train_inner | epoch 025:    739 / 1474 loss=4.106, trans_loss=5.344, nll_loss=2.661, w2v_ctc_loss=1.218, contrastive_loss=0, total=4123.21, n_correct=2646.58, ppl=6.32, accuracy=64.187, wps=4138.7, ups=1, wpb=4123.2, bsz=150.4, num_updates=36100, lr=7.44323e-05, gnorm=1.093, clip=0, loss_scale=64, train_wall=52, gb_free=14.9, wall=26843
2023-08-07 21:31:12 | INFO | train_inner | epoch 025:    839 / 1474 loss=4.107, trans_loss=5.345, nll_loss=2.664, w2v_ctc_loss=1.219, contrastive_loss=0, total=4197.27, n_correct=2693.77, ppl=6.34, accuracy=64.179, wps=8069.1, ups=1.92, wpb=4197.3, bsz=164.1, num_updates=36200, lr=7.43294e-05, gnorm=1.093, clip=0, loss_scale=64, train_wall=52, gb_free=16.3, wall=26895
2023-08-07 21:32:04 | INFO | train_inner | epoch 025:    939 / 1474 loss=4.107, trans_loss=5.345, nll_loss=2.664, w2v_ctc_loss=1.218, contrastive_loss=0, total=4137.23, n_correct=2653.48, ppl=6.34, accuracy=64.137, wps=7956.5, ups=1.92, wpb=4137.2, bsz=156.7, num_updates=36300, lr=7.4227e-05, gnorm=1.085, clip=0, loss_scale=64, train_wall=52, gb_free=16.7, wall=26947
2023-08-07 21:32:56 | INFO | train_inner | epoch 025:   1039 / 1474 loss=4.122, trans_loss=5.366, nll_loss=2.692, w2v_ctc_loss=1.221, contrastive_loss=0, total=4183.45, n_correct=2669.69, ppl=6.46, accuracy=63.816, wps=8017.3, ups=1.92, wpb=4183.4, bsz=155.5, num_updates=36400, lr=7.41249e-05, gnorm=1.091, clip=0, loss_scale=64, train_wall=52, gb_free=16.8, wall=26999
2023-08-07 21:33:48 | INFO | train_inner | epoch 025:   1139 / 1474 loss=4.116, trans_loss=5.357, nll_loss=2.678, w2v_ctc_loss=1.22, contrastive_loss=0, total=4045.24, n_correct=2583.7, ppl=6.4, accuracy=63.87, wps=7784.5, ups=1.92, wpb=4045.2, bsz=143.5, num_updates=36500, lr=7.40233e-05, gnorm=1.11, clip=0, loss_scale=64, train_wall=51, gb_free=16.5, wall=27051
2023-08-07 21:34:40 | INFO | train_inner | epoch 025:   1239 / 1474 loss=4.117, trans_loss=5.361, nll_loss=2.684, w2v_ctc_loss=1.213, contrastive_loss=0, total=4079.17, n_correct=2602.83, ppl=6.43, accuracy=63.808, wps=7835.5, ups=1.92, wpb=4079.2, bsz=146.2, num_updates=36600, lr=7.39221e-05, gnorm=1.1, clip=0, loss_scale=64, train_wall=52, gb_free=17.1, wall=27103
2023-08-07 21:35:32 | INFO | train_inner | epoch 025:   1339 / 1474 loss=4.114, trans_loss=5.353, nll_loss=2.675, w2v_ctc_loss=1.222, contrastive_loss=0, total=4173.55, n_correct=2674.48, ppl=6.38, accuracy=64.082, wps=8049.2, ups=1.93, wpb=4173.6, bsz=156.3, num_updates=36700, lr=7.38213e-05, gnorm=1.076, clip=0, loss_scale=64, train_wall=51, gb_free=16.8, wall=27155
2023-08-07 21:36:24 | INFO | train_inner | epoch 025:   1439 / 1474 loss=4.136, trans_loss=5.379, nll_loss=2.708, w2v_ctc_loss=1.236, contrastive_loss=0, total=4102.27, n_correct=2607.06, ppl=6.54, accuracy=63.552, wps=7850.7, ups=1.91, wpb=4102.3, bsz=149.9, num_updates=36800, lr=7.3721e-05, gnorm=1.103, clip=0, loss_scale=64, train_wall=52, gb_free=16.1, wall=27207
2023-08-07 21:36:42 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 21:37:04 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 4.329 | trans_loss 5.592 | nll_loss 2.894 | w2v_ctc_loss 1.381 | contrastive_loss 0 | total 4003.4 | n_correct 2462.2 | ppl 7.43 | accuracy 61.503 | uer 16.455 | wer 18.277 | raw_wer 18.277 | bleu 20.04 | wps 2368.5 | wpb 4003.4 | bsz 141.8 | num_updates 36835 | best_bleu 20.04
2023-08-07 21:37:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36835 updates
2023-08-07 21:37:04 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 21:37:18 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 21:37:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 25 @ 36835 updates, score 20.04) (writing took 26.028340300545096 seconds)
2023-08-07 21:37:30 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-08-07 21:37:30 | INFO | train | epoch 025 | loss 4.109 | trans_loss 5.346 | nll_loss 2.665 | w2v_ctc_loss 1.222 | contrastive_loss 0 | total 4138.51 | n_correct 2654.06 | ppl 6.34 | accuracy 64.131 | wps 6990.3 | ups 1.69 | wpb 4138.5 | bsz 152.9 | num_updates 36835 | lr 7.36859e-05 | gnorm 1.09 | clip 0 | loss_scale 64 | train_wall 761 | gb_free 14.5 | wall 27274
2023-08-07 21:37:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 21:37:31 | INFO | fairseq.trainer | begin training epoch 26
2023-08-07 21:37:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 21:38:13 | INFO | train_inner | epoch 026:     65 / 1474 loss=4.078, trans_loss=5.312, nll_loss=2.622, w2v_ctc_loss=1.199, contrastive_loss=0, total=4178.19, n_correct=2704.65, ppl=6.15, accuracy=64.733, wps=3823, ups=0.91, wpb=4178.2, bsz=158.7, num_updates=36900, lr=7.3621e-05, gnorm=1.077, clip=0, loss_scale=64, train_wall=52, gb_free=14.5, wall=27316
2023-08-07 21:39:05 | INFO | train_inner | epoch 026:    165 / 1474 loss=4.06, trans_loss=5.297, nll_loss=2.601, w2v_ctc_loss=1.175, contrastive_loss=0, total=4269.55, n_correct=2776.01, ppl=6.07, accuracy=65.019, wps=8192, ups=1.92, wpb=4269.6, bsz=170.7, num_updates=37000, lr=7.35215e-05, gnorm=1.068, clip=0, loss_scale=64, train_wall=52, gb_free=15.5, wall=27369
2023-08-07 21:39:58 | INFO | train_inner | epoch 026:    265 / 1474 loss=4.08, trans_loss=5.307, nll_loss=2.614, w2v_ctc_loss=1.217, contrastive_loss=0, total=4128.39, n_correct=2673.16, ppl=6.12, accuracy=64.751, wps=7856.7, ups=1.9, wpb=4128.4, bsz=153.4, num_updates=37100, lr=7.34223e-05, gnorm=1.095, clip=0, loss_scale=64, train_wall=52, gb_free=10.5, wall=27421
2023-08-07 21:40:50 | INFO | train_inner | epoch 026:    365 / 1474 loss=4.086, trans_loss=5.313, nll_loss=2.621, w2v_ctc_loss=1.223, contrastive_loss=0, total=4166.22, n_correct=2694.58, ppl=6.15, accuracy=64.677, wps=8037.9, ups=1.93, wpb=4166.2, bsz=157.5, num_updates=37200, lr=7.33236e-05, gnorm=1.092, clip=0, loss_scale=64, train_wall=51, gb_free=16.9, wall=27473
2023-08-07 21:41:42 | INFO | train_inner | epoch 026:    465 / 1474 loss=4.07, trans_loss=5.301, nll_loss=2.606, w2v_ctc_loss=1.2, contrastive_loss=0, total=4171.18, n_correct=2710.98, ppl=6.09, accuracy=64.993, wps=8052.6, ups=1.93, wpb=4171.2, bsz=157.7, num_updates=37300, lr=7.32252e-05, gnorm=1.077, clip=0, loss_scale=64, train_wall=51, gb_free=16.8, wall=27525
2023-08-07 21:42:34 | INFO | train_inner | epoch 026:    565 / 1474 loss=4.096, trans_loss=5.323, nll_loss=2.635, w2v_ctc_loss=1.231, contrastive_loss=0, total=4139.82, n_correct=2671.19, ppl=6.21, accuracy=64.524, wps=7949.1, ups=1.92, wpb=4139.8, bsz=150, num_updates=37400, lr=7.31272e-05, gnorm=1.09, clip=0, loss_scale=64, train_wall=52, gb_free=15.3, wall=27577
2023-08-07 21:43:26 | INFO | train_inner | epoch 026:    665 / 1474 loss=4.083, trans_loss=5.32, nll_loss=2.631, w2v_ctc_loss=1.196, contrastive_loss=0, total=4146.72, n_correct=2675.91, ppl=6.19, accuracy=64.531, wps=7944.9, ups=1.92, wpb=4146.7, bsz=151.4, num_updates=37500, lr=7.30297e-05, gnorm=1.093, clip=0, loss_scale=128, train_wall=52, gb_free=17, wall=27629
2023-08-07 21:43:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-07 21:44:19 | INFO | train_inner | epoch 026:    766 / 1474 loss=4.092, trans_loss=5.327, nll_loss=2.64, w2v_ctc_loss=1.211, contrastive_loss=0, total=4066.26, n_correct=2619.52, ppl=6.23, accuracy=64.421, wps=7698.1, ups=1.89, wpb=4066.3, bsz=145.3, num_updates=37600, lr=7.29325e-05, gnorm=1.108, clip=0, loss_scale=64, train_wall=52, gb_free=15.2, wall=27682
2023-08-07 21:45:10 | INFO | train_inner | epoch 026:    866 / 1474 loss=4.097, trans_loss=5.328, nll_loss=2.641, w2v_ctc_loss=1.224, contrastive_loss=0, total=4183.26, n_correct=2696.84, ppl=6.24, accuracy=64.467, wps=8085.1, ups=1.93, wpb=4183.3, bsz=154.1, num_updates=37700, lr=7.28357e-05, gnorm=1.088, clip=0, loss_scale=64, train_wall=51, gb_free=17.4, wall=27734
2023-08-07 21:46:03 | INFO | train_inner | epoch 026:    966 / 1474 loss=4.097, trans_loss=5.34, nll_loss=2.657, w2v_ctc_loss=1.195, contrastive_loss=0, total=4137.96, n_correct=2656.07, ppl=6.31, accuracy=64.188, wps=7922.2, ups=1.91, wpb=4138, bsz=149.5, num_updates=37800, lr=7.27393e-05, gnorm=1.09, clip=0, loss_scale=64, train_wall=52, gb_free=17, wall=27786
2023-08-07 21:46:55 | INFO | train_inner | epoch 026:   1066 / 1474 loss=4.092, trans_loss=5.33, nll_loss=2.644, w2v_ctc_loss=1.205, contrastive_loss=0, total=4120.53, n_correct=2656.07, ppl=6.25, accuracy=64.459, wps=7889.5, ups=1.91, wpb=4120.5, bsz=147.1, num_updates=37900, lr=7.26433e-05, gnorm=1.099, clip=0, loss_scale=64, train_wall=52, gb_free=16.7, wall=27838
2023-08-07 21:47:47 | INFO | train_inner | epoch 026:   1166 / 1474 loss=4.105, trans_loss=5.343, nll_loss=2.66, w2v_ctc_loss=1.218, contrastive_loss=0, total=4113.86, n_correct=2641.72, ppl=6.32, accuracy=64.215, wps=7929, ups=1.93, wpb=4113.9, bsz=149.2, num_updates=38000, lr=7.25476e-05, gnorm=1.109, clip=0, loss_scale=64, train_wall=51, gb_free=16.7, wall=27890
2023-08-07 21:47:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 21:48:09 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 4.334 | trans_loss 5.601 | nll_loss 2.898 | w2v_ctc_loss 1.376 | contrastive_loss 0 | total 4003.4 | n_correct 2457.4 | ppl 7.45 | accuracy 61.383 | uer 16.285 | wer 18.034 | raw_wer 18.034 | bleu 19.72 | wps 2429.6 | wpb 4003.4 | bsz 141.8 | num_updates 38000 | best_bleu 20.04
2023-08-07 21:48:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38000 updates
2023-08-07 21:48:09 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_26_38000.pt
2023-08-07 21:48:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_26_38000.pt
2023-08-07 21:48:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_26_38000.pt (epoch 26 @ 38000 updates, score 19.72) (writing took 38.22978391870856 seconds)
2023-08-07 21:49:40 | INFO | train_inner | epoch 026:   1266 / 1474 loss=4.126, trans_loss=5.359, nll_loss=2.681, w2v_ctc_loss=1.248, contrastive_loss=0, total=3996.19, n_correct=2555.7, ppl=6.41, accuracy=63.953, wps=3532.4, ups=0.88, wpb=3996.2, bsz=139.6, num_updates=38100, lr=7.24524e-05, gnorm=1.119, clip=0, loss_scale=64, train_wall=52, gb_free=17.8, wall=28003
2023-08-07 21:50:32 | INFO | train_inner | epoch 026:   1366 / 1474 loss=4.104, trans_loss=5.345, nll_loss=2.665, w2v_ctc_loss=1.207, contrastive_loss=0, total=4159.74, n_correct=2671.45, ppl=6.34, accuracy=64.222, wps=7967.5, ups=1.92, wpb=4159.7, bsz=155.7, num_updates=38200, lr=7.23575e-05, gnorm=1.096, clip=0, loss_scale=64, train_wall=52, gb_free=17.3, wall=28055
2023-08-07 21:51:24 | INFO | train_inner | epoch 026:   1466 / 1474 loss=4.088, trans_loss=5.333, nll_loss=2.649, w2v_ctc_loss=1.183, contrastive_loss=0, total=4165.66, n_correct=2682.6, ppl=6.27, accuracy=64.398, wps=8035.7, ups=1.93, wpb=4165.7, bsz=158.7, num_updates=38300, lr=7.22629e-05, gnorm=1.089, clip=0, loss_scale=64, train_wall=51, gb_free=16.6, wall=28107
2023-08-07 21:51:28 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 21:51:50 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 4.33 | trans_loss 5.599 | nll_loss 2.889 | w2v_ctc_loss 1.369 | contrastive_loss 0 | total 4003.4 | n_correct 2465.1 | ppl 7.41 | accuracy 61.575 | uer 16.221 | wer 17.956 | raw_wer 17.956 | bleu 19.89 | wps 2424 | wpb 4003.4 | bsz 141.8 | num_updates 38308 | best_bleu 20.04
2023-08-07 21:51:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38308 updates
2023-08-07 21:51:50 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.8904.pt
2023-08-07 21:51:53 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.8904.pt
2023-08-07 21:52:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.8904.pt (epoch 26 @ 38308 updates, score 19.89) (writing took 37.02176888100803 seconds)
2023-08-07 21:52:27 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-08-07 21:52:27 | INFO | train | epoch 026 | loss 4.09 | trans_loss 5.324 | nll_loss 2.636 | w2v_ctc_loss 1.209 | contrastive_loss 0 | total 4137.32 | n_correct 2669.4 | ppl 6.22 | accuracy 64.52 | wps 6795.6 | ups 1.64 | wpb 4137.3 | bsz 152.6 | num_updates 38308 | lr 7.22554e-05 | gnorm 1.093 | clip 0 | loss_scale 64 | train_wall 761 | gb_free 16.2 | wall 28170
2023-08-07 21:52:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 21:52:27 | INFO | fairseq.trainer | begin training epoch 27
2023-08-07 21:52:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 21:53:23 | INFO | train_inner | epoch 027:     92 / 1474 loss=4.036, trans_loss=5.264, nll_loss=2.557, w2v_ctc_loss=1.171, contrastive_loss=0, total=4054.57, n_correct=2653.94, ppl=5.88, accuracy=65.456, wps=3402.7, ups=0.84, wpb=4054.6, bsz=141.2, num_updates=38400, lr=7.21688e-05, gnorm=1.111, clip=0, loss_scale=64, train_wall=51, gb_free=16.4, wall=28226
2023-08-07 21:54:16 | INFO | train_inner | epoch 027:    192 / 1474 loss=4.046, trans_loss=5.269, nll_loss=2.565, w2v_ctc_loss=1.192, contrastive_loss=0, total=4195.2, n_correct=2742.04, ppl=5.92, accuracy=65.361, wps=8006.1, ups=1.91, wpb=4195.2, bsz=161.6, num_updates=38500, lr=7.2075e-05, gnorm=1.09, clip=0, loss_scale=64, train_wall=52, gb_free=17.2, wall=28279
2023-08-07 21:55:08 | INFO | train_inner | epoch 027:    292 / 1474 loss=4.066, trans_loss=5.291, nll_loss=2.593, w2v_ctc_loss=1.205, contrastive_loss=0, total=4162.23, n_correct=2709.4, ppl=6.03, accuracy=65.095, wps=7998.9, ups=1.92, wpb=4162.2, bsz=152.7, num_updates=38600, lr=7.19816e-05, gnorm=1.091, clip=0, loss_scale=64, train_wall=52, gb_free=17.3, wall=28331
2023-08-07 21:56:00 | INFO | train_inner | epoch 027:    392 / 1474 loss=4.065, trans_loss=5.297, nll_loss=2.601, w2v_ctc_loss=1.189, contrastive_loss=0, total=4079.05, n_correct=2651.14, ppl=6.07, accuracy=64.994, wps=7782.4, ups=1.91, wpb=4079.1, bsz=148.6, num_updates=38700, lr=7.18885e-05, gnorm=1.104, clip=0, loss_scale=64, train_wall=52, gb_free=17, wall=28383
2023-08-07 21:56:53 | INFO | train_inner | epoch 027:    492 / 1474 loss=4.074, trans_loss=5.304, nll_loss=2.611, w2v_ctc_loss=1.205, contrastive_loss=0, total=4243.25, n_correct=2749.59, ppl=6.11, accuracy=64.799, wps=8047.7, ups=1.9, wpb=4243.2, bsz=165.5, num_updates=38800, lr=7.17958e-05, gnorm=1.085, clip=0, loss_scale=64, train_wall=52, gb_free=15.8, wall=28436
2023-08-07 21:57:45 | INFO | train_inner | epoch 027:    592 / 1474 loss=4.065, trans_loss=5.293, nll_loss=2.596, w2v_ctc_loss=1.201, contrastive_loss=0, total=4137.92, n_correct=2690.51, ppl=6.05, accuracy=65.021, wps=7980, ups=1.93, wpb=4137.9, bsz=156.7, num_updates=38900, lr=7.17035e-05, gnorm=1.096, clip=0, loss_scale=64, train_wall=51, gb_free=16, wall=28488
2023-08-07 21:58:37 | INFO | train_inner | epoch 027:    692 / 1474 loss=4.081, trans_loss=5.311, nll_loss=2.619, w2v_ctc_loss=1.212, contrastive_loss=0, total=4158.48, n_correct=2691.96, ppl=6.14, accuracy=64.734, wps=7964.6, ups=1.92, wpb=4158.5, bsz=152, num_updates=39000, lr=7.16115e-05, gnorm=1.104, clip=0, loss_scale=64, train_wall=52, gb_free=15.8, wall=28540
2023-08-07 21:59:28 | INFO | train_inner | epoch 027:    792 / 1474 loss=4.078, trans_loss=5.307, nll_loss=2.614, w2v_ctc_loss=1.21, contrastive_loss=0, total=4100.88, n_correct=2656.22, ppl=6.12, accuracy=64.772, wps=7942.9, ups=1.94, wpb=4100.9, bsz=146.1, num_updates=39100, lr=7.15199e-05, gnorm=1.104, clip=0, loss_scale=64, train_wall=51, gb_free=15.8, wall=28592
2023-08-07 22:00:20 | INFO | train_inner | epoch 027:    892 / 1474 loss=4.079, trans_loss=5.317, nll_loss=2.627, w2v_ctc_loss=1.19, contrastive_loss=0, total=4111.94, n_correct=2657.3, ppl=6.18, accuracy=64.624, wps=7967.1, ups=1.94, wpb=4111.9, bsz=147.4, num_updates=39200, lr=7.14286e-05, gnorm=1.112, clip=0, loss_scale=64, train_wall=51, gb_free=16.4, wall=28643
2023-08-07 22:01:13 | INFO | train_inner | epoch 027:    992 / 1474 loss=4.083, trans_loss=5.316, nll_loss=2.627, w2v_ctc_loss=1.205, contrastive_loss=0, total=4189.27, n_correct=2708.24, ppl=6.18, accuracy=64.647, wps=7975, ups=1.9, wpb=4189.3, bsz=157.5, num_updates=39300, lr=7.13376e-05, gnorm=1.095, clip=0, loss_scale=64, train_wall=52, gb_free=14.5, wall=28696
2023-08-07 22:02:05 | INFO | train_inner | epoch 027:   1092 / 1474 loss=4.072, trans_loss=5.307, nll_loss=2.614, w2v_ctc_loss=1.191, contrastive_loss=0, total=4160.42, n_correct=2691.55, ppl=6.12, accuracy=64.694, wps=8008.7, ups=1.92, wpb=4160.4, bsz=153.7, num_updates=39400, lr=7.1247e-05, gnorm=1.09, clip=0, loss_scale=64, train_wall=52, gb_free=17.1, wall=28748
2023-08-07 22:02:57 | INFO | train_inner | epoch 027:   1192 / 1474 loss=4.084, trans_loss=5.317, nll_loss=2.627, w2v_ctc_loss=1.208, contrastive_loss=0, total=4103.72, n_correct=2651.03, ppl=6.18, accuracy=64.601, wps=7787.3, ups=1.9, wpb=4103.7, bsz=148.6, num_updates=39500, lr=7.11568e-05, gnorm=1.117, clip=0, loss_scale=64, train_wall=52, gb_free=17.7, wall=28800
2023-08-07 22:03:49 | INFO | train_inner | epoch 027:   1292 / 1474 loss=4.093, trans_loss=5.328, nll_loss=2.642, w2v_ctc_loss=1.212, contrastive_loss=0, total=4065.94, n_correct=2617.16, ppl=6.24, accuracy=64.368, wps=7878.4, ups=1.94, wpb=4065.9, bsz=146.2, num_updates=39600, lr=7.10669e-05, gnorm=1.116, clip=0, loss_scale=128, train_wall=51, gb_free=16.2, wall=28852
2023-08-07 22:04:41 | INFO | train_inner | epoch 027:   1392 / 1474 loss=4.079, trans_loss=5.315, nll_loss=2.625, w2v_ctc_loss=1.195, contrastive_loss=0, total=4149.21, n_correct=2682.25, ppl=6.17, accuracy=64.645, wps=8034, ups=1.94, wpb=4149.2, bsz=156.3, num_updates=39700, lr=7.09773e-05, gnorm=1.087, clip=0, loss_scale=128, train_wall=51, gb_free=17, wall=28904
2023-08-07 22:05:23 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 22:05:47 | INFO | dev_st | epoch 027 | valid on 'dev_st' subset | loss 4.312 | trans_loss 5.584 | nll_loss 2.877 | w2v_ctc_loss 1.342 | contrastive_loss 0 | total 4003.4 | n_correct 2470.1 | ppl 7.35 | accuracy 61.7 | uer 16.195 | wer 18.034 | raw_wer 18.034 | bleu 19.94 | wps 2196.6 | wpb 4003.4 | bsz 141.8 | num_updates 39782 | best_bleu 20.04
2023-08-07 22:05:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 39782 updates
2023-08-07 22:05:47 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.9402.pt
2023-08-07 22:05:51 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.9402.pt
2023-08-07 22:06:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.9402.pt (epoch 27 @ 39782 updates, score 19.94) (writing took 35.88393998518586 seconds)
2023-08-07 22:06:23 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-08-07 22:06:23 | INFO | train | epoch 027 | loss 4.072 | trans_loss 5.303 | nll_loss 2.609 | w2v_ctc_loss 1.199 | contrastive_loss 0 | total 4138.65 | n_correct 2683.14 | ppl 6.1 | accuracy 64.831 | wps 7297.9 | ups 1.76 | wpb 4138.6 | bsz 152.8 | num_updates 39782 | lr 7.09042e-05 | gnorm 1.1 | clip 0 | loss_scale 128 | train_wall 761 | gb_free 17.9 | wall 29006
2023-08-07 22:06:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 22:06:23 | INFO | fairseq.trainer | begin training epoch 28
2023-08-07 22:06:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 22:06:41 | INFO | train_inner | epoch 028:     18 / 1474 loss=4.075, trans_loss=5.307, nll_loss=2.614, w2v_ctc_loss=1.2, contrastive_loss=0, total=4106.72, n_correct=2657.55, ppl=6.12, accuracy=64.712, wps=3419.6, ups=0.83, wpb=4106.7, bsz=152.5, num_updates=39800, lr=7.08881e-05, gnorm=1.111, clip=0, loss_scale=128, train_wall=52, gb_free=16.3, wall=29024
2023-08-07 22:07:33 | INFO | train_inner | epoch 028:    118 / 1474 loss=4.036, trans_loss=5.256, nll_loss=2.546, w2v_ctc_loss=1.189, contrastive_loss=0, total=4103.42, n_correct=2692, ppl=5.84, accuracy=65.604, wps=7905.2, ups=1.93, wpb=4103.4, bsz=146, num_updates=39900, lr=7.07992e-05, gnorm=1.106, clip=0, loss_scale=128, train_wall=51, gb_free=16.2, wall=29076
2023-08-07 22:08:25 | INFO | train_inner | epoch 028:    218 / 1474 loss=4.036, trans_loss=5.26, nll_loss=2.554, w2v_ctc_loss=1.18, contrastive_loss=0, total=4200.12, n_correct=2754.81, ppl=5.87, accuracy=65.589, wps=8058.8, ups=1.92, wpb=4200.1, bsz=158.9, num_updates=40000, lr=7.07107e-05, gnorm=1.103, clip=0, loss_scale=128, train_wall=52, gb_free=11.4, wall=29128
2023-08-07 22:08:25 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 22:08:46 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 4.335 | trans_loss 5.601 | nll_loss 2.894 | w2v_ctc_loss 1.38 | contrastive_loss 0 | total 4003.4 | n_correct 2468.5 | ppl 7.43 | accuracy 61.66 | uer 16.365 | wer 18.143 | raw_wer 18.143 | bleu 19.59 | wps 2394.8 | wpb 4003.4 | bsz 141.8 | num_updates 40000 | best_bleu 20.04
2023-08-07 22:08:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 40000 updates
2023-08-07 22:08:46 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_28_40000.pt
2023-08-07 22:08:50 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_28_40000.pt
2023-08-07 22:09:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_28_40000.pt (epoch 28 @ 40000 updates, score 19.59) (writing took 19.61984413303435 seconds)
mt_weight tensor(0.0010, device='cuda:0')
asr_weight tensor(0.0053, device='cuda:0')
2023-08-07 22:09:59 | INFO | train_inner | epoch 028:    318 / 1474 loss=4.05, trans_loss=5.283, nll_loss=2.584, w2v_ctc_loss=1.171, contrastive_loss=0, total=4147.36, n_correct=2702.38, ppl=6, accuracy=65.159, wps=4407.6, ups=1.06, wpb=4147.4, bsz=157.7, num_updates=40100, lr=7.06225e-05, gnorm=1.09, clip=0, loss_scale=128, train_wall=52, gb_free=16.8, wall=29222
2023-08-07 22:10:51 | INFO | train_inner | epoch 028:    418 / 1474 loss=4.053, trans_loss=5.275, nll_loss=2.572, w2v_ctc_loss=1.2, contrastive_loss=0, total=4087.34, n_correct=2667.7, ppl=5.95, accuracy=65.267, wps=7855.2, ups=1.92, wpb=4087.3, bsz=147.6, num_updates=40200, lr=7.05346e-05, gnorm=1.105, clip=0, loss_scale=128, train_wall=52, gb_free=16.9, wall=29274
2023-08-07 22:11:42 | INFO | train_inner | epoch 028:    518 / 1474 loss=4.049, trans_loss=5.276, nll_loss=2.574, w2v_ctc_loss=1.184, contrastive_loss=0, total=4099.71, n_correct=2675.19, ppl=5.95, accuracy=65.253, wps=7950.9, ups=1.94, wpb=4099.7, bsz=148.1, num_updates=40300, lr=7.0447e-05, gnorm=1.098, clip=0, loss_scale=128, train_wall=51, gb_free=15.4, wall=29326
2023-08-07 22:12:34 | INFO | train_inner | epoch 028:    618 / 1474 loss=4.06, trans_loss=5.287, nll_loss=2.588, w2v_ctc_loss=1.197, contrastive_loss=0, total=4177.06, n_correct=2717.19, ppl=6.01, accuracy=65.05, wps=8061.9, ups=1.93, wpb=4177.1, bsz=152, num_updates=40400, lr=7.03598e-05, gnorm=1.099, clip=0, loss_scale=128, train_wall=51, gb_free=17.3, wall=29377
2023-08-07 22:13:27 | INFO | train_inner | epoch 028:    718 / 1474 loss=4.055, trans_loss=5.286, nll_loss=2.588, w2v_ctc_loss=1.182, contrastive_loss=0, total=4190.74, n_correct=2732.44, ppl=6.01, accuracy=65.202, wps=7983.6, ups=1.91, wpb=4190.7, bsz=164.2, num_updates=40500, lr=7.02728e-05, gnorm=1.089, clip=0, loss_scale=128, train_wall=52, gb_free=15.6, wall=29430
2023-08-07 22:14:18 | INFO | train_inner | epoch 028:    818 / 1474 loss=4.057, trans_loss=5.286, nll_loss=2.586, w2v_ctc_loss=1.192, contrastive_loss=0, total=4091.75, n_correct=2667.45, ppl=6.01, accuracy=65.191, wps=7918.7, ups=1.94, wpb=4091.8, bsz=153, num_updates=40600, lr=7.01862e-05, gnorm=1.109, clip=0, loss_scale=128, train_wall=51, gb_free=16.6, wall=29482
2023-08-07 22:15:11 | INFO | train_inner | epoch 028:    918 / 1474 loss=4.07, trans_loss=5.3, nll_loss=2.605, w2v_ctc_loss=1.2, contrastive_loss=0, total=4123.89, n_correct=2676.88, ppl=6.09, accuracy=64.912, wps=7830, ups=1.9, wpb=4123.9, bsz=150.5, num_updates=40700, lr=7.01e-05, gnorm=1.108, clip=0, loss_scale=128, train_wall=52, gb_free=17.2, wall=29534
2023-08-07 22:16:03 | INFO | train_inner | epoch 028:   1018 / 1474 loss=4.071, trans_loss=5.3, nll_loss=2.605, w2v_ctc_loss=1.203, contrastive_loss=0, total=4176.06, n_correct=2708.2, ppl=6.08, accuracy=64.851, wps=8006.3, ups=1.92, wpb=4176.1, bsz=155.7, num_updates=40800, lr=7.0014e-05, gnorm=1.102, clip=0, loss_scale=128, train_wall=52, gb_free=16.9, wall=29586
2023-08-07 22:16:56 | INFO | train_inner | epoch 028:   1118 / 1474 loss=4.063, trans_loss=5.29, nll_loss=2.593, w2v_ctc_loss=1.199, contrastive_loss=0, total=4206.08, n_correct=2732.06, ppl=6.03, accuracy=64.955, wps=8039.6, ups=1.91, wpb=4206.1, bsz=158.7, num_updates=40900, lr=6.99284e-05, gnorm=1.096, clip=0, loss_scale=128, train_wall=52, gb_free=17.3, wall=29639
2023-08-07 22:17:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-07 22:17:48 | INFO | train_inner | epoch 028:   1219 / 1474 loss=4.061, trans_loss=5.296, nll_loss=2.6, w2v_ctc_loss=1.179, contrastive_loss=0, total=4092.49, n_correct=2657.23, ppl=6.06, accuracy=64.929, wps=7843.6, ups=1.92, wpb=4092.5, bsz=151.2, num_updates=41000, lr=6.9843e-05, gnorm=1.098, clip=0, loss_scale=64, train_wall=52, gb_free=16.6, wall=29691
2023-08-07 22:18:40 | INFO | train_inner | epoch 028:   1319 / 1474 loss=4.083, trans_loss=5.312, nll_loss=2.621, w2v_ctc_loss=1.215, contrastive_loss=0, total=4087.78, n_correct=2642.2, ppl=6.15, accuracy=64.637, wps=7840.3, ups=1.92, wpb=4087.8, bsz=142.6, num_updates=41100, lr=6.9758e-05, gnorm=1.123, clip=0, loss_scale=64, train_wall=52, gb_free=15.2, wall=29743
2023-08-07 22:19:32 | INFO | train_inner | epoch 028:   1419 / 1474 loss=4.074, trans_loss=5.305, nll_loss=2.61, w2v_ctc_loss=1.203, contrastive_loss=0, total=4145.03, n_correct=2687.39, ppl=6.11, accuracy=64.834, wps=7979.6, ups=1.93, wpb=4145, bsz=148.8, num_updates=41200, lr=6.96733e-05, gnorm=1.104, clip=0, loss_scale=64, train_wall=51, gb_free=17.7, wall=29795
2023-08-07 22:20:00 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.0010, device='cuda:6')
asr_weight tensor(0.0053, device='cuda:6')
mt_weight tensor(0.0010, device='cuda:3')
asr_weight tensor(0.0053, device='cuda:3')
mt_weight tensor(0.0010, device='cuda:5')
asr_weight tensor(0.0053, device='cuda:5')
mt_weight tensor(0.0010, device='cuda:7')
asr_weight tensor(0.0053, device='cuda:7')
mt_weight tensor(0.0010, device='cuda:4')
asr_weight tensor(0.0053, device='cuda:4')
mt_weight tensor(0.0010, device='cuda:1')
asr_weight tensor(0.0053, device='cuda:1')
mt_weight tensor(0.0010, device='cuda:2')
asr_weight tensor(0.0053, device='cuda:2')
2023-08-07 22:20:22 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 4.32 | trans_loss 5.593 | nll_loss 2.881 | w2v_ctc_loss 1.351 | contrastive_loss 0 | total 4003.4 | n_correct 2472.4 | ppl 7.37 | accuracy 61.758 | uer 16.139 | wer 18.012 | raw_wer 18.012 | bleu 20.2 | wps 2302.8 | wpb 4003.4 | bsz 141.8 | num_updates 41255 | best_bleu 20.2
2023-08-07 22:20:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 41255 updates
2023-08-07 22:20:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 22:20:37 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt
2023-08-07 22:20:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_best.pt (epoch 28 @ 41255 updates, score 20.2) (writing took 27.19664810411632 seconds)
2023-08-07 22:20:50 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-08-07 22:20:50 | INFO | train | epoch 028 | loss 4.058 | trans_loss 5.286 | nll_loss 2.587 | w2v_ctc_loss 1.193 | contrastive_loss 0 | total 4137.65 | n_correct 2694.15 | ppl 6.01 | accuracy 65.113 | wps 7028.2 | ups 1.7 | wpb 4137.6 | bsz 152.7 | num_updates 41255 | lr 6.96268e-05 | gnorm 1.103 | clip 0 | loss_scale 64 | train_wall 760 | gb_free 16.6 | wall 29873
2023-08-07 22:20:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 22:20:51 | INFO | fairseq.trainer | begin training epoch 29
2023-08-07 22:20:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 22:21:22 | INFO | train_inner | epoch 029:     45 / 1474 loss=4.037, trans_loss=5.258, nll_loss=2.55, w2v_ctc_loss=1.188, contrastive_loss=0, total=4163.06, n_correct=2728.7, ppl=5.86, accuracy=65.546, wps=3775, ups=0.91, wpb=4163.1, bsz=157, num_updates=41300, lr=6.95889e-05, gnorm=1.11, clip=0, loss_scale=64, train_wall=51, gb_free=16.4, wall=29905
2023-08-07 22:22:15 | INFO | train_inner | epoch 029:    145 / 1474 loss=4.028, trans_loss=5.246, nll_loss=2.535, w2v_ctc_loss=1.185, contrastive_loss=0, total=4116.29, n_correct=2707.49, ppl=5.79, accuracy=65.775, wps=7837.3, ups=1.9, wpb=4116.3, bsz=154.3, num_updates=41400, lr=6.95048e-05, gnorm=1.103, clip=0, loss_scale=64, train_wall=52, gb_free=14.3, wall=29958
2023-08-07 22:23:07 | INFO | train_inner | epoch 029:    245 / 1474 loss=4.014, trans_loss=5.239, nll_loss=2.526, w2v_ctc_loss=1.156, contrastive_loss=0, total=4197.24, n_correct=2766.62, ppl=5.76, accuracy=65.915, wps=8036.9, ups=1.91, wpb=4197.2, bsz=165, num_updates=41500, lr=6.9421e-05, gnorm=1.084, clip=0, loss_scale=64, train_wall=52, gb_free=17.2, wall=30010
2023-08-07 22:23:59 | INFO | train_inner | epoch 029:    345 / 1474 loss=4.047, trans_loss=5.268, nll_loss=2.562, w2v_ctc_loss=1.198, contrastive_loss=0, total=4092.21, n_correct=2675.11, ppl=5.91, accuracy=65.371, wps=7830.4, ups=1.91, wpb=4092.2, bsz=145.3, num_updates=41600, lr=6.93375e-05, gnorm=1.112, clip=0, loss_scale=64, train_wall=52, gb_free=15.8, wall=30062
2023-08-07 22:24:51 | INFO | train_inner | epoch 029:    445 / 1474 loss=4.014, trans_loss=5.236, nll_loss=2.521, w2v_ctc_loss=1.163, contrastive_loss=0, total=4161.27, n_correct=2741.67, ppl=5.74, accuracy=65.885, wps=8023.8, ups=1.93, wpb=4161.3, bsz=153.9, num_updates=41700, lr=6.92543e-05, gnorm=1.101, clip=0, loss_scale=64, train_wall=51, gb_free=16.2, wall=30114
2023-08-07 22:25:43 | INFO | train_inner | epoch 029:    545 / 1474 loss=4.05, trans_loss=5.276, nll_loss=2.574, w2v_ctc_loss=1.188, contrastive_loss=0, total=4159.68, n_correct=2715.24, ppl=5.95, accuracy=65.275, wps=7948.6, ups=1.91, wpb=4159.7, bsz=148.2, num_updates=41800, lr=6.91714e-05, gnorm=1.099, clip=0, loss_scale=64, train_wall=52, gb_free=16.6, wall=30166
2023-08-07 22:26:36 | INFO | train_inner | epoch 029:    645 / 1474 loss=4.031, trans_loss=5.257, nll_loss=2.55, w2v_ctc_loss=1.172, contrastive_loss=0, total=4143.76, n_correct=2716.8, ppl=5.85, accuracy=65.564, wps=7905.3, ups=1.91, wpb=4143.8, bsz=159.4, num_updates=41900, lr=6.90889e-05, gnorm=1.112, clip=0, loss_scale=64, train_wall=52, gb_free=17.4, wall=30219
2023-08-07 22:27:28 | INFO | train_inner | epoch 029:    745 / 1474 loss=4.032, trans_loss=5.258, nll_loss=2.551, w2v_ctc_loss=1.172, contrastive_loss=0, total=4234.8, n_correct=2775.89, ppl=5.86, accuracy=65.549, wps=8049.5, ups=1.9, wpb=4234.8, bsz=164.1, num_updates=42000, lr=6.90066e-05, gnorm=1.083, clip=0, loss_scale=64, train_wall=52, gb_free=16.9, wall=30271
2023-08-07 22:27:28 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 22:27:51 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 4.345 | trans_loss 5.597 | nll_loss 2.893 | w2v_ctc_loss 1.422 | contrastive_loss 0 | total 4003.4 | n_correct 2465.2 | ppl 7.43 | accuracy 61.578 | uer 16.346 | wer 18.023 | raw_wer 18.023 | bleu 19.83 | wps 2254.8 | wpb 4003.4 | bsz 141.8 | num_updates 42000 | best_bleu 20.2
2023-08-07 22:27:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42000 updates
2023-08-07 22:27:51 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_29_42000.pt
2023-08-07 22:27:54 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_29_42000.pt
2023-08-07 22:28:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_29_42000.pt (epoch 29 @ 42000 updates, score 19.83) (writing took 37.27502058446407 seconds)
2023-08-07 22:29:21 | INFO | train_inner | epoch 029:    845 / 1474 loss=4.052, trans_loss=5.283, nll_loss=2.582, w2v_ctc_loss=1.178, contrastive_loss=0, total=4033.21, n_correct=2625.73, ppl=5.99, accuracy=65.103, wps=3593.2, ups=0.89, wpb=4033.2, bsz=140.8, num_updates=42100, lr=6.89246e-05, gnorm=1.127, clip=0, loss_scale=64, train_wall=51, gb_free=16.7, wall=30384
2023-08-07 22:30:12 | INFO | train_inner | epoch 029:    945 / 1474 loss=4.06, trans_loss=5.287, nll_loss=2.588, w2v_ctc_loss=1.196, contrastive_loss=0, total=4085.97, n_correct=2660.16, ppl=6.01, accuracy=65.105, wps=7936.3, ups=1.94, wpb=4086, bsz=148.4, num_updates=42200, lr=6.88428e-05, gnorm=1.116, clip=0, loss_scale=64, train_wall=51, gb_free=17.3, wall=30435
2023-08-07 22:31:05 | INFO | train_inner | epoch 029:   1045 / 1474 loss=4.043, trans_loss=5.271, nll_loss=2.568, w2v_ctc_loss=1.178, contrastive_loss=0, total=4140.84, n_correct=2707.62, ppl=5.93, accuracy=65.388, wps=7876.8, ups=1.9, wpb=4140.8, bsz=153.4, num_updates=42300, lr=6.87614e-05, gnorm=1.106, clip=0, loss_scale=64, train_wall=52, gb_free=16.8, wall=30488
2023-08-07 22:31:56 | INFO | train_inner | epoch 029:   1145 / 1474 loss=4.057, trans_loss=5.283, nll_loss=2.582, w2v_ctc_loss=1.197, contrastive_loss=0, total=4068.4, n_correct=2648.83, ppl=5.99, accuracy=65.107, wps=7875.6, ups=1.94, wpb=4068.4, bsz=142.1, num_updates=42400, lr=6.86803e-05, gnorm=1.117, clip=0, loss_scale=64, train_wall=51, gb_free=17.6, wall=30539
2023-08-07 22:32:48 | INFO | train_inner | epoch 029:   1245 / 1474 loss=4.057, trans_loss=5.285, nll_loss=2.584, w2v_ctc_loss=1.194, contrastive_loss=0, total=4154.79, n_correct=2703.48, ppl=6, accuracy=65.069, wps=7968.2, ups=1.92, wpb=4154.8, bsz=149.8, num_updates=42500, lr=6.85994e-05, gnorm=1.114, clip=0, loss_scale=64, train_wall=52, gb_free=17, wall=30592
2023-08-07 22:33:41 | INFO | train_inner | epoch 029:   1345 / 1474 loss=4.045, trans_loss=5.275, nll_loss=2.574, w2v_ctc_loss=1.175, contrastive_loss=0, total=4166.4, n_correct=2715.78, ppl=5.95, accuracy=65.183, wps=7992.6, ups=1.92, wpb=4166.4, bsz=155.8, num_updates=42600, lr=6.85189e-05, gnorm=1.108, clip=0, loss_scale=64, train_wall=52, gb_free=16.7, wall=30644
2023-08-07 22:34:32 | INFO | train_inner | epoch 029:   1445 / 1474 loss=4.057, trans_loss=5.283, nll_loss=2.583, w2v_ctc_loss=1.198, contrastive_loss=0, total=4169.4, n_correct=2716.3, ppl=5.99, accuracy=65.148, wps=8082.6, ups=1.94, wpb=4169.4, bsz=156, num_updates=42700, lr=6.84386e-05, gnorm=1.11, clip=0, loss_scale=64, train_wall=51, gb_free=16, wall=30695
2023-08-07 22:34:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 22:35:10 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 4.32 | trans_loss 5.588 | nll_loss 2.879 | w2v_ctc_loss 1.361 | contrastive_loss 0 | total 4003.4 | n_correct 2469.1 | ppl 7.35 | accuracy 61.675 | uer 16.242 | wer 18.146 | raw_wer 18.146 | bleu 19.58 | wps 2251 | wpb 4003.4 | bsz 141.8 | num_updates 42729 | best_bleu 20.2
2023-08-07 22:35:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42729 updates
2023-08-07 22:35:10 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_last.pt
2023-08-07 22:35:24 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_last.pt
2023-08-07 22:35:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_last.pt (epoch 29 @ 42729 updates, score 19.58) (writing took 14.256853042170405 seconds)
2023-08-07 22:35:24 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-08-07 22:35:24 | INFO | train | epoch 029 | loss 4.041 | trans_loss 5.266 | nll_loss 2.561 | w2v_ctc_loss 1.182 | contrastive_loss 0 | total 4138.65 | n_correct 2707.06 | ppl 5.9 | accuracy 65.409 | wps 6981.2 | ups 1.69 | wpb 4138.6 | bsz 152.8 | num_updates 42729 | lr 6.84154e-05 | gnorm 1.106 | clip 0 | loss_scale 64 | train_wall 761 | gb_free 16.3 | wall 30747
2023-08-07 22:35:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 22:35:24 | INFO | fairseq.trainer | begin training epoch 30
2023-08-07 22:35:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 22:36:09 | INFO | train_inner | epoch 030:     71 / 1474 loss=4.012, trans_loss=5.234, nll_loss=2.519, w2v_ctc_loss=1.16, contrastive_loss=0, total=4176.73, n_correct=2755.23, ppl=5.73, accuracy=65.966, wps=4294.3, ups=1.03, wpb=4176.7, bsz=159.5, num_updates=42800, lr=6.83586e-05, gnorm=1.097, clip=0, loss_scale=64, train_wall=52, gb_free=17.2, wall=30793
2023-08-07 22:37:01 | INFO | train_inner | epoch 030:    171 / 1474 loss=4.003, trans_loss=5.217, nll_loss=2.497, w2v_ctc_loss=1.171, contrastive_loss=0, total=4202.84, n_correct=2785.56, ppl=5.64, accuracy=66.278, wps=8087.1, ups=1.92, wpb=4202.8, bsz=159.3, num_updates=42900, lr=6.82789e-05, gnorm=1.095, clip=0, loss_scale=64, train_wall=52, gb_free=13, wall=30845
2023-08-07 22:37:54 | INFO | train_inner | epoch 030:    271 / 1474 loss=4.017, trans_loss=5.233, nll_loss=2.517, w2v_ctc_loss=1.181, contrastive_loss=0, total=4120.08, n_correct=2719.71, ppl=5.72, accuracy=66.011, wps=7903.1, ups=1.92, wpb=4120.1, bsz=148.2, num_updates=43000, lr=6.81994e-05, gnorm=1.112, clip=0, loss_scale=64, train_wall=52, gb_free=16.9, wall=30897
2023-08-07 22:38:46 | INFO | train_inner | epoch 030:    371 / 1474 loss=4.008, trans_loss=5.228, nll_loss=2.51, w2v_ctc_loss=1.16, contrastive_loss=0, total=4175.82, n_correct=2759.6, ppl=5.7, accuracy=66.085, wps=8035.6, ups=1.92, wpb=4175.8, bsz=153.1, num_updates=43100, lr=6.81203e-05, gnorm=1.092, clip=0, loss_scale=128, train_wall=52, gb_free=15.8, wall=30949
2023-08-07 22:39:37 | INFO | train_inner | epoch 030:    471 / 1474 loss=4.016, trans_loss=5.238, nll_loss=2.524, w2v_ctc_loss=1.166, contrastive_loss=0, total=4128.9, n_correct=2719.15, ppl=5.75, accuracy=65.857, wps=7964.4, ups=1.93, wpb=4128.9, bsz=156.2, num_updates=43200, lr=6.80414e-05, gnorm=1.109, clip=0, loss_scale=128, train_wall=51, gb_free=16.9, wall=31001
2023-08-07 22:40:30 | INFO | train_inner | epoch 030:    571 / 1474 loss=4.029, trans_loss=5.252, nll_loss=2.543, w2v_ctc_loss=1.175, contrastive_loss=0, total=4162.83, n_correct=2732.8, ppl=5.83, accuracy=65.648, wps=7927.5, ups=1.9, wpb=4162.8, bsz=155.7, num_updates=43300, lr=6.79628e-05, gnorm=1.109, clip=0, loss_scale=128, train_wall=52, gb_free=15.2, wall=31053
2023-08-07 22:41:22 | INFO | train_inner | epoch 030:    671 / 1474 loss=4.024, trans_loss=5.245, nll_loss=2.533, w2v_ctc_loss=1.176, contrastive_loss=0, total=4197.56, n_correct=2758.54, ppl=5.79, accuracy=65.718, wps=8060.1, ups=1.92, wpb=4197.6, bsz=158.8, num_updates=43400, lr=6.78844e-05, gnorm=1.1, clip=0, loss_scale=128, train_wall=52, gb_free=16.7, wall=31105
2023-08-07 22:42:14 | INFO | train_inner | epoch 030:    771 / 1474 loss=4.043, trans_loss=5.264, nll_loss=2.558, w2v_ctc_loss=1.193, contrastive_loss=0, total=4097.27, n_correct=2681.87, ppl=5.89, accuracy=65.455, wps=7825.9, ups=1.91, wpb=4097.3, bsz=150.6, num_updates=43500, lr=6.78064e-05, gnorm=1.117, clip=0, loss_scale=128, train_wall=52, gb_free=15.1, wall=31157
2023-08-07 22:42:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-07 22:43:07 | INFO | train_inner | epoch 030:    872 / 1474 loss=4.033, trans_loss=5.257, nll_loss=2.548, w2v_ctc_loss=1.176, contrastive_loss=0, total=4087.09, n_correct=2676.8, ppl=5.85, accuracy=65.494, wps=7825.2, ups=1.91, wpb=4087.1, bsz=145.1, num_updates=43600, lr=6.77285e-05, gnorm=1.12, clip=0, loss_scale=64, train_wall=52, gb_free=16.8, wall=31210
2023-08-07 22:43:58 | INFO | train_inner | epoch 030:    972 / 1474 loss=4.04, trans_loss=5.262, nll_loss=2.555, w2v_ctc_loss=1.188, contrastive_loss=0, total=4140.03, n_correct=2710.66, ppl=5.88, accuracy=65.474, wps=7972.7, ups=1.93, wpb=4140, bsz=152, num_updates=43700, lr=6.7651e-05, gnorm=1.121, clip=0, loss_scale=64, train_wall=51, gb_free=14.1, wall=31262
2023-08-07 22:44:51 | INFO | train_inner | epoch 030:   1072 / 1474 loss=4.055, trans_loss=5.281, nll_loss=2.58, w2v_ctc_loss=1.195, contrastive_loss=0, total=4101.12, n_correct=2670.38, ppl=5.98, accuracy=65.113, wps=7810.6, ups=1.9, wpb=4101.1, bsz=141.4, num_updates=43800, lr=6.75737e-05, gnorm=1.123, clip=0, loss_scale=64, train_wall=52, gb_free=16.9, wall=31314
2023-08-07 22:45:43 | INFO | train_inner | epoch 030:   1172 / 1474 loss=4.034, trans_loss=5.261, nll_loss=2.555, w2v_ctc_loss=1.171, contrastive_loss=0, total=4168.22, n_correct=2731.32, ppl=5.88, accuracy=65.527, wps=7941.7, ups=1.91, wpb=4168.2, bsz=157.3, num_updates=43900, lr=6.74967e-05, gnorm=1.102, clip=0, loss_scale=64, train_wall=52, gb_free=16.8, wall=31367
2023-08-07 22:46:35 | INFO | train_inner | epoch 030:   1272 / 1474 loss=4.053, trans_loss=5.277, nll_loss=2.574, w2v_ctc_loss=1.199, contrastive_loss=0, total=4032.74, n_correct=2630.71, ppl=5.96, accuracy=65.234, wps=7776.2, ups=1.93, wpb=4032.7, bsz=140.9, num_updates=44000, lr=6.742e-05, gnorm=1.139, clip=0, loss_scale=64, train_wall=51, gb_free=17, wall=31418
2023-08-07 22:46:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 22:46:57 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 4.325 | trans_loss 5.585 | nll_loss 2.876 | w2v_ctc_loss 1.384 | contrastive_loss 0 | total 4003.4 | n_correct 2474.8 | ppl 7.34 | accuracy 61.817 | uer 16.088 | wer 17.867 | raw_wer 17.867 | bleu 19.96 | wps 2428.2 | wpb 4003.4 | bsz 141.8 | num_updates 44000 | best_bleu 20.2
2023-08-07 22:46:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44000 updates
2023-08-07 22:46:57 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_30_44000.pt
2023-08-07 22:47:01 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_30_44000.pt
2023-08-07 22:47:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_30_44000.pt (epoch 30 @ 44000 updates, score 19.96) (writing took 36.975812612101436 seconds)
2023-08-07 22:48:27 | INFO | train_inner | epoch 030:   1372 / 1474 loss=4.028, trans_loss=5.254, nll_loss=2.546, w2v_ctc_loss=1.169, contrastive_loss=0, total=4166.96, n_correct=2735.94, ppl=5.84, accuracy=65.658, wps=3743.9, ups=0.9, wpb=4167, bsz=161.1, num_updates=44100, lr=6.73435e-05, gnorm=1.103, clip=0, loss_scale=64, train_wall=51, gb_free=16.2, wall=31530
2023-08-07 22:49:18 | INFO | train_inner | epoch 030:   1472 / 1474 loss=4.037, trans_loss=5.268, nll_loss=2.564, w2v_ctc_loss=1.166, contrastive_loss=0, total=4125.17, n_correct=2698.49, ppl=5.91, accuracy=65.415, wps=8011.5, ups=1.94, wpb=4125.2, bsz=155, num_updates=44200, lr=6.72673e-05, gnorm=1.118, clip=0, loss_scale=64, train_wall=51, gb_free=17.1, wall=31581
2023-08-07 22:49:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 22:49:41 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 4.315 | trans_loss 5.589 | nll_loss 2.876 | w2v_ctc_loss 1.342 | contrastive_loss 0 | total 4003.4 | n_correct 2468.3 | ppl 7.34 | accuracy 61.655 | uer 16.123 | wer 17.971 | raw_wer 17.971 | bleu 19.79 | wps 2415.6 | wpb 4003.4 | bsz 141.8 | num_updates 44202 | best_bleu 20.2
2023-08-07 22:49:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44202 updates
2023-08-07 22:49:41 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.7906.pt
2023-08-07 22:49:45 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.7906.pt
2023-08-07 22:50:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.7906.pt (epoch 30 @ 44202 updates, score 19.79) (writing took 20.567302454262972 seconds)
2023-08-07 22:50:02 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-08-07 22:50:02 | INFO | train | epoch 030 | loss 4.028 | trans_loss 5.251 | nll_loss 2.541 | w2v_ctc_loss 1.176 | contrastive_loss 0 | total 4138.25 | n_correct 2717.53 | ppl 5.82 | accuracy 65.669 | wps 6946 | ups 1.68 | wpb 4138.3 | bsz 152.8 | num_updates 44202 | lr 6.72658e-05 | gnorm 1.111 | clip 0 | loss_scale 64 | train_wall 761 | gb_free 17.2 | wall 31625
2023-08-07 22:50:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 22:50:02 | INFO | fairseq.trainer | begin training epoch 31
2023-08-07 22:50:02 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 22:51:02 | INFO | train_inner | epoch 031:     98 / 1474 loss=4.004, trans_loss=5.214, nll_loss=2.493, w2v_ctc_loss=1.181, contrastive_loss=0, total=4081.34, n_correct=2706.8, ppl=5.63, accuracy=66.321, wps=3927.4, ups=0.96, wpb=4081.3, bsz=147.3, num_updates=44300, lr=6.71913e-05, gnorm=1.113, clip=0, loss_scale=64, train_wall=52, gb_free=16.5, wall=31685
2023-08-07 22:51:55 | INFO | train_inner | epoch 031:    198 / 1474 loss=3.999, trans_loss=5.215, nll_loss=2.493, w2v_ctc_loss=1.164, contrastive_loss=0, total=4146.03, n_correct=2746.94, ppl=5.63, accuracy=66.255, wps=7895.3, ups=1.9, wpb=4146, bsz=151.1, num_updates=44400, lr=6.71156e-05, gnorm=1.111, clip=0, loss_scale=64, train_wall=52, gb_free=13.6, wall=31738
2023-08-07 22:52:47 | INFO | train_inner | epoch 031:    298 / 1474 loss=4.006, trans_loss=5.219, nll_loss=2.499, w2v_ctc_loss=1.174, contrastive_loss=0, total=4146.75, n_correct=2740.86, ppl=5.65, accuracy=66.097, wps=7954.2, ups=1.92, wpb=4146.8, bsz=150.4, num_updates=44500, lr=6.70402e-05, gnorm=1.123, clip=0, loss_scale=64, train_wall=52, gb_free=17.7, wall=31790
2023-08-07 22:53:39 | INFO | train_inner | epoch 031:    398 / 1474 loss=4.01, trans_loss=5.23, nll_loss=2.513, w2v_ctc_loss=1.164, contrastive_loss=0, total=4089.43, n_correct=2699.42, ppl=5.71, accuracy=66.01, wps=7797.3, ups=1.91, wpb=4089.4, bsz=142.8, num_updates=44600, lr=6.6965e-05, gnorm=1.123, clip=0, loss_scale=64, train_wall=52, gb_free=16.3, wall=31842
2023-08-07 22:54:31 | INFO | train_inner | epoch 031:    498 / 1474 loss=4.006, trans_loss=5.22, nll_loss=2.5, w2v_ctc_loss=1.174, contrastive_loss=0, total=4114.41, n_correct=2719.6, ppl=5.66, accuracy=66.099, wps=7898.5, ups=1.92, wpb=4114.4, bsz=150.4, num_updates=44700, lr=6.689e-05, gnorm=1.107, clip=0, loss_scale=64, train_wall=52, gb_free=16.2, wall=31894
2023-08-07 22:55:23 | INFO | train_inner | epoch 031:    598 / 1474 loss=4.009, trans_loss=5.229, nll_loss=2.511, w2v_ctc_loss=1.162, contrastive_loss=0, total=4084.36, n_correct=2697.2, ppl=5.7, accuracy=66.037, wps=7864.5, ups=1.93, wpb=4084.4, bsz=147.5, num_updates=44800, lr=6.68153e-05, gnorm=1.127, clip=0, loss_scale=64, train_wall=51, gb_free=16.6, wall=31946
2023-08-07 22:56:15 | INFO | train_inner | epoch 031:    698 / 1474 loss=3.997, trans_loss=5.22, nll_loss=2.5, w2v_ctc_loss=1.144, contrastive_loss=0, total=4210.09, n_correct=2786.98, ppl=5.66, accuracy=66.198, wps=8095.2, ups=1.92, wpb=4210.1, bsz=157.4, num_updates=44900, lr=6.67409e-05, gnorm=1.089, clip=0, loss_scale=64, train_wall=52, gb_free=15, wall=31998
2023-08-07 22:57:07 | INFO | train_inner | epoch 031:    798 / 1474 loss=4.019, trans_loss=5.242, nll_loss=2.529, w2v_ctc_loss=1.164, contrastive_loss=0, total=4098.1, n_correct=2693.2, ppl=5.77, accuracy=65.718, wps=7876.6, ups=1.92, wpb=4098.1, bsz=147.8, num_updates=45000, lr=6.66667e-05, gnorm=1.122, clip=0, loss_scale=64, train_wall=52, gb_free=17.1, wall=32050
mt_weight tensor(0.0010, device='cuda:0')
asr_weight tensor(0.0053, device='cuda:0')
2023-08-07 22:58:00 | INFO | train_inner | epoch 031:    898 / 1474 loss=4.004, trans_loss=5.225, nll_loss=2.506, w2v_ctc_loss=1.155, contrastive_loss=0, total=4101.05, n_correct=2708.11, ppl=5.68, accuracy=66.035, wps=7833, ups=1.91, wpb=4101.1, bsz=148.4, num_updates=45100, lr=6.65927e-05, gnorm=1.113, clip=0, loss_scale=64, train_wall=52, gb_free=17.1, wall=32103
2023-08-07 22:58:52 | INFO | train_inner | epoch 031:    998 / 1474 loss=4.016, trans_loss=5.24, nll_loss=2.528, w2v_ctc_loss=1.159, contrastive_loss=0, total=4186.3, n_correct=2757.17, ppl=5.77, accuracy=65.862, wps=8031.5, ups=1.92, wpb=4186.3, bsz=159.3, num_updates=45200, lr=6.6519e-05, gnorm=1.113, clip=0, loss_scale=64, train_wall=52, gb_free=16.4, wall=32155
2023-08-07 22:59:44 | INFO | train_inner | epoch 031:   1098 / 1474 loss=4.016, trans_loss=5.24, nll_loss=2.527, w2v_ctc_loss=1.162, contrastive_loss=0, total=4147.34, n_correct=2732.04, ppl=5.77, accuracy=65.875, wps=7942.9, ups=1.92, wpb=4147.3, bsz=157.3, num_updates=45300, lr=6.64455e-05, gnorm=1.118, clip=0, loss_scale=64, train_wall=52, gb_free=17.2, wall=32207
2023-08-07 23:00:36 | INFO | train_inner | epoch 031:   1198 / 1474 loss=4.024, trans_loss=5.248, nll_loss=2.538, w2v_ctc_loss=1.169, contrastive_loss=0, total=4185.34, n_correct=2745.62, ppl=5.81, accuracy=65.601, wps=8114.1, ups=1.94, wpb=4185.3, bsz=160.8, num_updates=45400, lr=6.63723e-05, gnorm=1.117, clip=0, loss_scale=64, train_wall=51, gb_free=17.1, wall=32259
2023-08-07 23:01:28 | INFO | train_inner | epoch 031:   1298 / 1474 loss=4.021, trans_loss=5.245, nll_loss=2.535, w2v_ctc_loss=1.163, contrastive_loss=0, total=4223.54, n_correct=2777.25, ppl=5.79, accuracy=65.756, wps=8094.1, ups=1.92, wpb=4223.5, bsz=162.5, num_updates=45500, lr=6.62994e-05, gnorm=1.102, clip=0, loss_scale=64, train_wall=52, gb_free=13.8, wall=32311
2023-08-07 23:02:20 | INFO | train_inner | epoch 031:   1398 / 1474 loss=4.021, trans_loss=5.249, nll_loss=2.54, w2v_ctc_loss=1.155, contrastive_loss=0, total=4195.76, n_correct=2756.91, ppl=5.82, accuracy=65.707, wps=8037.8, ups=1.92, wpb=4195.8, bsz=164.1, num_updates=45600, lr=6.62266e-05, gnorm=1.106, clip=0, loss_scale=128, train_wall=52, gb_free=16.7, wall=32363
2023-08-07 23:02:59 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.0010, device='cuda:4')
asr_weight tensor(0.0053, device='cuda:4')
mt_weight tensor(0.0010, device='cuda:5')
asr_weight tensor(0.0053, device='cuda:5')
mt_weight tensor(0.0010, device='cuda:7')
asr_weight tensor(0.0053, device='cuda:7')
mt_weight tensor(0.0010, device='cuda:3')
asr_weight tensor(0.0053, device='cuda:3')
mt_weight tensor(0.0010, device='cuda:1')
asr_weight tensor(0.0053, device='cuda:1')
mt_weight tensor(0.0010, device='cuda:2')
asr_weight tensor(0.0053, device='cuda:2')
mt_weight tensor(0.0010, device='cuda:6')
asr_weight tensor(0.0053, device='cuda:6')
2023-08-07 23:03:21 | INFO | dev_st | epoch 031 | valid on 'dev_st' subset | loss 4.328 | trans_loss 5.585 | nll_loss 2.872 | w2v_ctc_loss 1.395 | contrastive_loss 0 | total 4003.4 | n_correct 2470.2 | ppl 7.32 | accuracy 61.703 | uer 16.11 | wer 17.937 | raw_wer 17.937 | bleu 19.72 | wps 2406 | wpb 4003.4 | bsz 141.8 | num_updates 45676 | best_bleu 20.2
2023-08-07 23:03:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 45676 updates
2023-08-07 23:03:21 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_last.pt
2023-08-07 23:03:34 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_last.pt
2023-08-07 23:03:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_last.pt (epoch 31 @ 45676 updates, score 19.72) (writing took 14.08309167996049 seconds)
2023-08-07 23:03:35 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-08-07 23:03:35 | INFO | train | epoch 031 | loss 4.012 | trans_loss 5.232 | nll_loss 2.517 | w2v_ctc_loss 1.164 | contrastive_loss 0 | total 4138.65 | n_correct 2729.7 | ppl 5.72 | accuracy 65.956 | wps 7505 | ups 1.81 | wpb 4138.6 | bsz 152.8 | num_updates 45676 | lr 6.61715e-05 | gnorm 1.114 | clip 0 | loss_scale 128 | train_wall 761 | gb_free 12.4 | wall 32438
2023-08-07 23:03:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 23:03:35 | INFO | fairseq.trainer | begin training epoch 32
2023-08-07 23:03:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 23:03:55 | INFO | train_inner | epoch 032:     24 / 1474 loss=4.009, trans_loss=5.23, nll_loss=2.515, w2v_ctc_loss=1.16, contrastive_loss=0, total=4039.04, n_correct=2665.51, ppl=5.71, accuracy=65.994, wps=4234.9, ups=1.05, wpb=4039, bsz=143.6, num_updates=45700, lr=6.61541e-05, gnorm=1.132, clip=0, loss_scale=128, train_wall=51, gb_free=17.8, wall=32458
2023-08-07 23:04:47 | INFO | train_inner | epoch 032:    124 / 1474 loss=3.961, trans_loss=5.172, nll_loss=2.438, w2v_ctc_loss=1.134, contrastive_loss=0, total=4224.84, n_correct=2825.97, ppl=5.42, accuracy=66.889, wps=8110.8, ups=1.92, wpb=4224.8, bsz=161.2, num_updates=45800, lr=6.60819e-05, gnorm=1.086, clip=0, loss_scale=128, train_wall=52, gb_free=16.7, wall=32511
2023-08-07 23:05:39 | INFO | train_inner | epoch 032:    224 / 1474 loss=3.986, trans_loss=5.198, nll_loss=2.473, w2v_ctc_loss=1.158, contrastive_loss=0, total=4163.01, n_correct=2769.95, ppl=5.55, accuracy=66.537, wps=7988.6, ups=1.92, wpb=4163, bsz=161.1, num_updates=45900, lr=6.60098e-05, gnorm=1.11, clip=0, loss_scale=128, train_wall=52, gb_free=17.2, wall=32563
2023-08-07 23:06:32 | INFO | train_inner | epoch 032:    324 / 1474 loss=3.958, trans_loss=5.175, nll_loss=2.443, w2v_ctc_loss=1.119, contrastive_loss=0, total=4185.21, n_correct=2801.09, ppl=5.44, accuracy=66.928, wps=8032.5, ups=1.92, wpb=4185.2, bsz=157.5, num_updates=46000, lr=6.5938e-05, gnorm=1.099, clip=0, loss_scale=128, train_wall=52, gb_free=16.5, wall=32615
2023-08-07 23:06:32 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 23:06:53 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 4.335 | trans_loss 5.595 | nll_loss 2.891 | w2v_ctc_loss 1.395 | contrastive_loss 0 | total 4003.4 | n_correct 2473.6 | ppl 7.42 | accuracy 61.787 | uer 16.269 | wer 18.087 | raw_wer 18.087 | bleu 19.94 | wps 2402.6 | wpb 4003.4 | bsz 141.8 | num_updates 46000 | best_bleu 20.2
2023-08-07 23:06:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 46000 updates
2023-08-07 23:06:53 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_32_46000.pt
2023-08-07 23:06:57 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_32_46000.pt
2023-08-07 23:07:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_32_46000.pt (epoch 32 @ 46000 updates, score 19.94) (writing took 23.6033418700099 seconds)
2023-08-07 23:07:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-07 23:08:10 | INFO | train_inner | epoch 032:    425 / 1474 loss=3.986, trans_loss=5.2, nll_loss=2.475, w2v_ctc_loss=1.153, contrastive_loss=0, total=4157.23, n_correct=2761.98, ppl=5.56, accuracy=66.438, wps=4217.5, ups=1.01, wpb=4157.2, bsz=153, num_updates=46100, lr=6.58665e-05, gnorm=1.122, clip=0, loss_scale=64, train_wall=52, gb_free=15, wall=32713
2023-08-07 23:09:03 | INFO | train_inner | epoch 032:    525 / 1474 loss=3.997, trans_loss=5.211, nll_loss=2.489, w2v_ctc_loss=1.163, contrastive_loss=0, total=4198.93, n_correct=2783.77, ppl=5.62, accuracy=66.297, wps=7937.2, ups=1.89, wpb=4198.9, bsz=158.8, num_updates=46200, lr=6.57952e-05, gnorm=1.101, clip=0, loss_scale=64, train_wall=52, gb_free=17.4, wall=32766
2023-08-07 23:09:55 | INFO | train_inner | epoch 032:    625 / 1474 loss=4.001, trans_loss=5.218, nll_loss=2.499, w2v_ctc_loss=1.161, contrastive_loss=0, total=4142.69, n_correct=2741.37, ppl=5.65, accuracy=66.174, wps=7911, ups=1.91, wpb=4142.7, bsz=150.8, num_updates=46300, lr=6.57241e-05, gnorm=1.117, clip=0, loss_scale=64, train_wall=52, gb_free=17.5, wall=32819
2023-08-07 23:10:48 | INFO | train_inner | epoch 032:    725 / 1474 loss=4.009, trans_loss=5.222, nll_loss=2.504, w2v_ctc_loss=1.177, contrastive_loss=0, total=4154.59, n_correct=2743.81, ppl=5.67, accuracy=66.043, wps=7954.2, ups=1.91, wpb=4154.6, bsz=150.9, num_updates=46400, lr=6.56532e-05, gnorm=1.114, clip=0, loss_scale=64, train_wall=52, gb_free=16.1, wall=32871
2023-08-07 23:11:40 | INFO | train_inner | epoch 032:    825 / 1474 loss=3.992, trans_loss=5.213, nll_loss=2.491, w2v_ctc_loss=1.142, contrastive_loss=0, total=4114.54, n_correct=2726.47, ppl=5.62, accuracy=66.264, wps=7919.5, ups=1.92, wpb=4114.5, bsz=147.4, num_updates=46500, lr=6.55826e-05, gnorm=1.115, clip=0, loss_scale=64, train_wall=52, gb_free=17.2, wall=32923
2023-08-07 23:12:32 | INFO | train_inner | epoch 032:    925 / 1474 loss=4.001, trans_loss=5.223, nll_loss=2.504, w2v_ctc_loss=1.15, contrastive_loss=0, total=4139.67, n_correct=2737.31, ppl=5.67, accuracy=66.124, wps=7974.4, ups=1.93, wpb=4139.7, bsz=149.2, num_updates=46600, lr=6.55122e-05, gnorm=1.131, clip=0, loss_scale=64, train_wall=51, gb_free=17, wall=32975
2023-08-07 23:13:24 | INFO | train_inner | epoch 032:   1025 / 1474 loss=4.009, trans_loss=5.231, nll_loss=2.515, w2v_ctc_loss=1.157, contrastive_loss=0, total=4119.15, n_correct=2714.69, ppl=5.72, accuracy=65.904, wps=7827.5, ups=1.9, wpb=4119.1, bsz=152.2, num_updates=46700, lr=6.5442e-05, gnorm=1.129, clip=0, loss_scale=64, train_wall=52, gb_free=16.6, wall=33027
2023-08-07 23:14:16 | INFO | train_inner | epoch 032:   1125 / 1474 loss=4.017, trans_loss=5.238, nll_loss=2.523, w2v_ctc_loss=1.167, contrastive_loss=0, total=4019.61, n_correct=2645.53, ppl=5.75, accuracy=65.816, wps=7693.9, ups=1.91, wpb=4019.6, bsz=135.7, num_updates=46800, lr=6.5372e-05, gnorm=1.139, clip=0, loss_scale=64, train_wall=52, gb_free=17.7, wall=33080
2023-08-07 23:15:09 | INFO | train_inner | epoch 032:   1225 / 1474 loss=4.028, trans_loss=5.25, nll_loss=2.54, w2v_ctc_loss=1.177, contrastive_loss=0, total=4149.28, n_correct=2720.18, ppl=5.82, accuracy=65.558, wps=7913.5, ups=1.91, wpb=4149.3, bsz=155.2, num_updates=46900, lr=6.53023e-05, gnorm=1.13, clip=0, loss_scale=64, train_wall=52, gb_free=16, wall=33132
2023-08-07 23:16:00 | INFO | train_inner | epoch 032:   1325 / 1474 loss=4.014, trans_loss=5.232, nll_loss=2.516, w2v_ctc_loss=1.172, contrastive_loss=0, total=4079.22, n_correct=2687.6, ppl=5.72, accuracy=65.885, wps=7911.3, ups=1.94, wpb=4079.2, bsz=148.1, num_updates=47000, lr=6.52328e-05, gnorm=1.143, clip=0, loss_scale=64, train_wall=51, gb_free=15.3, wall=33184
2023-08-07 23:16:52 | INFO | train_inner | epoch 032:   1425 / 1474 loss=4.021, trans_loss=5.242, nll_loss=2.53, w2v_ctc_loss=1.173, contrastive_loss=0, total=4111.41, n_correct=2709.65, ppl=5.78, accuracy=65.906, wps=7951.8, ups=1.93, wpb=4111.4, bsz=153.1, num_updates=47100, lr=6.51635e-05, gnorm=1.126, clip=0, loss_scale=64, train_wall=51, gb_free=16.1, wall=33235
2023-08-07 23:17:18 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 23:17:39 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 4.319 | trans_loss 5.579 | nll_loss 2.869 | w2v_ctc_loss 1.378 | contrastive_loss 0 | total 4003.4 | n_correct 2475.9 | ppl 7.31 | accuracy 61.845 | uer 16.054 | wer 17.732 | raw_wer 17.732 | bleu 19.92 | wps 2294.8 | wpb 4003.4 | bsz 141.8 | num_updates 47149 | best_bleu 20.2
2023-08-07 23:17:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 47149 updates
2023-08-07 23:17:39 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.9206.pt
2023-08-07 23:17:43 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.9206.pt
2023-08-07 23:17:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_19.9206.pt (epoch 32 @ 47149 updates, score 19.92) (writing took 18.685671415179968 seconds)
2023-08-07 23:17:59 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-08-07 23:17:59 | INFO | train | epoch 032 | loss 3.998 | trans_loss 5.216 | nll_loss 2.495 | w2v_ctc_loss 1.157 | contrastive_loss 0 | total 4138.93 | n_correct 2740.18 | ppl 5.64 | accuracy 66.205 | wps 7055.9 | ups 1.7 | wpb 4138.9 | bsz 152.9 | num_updates 47149 | lr 6.51297e-05 | gnorm 1.119 | clip 0 | loss_scale 64 | train_wall 762 | gb_free 16.7 | wall 33302
2023-08-07 23:17:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 23:17:59 | INFO | fairseq.trainer | begin training epoch 33
2023-08-07 23:17:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 23:18:34 | INFO | train_inner | epoch 033:     51 / 1474 loss=3.991, trans_loss=5.205, nll_loss=2.483, w2v_ctc_loss=1.157, contrastive_loss=0, total=4156.71, n_correct=2761.23, ppl=5.59, accuracy=66.428, wps=4086.1, ups=0.98, wpb=4156.7, bsz=161.2, num_updates=47200, lr=6.50945e-05, gnorm=1.119, clip=0, loss_scale=64, train_wall=52, gb_free=16.7, wall=33337
2023-08-07 23:19:26 | INFO | train_inner | epoch 033:    151 / 1474 loss=3.953, trans_loss=5.171, nll_loss=2.436, w2v_ctc_loss=1.113, contrastive_loss=0, total=4071.44, n_correct=2726.4, ppl=5.41, accuracy=66.964, wps=7867, ups=1.93, wpb=4071.4, bsz=142.1, num_updates=47300, lr=6.50256e-05, gnorm=1.111, clip=0, loss_scale=64, train_wall=51, gb_free=16.3, wall=33389
2023-08-07 23:20:18 | INFO | train_inner | epoch 033:    251 / 1474 loss=3.956, trans_loss=5.169, nll_loss=2.436, w2v_ctc_loss=1.125, contrastive_loss=0, total=4281.28, n_correct=2871.61, ppl=5.41, accuracy=67.074, wps=8174.7, ups=1.91, wpb=4281.3, bsz=173.2, num_updates=47400, lr=6.4957e-05, gnorm=1.095, clip=0, loss_scale=64, train_wall=52, gb_free=16.7, wall=33441
2023-08-07 23:21:10 | INFO | train_inner | epoch 033:    351 / 1474 loss=3.982, trans_loss=5.195, nll_loss=2.468, w2v_ctc_loss=1.151, contrastive_loss=0, total=4111.69, n_correct=2736.94, ppl=5.53, accuracy=66.565, wps=7944.6, ups=1.93, wpb=4111.7, bsz=149.2, num_updates=47500, lr=6.48886e-05, gnorm=1.12, clip=0, loss_scale=64, train_wall=51, gb_free=17, wall=33493
2023-08-07 23:22:01 | INFO | train_inner | epoch 033:    451 / 1474 loss=3.953, trans_loss=5.169, nll_loss=2.434, w2v_ctc_loss=1.116, contrastive_loss=0, total=4147.28, n_correct=2778.73, ppl=5.4, accuracy=67.001, wps=8023.8, ups=1.93, wpb=4147.3, bsz=156.7, num_updates=47600, lr=6.48204e-05, gnorm=1.11, clip=0, loss_scale=64, train_wall=51, gb_free=16.8, wall=33545
2023-08-07 23:22:54 | INFO | train_inner | epoch 033:    551 / 1474 loss=3.991, trans_loss=5.203, nll_loss=2.478, w2v_ctc_loss=1.161, contrastive_loss=0, total=4127.68, n_correct=2743.16, ppl=5.57, accuracy=66.458, wps=7901.1, ups=1.91, wpb=4127.7, bsz=146.3, num_updates=47700, lr=6.47524e-05, gnorm=1.126, clip=0, loss_scale=64, train_wall=52, gb_free=15.8, wall=33597
2023-08-07 23:23:46 | INFO | train_inner | epoch 033:    651 / 1474 loss=4, trans_loss=5.216, nll_loss=2.495, w2v_ctc_loss=1.163, contrastive_loss=0, total=4164.1, n_correct=2758.22, ppl=5.64, accuracy=66.238, wps=8021.1, ups=1.93, wpb=4164.1, bsz=151.2, num_updates=47800, lr=6.46846e-05, gnorm=1.123, clip=0, loss_scale=64, train_wall=51, gb_free=15.5, wall=33649
2023-08-07 23:24:38 | INFO | train_inner | epoch 033:    751 / 1474 loss=4.005, trans_loss=5.215, nll_loss=2.493, w2v_ctc_loss=1.18, contrastive_loss=0, total=4064.29, n_correct=2694.56, ppl=5.63, accuracy=66.298, wps=7812.6, ups=1.92, wpb=4064.3, bsz=142.9, num_updates=47900, lr=6.46171e-05, gnorm=1.146, clip=0, loss_scale=64, train_wall=52, gb_free=16.8, wall=33701
2023-08-07 23:25:29 | INFO | train_inner | epoch 033:    851 / 1474 loss=3.976, trans_loss=5.194, nll_loss=2.468, w2v_ctc_loss=1.132, contrastive_loss=0, total=4141.12, n_correct=2759.32, ppl=5.53, accuracy=66.632, wps=8062.5, ups=1.95, wpb=4141.1, bsz=159.3, num_updates=48000, lr=6.45497e-05, gnorm=1.115, clip=0, loss_scale=64, train_wall=51, gb_free=16.9, wall=33752
2023-08-07 23:25:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 23:25:51 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 4.338 | trans_loss 5.592 | nll_loss 2.891 | w2v_ctc_loss 1.411 | contrastive_loss 0 | total 4003.4 | n_correct 2466.1 | ppl 7.42 | accuracy 61.6 | uer 16.181 | wer 17.937 | raw_wer 17.937 | bleu 19.6 | wps 2365.1 | wpb 4003.4 | bsz 141.8 | num_updates 48000 | best_bleu 20.2
2023-08-07 23:25:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48000 updates
2023-08-07 23:25:51 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_33_48000.pt
2023-08-07 23:25:55 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_33_48000.pt
2023-08-07 23:26:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_33_48000.pt (epoch 33 @ 48000 updates, score 19.6) (writing took 20.73246924765408 seconds)
2023-08-07 23:27:05 | INFO | train_inner | epoch 033:    951 / 1474 loss=3.991, trans_loss=5.204, nll_loss=2.48, w2v_ctc_loss=1.16, contrastive_loss=0, total=4147.76, n_correct=2756.69, ppl=5.58, accuracy=66.462, wps=4317.3, ups=1.04, wpb=4147.8, bsz=154.1, num_updates=48100, lr=6.44826e-05, gnorm=1.122, clip=0, loss_scale=128, train_wall=52, gb_free=17.7, wall=33848
2023-08-07 23:27:58 | INFO | train_inner | epoch 033:   1051 / 1474 loss=3.989, trans_loss=5.206, nll_loss=2.484, w2v_ctc_loss=1.148, contrastive_loss=0, total=4137.41, n_correct=2747.56, ppl=5.59, accuracy=66.408, wps=7885.2, ups=1.91, wpb=4137.4, bsz=154, num_updates=48200, lr=6.44157e-05, gnorm=1.116, clip=0, loss_scale=128, train_wall=52, gb_free=16, wall=33901
2023-08-07 23:28:50 | INFO | train_inner | epoch 033:   1151 / 1474 loss=3.997, trans_loss=5.221, nll_loss=2.502, w2v_ctc_loss=1.142, contrastive_loss=0, total=4182.88, n_correct=2764.78, ppl=5.66, accuracy=66.098, wps=7967.3, ups=1.9, wpb=4182.9, bsz=154.3, num_updates=48300, lr=6.43489e-05, gnorm=1.127, clip=0, loss_scale=128, train_wall=52, gb_free=16.4, wall=33953
2023-08-07 23:29:42 | INFO | train_inner | epoch 033:   1251 / 1474 loss=4, trans_loss=5.215, nll_loss=2.494, w2v_ctc_loss=1.164, contrastive_loss=0, total=4102.27, n_correct=2714.52, ppl=5.63, accuracy=66.171, wps=7919.4, ups=1.93, wpb=4102.3, bsz=145.9, num_updates=48400, lr=6.42824e-05, gnorm=1.137, clip=0, loss_scale=128, train_wall=51, gb_free=15.9, wall=34005
2023-08-07 23:30:34 | INFO | train_inner | epoch 033:   1351 / 1474 loss=3.996, trans_loss=5.211, nll_loss=2.491, w2v_ctc_loss=1.16, contrastive_loss=0, total=4131.08, n_correct=2738.05, ppl=5.62, accuracy=66.279, wps=7942.4, ups=1.92, wpb=4131.1, bsz=157, num_updates=48500, lr=6.42161e-05, gnorm=1.116, clip=0, loss_scale=128, train_wall=52, gb_free=17.3, wall=34057
2023-08-07 23:30:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-07 23:31:27 | INFO | train_inner | epoch 033:   1452 / 1474 loss=3.995, trans_loss=5.217, nll_loss=2.499, w2v_ctc_loss=1.143, contrastive_loss=0, total=4126.51, n_correct=2732.74, ppl=5.65, accuracy=66.224, wps=7760.6, ups=1.88, wpb=4126.5, bsz=154.9, num_updates=48600, lr=6.415e-05, gnorm=1.125, clip=0, loss_scale=64, train_wall=53, gb_free=16.6, wall=34110
2023-08-07 23:31:38 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 23:32:01 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 4.322 | trans_loss 5.585 | nll_loss 2.869 | w2v_ctc_loss 1.374 | contrastive_loss 0 | total 4003.4 | n_correct 2474.3 | ppl 7.31 | accuracy 61.805 | uer 15.988 | wer 17.725 | raw_wer 17.725 | bleu 20.12 | wps 2367.6 | wpb 4003.4 | bsz 141.8 | num_updates 48622 | best_bleu 20.2
2023-08-07 23:32:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48622 updates
2023-08-07 23:32:01 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_20.1203.pt
2023-08-07 23:32:04 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_20.1203.pt
2023-08-07 23:32:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint.best_bleu_20.1203.pt (epoch 33 @ 48622 updates, score 20.12) (writing took 21.293660851195455 seconds)
2023-08-07 23:32:22 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-08-07 23:32:22 | INFO | train | epoch 033 | loss 3.984 | trans_loss 5.2 | nll_loss 2.475 | w2v_ctc_loss 1.148 | contrastive_loss 0 | total 4138.53 | n_correct 2752.2 | ppl 5.56 | accuracy 66.502 | wps 7058 | ups 1.71 | wpb 4138.5 | bsz 152.8 | num_updates 48622 | lr 6.41355e-05 | gnorm 1.121 | clip 0 | loss_scale 64 | train_wall 761 | gb_free 17.9 | wall 34166
2023-08-07 23:32:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-07 23:32:23 | INFO | fairseq.trainer | begin training epoch 34
2023-08-07 23:32:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-07 23:33:11 | INFO | train_inner | epoch 034:     78 / 1474 loss=3.954, trans_loss=5.164, nll_loss=2.428, w2v_ctc_loss=1.132, contrastive_loss=0, total=4128.94, n_correct=2770.31, ppl=5.38, accuracy=67.095, wps=3974.7, ups=0.96, wpb=4128.9, bsz=151.1, num_updates=48700, lr=6.40841e-05, gnorm=1.124, clip=0, loss_scale=64, train_wall=51, gb_free=15.4, wall=34214
2023-08-07 23:34:03 | INFO | train_inner | epoch 034:    178 / 1474 loss=3.951, trans_loss=5.156, nll_loss=2.417, w2v_ctc_loss=1.138, contrastive_loss=0, total=4071.22, n_correct=2735.5, ppl=5.34, accuracy=67.191, wps=7799.4, ups=1.92, wpb=4071.2, bsz=147.6, num_updates=48800, lr=6.40184e-05, gnorm=1.121, clip=0, loss_scale=64, train_wall=52, gb_free=15.7, wall=34266
2023-08-07 23:34:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-07 23:34:56 | INFO | train_inner | epoch 034:    279 / 1474 loss=3.965, trans_loss=5.18, nll_loss=2.45, w2v_ctc_loss=1.13, contrastive_loss=0, total=4233.41, n_correct=2830.04, ppl=5.46, accuracy=66.85, wps=8081.2, ups=1.91, wpb=4233.4, bsz=162.4, num_updates=48900, lr=6.39529e-05, gnorm=1.102, clip=0, loss_scale=32, train_wall=52, gb_free=17.8, wall=34319
2023-08-07 23:35:47 | INFO | train_inner | epoch 034:    379 / 1474 loss=3.951, trans_loss=5.16, nll_loss=2.423, w2v_ctc_loss=1.128, contrastive_loss=0, total=4156.17, n_correct=2790.84, ppl=5.36, accuracy=67.149, wps=8002.7, ups=1.93, wpb=4156.2, bsz=158.4, num_updates=49000, lr=6.38877e-05, gnorm=1.122, clip=0, loss_scale=32, train_wall=51, gb_free=17.8, wall=34371
2023-08-07 23:36:40 | INFO | train_inner | epoch 034:    479 / 1474 loss=3.974, trans_loss=5.183, nll_loss=2.451, w2v_ctc_loss=1.153, contrastive_loss=0, total=4070.55, n_correct=2717.79, ppl=5.47, accuracy=66.767, wps=7816.3, ups=1.92, wpb=4070.6, bsz=142.3, num_updates=49100, lr=6.38226e-05, gnorm=1.129, clip=0, loss_scale=32, train_wall=52, gb_free=17.5, wall=34423
2023-08-07 23:37:31 | INFO | train_inner | epoch 034:    579 / 1474 loss=3.957, trans_loss=5.167, nll_loss=2.431, w2v_ctc_loss=1.132, contrastive_loss=0, total=4119.38, n_correct=2763.38, ppl=5.39, accuracy=67.082, wps=7994.4, ups=1.94, wpb=4119.4, bsz=150.1, num_updates=49200, lr=6.37577e-05, gnorm=1.124, clip=0, loss_scale=32, train_wall=51, gb_free=13.4, wall=34474
2023-08-07 23:38:23 | INFO | train_inner | epoch 034:    679 / 1474 loss=3.956, trans_loss=5.17, nll_loss=2.436, w2v_ctc_loss=1.122, contrastive_loss=0, total=4124.83, n_correct=2761.44, ppl=5.41, accuracy=66.947, wps=7946.4, ups=1.93, wpb=4124.8, bsz=150.1, num_updates=49300, lr=6.3693e-05, gnorm=1.118, clip=0, loss_scale=32, train_wall=51, gb_free=14.5, wall=34526
2023-08-07 23:39:15 | INFO | train_inner | epoch 034:    779 / 1474 loss=3.98, trans_loss=5.201, nll_loss=2.475, w2v_ctc_loss=1.131, contrastive_loss=0, total=4082.07, n_correct=2711.02, ppl=5.56, accuracy=66.413, wps=7917.4, ups=1.94, wpb=4082.1, bsz=147.5, num_updates=49400, lr=6.36285e-05, gnorm=1.126, clip=0, loss_scale=32, train_wall=51, gb_free=15.7, wall=34578
2023-08-07 23:40:07 | INFO | train_inner | epoch 034:    879 / 1474 loss=3.981, trans_loss=5.196, nll_loss=2.47, w2v_ctc_loss=1.146, contrastive_loss=0, total=4100.9, n_correct=2728.27, ppl=5.54, accuracy=66.529, wps=7845.1, ups=1.91, wpb=4100.9, bsz=148.3, num_updates=49500, lr=6.35642e-05, gnorm=1.136, clip=0, loss_scale=32, train_wall=52, gb_free=12.5, wall=34630
2023-08-07 23:40:59 | INFO | train_inner | epoch 034:    979 / 1474 loss=3.975, trans_loss=5.189, nll_loss=2.461, w2v_ctc_loss=1.14, contrastive_loss=0, total=4168.39, n_correct=2778.29, ppl=5.51, accuracy=66.651, wps=7973.4, ups=1.91, wpb=4168.4, bsz=156, num_updates=49600, lr=6.35001e-05, gnorm=1.124, clip=0, loss_scale=32, train_wall=52, gb_free=16, wall=34682
2023-08-07 23:41:51 | INFO | train_inner | epoch 034:   1079 / 1474 loss=3.988, trans_loss=5.197, nll_loss=2.471, w2v_ctc_loss=1.166, contrastive_loss=0, total=4150.57, n_correct=2762.6, ppl=5.55, accuracy=66.56, wps=8062.7, ups=1.94, wpb=4150.6, bsz=154.2, num_updates=49700, lr=6.34361e-05, gnorm=1.137, clip=0, loss_scale=32, train_wall=51, gb_free=16.9, wall=34734
2023-08-07 23:42:42 | INFO | train_inner | epoch 034:   1179 / 1474 loss=3.98, trans_loss=5.195, nll_loss=2.468, w2v_ctc_loss=1.143, contrastive_loss=0, total=4098.77, n_correct=2729.35, ppl=5.53, accuracy=66.589, wps=7899.1, ups=1.93, wpb=4098.8, bsz=148.6, num_updates=49800, lr=6.33724e-05, gnorm=1.119, clip=0, loss_scale=32, train_wall=51, gb_free=17, wall=34786
2023-08-07 23:43:35 | INFO | train_inner | epoch 034:   1279 / 1474 loss=3.974, trans_loss=5.19, nll_loss=2.462, w2v_ctc_loss=1.136, contrastive_loss=0, total=4150.54, n_correct=2763.17, ppl=5.51, accuracy=66.574, wps=7970, ups=1.92, wpb=4150.5, bsz=150.5, num_updates=49900, lr=6.33089e-05, gnorm=1.114, clip=0, loss_scale=32, train_wall=52, gb_free=17.2, wall=34838
2023-08-07 23:44:27 | INFO | train_inner | epoch 034:   1379 / 1474 loss=3.987, trans_loss=5.202, nll_loss=2.479, w2v_ctc_loss=1.149, contrastive_loss=0, total=4196.91, n_correct=2785.97, ppl=5.57, accuracy=66.381, wps=7971.1, ups=1.9, wpb=4196.9, bsz=160.7, num_updates=50000, lr=6.32456e-05, gnorm=1.116, clip=0, loss_scale=32, train_wall=52, gb_free=16.2, wall=34890
2023-08-07 23:44:27 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2023-08-07 23:44:27 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-07 23:44:49 | INFO | dev_st | epoch 034 | valid on 'dev_st' subset | loss 4.334 | trans_loss 5.58 | nll_loss 2.864 | w2v_ctc_loss 1.427 | contrastive_loss 0 | total 4003.4 | n_correct 2481 | ppl 7.28 | accuracy 61.972 | uer 16.051 | wer 17.926 | raw_wer 17.926 | bleu 19.98 | wps 2386.6 | wpb 4003.4 | bsz 141.8 | num_updates 50000 | best_bleu 20.2
2023-08-07 23:44:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 50000 updates
2023-08-07 23:44:49 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_34_50000.pt
2023-08-07 23:44:53 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_34_50000.pt
2023-08-07 23:45:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0807_st/checkpoint_34_50000.pt (epoch 34 @ 50000 updates, score 19.98) (writing took 39.99753053486347 seconds)
2023-08-07 23:45:30 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2023-08-07 23:45:30 | INFO | train | epoch 034 | loss 3.969 | trans_loss 5.182 | nll_loss 2.451 | w2v_ctc_loss 1.139 | contrastive_loss 0 | total 4132.36 | n_correct 2759.34 | ppl 5.47 | accuracy 66.774 | wps 7233 | ups 1.75 | wpb 4132.4 | bsz 152 | num_updates 50000 | lr 6.32456e-05 | gnorm 1.122 | clip 0 | loss_scale 32 | train_wall 710 | gb_free 16.2 | wall 34953
2023-08-07 23:45:30 | INFO | fairseq_cli.train | done training in 34878.1 seconds
Exception in thread Thread-7:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-6:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    raise EOFError
EOFError
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    raise EOFError
EOFError
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1472 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
