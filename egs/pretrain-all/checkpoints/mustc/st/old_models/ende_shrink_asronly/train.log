2023-07-05 07:09:56 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:13192
2023-07-05 07:09:56 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:13192
2023-07-05 07:09:56 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:13192
2023-07-05 07:09:56 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:13192
2023-07-05 07:09:56 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:13192
2023-07-05 07:09:56 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:13192
2023-07-05 07:09:56 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:13192
2023-07-05 07:09:56 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:13192
2023-07-05 07:09:56 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-05 07:09:56 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-05 07:09:56 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-05 07:09:56 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-05 07:09:56 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-05 07:09:56 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-05 07:09:57 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-05 07:09:57 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-05 07:09:57 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 07:09:57 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-05 07:09:57 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 07:09:57 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-05 07:09:57 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 07:09:57 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 07:09:57 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 07:09:57 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 07:09:57 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 07:09:57 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-05 07:09:57 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-05 07:09:57 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-05 07:09:57 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-05 07:09:57 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-05 07:09:57 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 07:09:57 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-05 07:09:59 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_asronly', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13192', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 30000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_asronly', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=-1, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=30000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_asronly', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_asronly', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=30000, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=-1, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=30000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_asronly', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_asronly', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=30000, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=-1, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=30000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_asronly', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_asronly', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=30000, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-05 07:09:59 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-05 07:09:59 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-05 07:09:59 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-05 07:09:59 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-05 07:09:59 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-05 07:10:04 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-05 07:10:04 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-05 07:10:04 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-05 07:10:05 | INFO | root | load pretrained hubert
2023-07-05 07:10:08 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-05 07:10:09 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-05 07:10:11 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-05 07:10:11 | INFO | root | share the sematic adapter and textual encoder
2023-07-05 07:10:11 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-05 07:10:11 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-05 07:10:11 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-05 07:10:11 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJoint
2023-07-05 07:10:11 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-05 07:10:11 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-05 07:10:11 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-05 07:10:11 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-0.12.3/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-05 07:10:11 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-0.12.3/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-05 07:10:11 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-05 07:10:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-05 07:10:21 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-05 07:10:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-05 07:10:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-05 07:10:21 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 07:10:21 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 07:10:21 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 07:10:21 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 07:10:21 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 07:10:21 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 07:10:21 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 07:10:21 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 07:10:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-05 07:10:21 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-05 07:10:21 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-05 07:10:21 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 07:10:21 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 07:10:21 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-05 07:10:21 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-05 07:10:21 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-0.12.3/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-05 07:10:21 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-0.12.3/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-05 07:10:23 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-05 07:10:25 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-05 07:11:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 07:11:08 | INFO | fairseq.trainer | begin training epoch 1
2023-07-05 07:11:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 07:11:57 | INFO | train_inner | epoch 001:    100 / 1474 loss=26.737, trans_loss=6.008, nll_loss=5.482, w2v_ctc_loss=34.664, contrastive_loss=0, total=4207.04, n_correct=201.94, ppl=44.69, accuracy=4.8, wps=20368, ups=2.44, wpb=8344, bsz=314.3, num_updates=100, lr=4.098e-06, gnorm=1.316, clip=0, loss_scale=128, train_wall=43, gb_free=19.5, wall=96
2023-07-05 07:12:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-05 07:12:39 | INFO | train_inner | epoch 001:    201 / 1474 loss=23.486, trans_loss=6.007, nll_loss=5.481, w2v_ctc_loss=29.661, contrastive_loss=0, total=4121.36, n_correct=200.65, ppl=44.68, accuracy=4.869, wps=19890.1, ups=2.43, wpb=8184.2, bsz=307.6, num_updates=200, lr=8.096e-06, gnorm=5.7, clip=13, loss_scale=64, train_wall=41, gb_free=19.3, wall=137
2023-07-05 07:13:19 | INFO | train_inner | epoch 001:    301 / 1474 loss=12.918, trans_loss=6.03, nll_loss=5.51, w2v_ctc_loss=13.379, contrastive_loss=0, total=4079.62, n_correct=192.87, ppl=45.58, accuracy=4.728, wps=19851.2, ups=2.45, wpb=8107, bsz=292.2, num_updates=300, lr=1.2094e-05, gnorm=7.013, clip=7, loss_scale=64, train_wall=40, gb_free=20, wall=178
2023-07-05 07:14:00 | INFO | train_inner | epoch 001:    401 / 1474 loss=10.978, trans_loss=6.02, nll_loss=5.5, w2v_ctc_loss=10.406, contrastive_loss=0, total=4174.14, n_correct=184.64, ppl=45.27, accuracy=4.423, wps=20597.2, ups=2.48, wpb=8289.4, bsz=307, num_updates=400, lr=1.6092e-05, gnorm=4.47, clip=0, loss_scale=64, train_wall=40, gb_free=19, wall=218
2023-07-05 07:14:40 | INFO | train_inner | epoch 001:    501 / 1474 loss=10.313, trans_loss=5.95, nll_loss=5.432, w2v_ctc_loss=9.458, contrastive_loss=0, total=4176.18, n_correct=194.09, ppl=43.17, accuracy=4.648, wps=20675.9, ups=2.49, wpb=8303.5, bsz=318.2, num_updates=500, lr=2.009e-05, gnorm=2.096, clip=0, loss_scale=64, train_wall=40, gb_free=19.2, wall=259
2023-07-05 07:15:20 | INFO | train_inner | epoch 001:    601 / 1474 loss=9.94, trans_loss=5.946, nll_loss=5.443, w2v_ctc_loss=8.889, contrastive_loss=0, total=4147.79, n_correct=198.84, ppl=43.5, accuracy=4.794, wps=20441.1, ups=2.49, wpb=8223.8, bsz=322.8, num_updates=600, lr=2.4088e-05, gnorm=0.973, clip=0, loss_scale=64, train_wall=40, gb_free=19, wall=299
2023-07-05 07:16:00 | INFO | train_inner | epoch 001:    701 / 1474 loss=9.797, trans_loss=5.96, nll_loss=5.479, w2v_ctc_loss=8.653, contrastive_loss=0, total=4152.1, n_correct=210.61, ppl=44.61, accuracy=5.072, wps=20681.1, ups=2.51, wpb=8243.4, bsz=304.2, num_updates=700, lr=2.8086e-05, gnorm=0.593, clip=0, loss_scale=64, train_wall=39, gb_free=19.6, wall=339
2023-07-05 07:16:40 | INFO | train_inner | epoch 001:    801 / 1474 loss=9.523, trans_loss=5.991, nll_loss=5.521, w2v_ctc_loss=8.201, contrastive_loss=0, total=4123.83, n_correct=215.6, ppl=45.92, accuracy=5.228, wps=20577.4, ups=2.51, wpb=8182.2, bsz=309.4, num_updates=800, lr=3.2084e-05, gnorm=0.682, clip=0, loss_scale=64, train_wall=39, gb_free=19.2, wall=378
2023-07-05 07:17:20 | INFO | train_inner | epoch 001:    901 / 1474 loss=9.198, trans_loss=6, nll_loss=5.535, w2v_ctc_loss=7.689, contrastive_loss=0, total=4163.61, n_correct=221.14, ppl=46.36, accuracy=5.311, wps=20779, ups=2.51, wpb=8270.3, bsz=304.7, num_updates=900, lr=3.6082e-05, gnorm=0.735, clip=0, loss_scale=64, train_wall=39, gb_free=18.9, wall=418
2023-07-05 07:18:00 | INFO | train_inner | epoch 001:   1001 / 1474 loss=8.845, trans_loss=6.015, nll_loss=5.549, w2v_ctc_loss=7.131, contrastive_loss=0, total=4135.34, n_correct=224.54, ppl=46.81, accuracy=5.43, wps=20558.8, ups=2.5, wpb=8217.9, bsz=304.5, num_updates=1000, lr=4.008e-05, gnorm=0.831, clip=0, loss_scale=64, train_wall=40, gb_free=19.1, wall=458
2023-07-05 07:18:39 | INFO | train_inner | epoch 001:   1101 / 1474 loss=8.56, trans_loss=6.029, nll_loss=5.553, w2v_ctc_loss=6.675, contrastive_loss=0, total=4147.38, n_correct=228.99, ppl=46.95, accuracy=5.521, wps=20724.6, ups=2.52, wpb=8219.6, bsz=303.2, num_updates=1100, lr=4.4078e-05, gnorm=0.899, clip=0, loss_scale=64, train_wall=39, gb_free=18.9, wall=498
2023-07-05 07:19:19 | INFO | train_inner | epoch 001:   1201 / 1474 loss=8.316, trans_loss=6.014, nll_loss=5.533, w2v_ctc_loss=6.314, contrastive_loss=0, total=4139.9, n_correct=236.27, ppl=46.3, accuracy=5.707, wps=20769.9, ups=2.52, wpb=8226.6, bsz=293.4, num_updates=1200, lr=4.8076e-05, gnorm=1.038, clip=0, loss_scale=64, train_wall=39, gb_free=19, wall=538
2023-07-05 07:19:58 | INFO | train_inner | epoch 001:   1301 / 1474 loss=8.114, trans_loss=6.021, nll_loss=5.534, w2v_ctc_loss=5.998, contrastive_loss=0, total=4046.58, n_correct=232.51, ppl=46.33, accuracy=5.746, wps=20313.9, ups=2.53, wpb=8035, bsz=292.8, num_updates=1300, lr=5.2074e-05, gnorm=0.983, clip=0, loss_scale=64, train_wall=39, gb_free=19.8, wall=577
2023-07-05 07:20:38 | INFO | train_inner | epoch 001:   1401 / 1474 loss=7.909, trans_loss=6.001, nll_loss=5.504, w2v_ctc_loss=5.704, contrastive_loss=0, total=4133.18, n_correct=239.76, ppl=45.39, accuracy=5.801, wps=20602.2, ups=2.51, wpb=8216.8, bsz=303.3, num_updates=1400, lr=5.6072e-05, gnorm=1.165, clip=0, loss_scale=64, train_wall=40, gb_free=20, wall=617
2023-07-05 07:21:07 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2023-07-05 07:21:22 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 9.772 | trans_loss 11.873 | nll_loss 10.846 | w2v_ctc_loss 4.869 | contrastive_loss 0 | total 4003.4 | n_correct 269.8 | ppl 1840.72 | accuracy 6.739 | uer 62.182 | wer 60.482 | raw_wer 60.482 | wps 4663.6 | wpb 4003.4 | bsz 141.8 | num_updates 1473
2023-07-05 07:21:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1473 updates
2023-07-05 07:21:22 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_best.pt
2023-07-05 07:21:25 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_best.pt
2023-07-05 07:21:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_best.pt (epoch 1 @ 1473 updates, score 9.772) (writing took 3.622630058787763 seconds)
2023-07-05 07:21:26 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-07-05 07:21:26 | INFO | train | epoch 001 | loss 11.577 | trans_loss 6 | nll_loss 5.504 | w2v_ctc_loss 11.35 | contrastive_loss 0 | total 4138.36 | n_correct 214.292 | ppl 45.38 | accuracy 5.178 | wps 19858.7 | ups 2.42 | wpb 8216.6 | bsz 305.6 | num_updates 1473 | lr 5.89905e-05 | gnorm 1.998 | clip 1.4 | loss_scale 64 | train_wall 587 | gb_free 19.2 | wall 665
2023-07-05 07:21:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 07:21:26 | INFO | fairseq.trainer | begin training epoch 2
2023-07-05 07:21:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 07:21:44 | INFO | train_inner | epoch 002:     27 / 1474 loss=7.746, trans_loss=6.009, nll_loss=5.505, w2v_ctc_loss=5.45, contrastive_loss=0, total=4162.95, n_correct=239.61, ppl=45.42, accuracy=5.756, wps=12524.1, ups=1.52, wpb=8253.9, bsz=313.9, num_updates=1500, lr=6.007e-05, gnorm=1.298, clip=0, loss_scale=64, train_wall=39, gb_free=19.7, wall=683
2023-07-05 07:22:24 | INFO | train_inner | epoch 002:    127 / 1474 loss=7.65, trans_loss=6.014, nll_loss=5.508, w2v_ctc_loss=5.291, contrastive_loss=0, total=4155.98, n_correct=240.86, ppl=45.5, accuracy=5.796, wps=20699.5, ups=2.51, wpb=8238.9, bsz=301.1, num_updates=1600, lr=6.4068e-05, gnorm=1.147, clip=0, loss_scale=64, train_wall=39, gb_free=18.9, wall=723
2023-07-05 07:23:03 | INFO | train_inner | epoch 002:    227 / 1474 loss=7.474, trans_loss=5.987, nll_loss=5.477, w2v_ctc_loss=5.051, contrastive_loss=0, total=4179.21, n_correct=236.93, ppl=44.55, accuracy=5.669, wps=21024.8, ups=2.53, wpb=8305.4, bsz=325.9, num_updates=1700, lr=6.8066e-05, gnorm=1.188, clip=0, loss_scale=64, train_wall=39, gb_free=19, wall=762
2023-07-05 07:23:43 | INFO | train_inner | epoch 002:    327 / 1474 loss=7.413, trans_loss=6.003, nll_loss=5.49, w2v_ctc_loss=4.941, contrastive_loss=0, total=4146.1, n_correct=239.98, ppl=44.95, accuracy=5.788, wps=20711.4, ups=2.52, wpb=8228, bsz=298.5, num_updates=1800, lr=7.2064e-05, gnorm=1.169, clip=0, loss_scale=64, train_wall=39, gb_free=18.8, wall=802
2023-07-05 07:24:23 | INFO | train_inner | epoch 002:    427 / 1474 loss=7.317, trans_loss=5.973, nll_loss=5.457, w2v_ctc_loss=4.824, contrastive_loss=0, total=4037.99, n_correct=232.54, ppl=43.93, accuracy=5.759, wps=20429.2, ups=2.54, wpb=8031.2, bsz=276.9, num_updates=1900, lr=7.6062e-05, gnorm=1.087, clip=0, loss_scale=64, train_wall=39, gb_free=19, wall=841
2023-07-05 07:25:02 | INFO | train_inner | epoch 002:    527 / 1474 loss=7.208, trans_loss=5.973, nll_loss=5.451, w2v_ctc_loss=4.659, contrastive_loss=0, total=4176.97, n_correct=240.54, ppl=43.73, accuracy=5.759, wps=20968.5, ups=2.53, wpb=8286.5, bsz=312.4, num_updates=2000, lr=8.006e-05, gnorm=1.117, clip=0, loss_scale=64, train_wall=39, gb_free=19.7, wall=881
2023-07-05 07:25:02 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 07:25:17 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 9.462 | trans_loss 11.775 | nll_loss 10.69 | w2v_ctc_loss 4.065 | contrastive_loss 0 | total 4003.4 | n_correct 264.7 | ppl 1651.69 | accuracy 6.612 | uer 55.339 | wer 53.645 | raw_wer 53.645 | wps 4824.4 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_loss 9.462
2023-07-05 07:25:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-07-05 07:25:17 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_2_2000.pt
2023-07-05 07:25:19 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_2_2000.pt
2023-07-05 07:25:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 9.462) (writing took 8.841609944123775 seconds)
2023-07-05 07:26:06 | INFO | train_inner | epoch 002:    627 / 1474 loss=7.132, trans_loss=5.97, nll_loss=5.445, w2v_ctc_loss=4.541, contrastive_loss=0, total=4126.49, n_correct=238.94, ppl=43.57, accuracy=5.79, wps=12883.2, ups=1.57, wpb=8188.4, bsz=297, num_updates=2100, lr=8.4058e-05, gnorm=1.184, clip=0, loss_scale=64, train_wall=39, gb_free=19.2, wall=944
2023-07-05 07:26:45 | INFO | train_inner | epoch 002:    727 / 1474 loss=7.062, trans_loss=5.934, nll_loss=5.403, w2v_ctc_loss=4.472, contrastive_loss=0, total=4149.06, n_correct=239.76, ppl=42.3, accuracy=5.779, wps=20708.7, ups=2.51, wpb=8237.3, bsz=310.2, num_updates=2200, lr=8.8056e-05, gnorm=1.145, clip=0, loss_scale=64, train_wall=39, gb_free=19.3, wall=984
2023-07-05 07:27:25 | INFO | train_inner | epoch 002:    827 / 1474 loss=7.029, trans_loss=5.947, nll_loss=5.417, w2v_ctc_loss=4.407, contrastive_loss=0, total=4175.4, n_correct=243.34, ppl=42.72, accuracy=5.828, wps=21019.7, ups=2.53, wpb=8296.5, bsz=307.2, num_updates=2300, lr=9.2054e-05, gnorm=0.956, clip=0, loss_scale=128, train_wall=39, gb_free=19.9, wall=1024
2023-07-05 07:28:05 | INFO | train_inner | epoch 002:    927 / 1474 loss=6.974, trans_loss=5.967, nll_loss=5.436, w2v_ctc_loss=4.303, contrastive_loss=0, total=4104.2, n_correct=241.98, ppl=43.3, accuracy=5.896, wps=20456.6, ups=2.51, wpb=8148.9, bsz=297.3, num_updates=2400, lr=9.6052e-05, gnorm=1.051, clip=0, loss_scale=128, train_wall=39, gb_free=19, wall=1063
2023-07-05 07:28:45 | INFO | train_inner | epoch 002:   1027 / 1474 loss=6.928, trans_loss=5.966, nll_loss=5.437, w2v_ctc_loss=4.236, contrastive_loss=0, total=4102.5, n_correct=238.84, ppl=43.33, accuracy=5.822, wps=20412.4, ups=2.5, wpb=8148.7, bsz=304.2, num_updates=2500, lr=0.00010005, gnorm=1.01, clip=0, loss_scale=128, train_wall=40, gb_free=19.3, wall=1103
2023-07-05 07:29:24 | INFO | train_inner | epoch 002:   1127 / 1474 loss=6.876, trans_loss=5.975, nll_loss=5.444, w2v_ctc_loss=4.143, contrastive_loss=0, total=4187.61, n_correct=241.34, ppl=43.55, accuracy=5.763, wps=20876.2, ups=2.51, wpb=8308.1, bsz=324.8, num_updates=2600, lr=0.000104048, gnorm=0.92, clip=0, loss_scale=128, train_wall=39, gb_free=19.6, wall=1143
2023-07-05 07:30:04 | INFO | train_inner | epoch 002:   1227 / 1474 loss=6.847, trans_loss=5.966, nll_loss=5.433, w2v_ctc_loss=4.107, contrastive_loss=0, total=4221.06, n_correct=244.89, ppl=43.19, accuracy=5.802, wps=20950.3, ups=2.5, wpb=8375, bsz=328.5, num_updates=2700, lr=0.000108046, gnorm=0.863, clip=0, loss_scale=128, train_wall=40, gb_free=19.5, wall=1183
2023-07-05 07:30:44 | INFO | train_inner | epoch 002:   1327 / 1474 loss=6.831, trans_loss=5.984, nll_loss=5.455, w2v_ctc_loss=4.064, contrastive_loss=0, total=4157.86, n_correct=239.8, ppl=43.88, accuracy=5.767, wps=21024.8, ups=2.54, wpb=8267.7, bsz=307.1, num_updates=2800, lr=0.000112044, gnorm=0.817, clip=0, loss_scale=128, train_wall=39, gb_free=19.6, wall=1222
2023-07-05 07:31:23 | INFO | train_inner | epoch 002:   1427 / 1474 loss=6.815, trans_loss=5.995, nll_loss=5.466, w2v_ctc_loss=4.031, contrastive_loss=0, total=4054.34, n_correct=231.3, ppl=44.19, accuracy=5.705, wps=20379.8, ups=2.53, wpb=8052.6, bsz=292.5, num_updates=2900, lr=0.000116042, gnorm=0.932, clip=0, loss_scale=128, train_wall=39, gb_free=19.4, wall=1262
2023-07-05 07:31:42 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 07:31:57 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 9.251 | trans_loss 11.721 | nll_loss 10.607 | w2v_ctc_loss 3.489 | contrastive_loss 0 | total 4003.4 | n_correct 262.6 | ppl 1559.64 | accuracy 6.559 | uer 48.531 | wer 47.358 | raw_wer 47.358 | wps 4492.8 | wpb 4003.4 | bsz 141.8 | num_updates 2947 | best_loss 9.251
2023-07-05 07:31:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2947 updates
2023-07-05 07:31:57 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_best.pt
2023-07-05 07:32:01 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_best.pt
2023-07-05 07:32:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_best.pt (epoch 2 @ 2947 updates, score 9.251) (writing took 7.200721416156739 seconds)
2023-07-05 07:32:05 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-07-05 07:32:05 | INFO | train | epoch 002 | loss 7.112 | trans_loss 5.978 | nll_loss 5.454 | w2v_ctc_loss 4.505 | contrastive_loss 0 | total 4138.65 | n_correct 239.034 | ppl 43.84 | accuracy 5.776 | wps 18965.9 | ups 2.31 | wpb 8217.2 | bsz 305.7 | num_updates 2947 | lr 0.000117921 | gnorm 1.039 | clip 0 | loss_scale 128 | train_wall 579 | gb_free 19.3 | wall 1303
2023-07-05 07:32:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 07:32:05 | INFO | fairseq.trainer | begin training epoch 3
2023-07-05 07:32:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 07:32:33 | INFO | train_inner | epoch 003:     53 / 1474 loss=6.773, trans_loss=6.005, nll_loss=5.475, w2v_ctc_loss=3.953, contrastive_loss=0, total=4071.2, n_correct=234.14, ppl=44.48, accuracy=5.751, wps=11605.4, ups=1.44, wpb=8082.9, bsz=295, num_updates=3000, lr=0.00012004, gnorm=0.815, clip=0, loss_scale=128, train_wall=39, gb_free=19.1, wall=1332
2023-07-05 07:32:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-05 07:32:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-07-05 07:32:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-07-05 07:32:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-07-05 07:33:32 | INFO | train_inner | epoch 003:    157 / 1474 loss=8.458, trans_loss=5.925, nll_loss=5.388, w2v_ctc_loss=6.632, contrastive_loss=0, total=4143.63, n_correct=251, ppl=41.88, accuracy=6.057, wps=13866.2, ups=1.69, wpb=8227.9, bsz=304.8, num_updates=3100, lr=0.000124038, gnorm=1.683, clip=2, loss_scale=8, train_wall=59, gb_free=17.1, wall=1391
2023-07-05 07:34:30 | INFO | train_inner | epoch 003:    257 / 1474 loss=8.097, trans_loss=5.921, nll_loss=5.381, w2v_ctc_loss=6.083, contrastive_loss=0, total=4146.68, n_correct=254.87, ppl=41.66, accuracy=6.146, wps=14190.1, ups=1.72, wpb=8244.4, bsz=307.8, num_updates=3200, lr=0.000128036, gnorm=1.565, clip=0, loss_scale=8, train_wall=58, gb_free=15.4, wall=1449
2023-07-05 07:35:28 | INFO | train_inner | epoch 003:    357 / 1474 loss=8.215, trans_loss=5.958, nll_loss=5.421, w2v_ctc_loss=6.221, contrastive_loss=0, total=4171.72, n_correct=258.6, ppl=42.85, accuracy=6.199, wps=14381, ups=1.74, wpb=8277.2, bsz=312.1, num_updates=3300, lr=0.000132034, gnorm=1.666, clip=0, loss_scale=8, train_wall=57, gb_free=17.7, wall=1507
2023-07-05 07:36:26 | INFO | train_inner | epoch 003:    457 / 1474 loss=8.156, trans_loss=5.97, nll_loss=5.432, w2v_ctc_loss=6.12, contrastive_loss=0, total=4200.59, n_correct=263.05, ppl=43.16, accuracy=6.262, wps=14361.5, ups=1.72, wpb=8336.9, bsz=316.9, num_updates=3400, lr=0.000136032, gnorm=1.456, clip=0, loss_scale=8, train_wall=58, gb_free=12.8, wall=1565
2023-07-05 07:37:23 | INFO | train_inner | epoch 003:    557 / 1474 loss=8.009, trans_loss=6, nll_loss=5.465, w2v_ctc_loss=5.866, contrastive_loss=0, total=4093.13, n_correct=256.93, ppl=44.16, accuracy=6.277, wps=14175.2, ups=1.74, wpb=8135.6, bsz=293.4, num_updates=3500, lr=0.00014003, gnorm=1.376, clip=0, loss_scale=8, train_wall=57, gb_free=17.7, wall=1622
2023-07-05 07:38:22 | INFO | train_inner | epoch 003:    657 / 1474 loss=7.977, trans_loss=5.957, nll_loss=5.412, w2v_ctc_loss=5.858, contrastive_loss=0, total=4222.97, n_correct=269.1, ppl=42.57, accuracy=6.372, wps=14328.7, ups=1.71, wpb=8368.9, bsz=322.4, num_updates=3600, lr=0.000144028, gnorm=1.708, clip=0, loss_scale=8, train_wall=58, gb_free=17, wall=1681
2023-07-05 07:39:19 | INFO | train_inner | epoch 003:    757 / 1474 loss=8.048, trans_loss=5.93, nll_loss=5.387, w2v_ctc_loss=5.991, contrastive_loss=0, total=4164.5, n_correct=265.22, ppl=41.86, accuracy=6.369, wps=14415.3, ups=1.74, wpb=8273.9, bsz=313.7, num_updates=3700, lr=0.000148026, gnorm=1.971, clip=0, loss_scale=8, train_wall=57, gb_free=17, wall=1738
2023-07-05 07:40:17 | INFO | train_inner | epoch 003:    857 / 1474 loss=7.947, trans_loss=5.925, nll_loss=5.381, w2v_ctc_loss=5.838, contrastive_loss=0, total=4165.03, n_correct=269.8, ppl=41.66, accuracy=6.478, wps=14351.7, ups=1.74, wpb=8271.6, bsz=305.1, num_updates=3800, lr=0.000152024, gnorm=1.962, clip=0, loss_scale=8, train_wall=57, gb_free=14.9, wall=1796
2023-07-05 07:41:15 | INFO | train_inner | epoch 003:    957 / 1474 loss=7.796, trans_loss=5.91, nll_loss=5.359, w2v_ctc_loss=5.631, contrastive_loss=0, total=4159.73, n_correct=276.95, ppl=41.05, accuracy=6.658, wps=14248.9, ups=1.73, wpb=8249.6, bsz=311.4, num_updates=3900, lr=0.000156022, gnorm=1.926, clip=0, loss_scale=8, train_wall=57, gb_free=17.2, wall=1853
2023-07-05 07:42:12 | INFO | train_inner | epoch 003:   1057 / 1474 loss=7.737, trans_loss=5.914, nll_loss=5.362, w2v_ctc_loss=5.53, contrastive_loss=0, total=4059.97, n_correct=266.32, ppl=41.13, accuracy=6.56, wps=14074.5, ups=1.75, wpb=8064.1, bsz=292.3, num_updates=4000, lr=0.00016002, gnorm=1.957, clip=0, loss_scale=8, train_wall=57, gb_free=17.5, wall=1911
2023-07-05 07:42:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 07:42:28 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 8.217 | trans_loss 11.555 | nll_loss 10.401 | w2v_ctc_loss 0.429 | contrastive_loss 0 | total 4003.4 | n_correct 308.5 | ppl 1351.89 | accuracy 7.706 | uer 72.399 | wer 73.674 | raw_wer 73.674 | wps 4928.7 | wpb 4003.4 | bsz 141.8 | num_updates 4000 | best_loss 8.217
2023-07-05 07:42:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-07-05 07:42:28 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_3_4000.pt
2023-07-05 07:42:30 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_3_4000.pt
2023-07-05 07:42:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 8.217) (writing took 8.119776653125882 seconds)
2023-07-05 07:43:33 | INFO | train_inner | epoch 003:   1157 / 1474 loss=7.625, trans_loss=5.92, nll_loss=5.37, w2v_ctc_loss=5.36, contrastive_loss=0, total=4048.71, n_correct=265.77, ppl=41.34, accuracy=6.564, wps=9910, ups=1.23, wpb=8034.2, bsz=291.3, num_updates=4100, lr=0.000164018, gnorm=2.119, clip=0, loss_scale=8, train_wall=57, gb_free=16.8, wall=1992
2023-07-05 07:44:30 | INFO | train_inner | epoch 003:   1257 / 1474 loss=7.51, trans_loss=5.911, nll_loss=5.359, w2v_ctc_loss=5.18, contrastive_loss=0, total=4063.12, n_correct=269.53, ppl=41.03, accuracy=6.634, wps=14141.1, ups=1.75, wpb=8071.2, bsz=289.1, num_updates=4200, lr=0.000168016, gnorm=2.097, clip=0, loss_scale=8, train_wall=57, gb_free=13.8, wall=2049
2023-07-05 07:45:28 | INFO | train_inner | epoch 003:   1357 / 1474 loss=7.307, trans_loss=5.908, nll_loss=5.352, w2v_ctc_loss=4.895, contrastive_loss=0, total=4141.08, n_correct=275.53, ppl=40.83, accuracy=6.654, wps=14245.8, ups=1.73, wpb=8223.3, bsz=308.9, num_updates=4300, lr=0.000172014, gnorm=1.973, clip=0, loss_scale=8, train_wall=57, gb_free=17.4, wall=2107
2023-07-05 07:46:26 | INFO | train_inner | epoch 003:   1457 / 1474 loss=7.372, trans_loss=5.893, nll_loss=5.338, w2v_ctc_loss=4.998, contrastive_loss=0, total=4212.48, n_correct=276.96, ppl=40.46, accuracy=6.575, wps=14419.3, ups=1.72, wpb=8368.6, bsz=318.8, num_updates=4400, lr=0.000176012, gnorm=2.245, clip=0, loss_scale=8, train_wall=58, gb_free=16.9, wall=2165
2023-07-05 07:46:36 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 07:46:51 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 8.104 | trans_loss 11.56 | nll_loss 10.416 | w2v_ctc_loss 0.04 | contrastive_loss 0 | total 4003.4 | n_correct 297.7 | ppl 1366.51 | accuracy 7.436 | uer 75.954 | wer 77.007 | raw_wer 77.007 | wps 4887.9 | wpb 4003.4 | bsz 141.8 | num_updates 4417 | best_loss 8.104
2023-07-05 07:46:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4417 updates
2023-07-05 07:46:51 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_best.pt
2023-07-05 07:46:55 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_best.pt
2023-07-05 07:46:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_best.pt (epoch 3 @ 4417 updates, score 8.104) (writing took 7.472352322191 seconds)
2023-07-05 07:46:58 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-07-05 07:46:58 | INFO | train | epoch 003 | loss 7.827 | trans_loss 5.933 | nll_loss 5.388 | w2v_ctc_loss 5.652 | contrastive_loss 0 | total 4139.93 | n_correct 264.62 | ppl 41.89 | accuracy 6.392 | wps 13519.9 | ups 1.64 | wpb 8219.8 | bsz 305.9 | num_updates 4417 | lr 0.000176692 | gnorm 1.804 | clip 0.1 | loss_scale 8 | train_wall 834 | gb_free 17 | wall 2197
2023-07-05 07:46:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 07:46:59 | INFO | fairseq.trainer | begin training epoch 4
2023-07-05 07:46:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 07:47:54 | INFO | train_inner | epoch 004:     83 / 1474 loss=7.198, trans_loss=5.94, nll_loss=5.388, w2v_ctc_loss=4.673, contrastive_loss=0, total=4088.42, n_correct=270.34, ppl=41.87, accuracy=6.612, wps=9266, ups=1.14, wpb=8115.7, bsz=291.4, num_updates=4500, lr=0.00018001, gnorm=1.84, clip=0, loss_scale=8, train_wall=57, gb_free=16.4, wall=2252
2023-07-05 08:23:51 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:11718
2023-07-05 08:23:51 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:11718
2023-07-05 08:23:51 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:11718
2023-07-05 08:23:51 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:11718
2023-07-05 08:23:51 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:11718
2023-07-05 08:23:51 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:11718
2023-07-05 08:23:51 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:11718
2023-07-05 08:23:51 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:11718
2023-07-05 08:23:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-05 08:23:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-05 08:23:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-05 08:23:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-05 08:23:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-05 08:23:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-05 08:23:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-05 08:23:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-05 08:23:52 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:23:52 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-05 08:23:52 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:23:52 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:23:52 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:23:52 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-05 08:23:52 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-05 08:23:52 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-05 08:23:52 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:23:52 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-05 08:23:52 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:23:52 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-05 08:23:52 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:23:52 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-05 08:23:52 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:23:52 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-05 08:23:54 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_asronly', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11718', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 30000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_asronly', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=-1, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=30000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_asronly', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_asronly', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=30000, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=-1, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=30000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_asronly', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_asronly', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=30000, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=-1, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=30000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_asronly', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_asronly', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=30000, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-05 08:23:54 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-05 08:23:54 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-05 08:23:54 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-05 08:23:54 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-05 08:23:54 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-05 08:24:01 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-05 08:24:01 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-05 08:24:01 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-05 08:24:03 | INFO | root | load pretrained hubert
2023-07-05 08:24:07 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-05 08:24:09 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-05 08:24:13 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-05 08:24:13 | INFO | root | share the sematic adapter and textual encoder
2023-07-05 08:24:13 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-05 08:24:13 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-05 08:24:13 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-05 08:24:13 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJoint
2023-07-05 08:24:13 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-05 08:24:13 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-05 08:24:13 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-05 08:24:13 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-0.12.3/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-05 08:24:13 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-0.12.3/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-05 08:24:13 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-05 08:24:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-05 08:24:21 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-05 08:24:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-05 08:24:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-05 08:24:21 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:24:21 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:24:21 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:24:21 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:24:21 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:24:21 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:24:21 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:24:21 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:24:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-05 08:24:21 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-05 08:24:21 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-05 08:24:21 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 08:24:25 | INFO | fairseq.trainer | load the task parameters
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 572, in load_checkpoint
    self.task.load_state_dict(state["task_state"])
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 188, in load_state_dict
    self.asr_weight = state_dict["asr_weight"].cuda()
AttributeError: 'float' object has no attribute 'cuda'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 164, in main
    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(
  File "/workspace/fairseq-AT/fairseq/checkpoint_utils.py", line 248, in load_checkpoint
    extra_state = trainer.load_checkpoint(
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 586, in load_checkpoint
    raise Exception(
Exception: Cannot load model parameters from checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt; please ensure that the architectures match.

2023-07-05 08:25:52 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:12368
2023-07-05 08:25:52 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:12368
2023-07-05 08:25:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-05 08:25:52 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:12368
2023-07-05 08:25:52 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:12368
2023-07-05 08:25:52 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:12368
2023-07-05 08:25:52 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:12368
2023-07-05 08:25:52 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:12368
2023-07-05 08:25:52 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:12368
2023-07-05 08:25:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-05 08:25:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-05 08:25:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-05 08:25:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-05 08:25:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-05 08:25:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-05 08:25:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-05 08:25:52 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:25:52 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:25:52 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-05 08:25:52 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-05 08:25:52 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:25:52 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:25:52 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:25:52 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-05 08:25:52 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-05 08:25:52 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-05 08:25:52 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:25:52 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-05 08:25:52 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:25:52 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:25:52 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-05 08:25:52 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-05 08:25:54 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_asronly', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:12368', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 30000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_asronly', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=-1, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=30000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_asronly', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_asronly', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=30000, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=-1, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=30000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_asronly', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_asronly', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=30000, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=-1, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=30000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_asronly', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_asronly', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=30000, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-05 08:25:54 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-05 08:25:54 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-05 08:25:54 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-05 08:25:54 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-05 08:25:54 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-05 08:25:59 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-05 08:25:59 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-05 08:25:59 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-05 08:26:00 | INFO | root | load pretrained hubert
2023-07-05 08:26:04 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-05 08:26:06 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-05 08:26:10 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-05 08:26:11 | INFO | root | share the sematic adapter and textual encoder
2023-07-05 08:26:11 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-05 08:26:11 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-05 08:26:11 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-05 08:26:11 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJoint
2023-07-05 08:26:11 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-05 08:26:11 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-05 08:26:11 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-05 08:26:11 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-0.12.3/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-05 08:26:11 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-0.12.3/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-05 08:26:11 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-05 08:26:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-05 08:26:18 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-05 08:26:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-05 08:26:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-05 08:26:18 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:26:18 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:26:18 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:26:18 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:26:18 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:26:18 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:26:18 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:26:18 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:26:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-05 08:26:18 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-05 08:26:18 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-05 08:26:18 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 08:26:20 | INFO | fairseq.trainer | load the task parameters
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 572, in load_checkpoint
    self.task.load_state_dict(state["task_state"])
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 189, in load_state_dict
    self.asr_weight = state_dict["asr_weight"].cuda()
AttributeError: 'float' object has no attribute 'cuda'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 164, in main
    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(
  File "/workspace/fairseq-AT/fairseq/checkpoint_utils.py", line 248, in load_checkpoint
    extra_state = trainer.load_checkpoint(
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 586, in load_checkpoint
    raise Exception(
Exception: Cannot load model parameters from checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt; please ensure that the architectures match.

2023-07-05 08:27:09 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:16513
2023-07-05 08:27:09 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:16513
2023-07-05 08:27:09 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:16513
2023-07-05 08:27:10 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:16513
2023-07-05 08:27:10 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:16513
2023-07-05 08:27:10 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:16513
2023-07-05 08:27:10 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:16513
2023-07-05 08:27:10 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:16513
2023-07-05 08:27:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-05 08:27:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-05 08:27:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-05 08:27:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-05 08:27:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-05 08:27:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-05 08:27:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-05 08:27:11 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-05 08:27:11 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:27:11 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-05 08:27:11 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:27:11 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:27:11 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-05 08:27:11 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-05 08:27:11 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:27:11 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:27:11 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:27:11 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:27:11 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-05 08:27:11 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-05 08:27:11 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-05 08:27:11 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-05 08:27:11 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-05 08:27:11 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-05 08:27:13 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_asronly', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16513', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 30000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_asronly', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=-1, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=30000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_asronly', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_asronly', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=30000, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=-1, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=30000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_asronly', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_asronly', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=30000, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=-1, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=30000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_asronly', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_asronly', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=30000, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-05 08:27:13 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-05 08:27:13 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-05 08:27:13 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-05 08:27:13 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-05 08:27:13 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-05 08:27:17 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-05 08:27:17 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-05 08:27:18 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-05 08:27:19 | INFO | root | load pretrained hubert
2023-07-05 08:27:23 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-05 08:27:23 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-05 08:27:26 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-05 08:27:26 | INFO | root | share the sematic adapter and textual encoder
2023-07-05 08:27:26 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-05 08:27:26 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-05 08:27:26 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-05 08:27:26 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJoint
2023-07-05 08:27:26 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-05 08:27:26 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-05 08:27:26 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-05 08:27:26 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-0.12.3/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-05 08:27:26 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-0.12.3/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-05 08:27:26 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-05 08:27:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-05 08:27:32 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-05 08:27:32 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-05 08:27:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-05 08:27:33 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:27:33 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:27:33 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:27:33 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:27:33 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:27:33 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:27:33 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:27:33 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-05 08:27:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-05 08:27:33 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-05 08:27:33 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-05 08:27:33 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 08:27:35 | INFO | fairseq.trainer | load the task parameters
2023-07-05 08:27:35 | INFO | fairseq.trainer | Loaded checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 4 @ 4417 updates)
2023-07-05 08:27:35 | INFO | fairseq.trainer | loading train data for epoch 4
2023-07-05 08:27:35 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-05 08:27:35 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-0.12.3/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-05 08:27:35 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-0.12.3/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-05 08:27:39 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-05 08:27:41 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-05 08:28:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 08:28:24 | INFO | fairseq.trainer | begin training epoch 4
2023-07-05 08:28:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 08:29:27 | INFO | train_inner | epoch 004:     83 / 1474 loss=7.223, trans_loss=5.928, nll_loss=5.374, w2v_ctc_loss=4.737, contrastive_loss=0, total=4120.58, n_correct=273.518, ppl=41.48, accuracy=6.638, wps=14186.3, ups=1.73, wpb=8184.3, bsz=295.8, num_updates=4500, lr=0.00018001, gnorm=1.916, clip=0, loss_scale=8, train_wall=55, gb_free=16.4, wall=114
2023-07-05 08:30:46 | INFO | train_inner | epoch 004:    183 / 1474 loss=7.187, trans_loss=5.918, nll_loss=5.359, w2v_ctc_loss=4.69, contrastive_loss=0, total=4183.38, n_correct=280.75, ppl=41.05, accuracy=6.711, wps=10455.8, ups=1.26, wpb=8305.9, bsz=313.1, num_updates=4600, lr=0.000184008, gnorm=1.932, clip=0, loss_scale=8, train_wall=79, gb_free=16.9, wall=193
2023-07-05 08:31:44 | INFO | train_inner | epoch 004:    283 / 1474 loss=7.223, trans_loss=5.916, nll_loss=5.359, w2v_ctc_loss=4.747, contrastive_loss=0, total=4142.13, n_correct=272.29, ppl=41.05, accuracy=6.574, wps=14160.6, ups=1.72, wpb=8232.6, bsz=308.9, num_updates=4700, lr=0.000188006, gnorm=2.143, clip=0, loss_scale=8, train_wall=58, gb_free=13.6, wall=251
2023-07-05 08:32:42 | INFO | train_inner | epoch 004:    383 / 1474 loss=7.232, trans_loss=5.938, nll_loss=5.38, w2v_ctc_loss=4.731, contrastive_loss=0, total=4132.81, n_correct=271.28, ppl=41.65, accuracy=6.564, wps=14119, ups=1.72, wpb=8195.2, bsz=295.7, num_updates=4800, lr=0.000192004, gnorm=2.098, clip=0, loss_scale=8, train_wall=58, gb_free=14.9, wall=309
2023-07-05 08:33:40 | INFO | train_inner | epoch 004:    483 / 1474 loss=7.02, trans_loss=5.925, nll_loss=5.367, w2v_ctc_loss=4.417, contrastive_loss=0, total=4205.11, n_correct=273.8, ppl=41.27, accuracy=6.511, wps=14407.9, ups=1.73, wpb=8348.7, bsz=329.1, num_updates=4900, lr=0.000196002, gnorm=1.825, clip=0, loss_scale=8, train_wall=58, gb_free=17.2, wall=367
2023-07-05 08:34:39 | INFO | train_inner | epoch 004:    583 / 1474 loss=7.148, trans_loss=5.939, nll_loss=5.38, w2v_ctc_loss=4.607, contrastive_loss=0, total=4224.38, n_correct=279.87, ppl=41.63, accuracy=6.625, wps=14181.7, ups=1.69, wpb=8384.4, bsz=325.9, num_updates=5000, lr=0.0002, gnorm=2.058, clip=0, loss_scale=8, train_wall=59, gb_free=12.6, wall=426
2023-07-05 08:35:39 | INFO | train_inner | epoch 004:    683 / 1474 loss=7.115, trans_loss=5.97, nll_loss=5.413, w2v_ctc_loss=4.512, contrastive_loss=0, total=4182.18, n_correct=271.61, ppl=42.62, accuracy=6.494, wps=13962.4, ups=1.69, wpb=8285.5, bsz=304.9, num_updates=5100, lr=0.00019803, gnorm=1.946, clip=0, loss_scale=8, train_wall=59, gb_free=12.6, wall=486
2023-07-05 08:36:37 | INFO | train_inner | epoch 004:    783 / 1474 loss=7.099, trans_loss=5.957, nll_loss=5.401, w2v_ctc_loss=4.505, contrastive_loss=0, total=4025.8, n_correct=266.54, ppl=42.26, accuracy=6.621, wps=13759.7, ups=1.72, wpb=7996.4, bsz=279.9, num_updates=5200, lr=0.000196116, gnorm=1.969, clip=0, loss_scale=8, train_wall=58, gb_free=16.9, wall=544
2023-07-05 08:37:35 | INFO | train_inner | epoch 004:    883 / 1474 loss=6.999, trans_loss=5.953, nll_loss=5.396, w2v_ctc_loss=4.355, contrastive_loss=0, total=4179.58, n_correct=274.18, ppl=42.12, accuracy=6.56, wps=14208.6, ups=1.71, wpb=8302.5, bsz=309.6, num_updates=5300, lr=0.000194257, gnorm=1.905, clip=0, loss_scale=8, train_wall=58, gb_free=14.7, wall=602
2023-07-05 08:38:33 | INFO | train_inner | epoch 004:    983 / 1474 loss=6.967, trans_loss=5.949, nll_loss=5.392, w2v_ctc_loss=4.308, contrastive_loss=0, total=4131.6, n_correct=267.94, ppl=41.98, accuracy=6.485, wps=14110.8, ups=1.72, wpb=8208.3, bsz=306.3, num_updates=5400, lr=0.00019245, gnorm=1.938, clip=0, loss_scale=8, train_wall=58, gb_free=15.2, wall=660
2023-07-05 08:39:31 | INFO | train_inner | epoch 004:   1083 / 1474 loss=7.014, trans_loss=5.98, nll_loss=5.424, w2v_ctc_loss=4.346, contrastive_loss=0, total=4071.42, n_correct=260.62, ppl=42.92, accuracy=6.401, wps=13973, ups=1.73, wpb=8079.8, bsz=289.7, num_updates=5500, lr=0.000190693, gnorm=2.019, clip=0, loss_scale=8, train_wall=57, gb_free=16.5, wall=718
2023-07-05 08:40:29 | INFO | train_inner | epoch 004:   1183 / 1474 loss=6.891, trans_loss=5.968, nll_loss=5.413, w2v_ctc_loss=4.179, contrastive_loss=0, total=4161.85, n_correct=266.91, ppl=42.6, accuracy=6.413, wps=14335.5, ups=1.73, wpb=8273, bsz=322.6, num_updates=5600, lr=0.000188982, gnorm=2.108, clip=0, loss_scale=8, train_wall=57, gb_free=12.7, wall=776
2023-07-05 08:41:26 | INFO | train_inner | epoch 004:   1283 / 1474 loss=6.84, trans_loss=5.969, nll_loss=5.41, w2v_ctc_loss=4.093, contrastive_loss=0, total=4152.03, n_correct=270.42, ppl=42.52, accuracy=6.513, wps=14311.3, ups=1.74, wpb=8246.6, bsz=314, num_updates=5700, lr=0.000187317, gnorm=1.738, clip=0, loss_scale=8, train_wall=57, gb_free=16.8, wall=834
2023-07-05 08:42:24 | INFO | train_inner | epoch 004:   1383 / 1474 loss=6.879, trans_loss=5.97, nll_loss=5.414, w2v_ctc_loss=4.148, contrastive_loss=0, total=4099.25, n_correct=265.3, ppl=42.63, accuracy=6.472, wps=14198.8, ups=1.74, wpb=8143.7, bsz=291.2, num_updates=5800, lr=0.000185695, gnorm=2.012, clip=0, loss_scale=8, train_wall=57, gb_free=12, wall=891
2023-07-05 08:43:38 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2023-07-05 08:43:53 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 8.095 | trans_loss 11.5 | nll_loss 10.333 | w2v_ctc_loss 0.149 | contrastive_loss 0 | total 4003.4 | n_correct 317.1 | ppl 1289.76 | accuracy 7.921 | uer 62.153 | wer 63.79 | raw_wer 63.79 | wps 4807.9 | wpb 4003.4 | bsz 141.8 | num_updates 5891 | best_loss 8.095
2023-07-05 08:43:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5891 updates
2023-07-05 08:43:53 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_best.pt
2023-07-05 08:43:57 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_best.pt
2023-07-05 08:44:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_best.pt (epoch 4 @ 5891 updates, score 8.095) (writing took 6.89676955799996 seconds)
2023-07-05 08:44:00 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-07-05 08:44:00 | INFO | train | epoch 004 | loss 7.042 | trans_loss 5.95 | nll_loss 5.393 | w2v_ctc_loss 4.427 | contrastive_loss 0 | total 4138.65 | n_correct 270.763 | ppl 42.01 | accuracy 6.542 | wps 13151.9 | ups 1.6 | wpb 8217.2 | bsz 305.7 | num_updates 5891 | lr 0.000184256 | gnorm 1.978 | clip 0 | loss_scale 8 | train_wall 879 | gb_free 15.1 | wall 987
2023-07-05 08:44:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 08:44:00 | INFO | fairseq.trainer | begin training epoch 5
2023-07-05 08:44:00 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 08:44:11 | INFO | train_inner | epoch 005:      9 / 1474 loss=6.814, trans_loss=5.966, nll_loss=5.409, w2v_ctc_loss=4.06, contrastive_loss=0, total=4035.57, n_correct=265.04, ppl=42.47, accuracy=6.568, wps=7443.2, ups=0.93, wpb=8012.6, bsz=291.5, num_updates=5900, lr=0.000184115, gnorm=2.091, clip=0, loss_scale=8, train_wall=57, gb_free=16.2, wall=999
2023-07-05 08:45:10 | INFO | train_inner | epoch 005:    109 / 1474 loss=6.671, trans_loss=5.95, nll_loss=5.393, w2v_ctc_loss=3.864, contrastive_loss=0, total=4249.55, n_correct=278.6, ppl=42.01, accuracy=6.556, wps=14500, ups=1.72, wpb=8441.4, bsz=331.3, num_updates=6000, lr=0.000182574, gnorm=2.015, clip=0, loss_scale=8, train_wall=58, gb_free=17.8, wall=1057
2023-07-05 08:45:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 08:45:24 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 8.235 | trans_loss 11.484 | nll_loss 10.301 | w2v_ctc_loss 0.654 | contrastive_loss 0 | total 4003.4 | n_correct 317.8 | ppl 1261.29 | accuracy 7.938 | uer 61.445 | wer 62.116 | raw_wer 62.116 | wps 4879.6 | wpb 4003.4 | bsz 141.8 | num_updates 6000 | best_loss 8.095
2023-07-05 08:45:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6000 updates
2023-07-05 08:45:24 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_5_6000.pt
2023-07-05 08:45:27 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_5_6000.pt
2023-07-05 08:45:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_5_6000.pt (epoch 5 @ 6000 updates, score 8.235) (writing took 4.933314313999745 seconds)
2023-07-05 08:46:26 | INFO | train_inner | epoch 005:    209 / 1474 loss=6.728, trans_loss=5.989, nll_loss=5.429, w2v_ctc_loss=3.898, contrastive_loss=0, total=4195.93, n_correct=277.19, ppl=43.07, accuracy=6.606, wps=10835.6, ups=1.3, wpb=8321.9, bsz=326.8, num_updates=6100, lr=0.000181071, gnorm=1.953, clip=0, loss_scale=8, train_wall=57, gb_free=16.3, wall=1134
2023-07-05 08:47:24 | INFO | train_inner | epoch 005:    309 / 1474 loss=6.77, trans_loss=5.979, nll_loss=5.423, w2v_ctc_loss=3.968, contrastive_loss=0, total=4092.41, n_correct=266.47, ppl=42.9, accuracy=6.511, wps=14178.5, ups=1.74, wpb=8142.1, bsz=296.8, num_updates=6200, lr=0.000179605, gnorm=1.928, clip=0, loss_scale=8, train_wall=57, gb_free=16.3, wall=1191
2023-07-05 08:48:22 | INFO | train_inner | epoch 005:    409 / 1474 loss=6.653, trans_loss=5.962, nll_loss=5.403, w2v_ctc_loss=3.821, contrastive_loss=0, total=4142.04, n_correct=274.68, ppl=42.3, accuracy=6.632, wps=14253.3, ups=1.73, wpb=8237.8, bsz=314.2, num_updates=6300, lr=0.000178174, gnorm=1.946, clip=0, loss_scale=8, train_wall=57, gb_free=17.4, wall=1249
2023-07-05 08:49:19 | INFO | train_inner | epoch 005:    509 / 1474 loss=6.832, trans_loss=6.013, nll_loss=5.457, w2v_ctc_loss=4.039, contrastive_loss=0, total=4031.44, n_correct=261.67, ppl=43.91, accuracy=6.491, wps=13872.9, ups=1.73, wpb=8012.7, bsz=278.1, num_updates=6400, lr=0.000176777, gnorm=2.021, clip=0, loss_scale=8, train_wall=57, gb_free=12.6, wall=1307
2023-07-05 08:50:17 | INFO | train_inner | epoch 005:    609 / 1474 loss=6.758, trans_loss=6, nll_loss=5.439, w2v_ctc_loss=3.924, contrastive_loss=0, total=4113.46, n_correct=269.03, ppl=43.38, accuracy=6.54, wps=14094.6, ups=1.73, wpb=8155.9, bsz=301.4, num_updates=6500, lr=0.000175412, gnorm=2.014, clip=0, loss_scale=16, train_wall=57, gb_free=17.7, wall=1364
2023-07-05 08:51:15 | INFO | train_inner | epoch 005:    709 / 1474 loss=6.659, trans_loss=5.997, nll_loss=5.438, w2v_ctc_loss=3.792, contrastive_loss=0, total=4167.56, n_correct=273.63, ppl=43.36, accuracy=6.566, wps=14337.7, ups=1.73, wpb=8272.8, bsz=318.9, num_updates=6600, lr=0.000174078, gnorm=1.967, clip=0, loss_scale=16, train_wall=57, gb_free=12.1, wall=1422
2023-07-05 08:51:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-07-05 08:52:14 | INFO | train_inner | epoch 005:    810 / 1474 loss=6.657, trans_loss=6.032, nll_loss=5.474, w2v_ctc_loss=3.747, contrastive_loss=0, total=4135.84, n_correct=266.84, ppl=44.45, accuracy=6.452, wps=13896.2, ups=1.69, wpb=8207.3, bsz=300.6, num_updates=6700, lr=0.000172774, gnorm=1.795, clip=0, loss_scale=8, train_wall=59, gb_free=15.4, wall=1481
2023-07-05 08:53:12 | INFO | train_inner | epoch 005:    910 / 1474 loss=6.553, trans_loss=6.006, nll_loss=5.446, w2v_ctc_loss=3.622, contrastive_loss=0, total=4095.48, n_correct=269.36, ppl=43.58, accuracy=6.577, wps=13999.9, ups=1.72, wpb=8134, bsz=296.9, num_updates=6800, lr=0.000171499, gnorm=1.739, clip=0, loss_scale=8, train_wall=58, gb_free=15.8, wall=1539
2023-07-05 08:54:10 | INFO | train_inner | epoch 005:   1010 / 1474 loss=6.609, trans_loss=6.023, nll_loss=5.465, w2v_ctc_loss=3.671, contrastive_loss=0, total=4165.12, n_correct=267.78, ppl=44.18, accuracy=6.429, wps=14385, ups=1.74, wpb=8268.5, bsz=309, num_updates=6900, lr=0.000170251, gnorm=2.015, clip=0, loss_scale=8, train_wall=57, gb_free=16, wall=1597
2023-07-05 08:55:08 | INFO | train_inner | epoch 005:   1110 / 1474 loss=6.669, trans_loss=6.036, nll_loss=5.477, w2v_ctc_loss=3.766, contrastive_loss=0, total=4176.72, n_correct=272.88, ppl=44.55, accuracy=6.533, wps=14197.5, ups=1.71, wpb=8282.5, bsz=310.8, num_updates=7000, lr=0.000169031, gnorm=2.082, clip=0, loss_scale=8, train_wall=58, gb_free=17, wall=1655
2023-07-05 08:56:06 | INFO | train_inner | epoch 005:   1210 / 1474 loss=6.703, trans_loss=6.049, nll_loss=5.491, w2v_ctc_loss=3.793, contrastive_loss=0, total=4164.13, n_correct=276.28, ppl=44.99, accuracy=6.635, wps=14222.5, ups=1.72, wpb=8256.7, bsz=302.6, num_updates=7100, lr=0.000167836, gnorm=1.991, clip=0, loss_scale=8, train_wall=58, gb_free=17.1, wall=1713
2023-07-05 08:57:04 | INFO | train_inner | epoch 005:   1310 / 1474 loss=6.577, trans_loss=6.049, nll_loss=5.489, w2v_ctc_loss=3.604, contrastive_loss=0, total=4134.91, n_correct=271.54, ppl=44.92, accuracy=6.567, wps=14175.6, ups=1.73, wpb=8206.5, bsz=297.1, num_updates=7200, lr=0.000166667, gnorm=2.001, clip=0, loss_scale=8, train_wall=57, gb_free=16.7, wall=1771
2023-07-05 08:58:02 | INFO | train_inner | epoch 005:   1410 / 1474 loss=6.612, trans_loss=6.032, nll_loss=5.474, w2v_ctc_loss=3.678, contrastive_loss=0, total=4134.37, n_correct=267.82, ppl=44.45, accuracy=6.478, wps=14242.3, ups=1.73, wpb=8213.2, bsz=305.6, num_updates=7300, lr=0.000165521, gnorm=2.007, clip=0, loss_scale=8, train_wall=57, gb_free=17.9, wall=1829
2023-07-05 08:58:39 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 08:58:54 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 8.258 | trans_loss 11.745 | nll_loss 10.599 | w2v_ctc_loss 0.121 | contrastive_loss 0 | total 4003.4 | n_correct 303.7 | ppl 1551.02 | accuracy 7.586 | uer 60.33 | wer 62.414 | raw_wer 62.414 | wps 4806.4 | wpb 4003.4 | bsz 141.8 | num_updates 7364 | best_loss 8.095
2023-07-05 08:58:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7364 updates
2023-07-05 08:58:54 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 08:58:58 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 08:58:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 5 @ 7364 updates, score 8.258) (writing took 4.094593791999614 seconds)
2023-07-05 08:58:58 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-07-05 08:58:58 | INFO | train | epoch 005 | loss 6.676 | trans_loss 6.01 | nll_loss 5.452 | w2v_ctc_loss 3.798 | contrastive_loss 0 | total 4139.03 | n_correct 270.601 | ppl 43.76 | accuracy 6.538 | wps 13474.1 | ups 1.64 | wpb 8217.8 | bsz 305.7 | num_updates 7364 | lr 0.0001648 | gnorm 1.969 | clip 0 | loss_scale 8 | train_wall 846 | gb_free 16.5 | wall 1885
2023-07-05 08:58:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 08:58:58 | INFO | fairseq.trainer | begin training epoch 6
2023-07-05 08:58:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 08:59:26 | INFO | train_inner | epoch 006:     36 / 1474 loss=6.546, trans_loss=6.05, nll_loss=5.493, w2v_ctc_loss=3.557, contrastive_loss=0, total=4115.45, n_correct=267.79, ppl=45.02, accuracy=6.507, wps=9680.2, ups=1.19, wpb=8165.8, bsz=298.6, num_updates=7400, lr=0.000164399, gnorm=1.964, clip=0, loss_scale=8, train_wall=58, gb_free=16.6, wall=1913
2023-07-05 09:00:24 | INFO | train_inner | epoch 006:    136 / 1474 loss=6.475, trans_loss=6.039, nll_loss=5.48, w2v_ctc_loss=3.453, contrastive_loss=0, total=4154.25, n_correct=273.33, ppl=44.63, accuracy=6.58, wps=14304.9, ups=1.73, wpb=8253.2, bsz=304.1, num_updates=7500, lr=0.000163299, gnorm=1.976, clip=0, loss_scale=8, train_wall=57, gb_free=15.8, wall=1971
2023-07-05 09:01:22 | INFO | train_inner | epoch 006:    236 / 1474 loss=6.505, trans_loss=6.048, nll_loss=5.493, w2v_ctc_loss=3.499, contrastive_loss=0, total=4112.66, n_correct=263.6, ppl=45.04, accuracy=6.409, wps=14134.3, ups=1.73, wpb=8174.6, bsz=291.5, num_updates=7600, lr=0.000162221, gnorm=1.899, clip=0, loss_scale=8, train_wall=57, gb_free=16.4, wall=2029
2023-07-05 09:02:20 | INFO | train_inner | epoch 006:    336 / 1474 loss=6.409, trans_loss=6.035, nll_loss=5.475, w2v_ctc_loss=3.358, contrastive_loss=0, total=4177.51, n_correct=278.05, ppl=44.47, accuracy=6.656, wps=14123.2, ups=1.7, wpb=8296.3, bsz=327.5, num_updates=7700, lr=0.000161165, gnorm=1.825, clip=0, loss_scale=8, train_wall=58, gb_free=16.3, wall=2087
2023-07-05 09:03:18 | INFO | train_inner | epoch 006:    436 / 1474 loss=6.451, trans_loss=6.05, nll_loss=5.49, w2v_ctc_loss=3.416, contrastive_loss=0, total=4154.57, n_correct=275.59, ppl=44.95, accuracy=6.633, wps=14385.6, ups=1.74, wpb=8250.9, bsz=313.4, num_updates=7800, lr=0.000160128, gnorm=2.073, clip=0, loss_scale=8, train_wall=57, gb_free=16.4, wall=2145
2023-07-05 09:04:16 | INFO | train_inner | epoch 006:    536 / 1474 loss=6.457, trans_loss=6.064, nll_loss=5.504, w2v_ctc_loss=3.403, contrastive_loss=0, total=4167.79, n_correct=276.19, ppl=45.39, accuracy=6.627, wps=14265.7, ups=1.72, wpb=8270.7, bsz=303.4, num_updates=7900, lr=0.000159111, gnorm=1.741, clip=0, loss_scale=8, train_wall=58, gb_free=16, wall=2203
2023-07-05 09:05:13 | INFO | train_inner | epoch 006:    636 / 1474 loss=6.365, trans_loss=6.083, nll_loss=5.525, w2v_ctc_loss=3.244, contrastive_loss=0, total=4146.17, n_correct=268.82, ppl=46.05, accuracy=6.484, wps=14342.8, ups=1.74, wpb=8230.4, bsz=314.4, num_updates=8000, lr=0.000158114, gnorm=1.839, clip=0, loss_scale=8, train_wall=57, gb_free=16.8, wall=2260
2023-07-05 09:05:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 09:05:28 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 8.285 | trans_loss 11.687 | nll_loss 10.505 | w2v_ctc_loss 0.345 | contrastive_loss 0 | total 4003.4 | n_correct 323 | ppl 1453.58 | accuracy 8.068 | uer 53.688 | wer 55.688 | raw_wer 55.688 | wps 4805.6 | wpb 4003.4 | bsz 141.8 | num_updates 8000 | best_loss 8.095
2023-07-05 09:05:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8000 updates
2023-07-05 09:05:28 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_6_8000.pt
2023-07-05 09:05:30 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_6_8000.pt
2023-07-05 09:05:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_6_8000.pt (epoch 6 @ 8000 updates, score 8.285) (writing took 4.967795156999273 seconds)
2023-07-05 09:06:31 | INFO | train_inner | epoch 006:    736 / 1474 loss=6.454, trans_loss=6.096, nll_loss=5.54, w2v_ctc_loss=3.373, contrastive_loss=0, total=4148.65, n_correct=269.95, ppl=46.52, accuracy=6.507, wps=10595.6, ups=1.29, wpb=8239.4, bsz=302.5, num_updates=8100, lr=0.000157135, gnorm=1.942, clip=0, loss_scale=8, train_wall=58, gb_free=15.9, wall=2338
2023-07-05 09:07:29 | INFO | train_inner | epoch 006:    836 / 1474 loss=6.46, trans_loss=6.108, nll_loss=5.553, w2v_ctc_loss=3.354, contrastive_loss=0, total=4114.34, n_correct=267.43, ppl=46.93, accuracy=6.5, wps=14064.2, ups=1.72, wpb=8167.8, bsz=294.4, num_updates=8200, lr=0.000156174, gnorm=1.933, clip=0, loss_scale=8, train_wall=58, gb_free=15.3, wall=2396
2023-07-05 09:08:26 | INFO | train_inner | epoch 006:    936 / 1474 loss=6.474, trans_loss=6.109, nll_loss=5.553, w2v_ctc_loss=3.375, contrastive_loss=0, total=4081.53, n_correct=266.96, ppl=46.96, accuracy=6.541, wps=14083, ups=1.74, wpb=8099.8, bsz=296.4, num_updates=8300, lr=0.00015523, gnorm=1.824, clip=0, loss_scale=8, train_wall=57, gb_free=18, wall=2453
2023-07-05 09:09:24 | INFO | train_inner | epoch 006:   1036 / 1474 loss=6.397, trans_loss=6.122, nll_loss=5.567, w2v_ctc_loss=3.248, contrastive_loss=0, total=4165.84, n_correct=270.78, ppl=47.4, accuracy=6.5, wps=14332.5, ups=1.73, wpb=8269.8, bsz=318.1, num_updates=8400, lr=0.000154303, gnorm=1.961, clip=0, loss_scale=8, train_wall=57, gb_free=17, wall=2511
2023-07-05 09:10:22 | INFO | train_inner | epoch 006:   1136 / 1474 loss=6.573, trans_loss=6.164, nll_loss=5.612, w2v_ctc_loss=3.47, contrastive_loss=0, total=4072.29, n_correct=268.3, ppl=48.91, accuracy=6.588, wps=13945, ups=1.72, wpb=8085.3, bsz=285.3, num_updates=8500, lr=0.000153393, gnorm=2.151, clip=0, loss_scale=8, train_wall=58, gb_free=17.2, wall=2569
2023-07-05 09:11:20 | INFO | train_inner | epoch 006:   1236 / 1474 loss=6.439, trans_loss=6.133, nll_loss=5.579, w2v_ctc_loss=3.306, contrastive_loss=0, total=4141.55, n_correct=272.75, ppl=47.8, accuracy=6.586, wps=14123.9, ups=1.72, wpb=8229.3, bsz=316.5, num_updates=8600, lr=0.000152499, gnorm=1.917, clip=0, loss_scale=8, train_wall=58, gb_free=13.6, wall=2627
2023-07-05 09:12:18 | INFO | train_inner | epoch 006:   1336 / 1474 loss=6.395, trans_loss=6.163, nll_loss=5.606, w2v_ctc_loss=3.197, contrastive_loss=0, total=4125.31, n_correct=274.07, ppl=48.69, accuracy=6.644, wps=14274.7, ups=1.75, wpb=8179.7, bsz=301.8, num_updates=8700, lr=0.00015162, gnorm=1.77, clip=0, loss_scale=16, train_wall=57, gb_free=17.9, wall=2685
2023-07-05 09:13:16 | INFO | train_inner | epoch 006:   1436 / 1474 loss=6.432, trans_loss=6.176, nll_loss=5.622, w2v_ctc_loss=3.234, contrastive_loss=0, total=4196.2, n_correct=273.15, ppl=49.24, accuracy=6.509, wps=14360.9, ups=1.72, wpb=8329, bsz=307.7, num_updates=8800, lr=0.000150756, gnorm=1.974, clip=0, loss_scale=16, train_wall=58, gb_free=11.9, wall=2743
2023-07-05 09:13:37 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 09:13:52 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 8.361 | trans_loss 11.869 | nll_loss 10.708 | w2v_ctc_loss 0.176 | contrastive_loss 0 | total 4003.4 | n_correct 313.6 | ppl 1672.62 | accuracy 7.833 | uer 56.223 | wer 57.966 | raw_wer 57.966 | wps 4839.7 | wpb 4003.4 | bsz 141.8 | num_updates 8838 | best_loss 8.095
2023-07-05 09:13:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8838 updates
2023-07-05 09:13:52 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 09:13:56 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 09:13:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 6 @ 8838 updates, score 8.361) (writing took 4.036299801000496 seconds)
2023-07-05 09:13:56 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-07-05 09:13:56 | INFO | train | epoch 006 | loss 6.441 | trans_loss 6.098 | nll_loss 5.542 | w2v_ctc_loss 3.341 | contrastive_loss 0 | total 4138.65 | n_correct 271.446 | ppl 46.58 | accuracy 6.559 | wps 13483.6 | ups 1.64 | wpb 8217.2 | bsz 305.7 | num_updates 8838 | lr 0.000150431 | gnorm 1.908 | clip 0 | loss_scale 16 | train_wall 846 | gb_free 15.4 | wall 2783
2023-07-05 09:13:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 09:13:56 | INFO | fairseq.trainer | begin training epoch 7
2023-07-05 09:13:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 09:14:39 | INFO | train_inner | epoch 007:     62 / 1474 loss=6.278, trans_loss=6.114, nll_loss=5.556, w2v_ctc_loss=3.062, contrastive_loss=0, total=4108.19, n_correct=273.74, ppl=47.03, accuracy=6.663, wps=9744.6, ups=1.19, wpb=8158.4, bsz=307.9, num_updates=8900, lr=0.000149906, gnorm=1.745, clip=0, loss_scale=16, train_wall=57, gb_free=17.3, wall=2826
2023-07-05 09:14:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-07-05 09:15:38 | INFO | train_inner | epoch 007:    163 / 1474 loss=6.318, trans_loss=6.147, nll_loss=5.592, w2v_ctc_loss=3.104, contrastive_loss=0, total=4113.51, n_correct=265.53, ppl=48.24, accuracy=6.455, wps=14053.4, ups=1.72, wpb=8168, bsz=304.4, num_updates=9000, lr=0.000149071, gnorm=1.872, clip=0, loss_scale=8, train_wall=58, gb_free=13.9, wall=2885
2023-07-05 09:16:35 | INFO | train_inner | epoch 007:    263 / 1474 loss=6.287, trans_loss=6.161, nll_loss=5.605, w2v_ctc_loss=3.04, contrastive_loss=0, total=4133.29, n_correct=269.35, ppl=48.66, accuracy=6.517, wps=14321.5, ups=1.75, wpb=8202.5, bsz=303.6, num_updates=9100, lr=0.00014825, gnorm=2.004, clip=0, loss_scale=8, train_wall=57, gb_free=15.6, wall=2942
2023-07-05 09:17:33 | INFO | train_inner | epoch 007:    363 / 1474 loss=6.311, trans_loss=6.162, nll_loss=5.608, w2v_ctc_loss=3.059, contrastive_loss=0, total=4194.76, n_correct=275.24, ppl=48.78, accuracy=6.562, wps=14367.2, ups=1.73, wpb=8323.5, bsz=318.4, num_updates=9200, lr=0.000147442, gnorm=1.87, clip=0, loss_scale=8, train_wall=58, gb_free=13.4, wall=3000
2023-07-05 09:18:30 | INFO | train_inner | epoch 007:    463 / 1474 loss=6.281, trans_loss=6.158, nll_loss=5.605, w2v_ctc_loss=3.041, contrastive_loss=0, total=4153.22, n_correct=270.95, ppl=48.66, accuracy=6.524, wps=14322, ups=1.74, wpb=8250.1, bsz=308.7, num_updates=9300, lr=0.000146647, gnorm=1.856, clip=0, loss_scale=8, train_wall=57, gb_free=17, wall=3057
2023-07-05 09:19:28 | INFO | train_inner | epoch 007:    563 / 1474 loss=6.279, trans_loss=6.171, nll_loss=5.616, w2v_ctc_loss=3.009, contrastive_loss=0, total=4168.14, n_correct=272.04, ppl=49.04, accuracy=6.527, wps=14423.1, ups=1.74, wpb=8266.6, bsz=306.5, num_updates=9400, lr=0.000145865, gnorm=1.818, clip=0, loss_scale=8, train_wall=57, gb_free=17.1, wall=3115
2023-07-05 09:20:26 | INFO | train_inner | epoch 007:    663 / 1474 loss=6.207, trans_loss=6.159, nll_loss=5.605, w2v_ctc_loss=2.925, contrastive_loss=0, total=4157.82, n_correct=275.17, ppl=48.68, accuracy=6.618, wps=14236.3, ups=1.73, wpb=8248.5, bsz=303.6, num_updates=9500, lr=0.000145095, gnorm=1.866, clip=0, loss_scale=8, train_wall=58, gb_free=15.8, wall=3173
2023-07-05 09:21:23 | INFO | train_inner | epoch 007:    763 / 1474 loss=6.351, trans_loss=6.174, nll_loss=5.624, w2v_ctc_loss=3.118, contrastive_loss=0, total=4122.1, n_correct=270.21, ppl=49.33, accuracy=6.555, wps=14144.1, ups=1.73, wpb=8186.3, bsz=297.5, num_updates=9600, lr=0.000144338, gnorm=1.94, clip=0, loss_scale=8, train_wall=57, gb_free=15.8, wall=3231
2023-07-05 09:22:21 | INFO | train_inner | epoch 007:    863 / 1474 loss=6.325, trans_loss=6.163, nll_loss=5.61, w2v_ctc_loss=3.085, contrastive_loss=0, total=4147.23, n_correct=276.9, ppl=48.84, accuracy=6.677, wps=14186.9, ups=1.72, wpb=8230.4, bsz=307.2, num_updates=9700, lr=0.000143592, gnorm=1.944, clip=0, loss_scale=8, train_wall=58, gb_free=17.7, wall=3289
2023-07-05 09:23:19 | INFO | train_inner | epoch 007:    963 / 1474 loss=6.274, trans_loss=6.172, nll_loss=5.62, w2v_ctc_loss=3.012, contrastive_loss=0, total=4140.14, n_correct=278.11, ppl=49.19, accuracy=6.717, wps=14214.9, ups=1.73, wpb=8219, bsz=316.4, num_updates=9800, lr=0.000142857, gnorm=2, clip=0, loss_scale=8, train_wall=57, gb_free=16.2, wall=3346
2023-07-05 09:24:17 | INFO | train_inner | epoch 007:   1063 / 1474 loss=6.244, trans_loss=6.198, nll_loss=5.649, w2v_ctc_loss=2.94, contrastive_loss=0, total=4103.51, n_correct=269.59, ppl=50.2, accuracy=6.57, wps=14074.5, ups=1.73, wpb=8147.6, bsz=291.7, num_updates=9900, lr=0.000142134, gnorm=1.917, clip=0, loss_scale=8, train_wall=58, gb_free=17.1, wall=3404
2023-07-05 09:25:15 | INFO | train_inner | epoch 007:   1163 / 1474 loss=6.223, trans_loss=6.184, nll_loss=5.636, w2v_ctc_loss=2.911, contrastive_loss=0, total=4137.04, n_correct=277.29, ppl=49.74, accuracy=6.703, wps=14309.1, ups=1.74, wpb=8224.6, bsz=313.9, num_updates=10000, lr=0.000141421, gnorm=1.899, clip=0, loss_scale=8, train_wall=57, gb_free=16.3, wall=3462
2023-07-05 09:25:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 09:25:30 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 8.498 | trans_loss 11.981 | nll_loss 10.836 | w2v_ctc_loss 0.371 | contrastive_loss 0 | total 4003.4 | n_correct 309.7 | ppl 1828.25 | accuracy 7.736 | uer 52.504 | wer 54.394 | raw_wer 54.394 | wps 4896.1 | wpb 4003.4 | bsz 141.8 | num_updates 10000 | best_loss 8.095
2023-07-05 09:25:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10000 updates
2023-07-05 09:25:30 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_7_10000.pt
2023-07-05 09:25:32 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_7_10000.pt
2023-07-05 09:25:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_7_10000.pt (epoch 7 @ 10000 updates, score 8.498) (writing took 4.866646331999618 seconds)
2023-07-05 09:26:32 | INFO | train_inner | epoch 007:   1263 / 1474 loss=6.237, trans_loss=6.199, nll_loss=5.65, w2v_ctc_loss=2.924, contrastive_loss=0, total=4129.52, n_correct=269.7, ppl=50.2, accuracy=6.531, wps=10634.4, ups=1.3, wpb=8201.9, bsz=300.1, num_updates=10100, lr=0.00014072, gnorm=1.823, clip=0, loss_scale=8, train_wall=57, gb_free=17.1, wall=3539
2023-07-05 09:27:29 | INFO | train_inner | epoch 007:   1363 / 1474 loss=6.359, trans_loss=6.208, nll_loss=5.658, w2v_ctc_loss=3.091, contrastive_loss=0, total=4172.87, n_correct=276.46, ppl=50.49, accuracy=6.625, wps=14417, ups=1.74, wpb=8285.3, bsz=317.5, num_updates=10200, lr=0.000140028, gnorm=2.107, clip=0, loss_scale=8, train_wall=57, gb_free=17.4, wall=3596
2023-07-05 09:28:28 | INFO | train_inner | epoch 007:   1463 / 1474 loss=6.333, trans_loss=6.222, nll_loss=5.677, w2v_ctc_loss=3.047, contrastive_loss=0, total=4109.42, n_correct=267.13, ppl=51.15, accuracy=6.5, wps=13889.6, ups=1.7, wpb=8168.7, bsz=295.8, num_updates=10300, lr=0.000139347, gnorm=1.845, clip=0, loss_scale=8, train_wall=58, gb_free=16.7, wall=3655
2023-07-05 09:28:34 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 09:28:49 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 8.715 | trans_loss 11.989 | nll_loss 10.834 | w2v_ctc_loss 1.076 | contrastive_loss 0 | total 4003.4 | n_correct 317.3 | ppl 1825.6 | accuracy 7.926 | uer 46.888 | wer 48.924 | raw_wer 48.924 | wps 4851.2 | wpb 4003.4 | bsz 141.8 | num_updates 10311 | best_loss 8.095
2023-07-05 09:28:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10311 updates
2023-07-05 09:28:49 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 09:28:53 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 09:28:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 7 @ 10311 updates, score 8.715) (writing took 4.046339063999767 seconds)
2023-07-05 09:28:53 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-07-05 09:28:53 | INFO | train | epoch 007 | loss 6.288 | trans_loss 6.175 | nll_loss 5.623 | w2v_ctc_loss 3.025 | contrastive_loss 0 | total 4139.01 | n_correct 272.368 | ppl 49.27 | accuracy 6.581 | wps 13492.6 | ups 1.64 | wpb 8217.9 | bsz 305.8 | num_updates 10311 | lr 0.000139272 | gnorm 1.905 | clip 0 | loss_scale 8 | train_wall 845 | gb_free 13.6 | wall 3680
2023-07-05 09:28:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 09:28:54 | INFO | fairseq.trainer | begin training epoch 8
2023-07-05 09:28:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 09:29:52 | INFO | train_inner | epoch 008:     89 / 1474 loss=6.346, trans_loss=6.233, nll_loss=5.686, w2v_ctc_loss=3.047, contrastive_loss=0, total=4116.25, n_correct=270.11, ppl=51.47, accuracy=6.562, wps=9710.2, ups=1.19, wpb=8156.7, bsz=295.5, num_updates=10400, lr=0.000138675, gnorm=1.873, clip=0, loss_scale=8, train_wall=57, gb_free=17.2, wall=3739
2023-07-05 09:30:50 | INFO | train_inner | epoch 008:    189 / 1474 loss=6.369, trans_loss=6.234, nll_loss=5.687, w2v_ctc_loss=3.086, contrastive_loss=0, total=4037.23, n_correct=261.59, ppl=51.53, accuracy=6.479, wps=13932.5, ups=1.74, wpb=8004.3, bsz=285.7, num_updates=10500, lr=0.000138013, gnorm=1.972, clip=0, loss_scale=8, train_wall=57, gb_free=13.2, wall=3797
2023-07-05 09:31:47 | INFO | train_inner | epoch 008:    289 / 1474 loss=6.232, trans_loss=6.215, nll_loss=5.668, w2v_ctc_loss=2.901, contrastive_loss=0, total=4207.78, n_correct=276.36, ppl=50.83, accuracy=6.568, wps=14550.6, ups=1.74, wpb=8348.7, bsz=325.4, num_updates=10600, lr=0.000137361, gnorm=1.891, clip=0, loss_scale=8, train_wall=57, gb_free=13.3, wall=3854
2023-07-05 09:32:45 | INFO | train_inner | epoch 008:    389 / 1474 loss=6.353, trans_loss=6.259, nll_loss=5.713, w2v_ctc_loss=3.039, contrastive_loss=0, total=4127.24, n_correct=265.18, ppl=52.46, accuracy=6.425, wps=14053, ups=1.72, wpb=8189, bsz=294.3, num_updates=10700, lr=0.000136717, gnorm=2.111, clip=0, loss_scale=8, train_wall=58, gb_free=12.2, wall=3912
2023-07-05 09:33:43 | INFO | train_inner | epoch 008:    489 / 1474 loss=6.315, trans_loss=6.221, nll_loss=5.675, w2v_ctc_loss=3.01, contrastive_loss=0, total=4203.76, n_correct=275.33, ppl=51.08, accuracy=6.55, wps=14448.8, ups=1.73, wpb=8344.5, bsz=336.4, num_updates=10800, lr=0.000136083, gnorm=2.123, clip=0, loss_scale=8, train_wall=57, gb_free=14.8, wall=3970
2023-07-05 09:34:41 | INFO | train_inner | epoch 008:    589 / 1474 loss=6.267, trans_loss=6.239, nll_loss=5.698, w2v_ctc_loss=2.923, contrastive_loss=0, total=4062.5, n_correct=260.55, ppl=51.91, accuracy=6.414, wps=13988.6, ups=1.73, wpb=8082.9, bsz=285.2, num_updates=10900, lr=0.000135457, gnorm=2.1, clip=0, loss_scale=8, train_wall=57, gb_free=11.8, wall=4028
2023-07-05 09:35:38 | INFO | train_inner | epoch 008:    689 / 1474 loss=6.314, trans_loss=6.253, nll_loss=5.709, w2v_ctc_loss=2.982, contrastive_loss=0, total=4142.78, n_correct=270.62, ppl=52.31, accuracy=6.532, wps=14260.6, ups=1.73, wpb=8221.6, bsz=299.1, num_updates=11000, lr=0.00013484, gnorm=1.827, clip=0, loss_scale=16, train_wall=57, gb_free=16.1, wall=4086
2023-07-05 09:36:36 | INFO | train_inner | epoch 008:    789 / 1474 loss=6.255, trans_loss=6.229, nll_loss=5.684, w2v_ctc_loss=2.915, contrastive_loss=0, total=4118.9, n_correct=267.33, ppl=51.4, accuracy=6.49, wps=14230.3, ups=1.74, wpb=8192, bsz=298.5, num_updates=11100, lr=0.000134231, gnorm=1.928, clip=0, loss_scale=16, train_wall=57, gb_free=15.4, wall=4143
2023-07-05 09:37:34 | INFO | train_inner | epoch 008:    889 / 1474 loss=6.227, trans_loss=6.222, nll_loss=5.675, w2v_ctc_loss=2.879, contrastive_loss=0, total=4169.01, n_correct=275.62, ppl=51.09, accuracy=6.611, wps=14381.7, ups=1.74, wpb=8283.5, bsz=315.8, num_updates=11200, lr=0.000133631, gnorm=1.766, clip=0, loss_scale=16, train_wall=57, gb_free=16.4, wall=4201
2023-07-05 09:38:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-07-05 09:38:31 | INFO | train_inner | epoch 008:    990 / 1474 loss=6.202, trans_loss=6.259, nll_loss=5.714, w2v_ctc_loss=2.799, contrastive_loss=0, total=4147.3, n_correct=271.68, ppl=52.51, accuracy=6.551, wps=14258.5, ups=1.73, wpb=8234.8, bsz=307.5, num_updates=11300, lr=0.000133038, gnorm=1.939, clip=0, loss_scale=8, train_wall=57, gb_free=17.5, wall=4258
2023-07-05 09:39:29 | INFO | train_inner | epoch 008:   1090 / 1474 loss=6.266, trans_loss=6.269, nll_loss=5.728, w2v_ctc_loss=2.893, contrastive_loss=0, total=4197.39, n_correct=273.18, ppl=53.01, accuracy=6.508, wps=14348.6, ups=1.72, wpb=8332.1, bsz=310.9, num_updates=11400, lr=0.000132453, gnorm=1.992, clip=0, loss_scale=8, train_wall=58, gb_free=17, wall=4317
2023-07-05 09:40:27 | INFO | train_inner | epoch 008:   1190 / 1474 loss=6.087, trans_loss=6.243, nll_loss=5.699, w2v_ctc_loss=2.647, contrastive_loss=0, total=4180.55, n_correct=272.93, ppl=51.94, accuracy=6.529, wps=14491.5, ups=1.74, wpb=8306.7, bsz=315.1, num_updates=11500, lr=0.000131876, gnorm=1.83, clip=0, loss_scale=8, train_wall=57, gb_free=17.5, wall=4374
2023-07-05 09:41:24 | INFO | train_inner | epoch 008:   1290 / 1474 loss=6.276, trans_loss=6.268, nll_loss=5.725, w2v_ctc_loss=2.913, contrastive_loss=0, total=4062.6, n_correct=262.3, ppl=52.91, accuracy=6.456, wps=14128.8, ups=1.75, wpb=8072.7, bsz=291.9, num_updates=11600, lr=0.000131306, gnorm=1.895, clip=0, loss_scale=8, train_wall=57, gb_free=13.4, wall=4431
2023-07-05 09:42:21 | INFO | train_inner | epoch 008:   1390 / 1474 loss=6.266, trans_loss=6.3, nll_loss=5.758, w2v_ctc_loss=2.857, contrastive_loss=0, total=4159.11, n_correct=270.68, ppl=54.1, accuracy=6.508, wps=14396.8, ups=1.74, wpb=8259.9, bsz=312.5, num_updates=11700, lr=0.000130744, gnorm=1.93, clip=0, loss_scale=8, train_wall=57, gb_free=13.6, wall=4488
2023-07-05 09:43:09 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 09:43:24 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 8.706 | trans_loss 12.19 | nll_loss 11.05 | w2v_ctc_loss 0.575 | contrastive_loss 0 | total 4003.4 | n_correct 312 | ppl 2119.61 | accuracy 7.793 | uer 49.038 | wer 51.031 | raw_wer 51.031 | wps 4890.1 | wpb 4003.4 | bsz 141.8 | num_updates 11784 | best_loss 8.095
2023-07-05 09:43:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11784 updates
2023-07-05 09:43:24 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 09:43:28 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 09:43:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 8 @ 11784 updates, score 8.706) (writing took 3.988628738999978 seconds)
2023-07-05 09:43:28 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-07-05 09:43:28 | INFO | train | epoch 008 | loss 6.266 | trans_loss 6.249 | nll_loss 5.704 | w2v_ctc_loss 2.912 | contrastive_loss 0 | total 4138.34 | n_correct 269.576 | ppl 52.13 | accuracy 6.514 | wps 13833.9 | ups 1.68 | wpb 8216.6 | bsz 305.6 | num_updates 11784 | lr 0.000130277 | gnorm 1.94 | clip 0 | loss_scale 8 | train_wall 843 | gb_free 17.2 | wall 4555
2023-07-05 09:43:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 09:43:28 | INFO | fairseq.trainer | begin training epoch 9
2023-07-05 09:43:28 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 09:43:45 | INFO | train_inner | epoch 009:     16 / 1474 loss=6.22, trans_loss=6.29, nll_loss=5.746, w2v_ctc_loss=2.797, contrastive_loss=0, total=4121.25, n_correct=270.05, ppl=53.67, accuracy=6.553, wps=9778.4, ups=1.2, wpb=8177.2, bsz=310.7, num_updates=11800, lr=0.000130189, gnorm=1.889, clip=0, loss_scale=8, train_wall=57, gb_free=17.9, wall=4572
2023-07-05 09:44:42 | INFO | train_inner | epoch 009:    116 / 1474 loss=6.176, trans_loss=6.312, nll_loss=5.771, w2v_ctc_loss=2.701, contrastive_loss=0, total=4191.82, n_correct=271.65, ppl=54.62, accuracy=6.48, wps=14499.2, ups=1.74, wpb=8326.3, bsz=320, num_updates=11900, lr=0.000129641, gnorm=1.867, clip=0, loss_scale=8, train_wall=57, gb_free=16.2, wall=4629
2023-07-05 09:45:40 | INFO | train_inner | epoch 009:    216 / 1474 loss=6.252, trans_loss=6.32, nll_loss=5.78, w2v_ctc_loss=2.807, contrastive_loss=0, total=4061.27, n_correct=259.81, ppl=54.93, accuracy=6.397, wps=13893.4, ups=1.72, wpb=8065.2, bsz=287.6, num_updates=12000, lr=0.000129099, gnorm=1.858, clip=0, loss_scale=8, train_wall=58, gb_free=17.7, wall=4687
2023-07-05 09:45:40 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 09:45:55 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 8.925 | trans_loss 12.266 | nll_loss 11.121 | w2v_ctc_loss 1.13 | contrastive_loss 0 | total 4003.4 | n_correct 305.5 | ppl 2226.46 | accuracy 7.631 | uer 45.46 | wer 47.157 | raw_wer 47.157 | wps 4813.3 | wpb 4003.4 | bsz 141.8 | num_updates 12000 | best_loss 8.095
2023-07-05 09:45:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12000 updates
2023-07-05 09:45:55 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_9_12000.pt
2023-07-05 09:45:58 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_9_12000.pt
2023-07-05 09:46:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_9_12000.pt (epoch 9 @ 12000 updates, score 8.925) (writing took 4.919133395999779 seconds)
2023-07-05 09:46:58 | INFO | train_inner | epoch 009:    316 / 1474 loss=6.234, trans_loss=6.303, nll_loss=5.759, w2v_ctc_loss=2.805, contrastive_loss=0, total=4146.43, n_correct=270.16, ppl=54.16, accuracy=6.515, wps=10662.2, ups=1.29, wpb=8243.5, bsz=316.4, num_updates=12100, lr=0.000128565, gnorm=1.855, clip=0, loss_scale=8, train_wall=57, gb_free=16.9, wall=4765
2023-07-05 09:47:56 | INFO | train_inner | epoch 009:    416 / 1474 loss=6.213, trans_loss=6.293, nll_loss=5.751, w2v_ctc_loss=2.785, contrastive_loss=0, total=4194.84, n_correct=275.65, ppl=53.87, accuracy=6.571, wps=14273.9, ups=1.71, wpb=8329.8, bsz=311.2, num_updates=12200, lr=0.000128037, gnorm=1.983, clip=0, loss_scale=8, train_wall=58, gb_free=16.4, wall=4823
2023-07-05 09:48:54 | INFO | train_inner | epoch 009:    516 / 1474 loss=6.202, trans_loss=6.316, nll_loss=5.778, w2v_ctc_loss=2.733, contrastive_loss=0, total=4124.3, n_correct=269.65, ppl=54.88, accuracy=6.538, wps=14184.8, ups=1.73, wpb=8186, bsz=293, num_updates=12300, lr=0.000127515, gnorm=1.989, clip=0, loss_scale=8, train_wall=57, gb_free=12.1, wall=4881
2023-07-05 09:49:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-07-05 09:49:52 | INFO | train_inner | epoch 009:    617 / 1474 loss=6.196, trans_loss=6.296, nll_loss=5.758, w2v_ctc_loss=2.757, contrastive_loss=0, total=4139.18, n_correct=270.81, ppl=54.1, accuracy=6.543, wps=14107.1, ups=1.71, wpb=8230.6, bsz=308.8, num_updates=12400, lr=0.000127, gnorm=1.956, clip=0, loss_scale=4, train_wall=58, gb_free=15.8, wall=4939
2023-07-05 09:50:49 | INFO | train_inner | epoch 009:    717 / 1474 loss=6.103, trans_loss=6.311, nll_loss=5.774, w2v_ctc_loss=2.595, contrastive_loss=0, total=4075.27, n_correct=263.65, ppl=54.72, accuracy=6.47, wps=14271.7, ups=1.76, wpb=8101.1, bsz=295.8, num_updates=12500, lr=0.000126491, gnorm=1.977, clip=0, loss_scale=4, train_wall=56, gb_free=17.2, wall=4996
2023-07-05 09:51:47 | INFO | train_inner | epoch 009:    817 / 1474 loss=6.261, trans_loss=6.321, nll_loss=5.782, w2v_ctc_loss=2.825, contrastive_loss=0, total=4215.48, n_correct=276.91, ppl=55.04, accuracy=6.569, wps=14434.3, ups=1.72, wpb=8380.7, bsz=333.1, num_updates=12600, lr=0.000125988, gnorm=2.016, clip=0, loss_scale=4, train_wall=58, gb_free=17, wall=5054
2023-07-05 09:52:45 | INFO | train_inner | epoch 009:    917 / 1474 loss=6.232, trans_loss=6.353, nll_loss=5.811, w2v_ctc_loss=2.744, contrastive_loss=0, total=4152.4, n_correct=276.14, ppl=56.13, accuracy=6.65, wps=14103.9, ups=1.71, wpb=8235.8, bsz=300.5, num_updates=12700, lr=0.000125491, gnorm=1.871, clip=0, loss_scale=4, train_wall=58, gb_free=12.3, wall=5112
2023-07-05 09:53:43 | INFO | train_inner | epoch 009:   1017 / 1474 loss=6.322, trans_loss=6.383, nll_loss=5.843, w2v_ctc_loss=2.844, contrastive_loss=0, total=4101.32, n_correct=267.68, ppl=57.41, accuracy=6.527, wps=14119.9, ups=1.73, wpb=8141.1, bsz=283.2, num_updates=12800, lr=0.000125, gnorm=2.16, clip=0, loss_scale=4, train_wall=57, gb_free=16.1, wall=5170
2023-07-05 09:54:40 | INFO | train_inner | epoch 009:   1117 / 1474 loss=6.191, trans_loss=6.334, nll_loss=5.792, w2v_ctc_loss=2.7, contrastive_loss=0, total=4172.83, n_correct=276.05, ppl=55.42, accuracy=6.615, wps=14421.4, ups=1.74, wpb=8265, bsz=314.6, num_updates=12900, lr=0.000124515, gnorm=2.006, clip=0, loss_scale=4, train_wall=57, gb_free=16.7, wall=5227
2023-07-05 09:55:39 | INFO | train_inner | epoch 009:   1217 / 1474 loss=6.275, trans_loss=6.352, nll_loss=5.813, w2v_ctc_loss=2.82, contrastive_loss=0, total=4138.15, n_correct=273, ppl=56.24, accuracy=6.597, wps=14119.8, ups=1.72, wpb=8219.1, bsz=299.2, num_updates=13000, lr=0.000124035, gnorm=1.983, clip=0, loss_scale=4, train_wall=58, gb_free=16.5, wall=5286
2023-07-05 09:56:36 | INFO | train_inner | epoch 009:   1317 / 1474 loss=6.091, trans_loss=6.329, nll_loss=5.787, w2v_ctc_loss=2.555, contrastive_loss=0, total=4205.27, n_correct=280.51, ppl=55.22, accuracy=6.67, wps=14511.6, ups=1.74, wpb=8342.8, bsz=327.5, num_updates=13100, lr=0.00012356, gnorm=1.814, clip=0, loss_scale=4, train_wall=57, gb_free=16.9, wall=5343
2023-07-05 09:57:33 | INFO | train_inner | epoch 009:   1417 / 1474 loss=6.184, trans_loss=6.356, nll_loss=5.819, w2v_ctc_loss=2.666, contrastive_loss=0, total=4071.37, n_correct=265.04, ppl=56.44, accuracy=6.51, wps=14054.8, ups=1.74, wpb=8076.1, bsz=286, num_updates=13200, lr=0.000123091, gnorm=1.939, clip=0, loss_scale=4, train_wall=57, gb_free=15.2, wall=5401
2023-07-05 09:58:06 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 09:58:22 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 8.66 | trans_loss 12.26 | nll_loss 11.139 | w2v_ctc_loss 0.259 | contrastive_loss 0 | total 4003.4 | n_correct 294.3 | ppl 2254.86 | accuracy 7.351 | uer 50.837 | wer 53.264 | raw_wer 53.264 | wps 4364.6 | wpb 4003.4 | bsz 141.8 | num_updates 13257 | best_loss 8.095
2023-07-05 09:58:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 13257 updates
2023-07-05 09:58:22 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 09:58:26 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 09:58:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 9 @ 13257 updates, score 8.66) (writing took 4.201214968000386 seconds)
2023-07-05 09:58:26 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-07-05 09:58:26 | INFO | train | epoch 009 | loss 6.212 | trans_loss 6.327 | nll_loss 5.787 | w2v_ctc_loss 2.742 | contrastive_loss 0 | total 4138.68 | n_correct 270.881 | ppl 55.22 | accuracy 6.545 | wps 13487.2 | ups 1.64 | wpb 8217.2 | bsz 305.7 | num_updates 13257 | lr 0.000122827 | gnorm 1.952 | clip 0 | loss_scale 4 | train_wall 844 | gb_free 12 | wall 5453
2023-07-05 09:58:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 09:58:26 | INFO | fairseq.trainer | begin training epoch 10
2023-07-05 09:58:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 09:58:57 | INFO | train_inner | epoch 010:     43 / 1474 loss=6.172, trans_loss=6.334, nll_loss=5.795, w2v_ctc_loss=2.679, contrastive_loss=0, total=4113.02, n_correct=267.63, ppl=55.53, accuracy=6.507, wps=9726.4, ups=1.19, wpb=8163.4, bsz=317.3, num_updates=13300, lr=0.000122628, gnorm=1.951, clip=0, loss_scale=4, train_wall=56, gb_free=16.6, wall=5484
2023-07-05 09:59:55 | INFO | train_inner | epoch 010:    143 / 1474 loss=6.188, trans_loss=6.389, nll_loss=5.852, w2v_ctc_loss=2.643, contrastive_loss=0, total=4234.99, n_correct=274.82, ppl=57.75, accuracy=6.489, wps=14599.8, ups=1.74, wpb=8412.2, bsz=315.5, num_updates=13400, lr=0.000122169, gnorm=1.932, clip=0, loss_scale=4, train_wall=57, gb_free=16.7, wall=5542
2023-07-05 10:00:52 | INFO | train_inner | epoch 010:    243 / 1474 loss=6.266, trans_loss=6.406, nll_loss=5.866, w2v_ctc_loss=2.739, contrastive_loss=0, total=4131.11, n_correct=269.46, ppl=58.34, accuracy=6.523, wps=14305.4, ups=1.75, wpb=8197.6, bsz=309.3, num_updates=13500, lr=0.000121716, gnorm=1.959, clip=0, loss_scale=4, train_wall=57, gb_free=16.5, wall=5599
2023-07-05 10:01:50 | INFO | train_inner | epoch 010:    343 / 1474 loss=6.121, trans_loss=6.38, nll_loss=5.842, w2v_ctc_loss=2.544, contrastive_loss=0, total=4135.65, n_correct=272.95, ppl=57.35, accuracy=6.6, wps=14193.4, ups=1.73, wpb=8226.7, bsz=302.7, num_updates=13600, lr=0.000121268, gnorm=1.908, clip=0, loss_scale=4, train_wall=58, gb_free=15.9, wall=5657
2023-07-05 10:02:49 | INFO | train_inner | epoch 010:    443 / 1474 loss=6.205, trans_loss=6.405, nll_loss=5.867, w2v_ctc_loss=2.648, contrastive_loss=0, total=4199.14, n_correct=278.77, ppl=58.37, accuracy=6.639, wps=14241.3, ups=1.71, wpb=8336.8, bsz=321.5, num_updates=13700, lr=0.000120824, gnorm=1.864, clip=0, loss_scale=4, train_wall=58, gb_free=17.2, wall=5716
2023-07-05 10:03:47 | INFO | train_inner | epoch 010:    543 / 1474 loss=6.205, trans_loss=6.412, nll_loss=5.877, w2v_ctc_loss=2.648, contrastive_loss=0, total=4094.23, n_correct=266.19, ppl=58.77, accuracy=6.502, wps=14025.5, ups=1.73, wpb=8115.3, bsz=288.8, num_updates=13800, lr=0.000120386, gnorm=1.966, clip=0, loss_scale=4, train_wall=57, gb_free=14.7, wall=5774
2023-07-05 10:04:44 | INFO | train_inner | epoch 010:    643 / 1474 loss=6.187, trans_loss=6.418, nll_loss=5.882, w2v_ctc_loss=2.607, contrastive_loss=0, total=4182.84, n_correct=277.09, ppl=58.99, accuracy=6.624, wps=14364.5, ups=1.73, wpb=8298.4, bsz=320.9, num_updates=13900, lr=0.000119952, gnorm=1.855, clip=0, loss_scale=4, train_wall=57, gb_free=16.9, wall=5832
2023-07-05 10:05:42 | INFO | train_inner | epoch 010:    743 / 1474 loss=6.281, trans_loss=6.411, nll_loss=5.875, w2v_ctc_loss=2.764, contrastive_loss=0, total=4120.62, n_correct=269.94, ppl=58.69, accuracy=6.551, wps=14259.3, ups=1.74, wpb=8180.6, bsz=301.1, num_updates=14000, lr=0.000119523, gnorm=2.235, clip=0, loss_scale=4, train_wall=57, gb_free=17.2, wall=5889
2023-07-05 10:05:42 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 10:05:57 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 8.799 | trans_loss 12.461 | nll_loss 11.363 | w2v_ctc_loss 0.255 | contrastive_loss 0 | total 4003.4 | n_correct 301.5 | ppl 2633.79 | accuracy 7.531 | uer 51.597 | wer 53.428 | raw_wer 53.428 | wps 4855.4 | wpb 4003.4 | bsz 141.8 | num_updates 14000 | best_loss 8.095
2023-07-05 10:05:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14000 updates
2023-07-05 10:05:57 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_10_14000.pt
2023-07-05 10:05:59 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_10_14000.pt
2023-07-05 10:06:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_10_14000.pt (epoch 10 @ 14000 updates, score 8.799) (writing took 4.8830079050003405 seconds)
2023-07-05 10:07:00 | INFO | train_inner | epoch 010:    843 / 1474 loss=6.123, trans_loss=6.388, nll_loss=5.852, w2v_ctc_loss=2.538, contrastive_loss=0, total=4132.62, n_correct=272.62, ppl=57.75, accuracy=6.597, wps=10552.9, ups=1.29, wpb=8206.6, bsz=304.9, num_updates=14100, lr=0.000119098, gnorm=1.857, clip=0, loss_scale=4, train_wall=57, gb_free=17, wall=5967
2023-07-05 10:07:57 | INFO | train_inner | epoch 010:    943 / 1474 loss=6.22, trans_loss=6.406, nll_loss=5.865, w2v_ctc_loss=2.667, contrastive_loss=0, total=4160.84, n_correct=276.42, ppl=58.3, accuracy=6.643, wps=14496.5, ups=1.76, wpb=8250.4, bsz=312, num_updates=14200, lr=0.000118678, gnorm=1.823, clip=0, loss_scale=4, train_wall=56, gb_free=16, wall=6024
2023-07-05 10:08:54 | INFO | train_inner | epoch 010:   1043 / 1474 loss=6.237, trans_loss=6.435, nll_loss=5.899, w2v_ctc_loss=2.67, contrastive_loss=0, total=4059.22, n_correct=265.21, ppl=59.69, accuracy=6.534, wps=13954.4, ups=1.73, wpb=8060.8, bsz=287.4, num_updates=14300, lr=0.000118262, gnorm=1.974, clip=0, loss_scale=4, train_wall=57, gb_free=10.1, wall=6081
2023-07-05 10:09:51 | INFO | train_inner | epoch 010:   1143 / 1474 loss=6.353, trans_loss=6.441, nll_loss=5.906, w2v_ctc_loss=2.838, contrastive_loss=0, total=4045.82, n_correct=262.27, ppl=59.97, accuracy=6.482, wps=14076, ups=1.75, wpb=8033.5, bsz=281.9, num_updates=14400, lr=0.000117851, gnorm=2.138, clip=0, loss_scale=8, train_wall=57, gb_free=13.2, wall=6138
2023-07-05 10:10:49 | INFO | train_inner | epoch 010:   1243 / 1474 loss=6.145, trans_loss=6.382, nll_loss=5.848, w2v_ctc_loss=2.577, contrastive_loss=0, total=4107.6, n_correct=263.16, ppl=57.61, accuracy=6.407, wps=14236.5, ups=1.74, wpb=8176.9, bsz=297.6, num_updates=14500, lr=0.000117444, gnorm=1.919, clip=0, loss_scale=8, train_wall=57, gb_free=17, wall=6196
2023-07-05 10:11:46 | INFO | train_inner | epoch 010:   1343 / 1474 loss=6.239, trans_loss=6.404, nll_loss=5.867, w2v_ctc_loss=2.71, contrastive_loss=0, total=4127.69, n_correct=268.9, ppl=58.38, accuracy=6.515, wps=14223, ups=1.73, wpb=8198.7, bsz=301.4, num_updates=14600, lr=0.000117041, gnorm=1.876, clip=0, loss_scale=8, train_wall=57, gb_free=16.4, wall=6254
2023-07-05 10:12:44 | INFO | train_inner | epoch 010:   1443 / 1474 loss=6.161, trans_loss=6.388, nll_loss=5.852, w2v_ctc_loss=2.594, contrastive_loss=0, total=4195.02, n_correct=271.8, ppl=57.77, accuracy=6.479, wps=14406.9, ups=1.73, wpb=8319.1, bsz=322, num_updates=14700, lr=0.000116642, gnorm=1.995, clip=0, loss_scale=8, train_wall=57, gb_free=16.8, wall=6311
2023-07-05 10:13:02 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 10:13:17 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 9.09 | trans_loss 12.295 | nll_loss 11.165 | w2v_ctc_loss 1.612 | contrastive_loss 0 | total 4003.4 | n_correct 315 | ppl 2295.8 | accuracy 7.868 | uer 42.473 | wer 44.286 | raw_wer 44.286 | wps 4815.6 | wpb 4003.4 | bsz 141.8 | num_updates 14731 | best_loss 8.095
2023-07-05 10:13:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14731 updates
2023-07-05 10:13:17 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 10:13:21 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 10:13:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 10 @ 14731 updates, score 9.09) (writing took 4.035503257000528 seconds)
2023-07-05 10:13:21 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-07-05 10:13:21 | INFO | train | epoch 010 | loss 6.203 | trans_loss 6.402 | nll_loss 5.866 | w2v_ctc_loss 2.65 | contrastive_loss 0 | total 4138.65 | n_correct 270.83 | ppl 58.31 | accuracy 6.544 | wps 13527.3 | ups 1.65 | wpb 8217.2 | bsz 305.7 | num_updates 14731 | lr 0.00011652 | gnorm 1.943 | clip 0 | loss_scale 8 | train_wall 842 | gb_free 17.4 | wall 6348
2023-07-05 10:13:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 10:13:21 | INFO | fairseq.trainer | begin training epoch 11
2023-07-05 10:13:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 10:14:08 | INFO | train_inner | epoch 011:     69 / 1474 loss=6.161, trans_loss=6.402, nll_loss=5.869, w2v_ctc_loss=2.584, contrastive_loss=0, total=4166, n_correct=271.17, ppl=58.45, accuracy=6.509, wps=9912.7, ups=1.2, wpb=8270.1, bsz=317.1, num_updates=14800, lr=0.000116248, gnorm=1.854, clip=0, loss_scale=8, train_wall=56, gb_free=18, wall=6395
2023-07-05 10:15:05 | INFO | train_inner | epoch 011:    169 / 1474 loss=6.237, trans_loss=6.408, nll_loss=5.875, w2v_ctc_loss=2.698, contrastive_loss=0, total=4100.74, n_correct=263.43, ppl=58.7, accuracy=6.424, wps=14126.1, ups=1.73, wpb=8150.4, bsz=300.4, num_updates=14900, lr=0.000115857, gnorm=2.103, clip=0, loss_scale=8, train_wall=57, gb_free=14.7, wall=6452
2023-07-05 10:16:03 | INFO | train_inner | epoch 011:    269 / 1474 loss=6.239, trans_loss=6.425, nll_loss=5.891, w2v_ctc_loss=2.673, contrastive_loss=0, total=4115.58, n_correct=264.55, ppl=59.35, accuracy=6.428, wps=14248.3, ups=1.74, wpb=8175, bsz=296.1, num_updates=15000, lr=0.00011547, gnorm=2.055, clip=0, loss_scale=8, train_wall=57, gb_free=16.3, wall=6510
2023-07-05 10:17:00 | INFO | train_inner | epoch 011:    369 / 1474 loss=6.182, trans_loss=6.439, nll_loss=5.904, w2v_ctc_loss=2.579, contrastive_loss=0, total=4094.16, n_correct=262.46, ppl=59.88, accuracy=6.411, wps=14262, ups=1.76, wpb=8117.4, bsz=295.6, num_updates=15100, lr=0.000115087, gnorm=1.946, clip=0, loss_scale=8, train_wall=56, gb_free=13.2, wall=6567
2023-07-05 10:17:58 | INFO | train_inner | epoch 011:    469 / 1474 loss=6.269, trans_loss=6.458, nll_loss=5.924, w2v_ctc_loss=2.695, contrastive_loss=0, total=4112.8, n_correct=266.39, ppl=60.7, accuracy=6.477, wps=14005.5, ups=1.72, wpb=8151.7, bsz=302.2, num_updates=15200, lr=0.000114708, gnorm=2.04, clip=0, loss_scale=8, train_wall=58, gb_free=17.3, wall=6625
2023-07-05 10:18:56 | INFO | train_inner | epoch 011:    569 / 1474 loss=6.204, trans_loss=6.406, nll_loss=5.874, w2v_ctc_loss=2.642, contrastive_loss=0, total=4071.06, n_correct=270.02, ppl=58.63, accuracy=6.633, wps=13981.7, ups=1.73, wpb=8095.2, bsz=292.6, num_updates=15300, lr=0.000114332, gnorm=2.063, clip=0, loss_scale=8, train_wall=57, gb_free=16.7, wall=6683
2023-07-05 10:19:53 | INFO | train_inner | epoch 011:    669 / 1474 loss=6.171, trans_loss=6.459, nll_loss=5.924, w2v_ctc_loss=2.545, contrastive_loss=0, total=4156.4, n_correct=272.02, ppl=60.72, accuracy=6.545, wps=14300.8, ups=1.73, wpb=8244.8, bsz=310.2, num_updates=15400, lr=0.000113961, gnorm=1.955, clip=0, loss_scale=8, train_wall=57, gb_free=16.8, wall=6741
2023-07-05 10:20:51 | INFO | train_inner | epoch 011:    769 / 1474 loss=6.246, trans_loss=6.465, nll_loss=5.932, w2v_ctc_loss=2.644, contrastive_loss=0, total=4169.17, n_correct=266.96, ppl=61.06, accuracy=6.403, wps=14359.5, ups=1.74, wpb=8276.1, bsz=304.8, num_updates=15500, lr=0.000113592, gnorm=2.085, clip=0, loss_scale=8, train_wall=57, gb_free=12.5, wall=6798
2023-07-05 10:21:49 | INFO | train_inner | epoch 011:    869 / 1474 loss=6.14, trans_loss=6.423, nll_loss=5.892, w2v_ctc_loss=2.532, contrastive_loss=0, total=4120.01, n_correct=265.95, ppl=59.38, accuracy=6.455, wps=14187.5, ups=1.73, wpb=8180.7, bsz=293.5, num_updates=15600, lr=0.000113228, gnorm=2.072, clip=0, loss_scale=8, train_wall=57, gb_free=13.8, wall=6856
2023-07-05 10:22:46 | INFO | train_inner | epoch 011:    969 / 1474 loss=6.066, trans_loss=6.421, nll_loss=5.891, w2v_ctc_loss=2.415, contrastive_loss=0, total=4145.45, n_correct=265.04, ppl=59.35, accuracy=6.394, wps=14271.2, ups=1.73, wpb=8227.7, bsz=303.7, num_updates=15700, lr=0.000112867, gnorm=1.937, clip=0, loss_scale=8, train_wall=57, gb_free=17.3, wall=6913
2023-07-05 10:23:44 | INFO | train_inner | epoch 011:   1069 / 1474 loss=6.161, trans_loss=6.418, nll_loss=5.886, w2v_ctc_loss=2.571, contrastive_loss=0, total=4141.18, n_correct=266, ppl=59.13, accuracy=6.423, wps=14303.2, ups=1.74, wpb=8227.9, bsz=309.4, num_updates=15800, lr=0.000112509, gnorm=2.001, clip=0, loss_scale=8, train_wall=57, gb_free=16.8, wall=6971
2023-07-05 10:24:42 | INFO | train_inner | epoch 011:   1169 / 1474 loss=6.083, trans_loss=6.417, nll_loss=5.888, w2v_ctc_loss=2.447, contrastive_loss=0, total=4173.93, n_correct=263.34, ppl=59.23, accuracy=6.309, wps=14318.5, ups=1.73, wpb=8292.2, bsz=307.2, num_updates=15900, lr=0.000112154, gnorm=2.053, clip=0, loss_scale=8, train_wall=58, gb_free=17.1, wall=7029
2023-07-05 10:25:40 | INFO | train_inner | epoch 011:   1269 / 1474 loss=6.18, trans_loss=6.438, nll_loss=5.909, w2v_ctc_loss=2.569, contrastive_loss=0, total=4174.26, n_correct=264.64, ppl=60.11, accuracy=6.34, wps=14333, ups=1.73, wpb=8290.4, bsz=314.4, num_updates=16000, lr=0.000111803, gnorm=2.03, clip=0, loss_scale=8, train_wall=57, gb_free=17.6, wall=7087
2023-07-05 10:25:40 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 10:25:55 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 8.765 | trans_loss 12.433 | nll_loss 11.353 | w2v_ctc_loss 0.207 | contrastive_loss 0 | total 4003.4 | n_correct 281.5 | ppl 2615.75 | accuracy 7.032 | uer 54.211 | wer 56.396 | raw_wer 56.396 | wps 4787.3 | wpb 4003.4 | bsz 141.8 | num_updates 16000 | best_loss 8.095
2023-07-05 10:25:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16000 updates
2023-07-05 10:25:55 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_11_16000.pt
2023-07-05 10:25:57 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_11_16000.pt
2023-07-05 10:26:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_11_16000.pt (epoch 11 @ 16000 updates, score 8.765) (writing took 5.047280531998695 seconds)
2023-07-05 10:26:58 | INFO | train_inner | epoch 011:   1369 / 1474 loss=6.155, trans_loss=6.446, nll_loss=5.919, w2v_ctc_loss=2.525, contrastive_loss=0, total=4191.56, n_correct=269.09, ppl=60.49, accuracy=6.42, wps=10615.8, ups=1.28, wpb=8318.6, bsz=327.7, num_updates=16100, lr=0.000111456, gnorm=2.078, clip=0, loss_scale=8, train_wall=57, gb_free=17.6, wall=7165
2023-07-05 10:27:56 | INFO | train_inner | epoch 011:   1469 / 1474 loss=6.148, trans_loss=6.452, nll_loss=5.926, w2v_ctc_loss=2.51, contrastive_loss=0, total=4161.81, n_correct=261.6, ppl=60.8, accuracy=6.286, wps=14278.6, ups=1.73, wpb=8263, bsz=313.2, num_updates=16200, lr=0.000111111, gnorm=1.913, clip=0, loss_scale=8, train_wall=57, gb_free=17, wall=7223
2023-07-05 10:27:59 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 10:28:14 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 9.265 | trans_loss 12.506 | nll_loss 11.394 | w2v_ctc_loss 1.702 | contrastive_loss 0 | total 4003.4 | n_correct 308.2 | ppl 2691.57 | accuracy 7.698 | uer 41.266 | wer 42.802 | raw_wer 42.802 | wps 4782.1 | wpb 4003.4 | bsz 141.8 | num_updates 16205 | best_loss 8.095
2023-07-05 10:28:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16205 updates
2023-07-05 10:28:14 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 10:28:18 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 10:28:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 11 @ 16205 updates, score 9.265) (writing took 4.081803138000396 seconds)
2023-07-05 10:28:18 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-07-05 10:28:18 | INFO | train | epoch 011 | loss 6.177 | trans_loss 6.432 | nll_loss 5.901 | w2v_ctc_loss 2.576 | contrastive_loss 0 | total 4138.65 | n_correct 265.948 | ppl 59.76 | accuracy 6.426 | wps 13504 | ups 1.64 | wpb 8217.2 | bsz 305.7 | num_updates 16205 | lr 0.000111094 | gnorm 2.015 | clip 0 | loss_scale 8 | train_wall 844 | gb_free 17.4 | wall 7245
2023-07-05 10:28:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 10:28:18 | INFO | fairseq.trainer | begin training epoch 12
2023-07-05 10:28:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 10:29:20 | INFO | train_inner | epoch 012:     95 / 1474 loss=6.237, trans_loss=6.506, nll_loss=5.978, w2v_ctc_loss=2.59, contrastive_loss=0, total=4139.2, n_correct=268.57, ppl=63.02, accuracy=6.488, wps=9761.9, ups=1.19, wpb=8214.7, bsz=312.5, num_updates=16300, lr=0.00011077, gnorm=1.979, clip=0, loss_scale=8, train_wall=57, gb_free=16.1, wall=7307
2023-07-05 10:30:18 | INFO | train_inner | epoch 012:    195 / 1474 loss=6.205, trans_loss=6.49, nll_loss=5.963, w2v_ctc_loss=2.56, contrastive_loss=0, total=4126.87, n_correct=262.29, ppl=62.39, accuracy=6.356, wps=14264, ups=1.74, wpb=8208.1, bsz=295.9, num_updates=16400, lr=0.000110432, gnorm=2.045, clip=0, loss_scale=8, train_wall=57, gb_free=16.7, wall=7365
2023-07-05 10:30:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-07-05 10:31:16 | INFO | train_inner | epoch 012:    296 / 1474 loss=6.115, trans_loss=6.481, nll_loss=5.953, w2v_ctc_loss=2.435, contrastive_loss=0, total=4212.28, n_correct=272.64, ppl=61.97, accuracy=6.473, wps=14304.7, ups=1.71, wpb=8368.3, bsz=323.4, num_updates=16500, lr=0.000110096, gnorm=1.975, clip=0, loss_scale=8, train_wall=58, gb_free=12.7, wall=7423
2023-07-05 10:32:14 | INFO | train_inner | epoch 012:    396 / 1474 loss=6.174, trans_loss=6.461, nll_loss=5.933, w2v_ctc_loss=2.539, contrastive_loss=0, total=4144.42, n_correct=264.46, ppl=61.08, accuracy=6.381, wps=14265.6, ups=1.73, wpb=8233.6, bsz=305.3, num_updates=16600, lr=0.000109764, gnorm=1.979, clip=0, loss_scale=8, train_wall=57, gb_free=16.7, wall=7481
2023-07-05 10:33:11 | INFO | train_inner | epoch 012:    496 / 1474 loss=6.163, trans_loss=6.494, nll_loss=5.968, w2v_ctc_loss=2.489, contrastive_loss=0, total=4095.26, n_correct=260.76, ppl=62.58, accuracy=6.367, wps=14114.1, ups=1.74, wpb=8107.3, bsz=299.8, num_updates=16700, lr=0.000109435, gnorm=2.054, clip=0, loss_scale=8, train_wall=57, gb_free=17.8, wall=7538
2023-07-05 10:34:09 | INFO | train_inner | epoch 012:    596 / 1474 loss=6.114, trans_loss=6.459, nll_loss=5.934, w2v_ctc_loss=2.46, contrastive_loss=0, total=4204.6, n_correct=265.42, ppl=61.15, accuracy=6.313, wps=14374.4, ups=1.72, wpb=8361.1, bsz=319.2, num_updates=16800, lr=0.000109109, gnorm=1.959, clip=0, loss_scale=8, train_wall=58, gb_free=16.6, wall=7597
2023-07-05 10:35:07 | INFO | train_inner | epoch 012:    696 / 1474 loss=6.101, trans_loss=6.477, nll_loss=5.95, w2v_ctc_loss=2.41, contrastive_loss=0, total=4197.19, n_correct=271.66, ppl=61.83, accuracy=6.472, wps=14520.8, ups=1.75, wpb=8316.1, bsz=323.2, num_updates=16900, lr=0.000108786, gnorm=2.018, clip=0, loss_scale=8, train_wall=57, gb_free=16.8, wall=7654
2023-07-05 10:36:04 | INFO | train_inner | epoch 012:    796 / 1474 loss=6.198, trans_loss=6.481, nll_loss=5.959, w2v_ctc_loss=2.557, contrastive_loss=0, total=4094.06, n_correct=260.06, ppl=62.21, accuracy=6.352, wps=14084.8, ups=1.73, wpb=8125.6, bsz=298.2, num_updates=17000, lr=0.000108465, gnorm=2.002, clip=0, loss_scale=8, train_wall=57, gb_free=15, wall=7711
2023-07-05 10:37:02 | INFO | train_inner | epoch 012:    896 / 1474 loss=6.088, trans_loss=6.455, nll_loss=5.934, w2v_ctc_loss=2.419, contrastive_loss=0, total=4163.5, n_correct=264.34, ppl=61.12, accuracy=6.349, wps=14305.4, ups=1.73, wpb=8272.3, bsz=305.1, num_updates=17100, lr=0.000108148, gnorm=1.989, clip=0, loss_scale=8, train_wall=57, gb_free=13.1, wall=7769
2023-07-05 10:38:00 | INFO | train_inner | epoch 012:    996 / 1474 loss=6.195, trans_loss=6.509, nll_loss=5.986, w2v_ctc_loss=2.516, contrastive_loss=0, total=4124.99, n_correct=259.37, ppl=63.39, accuracy=6.288, wps=14169, ups=1.73, wpb=8186, bsz=302.7, num_updates=17200, lr=0.000107833, gnorm=2.038, clip=0, loss_scale=8, train_wall=57, gb_free=11.6, wall=7827
2023-07-05 10:38:58 | INFO | train_inner | epoch 012:   1096 / 1474 loss=6.214, trans_loss=6.526, nll_loss=6.003, w2v_ctc_loss=2.53, contrastive_loss=0, total=4046.6, n_correct=255.15, ppl=64.15, accuracy=6.305, wps=13906.4, ups=1.73, wpb=8031.9, bsz=289.8, num_updates=17300, lr=0.000107521, gnorm=2.069, clip=0, loss_scale=8, train_wall=57, gb_free=16.7, wall=7885
2023-07-05 10:39:56 | INFO | train_inner | epoch 012:   1196 / 1474 loss=6.199, trans_loss=6.49, nll_loss=5.968, w2v_ctc_loss=2.549, contrastive_loss=0, total=4196.85, n_correct=263.24, ppl=62.61, accuracy=6.272, wps=14382.3, ups=1.72, wpb=8339.3, bsz=319, num_updates=17400, lr=0.000107211, gnorm=2.057, clip=0, loss_scale=8, train_wall=58, gb_free=16.8, wall=7943
2023-07-05 10:40:54 | INFO | train_inner | epoch 012:   1296 / 1474 loss=6.294, trans_loss=6.553, nll_loss=6.034, w2v_ctc_loss=2.63, contrastive_loss=0, total=4067.78, n_correct=257.63, ppl=65.55, accuracy=6.333, wps=13975.3, ups=1.73, wpb=8085.3, bsz=285.5, num_updates=17500, lr=0.000106904, gnorm=1.866, clip=0, loss_scale=8, train_wall=57, gb_free=15.5, wall=8001
2023-07-05 10:41:52 | INFO | train_inner | epoch 012:   1396 / 1474 loss=6.22, trans_loss=6.542, nll_loss=6.019, w2v_ctc_loss=2.53, contrastive_loss=0, total=4142.88, n_correct=257.5, ppl=64.84, accuracy=6.215, wps=14186.9, ups=1.73, wpb=8218.4, bsz=306.2, num_updates=17600, lr=0.0001066, gnorm=1.947, clip=0, loss_scale=8, train_wall=58, gb_free=15.8, wall=8059
2023-07-05 10:42:37 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 10:42:52 | INFO | dev_st | epoch 012 | valid on 'dev_st' subset | loss 9.247 | trans_loss 12.638 | nll_loss 11.541 | w2v_ctc_loss 1.334 | contrastive_loss 0 | total 4003.4 | n_correct 300.6 | ppl 2980.41 | accuracy 7.509 | uer 44.738 | wer 46.937 | raw_wer 46.937 | wps 4873.9 | wpb 4003.4 | bsz 141.8 | num_updates 17678 | best_loss 8.095
2023-07-05 10:42:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 17678 updates
2023-07-05 10:42:52 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 10:42:56 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 10:42:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 12 @ 17678 updates, score 9.247) (writing took 4.098370294001143 seconds)
2023-07-05 10:42:56 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-07-05 10:42:56 | INFO | train | epoch 012 | loss 6.181 | trans_loss 6.496 | nll_loss 5.972 | w2v_ctc_loss 2.516 | contrastive_loss 0 | total 4138.93 | n_correct 262.733 | ppl 62.76 | accuracy 6.348 | wps 13788.6 | ups 1.68 | wpb 8217.7 | bsz 305.7 | num_updates 17678 | lr 0.000106365 | gnorm 2.005 | clip 0 | loss_scale 8 | train_wall 845 | gb_free 13.2 | wall 8123
2023-07-05 10:42:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 10:42:56 | INFO | fairseq.trainer | begin training epoch 13
2023-07-05 10:42:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 10:43:16 | INFO | train_inner | epoch 013:     22 / 1474 loss=6.223, trans_loss=6.519, nll_loss=5.998, w2v_ctc_loss=2.554, contrastive_loss=0, total=4097.08, n_correct=254.11, ppl=63.93, accuracy=6.202, wps=9652.7, ups=1.19, wpb=8140.3, bsz=296.6, num_updates=17700, lr=0.000106299, gnorm=2.1, clip=0, loss_scale=8, train_wall=57, gb_free=16.6, wall=8143
2023-07-05 10:44:13 | INFO | train_inner | epoch 013:    122 / 1474 loss=6.081, trans_loss=6.505, nll_loss=5.984, w2v_ctc_loss=2.349, contrastive_loss=0, total=4164.24, n_correct=260.91, ppl=63.29, accuracy=6.265, wps=14355.1, ups=1.74, wpb=8268.9, bsz=301.9, num_updates=17800, lr=0.000106, gnorm=1.964, clip=0, loss_scale=8, train_wall=57, gb_free=17.1, wall=8201
2023-07-05 10:45:12 | INFO | train_inner | epoch 013:    222 / 1474 loss=6.1, trans_loss=6.513, nll_loss=5.988, w2v_ctc_loss=2.374, contrastive_loss=0, total=4201.52, n_correct=269.41, ppl=63.47, accuracy=6.412, wps=14297.7, ups=1.72, wpb=8335.4, bsz=328.5, num_updates=17900, lr=0.000105703, gnorm=1.847, clip=0, loss_scale=8, train_wall=58, gb_free=13.4, wall=8259
2023-07-05 10:46:09 | INFO | train_inner | epoch 013:    322 / 1474 loss=6.228, trans_loss=6.54, nll_loss=6.017, w2v_ctc_loss=2.536, contrastive_loss=0, total=4102.53, n_correct=259.84, ppl=64.78, accuracy=6.334, wps=14166.6, ups=1.74, wpb=8134.9, bsz=293.9, num_updates=18000, lr=0.000105409, gnorm=2.109, clip=0, loss_scale=8, train_wall=57, gb_free=16.3, wall=8316
2023-07-05 10:46:09 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 10:46:25 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 9.281 | trans_loss 12.602 | nll_loss 11.507 | w2v_ctc_loss 1.533 | contrastive_loss 0 | total 4003.4 | n_correct 299.8 | ppl 2909.79 | accuracy 7.489 | uer 41.226 | wer 43.067 | raw_wer 43.067 | wps 4809.7 | wpb 4003.4 | bsz 141.8 | num_updates 18000 | best_loss 8.095
2023-07-05 10:46:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 18000 updates
2023-07-05 10:46:25 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_13_18000.pt
2023-07-05 10:46:27 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_13_18000.pt
2023-07-05 10:46:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_13_18000.pt (epoch 13 @ 18000 updates, score 9.281) (writing took 4.830066243999681 seconds)
2023-07-05 10:47:27 | INFO | train_inner | epoch 013:    422 / 1474 loss=6.089, trans_loss=6.525, nll_loss=6.003, w2v_ctc_loss=2.338, contrastive_loss=0, total=4190.45, n_correct=268.54, ppl=64.13, accuracy=6.408, wps=10757.6, ups=1.29, wpb=8314.9, bsz=320.3, num_updates=18100, lr=0.000105118, gnorm=1.936, clip=0, loss_scale=8, train_wall=57, gb_free=16.1, wall=8394
2023-07-05 10:48:25 | INFO | train_inner | epoch 013:    522 / 1474 loss=6.105, trans_loss=6.521, nll_loss=5.999, w2v_ctc_loss=2.37, contrastive_loss=0, total=4194.45, n_correct=265.75, ppl=63.96, accuracy=6.336, wps=14341.8, ups=1.72, wpb=8322.2, bsz=319, num_updates=18200, lr=0.000104828, gnorm=1.999, clip=0, loss_scale=8, train_wall=58, gb_free=17.5, wall=8452
2023-07-05 10:49:22 | INFO | train_inner | epoch 013:    622 / 1474 loss=6.151, trans_loss=6.549, nll_loss=6.026, w2v_ctc_loss=2.41, contrastive_loss=0, total=4158.04, n_correct=262.65, ppl=65.18, accuracy=6.317, wps=14322.7, ups=1.73, wpb=8258.1, bsz=306.7, num_updates=18300, lr=0.000104542, gnorm=1.949, clip=0, loss_scale=8, train_wall=57, gb_free=13.8, wall=8509
2023-07-05 10:50:20 | INFO | train_inner | epoch 013:    722 / 1474 loss=6.3, trans_loss=6.583, nll_loss=6.061, w2v_ctc_loss=2.604, contrastive_loss=0, total=4099.91, n_correct=260.16, ppl=66.77, accuracy=6.346, wps=14055.5, ups=1.73, wpb=8140.3, bsz=285.5, num_updates=18400, lr=0.000104257, gnorm=2.035, clip=0, loss_scale=8, train_wall=58, gb_free=17, wall=8567
2023-07-05 10:51:19 | INFO | train_inner | epoch 013:    822 / 1474 loss=6.229, trans_loss=6.551, nll_loss=6.032, w2v_ctc_loss=2.525, contrastive_loss=0, total=4122.78, n_correct=262.34, ppl=65.42, accuracy=6.363, wps=13968.7, ups=1.71, wpb=8189.7, bsz=306, num_updates=18500, lr=0.000103975, gnorm=2.018, clip=0, loss_scale=16, train_wall=58, gb_free=15.2, wall=8626
2023-07-05 10:52:16 | INFO | train_inner | epoch 013:    922 / 1474 loss=6.162, trans_loss=6.536, nll_loss=6.015, w2v_ctc_loss=2.447, contrastive_loss=0, total=4102.59, n_correct=255.64, ppl=64.65, accuracy=6.231, wps=14215.6, ups=1.75, wpb=8146.2, bsz=296.6, num_updates=18600, lr=0.000103695, gnorm=2.013, clip=0, loss_scale=16, train_wall=57, gb_free=17.5, wall=8683
2023-07-05 10:52:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-07-05 10:53:14 | INFO | train_inner | epoch 013:   1023 / 1474 loss=6.211, trans_loss=6.541, nll_loss=6.024, w2v_ctc_loss=2.516, contrastive_loss=0, total=4082.13, n_correct=258.94, ppl=65.06, accuracy=6.343, wps=13990.5, ups=1.72, wpb=8124.7, bsz=292.6, num_updates=18700, lr=0.000103418, gnorm=1.995, clip=0, loss_scale=8, train_wall=58, gb_free=17.6, wall=8741
2023-07-05 10:54:11 | INFO | train_inner | epoch 013:   1123 / 1474 loss=6.147, trans_loss=6.551, nll_loss=6.031, w2v_ctc_loss=2.397, contrastive_loss=0, total=4103.17, n_correct=259.3, ppl=65.37, accuracy=6.32, wps=14225.2, ups=1.75, wpb=8146.3, bsz=305.6, num_updates=18800, lr=0.000103142, gnorm=2.044, clip=0, loss_scale=8, train_wall=57, gb_free=17.8, wall=8798
2023-07-05 10:55:09 | INFO | train_inner | epoch 013:   1223 / 1474 loss=6.212, trans_loss=6.549, nll_loss=6.029, w2v_ctc_loss=2.496, contrastive_loss=0, total=4124.88, n_correct=257.83, ppl=65.31, accuracy=6.251, wps=14142.9, ups=1.73, wpb=8189.3, bsz=296.7, num_updates=18900, lr=0.000102869, gnorm=2.148, clip=0, loss_scale=8, train_wall=58, gb_free=17.8, wall=8856
2023-07-05 10:56:07 | INFO | train_inner | epoch 013:   1323 / 1474 loss=6.163, trans_loss=6.569, nll_loss=6.05, w2v_ctc_loss=2.397, contrastive_loss=0, total=4108.18, n_correct=258.18, ppl=66.25, accuracy=6.285, wps=14113.1, ups=1.73, wpb=8163.5, bsz=308.4, num_updates=19000, lr=0.000102598, gnorm=2.023, clip=0, loss_scale=8, train_wall=57, gb_free=16.8, wall=8914
2023-07-05 10:57:06 | INFO | train_inner | epoch 013:   1423 / 1474 loss=6.186, trans_loss=6.557, nll_loss=6.036, w2v_ctc_loss=2.457, contrastive_loss=0, total=4171.47, n_correct=264.76, ppl=65.61, accuracy=6.347, wps=14152, ups=1.71, wpb=8273.6, bsz=310.5, num_updates=19100, lr=0.000102329, gnorm=2.019, clip=0, loss_scale=8, train_wall=58, gb_free=15.5, wall=8973
2023-07-05 10:57:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 10:57:50 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 9.259 | trans_loss 12.67 | nll_loss 11.586 | w2v_ctc_loss 1.299 | contrastive_loss 0 | total 4003.4 | n_correct 298 | ppl 3074.05 | accuracy 7.444 | uer 40.639 | wer 43.052 | raw_wer 43.052 | wps 4788.6 | wpb 4003.4 | bsz 141.8 | num_updates 19151 | best_loss 8.095
2023-07-05 10:57:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 19151 updates
2023-07-05 10:57:50 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 10:57:54 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 10:57:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 13 @ 19151 updates, score 9.259) (writing took 4.073309743000209 seconds)
2023-07-05 10:57:54 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-07-05 10:57:54 | INFO | train | epoch 013 | loss 6.17 | trans_loss 6.542 | nll_loss 6.021 | w2v_ctc_loss 2.446 | contrastive_loss 0 | total 4138.9 | n_correct 261.749 | ppl 64.95 | accuracy 6.324 | wps 13472.9 | ups 1.64 | wpb 8217.7 | bsz 305.7 | num_updates 19151 | lr 0.000102193 | gnorm 2.009 | clip 0 | loss_scale 8 | train_wall 845 | gb_free 17.8 | wall 9021
2023-07-05 10:57:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 10:57:55 | INFO | fairseq.trainer | begin training epoch 14
2023-07-05 10:57:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 10:58:30 | INFO | train_inner | epoch 014:     49 / 1474 loss=6.159, trans_loss=6.567, nll_loss=6.047, w2v_ctc_loss=2.407, contrastive_loss=0, total=4182.69, n_correct=267.03, ppl=66.12, accuracy=6.384, wps=9868.1, ups=1.19, wpb=8310.2, bsz=322.3, num_updates=19200, lr=0.000102062, gnorm=2.012, clip=0, loss_scale=8, train_wall=57, gb_free=16.2, wall=9057
2023-07-05 10:59:27 | INFO | train_inner | epoch 014:    149 / 1474 loss=6.161, trans_loss=6.592, nll_loss=6.072, w2v_ctc_loss=2.384, contrastive_loss=0, total=4086.4, n_correct=261.38, ppl=67.26, accuracy=6.396, wps=14163.7, ups=1.75, wpb=8114.3, bsz=301.5, num_updates=19300, lr=0.000101797, gnorm=1.941, clip=0, loss_scale=8, train_wall=57, gb_free=17.2, wall=9114
2023-07-05 11:00:24 | INFO | train_inner | epoch 014:    249 / 1474 loss=6.214, trans_loss=6.64, nll_loss=6.118, w2v_ctc_loss=2.416, contrastive_loss=0, total=4103.37, n_correct=262.48, ppl=69.47, accuracy=6.397, wps=14191.7, ups=1.75, wpb=8131.4, bsz=294, num_updates=19400, lr=0.000101535, gnorm=2.076, clip=0, loss_scale=8, train_wall=57, gb_free=17.4, wall=9172
2023-07-05 11:01:22 | INFO | train_inner | epoch 014:    349 / 1474 loss=6.159, trans_loss=6.571, nll_loss=6.049, w2v_ctc_loss=2.399, contrastive_loss=0, total=4168.35, n_correct=269.94, ppl=66.19, accuracy=6.476, wps=14411.3, ups=1.74, wpb=8285.5, bsz=318.7, num_updates=19500, lr=0.000101274, gnorm=2.098, clip=0, loss_scale=8, train_wall=57, gb_free=16.5, wall=9229
2023-07-05 11:02:20 | INFO | train_inner | epoch 014:    449 / 1474 loss=6.205, trans_loss=6.581, nll_loss=6.061, w2v_ctc_loss=2.455, contrastive_loss=0, total=4155.83, n_correct=267.89, ppl=66.75, accuracy=6.446, wps=14317.5, ups=1.74, wpb=8244.3, bsz=306.7, num_updates=19600, lr=0.000101015, gnorm=1.981, clip=0, loss_scale=8, train_wall=57, gb_free=16.2, wall=9287
2023-07-05 11:03:18 | INFO | train_inner | epoch 014:    549 / 1474 loss=6.243, trans_loss=6.582, nll_loss=6.069, w2v_ctc_loss=2.512, contrastive_loss=0, total=4064.87, n_correct=256.44, ppl=67.12, accuracy=6.309, wps=13912.2, ups=1.72, wpb=8088.6, bsz=288.5, num_updates=19700, lr=0.000100759, gnorm=2.122, clip=0, loss_scale=8, train_wall=58, gb_free=18, wall=9345
2023-07-05 11:04:15 | INFO | train_inner | epoch 014:    649 / 1474 loss=6.082, trans_loss=6.561, nll_loss=6.043, w2v_ctc_loss=2.305, contrastive_loss=0, total=4167.34, n_correct=263.16, ppl=65.93, accuracy=6.315, wps=14312.7, ups=1.73, wpb=8271.3, bsz=307.8, num_updates=19800, lr=0.000100504, gnorm=1.969, clip=0, loss_scale=8, train_wall=57, gb_free=17.3, wall=9403
2023-07-05 11:05:13 | INFO | train_inner | epoch 014:    749 / 1474 loss=6.13, trans_loss=6.565, nll_loss=6.047, w2v_ctc_loss=2.359, contrastive_loss=0, total=4142.94, n_correct=262.13, ppl=66.12, accuracy=6.327, wps=14362.2, ups=1.74, wpb=8233.6, bsz=308.6, num_updates=19900, lr=0.000100251, gnorm=1.998, clip=0, loss_scale=8, train_wall=57, gb_free=16.6, wall=9460
2023-07-05 11:06:10 | INFO | train_inner | epoch 014:    849 / 1474 loss=6.138, trans_loss=6.576, nll_loss=6.058, w2v_ctc_loss=2.361, contrastive_loss=0, total=4173.06, n_correct=264.97, ppl=66.65, accuracy=6.35, wps=14373.8, ups=1.73, wpb=8293.5, bsz=319.1, num_updates=20000, lr=0.0001, gnorm=1.947, clip=0, loss_scale=8, train_wall=57, gb_free=13.2, wall=9518
2023-07-05 11:06:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 11:06:26 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 9.3 | trans_loss 12.722 | nll_loss 11.636 | w2v_ctc_loss 1.315 | contrastive_loss 0 | total 4003.4 | n_correct 304.9 | ppl 3182.67 | accuracy 7.616 | uer 38.402 | wer 41.102 | raw_wer 41.102 | wps 4799.3 | wpb 4003.4 | bsz 141.8 | num_updates 20000 | best_loss 8.095
2023-07-05 11:06:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20000 updates
2023-07-05 11:06:26 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_14_20000.pt
2023-07-05 11:06:28 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_14_20000.pt
2023-07-05 11:06:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_14_20000.pt (epoch 14 @ 20000 updates, score 9.3) (writing took 4.926898478999647 seconds)
2023-07-05 11:07:29 | INFO | train_inner | epoch 014:    949 / 1474 loss=6.18, trans_loss=6.595, nll_loss=6.079, w2v_ctc_loss=2.394, contrastive_loss=0, total=4166.71, n_correct=267.02, ppl=67.62, accuracy=6.408, wps=10525.4, ups=1.27, wpb=8282.8, bsz=310.6, num_updates=20100, lr=9.97509e-05, gnorm=2.113, clip=0, loss_scale=8, train_wall=58, gb_free=16.9, wall=9596
2023-07-05 11:08:27 | INFO | train_inner | epoch 014:   1049 / 1474 loss=6.183, trans_loss=6.621, nll_loss=6.102, w2v_ctc_loss=2.383, contrastive_loss=0, total=4145.57, n_correct=260.15, ppl=68.68, accuracy=6.275, wps=14102.2, ups=1.72, wpb=8217.9, bsz=301.1, num_updates=20200, lr=9.95037e-05, gnorm=1.942, clip=0, loss_scale=8, train_wall=58, gb_free=17.6, wall=9655
2023-07-05 11:09:26 | INFO | train_inner | epoch 014:   1149 / 1474 loss=6.225, trans_loss=6.594, nll_loss=6.075, w2v_ctc_loss=2.478, contrastive_loss=0, total=4219.9, n_correct=269.75, ppl=67.44, accuracy=6.392, wps=14359.5, ups=1.72, wpb=8369.1, bsz=325.2, num_updates=20300, lr=9.92583e-05, gnorm=1.989, clip=0, loss_scale=8, train_wall=58, gb_free=16.6, wall=9713
2023-07-05 11:10:23 | INFO | train_inner | epoch 014:   1249 / 1474 loss=6.246, trans_loss=6.636, nll_loss=6.123, w2v_ctc_loss=2.456, contrastive_loss=0, total=4032.06, n_correct=250.79, ppl=69.67, accuracy=6.22, wps=13963.6, ups=1.75, wpb=8000, bsz=274.4, num_updates=20400, lr=9.90148e-05, gnorm=2.099, clip=0, loss_scale=8, train_wall=57, gb_free=17.8, wall=9770
2023-07-05 11:11:21 | INFO | train_inner | epoch 014:   1349 / 1474 loss=6.163, trans_loss=6.606, nll_loss=6.089, w2v_ctc_loss=2.363, contrastive_loss=0, total=4205.07, n_correct=263.79, ppl=68.09, accuracy=6.273, wps=14472.3, ups=1.73, wpb=8342.7, bsz=317.3, num_updates=20500, lr=9.8773e-05, gnorm=1.931, clip=0, loss_scale=8, train_wall=57, gb_free=16.9, wall=9828
2023-07-05 11:12:18 | INFO | train_inner | epoch 014:   1449 / 1474 loss=6.198, trans_loss=6.617, nll_loss=6.102, w2v_ctc_loss=2.411, contrastive_loss=0, total=4126.44, n_correct=259.29, ppl=68.71, accuracy=6.284, wps=14284, ups=1.74, wpb=8191.5, bsz=303.9, num_updates=20600, lr=9.85329e-05, gnorm=2.032, clip=0, loss_scale=8, train_wall=57, gb_free=16.4, wall=9885
2023-07-05 11:12:33 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 11:12:48 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 9.309 | trans_loss 12.759 | nll_loss 11.683 | w2v_ctc_loss 1.258 | contrastive_loss 0 | total 4003.4 | n_correct 296.5 | ppl 3287.61 | accuracy 7.406 | uer 40.196 | wer 42.258 | raw_wer 42.258 | wps 4732.3 | wpb 4003.4 | bsz 141.8 | num_updates 20625 | best_loss 8.095
2023-07-05 11:12:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20625 updates
2023-07-05 11:12:48 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 11:12:52 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 11:12:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 14 @ 20625 updates, score 9.309) (writing took 4.186727602998872 seconds)
2023-07-05 11:12:52 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-07-05 11:12:52 | INFO | train | epoch 014 | loss 6.18 | trans_loss 6.594 | nll_loss 6.077 | w2v_ctc_loss 2.406 | contrastive_loss 0 | total 4138.65 | n_correct 262.834 | ppl 67.49 | accuracy 6.351 | wps 13488.5 | ups 1.64 | wpb 8217.2 | bsz 305.7 | num_updates 20625 | lr 9.84732e-05 | gnorm 2.018 | clip 0 | loss_scale 8 | train_wall 844 | gb_free 16.7 | wall 9919
2023-07-05 11:12:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 11:12:53 | INFO | fairseq.trainer | begin training epoch 15
2023-07-05 11:12:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 11:13:42 | INFO | train_inner | epoch 015:     75 / 1474 loss=6.169, trans_loss=6.597, nll_loss=6.083, w2v_ctc_loss=2.381, contrastive_loss=0, total=4090.99, n_correct=258.92, ppl=67.78, accuracy=6.329, wps=9698.9, ups=1.19, wpb=8128.2, bsz=300.8, num_updates=20700, lr=9.82946e-05, gnorm=2.011, clip=0, loss_scale=8, train_wall=56, gb_free=17, wall=9969
2023-07-05 11:14:39 | INFO | train_inner | epoch 015:    175 / 1474 loss=6.209, trans_loss=6.617, nll_loss=6.104, w2v_ctc_loss=2.42, contrastive_loss=0, total=4115.56, n_correct=257.6, ppl=68.78, accuracy=6.259, wps=14222.9, ups=1.74, wpb=8171.9, bsz=298.5, num_updates=20800, lr=9.80581e-05, gnorm=2.088, clip=0, loss_scale=16, train_wall=57, gb_free=17.1, wall=10026
2023-07-05 11:15:37 | INFO | train_inner | epoch 015:    275 / 1474 loss=6.142, trans_loss=6.599, nll_loss=6.085, w2v_ctc_loss=2.34, contrastive_loss=0, total=4182.19, n_correct=260.7, ppl=67.9, accuracy=6.234, wps=14441.2, ups=1.74, wpb=8312.6, bsz=310.5, num_updates=20900, lr=9.78232e-05, gnorm=1.978, clip=0, loss_scale=16, train_wall=57, gb_free=16.7, wall=10084
2023-07-05 11:16:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-07-05 11:16:35 | INFO | train_inner | epoch 015:    376 / 1474 loss=6.226, trans_loss=6.615, nll_loss=6.099, w2v_ctc_loss=2.458, contrastive_loss=0, total=4170.22, n_correct=262.54, ppl=68.54, accuracy=6.296, wps=14139.6, ups=1.71, wpb=8267.2, bsz=308, num_updates=21000, lr=9.759e-05, gnorm=2.092, clip=0, loss_scale=8, train_wall=58, gb_free=16.6, wall=10142
2023-07-05 11:17:33 | INFO | train_inner | epoch 015:    476 / 1474 loss=6.153, trans_loss=6.632, nll_loss=6.119, w2v_ctc_loss=2.337, contrastive_loss=0, total=4076.17, n_correct=253.63, ppl=69.52, accuracy=6.222, wps=14000.4, ups=1.73, wpb=8099.5, bsz=293.1, num_updates=21100, lr=9.73585e-05, gnorm=2.016, clip=0, loss_scale=8, train_wall=57, gb_free=16.9, wall=10200
2023-07-05 11:18:31 | INFO | train_inner | epoch 015:    576 / 1474 loss=6.199, trans_loss=6.658, nll_loss=6.145, w2v_ctc_loss=2.368, contrastive_loss=0, total=4151.89, n_correct=258.23, ppl=70.75, accuracy=6.22, wps=14311.2, ups=1.74, wpb=8243.1, bsz=302, num_updates=21200, lr=9.71286e-05, gnorm=1.904, clip=0, loss_scale=8, train_wall=57, gb_free=13.9, wall=10258
2023-07-05 11:19:28 | INFO | train_inner | epoch 015:    676 / 1474 loss=6.17, trans_loss=6.642, nll_loss=6.129, w2v_ctc_loss=2.341, contrastive_loss=0, total=4122.17, n_correct=262.08, ppl=69.97, accuracy=6.358, wps=14230.2, ups=1.74, wpb=8184, bsz=304, num_updates=21300, lr=9.69003e-05, gnorm=1.918, clip=0, loss_scale=8, train_wall=57, gb_free=17.4, wall=10315
2023-07-05 11:20:26 | INFO | train_inner | epoch 015:    776 / 1474 loss=6.177, trans_loss=6.631, nll_loss=6.12, w2v_ctc_loss=2.358, contrastive_loss=0, total=4181.07, n_correct=260.51, ppl=69.56, accuracy=6.231, wps=14293, ups=1.73, wpb=8283.4, bsz=307.1, num_updates=21400, lr=9.66736e-05, gnorm=2.011, clip=0, loss_scale=8, train_wall=58, gb_free=17, wall=10373
2023-07-05 11:21:24 | INFO | train_inner | epoch 015:    876 / 1474 loss=6.193, trans_loss=6.611, nll_loss=6.104, w2v_ctc_loss=2.415, contrastive_loss=0, total=4052.17, n_correct=250.46, ppl=68.8, accuracy=6.181, wps=14042.3, ups=1.74, wpb=8059.3, bsz=286.4, num_updates=21500, lr=9.64486e-05, gnorm=2.166, clip=0, loss_scale=8, train_wall=57, gb_free=17.3, wall=10431
2023-07-05 11:22:21 | INFO | train_inner | epoch 015:    976 / 1474 loss=6.043, trans_loss=6.582, nll_loss=6.072, w2v_ctc_loss=2.209, contrastive_loss=0, total=4135.95, n_correct=253.07, ppl=67.28, accuracy=6.119, wps=14362.4, ups=1.75, wpb=8205.4, bsz=304.3, num_updates=21600, lr=9.6225e-05, gnorm=1.859, clip=0, loss_scale=8, train_wall=57, gb_free=16.8, wall=10488
2023-07-05 11:23:19 | INFO | train_inner | epoch 015:   1076 / 1474 loss=6.182, trans_loss=6.606, nll_loss=6.095, w2v_ctc_loss=2.393, contrastive_loss=0, total=4187.18, n_correct=263.52, ppl=68.36, accuracy=6.293, wps=14288.5, ups=1.72, wpb=8309.8, bsz=324.7, num_updates=21700, lr=9.60031e-05, gnorm=1.985, clip=0, loss_scale=8, train_wall=58, gb_free=14.2, wall=10546
2023-07-05 11:24:16 | INFO | train_inner | epoch 015:   1176 / 1474 loss=6.067, trans_loss=6.605, nll_loss=6.093, w2v_ctc_loss=2.226, contrastive_loss=0, total=4184.18, n_correct=264.76, ppl=68.26, accuracy=6.328, wps=14512.5, ups=1.75, wpb=8312, bsz=328.4, num_updates=21800, lr=9.57826e-05, gnorm=1.914, clip=0, loss_scale=8, train_wall=57, gb_free=16, wall=10603
2023-07-05 11:25:14 | INFO | train_inner | epoch 015:   1276 / 1474 loss=6.211, trans_loss=6.648, nll_loss=6.136, w2v_ctc_loss=2.398, contrastive_loss=0, total=4141.39, n_correct=258.76, ppl=70.31, accuracy=6.248, wps=14263.3, ups=1.74, wpb=8220, bsz=302.1, num_updates=21900, lr=9.55637e-05, gnorm=2.09, clip=0, loss_scale=8, train_wall=57, gb_free=16.8, wall=10661
2023-07-05 11:26:12 | INFO | train_inner | epoch 015:   1376 / 1474 loss=6.177, trans_loss=6.646, nll_loss=6.137, w2v_ctc_loss=2.348, contrastive_loss=0, total=4106.11, n_correct=254.88, ppl=70.37, accuracy=6.207, wps=14099.8, ups=1.73, wpb=8148.9, bsz=294.2, num_updates=22000, lr=9.53463e-05, gnorm=2.064, clip=0, loss_scale=8, train_wall=57, gb_free=17.1, wall=10719
2023-07-05 11:26:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 11:26:27 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 9.588 | trans_loss 12.818 | nll_loss 11.744 | w2v_ctc_loss 2.053 | contrastive_loss 0 | total 4003.4 | n_correct 298.1 | ppl 3430.67 | accuracy 7.446 | uer 37.127 | wer 38.899 | raw_wer 38.899 | wps 4866 | wpb 4003.4 | bsz 141.8 | num_updates 22000 | best_loss 8.095
2023-07-05 11:26:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22000 updates
2023-07-05 11:26:27 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_15_22000.pt
2023-07-05 11:26:29 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_15_22000.pt
2023-07-05 11:26:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_15_22000.pt (epoch 15 @ 22000 updates, score 9.588) (writing took 5.025330471000416 seconds)
2023-07-05 11:27:30 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 11:27:45 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 9.642 | trans_loss 12.818 | nll_loss 11.749 | w2v_ctc_loss 2.234 | contrastive_loss 0 | total 4003.4 | n_correct 292.7 | ppl 3443.11 | accuracy 7.311 | uer 36.488 | wer 38.582 | raw_wer 38.582 | wps 4789.7 | wpb 4003.4 | bsz 141.8 | num_updates 22098 | best_loss 8.095
2023-07-05 11:27:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22098 updates
2023-07-05 11:27:45 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 11:27:49 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 11:27:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 15 @ 22098 updates, score 9.642) (writing took 4.167082175999894 seconds)
2023-07-05 11:27:49 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-07-05 11:27:49 | INFO | train | epoch 015 | loss 6.16 | trans_loss 6.619 | nll_loss 6.108 | w2v_ctc_loss 2.35 | contrastive_loss 0 | total 4138.64 | n_correct 258.318 | ppl 68.95 | accuracy 6.242 | wps 13495 | ups 1.64 | wpb 8217.2 | bsz 305.7 | num_updates 22098 | lr 9.51346e-05 | gnorm 2.006 | clip 0 | loss_scale 8 | train_wall 843 | gb_free 17.2 | wall 10816
2023-07-05 11:27:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 11:27:49 | INFO | fairseq.trainer | begin training epoch 16
2023-07-05 11:27:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 11:27:58 | INFO | train_inner | epoch 016:      2 / 1474 loss=6.106, trans_loss=6.59, nll_loss=6.085, w2v_ctc_loss=2.298, contrastive_loss=0, total=4152.6, n_correct=254.15, ppl=67.86, accuracy=6.12, wps=7772.6, ups=0.94, wpb=8258.1, bsz=316.4, num_updates=22100, lr=9.51303e-05, gnorm=2.014, clip=0, loss_scale=8, train_wall=58, gb_free=16.8, wall=10825
2023-07-05 11:28:55 | INFO | train_inner | epoch 016:    102 / 1474 loss=6.123, trans_loss=6.597, nll_loss=6.09, w2v_ctc_loss=2.317, contrastive_loss=0, total=4115.14, n_correct=255.83, ppl=68.12, accuracy=6.217, wps=14350.2, ups=1.76, wpb=8175.7, bsz=313.9, num_updates=22200, lr=9.49158e-05, gnorm=2.001, clip=0, loss_scale=8, train_wall=56, gb_free=12.9, wall=10882
2023-07-05 11:29:53 | INFO | train_inner | epoch 016:    202 / 1474 loss=6.133, trans_loss=6.597, nll_loss=6.092, w2v_ctc_loss=2.33, contrastive_loss=0, total=4109.58, n_correct=254.05, ppl=68.2, accuracy=6.182, wps=14141.2, ups=1.73, wpb=8169.7, bsz=297.3, num_updates=22300, lr=9.47027e-05, gnorm=2.095, clip=0, loss_scale=8, train_wall=57, gb_free=15.3, wall=10940
2023-07-05 11:30:50 | INFO | train_inner | epoch 016:    302 / 1474 loss=6.172, trans_loss=6.646, nll_loss=6.137, w2v_ctc_loss=2.337, contrastive_loss=0, total=4164.1, n_correct=256.73, ppl=70.39, accuracy=6.165, wps=14335.6, ups=1.73, wpb=8262.9, bsz=308.6, num_updates=22400, lr=9.44911e-05, gnorm=2.056, clip=0, loss_scale=8, train_wall=57, gb_free=17.4, wall=10997
2023-07-05 11:31:48 | INFO | train_inner | epoch 016:    402 / 1474 loss=6.121, trans_loss=6.644, nll_loss=6.139, w2v_ctc_loss=2.259, contrastive_loss=0, total=4065.22, n_correct=245.43, ppl=70.47, accuracy=6.037, wps=14095.9, ups=1.75, wpb=8065.8, bsz=286.4, num_updates=22500, lr=9.42809e-05, gnorm=2.107, clip=0, loss_scale=8, train_wall=57, gb_free=16.5, wall=11055
2023-07-05 11:32:46 | INFO | train_inner | epoch 016:    502 / 1474 loss=6.113, trans_loss=6.635, nll_loss=6.125, w2v_ctc_loss=2.254, contrastive_loss=0, total=4181.93, n_correct=261.03, ppl=69.77, accuracy=6.242, wps=14266.7, ups=1.72, wpb=8297.7, bsz=320.3, num_updates=22600, lr=9.40721e-05, gnorm=1.974, clip=0, loss_scale=8, train_wall=58, gb_free=16, wall=11113
2023-07-05 11:33:43 | INFO | train_inner | epoch 016:    602 / 1474 loss=6.084, trans_loss=6.661, nll_loss=6.155, w2v_ctc_loss=2.179, contrastive_loss=0, total=4122.97, n_correct=249.77, ppl=71.27, accuracy=6.058, wps=14338.7, ups=1.75, wpb=8175.2, bsz=299, num_updates=22700, lr=9.38647e-05, gnorm=2.122, clip=0, loss_scale=8, train_wall=57, gb_free=16.4, wall=11170
2023-07-05 11:34:40 | INFO | train_inner | epoch 016:    702 / 1474 loss=6.153, trans_loss=6.656, nll_loss=6.15, w2v_ctc_loss=2.295, contrastive_loss=0, total=4093.15, n_correct=250.98, ppl=71, accuracy=6.132, wps=14144.6, ups=1.74, wpb=8126.6, bsz=296.5, num_updates=22800, lr=9.36586e-05, gnorm=1.982, clip=0, loss_scale=8, train_wall=57, gb_free=18, wall=11227
2023-07-05 11:35:38 | INFO | train_inner | epoch 016:    802 / 1474 loss=6.133, trans_loss=6.629, nll_loss=6.12, w2v_ctc_loss=2.297, contrastive_loss=0, total=4183.24, n_correct=257.9, ppl=69.57, accuracy=6.165, wps=14367.2, ups=1.73, wpb=8310.7, bsz=312.1, num_updates=22900, lr=9.34539e-05, gnorm=1.938, clip=0, loss_scale=8, train_wall=57, gb_free=17.9, wall=11285
2023-07-05 11:36:35 | INFO | train_inner | epoch 016:    902 / 1474 loss=6.173, trans_loss=6.675, nll_loss=6.169, w2v_ctc_loss=2.301, contrastive_loss=0, total=4150.23, n_correct=255.44, ppl=71.94, accuracy=6.155, wps=14356.6, ups=1.74, wpb=8242.5, bsz=306.5, num_updates=23000, lr=9.32505e-05, gnorm=2.03, clip=0, loss_scale=8, train_wall=57, gb_free=12.5, wall=11343
2023-07-05 11:37:33 | INFO | train_inner | epoch 016:   1002 / 1474 loss=6.206, trans_loss=6.655, nll_loss=6.15, w2v_ctc_loss=2.378, contrastive_loss=0, total=4116.59, n_correct=252.85, ppl=70.99, accuracy=6.142, wps=14126, ups=1.73, wpb=8180.4, bsz=300.6, num_updates=23100, lr=9.30484e-05, gnorm=2.016, clip=0, loss_scale=16, train_wall=57, gb_free=17, wall=11400
2023-07-05 11:38:31 | INFO | train_inner | epoch 016:   1102 / 1474 loss=6.206, trans_loss=6.691, nll_loss=6.187, w2v_ctc_loss=2.347, contrastive_loss=0, total=4112.71, n_correct=249.42, ppl=72.86, accuracy=6.065, wps=14067.7, ups=1.72, wpb=8171.4, bsz=295.7, num_updates=23200, lr=9.28477e-05, gnorm=2.06, clip=0, loss_scale=16, train_wall=58, gb_free=12.6, wall=11459
2023-07-05 11:39:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-07-05 11:39:30 | INFO | train_inner | epoch 016:   1203 / 1474 loss=6.094, trans_loss=6.657, nll_loss=6.153, w2v_ctc_loss=2.215, contrastive_loss=0, total=4165.09, n_correct=249.18, ppl=71.15, accuracy=5.983, wps=14046.2, ups=1.7, wpb=8267.6, bsz=308.5, num_updates=23300, lr=9.26482e-05, gnorm=2.011, clip=0, loss_scale=8, train_wall=58, gb_free=16.7, wall=11517
2023-07-05 11:40:28 | INFO | train_inner | epoch 016:   1303 / 1474 loss=6.211, trans_loss=6.676, nll_loss=6.172, w2v_ctc_loss=2.367, contrastive_loss=0, total=4150.54, n_correct=251.26, ppl=72.11, accuracy=6.054, wps=14254.3, ups=1.73, wpb=8245.2, bsz=312.3, num_updates=23400, lr=9.245e-05, gnorm=2.058, clip=0, loss_scale=8, train_wall=57, gb_free=13.2, wall=11575
2023-07-05 11:41:26 | INFO | train_inner | epoch 016:   1403 / 1474 loss=6.11, trans_loss=6.669, nll_loss=6.164, w2v_ctc_loss=2.217, contrastive_loss=0, total=4198.78, n_correct=256.02, ppl=71.71, accuracy=6.097, wps=14322, ups=1.72, wpb=8332.1, bsz=322.4, num_updates=23500, lr=9.22531e-05, gnorm=1.995, clip=0, loss_scale=8, train_wall=58, gb_free=17.9, wall=11633
2023-07-05 11:42:07 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 11:42:23 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 9.47 | trans_loss 13.013 | nll_loss 11.954 | w2v_ctc_loss 1.203 | contrastive_loss 0 | total 4003.4 | n_correct 285.3 | ppl 3968.05 | accuracy 7.126 | uer 40.477 | wer 43.22 | raw_wer 43.22 | wps 4907.4 | wpb 4003.4 | bsz 141.8 | num_updates 23571 | best_loss 8.095
2023-07-05 11:42:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 23571 updates
2023-07-05 11:42:23 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 11:42:27 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 11:42:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 16 @ 23571 updates, score 9.47) (writing took 3.9818356779996975 seconds)
2023-07-05 11:42:27 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-07-05 11:42:27 | INFO | train | epoch 016 | loss 6.148 | trans_loss 6.651 | nll_loss 6.145 | w2v_ctc_loss 2.294 | contrastive_loss 0 | total 4139.07 | n_correct 253.436 | ppl 70.76 | accuracy 6.123 | wps 13797.8 | ups 1.68 | wpb 8218 | bsz 305.7 | num_updates 23571 | lr 9.21141e-05 | gnorm 2.032 | clip 0 | loss_scale 8 | train_wall 844 | gb_free 15.7 | wall 11694
2023-07-05 11:42:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 11:42:27 | INFO | fairseq.trainer | begin training epoch 17
2023-07-05 11:42:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 11:42:51 | INFO | train_inner | epoch 017:     29 / 1474 loss=6.184, trans_loss=6.695, nll_loss=6.188, w2v_ctc_loss=2.302, contrastive_loss=0, total=4138.06, n_correct=256.3, ppl=72.9, accuracy=6.194, wps=9663.8, ups=1.18, wpb=8206.3, bsz=300.4, num_updates=23600, lr=9.20575e-05, gnorm=2.034, clip=0, loss_scale=8, train_wall=58, gb_free=17.8, wall=11718
2023-07-05 11:43:49 | INFO | train_inner | epoch 017:    129 / 1474 loss=6.154, trans_loss=6.68, nll_loss=6.176, w2v_ctc_loss=2.274, contrastive_loss=0, total=4110.37, n_correct=251.09, ppl=72.33, accuracy=6.109, wps=14196.9, ups=1.74, wpb=8153.6, bsz=295.6, num_updates=23700, lr=9.1863e-05, gnorm=2.018, clip=0, loss_scale=8, train_wall=57, gb_free=16.7, wall=11776
2023-07-05 11:44:46 | INFO | train_inner | epoch 017:    229 / 1474 loss=6.07, trans_loss=6.676, nll_loss=6.171, w2v_ctc_loss=2.153, contrastive_loss=0, total=4181.59, n_correct=256.13, ppl=72.07, accuracy=6.125, wps=14467.1, ups=1.74, wpb=8307.2, bsz=322.2, num_updates=23800, lr=9.16698e-05, gnorm=2.011, clip=0, loss_scale=8, train_wall=57, gb_free=16.1, wall=11833
2023-07-05 11:45:44 | INFO | train_inner | epoch 017:    329 / 1474 loss=6.146, trans_loss=6.676, nll_loss=6.17, w2v_ctc_loss=2.263, contrastive_loss=0, total=4157.97, n_correct=252.77, ppl=71.99, accuracy=6.079, wps=14373, ups=1.74, wpb=8247.1, bsz=304, num_updates=23900, lr=9.14779e-05, gnorm=2, clip=0, loss_scale=8, train_wall=57, gb_free=16.2, wall=11891
2023-07-05 11:46:41 | INFO | train_inner | epoch 017:    429 / 1474 loss=6.157, trans_loss=6.688, nll_loss=6.186, w2v_ctc_loss=2.27, contrastive_loss=0, total=4135.12, n_correct=245.42, ppl=72.8, accuracy=5.935, wps=14189.1, ups=1.73, wpb=8222, bsz=306.1, num_updates=24000, lr=9.12871e-05, gnorm=2.042, clip=0, loss_scale=8, train_wall=58, gb_free=13.1, wall=11949
2023-07-05 11:46:41 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 11:46:57 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 9.651 | trans_loss 12.996 | nll_loss 11.935 | w2v_ctc_loss 1.846 | contrastive_loss 0 | total 4003.4 | n_correct 281.9 | ppl 3914.47 | accuracy 7.042 | uer 38.229 | wer 40.427 | raw_wer 40.427 | wps 4871.5 | wpb 4003.4 | bsz 141.8 | num_updates 24000 | best_loss 8.095
2023-07-05 11:46:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 24000 updates
2023-07-05 11:46:57 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_17_24000.pt
2023-07-05 11:46:59 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_17_24000.pt
2023-07-05 11:47:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_17_24000.pt (epoch 17 @ 24000 updates, score 9.651) (writing took 4.95350029400106 seconds)
2023-07-05 11:48:01 | INFO | train_inner | epoch 017:    529 / 1474 loss=6.259, trans_loss=6.718, nll_loss=6.211, w2v_ctc_loss=2.4, contrastive_loss=0, total=4185.81, n_correct=254.93, ppl=74.1, accuracy=6.09, wps=10478.9, ups=1.26, wpb=8294.9, bsz=308.6, num_updates=24100, lr=9.10975e-05, gnorm=1.984, clip=0, loss_scale=8, train_wall=58, gb_free=16.4, wall=12028
2023-07-05 11:48:58 | INFO | train_inner | epoch 017:    629 / 1474 loss=6.12, trans_loss=6.677, nll_loss=6.173, w2v_ctc_loss=2.226, contrastive_loss=0, total=4168.62, n_correct=253.76, ppl=72.15, accuracy=6.087, wps=14331.5, ups=1.73, wpb=8271.9, bsz=303.2, num_updates=24200, lr=9.09091e-05, gnorm=1.959, clip=0, loss_scale=8, train_wall=57, gb_free=14.6, wall=12085
2023-07-05 11:49:56 | INFO | train_inner | epoch 017:    729 / 1474 loss=6.173, trans_loss=6.687, nll_loss=6.183, w2v_ctc_loss=2.302, contrastive_loss=0, total=4167.34, n_correct=254.08, ppl=72.65, accuracy=6.097, wps=14348.2, ups=1.73, wpb=8274, bsz=307.7, num_updates=24300, lr=9.07218e-05, gnorm=2.039, clip=0, loss_scale=8, train_wall=57, gb_free=10.6, wall=12143
2023-07-05 11:50:53 | INFO | train_inner | epoch 017:    829 / 1474 loss=6.143, trans_loss=6.701, nll_loss=6.2, w2v_ctc_loss=2.232, contrastive_loss=0, total=4092.64, n_correct=245.09, ppl=73.51, accuracy=5.989, wps=14230.2, ups=1.75, wpb=8121.8, bsz=296.2, num_updates=24400, lr=9.05357e-05, gnorm=2.052, clip=0, loss_scale=8, train_wall=57, gb_free=16, wall=12200
2023-07-05 11:51:50 | INFO | train_inner | epoch 017:    929 / 1474 loss=6.139, trans_loss=6.694, nll_loss=6.188, w2v_ctc_loss=2.235, contrastive_loss=0, total=4109.5, n_correct=250.51, ppl=72.92, accuracy=6.096, wps=14423.6, ups=1.77, wpb=8162.8, bsz=305.4, num_updates=24500, lr=9.03508e-05, gnorm=1.968, clip=0, loss_scale=8, train_wall=56, gb_free=16.8, wall=12257
2023-07-05 11:52:47 | INFO | train_inner | epoch 017:   1029 / 1474 loss=6.177, trans_loss=6.664, nll_loss=6.164, w2v_ctc_loss=2.335, contrastive_loss=0, total=4098.36, n_correct=247.82, ppl=71.68, accuracy=6.047, wps=14174, ups=1.74, wpb=8147, bsz=301.7, num_updates=24600, lr=9.0167e-05, gnorm=2.138, clip=0, loss_scale=8, train_wall=57, gb_free=16.1, wall=12314
2023-07-05 11:53:45 | INFO | train_inner | epoch 017:   1129 / 1474 loss=6.139, trans_loss=6.678, nll_loss=6.179, w2v_ctc_loss=2.256, contrastive_loss=0, total=4100.14, n_correct=246.34, ppl=72.45, accuracy=6.008, wps=14150.9, ups=1.74, wpb=8135.5, bsz=299.3, num_updates=24700, lr=8.99843e-05, gnorm=2.035, clip=0, loss_scale=8, train_wall=57, gb_free=17, wall=12372
2023-07-05 11:54:43 | INFO | train_inner | epoch 017:   1229 / 1474 loss=6.16, trans_loss=6.685, nll_loss=6.183, w2v_ctc_loss=2.273, contrastive_loss=0, total=4173.98, n_correct=256.2, ppl=72.68, accuracy=6.138, wps=14202.9, ups=1.71, wpb=8292.6, bsz=325.9, num_updates=24800, lr=8.98027e-05, gnorm=2.059, clip=0, loss_scale=8, train_wall=58, gb_free=16.2, wall=12430
2023-07-05 11:55:41 | INFO | train_inner | epoch 017:   1329 / 1474 loss=6.077, trans_loss=6.674, nll_loss=6.175, w2v_ctc_loss=2.166, contrastive_loss=0, total=4146.07, n_correct=246.06, ppl=72.23, accuracy=5.935, wps=14307.3, ups=1.74, wpb=8230.5, bsz=303.2, num_updates=24900, lr=8.96221e-05, gnorm=2.114, clip=0, loss_scale=8, train_wall=57, gb_free=17, wall=12488
2023-07-05 11:56:39 | INFO | train_inner | epoch 017:   1429 / 1474 loss=6.107, trans_loss=6.683, nll_loss=6.186, w2v_ctc_loss=2.194, contrastive_loss=0, total=4119.23, n_correct=246.27, ppl=72.79, accuracy=5.979, wps=14060.3, ups=1.72, wpb=8192.8, bsz=303.4, num_updates=25000, lr=8.94427e-05, gnorm=1.983, clip=0, loss_scale=8, train_wall=58, gb_free=13.5, wall=12546
2023-07-05 11:57:05 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 11:57:20 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 9.501 | trans_loss 12.926 | nll_loss 11.873 | w2v_ctc_loss 1.51 | contrastive_loss 0 | total 4003.4 | n_correct 288.6 | ppl 3750.79 | accuracy 7.209 | uer 38.165 | wer 40.696 | raw_wer 40.696 | wps 4796.9 | wpb 4003.4 | bsz 141.8 | num_updates 25045 | best_loss 8.095
2023-07-05 11:57:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 25045 updates
2023-07-05 11:57:20 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 11:57:24 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 11:57:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 17 @ 25045 updates, score 9.501) (writing took 4.15283837700008 seconds)
2023-07-05 11:57:25 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-07-05 11:57:25 | INFO | train | epoch 017 | loss 6.145 | trans_loss 6.685 | nll_loss 6.182 | w2v_ctc_loss 2.256 | contrastive_loss 0 | total 4138.65 | n_correct 250.542 | ppl 72.63 | accuracy 6.054 | wps 13488.1 | ups 1.64 | wpb 8217.2 | bsz 305.7 | num_updates 25045 | lr 8.93623e-05 | gnorm 2.028 | clip 0 | loss_scale 8 | train_wall 844 | gb_free 16.6 | wall 12592
2023-07-05 11:57:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 11:57:25 | INFO | fairseq.trainer | begin training epoch 18
2023-07-05 11:57:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 11:58:04 | INFO | train_inner | epoch 018:     55 / 1474 loss=6.179, trans_loss=6.694, nll_loss=6.196, w2v_ctc_loss=2.289, contrastive_loss=0, total=4128.93, n_correct=250.14, ppl=73.29, accuracy=6.058, wps=9593.4, ups=1.17, wpb=8196.7, bsz=301.7, num_updates=25100, lr=8.92644e-05, gnorm=1.97, clip=0, loss_scale=8, train_wall=58, gb_free=16.8, wall=12631
2023-07-05 11:59:02 | INFO | train_inner | epoch 018:    155 / 1474 loss=6.051, trans_loss=6.67, nll_loss=6.17, w2v_ctc_loss=2.123, contrastive_loss=0, total=4158.38, n_correct=250.24, ppl=71.98, accuracy=6.018, wps=14337.3, ups=1.74, wpb=8250.4, bsz=313.7, num_updates=25200, lr=8.90871e-05, gnorm=1.948, clip=0, loss_scale=8, train_wall=57, gb_free=16.9, wall=12689
2023-07-05 12:00:00 | INFO | train_inner | epoch 018:    255 / 1474 loss=6.123, trans_loss=6.681, nll_loss=6.18, w2v_ctc_loss=2.218, contrastive_loss=0, total=4161.92, n_correct=255.88, ppl=72.49, accuracy=6.148, wps=14267.1, ups=1.73, wpb=8265.9, bsz=312.8, num_updates=25300, lr=8.89108e-05, gnorm=2.083, clip=0, loss_scale=8, train_wall=58, gb_free=17.1, wall=12747
2023-07-05 12:00:58 | INFO | train_inner | epoch 018:    355 / 1474 loss=6.154, trans_loss=6.708, nll_loss=6.208, w2v_ctc_loss=2.239, contrastive_loss=0, total=4167.42, n_correct=246.48, ppl=73.94, accuracy=5.914, wps=14272.8, ups=1.73, wpb=8264.7, bsz=301.2, num_updates=25400, lr=8.87357e-05, gnorm=2.09, clip=0, loss_scale=16, train_wall=58, gb_free=15.8, wall=12805
2023-07-05 12:01:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-07-05 12:01:57 | INFO | train_inner | epoch 018:    456 / 1474 loss=6.228, trans_loss=6.694, nll_loss=6.195, w2v_ctc_loss=2.373, contrastive_loss=0, total=4071.92, n_correct=242.42, ppl=73.28, accuracy=5.953, wps=13713.3, ups=1.69, wpb=8092.6, bsz=292.9, num_updates=25500, lr=8.85615e-05, gnorm=2.187, clip=0, loss_scale=8, train_wall=59, gb_free=15.8, wall=12864
2023-07-05 12:02:54 | INFO | train_inner | epoch 018:    556 / 1474 loss=6.12, trans_loss=6.676, nll_loss=6.177, w2v_ctc_loss=2.228, contrastive_loss=0, total=4224.88, n_correct=257.67, ppl=72.34, accuracy=6.099, wps=14651.3, ups=1.74, wpb=8399.1, bsz=330.4, num_updates=25600, lr=8.83883e-05, gnorm=1.927, clip=0, loss_scale=8, train_wall=57, gb_free=17.1, wall=12921
2023-07-05 12:03:51 | INFO | train_inner | epoch 018:    656 / 1474 loss=6.194, trans_loss=6.704, nll_loss=6.206, w2v_ctc_loss=2.301, contrastive_loss=0, total=4087.72, n_correct=245.82, ppl=73.82, accuracy=6.014, wps=14180.7, ups=1.75, wpb=8112.1, bsz=298.1, num_updates=25700, lr=8.82162e-05, gnorm=2.16, clip=0, loss_scale=8, train_wall=57, gb_free=16.9, wall=12978
2023-07-05 12:04:49 | INFO | train_inner | epoch 018:    756 / 1474 loss=6.202, trans_loss=6.733, nll_loss=6.233, w2v_ctc_loss=2.29, contrastive_loss=0, total=4202.56, n_correct=255.36, ppl=75.2, accuracy=6.076, wps=14472.4, ups=1.73, wpb=8345.4, bsz=322.9, num_updates=25800, lr=8.80451e-05, gnorm=2.024, clip=0, loss_scale=8, train_wall=57, gb_free=17.9, wall=13036
2023-07-05 12:05:47 | INFO | train_inner | epoch 018:    856 / 1474 loss=6.171, trans_loss=6.72, nll_loss=6.22, w2v_ctc_loss=2.256, contrastive_loss=0, total=4181.64, n_correct=251.54, ppl=74.56, accuracy=6.015, wps=14365.4, ups=1.73, wpb=8300.9, bsz=306.1, num_updates=25900, lr=8.7875e-05, gnorm=1.996, clip=0, loss_scale=8, train_wall=57, gb_free=15.9, wall=13094
2023-07-05 12:06:43 | INFO | train_inner | epoch 018:    956 / 1474 loss=6.118, trans_loss=6.699, nll_loss=6.2, w2v_ctc_loss=2.2, contrastive_loss=0, total=4133.45, n_correct=249.25, ppl=73.54, accuracy=6.03, wps=14462.1, ups=1.76, wpb=8204.4, bsz=312.7, num_updates=26000, lr=8.77058e-05, gnorm=2.191, clip=0, loss_scale=8, train_wall=56, gb_free=14.9, wall=13151
2023-07-05 12:06:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 12:06:59 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 9.759 | trans_loss 12.949 | nll_loss 11.903 | w2v_ctc_loss 2.313 | contrastive_loss 0 | total 4003.4 | n_correct 285.9 | ppl 3828.85 | accuracy 7.141 | uer 33.945 | wer 36.02 | raw_wer 36.02 | wps 4894.1 | wpb 4003.4 | bsz 141.8 | num_updates 26000 | best_loss 8.095
2023-07-05 12:06:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26000 updates
2023-07-05 12:06:59 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_18_26000.pt
2023-07-05 12:07:01 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_18_26000.pt
2023-07-05 12:07:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_18_26000.pt (epoch 18 @ 26000 updates, score 9.759) (writing took 5.027088008000646 seconds)
2023-07-05 12:08:02 | INFO | train_inner | epoch 018:   1056 / 1474 loss=6.219, trans_loss=6.711, nll_loss=6.212, w2v_ctc_loss=2.338, contrastive_loss=0, total=4133.66, n_correct=248.26, ppl=74.16, accuracy=6.006, wps=10442.6, ups=1.27, wpb=8212.5, bsz=298.3, num_updates=26100, lr=8.75376e-05, gnorm=2.039, clip=0, loss_scale=8, train_wall=58, gb_free=17.1, wall=13229
2023-07-05 12:09:00 | INFO | train_inner | epoch 018:   1156 / 1474 loss=6.144, trans_loss=6.7, nll_loss=6.201, w2v_ctc_loss=2.238, contrastive_loss=0, total=4156.35, n_correct=247.11, ppl=73.57, accuracy=5.945, wps=14312.9, ups=1.73, wpb=8249.9, bsz=315.8, num_updates=26200, lr=8.73704e-05, gnorm=1.994, clip=0, loss_scale=8, train_wall=57, gb_free=14.1, wall=13287
2023-07-05 12:09:57 | INFO | train_inner | epoch 018:   1256 / 1474 loss=6.195, trans_loss=6.749, nll_loss=6.252, w2v_ctc_loss=2.263, contrastive_loss=0, total=4093.35, n_correct=243.1, ppl=76.2, accuracy=5.939, wps=14064.8, ups=1.73, wpb=8119.8, bsz=288, num_updates=26300, lr=8.72041e-05, gnorm=2.172, clip=0, loss_scale=8, train_wall=57, gb_free=16.4, wall=13345
2023-07-05 12:10:55 | INFO | train_inner | epoch 018:   1356 / 1474 loss=6.254, trans_loss=6.73, nll_loss=6.235, w2v_ctc_loss=2.373, contrastive_loss=0, total=4056.71, n_correct=243.08, ppl=75.31, accuracy=5.992, wps=14105.2, ups=1.75, wpb=8067.3, bsz=289.2, num_updates=26400, lr=8.70388e-05, gnorm=2.064, clip=0, loss_scale=8, train_wall=57, gb_free=17.4, wall=13402
2023-07-05 12:11:53 | INFO | train_inner | epoch 018:   1456 / 1474 loss=6.18, trans_loss=6.72, nll_loss=6.226, w2v_ctc_loss=2.275, contrastive_loss=0, total=4125.39, n_correct=246.54, ppl=74.83, accuracy=5.976, wps=14087.3, ups=1.72, wpb=8192.2, bsz=299, num_updates=26500, lr=8.68744e-05, gnorm=2.139, clip=0, loss_scale=8, train_wall=58, gb_free=16.3, wall=13460
2023-07-05 12:12:03 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 12:12:19 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 9.58 | trans_loss 13.002 | nll_loss 11.965 | w2v_ctc_loss 1.594 | contrastive_loss 0 | total 4003.4 | n_correct 284.1 | ppl 3997.86 | accuracy 7.096 | uer 34.9 | wer 37.631 | raw_wer 37.631 | wps 4174.6 | wpb 4003.4 | bsz 141.8 | num_updates 26518 | best_loss 8.095
2023-07-05 12:12:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26518 updates
2023-07-05 12:12:19 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 12:12:23 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 12:12:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 18 @ 26518 updates, score 9.58) (writing took 4.200330008001401 seconds)
2023-07-05 12:12:24 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-07-05 12:12:24 | INFO | train | epoch 018 | loss 6.167 | trans_loss 6.706 | nll_loss 6.208 | w2v_ctc_loss 2.264 | contrastive_loss 0 | total 4138.44 | n_correct 248.707 | ppl 73.91 | accuracy 6.01 | wps 13463.1 | ups 1.64 | wpb 8216.7 | bsz 305.6 | num_updates 26518 | lr 8.6845e-05 | gnorm 2.066 | clip 0 | loss_scale 8 | train_wall 845 | gb_free 16.2 | wall 13491
2023-07-05 12:12:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 12:12:24 | INFO | fairseq.trainer | begin training epoch 19
2023-07-05 12:12:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 12:13:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-07-05 12:13:18 | INFO | train_inner | epoch 019:     83 / 1474 loss=6.14, trans_loss=6.704, nll_loss=6.209, w2v_ctc_loss=2.222, contrastive_loss=0, total=4103.35, n_correct=242.13, ppl=73.99, accuracy=5.901, wps=9515.8, ups=1.17, wpb=8143.6, bsz=297.1, num_updates=26600, lr=8.6711e-05, gnorm=2.034, clip=0, loss_scale=4, train_wall=57, gb_free=16.7, wall=13545
2023-07-05 12:14:17 | INFO | train_inner | epoch 019:    183 / 1474 loss=6.15, trans_loss=6.722, nll_loss=6.223, w2v_ctc_loss=2.227, contrastive_loss=0, total=4219.78, n_correct=253.83, ppl=74.7, accuracy=6.015, wps=14383.9, ups=1.71, wpb=8387.3, bsz=323.2, num_updates=26700, lr=8.65485e-05, gnorm=2.03, clip=0, loss_scale=4, train_wall=58, gb_free=13.2, wall=13604
2023-07-05 12:15:14 | INFO | train_inner | epoch 019:    283 / 1474 loss=6.188, trans_loss=6.753, nll_loss=6.256, w2v_ctc_loss=2.244, contrastive_loss=0, total=4192.4, n_correct=248.08, ppl=76.43, accuracy=5.917, wps=14442.6, ups=1.74, wpb=8323.2, bsz=308.2, num_updates=26800, lr=8.63868e-05, gnorm=2.012, clip=0, loss_scale=4, train_wall=57, gb_free=16.3, wall=13661
2023-07-05 12:16:12 | INFO | train_inner | epoch 019:    383 / 1474 loss=6.124, trans_loss=6.725, nll_loss=6.229, w2v_ctc_loss=2.181, contrastive_loss=0, total=4172.45, n_correct=249.48, ppl=75.01, accuracy=5.979, wps=14444.1, ups=1.74, wpb=8279, bsz=312.3, num_updates=26900, lr=8.62261e-05, gnorm=2.058, clip=0, loss_scale=4, train_wall=57, gb_free=14.7, wall=13719
2023-07-05 12:17:09 | INFO | train_inner | epoch 019:    483 / 1474 loss=6.157, trans_loss=6.739, nll_loss=6.244, w2v_ctc_loss=2.209, contrastive_loss=0, total=4100.34, n_correct=248.03, ppl=75.79, accuracy=6.049, wps=14207.8, ups=1.75, wpb=8136.1, bsz=298.5, num_updates=27000, lr=8.60663e-05, gnorm=2.051, clip=0, loss_scale=4, train_wall=57, gb_free=14.5, wall=13776
2023-07-05 12:18:06 | INFO | train_inner | epoch 019:    583 / 1474 loss=6.156, trans_loss=6.704, nll_loss=6.21, w2v_ctc_loss=2.248, contrastive_loss=0, total=4135.81, n_correct=250.23, ppl=74.01, accuracy=6.05, wps=14363, ups=1.75, wpb=8215.5, bsz=306.5, num_updates=27100, lr=8.59074e-05, gnorm=2.087, clip=0, loss_scale=4, train_wall=57, gb_free=17, wall=13833
2023-07-05 12:19:03 | INFO | train_inner | epoch 019:    683 / 1474 loss=6.132, trans_loss=6.717, nll_loss=6.222, w2v_ctc_loss=2.194, contrastive_loss=0, total=4201.16, n_correct=255.96, ppl=74.64, accuracy=6.093, wps=14551.4, ups=1.74, wpb=8340.1, bsz=321.9, num_updates=27200, lr=8.57493e-05, gnorm=1.957, clip=0, loss_scale=4, train_wall=57, gb_free=17.4, wall=13891
2023-07-05 12:20:01 | INFO | train_inner | epoch 019:    783 / 1474 loss=6.127, trans_loss=6.725, nll_loss=6.235, w2v_ctc_loss=2.183, contrastive_loss=0, total=4144.63, n_correct=246.87, ppl=75.31, accuracy=5.956, wps=14203.3, ups=1.72, wpb=8238.1, bsz=305.6, num_updates=27300, lr=8.55921e-05, gnorm=2.136, clip=0, loss_scale=4, train_wall=58, gb_free=12.8, wall=13949
2023-07-05 12:20:59 | INFO | train_inner | epoch 019:    883 / 1474 loss=6.199, trans_loss=6.774, nll_loss=6.281, w2v_ctc_loss=2.233, contrastive_loss=0, total=4151.05, n_correct=247.8, ppl=77.74, accuracy=5.97, wps=14261.7, ups=1.73, wpb=8236.9, bsz=303.2, num_updates=27400, lr=8.54358e-05, gnorm=2.162, clip=0, loss_scale=4, train_wall=57, gb_free=16.5, wall=14006
2023-07-05 12:21:58 | INFO | train_inner | epoch 019:    983 / 1474 loss=6.112, trans_loss=6.736, nll_loss=6.242, w2v_ctc_loss=2.156, contrastive_loss=0, total=4099.07, n_correct=246.5, ppl=75.67, accuracy=6.014, wps=13947.5, ups=1.71, wpb=8135.4, bsz=307.5, num_updates=27500, lr=8.52803e-05, gnorm=2.126, clip=0, loss_scale=4, train_wall=58, gb_free=14.6, wall=14065
2023-07-05 12:22:55 | INFO | train_inner | epoch 019:   1083 / 1474 loss=6.201, trans_loss=6.734, nll_loss=6.242, w2v_ctc_loss=2.287, contrastive_loss=0, total=4042.41, n_correct=241.86, ppl=75.67, accuracy=5.983, wps=13998.6, ups=1.74, wpb=8033, bsz=293.6, num_updates=27600, lr=8.51257e-05, gnorm=2.137, clip=0, loss_scale=4, train_wall=57, gb_free=17, wall=14122
2023-07-05 12:23:53 | INFO | train_inner | epoch 019:   1183 / 1474 loss=6.23, trans_loss=6.759, nll_loss=6.268, w2v_ctc_loss=2.3, contrastive_loss=0, total=4134.94, n_correct=246.15, ppl=77.05, accuracy=5.953, wps=14111.7, ups=1.72, wpb=8209.6, bsz=305.9, num_updates=27700, lr=8.49719e-05, gnorm=2.073, clip=0, loss_scale=4, train_wall=58, gb_free=16.8, wall=14180
2023-07-05 12:24:50 | INFO | train_inner | epoch 019:   1283 / 1474 loss=6.179, trans_loss=6.75, nll_loss=6.257, w2v_ctc_loss=2.232, contrastive_loss=0, total=4144.17, n_correct=247.37, ppl=76.48, accuracy=5.969, wps=14381.6, ups=1.75, wpb=8222.3, bsz=301.4, num_updates=27800, lr=8.48189e-05, gnorm=2.013, clip=0, loss_scale=4, train_wall=57, gb_free=16.7, wall=14237
2023-07-05 12:25:48 | INFO | train_inner | epoch 019:   1383 / 1474 loss=6.19, trans_loss=6.77, nll_loss=6.274, w2v_ctc_loss=2.24, contrastive_loss=0, total=4123.85, n_correct=248.02, ppl=77.37, accuracy=6.014, wps=14220.2, ups=1.74, wpb=8177.8, bsz=300.7, num_updates=27900, lr=8.46668e-05, gnorm=2.14, clip=0, loss_scale=4, train_wall=57, gb_free=15.8, wall=14295
2023-07-05 12:26:40 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 12:26:56 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 9.915 | trans_loss 13.098 | nll_loss 12.06 | w2v_ctc_loss 2.488 | contrastive_loss 0 | total 4003.4 | n_correct 282.7 | ppl 4268.48 | accuracy 7.061 | uer 32.838 | wer 34.63 | raw_wer 34.63 | wps 4710.5 | wpb 4003.4 | bsz 141.8 | num_updates 27991 | best_loss 8.095
2023-07-05 12:26:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 27991 updates
2023-07-05 12:26:56 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 12:27:00 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 12:27:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 19 @ 27991 updates, score 9.915) (writing took 4.047988662001444 seconds)
2023-07-05 12:27:00 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-07-05 12:27:00 | INFO | train | epoch 019 | loss 6.165 | trans_loss 6.737 | nll_loss 6.243 | w2v_ctc_loss 2.227 | contrastive_loss 0 | total 4138.76 | n_correct 248.28 | ppl 75.73 | accuracy 5.999 | wps 13808.5 | ups 1.68 | wpb 8217.4 | bsz 305.7 | num_updates 27991 | lr 8.4529e-05 | gnorm 2.074 | clip 0 | loss_scale 4 | train_wall 843 | gb_free 17.5 | wall 14367
2023-07-05 12:27:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 12:27:00 | INFO | fairseq.trainer | begin training epoch 20
2023-07-05 12:27:00 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 12:27:13 | INFO | train_inner | epoch 020:      9 / 1474 loss=6.177, trans_loss=6.74, nll_loss=6.246, w2v_ctc_loss=2.243, contrastive_loss=0, total=4119.95, n_correct=250.99, ppl=75.87, accuracy=6.092, wps=9585, ups=1.17, wpb=8187.6, bsz=303.3, num_updates=28000, lr=8.45154e-05, gnorm=2.063, clip=0, loss_scale=4, train_wall=57, gb_free=17.3, wall=14380
2023-07-05 12:27:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 12:27:28 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 9.925 | trans_loss 13.183 | nll_loss 12.142 | w2v_ctc_loss 2.325 | contrastive_loss 0 | total 4003.4 | n_correct 281.9 | ppl 4520.78 | accuracy 7.042 | uer 33.74 | wer 36.14 | raw_wer 36.14 | wps 4762.9 | wpb 4003.4 | bsz 141.8 | num_updates 28000 | best_loss 8.095
2023-07-05 12:27:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 28000 updates
2023-07-05 12:27:28 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_20_28000.pt
2023-07-05 12:27:31 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_20_28000.pt
2023-07-05 12:27:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_20_28000.pt (epoch 20 @ 28000 updates, score 9.925) (writing took 4.9638096269991365 seconds)
2023-07-05 12:28:32 | INFO | train_inner | epoch 020:    109 / 1474 loss=6.148, trans_loss=6.736, nll_loss=6.243, w2v_ctc_loss=2.203, contrastive_loss=0, total=4200.95, n_correct=251.09, ppl=75.73, accuracy=5.977, wps=10617.1, ups=1.27, wpb=8352, bsz=314, num_updates=28100, lr=8.43649e-05, gnorm=2.095, clip=0, loss_scale=4, train_wall=58, gb_free=15.6, wall=14459
2023-07-05 12:29:30 | INFO | train_inner | epoch 020:    209 / 1474 loss=6.214, trans_loss=6.756, nll_loss=6.264, w2v_ctc_loss=2.282, contrastive_loss=0, total=4157.8, n_correct=248.87, ppl=76.85, accuracy=5.986, wps=14110.8, ups=1.71, wpb=8256.1, bsz=302.6, num_updates=28200, lr=8.42152e-05, gnorm=2.135, clip=0, loss_scale=4, train_wall=58, gb_free=15.9, wall=14517
2023-07-05 12:30:28 | INFO | train_inner | epoch 020:    309 / 1474 loss=6.115, trans_loss=6.732, nll_loss=6.239, w2v_ctc_loss=2.158, contrastive_loss=0, total=4186.94, n_correct=247.79, ppl=75.53, accuracy=5.918, wps=14485.3, ups=1.74, wpb=8312.2, bsz=324.3, num_updates=28300, lr=8.40663e-05, gnorm=1.932, clip=0, loss_scale=4, train_wall=57, gb_free=16.2, wall=14575
2023-07-05 12:31:25 | INFO | train_inner | epoch 020:    409 / 1474 loss=6.131, trans_loss=6.75, nll_loss=6.258, w2v_ctc_loss=2.162, contrastive_loss=0, total=4104.42, n_correct=245.05, ppl=76.54, accuracy=5.97, wps=14161.3, ups=1.74, wpb=8148.9, bsz=296.7, num_updates=28400, lr=8.39181e-05, gnorm=2.144, clip=0, loss_scale=4, train_wall=57, gb_free=16.2, wall=14632
2023-07-05 12:32:23 | INFO | train_inner | epoch 020:    509 / 1474 loss=6.179, trans_loss=6.773, nll_loss=6.279, w2v_ctc_loss=2.212, contrastive_loss=0, total=4120.52, n_correct=247.01, ppl=77.66, accuracy=5.995, wps=14165.7, ups=1.74, wpb=8162.5, bsz=302.3, num_updates=28500, lr=8.37708e-05, gnorm=1.963, clip=0, loss_scale=4, train_wall=57, gb_free=16.3, wall=14690
2023-07-05 12:33:20 | INFO | train_inner | epoch 020:    609 / 1474 loss=6.206, trans_loss=6.778, nll_loss=6.286, w2v_ctc_loss=2.253, contrastive_loss=0, total=4088.74, n_correct=241.82, ppl=78.05, accuracy=5.914, wps=14146.2, ups=1.74, wpb=8119.2, bsz=294.6, num_updates=28600, lr=8.36242e-05, gnorm=2.18, clip=0, loss_scale=4, train_wall=57, gb_free=12.7, wall=14747
2023-07-05 12:34:17 | INFO | train_inner | epoch 020:    709 / 1474 loss=6.155, trans_loss=6.738, nll_loss=6.244, w2v_ctc_loss=2.206, contrastive_loss=0, total=4134.98, n_correct=246.1, ppl=75.81, accuracy=5.952, wps=14385.3, ups=1.75, wpb=8211.1, bsz=300.5, num_updates=28700, lr=8.34784e-05, gnorm=2.046, clip=0, loss_scale=8, train_wall=57, gb_free=14.5, wall=14805
2023-07-05 12:35:15 | INFO | train_inner | epoch 020:    809 / 1474 loss=6.146, trans_loss=6.732, nll_loss=6.242, w2v_ctc_loss=2.212, contrastive_loss=0, total=4145.82, n_correct=245.1, ppl=75.68, accuracy=5.912, wps=14331.9, ups=1.74, wpb=8232.7, bsz=306.6, num_updates=28800, lr=8.33333e-05, gnorm=2.071, clip=0, loss_scale=8, train_wall=57, gb_free=13, wall=14862
2023-07-05 12:36:13 | INFO | train_inner | epoch 020:    909 / 1474 loss=6.156, trans_loss=6.762, nll_loss=6.269, w2v_ctc_loss=2.193, contrastive_loss=0, total=4161.77, n_correct=247.04, ppl=77.12, accuracy=5.936, wps=14136.2, ups=1.71, wpb=8244.5, bsz=324.4, num_updates=28900, lr=8.3189e-05, gnorm=1.932, clip=0, loss_scale=8, train_wall=58, gb_free=16.9, wall=14920
2023-07-05 12:37:11 | INFO | train_inner | epoch 020:   1009 / 1474 loss=6.183, trans_loss=6.748, nll_loss=6.257, w2v_ctc_loss=2.241, contrastive_loss=0, total=4167.85, n_correct=245.1, ppl=76.49, accuracy=5.881, wps=14223.7, ups=1.72, wpb=8256.9, bsz=306.3, num_updates=29000, lr=8.30455e-05, gnorm=2.119, clip=0, loss_scale=8, train_wall=58, gb_free=17.1, wall=14978
2023-07-05 12:38:09 | INFO | train_inner | epoch 020:   1109 / 1474 loss=6.108, trans_loss=6.731, nll_loss=6.241, w2v_ctc_loss=2.151, contrastive_loss=0, total=4169.06, n_correct=244.93, ppl=75.66, accuracy=5.875, wps=14430.5, ups=1.74, wpb=8284.5, bsz=315.8, num_updates=29100, lr=8.29027e-05, gnorm=2.114, clip=0, loss_scale=8, train_wall=57, gb_free=17, wall=15036
2023-07-05 12:39:06 | INFO | train_inner | epoch 020:   1209 / 1474 loss=6.173, trans_loss=6.744, nll_loss=6.255, w2v_ctc_loss=2.239, contrastive_loss=0, total=4023.64, n_correct=234.86, ppl=76.38, accuracy=5.837, wps=13929.3, ups=1.74, wpb=8003.5, bsz=282.8, num_updates=29200, lr=8.27606e-05, gnorm=2.186, clip=0, loss_scale=8, train_wall=57, gb_free=16.2, wall=15093
2023-07-05 12:40:04 | INFO | train_inner | epoch 020:   1309 / 1474 loss=6.173, trans_loss=6.732, nll_loss=6.24, w2v_ctc_loss=2.255, contrastive_loss=0, total=4128.46, n_correct=244.05, ppl=75.6, accuracy=5.911, wps=14214.3, ups=1.73, wpb=8212.8, bsz=299.2, num_updates=29300, lr=8.26192e-05, gnorm=2.111, clip=0, loss_scale=8, train_wall=57, gb_free=16.7, wall=15151
2023-07-05 12:41:02 | INFO | train_inner | epoch 020:   1409 / 1474 loss=6.179, trans_loss=6.755, nll_loss=6.265, w2v_ctc_loss=2.229, contrastive_loss=0, total=4120.53, n_correct=239.66, ppl=76.91, accuracy=5.816, wps=14119.8, ups=1.73, wpb=8180.1, bsz=294.2, num_updates=29400, lr=8.24786e-05, gnorm=2.124, clip=0, loss_scale=8, train_wall=58, gb_free=16, wall=15209
2023-07-05 12:41:39 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 12:41:55 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 9.78 | trans_loss 13.179 | nll_loss 12.149 | w2v_ctc_loss 1.85 | contrastive_loss 0 | total 4003.4 | n_correct 275.9 | ppl 4541.04 | accuracy 6.892 | uer 33.61 | wer 36.043 | raw_wer 36.043 | wps 4525.9 | wpb 4003.4 | bsz 141.8 | num_updates 29465 | best_loss 8.095
2023-07-05 12:41:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 29465 updates
2023-07-05 12:41:55 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 12:41:59 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt
2023-07-05 12:41:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_last.pt (epoch 20 @ 29465 updates, score 9.78) (writing took 4.342020713000238 seconds)
2023-07-05 12:41:59 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-07-05 12:41:59 | INFO | train | epoch 020 | loss 6.163 | trans_loss 6.747 | nll_loss 6.256 | w2v_ctc_loss 2.215 | contrastive_loss 0 | total 4138.65 | n_correct 245.103 | ppl 76.4 | accuracy 5.922 | wps 13474.5 | ups 1.64 | wpb 8217.2 | bsz 305.7 | num_updates 29465 | lr 8.23876e-05 | gnorm 2.079 | clip 0 | loss_scale 8 | train_wall 844 | gb_free 16.8 | wall 15266
2023-07-05 12:41:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-05 12:41:59 | INFO | fairseq.trainer | begin training epoch 21
2023-07-05 12:41:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-05 12:42:27 | INFO | train_inner | epoch 021:     35 / 1474 loss=6.178, trans_loss=6.756, nll_loss=6.262, w2v_ctc_loss=2.219, contrastive_loss=0, total=4145.63, n_correct=245.81, ppl=76.74, accuracy=5.929, wps=9712.7, ups=1.18, wpb=8228.2, bsz=315.4, num_updates=29500, lr=8.23387e-05, gnorm=2.046, clip=0, loss_scale=8, train_wall=57, gb_free=17, wall=15294
2023-07-05 12:43:24 | INFO | train_inner | epoch 021:    135 / 1474 loss=6.079, trans_loss=6.714, nll_loss=6.221, w2v_ctc_loss=2.126, contrastive_loss=0, total=4194.57, n_correct=247.98, ppl=74.6, accuracy=5.912, wps=14444.7, ups=1.73, wpb=8331.6, bsz=319.6, num_updates=29600, lr=8.21995e-05, gnorm=2.029, clip=0, loss_scale=8, train_wall=57, gb_free=16.7, wall=15351
2023-07-05 12:44:21 | INFO | train_inner | epoch 021:    235 / 1474 loss=6.093, trans_loss=6.735, nll_loss=6.244, w2v_ctc_loss=2.115, contrastive_loss=0, total=4152.42, n_correct=244.53, ppl=75.82, accuracy=5.889, wps=14455.2, ups=1.75, wpb=8252.4, bsz=312, num_updates=29700, lr=8.2061e-05, gnorm=2.135, clip=0, loss_scale=8, train_wall=57, gb_free=17.6, wall=15408
2023-07-05 12:45:19 | INFO | train_inner | epoch 021:    335 / 1474 loss=6.121, trans_loss=6.742, nll_loss=6.252, w2v_ctc_loss=2.156, contrastive_loss=0, total=4157.2, n_correct=249.19, ppl=76.22, accuracy=5.994, wps=14209.5, ups=1.72, wpb=8256.3, bsz=311.1, num_updates=29800, lr=8.19232e-05, gnorm=2.049, clip=0, loss_scale=8, train_wall=58, gb_free=15.4, wall=15467
2023-07-05 12:46:17 | INFO | train_inner | epoch 021:    435 / 1474 loss=6.149, trans_loss=6.788, nll_loss=6.295, w2v_ctc_loss=2.147, contrastive_loss=0, total=4181.07, n_correct=244.88, ppl=78.54, accuracy=5.857, wps=14462, ups=1.74, wpb=8287.8, bsz=308.2, num_updates=29900, lr=8.17861e-05, gnorm=2.039, clip=0, loss_scale=8, train_wall=57, gb_free=15.3, wall=15524
2023-07-05 12:47:15 | INFO | train_inner | epoch 021:    535 / 1474 loss=6.161, trans_loss=6.75, nll_loss=6.259, w2v_ctc_loss=2.208, contrastive_loss=0, total=4089.72, n_correct=242.04, ppl=76.57, accuracy=5.918, wps=14069.7, ups=1.73, wpb=8139.3, bsz=295.7, num_updates=30000, lr=8.16497e-05, gnorm=2.149, clip=0, loss_scale=8, train_wall=57, gb_free=14.7, wall=15582
2023-07-05 12:47:15 | INFO | fairseq_cli.train | Stopping training due to num_updates: 30000 >= max_update: 30000
2023-07-05 12:47:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-05 12:47:30 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 9.744 | trans_loss 13.135 | nll_loss 12.115 | w2v_ctc_loss 1.833 | contrastive_loss 0 | total 4003.4 | n_correct 280.4 | ppl 4434.57 | accuracy 7.004 | uer 34.316 | wer 36.475 | raw_wer 36.475 | wps 4709.8 | wpb 4003.4 | bsz 141.8 | num_updates 30000 | best_loss 8.095
2023-07-05 12:47:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30000 updates
2023-07-05 12:47:30 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_21_30000.pt
2023-07-05 12:47:32 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_asronly/checkpoint_21_30000.pt
2023-07-05 12:47:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_asronly/checkpoint_21_30000.pt (epoch 21 @ 30000 updates, score 9.744) (writing took 5.042426711999724 seconds)
2023-07-05 12:47:35 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-07-05 12:47:35 | INFO | train | epoch 021 | loss 6.124 | trans_loss 6.748 | nll_loss 6.257 | w2v_ctc_loss 2.152 | contrastive_loss 0 | total 4150.85 | n_correct 245.484 | ppl 76.46 | accuracy 5.914 | wps 13120.4 | ups 1.59 | wpb 8244 | bsz 308.2 | num_updates 30000 | lr 8.16497e-05 | gnorm 2.082 | clip 0 | loss_scale 8 | train_wall 306 | gb_free 14.7 | wall 15602
2023-07-05 12:47:35 | INFO | fairseq_cli.train | done training in 15550.9 seconds
/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 768 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
