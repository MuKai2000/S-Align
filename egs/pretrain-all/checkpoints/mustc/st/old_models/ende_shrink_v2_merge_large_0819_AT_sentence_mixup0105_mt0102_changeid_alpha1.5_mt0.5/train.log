2023-08-20 01:27:19 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:19471
2023-08-20 01:27:19 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:19471
2023-08-20 01:27:19 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:19471
2023-08-20 01:27:19 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:19471
2023-08-20 01:27:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-20 01:27:19 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:19471
2023-08-20 01:27:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-20 01:27:19 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:19471
2023-08-20 01:27:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-20 01:27:19 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:19471
2023-08-20 01:27:19 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:19471
2023-08-20 01:27:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-20 01:27:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-20 01:27:20 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-20 01:27:20 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-20 01:27:20 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-20 01:27:20 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-20 01:27:20 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 0
2023-08-20 01:27:20 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-20 01:27:20 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 3
2023-08-20 01:27:20 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-20 01:27:20 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 1
2023-08-20 01:27:20 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-20 01:27:20 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 5
2023-08-20 01:27:20 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-20 01:27:20 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-20 01:27:20 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-20 01:27:20 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-20 01:27:20 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 4
2023-08-20 01:27:20 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 7
2023-08-20 01:27:20 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 2
2023-08-20 01:27:20 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 6
2023-08-20 01:27:23 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19471', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=True, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=True, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=True, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-20 01:27:23 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-08-20 01:27:23 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-08-20 01:27:23 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-20 01:27:23 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-08-20 01:27:23 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-20 01:27:27 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-20 01:27:27 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-20 01:27:27 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-20 01:27:29 | INFO | root | load pretrained hubert
2023-08-20 01:27:37 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-20 01:27:40 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-20 01:27:47 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-20 01:27:47 | INFO | root | share the sematic adapter and textual encoder
2023-08-20 01:27:47 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-20 01:27:47 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-08-20 01:27:47 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-20 01:27:47 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-08-20 01:27:47 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-08-20 01:27:47 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-20 01:27:47 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-20 01:27:47 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-20 01:27:48 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-20 01:27:48 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-20 01:28:03 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-20 01:28:03 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-20 01:28:03 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-20 01:28:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-20 01:28:03 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-20 01:28:03 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-20 01:28:03 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-20 01:28:03 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-20 01:28:03 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-20 01:28:03 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-20 01:28:03 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-20 01:28:03 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-20 01:28:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-20 01:28:03 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-20 01:28:03 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-20 01:28:03 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-20 01:28:03 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-20 01:28:03 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-20 01:28:03 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-20 01:28:03 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-20 01:28:03 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-20 01:28:05 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-20 01:28:07 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-20 01:28:49 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-20 01:28:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 01:28:49 | INFO | fairseq.trainer | begin training epoch 1
2023-08-20 01:28:49 | INFO | fairseq_cli.train | Start iterating over samples
True tensor([[   7,  422,  874,    0,    0,    9, 6788,  369,   12, 3648,    0,   26,
            7,  874,    9, 2761,   12,   13,  572,    0,    7,  572,   26,    0,
            0,    9, 2761,    0,   12,   13,  874,    0,    8,  722,   26,    7,
         7465,  131,  166,  251,    0,  248,   63, 3249,   62,    9,    7,  772,
          207,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [1179,   25,  135,   21,  109,   86,    0, 1335, 2348,   12, 1613,   71,
          747, 2693,   11,    6,    8, 1335, 2348,   12,   13,    0,  265,    0,
         6466,    6,  148,  205,   10,    7,  281,  909,  119,  236, 1563, 8562,
            9,  108,  753, 1772,    9,  909,   35, 3080,  109,  909,   35, 9376,
           54,    8, 4429,    0,    8,   53,  467,  132, 2762,  159, 2767,    6,
            0,    2],
        [  67,   21,  465,   11,   18,  468,    7, 2932, 1880,  567,    0,    8,
          113,    7, 2932, 3612, 1381,  832,   18,  233, 1181,   33, 3612, 1181,
         3860,   59, 1710,   10, 5111,   15,   21,    6,  768,   10, 5574,    7,
         1749, 1381,    8,    7,  341,  409, 1955,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  29,   56, 1285,    0,  347,  192,   11,   18,   25,  450,  117, 6010,
           70,   21,   11,    6,  100,  378, 7000,   71,   33, 4716,  434,  427,
          821,  407,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   9,  409,    0, 2753,  427, 1105,  810, 3565,   17,   21,  659,   51,
         1953,    6,   12, 1655,   12,  215,  801,    0,  166,  818,   17,   33,
         1167,  659,   66,  245,   13, 2003,   15,  106,    7, 1247,   12,   13,
          258, 1591,   17, 2963, 1585, 1763,    7, 1644,  511,  762, 1054,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,   11,    6,   13,  563,   93, 2484, 9473,   13, 4866,   10, 2266,
          389,  552, 1252,   11,    6,   38, 5001,  411,  293,    3,   24,   73,
          205,   10,   91,   12,    7,  822,  608,   56, 9694, 1075,    0, 3109,
         3902,    0,    8,  150,  113,   13, 1523, 3609,   12, 2718, 7719,    0,
          106,  747,    9, 1665,   10, 1962,    9, 1912,    0,    2,    1,    1,
            1,    1],
        [  33, 2780,   34, 6291,  111, 2770,   55,    7, 2932, 1232,    0,   67,
          115,   21,   11,    6, 2823,   54,  100, 3139,  140, 2704,    6,    9,
            7,  832,    0,    6,    0,    8, 2627,    0,  206, 2406,    6,   63,
         4213,   48,   10, 3067, 3316, 1006,   85, 3646, 1729,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 108, 2476,   26,   10, 8711,   17,   53, 1516,   71,   13,  509,  841,
           12,  138,   10,  229,  214,  254,  120,   53, 6565,    0,    8,    7,
         2027, 6936,  475, 2041,   17,   25,   73, 1531,  214,  126,  131, 1974,
          416,   54,  336,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:0') tensor([51, 62, 45, 28, 50, 58, 47, 41], device='cuda:0') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
2023-08-20 01:29:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
True tensor([[  55,  663,    0,    9,  264,  109,  232,  221,    6,   24,   66,  682,
         3196,   22, 2932,  896,    0,  206,   53, 3860,   75,  241,  221,   77,
          236,  866,    8, 6387,    8,   29,  234,  452, 6029, 7401,    0,    2,
            1,    1,    1],
        [ 115,   24,   11,  121,  492, 1110,   13, 7123,  100,   33,   12,   13,
         1470, 3189,    0,   67, 1470, 3189,    6,   73,   51, 1346,    0,  276,
          103,   53,   11,   57,   86, 1110,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [ 168,   26,  138,  488,   12,   13,  775,  411,   37,   34,  859,    0,
           21,    0,   34, 1523,    0,    0,   10,    0,    0,  508,   25,   13,
          841,   12,    7,    0, 1823,    0,    0,    0,   84,   25,  205,    0,
            2,    1,    1],
        [ 115,    7, 1885, 5055,    0,   12,  538,    0,   26,  206,   13,  325,
           12,    7, 2394, 4793,    8, 2791,  513,  199,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  29,   21,  818,   17,   51,   20, 7931,  373,   63,  133,  324,   85,
          203, 4030,  140,   54, 2429,   35, 2611,    6,    0,    8,   29,   53,
           11,  121,  226,  529,   10, 3258,  251, 4513,   96,    0,    2,    1,
            1,    1,    1],
        [  24,   66,    7, 2216,  768,   10,  205,    0,   38, 5276,   26,   70,
           19,  213,   10,   87,    3,    8,    9,   13,  613,  982, 1479,    0,
           17,    0,   10,  110,    0,   26,   13, 1968,  279,    0,    2,    1,
            1,    1,    1],
        [  25,   73, 6265,  281,   12,   17,   13, 1674,    6,    0,  125,  103,
           25,   11,   57, 4298,  336, 3594,    0,    8,    7,  595, 3336, 1233,
         2233, 4457,  111,    0,   25,   11,   57,  492,  142,   10, 1713,   17,
          595,    0,    2],
        [  29,    0,   85,   33,  613,    0,   19,   87,  213,   10,  450,   25,
           17,  185,   12,    7, 2767, 4131,   24,   66,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 101, 1119,    0,   38,   18, 2704,  518,  155,  227,  715,   96,    3,
          125,   33,   26, 1320,  111, 2059,    0,   77, 2808,   10,    7, 9510,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,  131,   13,  316,  287,    0, 7492,  136,  760,  322,   93,
            8,  697, 1262, 2178,  713,    0,    8,   53,   11,   57, 2091,    9,
         2567, 1507,    0,  116, 5000,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   0,    9,  264, 1884,  166,    0,   63,  142,   10,   51, 1405,    0,
           33,  169,   86,   51, 3118, 1623,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  67,   21,  465,   11,   18,  229,    0,   13, 1878,   10,  565,    0,
          103,    0,  284,  708,  162,  142,   10,   51,    0,   13, 2988,  109,
            0,   13, 2088,    0,   53,  162,    0,  142,   10,  205,   10,  670,
            0,    2,    1],
        [  67,   24,  499,  213,   10,   51, 1123,   17,   33,   26,    7,  281,
         4089,    8, 2426, 3180,   17,   24,   73,   51, 2985,  199, 2406,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,  768,   62,  131, 1962,   35,  699,  119, 3050,    6,  411,
          639,    0, 7176,   20,  111,  752,   10, 1914, 4907,   10,  875,  134,
          199,    7, 1232,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,    7, 5714,   17, 7076,    9,    7, 3227,   12,  423,  759,
         2270, 1098,   18,  500,    6,  148,  618,   69, 1102,  926,    6,   12,
            7, 8783,    8, 6907, 4895,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,  213,   25,   10,  150,  138,   33, 7223,   26, 5802,   62,    0,
            8,   21,   11,    6,  142,   10, 2546,  436,   29,   25,  150,   17,
           21,   11,    6, 1117,  126,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1]], device='cuda:4') tensor([36, 32, 37, 22, 35, 35, 39, 22, 26, 31, 20, 38, 26, 29, 31, 31],
       device='cuda:4') tensor([1.0000, 1.0000, 0.8418, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8721, 0.8208, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
True True tensor([[ 800,   91,    0,  103,  886,  424, 3848,   11,    6,  853,  200,  237,
           26,  593, 3401,  120, 3213,  187, 2044,    6, 1570,    6,  199,    7,
          964,    0,  568,   17,  641,   21,   11,    6,  735,  226,  593, 3401,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  24, 8396, 1469,  106,    0,  108, 2914,   37,   93,    0,  131, 2532,
          336, 3494,    0,    0,   67, 1223,   24,   77,   66,   33, 1738,    0,
         2914,   37,   93,   17,   24,   63,  746,    0,   12,    0, 5472, 1117,
          108, 3183,    6,    8,    0, 1459,  994,   26,  126,   84,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [2937,  982,  218,  943,    6,    9,  108, 6249,  451,   66,  774,   62,
         3928,    0,    8, 1800,  282,   13, 2333,   10,  175,  850, 1408, 1072,
            6,    0,  109, 3119,  294, 2518,   12,  215, 3928,  254,  956,   69,
          984,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,  125,   29,  294,   12,    7, 2787,    9, 1370, 2924,   11,   18,
          116,    9,  670,    0,   53,   11,   57,    9, 1037,    8, 1237,    0,
           70,   25,  113,  296,    0, 6957,    0,   26,  143,   69,    7,  230,
          874,  926,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 115,    0,  103,   25, 1816,   66,   13, 5590,   20,  521,   20,  874,
           93,    0,  115,   11,    6,   13,  324,  183,   10,  388,   21,    9,
          155,  874,    8,  598,   21,    0, 2603,   69,    7,  467,    0,   17,
           11,    6,  916,    0,   25,   11,  158,  446,  126,  347,   21,   11,
            6,  434,   13,  862,  365,  232, 1218,   20, 5590,   20,    0,    2,
            1,    1],
        [ 248,  215,  581,   85, 1366,    0,   19,  154,   46,   19,   11,  121,
          367,   10,   33, 7034,   46,   19,  154,   19,  659,   66,  226, 5187,
          106,   13, 3643,  384, 7710,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,   11,    6,    0,    0,    0,    0,    9,    0,   89, 6525,  230,
          115,    0,   38,  187,  144,   33, 1850, 2536,   48,    0, 1850,   62,
           69,    0,    7, 1850,    6,   20, 2286,  307,    0,    0, 4813,  248,
          215,  581,   69, 2154, 1374,   18,  881,   11,    6,  383,    0,   21,
           34,    7,  133, 2354,    0,    7,  473, 2829,    9,    7,  535, 8311,
            0,    2],
        [  29,  115,    0,  103,   24,  274,  270,   85,    7, 3253,    0,   85,
          108, 8196,    6,   17,  162, 5690,   54, 1004,    7, 3253,    0,  117,
         8196,    6,   63,  100,    7,  384,  174, 2003,   12,  108, 2626,  567,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:2') tensor([38, 48, 39, 40, 60, 31, 62, 38], device='cuda:2') tensor([1.0000, 0.8271, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 1.0000],
       device='cuda:2', dtype=torch.float16)
True tensor([[ 225,    0,  509,  ...,    1,    1,    1],
        [ 101,  876,  140,  ...,    1,    1,    1],
        [1979,  265,    0,  ...,    1,    1,    1],
        ...,
        [ 109,   63,   24,  ...,    1,    1,    1],
        [ 117,   63, 4700,  ...,    1,    1,    1],
        [   8,   33,  321,  ...,    1,    1,    1]], device='cuda:2') tensor([14, 12, 21, 10, 15, 16, 11, 12, 18, 11, 13, 17, 11, 18,  9, 13, 12, 24,
        19, 14, 20, 21, 20, 19, 17, 18, 19, 24, 10, 15, 10, 13, 18, 14, 10, 15,
        17, 17, 19, 22, 21, 17, 19, 18, 20, 14, 12, 12], device='cuda:2') tensor([1.0000, 1.0000, 0.8433, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8740, 0.8652, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8677, 1.0000, 1.0000, 1.0000, 0.8979, 1.0000, 0.8882, 1.0000, 0.8799,
        1.0000, 1.0000, 1.0000], device='cuda:2', dtype=torch.float16)
True True tensor([[3706,   55,    7,    0, 3210,   12, 2058,   35,  249,   59,  240, 1144,
            0,   53,   11,   57,  133,  946, 2787,    9, 2047,   12,    0,    0,
           73,    0,   24,    0,  694,  214,   10, 2231, 2736,  941,    0, 1645,
         2078, 2399,   70, 2892, 1075,    0,   73,   87,    8, 2293,  359, 4531,
          946, 6710,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  13,  143, 4089,  207,   10,  154,   80,   89, 6674,    6,  506,   51,
            9, 2047,   12, 1387,  687,    8, 4480,    0,  206,   85,  419, 1800,
          613,    9,  183,    0,   71,  419, 1127,  723,    0,   19,  217, 1102,
         1387,    8, 8219,  106,   17, 2216,    0,   77,   79,   13, 2443,   12,
           70,   19,  296,   10,   87,  230,  115,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   7, 7374,   17, 5290, 1247,    0,   79, 4546, 1247,    0,  220,  508,
         3667,   10,   13,  733, 6608,   12,  341, 1603, 4345,   46, 1259, 1247,
            0,  618,   59, 1247,    0, 3109,  140,   57,  411,  266,   26, 2856,
         1247,   46, 4574,   48,    7, 2303,   12,    7, 1827,    8,    7, 4715,
           12,    7, 1136,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [4157,    0, 7957, 6266, 1119, 1017,   21, 1962,    0,   85,  109, 3019,
          964, 4157,    0,    8,  180, 4587,   13, 1206,  567,   10, 1005,   21,
           84,    0, 4958,    7,   59,  627, 1202,  290, 1408,    0, 6712, 5933,
         6962,   26, 2770,   10, 5989,   85, 1362,   20,  586,  922, 4157,   71,
         3955,   45,  502, 8366,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,   34,   39,    9, 1324,    9, 1954, 6935,    0,   19, 1346,
           80,   13, 1361,   12, 8188, 4039,  148,  144, 1828,  244, 1339, 1027,
          587,  195, 3485,    0,    8,   13, 8188, 1793,  148,  692,   10,  508,
         4460,   69,   17, 3485,    0,    8,  211,  218, 3280,  692,   10,  205,
            8,  601,  267,  508, 4460,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 103,    0,   84,   11,    6,    0,    0,    0,    0,   91,  759, 4039,
           46,    8,   19,  172,  135,   33,   46,  148,   63,    0, 3163,   10,
          229,    0, 1583,  583,    6,    0,  148,   63,    0,    0, 3163,   10,
           51,   69, 3836,    0,   19,  217, 2497, 1123,   17,   24,   73,    0,
          204,  468,    7,  538,   12, 1261,    0, 2318,    0,   55,    7, 4959,
           12, 1146,    0,    2,    1,    1,    1],
        [   8,   29,   19,  246,    0,   38,    6, 1049,    3,    0,   29,   19,
            0,  175,  518,    0,    8,  225, 1559,   69,    0,    8,   71,   13,
          277,  523,   12,    7,  170, 1522,    0,    0,    0,   13,  176,    0,
            0,  180,  225, 1518,  336,    0,    8,  225, 1143,    0,   80,  423,
         2360,    0,    8,    0,    0,  225, 1518,  270,  336,    0,    8,  225,
           11,    6,   77, 6087,    6,    0,    2],
        [  85,    7,  183,   19,   34, 1931,   54,    9,  117,  341, 1427,    0,
           19,  474,   19,   34, 3258,   54, 4029, 1352,    0,   67,   69, 1858,
         2007, 1498, 1261, 9096,  237, 1181,    0,    8,   19, 4600,   19,   11,
           48,  204,  226, 3258,   54,   13, 1127,  546,   55,  143,  254,  423,
          215,    0,    8,    7, 2838,   69,  264, 1736,   34,   21,    6, 2402,
          119, 8197,  271,    0,    2,    1,    1]], device='cuda:5') tensor([52, 57, 53, 54, 55, 64, 67, 65], device='cuda:5') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 0.8584, 1.0000],
       device='cuda:5', dtype=torch.float16)
True tensor([[  21,   11,    6,    0,   86,   79,  103,   19,  144,    0,  419,    0,
          479,   70,   21,  169,  274,  100,    0,    0,   67,   84,   34,   13,
          841,   17,   24,  169,   66,   10,   87,  250,   71,  264, 9520,    0,
           12, 5149,    8,  250, 1057,   10,   87,   71,    0, 1588,  468,    8,
          250, 1057,   10,   87,   71, 1136, 1370,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  21,   11,    6,  116,  226, 4174,    0,  166,  818,  251,  148,  162,
         7220,  144,   10,   66,   13,  207,   12, 8254,  839,   10, 1703,  251,
          248,    8,   13,  854,  759, 1086,    0,    8, 1065,    0,  143,   55,
            7, 6580, 8068,    0,    8,    7, 3742, 7856,  249,    0,    8,    7,
         4528,    0,    8, 3097,    7, 2248,  181, 3414,   20,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [  67,    7,  630,   13, 2003,   96,    0, 1189,    0,   29,  138,  261,
           87,   56,  234, 1265,  241, 2142,    0, 2994,  122,  241, 2142,    0,
           13,   59,  380,   18, 3110,    8, 3098,    7, 1503,   12,  886,  412,
         3276,  287,    9, 4928, 1703,   55,   17, 7752,    9, 2446,   10,   17,
         2340,   10,    7, 1503,   12, 7121,  484,    0,  166, 2231,    6,   17,
         3833, 1548,    0,    2,    1,    1],
        [  21,   11,    6,  434, 3919, 8570,    0,    8,   21, 6430,    6,    0,
         2584,  218,  214,    0, 7753, 2736,  296, 1212,    8,  227,   93, 1069,
           96,    0, 2797,   54, 1868,  187, 3635, 1319, 3562, 7289,    8,  218,
         2808,   35, 5444, 2840,    6,   10, 3814, 3001, 3188, 1068,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19,  258,  636,  132,    9, 5950,   20, 1006,    0,    8,    7, 3564,
          162,  172, 3633,   17,    7, 5317,  144,  226,   13, 2786,    0,  125,
           85,   17, 2110,    0,   19,  144,   13,  277,  523,   12, 2080,    9,
           91,   12,   89,  488,   10,   96,    0,    8,   19,  474,    0,   38,
          122,   57,  411,    0,  125,   19,   11,   45,  142,   10,    7, 8022,
            6,    3,   68,  194,   65,    2],
        [   8,  115,  339,  110,  873,  908, 2955,  187,  445,  128,    0,    8,
          289,   17, 4586,  931,   46,  166,  185,   12,    7, 4793,   24,  175,
          568,   69,   33, 4959,   46,  120,   25, 2554,    7,  282,   12, 2307,
            0,   13, 2980,   37,    0,   13, 3700,    0,   13, 1276,    0,   53,
           63,  521, 1015, 3127,   54, 7553,  111,  199,    7, 2340,    0,    2,
            1,    1,    1,    1,    1,    1],
        [   0,   19,  113,  213,   10, 2678,   33,   91,  132,   13,    0,    0,
          523,    0,  138,   73,    0,   53, 4477,    0,   19,   11,   45,    0,
          142,   10,  388,  134,    0,   69,   13,  824,  586,  457, 5445,    0,
            0,  166,    0,   26,    7,   38,  122,  241, 1515,   10, 4853, 1435,
           85, 8251, 1243,    3, 1189,    0,   29,  168,   21,   26,    0,    2,
            1,    1,    1,    1,    1,    1],
        [1220,    9, 3023,    0, 2058,  181,  541, 1775,  271, 4814,    6,   66,
          226,  619,   10, 1754, 7371,  955, 2028,    0,  100, 4896,    6,    0,
         2678,  777,  812,    6,    0, 3183,    0, 2011, 6504,    6,    8, 4222,
            0,   17,   66,  226, 3150,  111, 7225,   62,  199, 2116,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:5') tensor([57, 59, 64, 48, 66, 60, 60, 48], device='cuda:5') tensor([0.8740, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8369, 1.0000],
       device='cuda:5', dtype=torch.float16)
True True tensor([[  67,  108, 8830,   48,  589,    0, 7557,    6,    0,  166,   63,    7,
         7557,    6,   17,  601,  170,   10, 6108,    8, 2544,    8,   51,    0,
            0,   13,  562,   18,    0,    8, 9063,   62,    8,   29, 4404,    8,
           29,   69,    0,   53,    0,   63,  261,  143, 7678,   62,  131, 1912,
          688,    0,    2],
        [  24,  116,  144, 6985, 2421,    6, 4502,   80,  854,   39, 3350,  581,
           77, 1004,    7, 6869, 9271,    0, 3381,   70,   11,    6,  956,    0,
            8,   24,   11,  121, 2138,   13,  325,   12,  183,    9,    7, 1726,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 185, 2193, 6248,   22,    6,    9, 1777,  150,  156,   79,   93,  675,
           45,    9, 5222, 1290, 1373,    6,    6,  693,    0,   67,  294,   73,
           51, 5853,  131,    7, 2932, 3288,    0,    8,  203, 1203, 1015,  922,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 106, 1244,   35, 1966,  673,   54, 2409,   10, 2465, 3602,    6,   10,
           33, 2178, 2445,  266, 1192,   69,   89,  906,    0, 3927,   63, 4015,
         5869,   12, 2434,  108,  179,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,  154,   21,   26, 2497, 1226,   55, 1221,   10, 1772,    9,    7,
          245,  561,  929,   13, 2226, 1757,   55,   70,  169, 1085,   10,    7,
          461,  266,  763,  480,    6, 1042,    7, 3677,  188, 3699,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   7, 4100, 1649, 1085,  143, 1149,    0,   67,  914,   69,    7, 1296,
           12,   13, 4400,   15,   22,  996,    0,  333,  555, 7691,  109,  333,
          555, 3619,  215,    0,   67,   21,   11,    6,  499,  250,   10,   51,
         4837,   12,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   8,    7, 7745,  659,   51,    0,   67,   21,  689,   11,   18, 4432,
           10,  452, 1539,  128,    6,    0,   17,   24,   66, 5020, 6083, 5475,
            6,   46,    7, 1320,   59,  816,  300, 1079, 1643, 1794,  230,   85,
            7, 1219,  859,   35, 2514, 4209,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [   9,  409,    0,    7,   29,   70,   26,    0,    9,    7,  572,    0,
         2317,    9, 7418, 3782,    0,    7,  214,   17,  229,  155,  572,  283,
            0,    7,  214,   17,  229,  155, 3236,  283,    9,  155,  572,    0,
           63, 8220,  111,  415,   59, 8186,   71, 2317,    9, 2011, 3043,    0,
            2,    1,    1]], device='cuda:7') tensor([51, 38, 38, 31, 36, 40, 44, 49], device='cuda:7') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
True tensor([[ 131, 1183,  148,   26, 1948,  664,   62,  132,   71,    7, 4617,  292,
         2109, 1775, 4040,    6,   12,   33, 6536,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   19,   11,   48,  100,   10,  227, 2174,  232,  445,  159,  283,
           71,   13,  277, 7074, 4112,    0,    8,    7, 4112,   26,   33,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 1425,   19,  144,   10, 1336,  235,    0,   19,  442,   17, 3486,
            8,   79, 2392,   79,   19,  442,   21,    0,   39, 2285, 1790,   12,
         2922,  552,  244,  110,    0,    2,    1,    1,    1,    1],
        [ 877,  290, 2689, 2345, 1212,  188, 2277,   48,   10, 2699,  143,    8,
          143, 5593,   10,   33,    0, 5413, 3301, 5690,    6,  199,  459,  510,
           96,   17,  719, 2070,    0,    2,    1,    1,    1,    1],
        [  29,   24, 6103,  154,   12, 4781,   79,  378, 3816,  214,   17,   24,
          991,   55,  133, 3926, 3965,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  12,  538,    0,  108, 4734,  192,   11,   18,  116,  468, 1004,  183,
            0,   53,  113,  468,  106,  561,   10,  561,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53,   66, 4595, 1122,   59,  699,   35,    6,  819,  424, 1942,   96,
            8, 4515,    6,   17,   63,   86, 3412,   62,  131,  883,   48,  293,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  103, 2263,   96,  498,    0,   10, 3424,    0,  180,    7,  723,
           73, 5415,    0,    0,    7,  524,    8, 3536,    7, 2517,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 427,  412,  430,  158,   13,  188,   13, 1905,  321,   12, 6873, 1117,
           17,    0,  120, 8491,   48,    0,    7,  427,  412,  430,  158, 3336,
           10, 1051, 1672,  153,  336,    0,  116, 7743,    0,    2],
        [  29,  417,  140, 2362,    6,   93,   26, 3423,   13, 1396, 2233,  199,
           39, 3993, 1874,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33,  188,  226,    7, 3841,    6,   20,   12,  333, 1670, 2127, 9746,
            6,    0, 3448, 7414,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  225, 4576,    6,   10,  110,    7, 1790,  225, 1831,   79,   13,
          785,   35, 1933,   35, 1178, 3642,    9,   17, 6460, 4057,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  116,  323,   13, 4454, 2761,   69, 5335,   80,    7, 5169,    8,
         1095,  248, 8099,    6,  558,   10,  545,  218,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 4639,   17,   24,  220,   51, 1155,  143,  254, 3857,    8, 1064,
            0,   67,  138,   10, 3918,    6,   20,  126,    7,   91,  106,    7,
          218,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  29,    7, 2551,   12,  324,    8, 6790,  214,   26, 2720,   48,   10,
         7572,    8,  211,  234, 1410,  837, 5002,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  101,   11,    6,    0,   13, 4961, 1531,    0,    0,   79, 3708,
           18, 3304,    0,  169,   66,  246,    0,   79,  238,   79,   13, 1880,
         1531,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   8,   10, 4637,  373,    0, 2192,   26,  116,  486, 6101,  567, 4111,
           10,   51, 4637,   62,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 8900,   62,  250,   19,  172,  144,   13, 2184,   22,  311,   69,
            0,   26,   17,    7, 1956,  204,  213,    6,   10,  283,   55,  159,
          110,  128,    0,    2,    1,    1,    1,    1,    1,    1],
        [   8,   17,   11,    6,  593, 1706,   12,   13,  183, 2760,   55,   77,
            7,  341, 1565,  409, 1215,   17,   24,  135,   10,  468,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1777,  188, 1220, 6380,   48,   17, 2439,  558,  464,    0,   53,   11,
           57, 5332,   54,   13, 4369,  200, 1515, 2902,    8, 3585,  567,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  84,   63,  423,   12,  117, 3357,    0,    8,   53,   63, 4673,    9,
          159, 4268,    8,  159, 4972,    0,    8,   53,   63, 2762,   94,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   45,  976, 8372,   17,   24,   66,  535, 1313, 5260,    7,
          613,  206, 4387,    6, 3480,  108,   24, 1591, 2637,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  13,  717,   35, 1315,  322,  801, 5725,    6,  221, 1339, 5061,  292,
            6,    6, 3222, 1026,   14,   48,   71,   33,    9,   21,    6, 9442,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  69,    7,    0,  939, 2836,  834,   26,    0,    7,   56,  111,    6,
         3620,  266,  207,   17,  368,    6, 2829, 7337, 7393,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:7') tensor([20, 25, 30, 30, 20, 22, 26, 24, 34, 17, 18, 24, 22, 27, 21, 27, 18, 28,
        24, 25, 25, 23, 26, 23], device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8550, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8848, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8975], device='cuda:7',
       dtype=torch.float16)
True True tensor([[  67,  490,   24,   87,   17,    0,   19,   11,   48,  100,   25,   10,
          154,   17,   85,  419, 1800,  183,  120,   25,   11,   57, 2202,   84,
            0,    9,  155, 1992, 5896, 1247,    0,    7, 2583, 8748, 9169, 3413,
           54,   10,  155, 9879,    0,    7, 9879,   26, 3413,   54,   10,  155,
         2583, 8748, 9169,    8,   33,   26,  138,  155, 4941,   26, 3803,    8,
         8965,   48,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [5609,   10,    7, 1793,  183,  368, 5673,    0, 1276,    6,  499,   87,
         4986,   79,  261, 1269, 2536,   20,   79, 1970,    6,    0,  166,   26,
          509,  254,   21,   34,    9,   95,  424, 3400, 2197,   11,    6,  383,
            0,   67,   19,  499,  154,   17,  250,  225, 2396,   26, 4803, 5040,
            0,   38,  187,   66,   86,  226, 2373,    9,    7, 9091, 1313, 9094,
            3,   68,  194,   65,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  56,  525,   11,    6, 4197,  246,   10,  267, 1848,    0,   38,  160,
            0, 1491,    0,    0,   33, 4925, 2823,    6,   55,  815, 6091, 1086,
            3,  267,    0, 1848,    0,  148,   63, 1102, 4907,    0, 1260,   69,
            0, 4340,  982,    0,    0,   38,  511,   17,   11,    6,   86,   77,
           46,  415, 1040,    0,   26, 1028,  126,    0,   71,   13,  264,    0,
         1375,    0, 2199, 1132,  820,    3,  225,    0,    0,  246,    0,    0,
           38, 2207, 4925,  164,    0, 2823,   55, 1581,    3,    3,  225,    0,
         8640,   48,    8,  246,    0,    0,   38,  187,  192,   11,   18,  135,
            0,  103,   17,   11,    6,    0, 1581,    3, 1995,  241,  221,  109,
            0, 1581,    3, 1793,    0, 1086,    0,    0,   67,    0, 3296,    0,
           21,   11,    6, 1581,    3,    3,   68,  194,   65,    2],
        [   8,    9,    7,  370,  207,  294,   12,  170,   63,  115, 5954, 4273,
            6,   17, 3412,  108, 1259, 1882,    0,  108,  203,    6,  777, 1991,
            0,  108, 3857,    0,   69,    7, 1080,    6,   17,   33,  659,  601,
          170, 3560, 4390,    0,   24,   73,  884, 1179, 5625,   54,    8,   39,
          128,   93,  195,   54,    7, 1289,   24, 2056,    0,   24, 8099,    0,
           24, 3836,    0,   24, 2153,    0,   73,  450,  170, 4324,   12,  183,
         1179,  250,  659,  205, 1226,   71,  108, 3227,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   71,   33,  822, 5108,    9,   13, 3708, 4774,    0,   24,   66,
         6960,   48,    9,  824, 2723,   45,  726, 1090,   17,   24,   73, 3814,
            7, 1729,   35, 1727,  430,  690, 3275,  110,  249,  132,   10, 1421,
          439,   12,   70,   26, 5914, 1417,   46,  248, 1086,  690, 3275,  110,
          249,    0,   24,   87,   21,   85, 7738,    0,  820,  828,    0,    8,
           24,   73,  499,  875,   17, 3151,  346,    0,  630,   12, 1823,    0,
          103,   25,   73,  875,    9,  143, 3708,    6,    0,   24,   73,  204,
          229,    7,  126, 2446,  276, 3028,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 875,   54,   94,  970,   10,  970,   71,  108, 3648,   26,   13,  207,
           12,  875,   54,  134,  970,   10,  970,   71,   94, 1004,  183,    0,
         1004,  672,    0, 4355,  931,  659,   66,  226,  133,  341,   10,  108,
          447,    0,   67,  148,    0,  100,  170,    0,  144, 1080,    6,    8,
         4989,    0, 2084,  335,   18, 1991,    6,    8, 3128, 2261,    9,  159,
          931,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([ 64,  65, 130,  82,  92,  63], device='cuda:3') tensor([1.0000, 1.0000, 0.8271, 1.0000, 1.0000, 1.0000], device='cuda:3',
       dtype=torch.float16)
True tensor([[  67,   25,  296,  ...,    1,    1,    1],
        [  84,   11,    6,  ...,    1,    1,    1],
        [  29,   70,  321,  ...,    1,    1,    1],
        ...,
        [  29,   24,   73,  ...,    1,    1,    1],
        [ 229, 1118,   63,  ...,    1,    1,    1],
        [ 103,   25,   11,  ...,    1,    1,    1]], device='cuda:3') tensor([ 9,  9, 13, 11, 14, 10, 14, 16, 15, 10, 13, 13,  9, 18, 11, 17, 20, 14,
        17, 14, 13, 11,  7, 14,  7, 14, 13, 13, 16, 10, 12, 14, 13,  9, 11, 13,
        12, 14, 11, 12, 15, 10, 11, 11, 14,  9, 10, 11, 15, 13,  7, 15, 15, 13,
         8, 17, 13, 20,  7, 16, 14, 13,  7, 13], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 0.8105, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8794, 1.0000, 1.0000, 1.0000, 0.8555, 1.0000,
        1.0000, 0.8579, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8320, 1.0000,
        1.0000, 0.8984, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8652, 0.8369,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8882,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8677, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000], device='cuda:3', dtype=torch.float16)
True True tensor([[  67,   24,  150,   17,  545,   91,   12,  117,   26,  204, 4637,   54,
            0,    8,  188,   13,  341,  207,   12, 3101,  126,  138,   10,   87,
          282,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   89, 1970,   34,   13, 3090,  756,    0,    8,   89,  291, 3769,
            0,    0,   34,   13, 1390,   37,  684,    8,    0,  113,   13, 1116,
         1040,   37,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  164,  450,   25,    0,   13,  546,   12,  248,  341,    0, 2352,
            0,    0,  248, 1284, 2352,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  391, 1551, 1065,    0,   17, 2182,  144, 3814,   48,   89,  637,
            8,  333,  473,  279,    9,   21, 3706,   55,  110,   10,   13, 1098,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  24, 3195, 6482,   35, 6937,  227, 3134,  221,    0,   19,    0,   20,
            0,  108, 2805,    0,  132,   10,  108, 7009,    6,   11, 1492, 1485,
           12, 3402,   93,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  53,  246,   53,  169,  780,   29,  835,   10, 1008,    7, 8365,    6,
           12,    7, 3311,    0, 7443,   17, 1573,  162, 3754,    8,  401,    7,
          370,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   24,  289, 1228, 3374,    0,  281,   12,   25,  154,   80,  708,
            0,   21,   11,    6, 1301,    0, 2703,  901,  439,   12,  708,   87,
          722, 1228, 3374,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 291, 1105,   18, 3396, 1095,    0,    7, 4480,   79, 1019, 2071,    0,
         1095,    7,    0,    0,    0, 5640, 1375,   79, 1019, 2071,  755,    0,
          254,   94,  148,  162,    9,  509, 1862,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57,  116, 2213,   54,    8, 9171,    0,   38, 3127,   89,
         3397,   26,    3,   70,   11,    6, 1226,   71,  682,  616,  249,  407,
            6,    0,   38,  187,   11,  121,  144,  185,  453,  110,  128,    6,
           85,  682,  616,  249,  407,    6,    0,    2],
        [ 101, 5523,   17, 1381,  220,   51, 1405,  131,    7,   94,    0,    8,
           24,  583,  251,   94, 9475, 4637,  373,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  53,  162, 1052, 1563,   94,    0,   67,   53,  162,  113,  172, 7176,
           20, 3979, 2120,    0,    8,   53,  162, 9231,   62,   12, 4391, 3834,
           12, 3716,   35, 7305, 2527,   15,  609,   96,    0, 5653,   10,    7,
          218,  415, 2633, 2120,    0,    2,    1,    1],
        [  85, 4615,    0, 9785,  369,   46,   17,  101,   11,   48,  226,  987,
         1181,  131,   39, 6651, 2266,  160,   22,    0,   13, 5289,  126,   10,
         2585,   20,  366,  565,    0,  276,   10, 9304,    7,  282,  126,   12,
          565,    0,    2,    1,    1,    1,    1,    1],
        [   0,    8,   53,  276,    0, 2982,   80, 1790, 8661,    0,    0,  276,
            0, 1445,   53,  162,    9, 3454,    0,  276, 1445,   53,  162,  830,
         1785,  810,    7, 5167,  608, 2020,   12,  159,  931,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  117, 4312,   22,    6,    0, 1025, 3043,   54,    0,   53,  415,
          158, 1515,   71,    7, 7224,    0,    8,    9,  117,  415,  158,  340,
         1955,   53, 3121,   13, 1905, 1762,   12,  941,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,   34,   85, 8195,   12, 2931, 2735, 1221,   46, 1016, 4618,
          866,  693,   46,  120,  248, 4618,  866,  693,  162,   86,  952,   10,
          545,  218,    0,   19,  169,  939, 8966,  834,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 131,    0, 5114,  235, 2399, 1006,    0,  889,   63,    0,    0,    9,
           13,    0,  841,    0,    0, 1449, 1778,  159, 5186,   10,    7, 1722,
         3609,   12,  422, 6222,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([27, 28, 19, 26, 29, 27, 29, 33, 44, 21, 42, 39, 35, 34, 34, 30],
       device='cuda:1') tensor([1.0000, 0.8887, 0.8271, 1.0000, 1.0000, 1.0000, 1.0000, 0.8584, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8721, 1.0000, 1.0000, 0.8208],
       device='cuda:1', dtype=torch.float16)
True tensor([[  53, 2828,  987,  ...,    1,    1,    1],
        [6938,   25,   66,  ...,    1,    1,    1],
        [   8,   12,  538,  ...,    1,    1,    1],
        ...,
        [  29,   24,   66,  ...,    1,    1,    1],
        [  67,  985,  687,  ...,    1,    1,    1],
        [   0,    8,  432,  ...,    1,    1,    1]], device='cuda:1') tensor([14, 13, 27, 18, 21, 17, 19, 15, 18, 17, 24, 12, 15, 15, 33, 26, 20, 11,
        22, 14, 15, 22, 24, 13, 20, 16, 15, 17, 15,  9, 16, 21, 15, 13, 16, 26,
        19, 13, 24, 13], device='cuda:1') tensor([1.0000, 1.0000, 0.8550, 0.8975, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8838, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8848, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8677], device='cuda:1', dtype=torch.float16)
True True tensor([[   9,   13, 1880, 4745,    0,   25,  498,   71,  211, 2837,    8,  211,
         4774,   10, 1393,   17,  199, 2332,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   29,  131, 3696,    7, 4221,    0,   24,   73,  446,  126,  261,
          143,   80,  155, 1037,  254,   25,  914,  135,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 101,   11,    6,    7, 6099,   12,  958,    9,    7, 6869, 8521, 2353,
           12, 5205,    0,    9,  421,  234,   48,  365,  221,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  24,  162,   86, 2841,    9, 1979,    0,  290,    0,   69,    7, 5225,
           38, 2862,  781,  918,   93,  197, 1017,    0, 2503,   18,  650,  568,
           86, 2115,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [3398, 7936,    8,  284, 1543, 1948, 1985,   87,   21, 2373,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,   84,   11,    6,  288,   91,  227,   15,    6, 2158,    0,    0,
          207,   10,    0,  205,  106,   84,    0,    0,   25,  175,  717,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   0,   10,  101,  128,    0,    0,   10, 1008,    0,   10,  289,    0,
           38,  533, 2056,   13, 1738, 1234,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 206,   11,    6,    7, 2808,    0,   38,  533, 1412, 2215,  159, 2360,
           10,    7, 2182,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 473,  111,    0,   33,   26,   13, 2322,   35, 5444, 2595,  856,  293,
            0,  125,   19,   66,   29,  294, 1516,    6,    9,   89, 1995,  918,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [7494,  185, 1118,  307,   34,   39, 1830,   35, 1933,   35, 1178, 2988,
          120,  101,   34, 1920,   22, 1371,   62,  106, 3022, 1146,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   33,   26,  116,  133,  946,   46,  183,   26,   69,    7, 3089,
           35, 7988,  340,    8,    7, 2509,   26,   69,    7, 1995,   35, 7988,
          340,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,    0,  169,  154, 1827, 1794,    0,  162, 4491,    0,  166,  818,
           53,  451,  601,   25,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   0,    8,   10,   87,   17,    0,   25, 1275,    0,   66,    0,   10,
         4988,   13, 1472,   12, 2066,  416, 1374,  572,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    0, 1220,   25,  659,   51, 1211,   12, 1835,    0,   38,  187,
           11,   45,   86,    9,   18, 2687,  241,   54,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  70,   11,    6,    7,  613,   12,  117, 3742, 3228,    6,   17,   25,
           11,  121,  591,    0,   38,  511,  101,  246,    3,  238,    0,   19,
           11,  121,  211,  479,    0,    2,    1,    1,    1,    1,    1,    1,
            1],
        [  33,  188, 8166,   19,  362, 1838, 1428,   55,   70,    7, 1479,   26,
          142,   10,   87,    9,    7,  858,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  17,   11,    6, 5653,   10, 1719,   59, 5292, 3147,  111, 1830,  439,
           55, 2677,    8,  248,   10,  391,  439,   55, 5031,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,  180, 2568,  347,   19,  305,    0,    0,   29,  261, 7003,  106,
         4990,  589,  596,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   7, 2403,   26,   17,   84,   11,    6,   13,  785,   35, 9058, 3900,
            0,   86,    7, 1953,  322,   35, 3149,   35,  290,   35, 8856, 3900,
          100,   10, 4482, 2147,    0,   67,   13,  785,   35, 9058, 3900,    0,
            2],
        [   8,    7, 1714,  608,  423,  439,    0,   53,  305,   80,  248,  439,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 244, 2869, 4021,    0, 2008, 6248, 1405,   91,   12,    7,  281, 3861,
          338, 3232,  128, 4621,   24,  135,   80,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,   11,   45,  142,   10,  575,   25,  138, 7218,  204, 2895,    6,
           71,  108, 5967,  282,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 110,  153,  240, 1547,    6, 3067,   62, 6873,    0,    8, 1645, 9745,
            6,    0,    0,    0,  593,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  67,  103,   25,  192,   11,   18,   66, 6759,   35,    6, 2174,  297,
           62, 3867,    0,  180,    7,  630,   12, 1703, 6230,   26,   13,  341,
           91, 5020,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:6') tensor([20, 22, 23, 28, 12, 25, 20, 17, 26, 24, 27, 18, 22, 22, 30, 20, 23, 17,
        37, 14, 21, 18, 19, 28], device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 0.8721, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8916, 0.8208, 1.0000, 1.0000, 1.0000, 1.0000, 0.8887,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8706, 1.0000], device='cuda:6',
       dtype=torch.float16)
True tensor([[1250,    0,   53,  ...,    1,    1,    1],
        [ 832, 1337,   22,  ...,    1,    1,    1],
        [  67,   25,  113,  ...,    1,    1,    1],
        ...,
        [  24,  278,    0,  ...,    1,    1,    1],
        [  24,  321,   12,  ...,    1,    1,    1],
        [  19,   11,   45,  ...,    1,    1,    1]], device='cuda:6') tensor([11, 11,  9, 13, 12, 12, 12, 12, 16,  9, 10, 11, 10, 11, 11, 12, 13, 11,
        11, 10,  9, 13, 17, 15,  7,  9, 11, 12, 17,  6,  7,  9, 10,  8, 10,  8,
        10, 11, 14, 13, 11,  9,  8, 19, 12,  9,  9,  6,  8, 16,  8, 12, 11, 13,
        10, 12, 10, 13, 10, 15,  7, 12, 11, 12, 13,  9, 12,  8, 13, 20,  8,  8,
         8,  7,  9, 11, 12, 11, 11, 12], device='cuda:6') tensor([0.8091, 1.0000, 1.0000, 0.8789, 1.0000, 0.8735, 1.0000, 1.0000, 1.0000,
        0.8555, 1.0000, 0.8799, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8218, 0.8652, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8691, 1.0000, 1.0000, 0.8521, 1.0000, 1.0000, 0.8984, 1.0000,
        0.8286, 1.0000, 1.0000, 1.0000, 1.0000, 0.8105, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8838, 0.8677, 1.0000, 1.0000, 1.0000, 0.8369,
        0.8652, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:6', dtype=torch.float16)
True True tensor([[  67,    7,  143,   24, 1260,   85,   21,    0,    7,  143,   24, 1873,
           17, 1057,  453, 3540,   34,    7,  133, 1890,  279,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,    0,   24,  552,  132,   71,   13, 4445,    0,    8,    0,  204,
           70,  956,   34,   91,    0, 2767, 2923,   37,    9,  264, 1736,   46,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 211,    0,    0,   19,  641,    0,    0,   19,  154,   46,    0,  682,
            0,   67,   26,   17,  155, 4234,    0,  846,  174,    0,    0,  211,
            0,    0,  682,    0,   70,   34,   17,   91,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  33,   26,   89, 9264,    0,   86,  860,   13,  488,   91,    0,   19,
          213,   10,  289,  116,   13, 1455,   80, 2263,  556, 5610,   54,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19,   11,   45,  142,   10, 4431,   35,  140, 4076,   69,  832, 1486,
          335,    0,    8,   24,   73,  150,  832, 1486,  335, 2546,  829,   69,
           21,    6,  926, 1585,   71,   21,    6, 4241,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [   9, 1296,   10, 2572,   25,   33, 1678,    0,   19,   11,   45,  142,
           10,  296,  185, 5825,    6,   10,  367,  132,   69,    6, 5072,  172,
         2117,    0,    8,   24,   11,   57,  142,   10,   87,   13,  277, 1860,
           35,  399,  102,  287,    0,    2],
        [  24,  465,   11,   18,   87,   33,  131,  204, 2905, 3141,    0,  100,
           25,   87,    9,   13, 1192,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  17, 1073,   13,    0, 1878,  629, 1891,    7,  595,    0,    9, 1416,
           12,   25,    8,    0, 4958,   54,    7, 4292,    0,  109,  893,  199,
           39, 4292,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 115,   26,   33,  250,   17,   24,  451,   51,  101,   62, 1256,   12,
            0,   12,  538,    0,   21,   11,    6,  528,   17,   24,  135,   33,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  77,   12,   33,   10,  289,   17,    7, 4719,  629,   70,   24,   73,
         7039,    8,   70,   24,   73, 2032,   26,  288,  142,   10, 4038,   22,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  33,   26,  106,   38,  469,  291,   18,  187,   62, 1224,   12, 1491,
            3,    9,  564,  868, 1047,    0,   84,  162, 1107,  820, 5149,  690,
         7653,   20,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  60,  116,  188,   13,  264, 3742, 6211,   17,  368,    6, 1019,  908,
          768,  254,  108, 6211,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   7, 5346,   12, 1009,   17, 1241,   13, 1127,  567,   26,    0,  554,
            0, 2816,  419, 1661,   57,  969,    6,  369,  109,  419, 1620,   54,
         5974,  440,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 214,   63,  341,  125,  115,   19,  135,   17, 4751,   26,    7, 1037,
         2829,   17, 1459,  188,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 103,   25, 1260,   85, 2264,  241,  140, 1782, 5152, 1160,  158,   25,
          162, 2639,    0,   25, 2220, 1631,  830,  150,  347,   53, 5953,   48,
         6387,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  24,    0,  692,   10,   66,   13,  740,  672,   71,    0,    7,  230,
            0,  688,    0,   71,    7,  230, 4157,    0,   71,    7,  230, 1404,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:0') tensor([23, 25, 34, 25, 35, 42, 19, 28, 26, 26, 28, 19, 28, 18, 27, 26],
       device='cuda:0') tensor([1.0000, 0.8755, 0.8188, 1.0000, 1.0000, 1.0000, 1.0000, 0.8848, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8677],
       device='cuda:0', dtype=torch.float16)
True tensor([[3929,  155,  874,  103,   25,  154,   25,  135,    0,   84,   63, 1230,
            0,   91,   26,  850, 6566,  131,    7, 2932,    8,    7,  218, 1498,
         5804,   10,  170,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,    7,  467,    0,    7,  453, 6578,  148,   34,    7, 1951,   12,
           13, 1318,    8, 5555,   55,   24, 6868,    7, 1406,    0, 2082,    8,
          858, 4007,    6,   10,  367, 4404,    8,   51,   13,  461,   12,    7,
          546,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,   21,   11,    6, 3849,    0,   21, 3560,    6,    0, 1259, 1599,
            0,    0,   21, 2293,    6, 1890,   13,  525,  287, 7195,    6,    8,
         1890, 3577,  901, 7195,    6,    0,  100,    0,   56, 2175, 1488,   35,
          868,    6,    0,  166,   26,  133,  341,  106,    0, 1120,  419,  218,
         2475,   12, 6293,    0,    2,    1,    1,    1,    1],
        [  21,  204, 1847,  214,   24,  169, 2807, 3301,   46,  214,  100,  150,
           48, 2184,    6,  156,    6,  109, 4377,   93, 2058,  424,    6,    6,
           46,    8,   73, 2575,  134,  199,   13, 4458,   18,  160, 1032, 1116,
          111,   45,   37,    0,  166,   25,   73,  774,  199, 1120,  419, 1862,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7, 1384, 1224,  368,    6,   80,  423,  759, 9162,    6,   13,
          383,    0,  166,   26,   80, 1450,  439,   12,   77,    7, 1947,  619,
         5948,    9,    7,  179,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1291,   22,  140,  416,   22,  110,  556, 1949,   11,    6,   13,  391,
           35,  511,   35,  290,   35, 6979,   35,  715,  234, 1927, 3156,   12,
         1954, 6935,    9,    7,   43,   59,  371, 1644,  827, 1151, 3144,    6,
            0,   85,   80,    3, 5494, 1360, 5997,    0,    8,   19,   11,  121,
          226, 1797,   54,   84,   55,  294,  215,    0,    2],
        [  94,  506,  204,  498, 1557,   54,  126, 8282,    6,    0,  934,    0,
            0,  109,    0,   53,  506,   86,   66,    0,   13, 4561,  583,   71,
         1230,   94,   10,    0,  456,   80,   13, 6711,  120,   53,  220,  116,
           87,   13,    0, 4454, 3836,    8,    0,  175,   21,  691,    0,   71,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   7,  853,   22,  335,   26,   17,   25,   73,  446, 2533, 5203,  336,
           21,    0,   68,  194,   65,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   19,   11,  121,  116, 3498,   25,   63, 3302,   12,    7,  133,
          946,    8, 9743, 1081,   17, 5577,    6,    8,  896,  227, 2174,  232,
          445, 6442, 1052,   62,   48,  562,    6,    8, 1122, 1079,  292, 3751,
            6,   73, 9478, 2808,   55,  159,  447, 3965,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  274, 6269,    8, 6269,  199,    7, 1710,   12, 5554,    0,    8,
            9,   17,  207,   24,  175,  914, 4616,    8, 4616,   10,   33, 2570,
         2240,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  218, 2604,    6,   12,  813,    0,   79,   25,   11,   57,  211,
         6449,    0,    0,    0, 4837,   63, 2875,   80,  860, 4825,    6,    0,
          106, 3836, 4506,    6,    0,    0, 1817, 4506,    6,    0, 1817,  837,
         5002,    8,   29, 4404,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,   77,   87,   33,   13,  277, 4838,    0,  166,    0,   26,
          347,   24,    0,   73,    0,   77,  274,  132,   85,    7,  370, 1303,
            0, 3253,    8,  150,    0,   33,    8,  113,   33,    8,  113,   33,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 225, 1639,   85,    7, 1749,  747,  670,   79,   13, 2829, 1020,    9,
            7,  384,  221,   11,    6, 2100,    0,   29,  225,  278,   10,  150,
           77,    7,  807,   17,  278, 3172,   22,  126,   12, 2092,    0,   55,
         1976, 1043,    0,  148,  162, 4111,   10,   51, 7806,   48,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 116,  336,   17,  183,  206,   19,   34,  961,   17, 7374,    0,   19,
         1260, 1004,    7, 4895,   12, 5205,    8, 4776,   84,   34,  486,  546,
           24,  162, 5054,    0,    7,   82,    9, 7723,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   34, 3642,   84,    0,  116,   13,  110,   57, 1802,   35, 1933,
           35, 1178,    0,    8,   19,  934,   48,   13,  422,  378,    9,   13,
         2414, 2196,    0,  125,   33, 5353,  220,   86,  601, 1584,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   45, 2464, 1778,    0,   68,  194,   65,  211,    0,   53,
          162,  172,  488,  184, 2148,    6,    0,    8,   53,  465,   11,   18,
          135,  261,   80,  998,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:4') tensor([29, 39, 53, 50, 30, 57, 50, 18, 47, 27, 42, 38, 48, 34, 36, 30],
       device='cuda:4') tensor([1.0000, 1.0000, 0.8652, 1.0000, 1.0000, 1.0000, 0.8394, 1.0000, 1.0000,
        1.0000, 0.8887, 0.8433, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
True tensor([[  24,  192,   11,   18,  296,   10, 2226,   35, 4562,   77,    7, 2422,
            6,   10,  204,  175,  143, 3254,    8, 6534,    8,   66,  143, 2340,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [ 115,  168,    0,  108, 1711, 1402,    6,   63,  417,  827,  603,   59,
          829,  518,    7, 2085,    0,    8,   79,   21, 1005,    6,  893, 3139,
          249,    0,   85,  185,  613,    7, 1711,  943,   26,  142,   10,  110,
          543,  346,    0,  168,   21, 1143,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1],
        [2275,    0,   29,   19,   11,   48,  100,   10,  456,   13,  277,  523,
           80,    7,   94,  148,  229,    7,  214,   24,  368,  333,  383,    0,
          108,  227,  715,   96,    0,  108,  874,  174,  652,    6,    0,  108,
         2930,    8, 1603, 1583,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [3907,    7,  383,    0,   21,   11,    6,  923, 1738,   10,  367, 1004,
           13,  822, 4657, 1520,  561,    9,    7, 4975,    0,  116,  100,   33,
         8283, 4657,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  25,  135,    0,   24,  205, 2027,    0,    8,   46,   19,  446,   17,
           19,  225,   48, 3918, 1118,   13,  325,   71,   89, 9415, 1431,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [ 115,   10,   51, 2226,    0,   19,   11,   45,   13,    0, 5577,    8,
           86,    0,   13,   56, 7525,  365,    0,    0,    0,    0,   29,   70,
           19,   11,   48,  100,   10,   87, 6160,  111,   26, 3765,   13, 1153,
           12,   70,    0,    0,    7, 2082,    8,    7,  858,   63,  142,   10,
          274,  100,    0,    2],
        [ 554,    9, 5855,    0,   24,  150,   13, 1780,   17, 1119,   17, 3402,
         2536,    6, 3078,   55,  244,  854,   12,   77, 2767, 1661, 1838, 1428,
            9, 1067, 1803,  794, 5110,  266, 4894,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  24,   11,  121, 1828,  251,  452,  551,  140, 2130, 1390,   35, 2383,
           20, 7260, 2421,    6,    0,    8,   24,   11,  121,  691,  185, 1868,
           18, 1568, 5128,  369,    0,    8,  180,   24,   73,  875,   21,  199,
          391,   35, 3197,  282,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [4580, 1057,   13, 1221, 6510,  445,   17,   26,    7,   56,   15,  586,
           93,   12,    7,  179,    0,  143,  254,  854,   12,    7, 1793, 1136,
          192,   11,   18,  703,    9, 2569,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  24, 2527,   10, 7864, 3168,   20, 5244,    6,   79,  877,  340,  111,
          906, 1386,    6,    0, 1244,   35,   48,  608,  668,  140,   18,  366,
          596,    0, 4821,   48,  324,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  29,   24,   11,   57,  529,   10,  175,   13, 2964,   10, 3657,   13,
          384, 1838,  436, 3487,  100,   13,  227,  234,  122, 1568, 1319,  455,
            6,  156,    0,   85,  392,  825,    7, 3049,   12,   13,  422,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  29,   13, 5083,  106, 2469,   73,  172,  988,  125,   21,   73, 9095,
          108, 4774,   55, 1244,   35,   57,  181,  237, 6379,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   9, 6764, 2076,  140,  741,  266, 2599,    0,   24, 1425,   17,  103,
           25,  144, 6764,    6,  415,  158, 1515,    0,   21,  442,  841,    0,
           53,  169, 9304,  199,   91,  486,    0,   25,  169, 7737,   15,    7,
          775,  335,   18,    0,   25,   11,   48,  132,  297,  527,   21,    0,
            2,    1,    1,    1],
        [ 103,   25, 5039,   33, 1523, 3920,  366,  199, 4100, 1319,    6,  307,
            6,  206,   13, 4100, 1319,    6,  307,  204, 1452,    6, 2414, 4825,
            6,    0,  180,   84,   11,    6,   13, 2333,   12, 2786,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   8,   19,   11,   45,  113,  752,   10,   66,  185,  110,  128,    6,
            0,    8,   66,  185,  227,   22, 1785,    6,    0,    8,    0,   25,
          135,    0,    8, 1995, 1243,   85,   89,  708,    0,    8,   87,   77,
            7, 1417,  214,   17, 1005,   25, 2059,   62,    0,    2,    1,    1,
            1,    1,    1,    1],
        [ 154,   12, 2239, 1827,   79, 7753, 1738, 2059,    8,    0,   25,   66,
            7, 8968,   55, 1520,   33,  479,   10,    0,   13,  733,  264,  561,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1]], device='cuda:6') tensor([26, 44, 43, 28, 25, 52, 33, 42, 32, 32, 37, 23, 49, 36, 46, 26],
       device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8325, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8945],
       device='cuda:6', dtype=torch.float16)
True tensor([[2434,  835, 4388,  291,  699, 1305,   13, 6013,  768,  545,   12,  170,
         9785,   96,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 192,   11,   18,    0,  205,    9,    0,   84,    0,    0, 2537,  194,
           65,    8,   12,  538,    0,    7,  775, 1327, 1158, 1691,   34,    9,
            7,    0, 7123,    6,    0,    2,    1,    1,    1,    1,    1],
        [ 103,   19,   87,   91, 3212, 4112,  954, 5951,  131,  486,    0,    7,
         6600,   26,   13, 1575, 3212, 4112,  954,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 339,   11,    6, 2238,   69,    7, 2469,    6,  629,  108, 1431,    8,
          108, 1037,    8,  108,  415, 3455,  373,    8,  108, 3532, 1649,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,  689,   11,   18,  988, 1179,   25,   11,   57,  961,   13, 1797,
         1339, 1337,   45,    0,  109,   25,   11,   57,   39, 5410,    0,  109,
           25,   66,   13, 2591, 1625,   69,   29,  609, 2992,    0,    2],
        [   8,   79,   21, 1518,  126,    0, 3577,   26,   91,   12,    7,  772,
         2604,    6,   12, 5488, 5290, 1247,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 159,    0,  697, 2158,    6,   66,   13,  277,    9,    6,  236,   22,
          407,    0,   17, 1119,   38,  335, 6577,  197,    0,   69,  134,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   33,  417,  244,  168,   26,    7, 1244,   35, 2287, 1236,    6,
           62,  417,    0,   24, 1149,  175, 1244,   35, 2287, 1236,    6,   62,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57,  914,  826,   89, 4023, 2020,   17,  383,   34,    7,
         3395,    0,  109,  893,    9,    7, 1009,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    7,  245, 2696,   24,   11,   57,  142,   10, 2565,   26,  552,
         1252, 2102,  742, 2304,    9, 2503,  699,    0,   71,  109, 1409,  387,
          221,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  25,  192, 1631,  830, 6318,    9, 2409,    0,   25,  192, 1631,  830,
          456,   46,   25, 1631,  203,  142,   10,   66,   10,   87,  251,  214,
            0,   38,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,  985,    6,  108, 1259,    6,    0,    8, 1073,  170, 2252,   24,
           63,   13,  461,   12, 1377,    8,   24,   11,   57,   86, 4029,  106,
           21,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   24,   66,  890,   12,  134,    0,   24,   11,   57,  142,   10,
           66,   13, 4369,   17,   11,    6,   86, 2539, 7985,   54,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  77,   25,   66,   10,   87,   26, 3150,  111, 3768,    7,  245,  717,
         1629,  242,   18,  249, 1336,  608,    6,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53, 2153,  117,  453, 3891,  266, 2644, 1638,    6,  434, 2262,    0,
            8, 1780,    6,    0,    8, 1868,   35,   62, 5100,    6,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [3674, 5929,   22, 4173, 1143,  126, 3298,    7, 2688,    0,  456,    6,
           10, 2991,    0, 4855,    6,    7, 1636,    0,    8,  388,    6,   21,
            9,    7, 6297,    0,    2,    1,    1,    1,    1,    1,    1],
        [   7,  744,   12,  117,  391,  572, 1794,   26, 7524,  570,    0,   17,
         1362,  271,    0,  403,    6, 3211,   12, 1666,  570,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  339,   11,    6, 2565,  853, 1993,   93, 2101,  119, 1898,  338,
            0,  148, 1429,    9,    7, 7312,   12,  728,  649,   56, 2586,  684,
           85, 6579, 2811,    0,    2,    1,    1,    1,    1,    1,    1],
        [  17,   11,    6, 1155,    0, 5653,   10,   77,   12,    7,  218,  214,
           24,  659,   51,  752,   10,   87,   80,  941,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7,  473, 5407,   26,   70,   26, 2134,   79,  521, 2287,  569,
           54,    0,  125,   21,  521, 2287,  569,    6,  108, 2122,   10, 1531,
          126,   70,   11,    6,  142,   69,    0,    2,    1,    1,    1],
        [5894, 4877,    6,    9,  185,   12,    7, 1714,  608,    8,  281, 5309,
         1427,   69,  984,    0,    8,   84,   11,    6,   13, 1043,   55,   17,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   91,   12,    7,  214,   17,   19, 5016,   85,    7, 3212, 3008,
           34,   33,    0, 1579,   22,  549,   11,    6, 6954, 3212, 2179,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  641,  736,  439, 1723, 1882,   26,  976, 7069,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   25,  135,    0,   24,  321,   12,   13,   48, 1326,   57,   94,
          148,   87,   33,   85,   19,  262,  287, 3296,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:1') tensor([16, 30, 21, 25, 35, 20, 25, 26, 21, 28, 27, 27, 24, 21, 24, 29, 23, 29,
        22, 32, 26, 25, 11, 22], device='cuda:1') tensor([1.0000, 0.8311, 1.0000, 1.0000, 1.0000, 1.0000, 0.8350, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:1',
       dtype=torch.float16)
True tensor([[  24,  213,    0,   10,  135,    0,  138, 2676,    0,  109, 6756,  138,
         2676,    0,  248,  341, 7052,    6,   63,    0,    2,    1,    1,    1,
            1,    1,    1,    1],
        [   8, 3097,    0,   19, 3699,  132,   85,   13,  670,  206,   84,   34,
           13, 4225,   18, 1049,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   0,   33, 5562, 2191,    7,  574,   20,  556,   93, 3667,   12,  126,
           35, 3149,   35,  200,   62, 2044,    0,    0, 4460,    6, 1313,    7,
         5870,    6,    0,    2],
        [  21,   11,    6, 3754,    0,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  19,   87,  250,  115,   19,   11,  121,  492,    0,  700,  691,   69,
           13,   38,   48,  551,  901, 3254,  197, 4502,    0,  700,    0,    2,
            1,    1,    1,    1],
        [   0,   21,   11,    6,    0,   13,  172, 4766,    0, 1762,   12, 1752,
          352, 4041,  813,   17,   11,    6, 4477,   48,   71,    7, 1893,    0,
            2,    1,    1,    1],
        [   8,   24,  487,   10, 2153, 3236,  166,  220, 3249,    7,  660,  100,
           33,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  19,  213,   10, 1452,   91,  546,   80,   39, 2462,  434,  903,  763,
           19,   59, 2687,  271,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [ 116,   79,   19,   34, 5833,    0,   10,  367,  168, 7688,    0,   91,
           12,   89, 3636,    6,    0,    0,  487, 8065,   54,    0,    2,    1,
            1,    1,    1,    1],
        [   8,   25, 1220, 1346,   17,  908,  254,   91,  439,   12,    7, 1402,
           11,    6, 2103,   62,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  19,   11,   45,  142,   10,  456,   80,    7,  946, 2403,    9, 5247,
            9,    7, 2199,  119, 1910,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  67,  180,   19,  169,  205,   10,  747, 3061,    0,    8,  778,  188,
         1781,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  24,   11,  121, 2134,  138,   10, 6213, 5894, 1313,    7, 1839, 1101,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  19,  487,   71,   13, 5726,    0,   19,   11,  158,  467,   71,   91,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  25,  274,   85,    7, 2116,    0,   25,   73, 7619,    9,   77,    7,
         1206,    6,    0,   25,   73,  274,   85,  134,    0,   25,   73,  884,
          134,    0,    2,    1],
        [   7,  316,   22,   96,  168,  954, 2008, 1820,   85,   13, 2270,  430,
           12,  336, 5371, 5494,   13,  464,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  24,   63, 2188,  199,   13, 2179,   12,  183,    8,  672,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  17,   11,    6,  110, 1708,    7, 6162,    0,    0,   71,   89, 4197,
            8,   89,  772, 1751,    0,   85,    7,  183,    0,    2,    1,    1,
            1,    1,    1,    1],
        [4610,  518,  106,  101, 1374, 6481,    0,   33,   26,  914,  347,  596,
           63,  509,   85, 2541,   35,  455,    6, 1778,    0,    2,    1,    1,
            1,    1,    1,    1],
        [ 125,  432,   77,    0,   53,   63,  735, 9379,    8,   53,   63,  384,
         2653, 1256,  111, 6849,  541,   57,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   8,  180, 5408, 1370,    9,   77, 3061,    0, 2439,  106, 2811,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [1275,  427,  586,   54,  486,  800,   86,   10,   51, 6247,   26,  116,
           79, 3315,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   7, 3210,   10, 4110,   80,   26, 1179,  109,   86,   24,   11,   57,
          893,  108, 3718, 1452,   12, 3643,  687,    0,    2,    1,    1,    1,
            1,    1,    1,    1],
        [ 109,    0,  635,    7,  488, 5916,   26,   86,    7, 1913,   12,    0,
            7, 1479,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  17,   34, 1968,    0,   67,   21, 6686,  288,  423,  439,   12,    7,
          753,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  67,   53,  220,   51,  126,   84,    0,  133, 5029,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   7,  245,   91,    0,    0,    0, 3861,    8,   19,   66,  211,  948,
         5199,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [ 168,   11,    6,   70,  956,    0,  150,   17, 1375,   69,    7,  133,
         1219,   71,    7,  277, 6820,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   8,   53,    0,    9,  409,    0, 6686,    7,  370, 2528, 1569, 2792,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [1220,    0,   19,   11,  158,  508,   25,   39,  663,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  24,  296,   10,    0,  283, 1749,  111,    0,   67,   24,  113,    0,
          296,   10,  283, 1402,   35, 1373,    6,  160,   35,    0,    0,  200,
         1515,    0,    2,    1],
        [ 101,   11,    6,  204, 1520,  284,  447, 1153,    0, 3489,   71,   70,
           11,    6,  434,   13, 2421, 1420,  265,    0,    2,    1,    1,    1,
            1,    1,    1,    1]], device='cuda:3') tensor([21, 18, 28,  7, 24, 25, 15, 18, 23, 18, 19, 15, 15, 14, 27, 20, 12, 22,
        22, 20, 13, 16, 21, 16, 15, 11, 15, 19, 15, 11, 27, 21],
       device='cuda:3') tensor([0.8691, 1.0000, 0.8608, 0.8911, 1.0000, 0.8667, 1.0000, 1.0000, 0.8540,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8945,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8413, 1.0000, 1.0000, 0.8325,
        1.0000, 1.0000, 1.0000, 0.8105, 1.0000], device='cuda:3',
       dtype=torch.float16)
True tensor([[  21,   34,   13,  ...,    1,    1,    1],
        [  29,  103,   21,  ...,    1,    1,    1],
        [   8,   19,   11,  ...,    1,    1,    1],
        ...,
        [1953,  215,  581,  ...,    1,    1,    1],
        [  17,   11,    6,  ...,    1,    1,    1],
        [   8,    9,  409,  ...,    0,    0,    2]], device='cuda:3') tensor([10, 16, 12, 11, 12, 10, 13,  9, 13, 13, 12, 10,  9, 10,  9,  8, 12, 12,
         8,  8, 14,  9,  9, 11,  9, 16,  8, 11, 10, 14, 12,  9,  8,  6, 12, 11,
        12, 13,  8,  9,  8, 13, 13, 13, 10, 10, 11, 11,  9,  8,  9, 10,  8, 10,
        11, 11, 11, 10, 11, 10, 11,  9,  9, 11, 11,  8,  8, 11, 11,  8,  9, 10,
        10, 11, 10, 16, 14,  9, 11,  8, 10,  6, 10,  7, 11, 11, 11, 11, 12,  8,
         9, 11, 10, 13, 11, 21], device='cuda:3') tensor([[ 230,   85,    7,  467,   12,    7, 1421,    6,    0, 2645,  217,  174,
          236,  241,  429,   26,  142,  132,    0, 7003,   26,  321,   12,   69,
            7, 2503,  375,    0,  347,    0,    7, 2973, 3333, 2319, 1691, 3741,
            0, 2266,  181,  156,   26, 3489,    0,    7, 4186,  230,    6, 2080,
            0, 3753,   56,  338,  959,    6,    0,    7, 8713,   82,    0,  848,
         1999,   26, 3489,    0,  853, 1993,   93, 4293, 1251,   26, 3489,    0,
          563,  122,  436,    0,  274,   70, 1148,    0,    9, 5970,    0, 7003,
         1764,  502, 1604,    6,    0,    2],
        [1136,  672,    6,  735,   46,   33,   26,   21, 6609,   46, 1136,  672,
            6,  735,  296, 1051,  122,  233,  480, 8823,    6,    0,   86,  288,
           10, 5186,  134,   85,    7,  126,    6,  307,   55, 1136,  368,    0,
           67,   10,  694,  134,   55,    7,   94,   17,  368,  134,    0,  180,
           10, 6433,  134,   10, 8711,   17,   53,   63,   55, 1459,    0,   17,
           53,   63,   86, 1051,  416,  922,    0,    9,  827,   48,   62,    0,
           13, 2868,  399,   62,  109, 6265,   48,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,   53,  162,  417,  140, 2362,  816,    0,   24, 2693,  346,   10,
         3082,    0,    8,   21,   34,   29, 4949,    0,  125, 1241,  248, 1601,
            0,  225,  487,  203, 1635,   54,  110,    7,  546,   12,  267, 9241,
            8,  267, 4894,    8,  267, 1922,  626,    0,  276, 1445,    0,   79,
          267, 2346, 4030,  160,    0,   19, 1095,  267,  333, 1822,    0,    8,
           29,   19, 1425,   33,  546,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 211,  207,   10,  229,  841,   12,   13,  179,   17, 2220,   11,   18,
          339,   25, 2423, 3706,   10,  583,   39,  467,   10,    7,  207,   25,
          144,  367,    0,   10,  305,  126,  545, 3493,   25,  144, 2393,    8,
          688,  159,   19,  158,  502,  160,   62, 4209,    6,    0,    8,   10,
         1157,  134,   79,   53, 8638,   62,   69,    7, 2402, 4032,  688,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 103,    0,    7, 8561,   34,   80,   89,    0, 3289, 3046,    0, 3231,
           54,   89,  447, 9299,    0,  635,    7, 7646,  188,  226,    0,   80,
            0,   89,  126,   37,    0, 3046,    0, 8751,  126,  138,   19,   73,
            0,  368,   89,  916,    0,    0, 3557, 2573,   10,   51,   12,    0,
         3762,   10,    7,  179,    0,    8,   10,  305,  185,   12,  251,  214,
           17,   19,   11,  121,    0,    0, 1385,  126,   84,    0,    8, 4432,
          134,   10,    7, 2196,   17,  422, 4336,  115,  446,    6, 1584,    9,
            0,    2,    1,    1,    1,    1],
        [  24,  144,   13,  110,   45,  240,  457, 3040,    0, 1809,  525, 1144,
           26, 5207,   48,    0, 1586,    0,   55, 7099,   48,  596,  100,   89,
         3636,    8,   77,   12,  170,  168,  440,    0,   67,   86,   55,  281,
           12,  108, 4197,    6,    9,    7, 1590,   12,    7,  179,  148,   63,
          499, 6324,  199,  909, 2965, 1049, 5510,    0,  427, 3562,    0, 6324,
         4618,   46,   53,   66,  708,   17,   53,  192,   11,   18,  213,  109,
           53, 1985, 3018,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:2') tensor([90, 81, 67, 61, 86, 77], device='cuda:2') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8413, 1.0000], device='cuda:2',
       dtype=torch.float16)
True tensor([[   8,   19,  154,   24, 1831,  261, 4616,   17,  383,    0,   24, 1897,
           13,  325,   12, 6381,    6,   17,  465,   11,   18,  296,   10,   51,
           84,    0,    8,   24,   55, 2361,    7, 2695,   55,   70,  475, 1543,
            8, 4564,   26,   77,   80,    0,    2],
        [   8,  339,   11,    6,  970,   21,    0,    7, 6466, 3708,   54, 9135,
           66, 6358,   48,    0, 2985,    7,  913, 2340,   69,    7,  862,  160,
          156,    0,    8, 1094,   53,   66,   10,  468,    7,  207,   53, 5989,
            0,    2,    1,    1,    1,    1,    1],
        [ 476,   25,  133,    0,  261,    0,   68,  386,   65,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,   29,  117,   63,    7, 3402,  158, 2383,    0, 1816,    0,    7,
            0, 1620,   54, 6895, 6603,  603,  111,   85,    0,    7,  183,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  19,  154,    7,  281, 6149,   91,   17,   19,  552, 1004,   34,   13,
         1165, 2508,   12,   13, 1365,  148,  144,   39,  109, 3523,   45,  333,
          183,  225,  862,  335,  176,   62,  267, 7150,    0,   68,  194,   65,
            2,    1,    1,    1,    1,    1,    1],
        [   8,   29,  225, 1260,  199,    7, 4222,   12,   33,  830,    0,  203,
          505,    0,   91,   12,    7, 1911,  236,  176, 4222,    6,    0,    8,
          225,  204,  591,  185,  133,  916, 1710,    6,    9,   84,    0,    2,
            1,    1,    1,    1,    1,    1,    1],
        [ 238,    0,   19,  135,    7,  179,   26, 5352,  115,    0,   17,   11,
            6, 1301,    0,    8,    7, 1465,  188, 1914,   62,  333, 4209,   12,
            7,  179,    0,    7, 1714,  608,    0,    7, 5309,  119, 1427,    0,
            2,    1,    1,    1,    1,    1,    1],
        [ 115,  916,  111,    0,   84,   63,   13,  555, 6814,  589, 1369,    9,
          166, 3726,    6,   66, 5018, 1986, 1181,  184,   22,  235,  128,  407,
           17,  689,   11,   18, 2156,   55, 4224,  415,   37,  140,  369,   10,
          305,  561,    0,    2,    1,    1,    1]], device='cuda:2') tensor([43, 38, 10, 25, 37, 36, 37, 40], device='cuda:2') tensor([1.0000, 1.0000, 0.8701, 0.8491, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:2', dtype=torch.float16)
True tensor([[  19,  144,   29,  261, 2084,  335,   18, 1991,    9,  110,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 185,   12,  134,   63,  133, 1897,   20,    0,  939, 3769,   22,  311,
           62,   21,  834,   68,  194,   65,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1720,  235,  287,  403, 2687,  556,  287,    0,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [2869,  464, 1065,    0,   19, 6223,   10, 3150,  111, 4214,   77,   12,
           89, 6909,    6,    8,  465,   11,   18,  172, 1516,  670,   71,  261,
           10,  575,   55,   85,   77,    0,    8,   89, 1848,    0,  635,   79,
           13, 5351,    0, 5016,  110,   70, 1932,  126,   10,   51,   13,   91,
           35, 1408, 5539,  307,   10, 5170,    0,    8,   19,  552,  270,  637,
           80,  717,  215, 1065,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,  135,   17,   25,   73,   51, 9649,    9,   13, 5185,    8,
           25,   73,   51, 9649,    9,   13, 5510,    0,   29,    7,  744,  488,
         3974,   17,   24, 1385,   26,   17,   21,   11,    6,   86,  116,    7,
          800,   12, 1431,   25,   66,    0,    8,   21,   11,    6,   86, 1179,
          109,   86,   25,   11,   57,    9,   13, 8064, 2549,    0,   67,   21,
           11,    6,    7, 3316,   12,  155, 1387, 4310,   17,  988,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,    9, 8338,    0,   91, 7313,    9,  227, 5672,   37,  713,    0,
           53, 3111,   77,  591, 1574,   84,    0,   68,  386,   65,    8,   53,
          162, 7000,  131,   21,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  180,    9,    7, 6086,    6,    0, 2402, 6086,    6,    0,    7,
           29,   35, 8309, 5085, 9063,  480,    6,  162, 2534,  131, 2290, 3029,
         1558, 5872,    0,    8,   21,  278,   29,   17,   25,  220,    9, 1050,
          430,   13, 3768,  521,  586,  300,    6,  369,    0,   39, 4312, 1068,
          265, 1054, 2445,  266,  521,  586,  300,    6,  369,   46,   25,  220,
          150,   21,   69,    7,  572, 3228,    6,   46,  929, 7993,  419,  521,
          586,  300,    6,  369,    9,    7,  955, 3706,   13,  277,  523,   12,
          830, 2068,   18,  311,   54,   12,    7,   10,   96,    0,    2],
        [   8,  125,  117,    0, 8572,    6,    0, 1085, 1719,   59, 5292, 3147,
          111,   79,   13, 2443,   12,  183,    0,   25,   73, 2575,  117, 4839,
           10, 5318,    6,   12,  183,    0,    0,  206,    7,  248, 2048,    0,
         7008,    0,    0,  164,    0, 1452,   13,    0, 1738,   39,  430,    6,
         1198,    0,    0,    0,   80,    0,  854,   13,  759,  215,    0,  581,
            0,    8,   71,    7, 6834,    6,    0,    0,   21,  164,   51,    9,
            7, 1296,   12,  785,  759,  215,    0,  581,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([12, 19, 10, 66, 73, 30, 95, 82], device='cuda:0') tensor([1.0000, 1.0000, 0.8701, 1.0000, 1.0000, 1.0000, 1.0000, 0.8140],
       device='cuda:0', dtype=torch.float16)
True tensor([[ 117, 3586,  214,  ...,    1,    1,    1],
        [   8,   70,   25,  ...,    1,    1,    1],
        [ 101,  692,   10,  ...,    1,    1,    1],
        ...,
        [2048,   66,    0,  ...,    1,    1,    1],
        [  25,  135,   17,  ...,    1,    1,    1],
        [ 248,   35,  511,  ...,    1,    1,    1]], device='cuda:0') tensor([13, 15, 19, 18, 13, 14, 13, 16, 15, 11, 12, 16, 17, 15, 15, 13, 11, 15,
         9, 10, 26, 22, 13, 14, 15, 10, 15, 16, 20, 19, 20, 16, 17,  9, 28,  8,
        17, 10, 13, 13, 21, 16, 12, 12, 15, 19, 14, 14, 20,  9, 18, 14, 21, 18,
        12, 11], device='cuda:0') tensor([0.8989, 1.0000, 1.0000, 1.0000, 1.0000, 0.8096, 1.0000, 1.0000, 0.8442,
        1.0000, 0.8857, 0.8101, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8662, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8955, 0.8120, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8906, 1.0000, 1.0000, 0.8740, 1.0000, 0.8350, 0.8379, 0.8408,
        1.0000, 1.0000], device='cuda:0', dtype=torch.float16)
True tensor([[1179,   21,   11,    6, 5974,  109, 1298,    0,   21, 1847,   17,  535,
           10,   51,  529,   10, 1772,   10,  468,  250,    9,   13,  207,   17,
           21,   11,    6,  509,  254,   70,   34,   84,  490,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,    0,   70,  506,   86,   66,    0, 7747,   10,   25,   26,    0,
           70,   63,    7,    0, 5508, 2459,    6,  172,  100,   12,  740,   69,
            0,    0,    0,    0,  486,  943,   46,    0,   12, 1074,   69,  248,
          943,    6,  120,   84,   63,   94,   69,    7,    0,  984,    8,    0,
           84,   63, 1273, 1305,  109,   94,   69, 2961,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 238,  432,   13,  555,  215,    0,    0,   19, 1446,   10,  205,   10,
            7,  832,    0,    6,    0,    8,  415,  675,   45,  174,  407, 2811,
           10,    0, 3872,    6, 1317,    0,   89, 1850, 4804,  241,  436, 3719,
            0,    8,  180,  487,    0,   89,  447, 1827, 1484,    0,  166,   34,
          291,  322, 4399,   18,    0,   12,   56, 1628,    7,    0,    0,  215,
           17,   19,  487,   89, 3557,    0,    2],
        [ 103,   19, 5164,   69,   21,   25,  450,  110,  103,   25, 1510,  995,
            0,   68,  174, 3007,   54,   65,   68,  779,  569, 2317,   65,  339,
          110,  116, 2677,   33,   71,   13,  277, 2705,  168,    0,   68,  679,
         1638,   65,   17,   34,    7, 7418, 2018, 1069, 1882,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,  172,    7, 3900,   35, 5444,    0, 7887,   22, 1011,
         1370,  567,    9, 1662,   17,   26, 5555,   55, 6355,   77, 4564,    0,
           38,  469,   93,  246,    0, 1250,    0,  103,   25,  213,   10,  150,
          475, 4564,    0,  205,   10, 8424, 6068,    0,    8,  274,   85, 1881,
          100, 2991,    0, 8564,    0,    9, 1795,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  19, 3013,  101,  220,   87,   33,   71,   13,  203, 5521,    0,   13,
         2771, 1056, 3479,   12,  284, 1227,    0,   38,  469, 2403,  164, 1017,
           25, 1363,    3,   17,   11,    6,   70,   21, 1119,    9,    7,  697,
         2158,    0,    8,   21,   11,    6,  250,   17,   19,  213,   10,  618,
          131,    0,  593,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  13,  122,    0,  294,   12,   25,  506,   86, 2252,    8,  703,   17,
           24,   66, 3090,   33, 2913, 1670,  111,   46,   70,   19,  583,   13,
         1103,  584,  122, 1478,    0, 6012,  292,  959,    6,   10,  913,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   0,   67,   25,  135,  138, 1662,  188,    0,   13,    0,    0,  325,
           12,  316,  119,    9,    0,    7, 4975,    0,    8,    7,  143,  316,
          119,   25,  169,   66,  142,  132,    9,    0,    7, 1404,    0,   69,
            7, 1950, 1780,   25,   73, 1120,  150,    0,   67,   84,    0,   26,
            0,   33, 3020,   93,  461,  100,  120,   25, 7490,   13, 3020,   37,
            0,    2,    1,    1,    1,    1,    1]], device='cuda:6') tensor([35, 58, 67, 47, 57, 53, 37, 62], device='cuda:6') tensor([1.0000, 0.8066, 0.8867, 1.0000, 1.0000, 1.0000, 1.0000, 0.8574],
       device='cuda:6', dtype=torch.float16)
True tensor([[  67,  939,  821,  684,  834,    0, 1586,    0,   19,  842, 6791,   55,
          391,  215,    0,  939,  821,  684,  834,    0,    8,   21,   34,  923,
            7, 1064,  125,   19,   34,    7,  288, 1470, 2088,    9,    7, 2092,
            0,  276,    9,    7,  291,  670,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  29,  120,   19,  175,  129,  603, 1852,   18,   80,    7, 1503,   12,
         9550,    0,  276, 1445,  117,   63,  976, 3315,  825,  125,   24,   11,
          121,  278,    7, 3515,    8,  110, 1533, 1151, 1492, 1485, 4445, 8933,
           54,    7, 6879,    0,  166,   26,  453,    0,   24,   11,  121,  278,
          886,   18,   18,  255, 1315,  142,   69, 9091, 8256,    0,  166,   26,
          453,   55, 3649,    0,  133,  999,   55,  284,  415, 2633,    0,    2,
            1,    1,    1,    1,    1],
        [ 225,  113, 1119,  214,  100, 2034, 1771,   12, 2072, 1849, 1516,    6,
            0,   53, 4198, 3222,  614,  292, 4494,  158,    8,  172, 4834,  436,
          155, 2011,    0,    8, 2881,  148,   11,    6,  691,  670, 3804, 1008,
            6,   17, 3222,  614,  292, 4494,  158,    8, 3222,  614, 2566, 3322,
            6,  288,  229, 4834,    9, 7616,    0,    8,   21,   11,    6,  923,
         2072,    9,  155,  853,  200,  356,    6,  432,   25,   11,  121, 2034,
           15, 3954, 1040,    0,    2],
        [  91,  723,   19, 2275, 1314, 1831,   17,  125,  101, 7807,   62,   39,
         1362, 1547, 2811,    0,   84,  162, 1905, 7310, 2261,   17,  162, 9341,
          565,    0,  100, 2076,  362,  240, 3287,  111,  401,   81, 1522, 4618,
           10,  509,  661,   39, 5317,    0, 3097,    0,  101, 1336,  235,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  53,   66,  226, 5653,   10, 4623,  760,  387,    6,   46,  117, 1362,
          345, 7863, 1215,  148, 7615, 4690,  985, 1402,    6,    9, 4431,   35,
         1898,  158,   62,   73,  287,   96, 1655,   12, 2602,  106,   13,  822,
         3485,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8,  923, 7714,    0,   19,  154,  103,  415, 2633,  356, 3839,  373,
           34,   10,   51, 7989,  138,  101, 1455,   62,   21,    0,  101,  220,
          204, 9876,   39, 2583, 1472,    0,   68,  194,   65,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 103,   25,  162,    9,   13, 2406,    0,  101,  246,    0,   21,   34,
          142,   10,   87,   25,  324,  288,  125,   21, 2797,   62,   25,  185,
           82,   45,  322,    0,  185,  896,    0, 8257,    0,    8,  635,    7,
          595,   54, 2303,   12,   13,  149,    6,   20,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8,   79,   13, 2173,    0,  120,   24,  289,    0,   38, 5276, 1382,
           11,   18,  572, 4894,    3,   17, 1703,    6, 3060, 3127,   20,   10,
          138, 1623,   21,   34,   55, 2307,   10, 1082,  106,  159, 4113,    6,
            9,   13, 1726,   12, 3023,   17,   34, 6320,   29, 3634, 3392, 4068,
            9,   21,    6,  427, 2515,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:6') tensor([44, 72, 77, 49, 39, 34, 46, 56], device='cuda:6') tensor([[   8,    0,   33, 4515,    0,    0,  923, 8121,    0,   26,  434,    7,
         1720,   18,  407,  371,    0, 1107,    0,    8,   19,    0,  513, 7615,
           54,   69,   21,    0,    0,    8,   24,    0,  323, 5673,    6, 3907,
            7, 6687, 2008, 1777, 1580,    8, 2603,    7,  566,  827, 1580,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  13, 2406,   13,   48, 2921, 5951,    0,    7,  245,   12,  294,    0,
           13, 9241,   12, 8716,  407,  552,  558,    0,    8,  180,    0, 4615,
           12,   77,    0,   13, 7377,    0,   10,   59,  445,   54,  841,   12,
         1080,  982,  687,    0, 3349,  233,  187,  271,    8, 9653,   80, 1222,
            8,   89,  427, 2515,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 830,    6,    0,   19,   66, 2154,  122, 1317, 7471,   12,    7,  558,
          383,    0,    7,  432, 4608,   12, 4058,   54,    0,   13, 1905, 2178,
         3007,  687,   17,   19, 2096,   10,  574,  597,  232,    0, 1155,  143,
            0,   67,   19,  465,   11,   18,  575,  132,   85, 1911, 1124,  340,
           11,    6, 2657,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 115,    0,  339,  110,  456,   13,  277,  523,   80,  250,   17, 1148,
          276,    9,    7,  641, 2371,    0,    8,   17,   26,    0, 1017,  187,
            0,   19,  154,    0,   26,  528,    0,  125,   21,   11,    6, 7257,
            0,    8,   21,   11,    6,   86,  288, 7257,    0,   21,   11,    6,
         1661,   57,  969,    6, 2158, 7257,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [8369,    0,   24,   11,   57,    0,    0,   86,    0, 1557,   54, 2048,
            0,  132,   85,    7,    0,    0,  765,    0,  238,   85, 1686,   71,
          108,    0,  447, 5900, 4344,    6,    0,    0,   67, 7002,   26, 1019,
            0,    0,  106, 2429,    0,    8,   91,   12,    7, 2826,  347,   24,
         2153,   13, 1810,  100,   33,   26,   29,   17,   94,    0, 2252,   17,
           84,   11,    6,   29,  294,  218,  214,   17,   24,   11,   57,  401,
            0,    2],
        [  21,   11,    6, 3310,    0,   29,   70, 1148,    0,  180,    0,  120,
           33, 5353, 7076,  244,    7, 8126,   12, 6817,  199, 3612, 1491,   55,
            7, 6852,    8,  180,    7, 3571, 3195,    6,  336,    8,   21, 7076,
          270,    0,    8,   21,  925,  270,   10,   33,    0,   13, 5966,  111,
           29,   48,   48,   62,  205, 1591,  538,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  24,  144,   94, 7743,    8, 2532,    8, 3793,   54,   79,   53, 2096,
           10, 8491,    7, 9236,    6,    9,  341, 1081,    0,    8,   85,   13,
         1905,  613,   53,  169,  321,   12, 1233,    8, 1008,   17,   53,  162,
            9,   39, 7996,   80, 1723,    0,    8,   17,  635,   17,   11,    6,
           86,  138,   25,   11,   57, 3263,   10, 1529,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,   21,   11,    6,   86,  116,  125,   12,  108,   30,  176,   54,
         1392, 1611,    0, 1629,  187,  240, 2469,  128, 4159,    8, 8871, 9828,
         3885,    0,   21,   11,    6,  113,  125,   24,   11,  121,  204,  691,
           39, 6432,  325,   12,  283,  199,    7, 3603,    6,   12,  138,   10,
          446,    7, 2636, 3883,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:4') tensor([49, 55, 53, 56, 74, 57, 58, 54], device='cuda:4') tensor([0.8691, 1.0000, 1.0000, 1.0000, 0.8174, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
True tensor([[  19,   11,  121, 3556, 1222,   10,  987,  200,    0, 3673,    0, 2828,
         1764,  502,  174,   54,    0,  991, 8517,    0,   19,   73,  276, 3861,
         1222,   69,    7,  270,  120, 5763,    0,   77,   29,   19,  192,   11,
           18,   66,   10,  884, 2307,   55,  995,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 103, 2881,    0,  188,  700,  226,    0, 5307,  890,   10,   51,    0,
            9, 5170,    0,   25,   11,  121,    0, 1110,  134, 1028,  126,    0,
           12,    7,  853,   18,  221,  266, 5291,    6,    9,  227,   93,   48,
         2739,    0,    8,  103,   25,  116,  274,   85,  159,  970,    0,   25,
           73,  150,   53,   66,  261,    0,    0,  261, 3847, 1893,    8,  261,
         4100, 4896,    6,    0,    2],
        [  19,   11,  158, 2117,  722,   13, 1706,  461,   12,    7, 1404,    0,
            8,  180,   19,   11,   45,  142,   10,  175,  199,  321,   12,   13,
         3310,  269,   20,   17,   26,  133, 1623,   10,  722,  120,   25,   11,
           57,   86,   82, 3080,  132,    0,   29,    0,  103,   19, 4270,   21,
          132,    0, 8090,   25,  100,   21, 3296,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 238,    0,   19,   11,   45,   86,  276,  142,   10, 3536,    7, 3761,
         9260, 3140,    6,   12,   94,  100,    7, 2562, 1534,  266, 5694,    0,
          148,  154, 3524,   17,  103,   25,  508,  126,  521,  870,   45,    6,
            0, 1459,   11,    6,  142,   10, 1202,  126,    8,   66, 2314,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  84,   34,    7, 5556,   59,  365,    0,    8,   19,   73,  661,   17,
            0, 1042,  554,    0,   24,   11,   57, 2985, 4213,   69,    7,   69,
           18, 2992,   12, 1298,   79,   24,  135,   21, 7957,  111,    0,   67,
          339,   11,    6,  274,   85,   91,  473, 1472,  440,   19,   11,   45,
          142,   10, 1452,   71,   25,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  89, 2780,   34,   13, 5931,   12, 6161, 3805,   96,   17,  162, 5058,
           62,   10, 2326,  111,   13,  562,   18,    7,  170,   37,   10, 7650,
         2317,    9,    7, 3550,    0,  490,    7,  422, 4896,  220, 2213,   21,
          132,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  13, 1435,   19,  452, 1547,    0,    9,   33,  183, 4868, 3258,   54,
            9,    0,    7, 1227,   38,  469, 1487, 4200, 2958,    3,    0,    0,
            0,  591,    0, 1992,  439,   12, 1970,    6,    8, 5411,  439,   12,
         1276,    6,    0, 9236,    0,   13, 5559, 1068,   55,   85, 1686,   91,
         1269,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 103, 1777, 2923,    6, 4226,  941, 3069,    8, 4226, 4774,   10, 1799,
           71,   21,    6, 1195,   12, 2718, 6927,    0,   17,   11,    6,  324,
           55,  170,   79,  238,   79,  324,   55, 1777,   79,  238,   79,  324,
           55, 1308,  994,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:4') tensor([45, 65, 57, 49, 55, 39, 51, 41], device='cuda:4') tensor([1.0000, 0.8926, 1.0000, 1.0000, 1.0000, 1.0000, 0.8066, 1.0000],
       device='cuda:4', dtype=torch.float16)
True tensor([[  67,  168,   11,    6,    7,  279,    0,    7,  660,   26, 3243,   17,
            7,  853,  233,   54, 3421, 1620,    6, 5709,   12, 2108,  237, 3760,
         1144,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 276,    9,   89, 1492,  158,   37,   93, 2191,    0,   19,  780,    8,
          203, 2329,  235,  284, 1198,  266, 2792,    6,  100, 1031,  174,  338,
          886,    6, 4461,   48,    0,  838, 6877,  288,   21,    6, 2294,  203,
            6,  494, 1317,    8, 1683,   89,  447,  282,    0,    2,    1,    1,
            1,    1,    1,    1,    1],
        [6902,    0,   70,   25,  150,    9, 1849,   26,   86, 4987, 7489,    0,
            8,   70,   25,  150,    9, 5994,   26,    0,    8,  117,   63,  131,
         1019,    7, 4023, 2384,   12,    7,  984,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8,   19, 1873,   13,  535,  183,  581,   17,  103,   19, 2020,   94,
           10,  991,    7, 6629,   53,  296,  126,   12,    7, 9057,   19,  446,
            0,   21,   11,    6,   13,  453,  207,   10, 1974,  532, 4564,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   7,  733, 1478,   12,  378,  427, 2553,    0,    8, 1492,   45, 1755,
           26,  735,   80, 2313, 3781,    0,   55, 2313,    0,   94,    0,   17,
           11,    6, 1565,   93,    0,    0,    7, 2426,    0,    0,    0,  279,
           10,   87,  115,   26,   10,   51,    0,   85,    7, 2084,   54,   96,
            0,   51, 4673,    0,    2],
        [ 103,   25, 1897,    0,   71,   13, 8419, 9789,    0,   91,   12,    7,
         5556,   57,   18,    6,   12,   13,  682,  300,  597, 3007,   37,    8,
          274,   85,   21, 4029,  111,    0,   25,  154,   12,   13,  733,  682,
          300,  597, 3007,   37,    0,   67, 4100,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   0,   70,  956,    0,   17,   26,    0,    0,   69,    7, 1303,    9,
            7,  464, 1581,  820, 1047,  120, 1720, 2689,  424,   48, 5692,    7,
          245,  203,  121,  237,  271,   12,    7,    0,    0, 1097, 1486,   69,
           13, 3144,    0,  116, 1663,  110,  140, 1027,    0,    2,    1,    1,
            1,    1,    1,    1,    1],
        [   8,  172,   87,  508,  134,   13, 2184,  122,    0,  476,   25,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:7') tensor([27, 46, 33, 37, 53, 45, 46, 13], device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8262, 1.0000, 0.8853, 1.0000],
       device='cuda:7', dtype=torch.float16)
True tensor([[   8,   19,   11,  ...,    1,    1,    1],
        [   9, 4768,   19,  ...,    1,    1,    1],
        [   8,  432,   17,  ...,    1,    1,    1],
        ...,
        [  91,  473,  663,  ...,    1,    1,    1],
        [  85,    7,  370,  ...,    1,    1,    1],
        [   8, 1179,   53,  ...,    1,    1,    1]], device='cuda:7') tensor([38, 18, 31, 54, 37, 29, 26, 36, 30, 65, 34, 34, 44, 26, 32, 31],
       device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 0.8486, 1.0000, 1.0000, 1.0000, 0.8276, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8413, 1.0000],
       device='cuda:7', dtype=torch.float16)
True tensor([[  19, 1008,   19,  842,   13, 4082,   10, 5170,    0,  513,   10,   39,
         3485,  434,   91, 3601, 3485,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   19,  116,  213,   10,  476,    7,   13,   59, 1050,  160,  287,
         1237,   55,  378,    7,  772,    0,    8,  116,  333,  383,  961, 1771,
           12, 3357,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19, 4600,  134,   77,    0,  276,  103,   53,  465,   11,   18,  735,
          661,  545,  218,    0,   53,  162,   77,   89,   94,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,  103, 5706, 1382,   11,   18, 5020, 4514,    0,    0,  154,   12,
         2314,    0,   68,  194,    0,   65,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  33,  558, 2420,    0,    0,   26,  914,    7, 1339,  265,  443,    0,
         3360,   12, 9302, 4290,  998,    0, 1319, 1158,  176, 2135,  265,  455,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 238,    0,    7, 1187, 1031, 1957,    0,  689,   11,   18,    0,  204,
          641,    0,  995,   10,   25,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  67,   19,  323,   55,    6, 2704,  134,    8,   19,  465,   11,   18,
            0,  985,   17, 5127,  140, 2614,    0,  554, 1325,   19,    0,   34,
          270,  637,   71,   89, 1037,   85,    7,  467,   12,    7, 4267,    0,
            2],
        [  19,  135,    0,  230,    0, 1273, 2280,   11,    6, 1037, 6466,    6,
          862,  589,  760,   71, 4867, 1273, 2280,   10, 1082, 2781,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 180,    0,  106, 1137, 1230,   10, 1450,    0,   89, 9339,    6, 2694,
          366,  111,  384,  249,  187,  240,  922,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 168,   11,    6,   89, 3397,    0,  568,   21,  283,    0,   26,   21,
          419,  324,    0,   63,    7,  909,   45, 1638,    6,   82, 1486, 1011,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  25,   11,   57, 5869,   12, 5655, 3382,   54, 1466, 5412,    6, 4820,
         1032,  111,  120,   25,   73,  150,  134,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,   24,   66,   13, 6897,    0,    0,   12,    7, 7179,    6,   17,
           24,  323, 1016,  215,  581,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 333,  183,   13,  264,  639,   26,  378, 7080,    0, 6959,    6,   63,
           84,   10, 8020,   21,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   33,  283, 5128,    6,   17, 6443,    9,  108, 1507,    0,   17,
          384, 1898,  684, 2399, 6443,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   33,   26,  378, 1405,  131,   13, 1361,   12, 1613,   85,   28,
         2471,   55,   13,  694, 5423,    9, 3804,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,    7,   38, 2415,  480,  197,    9,    7, 1726,   26,    9,    7,
          964, 7688,    8,   26, 4379, 6288, 2223,   46,   39,  741, 1353,  255,
          424,    6, 1353,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 103,   24,  220,   87,   17,    0,  169,   24,  213,   10,  305,   94,
           11,    6, 9521, 6443,  755,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [7008,    0,   94,   17,   66, 4391, 5669,    6,   63,  143, 8372,    9,
          159, 4548,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  84,   63, 1075,   17, 8712,    7,   39, 1674,  307,  693,    8,    7,
         2762, 2883,    6,   12, 1276,    6,    8, 1970,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 333,  464, 1146, 1744,    6,  423, 1072, 1086,   10,   97, 1411,  896,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  70,   33,   26,  401,   26,  110,   57,  111, 2432,  170,   17,   24,
           63, 7679,   54,    7, 2085,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 101,  203, 9793,   48,   13,  277,  523,    0,   53, 1742,  565,   13,
         4004,   12, 2840,    6,    0,    8,  101,   34, 3423, 3841,   62,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 238,    0,   19,  552,  132,   71,   33,  321,   12, 7336, 5560,  279,
           71,  117,  682,   45,    6,    8, 6382,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   9,    7,   73,  128,    0,  166,   26,  116, 1663,    7, 5743,    0,
           84,   26, 2386,   12, 5494,   12,  415,  233, 2180, 2233,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:7') tensor([19, 28, 23, 19, 26, 19, 37, 24, 21, 26, 21, 19, 18, 19, 21, 29, 19, 16,
        23, 14, 19, 25, 21, 24], device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 0.8535, 0.8862, 0.8169, 0.8994, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8608, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:7',
       dtype=torch.float16)
True tensor([[  67,  113,    7, 5707,   54, 2463,   12,  117,  708,   26, 1897,    9,
          854,    9,  159, 6268,  316,   20,   10,    7, 3941,   22,  945,   17,
         1148,    9, 1666,  215,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   0,   13,    0,  613, 2816,  166,   25,    0,  192,   11,   18,  135,
          995,  994,   80,  155,   39,  430,  119, 1215,    0,    0,    8,    0,
           25, 3284,    0,   33, 2072,    8, 8897,    0,  475,   45,   24,  583,
         1261,   17,   24,   66,   10,  598,  108,  207,  359,   71,    0, 4656,
            6,  737,   62, 2135,  494,  833,    0,    2],
        [  29,  116, 1730,    0, 1167,   26,   39, 8999, 5204,   12,    7, 2122,
           12,  108, 1247,   10, 5039,    8,   10, 4199,   10,  159, 1422,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   10,    7,  690,  241,  879,  221, 1381,    0,   21,   11,    6,
          116,  486, 7259,   12,  291,  927,  153,  140, 1011, 1070, 3163,   55,
         2394,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 103,   25,  305,    7,    9, 2446,   10,   59, 1090,   17,   25,  388,
            9,    0,   25,  305,    7, 5525,   69,   33, 3634,    0,    8,    7,
         8792, 5525,   69,  117, 6381,    6,    0,   25,  451,   51,  529,   10,
         6602,  111, 2026,   33,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 101, 1042,    0, 3450,  122, 2361,   17,    0,  101,    0,  144, 1932,
            0, 2703,    0,  126,   12,  392,  807, 6037, 1314,   10,  284,    0,
         2100,   79,   13,  387, 2459,  131,  218, 5808,  589,    6,  929, 2385,
          134,   39, 7103, 9241,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  87,   24,  213,  108,  595,   10, 1645,   51,   13, 3883,    0,   13,
          415, 1040,    0, 1183,   17,   73,  368,  159, 2434,   12,    7, 2196,
           10,  601,  170, 1914,  108, 2463,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  24, 2955,    0,    0,   21,   10, 8961,    0,    0, 6761,    0,    0,
          206,   13,    0, 1361,   12, 5426,   62,    0, 9096,  237, 2445,  373,
            0, 2293,   48,   33, 1153,    0,  166, 4327,  170,  661, 1456,  138,
           33, 6737,   26,   29, 7528,   55,   33, 4188, 3978,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:5') tensor([30, 56, 26, 27, 42, 42, 32, 47], device='cuda:5') tensor([1.0000, 0.8384, 1.0000, 1.0000, 1.0000, 0.8203, 1.0000, 0.8174],
       device='cuda:5', dtype=torch.float16)
True tensor([[ 251, 4159, 4333,  778,  106, 5459, 4159,  103,   21,   11,    6,   13,
          639, 1201,    0,   10, 1232,   54,    8, 2263,   96,    8, 6336,    8,
           29,   69,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   70,   26,  264,   26,   17,    0,  120,   24,    0, 2546,  436,
           33, 3634,   80,   33,   13, 1674,    6,    0,    0,    0,   33, 3634,
           12,  688,  204, 2882,   13, 7153,   12,  688,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70, 1073,  170,  422,   26,   17,    0,   12,   77,  117,  214,   17,
          108, 3227,    8,  159, 3227,   66,    0,   24,   63,    7,  281, 4391,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,    0,   70,   11,    6,  916,   26,   17,   33,    0, 1760,  207,
           12,  665,   85,    7,  179,    0,   26,    0,   19,    0,  154,    0,
            0,  288,   91,    0,   12,  717,  341, 1081,   17, 8197,    0, 1574,
            9,  341, 1726,    6,   12,  467,   20,  290, 1900,    0,    2],
        [  70,   63,   24,  142,   10,   87,   71,   77,   33, 1221,    0,   51,
         1763,    6, 5037,    7, 7147,   12,   33, 4889, 1479,   17,   24,   66,
          629,  108, 4896,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,  591,  126,    0,   17,   84,   63,   13,  733, 3631,   12,
         8860,    0,   17,  465,   11,   18,   66, 4585, 2836, 6672,    0,   19,
          474,    0,   38, 1871, 1456,    0,   26,    0,    7,  850,  111,   54,
         6027,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 180,   21,  116,  842,  486,  391, 1345,   55,  939,  469,  834,  832,
            0,   22,    0,   10,  175,   21,  199,  159, 3860,   59,    0,    8,
           69, 9474,   24,  278,    7,  264, 5540, 1375,   46,   21,   34,  346,
          168,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,  121, 1110,   33,  464,   71, 2806,    0,   24,   11,   57,
          735,   13,  464, 1708,    8,   13, 6049, 1706,  120,   21,  925,   10,
            7,  230, 6409,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8, 7008,   17,  188, 3463,  203,   18,  551,   54,   10,    7, 1829,
         4879,    0,  109,  142,  346,   10,   13,  338, 3528,  760,   10,  722,
          205, 1591,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 166, 2141,    6,  110,   10,  154,   17, 7520,  184, 2445,   93, 2734,
         1553, 2232,    0, 4459, 7052,    6,   55,   33,  321,   12,  279,    0,
           63,  172, 4050,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  69, 1219,   12,   17,    0,   81,  234,  241,  188, 6069,   59, 1236,
          849,  340,    0, 1094,  101,   11,    6,  499, 6324,   10,  283,  383,
            9,    8,  383,  126,    9,   17, 2483,  227, 2131,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33, 1248, 1365,    0, 3577,  187,  424, 1492,  233, 4160,    0,   34,
         2497, 4802,  128,    9,   70,  513,   69,   79,  225,   34,    7, 4415,
            6, 4382,   55,    7, 7859, 1525,    7, 5071,   22,    6,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  276,    9,    7, 1752,   35, 7311,  179,    0,   13,  179,   24,
         1275,  154,   12,   79,  378, 2912,  131,  143,  596,    0,  596,   85,
            7, 1219,    0,  423,  439,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9, 2047,   12, 6566,    8, 7398,    0,   33,   26,   13,   56, 8715,
           35, 7596, 7927, 2477,   17,   34, 1405,    9,    7,  501,  895, 1047,
            6,    9,  217,  532,   48,  727,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33,   26,  452,  340,   35,  868,   35,  176,  783,   15,   20,   35,
          820,   35,  416,    0,    8,   19,  144,   10, 1082, 6873,   69,    7,
         2618,    9,    7,  473,  391,  215,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 225,  968,  170,   12,  277, 2352, 5320,   62,  131,  889,  148,  703,
           17, 1057, 2314,   71,   13,  133,  963, 7995,  164, 6213,  134,  106,
         5408,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([28, 34, 26, 47, 30, 39, 39, 29, 28, 29, 35, 36, 31, 32, 32, 27],
       device='cuda:5') tensor([1.0000, 0.8892, 1.0000, 0.8120, 1.0000, 0.8740, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:5', dtype=torch.float16)
True tensor([[  25,  220, 1644,  ...,    1,    1,    1],
        [  19,  154,   33,  ...,    1,    1,    1],
        [  67,   53,  162,  ...,    1,    1,    1],
        ...,
        [  19,  213,   10,  ...,    1,    1,    1],
        [  70,  164,   24,  ...,    1,    1,    1],
        [  19,  192,   11,  ...,    1,    1,    1]], device='cuda:5') tensor([10,  9, 11,  7,  9, 10, 10, 11,  9,  6, 11,  8,  9, 10,  7,  9,  9, 11,
        11, 12, 12,  8, 12, 11, 10, 10,  6, 10,  7, 10,  8,  9, 10,  8, 11, 10,
         8,  7, 10,  8,  9,  6, 12, 11,  7,  8, 12, 13,  9,  8, 11, 13,  9, 10,
         8,  7, 10,  6,  7,  7,  7, 10,  8, 15, 14,  7,  9, 10,  8, 10,  9, 11,
        12,  7,  9,  8,  9,  9,  8, 11,  8,  9,  9,  6,  9, 19,  9, 10, 10, 10,
         8,  9, 13,  8,  8,  9, 13,  9, 14, 12,  8,  8, 12,  9],
       device='cuda:5') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8740, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8267, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8076, 1.0000, 0.8994, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8862, 1.0000, 0.8169,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8252, 1.0000, 1.0000, 1.0000, 1.0000, 0.8516, 0.8394, 0.8916, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8340, 0.8486,
        1.0000, 0.8354, 0.8047, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8560,
        0.8594, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8506, 1.0000, 1.0000, 1.0000, 0.8096, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8657, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8662, 1.0000], device='cuda:5',
       dtype=torch.float16)
True tensor([[ 115,    0,   71,  846, 1105,   48,    6,    8,  218,  214,   46, 1976,
          639,   21,   26,    0,   21,  689,   11,   18,  172,  988,    0,    7,
          613,   26,   17,  778,  164,   66, 8600,    9,   21,  185, 4273, 2273,
           54,   21,   10,    7, 1677,    0,    8,   29,   24,   66,    0, 1223,
            0,   39, 1465,   12,  214,    0,    2,    1,    1,    1,    1,    1,
            1,    1],
        [ 180,   53,  144, 5543,    6,    0,  294, 5543,    6,    0, 6405,    7,
          723,   11,    6, 1392,  429,  116,  106, 5063,    0,    8,   70,   53,
          591,   34,  251, 7955,    6,   12,    7, 1392,  429,  162,  976,  261,
         8093,    0, 4803,  415,   59, 8186,    0, 3012,   17, 5063, 8831,    6,
          172,   87, 3982,  108, 4129, 1392,  429,    0,    2,    1,    1,    1,
            1,    1],
        [  24,   73,   87,   33,    0,   38, 3179,  294, 1427,    9, 1491,   63,
          142,   10,  468,    0, 3695,    0, 3695,    0, 1189,    0, 1768,    0,
           84,   63,  248,    0,   77,  230,    0, 1586,    0,   84,   11,  158,
           51,  185,  561,    0,  125,  185, 6010,  164,   87,    7,  230,  279,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 432, 1858, 2007, 1498,    0,    7, 7834, 1882,    9,  264, 1736,  942,
          513,  346,  131, 1016,  439,    0,    7, 8423, 1882,  513,  346,    0,
            7, 7648, 5154, 1882,    9,  264, 1736,  513,  346,  432, 1858, 2007,
         1498,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   7,  524,   71,   17,   26,  180,   21,  598,    6,  100,  155, 3762,
           34,   86,  276, 8712,   48,    0,  100,  211,   91,  276,  595,   62,
            0,   38, 2847,  156,   25,   55,  155, 3762,    3,    8,   24,  954,
           69,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 238,    0,   21, 1518,  126,   17,   84,   11,    6,  226,   13,  453,
         1799,   12, 9547,  793, 1241,    7, 2519, 1237,  244,    7,  215,   80,
         1179,   84,   26,  250, 7071, 1226,   71,  205, 5731,  322,    0,   39,
         5687,   10,  229,  841,   12,   77,   12,  251, 1719,  293,   15,   18,
           39, 5854, 3688,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   0,   25,    0,   63,   86,  142,   10,  175,    0, 6405,   48,    0,
           69,  155,    0,  999, 1288,    0,   13,  325,   12,   94,  154,   53,
            0,  164,    0,  103,   25,  274,    0, 1004, 8860,    8,    0,  884,
           94,   80,  159, 3187,  479,    0,    0,  159,  281,  528, 2666,  369,
            0, 6567,  439,   12,  134, 1917,   62, 7862, 1250,   12, 4379,  132,
            0,    2],
        [  67,    0,    9,  409,    0,  281, 4478, 2527,   10,   51,   13, 1905,
          207,    0,    8,  281, 2352, 2527,   10,   51,   13, 1905,  207,    0,
            8,    7,  613,   26,   17,    0,   55, 4478,    0,    7,  207,   17,
           53, 1620,    8,    7, 1507,   17,   53, 6438, 1382,   11,   18,  740,
          238,    9, 3061,  115,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:1') tensor([55, 57, 50, 39, 39, 53, 62, 54], device='cuda:1') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8301, 1.0000],
       device='cuda:1', dtype=torch.float16)
True tensor([[  29,   26,   33,  116,   13,  546,  106, 2008,  830,  443,  187,  713,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   53,   73, 2020, 1381, 3411,  652,  511,  290,  131, 2567, 3993,
         1827,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [ 248,  215,  581,    0,   19, 1095, 2737,   56, 1295,  176, 1339,  783,
          511,   37,    0,    2,    1,    1,    1,    1,    1,    1],
        [  67,   24,   11,   57,   86,  142,   10,  467,   82,  131, 2432,   94,
           17, 3079,   26, 2645,  111, 1226,    0,    2,    1,    1],
        [  19,  144,  391, 3002,    0,  976,  261,    0,   17,   19, 5472,   71,
         6244,  111,   77,    7,  183,    0,    2,    1,    1,    1],
        [  84,   63,   29,  294, 4012,  378,  442,  120,   25, 1570,   69,    7,
         5113,  989,    0,    2,    1,    1,    1,    1,    1,    1],
        [  19, 2138, 1551, 2202,   69,   21,    9,    7, 5291, 2484,   54,   12,
           89, 2476,    0,    2,    1,    1,    1,    1,    1,    1],
        [ 101, 3524,  144,   10, 7221,   17,  835,   35,    6, 2286,  541, 1755,
         2980,  106,  166,  101,   34, 2188,    0,    2,    1,    1],
        [ 251,  162,  391,  214,   17,   19, 5472,   71,  976, 1387,  111,   77,
            7,  183,    0,    2,    1,    1,    1,    1,    1,    1],
        [  67,   70,   11,    6,  528,  168,   26,   17,  116,   55,  822,  690,
         1079, 3352,  271,    0,    7, 9035,   63,  230,    0,    2],
        [   8,  168,   26,   39, 1683,   12,   13, 2672, 1683,   48,    9, 1802,
          341, 3228, 2868,    6,   12,  688,    0,    2,    1,    1],
        [6792,    6,   63, 5744,   91, 1396,   85,   13,  183,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  168,   26,  486,  663,   12,   13, 2994,    6,  235,  266, 6788,
          369,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  13, 1127, 4187,   81,    0,    8,  101,   14,   48,  929, 3101,   91,
          270,    9,  877,   20,   20,  430,    0,    2,    1,    1],
        [ 117,   63,    7, 4233,    6,   12,   70,  169,   51,    7,  324, 1427,
           10,  205,    8, 1233,    0,    2,    1,    1,    1,    1],
        [  91,   12,  134,    0,  106,   56, 5004, 2992,    0,   26,    7, 1435,
           12,  422, 5201,    6,    0,    2,    1,    1,    1,    1],
        [  67,  120,   19,  154,   80,   21,    0,   33, 1382,   11,   18,    7,
          630,   17,   24,  451,   51, 8569,   71,    0,    2,    1],
        [  19, 1109,  168,  490,   25, 7688,   79,   39,  217, 1373,    6,    6,
          556,  240,   55,   89,  733, 1543,    0,    2,    1,    1],
        [  19,  296,   13,  277,  523,  518,    0,   38,  469,   93,  465,   11,
           18,  276,  213,   10,  205,   84,    0,    2,    1,    1],
        [  89,  775, 5608,   62, 3046,  188, 1828,  110,   10,  185,  133, 8791,
         1427,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  67,   17, 2527,    6,   10,   51,   13,  133,  422,   35, 2109, 1775,
         1466,   12, 3838,    0,    2,    1,    1,    1,    1,    1],
        [ 359, 1377,    0,    7, 1377,   12,    7, 1479,    8,    7, 1377,   12,
           81,    0,   24, 6932,  150,  156, 2403,    0,    2,    1],
        [  68,  194,   65,    9,    7,  501, 7015,    6,  168,    0,   25,   66,
         9751,  499, 2584,    7, 2593, 1075,    0,    2,    1,    1],
        [  19,  154,   25,   11,  158, 2990,   71,  110,   17,   17,   11,    6,
           13,  976, 4501, 1136,  958, 7681,    0,    2,    1,    1],
        [   0,    8,  180,   24, 4745,  835,   10,    0,  468,    7,  567, 1584,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   11,  158,  498,  168,    0,  473,  211,  121,   45,  571,   84,
           34,   13, 3398, 1010, 5169,    0,    2,    1,    1,    1],
        [ 103,   19, 7461, 8760,    0,   84,   11,    6,  923,   13, 1878, 1241,
         8760,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   0,   19,  154,    0,   91,   12,    7,  281,  916, 3302,   12,   33,
          925,  106, 5170,    0,    2,    1,    1,    1,    1,    1],
        [ 125,  595, 2415,  158,  188,  423,   10, 1450,  439,   12,  913, 3003,
           45, 1947,    0,    2,    1,    1,    1,    1,    1,    1],
        [   8,   25,   11,   57, 5655,   69,   71,   91,  874,    0,  665,   85,
            0,    7,  179, 1393,   51, 1763,    0,   25,    0,    2],
        [  67, 5784,   24,  498,    0, 6501,    0,  251, 8691,    6,    0,    0,
           24,   11,   57,   86,  142,   10,   87,  509,    0,    2],
        [ 635,    7,  473, 7173,  524,  172,  451,   51,  474,   12,   79,    7,
          473, 7173, 2212,    0,    2,    1,    1,    1,    1,    1]],
       device='cuda:1') tensor([14, 15, 16, 20, 19, 16, 16, 20, 16, 22, 20, 11, 15, 20, 18, 18, 21, 20,
        20, 15, 17, 21, 20, 20, 14, 19, 15, 17, 16, 22, 22, 17],
       device='cuda:1') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8188, 1.0000, 1.0000,
        0.8647, 1.0000, 0.8984, 0.8062, 1.0000], device='cuda:1',
       dtype=torch.float16)
True tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8491, 0.8701, 0.8652, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8193, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8511, 1.0000, 0.8647, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8062, 1.0000, 1.0000, 1.0000, 0.8188, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000, 1.0000, 1.0000,
        0.8066, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8369, 1.0000, 0.8120, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8164, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8066, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8574, 0.8853, 1.0000, 0.8032], device='cuda:3',
       dtype=torch.float16)
True tensor([[   8,  115,    7,  467,   26, 3019,    0,   19,   11,   45, 4594,   10,
         1723,   12,   33,  801, 1603, 1583,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7, 1043,   26,   17,  120,   24,   11,   57,    9,   13, 1503,  206,
           24,   73,   51, 5625,   62,    0,  206,   24,   73,   51, 6090,    0,
          108, 1588, 2317, 7865,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  168,    0, 3143,  660,   17, 6144,  879,  416,  307,   46,   17,
           11,    6, 7616,   46,  188, 3148,    6,   69,    7, 2011, 3043,    8,
            7, 2788, 1353,  827,    6,  140, 2130,  567,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  10,   87,   33, 1986, 4343,    0,   24, 1446,   10, 3067,   13, 6243,
         3134,  266,  232,  442,   12, 1291,  777,   48,    6,    0,  166,   63,
            7,  370, 3422,   17, 1661, 1782,  155, 1603, 9745,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,  339,   21, 2322, 5352,   69,    7, 3686,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   34,   13, 1651,   35, 6937,  876,  290, 2404, 4093, 1344,  266,
         7153,   10,   51, 1930, 6738,   62,  244,    7, 3156, 3421,    9,  264,
         1736,  942,    0,    9, 1722, 1466,   12,    7, 1384, 5151,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   19,   11,   45,   34,  176,   54,   33, 1111, 1551,    0, 1111,
         1113,    0,    8,   82,    6,   63, 1028,  518,  117, 4222,    6,    0,
            8, 4015, 1254,   46,   39, 2447,  293,  457, 5227,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24, 1742,    0,  134,    0,   13, 1234,    0,  347,  192,   11,   18,
           53,  368,   21,    0, 2537,  194,   65,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,    0,  244,  655,    0,  215,    0, 1352,  100,   33,   66,    0,
          226, 1944,  170,   10, 2952,    0,    0,   24, 4355,  807,   66, 8263,
           10, 2034,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1382,   11,   18,   21, 1968,    0,   38,  511,  274,  336,   25,    8,
          135,   17,   70,   24,  172, 1006,   80,   26,   17,  277,  523,   12,
         1878,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   19,  154,    9,    7,  858,    0, 3422,  164, 7831,    0,    8,
           53,  164,  274,    8,  598,  100, 7893,    6,   24,  135,  440,    0,
          100,  415,   18,  741,  109, 7011,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  17,  506,   51,    7,  288,  207,   17,   24,  164,  172,   51,  529,
           10, 3128,  108,  447,  422, 2463,    8,  172,   51,    7,  211, 1755,
         1369,   24, 1080,   10,   77,   51,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 101,   34,  529,   10,    0, 2273,  284, 2434,   12, 4312,   45,  652,
          375,  816, 7072,   71,  284, 2410,   12, 3673,   54,    9, 1296,   10,
         5032,    0,   46,  419, 2574,   96,    0,   46,    7,    0, 2481, 1154,
          121,    0,   56, 2147,    0,   15,    0,    2,    1,    1],
        [  56, 5084, 1020, 3860,   48,    7,  832, 1488,  297,   71,   13, 2084,
           93,   12, 6293,    8,   10,  424,  412,   96,    0,    7, 1715, 2185,
         5657,   48,  346,   69,    7, 7078, 6901, 5124,    0,    8,   24,  690,
            6,  777, 1314,   79,   24,   85,   20,    0,    2,    1],
        [  91,   12,    7, 1890, 2671, 4298, 2239, 5807, 2839,   54,   26,    7,
            9,  679, 1236, 1032,   35,  779,  569,   54, 2848, 5188, 7521, 6764,
         1157,   37,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55, 4212,    0,   19,  465,   11,   18,  135,   17,    7,  851, 4160,
          918,    6, 4595,    7,  824,  265,    0,   67,   53,  162,  133, 5592,
           29,  333,  851, 4160,  918,  246,   38,  372,  664,   57,  606,    3,
            8,   21,  128,  589,    6,  323,    7, 9877,    0,    2]],
       device='cuda:3') tensor([20, 30, 34, 35, 10, 36, 35, 20, 28, 27, 32, 32, 44, 45, 28, 46],
       device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8594, 0.8271,
        1.0000, 1.0000, 1.0000, 0.8569, 1.0000, 1.0000, 1.0000],
       device='cuda:3', dtype=torch.float16)
True tensor([[   8,    0,   79, 1149, 1148,    9, 1491,    0,   97,   45,  236, 1991,
         1505,  227, 1806,  399,   93,   45, 1032,   71,  110, 1674, 3760,    6,
            0,    2,    1,    1,    1,    1,    1],
        [  21,   11,    6,  133, 1729,   35, 1987,  616,  140,   18,  366,    0,
            8,   21,   11,    6,  133,  110, 1098,   35,   93,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,   79,    7, 3035,  160,  140,  763,  128,    0,   19,   66,   10,
            9, 2515,   70,   19, 2438,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   7, 1799,   26,    0,  419,  183,   25,   11,   57,    9,   13, 2345,
           35, 1105, 3139,    6,  603,   18,    0,   77,  155,  583,    6,   63,
         1363,    0,    2,    1,    1,    1,    1],
        [ 115,    0,   19,  154,   17,   24,   73,   77, 2990,   17, 8371,    6,
           63, 3818,  111, 8871,   85, 3101,  570,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,   19,  169, 1948,  305,    7, 2290,  287,   15, 4125,    8,  229,
           21,   13,  453,  488, 5490,    9,    7, 1403,   12,    7, 5714,    0,
            2,    1,    1,    1,    1,    1,    1],
        [  84,   63,  409,    6,    0,   84,   63, 5669,    6,    0,    0,    8,
           84,    0,   63, 4433,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [1706,  111,   19,   11,  158,  450,   25,  347,   19,   34,    9, 2060,
         1567,    0,  168,   24,  205,    0,    8,  251,   63,  116,    7, 1913,
            0,    2,    1,    1,    1,    1,    1],
        [   0,   67,    7,  524,   26,   17,   24,   63, 1028,  126,   12,   13,
         4430,  206,    0,   24,    0,  144,   13, 1728,    0,   12, 2677,   54,
           21,    0,    2,    1,    1,    1,    1],
        [   8,  101,   73,   11,   18,  661,  138,   33, 1598,  712, 1144,   73,
           51,   10,  562,  922,    0,    8,  101, 5668,    6,  284, 2084,  335,
           18, 1991,    6,    0,    2,    1,    1],
        [ 115,  117,   63,  521,   18,  292, 1305, 1010,    0, 1532,    0,    8,
           29,   24,    0, 2527,   10, 9166,  106,    0,  134,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  24,  278,   13, 2914,  287,  122, 1806,  365,   13,   48,  525,  365,
           59,  866, 1557,   54,  170,  130,   20,   35,  424,  233, 2182,   48,
          106,   13,  488, 1953,  670,    0,    2],
        [  10,  661,   33,    0,  339,   11,    6,  884,   33,  630,    0,  138,
          294, 7224,   63,    9,   13,  877, 1034, 1987,   59, 2738,    0,    2,
            1,    1,    1,    1,    1,    1,    1],
        [  29,    7,    0, 3109,  266,    0, 9232, 3336, 6263,  284, 1066,    0,
            0,    8,   13,    0,  555, 4299, 1065,    0,    7,  733,  567,   11,
            6,    9,  659,  620,   45,    0,    2],
        [ 116,   10,   51, 2226,    0,   19,  154, 2930,   73,  172,  601,   71,
           33,  524,    0,  204,  229,   21,  143, 2695, 1522,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  67,    0,   25,  135,    0,   21,  188,   13, 3148,    9, 2047,   12,
          858,  535,   20,  586,  429,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  67,   13, 1968, 1300,  101,   34,    0, 1968, 8097,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  67,  566,   59,   59, 1502,   11,    6, 8217,   34, 1226,    0,    8,
          476,  324,  687,  101, 1781,  284, 1066,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  55,  214,   17,  162, 3924,  143,  254,  655,  215,  581,    0,    7,
         1192, 1985, 2613,   80,  640,  439,   12,    7, 1289,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  29,    7, 5974,   55,    7,   13,   48, 4258,  160,   54, 3183,    6,
           26,  133, 2414,   10,   70,   24,  296,   55,    7,  264,   91,    0,
            2,    1,    1,    1,    1,    1,    1],
        [6183,   12, 1491,  296,   10, 1766, 5349,    6,    0, 7936,   10,   87,
            7,  214,   17,  225,  213,    6,   10,   87,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   48,  100,   10, 1452,   71,   25,    7,  546,   12,   91,
           12,   89, 2116,  434,  452,   20, 1386,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  33,   26,    7,   38,  870,  819,  232, 3833,   48, 2566,    3,   12,
           77,   89, 7286,    6,    0,   21,   11,    6,    7,  281,  456, 1197,
            0,    2,    1,    1,    1,    1,    1],
        [  67, 1880, 3109,   35, 2398, 1775,  221, 1144,   26, 3465,    7, 2593,
          291,  429,   12,    7, 1880, 1362, 1547,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1]], device='cuda:4') tensor([26, 23, 19, 27, 21, 25, 18, 26, 27, 29, 23, 31, 24, 31, 23, 19, 11, 21,
        23, 25, 22, 21, 26, 21], device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8862, 1.0000, 0.8120,
        1.0000, 0.8369, 1.0000, 1.0000, 0.8555, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:4',
       dtype=torch.float16)
True tensor([[ 115,    0,   21, 2413,   10,  110,   17, 1234,   26,  172,    7,  572,
           11,    6, 6180,   10, 6816,   33, 2237,    0, 2541,   35, 3197,  474,
           69,   91,  874,  199, 4340,   69,    7,  218,  874,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   19,  474,    0,   38, 4660,   86,  305,   33,    8, 2273,   21,
           10,   89, 1578, 3327,  660,    8,   66, 2991,  984,   55,   89,  955,
            3,   70,  323, 2991,  367,  126,   71,   33,  464,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  168,   11,    6,   13,  822, 2326, 8194,    0,   79,  103,    7,
          248,  926,    6,   12,    7,  572,   63,  746,   12, 1747, 3488,   54,
         1525,  545,  218,   10,  321,   12,  229,  841,   12,    7,  316,  128,
          429,    8,    7, 1953,    6,  369,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  180,   24,    0, 1060,  267,    0,   38, 1824, 2142,    0,    0,
           70,  994,   87,   25,  296,    3,    0,    0,    8,  225,  246,    0,
           38, 1367,    0,  115,    0,   19,  296,   13, 1318,    3,   29,    0,
         1646,  215,  581,   13,  122,   22,   96,    0, 1405,    7,  245, 3050,
           35, 2366,    0, 2426, 1318,    9, 1122,  292,  156,    0,    0, 5712,
            0,    0,    9,    7,  886, 1216,  187, 1070,    0,    2],
        [ 248, 2168,  581,    0,   19,   34, 2202,   85,    7, 6787, 2722,   71,
           89, 3780, 2101,   18, 4128,    0,    8,   24,  162,  952,   80,   70,
           19,   34,  142,   10,  456,   80,  440,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,  101,  109,  225, 2456,    6,  346,    0,   39,  389,  436,
            6,   21,    0,    8, 3060,   96,   10, 9237,   70,   11,    6,  142,
           10, 1085,    0,    8,  180,  117, 1760, 8194,    6,   63,  116, 4929,
          270,   85, 7175,  825,    9,    7, 1192, 1678,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  68, 9336,    0,   19,  116,  692,   10,  884,   25,    0,  120,   24,
           24,  293, 3981, 1368,   18,    6,   24,  192,   11,   18, 5607,   24,
          293,  134,  116,   10, 3560, 4513,   12,  282,    0,   21,   11,    6,
          113,   10, 3560, 1771,   12, 3795, 9028,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  69,    7,  859,    0,   33,   26,   13,  862,  235,   18,  232, 2145,
            0,   21,   11,    6, 6686,   71, 7260,   96,   17,    7,   94,   85,
         1886, 2109, 2671,   66,  591,   66,  211, 9478,  369, 9335,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([35, 35, 44, 70, 33, 46, 45, 36], device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 0.8765, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
True tensor([[  84,   63,   55,  627, 6500,    6,   46, 3061,    0, 2900,    6,    0,
         2406,    6,   46,    9,  166, 2462,   73,  305,  561,    0,    8,    9,
          891,  128, 6500,    6,   46, 3295,    0, 3373,    0,  837, 5002,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   19,    0, 1446,   10,    0,  746,   12,    0,    0, 5802,   21,
          133,    0,  133,    0, 5830,    0, 5655,    0,   89, 3200,    0,    0,
          740,    0,  629,   89, 1259,  242,  411,    0,    8,  961, 1123,    0,
          778,   26, 1648,   62,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  94,  162, 5187,    8, 8569,    0, 4341,    8,  179, 3348, 1918,   10,
          575,  170,   17,   53,  162, 6125, 2303,    8,  162,  740,   10, 2026,
           21,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  204,    7,  744, 2737, 2406,    9, 1051,   15,  760,    0,    9,
           80, 1061, 5745,    0,   34,  206,   24,  487,   10,  991,  126,   39,
         1711, 9655,   55, 4313,  110,   48, 1613, 1986,   18,  693,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   55,    7,  558,  785,  215,    0,   19,    0, 5610,   62,   79,
           13, 2988,    0,   10,   36, 5929,   18,    0,    0,    0,   89, 3563,
         4197,    0,    0,  148,   34,  211, 1978, 3531,   10,   51, 1663, 2373,
            0,   10,   13, 2829,  670,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,  185,   12,   25,  659,   66,  367,   10,    7, 1066,   17,
           26,   33,   86, 5868,   10,    7, 1470, 2066,  221, 2695,   25,   66,
         1346,   80,   56, 8456,  111,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [3537,  297,   20, 2912,  242,   18,  249,    0, 6481,    0,   19,  474,
            0, 2982,  133,  998,  266, 5366,  111,   80,   33,  296,   10,  875,
         4995,    6,    0,   10,  875,   94,  199,    7,  979,   12, 2567,  214,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  21,  842,  170,  288,    3,   10,  229, 4619,  403,    6,  416, 2022,
            0,   70,   39, 3128,  445,   17,   26,    0,   67,   24,   66,   10,
         7369,   17,  199,  108,  447,  858,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 775,    0,  120,   19, 1387, 2469,    6,   71, 1771,    0,   12,   94,
            0,   19,  735,  884,   33,  630,    0,   70, 1503,   12, 1066,    0,
            0,   70,    0, 3316,   12, 1066,   26,   21,   17,    0,  188, 3860,
           48,    0,   25,  772,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 432,   39, 3350,    0,    9,  117, 3530,    0,   21,   11,    6,   29,
         4391,   17,    0,  120,   19,  205,  346,    0, 1120,  333,  876,  121,
            0,   19,  602,  235,    0,    0,  199,   89,  203,  122, 6287,    0,
          125,   89,  955,   73,   11,   18,    0, 1799,   71,    7,    0, 3611,
           12,    7, 3401,   69,   89,  906,    0,    2],
        [  10,  205,  199,   13, 1507,    8,   10,  288,  700,  979,   94,  359,
           17,  321,   12,  227,  156,  338,   45, 3841,   18, 1617,   26,   10,
          492, 3584,  175,    7, 1507,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  432,  378,   85,   60,   55,   13,  555,  215,    0,   19, 1873,
           17, 2905, 6904, 1780,    6,   26,   86,   17, 3315,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,   33,   26,    0,  143,  109,  908,    0,  103,   25,  274,
           85,    7, 2313,  660,   12,    7, 1075,   46,   53,   63,  100,   33,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 117,   63, 2455,   17,  367,  106, 1333,   22,  760,  255,  424,    6,
         1353,   11,    6, 2260,    0,    8,  166,  575,   25,    0,    9,   13,
         1074,  572,    0,    7, 8754,  369,   12,   17,  572,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 288,   13,  277,  244,  854,   12, 4495,    0, 2900, 4853,    6,    9,
            7, 1384, 1224,    0,  148,  175,   13, 3286, 1370,  204,   63,  740,
            9, 3254,   17, 2720,   17, 1370,    0,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  84,  162, 4712,  785,  759, 3836,    6,    0,  248,  759, 1052,   48,
          181,    6,   17, 1918,   10,   51, 3327, 2871,    8,    9,   48,  783,
           62,    0,    8, 2518,  143, 7580,    6,    8,  218, 1587,   12, 4006,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:0') tensor([37, 42, 27, 36, 43, 31, 38, 32, 42, 56, 31, 23, 26, 35, 33, 39],
       device='cuda:0') tensor([1.0000, 0.8027, 1.0000, 1.0000, 0.8330, 1.0000, 1.0000, 1.0000, 0.8735,
        0.8462, 1.0000, 1.0000, 1.0000, 1.0000, 0.8950, 1.0000],
       device='cuda:0', dtype=torch.float16)
2023-08-20 01:30:00 | INFO | train_inner | epoch 001:    101 / 1474 loss=20.04, trans_loss=5.874, nll_loss=4.683, w2v_ctc_loss=22.359, task_loss=1.72, contrastive_loss=3.242, total=4212.33, n_correct=124.21, ppl=25.69, accuracy=2.949, wps=20971.8, ups=1.67, wpb=12566.1, bsz=472.9, num_updates=100, lr=4.098e-06, gnorm=2.702, clip=0, loss_scale=64, train_wall=63, gb_free=18.7, wall=116
2023-08-20 01:30:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-20 01:31:00 | INFO | train_inner | epoch 001:    202 / 1474 loss=16.671, trans_loss=5.876, nll_loss=4.714, w2v_ctc_loss=17.166, task_loss=1.675, contrastive_loss=3.258, total=4127.88, n_correct=124.86, ppl=26.25, accuracy=3.025, wps=20529.8, ups=1.67, wpb=12326, bsz=463, num_updates=200, lr=8.096e-06, gnorm=7.006, clip=17, loss_scale=32, train_wall=59, gb_free=18.7, wall=176
2023-08-20 01:32:00 | INFO | train_inner | epoch 001:    302 / 1474 loss=9.935, trans_loss=5.831, nll_loss=4.699, w2v_ctc_loss=6.88, task_loss=1.687, contrastive_loss=3.221, total=4077.62, n_correct=133.87, ppl=25.97, accuracy=3.283, wps=20215.4, ups=1.66, wpb=12179.5, bsz=437.4, num_updates=300, lr=1.2094e-05, gnorm=1.702, clip=0, loss_scale=32, train_wall=60, gb_free=19.2, wall=237
2023-08-20 01:33:00 | INFO | train_inner | epoch 001:    402 / 1474 loss=9.4, trans_loss=5.8, nll_loss=4.686, w2v_ctc_loss=6.015, task_loss=1.389, contrastive_loss=3.271, total=4177.45, n_correct=128.45, ppl=25.74, accuracy=3.075, wps=20903.2, ups=1.68, wpb=12474.2, bsz=462.8, num_updates=400, lr=1.6092e-05, gnorm=0.619, clip=0, loss_scale=32, train_wall=59, gb_free=18.7, wall=296
2023-08-20 01:33:59 | INFO | train_inner | epoch 001:    502 / 1474 loss=9.206, trans_loss=5.823, nll_loss=4.717, w2v_ctc_loss=5.68, task_loss=1.243, contrastive_loss=3.329, total=4202.06, n_correct=108.97, ppl=26.3, accuracy=2.593, wps=21058.9, ups=1.68, wpb=12556.3, bsz=490.7, num_updates=500, lr=2.009e-05, gnorm=0.457, clip=0, loss_scale=32, train_wall=59, gb_free=12.9, wall=356
2023-08-20 01:34:58 | INFO | train_inner | epoch 001:    602 / 1474 loss=9.02, trans_loss=5.877, nll_loss=4.779, w2v_ctc_loss=5.433, task_loss=1.232, contrastive_loss=3.247, total=4124.52, n_correct=90.74, ppl=27.46, accuracy=2.2, wps=20764.4, ups=1.69, wpb=12301.1, bsz=471, num_updates=600, lr=2.4088e-05, gnorm=0.424, clip=0, loss_scale=32, train_wall=59, gb_free=18.7, wall=415
2023-08-20 01:35:57 | INFO | train_inner | epoch 001:    702 / 1474 loss=8.76, trans_loss=5.902, nll_loss=4.814, w2v_ctc_loss=5.073, task_loss=1.268, contrastive_loss=3.134, total=4147.01, n_correct=74.67, ppl=28.12, accuracy=1.801, wps=21011, ups=1.7, wpb=12381.3, bsz=455.2, num_updates=700, lr=2.8086e-05, gnorm=0.502, clip=0, loss_scale=32, train_wall=58, gb_free=19.1, wall=474
2023-08-20 01:36:57 | INFO | train_inner | epoch 001:    802 / 1474 loss=8.464, trans_loss=5.904, nll_loss=4.817, w2v_ctc_loss=4.643, task_loss=1.238, contrastive_loss=3.154, total=4121.11, n_correct=81.2, ppl=28.18, accuracy=1.97, wps=20584.6, ups=1.67, wpb=12298.3, bsz=463.4, num_updates=800, lr=3.2084e-05, gnorm=0.741, clip=0, loss_scale=32, train_wall=59, gb_free=19, wall=534
2023-08-20 01:37:56 | INFO | train_inner | epoch 001:    902 / 1474 loss=8.177, trans_loss=5.821, nll_loss=4.72, w2v_ctc_loss=4.371, task_loss=1.258, contrastive_loss=3.042, total=4167.98, n_correct=115.55, ppl=26.36, accuracy=2.772, wps=21149.8, ups=1.7, wpb=12446.6, bsz=457.5, num_updates=900, lr=3.6082e-05, gnorm=0.829, clip=0, loss_scale=32, train_wall=58, gb_free=18.9, wall=593
2023-08-20 01:38:55 | INFO | train_inner | epoch 001:   1002 / 1474 loss=7.921, trans_loss=5.733, nll_loss=4.613, w2v_ctc_loss=4.18, task_loss=1.259, contrastive_loss=2.985, total=4136.38, n_correct=112.33, ppl=24.47, accuracy=2.716, wps=20872, ups=1.69, wpb=12354.6, bsz=458.8, num_updates=1000, lr=4.008e-05, gnorm=1.042, clip=0, loss_scale=32, train_wall=59, gb_free=19.2, wall=652
2023-08-20 01:39:54 | INFO | train_inner | epoch 001:   1102 / 1474 loss=7.754, trans_loss=5.754, nll_loss=4.635, w2v_ctc_loss=4.049, task_loss=1.287, contrastive_loss=2.857, total=4148.31, n_correct=116.28, ppl=24.85, accuracy=2.803, wps=21043.3, ups=1.7, wpb=12371.7, bsz=453.4, num_updates=1100, lr=4.4078e-05, gnorm=1.166, clip=0, loss_scale=32, train_wall=58, gb_free=18.5, wall=711
2023-08-20 01:40:53 | INFO | train_inner | epoch 001:   1202 / 1474 loss=7.613, trans_loss=5.748, nll_loss=4.631, w2v_ctc_loss=3.947, task_loss=1.344, contrastive_loss=2.745, total=4134.43, n_correct=117.95, ppl=24.78, accuracy=2.853, wps=20908.6, ups=1.69, wpb=12350.3, bsz=438.4, num_updates=1200, lr=4.8076e-05, gnorm=1.271, clip=0, loss_scale=32, train_wall=58, gb_free=19.3, wall=770
2023-08-20 01:41:53 | INFO | train_inner | epoch 001:   1302 / 1474 loss=7.497, trans_loss=5.755, nll_loss=4.639, w2v_ctc_loss=3.829, task_loss=1.272, contrastive_loss=2.67, total=4055.44, n_correct=118.41, ppl=24.92, accuracy=2.92, wps=20359.1, ups=1.68, wpb=12107.5, bsz=442.9, num_updates=1300, lr=5.2074e-05, gnorm=1.304, clip=0, loss_scale=32, train_wall=59, gb_free=19, wall=829
2023-08-20 01:42:52 | INFO | train_inner | epoch 001:   1402 / 1474 loss=7.386, trans_loss=5.735, nll_loss=4.617, w2v_ctc_loss=3.726, task_loss=1.283, contrastive_loss=2.771, total=4125.61, n_correct=122.33, ppl=24.55, accuracy=2.965, wps=20835.8, ups=1.69, wpb=12328.1, bsz=452, num_updates=1400, lr=5.6072e-05, gnorm=1.329, clip=0, loss_scale=32, train_wall=59, gb_free=18.5, wall=888
2023-08-20 01:43:34 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([[   8,   21, 2700,  170,   13,  264,  207,   12,  826,   80,   86,  288,
           70,   11,    6,  142,   69,   69,  108,  943,    0,   67,   70,  506,
           51,  142,   69,  994, 6228,    9,    7,  415,    6,  626,    6,    0,
            2,    1,    1,    1],
        [  67,  115,    0, 3474,   54,   39, 1711, 7826,    0,   21,  188,   22,
           11,   18,  226,   39, 1829, 3046,    0, 2734,   26,   21,  276, 3768,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [ 115,    0,    7, 4690, 5573,   12, 3782,   69, 5335,  388,    6,  211,
           91,    9, 3919,   11,    6,  207,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [1042,   25, 4624, 2214,   10,  218,   94,   11,    6, 6466, 2788,  813,
            0,   25,   73,  116,  205, 1817,    8, 1713, 1976,   25,  213,   71,
           33,  813,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [3603, 1043,   54,   46,    0,    0,   24,   11,  158,  583,   21,    7,
         5130,    0,   12, 3603, 6710,   10,    7,  179,    0,    0,  336,  170,
           46,   33,   26,  835,   10, 2572,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1],
        [3683,   25,   12, 2307,    0,  916,    0,   19,   11,   45,    0,   79,
           25,   73, 2574,    0,   86,   13, 2468,  365,    0,   19,   11,   45,
           39,   97, 5116,  365,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  71,    7,  839,  442,  106,    7, 8430, 1228,    0,   24,  162,  529,
           10,  339, 1613,    9,  108, 5003,   77,  229,  159,  447, 1678,    9,
           13, 2179,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  33,   26,    7, 3050,   15, 1317,  206,    0,   79,   13,  963,   81,
            0,  185,   12,    7, 1298,   17,   19, 2396,   34,  245, 3657,   62,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [8307,    6,   26,   91,   12,    7, 5624, 1288,   55,  138,   24,  220,
          175,  518,  108, 1737,  439, 4067,   35,  181, 3800, 6201,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [1313,  180,    0,   24,   66,  442, 4116,  860, 2422,    6,    9, 1450,
         1884, 1004,    7,  179,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   8,    7,  245,  279,   19,  692,   10,   87,   79,   13, 3651,   34,
            0,   10,  205,   10,  110,  140, 1027,    8,    0, 3082,    7, 2101,
          541,  290,    0,    0,    7, 2178,  693,   18, 2951, 1942,   20,    0,
           12, 3967,    0,    2],
        [  19,   11,  121,  226, 5307,  890,  244,    7,  473,  392,  215,   10,
          283,   71, 1248,   94,  148,   11,  121, 4327,  110, 3251, 2172,   20,
         5247,    9,   13,  207,   17,   19,  154,  188,  442,  110, 8661,    0,
            2,    1,    1,    1],
        [  29,    0,   17,  818,    0,   25,  135,    0,    0, 1482,  148,   11,
            6,    0, 2219,    9,    0, 2319,  340,    6,  763,  777,  135,    6,
          143,   80, 2319,  340,    6,  763,  777,    0,  254,   19,   87,    0,
            2,    1,    1,    1],
        [  33,   26,   86,   56, 1295,  176, 1598,  724,  401,   13, 3212, 4112,
            0,   33,   26,   13, 3638,  387,  195,  116, 2841,   71,  185,  563,
         5109,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  21,   34, 1416, 2672,   12,    7,  264, 1736,  825,   13, 1323,    0,
         1345,  581,    0, 6605,  111, 4532,  266,  300,   62,  131,    0,    7,
         1827,   55,    0,    0,  378,   29, 3310, 3816,    0,    2,    1,    1,
            1,    1,    1,    1],
        [4950, 2666,   17,  688,   22,   54,  220,   51,    7,  558, 8070,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  25,  289,   70,   25,   11,   57,  401,    9, 1802, 1047, 2232,    6,
          109,  908,    0,    8,   94,  148,   63, 2091,    9,   25,  175,  251,
          132,   48,  436,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [ 109,  736,   94,  148,  842, 7997,  120,   53,  162,  277,    0,    8,
          736,   94,  148,  323,   86,  305, 7997,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   8,   24,  154,   84,   11,    6,   13,  325,   12,  250,  126,   84,
           67,   17,   11,    6,  125,   24,   11,   57,   56,   15,    6,  372,
          121,   48,  131,  108, 4854,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   0,   24,   11,  121, 3043,   22, 4986,    0,    9,    7,  245, 5083,
            0,  211,   13,   37, 1158,   93,  760, 3784,    0, 1206,   12,    0,
            7, 4344,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [3917,  424,   56,  668, 1859,   11,    6, 1396, 1610, 2071,  877, 5296,
            6,   34,   91,   12,  555, 1470,  889,   10, 7653,  106,  595,  375,
         3908,   11,    6, 4400,   71,  284, 2705,    6,  369,    0,    2,    1,
            1,    1,    1,    1],
        [   8,  138,   87,   24, 1927, 1661,  297,  833,   10,   51,   13, 4597,
         1181, 1006, 1757,   17,  368,    6,   77,   33,  453,  639,   17,   11,
            6,  336,  170,   10,  468,  108, 1588,    0,    2,    1,    1,    1,
            1,    1,    1,    1],
        [  67,   79, 1947, 3151,    6, 1273,    6,   20,    0, 5071,   22, 4341,
           66, 2516,   62,    9, 4689,   10, 4533,   20,    7,  753,    0, 3156,
            8, 3022,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   8,   19, 3683,   62, 1222,   13, 1323, 1551,    9,    0,   25,  135,
            0,    7,  227, 1411,   26,  746,   12,   13, 2481,  699,    6,   45,
           12,  282, 1584,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1]], device='cuda:4') tensor([37, 26, 20, 28, 32, 30, 28, 26, 24, 18, 40, 37, 37, 28, 34, 13, 30, 21,
        32, 28, 35, 33, 28, 29], device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8140, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8857, 1.0000, 0.8291, 1.0000, 0.8496, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8628, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:4',
       dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([[  29,    9,    7, 2223,    0,    9,  629, 1111,    8, 1646,    9,    7,
         2223,    0,    7,  955,   26,  180, 4208,   62, 1117,   12,    7,  955,
         4925,   10,  108, 1740,    8,   26,  378, 3327, 2871,  359,   91,   12,
            7,  452,   18, 3327,  706,    6,    0,    2,    1,    1,    1,    1,
            1],
        [  55,  663,    0,   19,  591,   33, 1636,  126,   12,  561,   85,  245,
          125,   33,   26,   70, 2932, 1613, 3367,  132,   71,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,    7, 2173,   12,  108, 3321,   34,   13, 6129, 6361,   35, 5952,
           62,    0, 5651,   24,    0,  583,   63,    6,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   7, 1470,   86,   96,  162, 3020,   54,    0,    7, 1950,   86,   96,
          162,  126,   12,  269,   20,    0,    7, 1052,   62,  128,    6,  465,
           11,   18,  283,    8,    7, 7860, 1584,   34,  116,  593,  822,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 832,  176,   35, 1461,    0,   70,   63,   25,  665,    0,    0,   85,
            0,    0,   13,  200,    0, 1233,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 162,   24, 1891,    7,  475, 1799,   84,    0,  848,  122,    0, 2497,
            7,  475, 1799,    0,   86,  288,   17,    0,   24,  842, 2583, 2032,
            6,   10, 4502,   21,   71,   13,  205,  927,  359,    7, 4129, 7260,
            9,    7, 4531, 1228,    6,   17,   25,   11,  121, 1110,  168,    0,
            2],
        [  69,  419, 1800, 2685,  271,    0,  423,  439,   12,    7, 3379, 2231,
         1536,  439,   12,    7, 6929,    0,   29, 2961,   26,  665,   85,    7,
         4221,    0,   53,   11,   57,  987, 1090,   22,  140,   54,    7, 4221,
           12,    7,  415,  699,  290, 2685,    0,    2,    1,    1,    1,    1,
            1],
        [  68,  386,   65,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,   11,   45,   13, 4853, 3173,   85,    7, 2811,   12, 6062,    0,
            8,   19,  283,   69,   13, 1039,  434, 2072, 1430,    0,  166,   26,
           13, 7418, 2493, 7599,   55, 3707,    8, 6501, 1192, 2805, 3555,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  89, 1276,   34,    0,  746,   12,    0,  133,  227,   15,    6,  235,
          366,   10,  284, 1276,    0,  148,   21, 4639,   34,   13,  277,  523,
         8822,  128,   80,    7,  733, 3993, 8185,  279,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    0,    7,  288,  207,   10,  175,  336,   17,   26,   10,  204,
         9481,    7,  422,  955,    8,   10, 9481,   17,  523,   12,    7, 6385,
          567,   12,    7,  572,   17, 1206,    6,   17,  955,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  67,   79,   24, 1005, 5993,   54,  143,    8,  143,    8,  143, 2718,
         7983,  199,    7, 4309,  143,   26,  838,    6,  416,  586,   54,  199,
            7, 1402,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  24,   66,  115,  660,   12,   80,  798,  215,    0,   10,  150,  138,
           17, 2422,   26,  521, 1015, 3127,   54,   10,    7, 2718, 8570,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 238,    0,   19, 2264,   20,  221,   62,  126,  391, 3676,    6,  106,
         2084, 1265,  128,    9,   22, 2147,  866,    6,  336,    7,  179,   17,
           19,  213,   10, 1452,   71,   25,   17,   25,   73, 4432,    9,  155,
          447, 3182,   10,   87,  143,   71,  908,    0,    2,    1,    1,    1,
            1],
        [  53,  154,   53,   11,   57,  142,   10,  635,  367,   10,  100,    7,
         1153,   53, 5248,   13,  277,  143,  254,    7,   91,   53,  859, 1708,
            0,   67,  117,   63,   86, 4233, 1490, 4050, 4839,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,   79,   13, 4620,    0,   70,   24,  323,   34,   24, 1781,    7,
          479,   12,  813,    0, 1250,   12, 2410,   26,  768,    0,   10,   91,
          206, 4912,   26,  768,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:3') tensor([44, 23, 22, 37, 20, 49, 44,  4, 37, 34, 35, 29, 26, 45, 35, 30],
       device='cuda:3') tensor([1.0000, 1.0000, 0.8931, 1.0000, 0.8325, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:3', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([[   8,    7,  613,   26,  172,   17,   24, 5018,   10,   51, 2693,  340,
         1105,   62,  131,    7,  179,    9, 1760, 1081,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  17,   11,    6,  100,   13, 3001,  166, 2750,   96,  110,   10, 1005,
          740,    0,  276,  929, 1618, 1775,    6,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  53,   77, 2721,   33, 3643,  746,   12,  227,  156,  829, 1492,  235,
           17,   53, 5332,   62, 1042,    7, 4638,  487,   10,  954,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,  245, 1346,   33,  120,   19,   34,   13, 1920,    0,    8,   19,
         3532,   21,    0,   67,   19,  144,  211,  479,   70,   21, 3463,    0,
           68,  194,   65,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 333, 1822,    0,    7,   81,  169,  875, 5852,   71,  883,  340, 2374,
            0,   71,   77, 3868,   12,  324,  214,    0,  388,   21,  199,    7,
           13,   59,  156,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   79,   24,   77, 1772,   10,  367,  270,   10,   17, 2827, 2434,
            0,  264, 5277,   55, 2332, 1772,   10, 4504,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,    7, 4112,   93,  279,    0,   19,   11,   48,  289,    0,   80,
          117,  248,  926,    6,   12,  108, 1377,   26,   53,  283,  131,  341,
         6027,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    0,   19,  220,  229,   13, 1678,  434,    0, 2180,   20, 2286,
         5575,    0,  206,   77,   25,  144,    0,   10,    0,   87,   34,  175,
           13,  759, 2180,   96,  109,    0,   13, 3619, 2180,   96,    0,    2,
            1],
        [  68,  194,   65,   94,  289,  708,   63,  775, 3800,    0,   67,   19,
           34,  492,  442, 2079,   12,   79,   13, 1269,  109,   39, 5488,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   7, 2327, 1034,  240,   20, 3791, 2523, 2396,   80,  149,    6,   96,
         3447,   54,   10,  283,  432,  535, 3557, 2033,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  33,   26,   86,   39, 4312,   22, 2481, 8068,    0,   33,   26,  116,
           13, 5225, 3743, 3784,   59, 2445,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   29,    0,   21,   34,  172,  251, 3974,    6,   17,  442,  110,
         4007,   10,  991,   13, 1236,  588, 2872,   80, 1111,  215,  581,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   19,  154,   19, 1157,   17,   79,   13, 1920,   25,  513,   10,
         7686,  341, 3061,    0,   73,   17,   51,  230,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [7987,    7,  179,   11,    6, 1195,   73,   51,  691,    0,  103,   25,
          172,  388,  155, 1066,   10,   21,    8,  368, 2736,    0, 8307,  941,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 125,    7, 3953, 2320,   29, 1019,    9,    7, 1406,    0,   19, 1918,
         1155, 1706,   12,   13,  183, 1677,   10, 2214,  134,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,  168,   24,  150,  391, 2110,    6,   12, 2394,   17,   21,  513,
          359,    9,  640,  215,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 120,   21,   11,    6, 2072,    0,    9,   13, 9134,    0,  103,   13,
          723, 3684,   22,   96,   13,   51,  727,   12,  688, 4481, 1004,  155,
         1893,    0,   25, 1704,   11,   18,  150,   21,    0,    2,    1,    1,
            1],
        [   8,   19,  154,    0,   13,  325,   12,  825,    0,   70,   24,   87,
           26,  116,    0,  746,   12,    0, 2215,    7, 6384,  132,   10,  108,
         7009,    6,    0,    8,  746,   12,  205,    0,   38,   48, 6684,    0,
            2],
        [   8,   19,   87, 1080,    0,  131,    7,  207,    0,   17,    7,  475,
         2476,   12,    7, 7996,   26,  142,   10,   66,    7,  370, 2958,   69,
           25,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,  180,    0,    7,  133,  473,  630,   46,   19,  169,  735,   66,
           10,  283, 1222,  132,   10,   33,  630,    0,  125,   19,   11,   45,
           86,   13, 7685,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 934,   17,  120,   24,  278,   69,   33, 4515,    0,   24,  162,   77,
         1800,  248,  574,   93,  292, 2287,  727, 2035,  221,  387,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,  250, 2414,  220, 1085,   55,    7, 8575,  853,  650, 2239,    9,
            7, 2541, 1305,   20,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,  278, 2672,   48,  131,    7,  417,    0,   59,    0,  336, 1056,
            9,    7, 2223,   10,  367,    8,  150,   13, 1365,   71,   13,  876,
          541,   20,  816,   56,  300,  140,   37,   69,  267, 3687,    0,    2,
            1],
        [ 117,  214,   63, 1794,   17,   63, 2770,   10, 9305,  244,  183,    0,
           10,  229,  170,  598,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:1') tensor([22, 21, 24, 28, 29, 22, 28, 36, 25, 23, 20, 25, 22, 26, 23, 18, 34, 37,
        27, 29, 25, 18, 36, 18], device='cuda:1') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8867, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:1',
       dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([[1886,  656,  187,  111,  115,    0, 2008, 1491,   34, 2439,   10, 7124,
           85,    7,  370,  183,   79, 1777,   11,    6, 3667,    0,    8,   29,
         4005,    0, 4749,    8, 4139,  162,  499,  185, 1871, 4941,   48,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   29,   21,   11,    6,  172,  528,   10,  135,  103,    7, 1107,
          895,   17,   25,   11,   57, 2807,   54, 2385,  736,  759, 1086,   10,
           26,   13,  324, 4502,   37,  148, 1847,  999, 3489,    6,  109,   13,
          999, 4502,   37,  148, 1847,  324, 3489,    6,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   91,   12,    7,  555,  825,   24,  150, 2178,  458,
            6, 3122,    9,    7,   89,  532,  693,    0,    8,   19,  446,   21,
          133, 1944,    0,   33,  277, 5082,    0,   67,   21,   11,    6,  113,
         8897,    0,  230,    0,   21, 2413,   10, 2644,  728,  128, 1032,   93,
           79,   13,  524,   12, 4093, 1604,  724,    0,   86, 6408,    0,    2],
        [  29,   10,  175,   10, 9171,   95,  266, 2346,  803,   11,    6, 1968,
          486,  179,    0, 2584,  119, 1573,    0,   24,  388,  540,   13, 6951,
           12, 3698,    6,    0,    8,   24, 1639,   69,   33,   13,  140, 1841,
            6, 2278,    0,    8,   21,   11,    6, 4402, 1222,    8,  421,  436,
          595,  626, 1251,    8, 6245, 1492,  158,  287, 1408,    0,    2,    1],
        [   8,  180, 1482, 7549,   62,    0,    7,  942,  552,  346,   69,  110,
            0,    8, 1223, 1742,  110,   13,  452,  235,  271, 1211,   17,   19,
          144,   10, 4988,   89, 5291,    0,  166,   33,  452,  235,  271,   34,
         5413,  199,   13,   82, 1486,   18,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,   73,   66, 1102, 3555,    9,  108,  906, 9269,   46,   17,  321,
            0,    0,    0,   12,    0,  524,  206,   24,   11,   57, 5655, 1102,
         4699,    6,  540,    0,    7, 6168,  838,  650,  833,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  84,    0,   34,   29,  261, 3454,    8, 1436,   22,  356, 6507,    0,
           84,   11,    6,    0,   29,  261, 2285,  214,  120,   25,  274,    9,
         1482,  994,   11,    6, 1893,    0,    0,  125,    9,    0,    7, 1492,
          658,   71,   17, 2701,    0,    0, 5543,    0,   17,   25,  492,  276,
          289,   91,    0, 1455,   46,  778,  956,    0,    2,    1,    1,    1],
        [   8,   55,    7,  427,  140,  371,  119,  160,  866,    0,   17, 3382,
         2527,    6,   10,  467,   13, 1905,  207,  333,  183,    0, 5833,  565,
         5275,   13,  325,   12,  183,    9,   33, 6411, 5331,    0,   39, 1829,
            8, 2079,  561,   17,   11,    6, 5020,  126,   12,    7, 1073,  841,
         4210,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:7') tensor([37, 46, 60, 59, 44, 35, 57, 51], device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8804, 0.8550, 1.0000],
       device='cuda:7', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:6', dtype=torch.float16)
True tensor([[  67,   19,   11,   45,  142,   10,  205,  244,    7, 7118,    0,  837,
            8,  746,   12,  206,   63,   24,   79,   13,  733,    0,  752,   10,
          175,   10,   17, 1760, 2805,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   70,   19,   11,   48,  100,   10,  575,   25,  115,    0,    9,
            7, 1323,   12, 1601,   19,   66,  859,    0,   26,   13, 1819,   17,
           13, 3173,  442,    9, 2135,  436,  627,  290,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,    0,    0,   33,   26,   77,  324,    0,   67,  120,   25,
          498,  826,   80,   33,    0,   25,   66,   10, 1452,  117, 2522,   71,
           94,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  225,    8,  267, 1037, 1509,   62, 7723,  359,    7, 4987,  199,
         2266, 1124,  221,    8,  225,   11,    6,  226, 1074,    9,   33, 3539,
           55,    7,  473,  464,    8,   13,  854,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   79,   24, 2277,    0,   24, 1772,   10,  661,    7, 4690,  800,
           12,  251,  148,   66, 1094,   10,   51,  388,   10, 1590,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 700,  608,   46,    8,  225,   26,   91,   12,    7,  281, 1284, 3144,
            6,   69,    7,  984,   46,    8,   19,  246,   10, 1222,    0,  116,
           87,   33, 5830,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   34,   29, 4876,   62,    0,   33,   34,    7,  245,  183,   19,
         1346,   17,   94,    9,   89,  753,  162, 5187,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  703,   24,   11,   57,   85,   13, 1160, 4635,  613,  206,   33,
           26,  115,  250,   17,   24,   73,   11,   18, 4958,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53,  296, 3254,    9,    7, 1136,  958, 5055,  109,    9, 1752, 7311,
          958, 1740,    6,   10,  388,  159, 3707,    8, 7552,   10,  283,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 659,   25,    0,  446,   13, 1720,   48, 3997,   48,   48,  232,   10,
         3793,    9,  185, 4529,    0,    0,  109,  446,   13,  207,    0,    0,
           10,  175,    0,  244,    0,  336,    0,  109,  359,  419, 1832,   17,
            0, 1109,    6,  629,   25,    8,   91,   12,  155, 4989,    0,    2],
        [  67,   24,  692,   21,   10,  274,   85, 7224,    0,   85, 4371, 7224,
            0,   29,   17,   24,  220, 1157,    7, 2672,  106,  490,    8,  432,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   11,  121, 5915,   13, 1336,  608,   17, 1772,    6,    9,    7,
          179,   12,    7,  133,  822,    0,    8, 3411,  356,    6,  170,   10,
            7,  179,   12,    7, 1019,  858,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33, 1645,   26,  908,  916,    0,   67,   24,   11,   57,  752,   10,
          175,    7, 4341,   10,   77,  283,  540,    8,   21,   11,    6,   86,
         1829,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   86,  250, 3926,   10, 2048,   46,   21,   11,    6,
           13,  172, 4748, 1588,  166,  172,  601,    6,  170, 9878,  138,   24,
          598,    8, 1073,  170,  598,  509,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,   91,  207,   10,    0, 3231,    0,   80,    0,    0, 7079,    6,
           26,   10,  269,   20,  155, 3742, 6388,    8, 2632,   10,    7, 3413,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  79,   19, 1808,   10,  154,   80, 1352,  100,   33,    8, 1510,   80,
         1352,  100,   33,    0,   19, 1808,   10,  884, 1222,    8,  934,    0,
           26,   84,   39, 3993,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:6') tensor([31, 34, 27, 33, 24, 29, 22, 23, 25, 48, 26, 32, 27, 32, 27, 30],
       device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8623, 1.0000, 1.0000, 1.0000, 1.0000, 0.8311, 1.0000],
       device='cuda:6', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([[  24,   11,   48,  204,  116,  289,    0, 1189,    0,  339,   11,    6,
         1531,  126,    7, 1587,   12, 2671,   24,   73,   11,   18, 1799,   71,
            0,    7, 1587,   12, 1422,    6,   17,   63,  999,   46,  175, 4532,
           12,  251,    0,  694,  214,  509,    0,    8,   24,  451,   51,    7,
          211, 1755, 1369,   17,   24, 2438, 1469,   10,   51,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [   8,   29,    0,   25,  135,    0,   55,  663,    0,   71,  117, 5259,
            0, 2116,   73,  150,  946,  214,  100, 5368,  688,    6,    8,  747,
         7692, 4389,    6,    0,   86,  133,  261,  143,    0,   29, 1155, 1387,
           10, 1417, 2805,  188,  226, 1254,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  33,   26,   91,    0,  593,    0,  125,  120,    0,   24,    0,  661,
          138, 3257,  283,    0,    0,   24,   11,  158,    0,   51,  529,   10,
          991, 3021, 3927,    0,    0,   17,   11,    6,   13,  324,  279,   69,
            0,    7,  733,    0,   71, 4960,    0, 3148,    6,   10, 1504,    0,
          116,  100,   13, 2570,  639,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   7,  133, 2980,    6,   17,  619,   10, 7572,   51,   96,   63,  115,
         8633,  896, 4987,    6,    0, 9090,   48,  131,   91,  109,  248, 2685,
         1369,  100, 8251,    8,   29,   93,  242,  221,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   8,  333,  183,   84,   11,    6,   13, 6249,   46,   85,   21,    6,
         5807,   84,   11,    6,   13, 6249,   46,    8,  103,   24,  135,  995,
           80,    7, 6249,    0,  166,   24,   87,    0,  125,   84,   11,    6,
           13, 1665,    6,  803,  527, 2032,  445,    8,  778,    0,   25,  388,
            9,    7, 2475,   12, 6249,    8,    7, 1650,    0,   29,   33,   26,
            7,  475, 2015,  271,    0,    2],
        [1609,   10,    0,  618,   69,  159,  447,   46,    8,   21,   11,  158,
          305,    0, 1323,   12,  143,  215,   10,  339,  134, 1570,   69,  159,
          447,    0,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,    0,   79,   24, 3121,    7, 3317,   12,  108, 4828,    0,   21,
         2882, 2749,  528,   17,   70,   25, 1452, 2882,    7, 3317,   12, 4240,
         5898,    0, 2882,    7, 3317,    9,  166,   24,  618,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   0,  274,   85,   33,    0, 2993,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:2') tensor([59, 44, 55, 35, 66, 28, 35,  8], device='cuda:2') tensor([1.0000, 1.0000, 0.8545, 1.0000, 1.0000, 0.8555, 1.0000, 0.8916],
       device='cuda:2', dtype=torch.float16)
True tensor([[  84,   11,    6,  ...,    1,    1,    1],
        [6339,  128, 6870,  ...,    1,    1,    1],
        [   7, 3187,   91,  ...,    1,    1,    1],
        ...,
        [  84,   11,    6,  ...,   21,    0,    2],
        [4402,    0,   53,  ...,    1,    1,    1],
        [   8,   19,  154,  ...,    1,    1,    1]], device='cuda:2') tensor([23, 23, 18, 26, 17, 19, 18, 26, 16, 22, 18, 15, 22, 11, 18, 24, 14, 21,
         7, 12, 20, 23, 20, 26, 22, 13, 24, 14, 25, 24, 26, 16, 25, 20, 15, 27,
        22, 27,  9, 24], device='cuda:2') tensor([1.0000, 0.8857, 1.0000, 1.0000, 1.0000, 0.8779, 0.8940, 0.8140, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8628, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8633, 1.0000, 0.8013, 1.0000, 0.8447, 0.8203, 0.8486,
        1.0000, 1.0000, 0.8628, 1.0000, 1.0000, 0.8564, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:2', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([[  19,  487, 2861,   54,   13,   37, 1010, 1931,    6,   12, 8188, 1793,
            8, 2008, 7646, 3717,    0,  288,    7, 2593, 1649,  162, 2084, 1625,
          128,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 185,  583,   21,   38,  469, 5867,    0,  832,   18,  233,  429,    3,
           29,    0, 5413,   69,   13, 3228, 5627,    0,  506,   86,  116, 3420,
            0,    9, 1218,  121, 2227, 1068,    0,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   84,   34,   33, 1678,    9, 6766,  564,  868, 1132,   17,  487,
           69,    7, 1575,   12, 6766,    8, 3699,   69,    7, 1802,  322,   12,
         6766,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   9,   17,  841,    0, 7154,  445,    0, 4828,    8,  837,   56, 9826,
           63,   77,    7, 2231,   59,    8, 2913,   12, 4089, 3135, 1144,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   0,   29,    0,   19,  164,   46, 1313,   19,  217,   86,  963,    8,
            0,  217,    0,  133,  801,    0,   19,  499,  164,  289,  250,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   9,  185, 3885,    0,   19,  135,   17,  294,   12,   25,   63, 3006,
            9, 3221,   35, 1236,  543,  234,  128, 9056,    6,    0,   25,   11,
           57, 3006,    9, 1288,   12, 4564,    8, 2462,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 415,  636,   11,    6, 2786,   26, 5040,    0,  125,  103,   24,   73,
         7394,   21,    0, 1082,  106,   21,    0,  180,   24,   73, 2554,  931,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,   19,  278,  126,   12,  670,    0,    0,   19,  278,   13, 1329,
            0,   19,  278,   13, 1703,  311, 2197,    0,    0,    0,    0,    0,
            0,   19,  278, 1222,   10,    7,  998,    0, 3008,    0,    8,   19,
          116,  513, 2073,   18,    6,    0, 1713,   54,  227, 2174, 3688,    0,
            2],
        [3524,    0,  929,  419,  601,  109, 2750,  106,  267, 3563, 3460,   85,
           77,    0,  217,   93, 4606,   62,  518,   12,    7, 1219,   12,    7,
          697, 2023, 2201,    8, 1070,   62,   71,   33, 5471,   69,    7, 3686,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  33,  168,   26,   13, 7416,    9,   89, 4221,   17,   19,   66, 4640,
           48,   71,   55,   13,  535,    0,  535,  183,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,    0,   79,  214,   63,   13,  277, 4865,    9, 1662,    0,    7,
         2696,   34, 9408,   48, 1241,   13, 1822,    8,   24,  162,  740,    0,
           68,  194,   65,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,   53, 8312, 2361,  565,    9,  554,    0,    8,   33,  183,   53,
          619, 1498, 1047, 2108,  543,    6,   55,  854,   13,  744,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [3575,    8, 1244,   35, 1218, 1105,   48, 1068,    0, 4313,  708,   17,
           26,   86,  100, 4313,  134,  138,   10, 1764, 1949,   13, 1726,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   9,   89, 1466,    0,    0,    0,   86,  288,   26,  948,  415,   59,
          292,    6,  366,    0,   10, 3334,    0, 3334,   26,  415,   59,  292,
            6,  366,    0,   10,  948,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 120,   19,  323,   33,   91,    0,   13,  184,   22,   18,  232,  684,
         1110,   21,    8,  246,   10,  110,    0,   38,  469,   57,   11,    6,
          211,  207,   25,   73,   87,   33,    0,   25, 1412,   66,  619,  185,
          321,   12, 1677,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  17,  963,   81, 1359,   11,   18,   13, 5911,    0,  101, 1359,   11,
           18,   13, 1201,  684,    0,   13, 4186,  230,    6, 7933,  109,   13,
         5384, 5949,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:5') tensor([27, 33, 27, 25, 25, 34, 26, 49, 38, 22, 28, 24, 25, 31, 41, 28],
       device='cuda:5') tensor([1.0000, 0.8687, 1.0000, 1.0000, 0.8394, 1.0000, 1.0000, 0.8105, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8281, 1.0000, 1.0000],
       device='cuda:5', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2023-08-20 01:44:21 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 11.119 | trans_loss 12.315 | nll_loss 11.742 | w2v_ctc_loss 4.771 | task_loss 7.545 | contrastive_loss 3.779 | total 4003.4 | n_correct 137.8 | ppl 3425.82 | accuracy 3.442 | uer 59.939 | wer 60.956 | raw_wer 60.956 | bleu 0 | wps 1049.4 | wpb 4003.4 | bsz 141.8 | num_updates 1472
2023-08-20 01:44:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1472 updates
2023-08-20 01:44:21 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 01:44:23 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 01:44:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt (epoch 1 @ 1472 updates, score 0.0) (writing took 4.9660593459848315 seconds)
2023-08-20 01:44:25 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-08-20 01:44:25 | INFO | train | epoch 001 | loss 9.736 | trans_loss 5.814 | nll_loss 4.694 | w2v_ctc_loss 6.812 | task_loss 1.361 | contrastive_loss 3.051 | total 4139.23 | n_correct 112.688 | ppl 25.89 | accuracy 2.722 | wps 19647.6 | ups 1.59 | wpb 12357.5 | bsz 458.7 | num_updates 1472 | lr 5.89506e-05 | gnorm 1.503 | clip 1.2 | loss_scale 32 | train_wall 868 | gb_free 18.8 | wall 982
2023-08-20 01:44:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 01:44:26 | INFO | fairseq.trainer | begin training epoch 2
2023-08-20 01:44:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 01:44:49 | INFO | train_inner | epoch 002:     28 / 1474 loss=7.302, trans_loss=5.75, nll_loss=4.632, w2v_ctc_loss=3.617, task_loss=1.205, contrastive_loss=2.704, total=4164.65, n_correct=122.41, ppl=24.8, accuracy=2.939, wps=10575.4, ups=0.85, wpb=12421.6, bsz=471.6, num_updates=1500, lr=6.007e-05, gnorm=1.413, clip=0, loss_scale=32, train_wall=58, gb_free=18.6, wall=1006
2023-08-20 01:45:48 | INFO | train_inner | epoch 002:    128 / 1474 loss=7.186, trans_loss=5.736, nll_loss=4.615, w2v_ctc_loss=3.565, task_loss=1.299, contrastive_loss=2.577, total=4153.14, n_correct=122.58, ppl=24.51, accuracy=2.952, wps=21014.2, ups=1.7, wpb=12386.1, bsz=450.6, num_updates=1600, lr=6.4068e-05, gnorm=1.259, clip=0, loss_scale=32, train_wall=58, gb_free=19.5, wall=1065
2023-08-20 01:46:47 | INFO | train_inner | epoch 002:    228 / 1474 loss=7.156, trans_loss=5.759, nll_loss=4.648, w2v_ctc_loss=3.446, task_loss=1.106, contrastive_loss=2.656, total=4192.9, n_correct=119.88, ppl=25.07, accuracy=2.859, wps=21295.2, ups=1.7, wpb=12523.7, bsz=493, num_updates=1700, lr=6.8066e-05, gnorm=1.323, clip=0, loss_scale=32, train_wall=58, gb_free=18.9, wall=1124
2023-08-20 01:47:47 | INFO | train_inner | epoch 002:    328 / 1474 loss=6.996, trans_loss=5.735, nll_loss=4.617, w2v_ctc_loss=3.42, task_loss=1.307, contrastive_loss=2.425, total=4132.45, n_correct=124.06, ppl=24.54, accuracy=3.002, wps=20717.8, ups=1.68, wpb=12335.7, bsz=443.2, num_updates=1800, lr=7.2064e-05, gnorm=1.246, clip=0, loss_scale=32, train_wall=59, gb_free=19.1, wall=1183
2023-08-20 01:48:45 | INFO | train_inner | epoch 002:    428 / 1474 loss=6.874, trans_loss=5.705, nll_loss=4.583, w2v_ctc_loss=3.381, task_loss=1.412, contrastive_loss=2.292, total=4037.61, n_correct=127.11, ppl=23.96, accuracy=3.148, wps=20590.5, ups=1.71, wpb=12068.2, bsz=415.8, num_updates=1900, lr=7.6062e-05, gnorm=1.202, clip=0, loss_scale=32, train_wall=58, gb_free=19.6, wall=1242
2023-08-20 01:49:44 | INFO | train_inner | epoch 002:    528 / 1474 loss=6.837, trans_loss=5.719, nll_loss=4.598, w2v_ctc_loss=3.28, task_loss=1.224, contrastive_loss=2.397, total=4183.4, n_correct=127.01, ppl=24.21, accuracy=3.036, wps=21350.6, ups=1.71, wpb=12481.5, bsz=470.3, num_updates=2000, lr=8.006e-05, gnorm=1.172, clip=0, loss_scale=32, train_wall=58, gb_free=18.6, wall=1300
2023-08-20 01:49:44 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 01:50:30 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 10.682 | trans_loss 12.126 | nll_loss 11.514 | w2v_ctc_loss 4.319 | task_loss 7.545 | contrastive_loss 3.329 | total 4003.4 | n_correct 151 | ppl 2925.05 | accuracy 3.772 | uer 55.392 | wer 56.005 | raw_wer 56.005 | bleu 0 | wps 1063.2 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_bleu 0
2023-08-20 01:50:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-08-20 01:50:30 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_2_2000.pt
2023-08-20 01:50:32 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_2_2000.pt
2023-08-20 01:50:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.0) (writing took 13.479496810992714 seconds)
2023-08-20 01:51:42 | INFO | train_inner | epoch 002:    628 / 1474 loss=6.724, trans_loss=5.717, nll_loss=4.595, w2v_ctc_loss=3.224, task_loss=1.265, contrastive_loss=2.225, total=4126.46, n_correct=124.36, ppl=24.17, accuracy=3.014, wps=10438.7, ups=0.85, wpb=12316, bsz=446.7, num_updates=2100, lr=8.4058e-05, gnorm=1.25, clip=0, loss_scale=32, train_wall=58, gb_free=19, wall=1418
2023-08-20 01:52:40 | INFO | train_inner | epoch 002:    728 / 1474 loss=6.664, trans_loss=5.693, nll_loss=4.566, w2v_ctc_loss=3.191, task_loss=1.256, contrastive_loss=2.297, total=4148.66, n_correct=124.84, ppl=23.68, accuracy=3.009, wps=21169.9, ups=1.71, wpb=12384.1, bsz=462.9, num_updates=2200, lr=8.8056e-05, gnorm=1.293, clip=0, loss_scale=64, train_wall=58, gb_free=19.4, wall=1477
2023-08-20 01:53:40 | INFO | train_inner | epoch 002:    828 / 1474 loss=6.589, trans_loss=5.685, nll_loss=4.558, w2v_ctc_loss=3.156, task_loss=1.287, contrastive_loss=2.249, total=4164.61, n_correct=128.22, ppl=23.56, accuracy=3.079, wps=20854, ups=1.68, wpb=12439.5, bsz=459.2, num_updates=2300, lr=9.2054e-05, gnorm=1.168, clip=0, loss_scale=64, train_wall=59, gb_free=19.2, wall=1536
2023-08-20 01:54:39 | INFO | train_inner | epoch 002:    928 / 1474 loss=6.497, trans_loss=5.67, nll_loss=4.54, w2v_ctc_loss=3.103, task_loss=1.296, contrastive_loss=2.206, total=4109.63, n_correct=127.79, ppl=23.26, accuracy=3.11, wps=20855.9, ups=1.7, wpb=12270.1, bsz=447.9, num_updates=2400, lr=9.6052e-05, gnorm=1.297, clip=0, loss_scale=64, train_wall=58, gb_free=18.6, wall=1595
2023-08-20 01:55:37 | INFO | train_inner | epoch 002:   1028 / 1474 loss=6.415, trans_loss=5.674, nll_loss=4.545, w2v_ctc_loss=3.06, task_loss=1.281, contrastive_loss=2.091, total=4101.19, n_correct=120.53, ppl=23.35, accuracy=2.939, wps=20969.9, ups=1.71, wpb=12245.2, bsz=454.5, num_updates=2500, lr=0.00010005, gnorm=1.206, clip=0, loss_scale=64, train_wall=58, gb_free=18.7, wall=1654
2023-08-20 01:56:36 | INFO | train_inner | epoch 002:   1128 / 1474 loss=6.405, trans_loss=5.681, nll_loss=4.555, w2v_ctc_loss=3.013, task_loss=1.142, contrastive_loss=2.235, total=4192.73, n_correct=124.1, ppl=23.5, accuracy=2.96, wps=21279.4, ups=1.7, wpb=12513.6, bsz=488.9, num_updates=2600, lr=0.000104048, gnorm=1.094, clip=0, loss_scale=64, train_wall=58, gb_free=18.5, wall=1713
2023-08-20 01:57:35 | INFO | train_inner | epoch 002:   1228 / 1474 loss=6.335, trans_loss=5.676, nll_loss=4.548, w2v_ctc_loss=2.99, task_loss=1.164, contrastive_loss=2.157, total=4219.96, n_correct=125.85, ppl=23.4, accuracy=2.982, wps=21251.1, ups=1.69, wpb=12591.9, bsz=491.8, num_updates=2700, lr=0.000108046, gnorm=1.167, clip=0, loss_scale=64, train_wall=59, gb_free=18.6, wall=1772
2023-08-20 01:58:34 | INFO | train_inner | epoch 002:   1328 / 1474 loss=6.219, trans_loss=5.658, nll_loss=4.529, w2v_ctc_loss=2.974, task_loss=1.216, contrastive_loss=1.894, total=4163.26, n_correct=129.4, ppl=23.09, accuracy=3.108, wps=21028.8, ups=1.69, wpb=12441.6, bsz=463, num_updates=2800, lr=0.000112044, gnorm=1.084, clip=0, loss_scale=64, train_wall=58, gb_free=18.6, wall=1831
2023-08-20 01:59:33 | INFO | train_inner | epoch 002:   1428 / 1474 loss=6.112, trans_loss=5.634, nll_loss=4.498, w2v_ctc_loss=2.956, task_loss=1.376, contrastive_loss=1.916, total=4049.42, n_correct=127.01, ppl=22.6, accuracy=3.136, wps=20605.5, ups=1.7, wpb=12091.6, bsz=437.6, num_updates=2900, lr=0.000116042, gnorm=1.208, clip=0, loss_scale=64, train_wall=58, gb_free=18.9, wall=1890
2023-08-20 02:00:00 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 02:00:44 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 10.096 | trans_loss 11.925 | nll_loss 11.273 | w2v_ctc_loss 3.841 | task_loss 7.545 | contrastive_loss 2.55 | total 4003.4 | n_correct 147.8 | ppl 2474.29 | accuracy 3.692 | uer 50.498 | wer 50.438 | raw_wer 50.438 | bleu 0 | wps 1120 | wpb 4003.4 | bsz 141.8 | num_updates 2946 | best_bleu 0
2023-08-20 02:00:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2946 updates
2023-08-20 02:00:44 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 02:00:51 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 02:00:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt (epoch 2 @ 2946 updates, score 0.0) (writing took 13.674042888975237 seconds)
2023-08-20 02:00:58 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-08-20 02:00:58 | INFO | train | epoch 002 | loss 6.639 | trans_loss 5.695 | nll_loss 4.57 | w2v_ctc_loss 3.196 | task_loss 1.255 | contrastive_loss 2.256 | total 4138.65 | n_correct 125.31 | ppl 23.76 | accuracy 3.028 | wps 18355.8 | ups 1.49 | wpb 12355.8 | bsz 458.5 | num_updates 2946 | lr 0.000117881 | gnorm 1.218 | clip 0 | loss_scale 64 | train_wall 857 | gb_free 18.9 | wall 1974
2023-08-20 02:00:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 02:00:58 | INFO | fairseq.trainer | begin training epoch 3
2023-08-20 02:00:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 02:01:36 | INFO | train_inner | epoch 003:     54 / 1474 loss=6.047, trans_loss=5.637, nll_loss=4.501, w2v_ctc_loss=2.909, task_loss=1.292, contrastive_loss=1.806, total=4067, n_correct=129.41, ppl=22.64, accuracy=3.182, wps=9824.9, ups=0.81, wpb=12142, bsz=441.4, num_updates=3000, lr=0.00012004, gnorm=1.182, clip=0, loss_scale=64, train_wall=58, gb_free=18.8, wall=2013
2023-08-20 02:01:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-20 02:01:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-20 02:01:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-20 02:01:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-20 02:01:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-20 02:01:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-08-20 02:02:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-08-20 02:02:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-08-20 02:03:08 | INFO | train_inner | epoch 003:    162 / 1474 loss=5.273, trans_loss=4.586, nll_loss=3.117, w2v_ctc_loss=2.98, task_loss=0.865, contrastive_loss=1.703, total=4150.37, n_correct=854.33, ppl=8.67, accuracy=20.584, wps=13540.8, ups=1.09, wpb=12395.6, bsz=462.8, num_updates=3100, lr=0.000124038, gnorm=10.095, clip=34, loss_scale=0.25, train_wall=91, gb_free=16.6, wall=2105
2023-08-20 02:04:36 | INFO | train_inner | epoch 003:    262 / 1474 loss=4.576, trans_loss=4.202, nll_loss=2.618, w2v_ctc_loss=2.743, task_loss=0.892, contrastive_loss=1.421, total=4149.55, n_correct=1255.87, ppl=6.14, accuracy=30.265, wps=14163, ups=1.14, wpb=12397.1, bsz=463.2, num_updates=3200, lr=0.000128036, gnorm=6.611, clip=13, loss_scale=0.25, train_wall=87, gb_free=16.1, wall=2192
2023-08-20 02:06:02 | INFO | train_inner | epoch 003:    362 / 1474 loss=4.327, trans_loss=4.167, nll_loss=2.57, w2v_ctc_loss=2.602, task_loss=0.86, contrastive_loss=1.352, total=4175.96, n_correct=1329.58, ppl=5.94, accuracy=31.839, wps=14450.4, ups=1.16, wpb=12461.5, bsz=474.2, num_updates=3300, lr=0.000132034, gnorm=3.744, clip=3, loss_scale=0.25, train_wall=86, gb_free=16.1, wall=2279
2023-08-20 02:07:28 | INFO | train_inner | epoch 003:    462 / 1474 loss=4.076, trans_loss=4.142, nll_loss=2.54, w2v_ctc_loss=2.507, task_loss=0.879, contrastive_loss=1.011, total=4187.52, n_correct=1363.82, ppl=5.82, accuracy=32.569, wps=14536.7, ups=1.16, wpb=12498.8, bsz=463.6, num_updates=3400, lr=0.000136032, gnorm=3.296, clip=2, loss_scale=0.25, train_wall=85, gb_free=13.9, wall=2365
2023-08-20 02:08:54 | INFO | train_inner | epoch 003:    562 / 1474 loss=3.838, trans_loss=4.13, nll_loss=2.528, w2v_ctc_loss=2.349, task_loss=0.956, contrastive_loss=0.902, total=4083.21, n_correct=1345.02, ppl=5.77, accuracy=32.94, wps=14183.8, ups=1.16, wpb=12199, bsz=437.9, num_updates=3500, lr=0.00014003, gnorm=2.249, clip=1, loss_scale=0.25, train_wall=85, gb_free=15.5, wall=2451
2023-08-20 02:10:20 | INFO | train_inner | epoch 003:    662 / 1474 loss=3.724, trans_loss=4.124, nll_loss=2.514, w2v_ctc_loss=2.235, task_loss=0.825, contrastive_loss=0.939, total=4232.39, n_correct=1423.94, ppl=5.71, accuracy=33.644, wps=14578.5, ups=1.16, wpb=12618.9, bsz=487.6, num_updates=3600, lr=0.000144028, gnorm=1.906, clip=0, loss_scale=0.25, train_wall=86, gb_free=16.1, wall=2537
2023-08-20 02:11:47 | INFO | train_inner | epoch 003:    762 / 1474 loss=3.564, trans_loss=4.1, nll_loss=2.488, w2v_ctc_loss=2.175, task_loss=0.852, contrastive_loss=0.645, total=4155.31, n_correct=1417.86, ppl=5.61, accuracy=34.122, wps=14336.5, ups=1.15, wpb=12412.9, bsz=465.8, num_updates=3700, lr=0.000148026, gnorm=2.041, clip=1, loss_scale=0.25, train_wall=86, gb_free=16, wall=2624
2023-08-20 02:13:14 | INFO | train_inner | epoch 003:    862 / 1474 loss=3.459, trans_loss=4.099, nll_loss=2.486, w2v_ctc_loss=2.109, task_loss=0.885, contrastive_loss=0.569, total=4170.95, n_correct=1432.97, ppl=5.6, accuracy=34.356, wps=14371.5, ups=1.15, wpb=12453.7, bsz=458.1, num_updates=3800, lr=0.000152024, gnorm=1.661, clip=0, loss_scale=0.25, train_wall=86, gb_free=17, wall=2710
2023-08-20 02:14:40 | INFO | train_inner | epoch 003:    962 / 1474 loss=3.39, trans_loss=4.085, nll_loss=2.465, w2v_ctc_loss=2.049, task_loss=0.84, contrastive_loss=0.575, total=4174.19, n_correct=1468.62, ppl=5.52, accuracy=35.183, wps=14416.9, ups=1.16, wpb=12449.2, bsz=475.7, num_updates=3900, lr=0.000156022, gnorm=1.664, clip=0, loss_scale=0.25, train_wall=86, gb_free=13.3, wall=2797
2023-08-20 02:16:05 | INFO | train_inner | epoch 003:   1062 / 1474 loss=3.324, trans_loss=4.078, nll_loss=2.459, w2v_ctc_loss=2.039, task_loss=0.955, contrastive_loss=0.476, total=4049.41, n_correct=1420.26, ppl=5.5, accuracy=35.073, wps=14152.8, ups=1.17, wpb=12096, bsz=436.3, num_updates=4000, lr=0.00016002, gnorm=1.713, clip=1, loss_scale=0.25, train_wall=85, gb_free=16, wall=2882
2023-08-20 02:16:05 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 02:16:42 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.773 | trans_loss 7.23 | nll_loss 5.11 | w2v_ctc_loss 2.404 | task_loss 4.098 | contrastive_loss 0.643 | total 4003.4 | n_correct 1465 | ppl 34.53 | accuracy 36.594 | uer 33.977 | wer 35.055 | raw_wer 35.055 | bleu 0.53 | wps 1395.9 | wpb 4003.4 | bsz 141.8 | num_updates 4000 | best_bleu 0.53
2023-08-20 02:16:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-08-20 02:16:42 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_3_4000.pt
2023-08-20 02:16:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_3_4000.pt
2023-08-20 02:16:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 0.53) (writing took 13.917077030986547 seconds)
2023-08-20 02:18:20 | INFO | train_inner | epoch 003:   1162 / 1474 loss=3.241, trans_loss=4.075, nll_loss=2.453, w2v_ctc_loss=1.964, task_loss=0.944, contrastive_loss=0.431, total=4044.1, n_correct=1435.7, ppl=5.48, accuracy=35.501, wps=8946.4, ups=0.74, wpb=12070.2, bsz=433.6, num_updates=4100, lr=0.000164018, gnorm=1.442, clip=0, loss_scale=0.25, train_wall=84, gb_free=15.2, wall=3017
2023-08-20 02:19:46 | INFO | train_inner | epoch 003:   1262 / 1474 loss=3.189, trans_loss=4.063, nll_loss=2.44, w2v_ctc_loss=1.922, task_loss=0.938, contrastive_loss=0.405, total=4065.1, n_correct=1451.05, ppl=5.42, accuracy=35.695, wps=14135.2, ups=1.16, wpb=12140.5, bsz=432.3, num_updates=4200, lr=0.000168016, gnorm=1.532, clip=1, loss_scale=0.25, train_wall=85, gb_free=16.8, wall=3103
2023-08-20 02:21:13 | INFO | train_inner | epoch 003:   1362 / 1474 loss=3.173, trans_loss=4.051, nll_loss=2.423, w2v_ctc_loss=1.887, task_loss=0.886, contrastive_loss=0.506, total=4132.35, n_correct=1501.97, ppl=5.36, accuracy=36.347, wps=14275, ups=1.16, wpb=12335.6, bsz=462.1, num_updates=4300, lr=0.000172014, gnorm=1.447, clip=0, loss_scale=0.25, train_wall=86, gb_free=17.3, wall=3189
2023-08-20 02:22:39 | INFO | train_inner | epoch 003:   1462 / 1474 loss=3.13, trans_loss=4.043, nll_loss=2.414, w2v_ctc_loss=1.852, task_loss=0.84, contrastive_loss=0.467, total=4206.88, n_correct=1541.18, ppl=5.33, accuracy=36.635, wps=14579.5, ups=1.16, wpb=12566, bsz=476, num_updates=4400, lr=0.000176012, gnorm=1.474, clip=1, loss_scale=0.25, train_wall=85, gb_free=14.7, wall=3276
2023-08-20 02:22:49 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 02:23:25 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.577 | trans_loss 7.104 | nll_loss 4.943 | w2v_ctc_loss 2.164 | task_loss 4.101 | contrastive_loss 0.527 | total 4003.4 | n_correct 1526.6 | ppl 30.76 | accuracy 38.133 | uer 31.335 | wer 32.527 | raw_wer 32.527 | bleu 1.18 | wps 1373.6 | wpb 4003.4 | bsz 141.8 | num_updates 4412 | best_bleu 1.18
2023-08-20 02:23:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4412 updates
2023-08-20 02:23:25 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 02:23:33 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 02:23:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt (epoch 3 @ 4412 updates, score 1.18) (writing took 14.01067502197111 seconds)
2023-08-20 02:23:39 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-08-20 02:23:39 | INFO | train | epoch 003 | loss 3.816 | trans_loss 4.194 | nll_loss 2.609 | w2v_ctc_loss 2.265 | task_loss 0.9 | contrastive_loss 0.854 | total 4139.16 | n_correct 1328.97 | ppl 6.1 | accuracy 32.107 | wps 13303.7 | ups 1.08 | wpb 12357.6 | bsz 458.8 | num_updates 4412 | lr 0.000176492 | gnorm 2.838 | clip 3.9 | loss_scale 0.25 | train_wall 1243 | gb_free 15.9 | wall 3336
2023-08-20 02:23:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 02:23:40 | INFO | fairseq.trainer | begin training epoch 4
2023-08-20 02:23:40 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 02:25:01 | INFO | train_inner | epoch 004:     88 / 1474 loss=3.018, trans_loss=4.026, nll_loss=2.388, w2v_ctc_loss=1.795, task_loss=0.929, contrastive_loss=0.299, total=4082.27, n_correct=1512.49, ppl=5.23, accuracy=37.05, wps=8559.9, ups=0.7, wpb=12182.1, bsz=434.2, num_updates=4500, lr=0.00018001, gnorm=1.348, clip=0, loss_scale=0.25, train_wall=84, gb_free=17.5, wall=3418
2023-08-20 02:26:26 | INFO | train_inner | epoch 004:    188 / 1474 loss=3.004, trans_loss=4.003, nll_loss=2.361, w2v_ctc_loss=1.777, task_loss=0.837, contrastive_loss=0.332, total=4184.92, n_correct=1578.57, ppl=5.14, accuracy=37.72, wps=14689.7, ups=1.18, wpb=12496.9, bsz=470.3, num_updates=4600, lr=0.000184008, gnorm=1.489, clip=1, loss_scale=0.25, train_wall=84, gb_free=12.7, wall=3503
2023-08-20 02:27:53 | INFO | train_inner | epoch 004:    288 / 1474 loss=3.049, trans_loss=4.005, nll_loss=2.365, w2v_ctc_loss=1.808, task_loss=0.877, contrastive_loss=0.46, total=4150, n_correct=1567.4, ppl=5.15, accuracy=37.769, wps=14243.5, ups=1.15, wpb=12397.9, bsz=465.2, num_updates=4700, lr=0.000188006, gnorm=1.894, clip=1, loss_scale=0.25, train_wall=86, gb_free=16.5, wall=3590
2023-08-20 02:29:18 | INFO | train_inner | epoch 004:    388 / 1474 loss=2.959, trans_loss=3.998, nll_loss=2.351, w2v_ctc_loss=1.755, task_loss=0.925, contrastive_loss=0.284, total=4114.32, n_correct=1570.15, ppl=5.1, accuracy=38.163, wps=14418.3, ups=1.17, wpb=12277.1, bsz=440, num_updates=4800, lr=0.000192004, gnorm=1.379, clip=0, loss_scale=0.25, train_wall=84, gb_free=11.6, wall=3675
2023-08-20 02:30:45 | INFO | train_inner | epoch 004:    488 / 1474 loss=3.021, trans_loss=3.981, nll_loss=2.33, w2v_ctc_loss=1.717, task_loss=0.782, contrastive_loss=0.705, total=4239.74, n_correct=1650.21, ppl=5.03, accuracy=38.922, wps=14620.6, ups=1.16, wpb=12652.2, bsz=506.4, num_updates=4900, lr=0.000196002, gnorm=1.382, clip=0, loss_scale=0.25, train_wall=86, gb_free=16.6, wall=3762
2023-08-20 02:32:11 | INFO | train_inner | epoch 004:    588 / 1474 loss=2.92, trans_loss=3.957, nll_loss=2.3, w2v_ctc_loss=1.715, task_loss=0.83, contrastive_loss=0.338, total=4219.26, n_correct=1672.75, ppl=4.93, accuracy=39.646, wps=14586.3, ups=1.16, wpb=12596.5, bsz=483.8, num_updates=5000, lr=0.0002, gnorm=1.264, clip=0, loss_scale=0.25, train_wall=86, gb_free=15.8, wall=3848
mt_weight tensor(0.5000)
asr_weight tensor(0.4415, device='cuda:0')
2023-08-20 02:33:38 | INFO | train_inner | epoch 004:    688 / 1474 loss=2.901, trans_loss=3.955, nll_loss=2.292, w2v_ctc_loss=1.702, task_loss=0.919, contrastive_loss=0.393, total=4171.93, n_correct=1677.78, ppl=4.9, accuracy=40.216, wps=14285.8, ups=1.15, wpb=12436.9, bsz=453.3, num_updates=5100, lr=0.00019803, gnorm=0.889, clip=0, loss_scale=0.5, train_wall=86, gb_free=15.2, wall=3935
2023-08-20 02:35:05 | INFO | train_inner | epoch 004:    788 / 1474 loss=2.884, trans_loss=3.93, nll_loss=2.264, w2v_ctc_loss=1.735, task_loss=0.963, contrastive_loss=0.276, total=4029.4, n_correct=1649.8, ppl=4.8, accuracy=40.944, wps=13918.6, ups=1.16, wpb=12032.2, bsz=423.7, num_updates=5200, lr=0.000196116, gnorm=1.091, clip=0, loss_scale=0.5, train_wall=86, gb_free=15.9, wall=4022
2023-08-20 02:36:31 | INFO | train_inner | epoch 004:    888 / 1474 loss=2.931, trans_loss=3.908, nll_loss=2.237, w2v_ctc_loss=1.757, task_loss=0.888, contrastive_loss=0.453, total=4175.28, n_correct=1739.84, ppl=4.71, accuracy=41.67, wps=14455.1, ups=1.16, wpb=12468.9, bsz=463.7, num_updates=5300, lr=0.000194257, gnorm=1.126, clip=0, loss_scale=0.5, train_wall=86, gb_free=15.2, wall=4108
2023-08-20 02:37:57 | INFO | train_inner | epoch 004:    988 / 1474 loss=2.876, trans_loss=3.887, nll_loss=2.209, w2v_ctc_loss=1.741, task_loss=0.904, contrastive_loss=0.327, total=4132.46, n_correct=1764.31, ppl=4.62, accuracy=42.694, wps=14306.3, ups=1.16, wpb=12343.2, bsz=455.3, num_updates=5400, lr=0.00019245, gnorm=1.095, clip=0, loss_scale=0.5, train_wall=86, gb_free=10.2, wall=4194
2023-08-20 02:39:23 | INFO | train_inner | epoch 004:   1088 / 1474 loss=2.846, trans_loss=3.874, nll_loss=2.191, w2v_ctc_loss=1.736, task_loss=0.947, contrastive_loss=0.3, total=4073.98, n_correct=1768.99, ppl=4.57, accuracy=43.422, wps=14136.1, ups=1.16, wpb=12161.7, bsz=437.9, num_updates=5500, lr=0.000190693, gnorm=1.018, clip=0, loss_scale=0.5, train_wall=85, gb_free=15.4, wall=4280
2023-08-20 02:40:49 | INFO | train_inner | epoch 004:   1188 / 1474 loss=2.83, trans_loss=3.845, nll_loss=2.155, w2v_ctc_loss=1.699, task_loss=0.819, contrastive_loss=0.4, total=4172.46, n_correct=1863.74, ppl=4.45, accuracy=44.668, wps=14557.8, ups=1.17, wpb=12459.8, bsz=487.1, num_updates=5600, lr=0.000188982, gnorm=0.954, clip=0, loss_scale=0.5, train_wall=85, gb_free=10.7, wall=4366
2023-08-20 02:42:15 | INFO | train_inner | epoch 004:   1288 / 1474 loss=2.847, trans_loss=3.83, nll_loss=2.135, w2v_ctc_loss=1.738, task_loss=0.855, contrastive_loss=0.381, total=4140.32, n_correct=1863.61, ppl=4.39, accuracy=45.011, wps=14357.8, ups=1.16, wpb=12365.2, bsz=467.7, num_updates=5700, lr=0.000187317, gnorm=1.296, clip=1, loss_scale=0.5, train_wall=85, gb_free=16.8, wall=4452
2023-08-20 02:43:40 | INFO | train_inner | epoch 004:   1388 / 1474 loss=2.822, trans_loss=3.821, nll_loss=2.124, w2v_ctc_loss=1.76, task_loss=0.916, contrastive_loss=0.253, total=4092.66, n_correct=1856.29, ppl=4.36, accuracy=45.357, wps=14472, ups=1.18, wpb=12224.9, bsz=436.3, num_updates=5800, lr=0.000185695, gnorm=1.4, clip=1, loss_scale=0.5, train_wall=84, gb_free=16.1, wall=4536
2023-08-20 02:44:52 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.4415, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.4415, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.4415, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.4415, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.4415, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.4415, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.4415, device='cuda:7')
2023-08-20 02:45:30 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 4.898 | trans_loss 6.184 | nll_loss 3.738 | w2v_ctc_loss 2.061 | task_loss 4.291 | contrastive_loss 0.437 | total 4003.4 | n_correct 2048.1 | ppl 13.34 | accuracy 51.159 | uer 28.631 | wer 30.368 | raw_wer 30.368 | bleu 9.29 | wps 1314.2 | wpb 4003.4 | bsz 141.8 | num_updates 5886 | best_bleu 9.29
2023-08-20 02:45:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5886 updates
2023-08-20 02:45:30 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 02:45:37 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 02:45:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt (epoch 4 @ 5886 updates, score 9.29) (writing took 13.53643189498689 seconds)
2023-08-20 02:45:44 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-08-20 02:45:44 | INFO | train | epoch 004 | loss 2.913 | trans_loss 3.922 | nll_loss 2.254 | w2v_ctc_loss 1.742 | task_loss 0.883 | contrastive_loss 0.371 | total 4138.65 | n_correct 1707.88 | ppl 4.77 | accuracy 41.267 | wps 13751.7 | ups 1.11 | wpb 12355.8 | bsz 458.5 | num_updates 5886 | lr 0.000184334 | gnorm 1.252 | clip 0.3 | loss_scale 0.5 | train_wall 1256 | gb_free 14.5 | wall 4661
2023-08-20 02:45:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 02:45:44 | INFO | fairseq.trainer | begin training epoch 5
2023-08-20 02:45:44 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 02:46:03 | INFO | train_inner | epoch 005:     14 / 1474 loss=2.768, trans_loss=3.8, nll_loss=2.093, w2v_ctc_loss=1.695, task_loss=0.885, contrastive_loss=0.294, total=4073.09, n_correct=1886, ppl=4.27, accuracy=46.304, wps=8490.8, ups=0.7, wpb=12158.4, bsz=451.4, num_updates=5900, lr=0.000184115, gnorm=1.086, clip=0, loss_scale=0.5, train_wall=84, gb_free=17, wall=4680
2023-08-20 02:47:28 | INFO | train_inner | epoch 005:    114 / 1474 loss=2.73, trans_loss=3.76, nll_loss=2.044, w2v_ctc_loss=1.67, task_loss=0.809, contrastive_loss=0.276, total=4233.69, n_correct=2012.18, ppl=4.12, accuracy=47.528, wps=14787.8, ups=1.17, wpb=12643.8, bsz=488.6, num_updates=6000, lr=0.000182574, gnorm=1.361, clip=1, loss_scale=0.5, train_wall=85, gb_free=15.4, wall=4765
2023-08-20 02:47:28 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 02:48:07 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.882 | trans_loss 6.165 | nll_loss 3.708 | w2v_ctc_loss 2.042 | task_loss 4.317 | contrastive_loss 0.449 | total 4003.4 | n_correct 2060 | ppl 13.07 | accuracy 51.456 | uer 29.605 | wer 30.838 | raw_wer 30.838 | bleu 9.73 | wps 1302.1 | wpb 4003.4 | bsz 141.8 | num_updates 6000 | best_bleu 9.73
2023-08-20 02:48:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6000 updates
2023-08-20 02:48:07 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_5_6000.pt
2023-08-20 02:48:09 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_5_6000.pt
2023-08-20 02:48:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_5_6000.pt (epoch 5 @ 6000 updates, score 9.73) (writing took 17.682428712025285 seconds)
2023-08-20 02:49:49 | INFO | train_inner | epoch 005:    214 / 1474 loss=2.771, trans_loss=3.761, nll_loss=2.043, w2v_ctc_loss=1.682, task_loss=0.822, contrastive_loss=0.488, total=4185.02, n_correct=2000.65, ppl=4.12, accuracy=47.805, wps=8900.5, ups=0.71, wpb=12486.5, bsz=485.7, num_updates=6100, lr=0.000181071, gnorm=1.241, clip=1, loss_scale=0.5, train_wall=84, gb_free=16.4, wall=4905
2023-08-20 02:51:14 | INFO | train_inner | epoch 005:    314 / 1474 loss=2.697, trans_loss=3.733, nll_loss=2.012, w2v_ctc_loss=1.654, task_loss=0.899, contrastive_loss=0.328, total=4097.42, n_correct=1980.55, ppl=4.03, accuracy=48.337, wps=14401.8, ups=1.18, wpb=12251.6, bsz=448, num_updates=6200, lr=0.000179605, gnorm=0.873, clip=0, loss_scale=0.5, train_wall=84, gb_free=13.7, wall=4990
2023-08-20 02:52:40 | INFO | train_inner | epoch 005:    414 / 1474 loss=2.665, trans_loss=3.716, nll_loss=1.988, w2v_ctc_loss=1.594, task_loss=0.872, contrastive_loss=0.409, total=4135.49, n_correct=2034.38, ppl=3.97, accuracy=49.193, wps=14404.9, ups=1.17, wpb=12359.1, bsz=465.9, num_updates=6300, lr=0.000178174, gnorm=0.842, clip=0, loss_scale=0.5, train_wall=85, gb_free=17.1, wall=5076
2023-08-20 02:54:05 | INFO | train_inner | epoch 005:    514 / 1474 loss=2.625, trans_loss=3.711, nll_loss=1.979, w2v_ctc_loss=1.591, task_loss=0.959, contrastive_loss=0.333, total=4035.21, n_correct=2001.88, ppl=3.94, accuracy=49.61, wps=14094.4, ups=1.17, wpb=12051.5, bsz=427.7, num_updates=6400, lr=0.000176777, gnorm=0.775, clip=0, loss_scale=0.5, train_wall=85, gb_free=16.8, wall=5162
2023-08-20 02:55:31 | INFO | train_inner | epoch 005:    614 / 1474 loss=2.6, trans_loss=3.708, nll_loss=1.973, w2v_ctc_loss=1.586, task_loss=0.927, contrastive_loss=0.238, total=4117.64, n_correct=2061.78, ppl=3.93, accuracy=50.072, wps=14377.2, ups=1.17, wpb=12284.8, bsz=443.7, num_updates=6500, lr=0.000175412, gnorm=0.807, clip=0, loss_scale=0.5, train_wall=85, gb_free=16.9, wall=5247
2023-08-20 02:56:56 | INFO | train_inner | epoch 005:    714 / 1474 loss=2.617, trans_loss=3.694, nll_loss=1.957, w2v_ctc_loss=1.575, task_loss=0.854, contrastive_loss=0.347, total=4153.99, n_correct=2103.42, ppl=3.88, accuracy=50.636, wps=14457.4, ups=1.17, wpb=12400.6, bsz=476.1, num_updates=6600, lr=0.000174078, gnorm=0.781, clip=0, loss_scale=0.5, train_wall=85, gb_free=17.4, wall=5333
2023-08-20 02:58:22 | INFO | train_inner | epoch 005:    814 / 1474 loss=2.59, trans_loss=3.681, nll_loss=1.94, w2v_ctc_loss=1.572, task_loss=0.891, contrastive_loss=0.28, total=4131.08, n_correct=2103.14, ppl=3.84, accuracy=50.91, wps=14315.5, ups=1.16, wpb=12332.3, bsz=455.1, num_updates=6700, lr=0.000172774, gnorm=0.857, clip=0, loss_scale=0.5, train_wall=86, gb_free=17.4, wall=5419
2023-08-20 02:59:48 | INFO | train_inner | epoch 005:    914 / 1474 loss=2.553, trans_loss=3.672, nll_loss=1.929, w2v_ctc_loss=1.552, task_loss=0.921, contrastive_loss=0.229, total=4109.29, n_correct=2108.86, ppl=3.81, accuracy=51.319, wps=14388.5, ups=1.17, wpb=12268.1, bsz=445.5, num_updates=6800, lr=0.000171499, gnorm=0.813, clip=0, loss_scale=0.5, train_wall=84, gb_free=11.5, wall=5504
2023-08-20 03:01:13 | INFO | train_inner | epoch 005:   1014 / 1474 loss=2.53, trans_loss=3.658, nll_loss=1.911, w2v_ctc_loss=1.519, task_loss=0.87, contrastive_loss=0.295, total=4163.73, n_correct=2167.42, ppl=3.76, accuracy=52.055, wps=14585, ups=1.17, wpb=12429.6, bsz=462.1, num_updates=6900, lr=0.000170251, gnorm=0.622, clip=0, loss_scale=0.5, train_wall=85, gb_free=15.9, wall=5590
2023-08-20 03:02:40 | INFO | train_inner | epoch 005:   1114 / 1474 loss=2.55, trans_loss=3.653, nll_loss=1.902, w2v_ctc_loss=1.541, task_loss=0.884, contrastive_loss=0.307, total=4172.75, n_correct=2183.38, ppl=3.74, accuracy=52.325, wps=14314.4, ups=1.15, wpb=12448.4, bsz=464, num_updates=7000, lr=0.000169031, gnorm=0.699, clip=0, loss_scale=0.5, train_wall=86, gb_free=12.3, wall=5677
2023-08-20 03:04:06 | INFO | train_inner | epoch 005:   1214 / 1474 loss=2.517, trans_loss=3.653, nll_loss=1.901, w2v_ctc_loss=1.523, task_loss=0.892, contrastive_loss=0.216, total=4164.91, n_correct=2181.85, ppl=3.74, accuracy=52.386, wps=14438.4, ups=1.16, wpb=12422.5, bsz=455.7, num_updates=7100, lr=0.000167836, gnorm=0.815, clip=0, loss_scale=0.5, train_wall=85, gb_free=14.6, wall=5763
2023-08-20 03:05:33 | INFO | train_inner | epoch 005:   1314 / 1474 loss=2.466, trans_loss=3.635, nll_loss=1.88, w2v_ctc_loss=1.488, task_loss=0.903, contrastive_loss=0.176, total=4127.88, n_correct=2191.02, ppl=3.68, accuracy=53.079, wps=14204.5, ups=1.15, wpb=12320.7, bsz=445.3, num_updates=7200, lr=0.000166667, gnorm=0.784, clip=1, loss_scale=1, train_wall=86, gb_free=15.9, wall=5849
2023-08-20 03:06:58 | INFO | train_inner | epoch 005:   1414 / 1474 loss=2.497, trans_loss=3.637, nll_loss=1.886, w2v_ctc_loss=1.506, task_loss=0.892, contrastive_loss=0.238, total=4135.9, n_correct=2191.97, ppl=3.7, accuracy=52.999, wps=14458.7, ups=1.17, wpb=12351.8, bsz=456.9, num_updates=7300, lr=0.000165521, gnorm=0.692, clip=0, loss_scale=1, train_wall=85, gb_free=16.5, wall=5935
2023-08-20 03:07:50 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 03:08:23 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.476 | trans_loss 5.719 | nll_loss 3.135 | w2v_ctc_loss 1.742 | task_loss 4.316 | contrastive_loss 0.398 | total 4003.4 | n_correct 2320.5 | ppl 8.78 | accuracy 57.963 | uer 26.173 | wer 27.631 | raw_wer 27.631 | bleu 14.66 | wps 1574.8 | wpb 4003.4 | bsz 141.8 | num_updates 7360 | best_bleu 14.66
2023-08-20 03:08:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7360 updates
2023-08-20 03:08:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 03:08:31 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 03:08:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt (epoch 5 @ 7360 updates, score 14.66) (writing took 15.052217933000065 seconds)
2023-08-20 03:08:38 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-08-20 03:08:38 | INFO | train | epoch 005 | loss 2.597 | trans_loss 3.689 | nll_loss 1.95 | w2v_ctc_loss 1.573 | task_loss 0.884 | contrastive_loss 0.298 | total 4138.65 | n_correct 2097.41 | ppl 3.86 | accuracy 50.679 | wps 13251.7 | ups 1.07 | wpb 12355.8 | bsz 458.5 | num_updates 7360 | lr 0.000164845 | gnorm 0.844 | clip 0.2 | loss_scale 1 | train_wall 1252 | gb_free 15.9 | wall 6035
2023-08-20 03:08:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 03:08:38 | INFO | fairseq.trainer | begin training epoch 6
2023-08-20 03:08:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 03:09:20 | INFO | train_inner | epoch 006:     40 / 1474 loss=2.448, trans_loss=3.609, nll_loss=1.846, w2v_ctc_loss=1.468, task_loss=0.91, contrastive_loss=0.229, total=4114.92, n_correct=2217.6, ppl=3.6, accuracy=53.892, wps=8646, ups=0.7, wpb=12278.2, bsz=446.4, num_updates=7400, lr=0.000164399, gnorm=0.609, clip=0, loss_scale=1, train_wall=85, gb_free=16.2, wall=6077
2023-08-20 03:10:45 | INFO | train_inner | epoch 006:    140 / 1474 loss=2.414, trans_loss=3.583, nll_loss=1.815, w2v_ctc_loss=1.424, task_loss=0.881, contrastive_loss=0.271, total=4157.03, n_correct=2268.83, ppl=3.52, accuracy=54.578, wps=14552, ups=1.17, wpb=12418.5, bsz=455.9, num_updates=7500, lr=0.000163299, gnorm=0.619, clip=0, loss_scale=1, train_wall=85, gb_free=14.7, wall=6162
2023-08-20 03:12:11 | INFO | train_inner | epoch 006:    240 / 1474 loss=2.429, trans_loss=3.594, nll_loss=1.83, w2v_ctc_loss=1.463, task_loss=0.925, contrastive_loss=0.202, total=4121.73, n_correct=2238.7, ppl=3.56, accuracy=54.315, wps=14332.6, ups=1.16, wpb=12312.8, bsz=446.5, num_updates=7600, lr=0.000162221, gnorm=0.71, clip=0, loss_scale=1, train_wall=85, gb_free=15.8, wall=6248
2023-08-20 03:13:39 | INFO | train_inner | epoch 006:    340 / 1474 loss=2.45, trans_loss=3.578, nll_loss=1.807, w2v_ctc_loss=1.409, task_loss=0.833, contrastive_loss=0.476, total=4170.63, n_correct=2293.98, ppl=3.5, accuracy=55.003, wps=14257.6, ups=1.15, wpb=12451, bsz=486.5, num_updates=7700, lr=0.000161165, gnorm=0.667, clip=0, loss_scale=1, train_wall=87, gb_free=12.4, wall=6335
2023-08-20 03:15:04 | INFO | train_inner | epoch 006:    440 / 1474 loss=2.375, trans_loss=3.573, nll_loss=1.802, w2v_ctc_loss=1.407, task_loss=0.853, contrastive_loss=0.192, total=4147.89, n_correct=2293.22, ppl=3.49, accuracy=55.286, wps=14587.2, ups=1.18, wpb=12386.2, bsz=467, num_updates=7800, lr=0.000160128, gnorm=0.635, clip=0, loss_scale=1, train_wall=84, gb_free=16.7, wall=6420
2023-08-20 03:16:30 | INFO | train_inner | epoch 006:    540 / 1474 loss=2.389, trans_loss=3.578, nll_loss=1.807, w2v_ctc_loss=1.432, task_loss=0.887, contrastive_loss=0.183, total=4170.36, n_correct=2302.4, ppl=3.5, accuracy=55.209, wps=14471.4, ups=1.16, wpb=12447.9, bsz=455.9, num_updates=7900, lr=0.000159111, gnorm=0.678, clip=0, loss_scale=1, train_wall=85, gb_free=15.2, wall=6506
2023-08-20 03:17:54 | INFO | train_inner | epoch 006:    640 / 1474 loss=2.37, trans_loss=3.569, nll_loss=1.797, w2v_ctc_loss=1.393, task_loss=0.842, contrastive_loss=0.233, total=4144.5, n_correct=2303.42, ppl=3.47, accuracy=55.578, wps=14622.7, ups=1.18, wpb=12372.4, bsz=470, num_updates=8000, lr=0.000158114, gnorm=0.594, clip=0, loss_scale=1, train_wall=84, gb_free=17, wall=6591
2023-08-20 03:17:54 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 03:18:30 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.391 | trans_loss 5.62 | nll_loss 3.003 | w2v_ctc_loss 1.71 | task_loss 4.371 | contrastive_loss 0.358 | total 4003.4 | n_correct 2375.3 | ppl 8.02 | accuracy 59.332 | uer 25.602 | wer 27.068 | raw_wer 27.068 | bleu 15.88 | wps 1446.3 | wpb 4003.4 | bsz 141.8 | num_updates 8000 | best_bleu 15.88
2023-08-20 03:18:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8000 updates
2023-08-20 03:18:30 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_6_8000.pt
2023-08-20 03:18:32 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_6_8000.pt
2023-08-20 03:18:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_6_8000.pt (epoch 6 @ 8000 updates, score 15.88) (writing took 15.152880343026482 seconds)
2023-08-20 03:20:11 | INFO | train_inner | epoch 006:    740 / 1474 loss=2.381, trans_loss=3.571, nll_loss=1.799, w2v_ctc_loss=1.424, task_loss=0.899, contrastive_loss=0.192, total=4145.29, n_correct=2301.17, ppl=3.48, accuracy=55.513, wps=9048.2, ups=0.73, wpb=12375, bsz=454.7, num_updates=8100, lr=0.000157135, gnorm=0.63, clip=0, loss_scale=1, train_wall=85, gb_free=17.5, wall=6728
2023-08-20 03:21:37 | INFO | train_inner | epoch 006:    840 / 1474 loss=2.351, trans_loss=3.566, nll_loss=1.793, w2v_ctc_loss=1.394, task_loss=0.916, contrastive_loss=0.17, total=4131.83, n_correct=2303.91, ppl=3.47, accuracy=55.76, wps=14339.1, ups=1.16, wpb=12335.3, bsz=446.9, num_updates=8200, lr=0.000156174, gnorm=0.56, clip=0, loss_scale=1, train_wall=85, gb_free=16.2, wall=6814
2023-08-20 03:23:03 | INFO | train_inner | epoch 006:    940 / 1474 loss=2.387, trans_loss=3.57, nll_loss=1.797, w2v_ctc_loss=1.412, task_loss=0.953, contrastive_loss=0.267, total=4068.88, n_correct=2263.86, ppl=3.48, accuracy=55.638, wps=14070.7, ups=1.16, wpb=12145.6, bsz=436.9, num_updates=8300, lr=0.00015523, gnorm=0.658, clip=0, loss_scale=1, train_wall=86, gb_free=15.8, wall=6900
2023-08-20 03:24:28 | INFO | train_inner | epoch 006:   1040 / 1474 loss=2.367, trans_loss=3.554, nll_loss=1.777, w2v_ctc_loss=1.373, task_loss=0.827, contrastive_loss=0.337, total=4176.69, n_correct=2347.38, ppl=3.43, accuracy=56.202, wps=14687.4, ups=1.18, wpb=12465.8, bsz=480, num_updates=8400, lr=0.000154303, gnorm=0.599, clip=0, loss_scale=1, train_wall=84, gb_free=16.2, wall=6985
2023-08-20 03:25:54 | INFO | train_inner | epoch 006:   1140 / 1474 loss=2.353, trans_loss=3.556, nll_loss=1.78, w2v_ctc_loss=1.397, task_loss=0.964, contrastive_loss=0.2, total=4078.12, n_correct=2282.97, ppl=3.43, accuracy=55.981, wps=14283, ups=1.17, wpb=12175.4, bsz=435.5, num_updates=8500, lr=0.000153393, gnorm=0.655, clip=0, loss_scale=1, train_wall=85, gb_free=14.6, wall=7070
2023-08-20 03:27:20 | INFO | train_inner | epoch 006:   1240 / 1474 loss=2.373, trans_loss=3.541, nll_loss=1.763, w2v_ctc_loss=1.366, task_loss=0.886, contrastive_loss=0.46, total=4126.61, n_correct=2332.89, ppl=3.39, accuracy=56.533, wps=14213.2, ups=1.15, wpb=12325.4, bsz=463.4, num_updates=8600, lr=0.000152499, gnorm=0.574, clip=0, loss_scale=1, train_wall=86, gb_free=13.2, wall=7157
2023-08-20 03:28:46 | INFO | train_inner | epoch 006:   1340 / 1474 loss=2.31, trans_loss=3.545, nll_loss=1.764, w2v_ctc_loss=1.365, task_loss=0.871, contrastive_loss=0.154, total=4127.1, n_correct=2340.07, ppl=3.4, accuracy=56.7, wps=14392.8, ups=1.17, wpb=12313, bsz=454.8, num_updates=8700, lr=0.00015162, gnorm=0.591, clip=0, loss_scale=1, train_wall=85, gb_free=16.4, wall=7243
2023-08-20 03:30:12 | INFO | train_inner | epoch 006:   1440 / 1474 loss=2.312, trans_loss=3.537, nll_loss=1.757, w2v_ctc_loss=1.37, task_loss=0.875, contrastive_loss=0.163, total=4197.14, n_correct=2384.79, ppl=3.38, accuracy=56.819, wps=14561.6, ups=1.16, wpb=12529.7, bsz=463.5, num_updates=8800, lr=0.000150756, gnorm=0.651, clip=0, loss_scale=1, train_wall=85, gb_free=15.2, wall=7329
2023-08-20 03:30:41 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 03:31:16 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.286 | trans_loss 5.512 | nll_loss 2.869 | w2v_ctc_loss 1.628 | task_loss 4.384 | contrastive_loss 0.339 | total 4003.4 | n_correct 2438.1 | ppl 7.31 | accuracy 60.901 | uer 24.145 | wer 25.566 | raw_wer 25.566 | bleu 17.15 | wps 1478.1 | wpb 4003.4 | bsz 141.8 | num_updates 8834 | best_bleu 17.15
2023-08-20 03:31:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8834 updates
2023-08-20 03:31:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 03:31:25 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 03:31:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt (epoch 6 @ 8834 updates, score 17.15) (writing took 15.416253397997934 seconds)
2023-08-20 03:31:31 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-08-20 03:31:31 | INFO | train | epoch 006 | loss 2.375 | trans_loss 3.565 | nll_loss 1.792 | w2v_ctc_loss 1.401 | task_loss 0.884 | contrastive_loss 0.248 | total 4138.65 | n_correct 2303.17 | ppl 3.46 | accuracy 55.65 | wps 13264.5 | ups 1.07 | wpb 12355.8 | bsz 458.5 | num_updates 8834 | lr 0.000150465 | gnorm 0.629 | clip 0 | loss_scale 1 | train_wall 1254 | gb_free 14.6 | wall 7408
2023-08-20 03:31:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 03:31:31 | INFO | fairseq.trainer | begin training epoch 7
2023-08-20 03:31:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 03:32:35 | INFO | train_inner | epoch 007:     66 / 1474 loss=2.279, trans_loss=3.523, nll_loss=1.737, w2v_ctc_loss=1.332, task_loss=0.867, contrastive_loss=0.175, total=4099.48, n_correct=2351.68, ppl=3.33, accuracy=57.365, wps=8529, ups=0.7, wpb=12236.9, bsz=460.3, num_updates=8900, lr=0.000149906, gnorm=0.612, clip=0, loss_scale=1, train_wall=85, gb_free=15.8, wall=7472
2023-08-20 03:34:00 | INFO | train_inner | epoch 007:    166 / 1474 loss=2.277, trans_loss=3.516, nll_loss=1.729, w2v_ctc_loss=1.318, task_loss=0.89, contrastive_loss=0.241, total=4107.4, n_correct=2359.47, ppl=3.32, accuracy=57.444, wps=14452.9, ups=1.18, wpb=12263.9, bsz=455.4, num_updates=9000, lr=0.000149071, gnorm=0.571, clip=0, loss_scale=1, train_wall=84, gb_free=16.5, wall=7557
2023-08-20 03:35:25 | INFO | train_inner | epoch 007:    266 / 1474 loss=2.249, trans_loss=3.507, nll_loss=1.716, w2v_ctc_loss=1.313, task_loss=0.886, contrastive_loss=0.15, total=4137.13, n_correct=2392.48, ppl=3.28, accuracy=57.829, wps=14490.7, ups=1.17, wpb=12346.5, bsz=454.9, num_updates=9100, lr=0.00014825, gnorm=0.552, clip=0, loss_scale=1, train_wall=85, gb_free=16, wall=7642
2023-08-20 03:36:52 | INFO | train_inner | epoch 007:    366 / 1474 loss=2.308, trans_loss=3.512, nll_loss=1.722, w2v_ctc_loss=1.309, task_loss=0.851, contrastive_loss=0.415, total=4199.72, n_correct=2425.62, ppl=3.3, accuracy=57.757, wps=14489.6, ups=1.16, wpb=12534, bsz=480.8, num_updates=9200, lr=0.000147442, gnorm=0.589, clip=0, loss_scale=2, train_wall=86, gb_free=17.2, wall=7729
2023-08-20 03:38:17 | INFO | train_inner | epoch 007:    466 / 1474 loss=2.285, trans_loss=3.51, nll_loss=1.722, w2v_ctc_loss=1.304, task_loss=0.879, contrastive_loss=0.333, total=4149.08, n_correct=2395.36, ppl=3.3, accuracy=57.732, wps=14557.8, ups=1.17, wpb=12391, bsz=459, num_updates=9300, lr=0.000146647, gnorm=0.606, clip=0, loss_scale=2, train_wall=84, gb_free=16.6, wall=7814
2023-08-20 03:39:43 | INFO | train_inner | epoch 007:    566 / 1474 loss=2.246, trans_loss=3.507, nll_loss=1.715, w2v_ctc_loss=1.304, task_loss=0.863, contrastive_loss=0.159, total=4168.84, n_correct=2419.15, ppl=3.28, accuracy=58.029, wps=14501.6, ups=1.17, wpb=12436.4, bsz=460, num_updates=9400, lr=0.000145865, gnorm=0.599, clip=0, loss_scale=2, train_wall=85, gb_free=15.9, wall=7900
2023-08-20 03:41:08 | INFO | train_inner | epoch 007:    666 / 1474 loss=2.23, trans_loss=3.499, nll_loss=1.706, w2v_ctc_loss=1.293, task_loss=0.869, contrastive_loss=0.141, total=4164.13, n_correct=2430.95, ppl=3.26, accuracy=58.378, wps=14507.4, ups=1.17, wpb=12427.2, bsz=458.5, num_updates=9500, lr=0.000145095, gnorm=0.545, clip=0, loss_scale=2, train_wall=85, gb_free=15.2, wall=7985
2023-08-20 03:42:35 | INFO | train_inner | epoch 007:    766 / 1474 loss=2.242, trans_loss=3.499, nll_loss=1.707, w2v_ctc_loss=1.31, task_loss=0.921, contrastive_loss=0.149, total=4126.5, n_correct=2396.83, ppl=3.27, accuracy=58.084, wps=14300.2, ups=1.16, wpb=12320.2, bsz=450.3, num_updates=9600, lr=0.000144338, gnorm=0.62, clip=0, loss_scale=2, train_wall=85, gb_free=14.9, wall=8071
2023-08-20 03:44:01 | INFO | train_inner | epoch 007:    866 / 1474 loss=2.229, trans_loss=3.504, nll_loss=1.712, w2v_ctc_loss=1.291, task_loss=0.907, contrastive_loss=0.152, total=4133.53, n_correct=2403.7, ppl=3.28, accuracy=58.151, wps=14228.8, ups=1.15, wpb=12333.5, bsz=453.5, num_updates=9700, lr=0.000143592, gnorm=0.537, clip=0, loss_scale=2, train_wall=86, gb_free=12.3, wall=8158
2023-08-20 03:45:28 | INFO | train_inner | epoch 007:    966 / 1474 loss=2.24, trans_loss=3.491, nll_loss=1.698, w2v_ctc_loss=1.274, task_loss=0.838, contrastive_loss=0.259, total=4144.45, n_correct=2425.66, ppl=3.24, accuracy=58.528, wps=14357.7, ups=1.16, wpb=12374.7, bsz=476.9, num_updates=9800, lr=0.000142857, gnorm=0.538, clip=0, loss_scale=2, train_wall=85, gb_free=15.7, wall=8244
2023-08-20 03:46:53 | INFO | train_inner | epoch 007:   1066 / 1474 loss=2.216, trans_loss=3.5, nll_loss=1.708, w2v_ctc_loss=1.288, task_loss=0.932, contrastive_loss=0.121, total=4100.65, n_correct=2393.83, ppl=3.27, accuracy=58.377, wps=14341.9, ups=1.17, wpb=12242.5, bsz=435.4, num_updates=9900, lr=0.000142134, gnorm=0.521, clip=0, loss_scale=2, train_wall=85, gb_free=15.3, wall=8330
2023-08-20 03:48:19 | INFO | train_inner | epoch 007:   1166 / 1474 loss=2.272, trans_loss=3.486, nll_loss=1.694, w2v_ctc_loss=1.281, task_loss=0.857, contrastive_loss=0.384, total=4138.96, n_correct=2427.78, ppl=3.24, accuracy=58.657, wps=14416, ups=1.17, wpb=12367.7, bsz=471.9, num_updates=10000, lr=0.000141421, gnorm=0.543, clip=0, loss_scale=2, train_wall=85, gb_free=16.3, wall=8415
2023-08-20 03:48:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 03:48:52 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.2 | trans_loss 5.432 | nll_loss 2.769 | w2v_ctc_loss 1.532 | task_loss 4.432 | contrastive_loss 0.32 | total 4003.4 | n_correct 2489.8 | ppl 6.82 | accuracy 62.192 | uer 23.144 | wer 24.887 | raw_wer 24.887 | bleu 18.69 | wps 1615.8 | wpb 4003.4 | bsz 141.8 | num_updates 10000 | best_bleu 18.69
2023-08-20 03:48:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10000 updates
2023-08-20 03:48:52 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_7_10000.pt
2023-08-20 03:48:55 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_7_10000.pt
2023-08-20 03:49:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_7_10000.pt (epoch 7 @ 10000 updates, score 18.69) (writing took 14.86175587400794 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:0')
2023-08-20 03:50:32 | INFO | train_inner | epoch 007:   1266 / 1474 loss=2.201, trans_loss=3.487, nll_loss=1.693, w2v_ctc_loss=1.268, task_loss=0.908, contrastive_loss=0.149, total=4120.42, n_correct=2417.82, ppl=3.23, accuracy=58.679, wps=9229.8, ups=0.75, wpb=12304.2, bsz=446.6, num_updates=10100, lr=0.00014072, gnorm=0.434, clip=0, loss_scale=2, train_wall=84, gb_free=15.1, wall=8549
2023-08-20 03:51:58 | INFO | train_inner | epoch 007:   1366 / 1474 loss=2.222, trans_loss=3.483, nll_loss=1.688, w2v_ctc_loss=1.276, task_loss=0.829, contrastive_loss=0.19, total=4186.45, n_correct=2466.98, ppl=3.22, accuracy=58.928, wps=14613.9, ups=1.17, wpb=12499.3, bsz=479.7, num_updates=10200, lr=0.000140028, gnorm=0.438, clip=0, loss_scale=2, train_wall=85, gb_free=11.6, wall=8634
2023-08-20 03:53:24 | INFO | train_inner | epoch 007:   1466 / 1474 loss=2.243, trans_loss=3.487, nll_loss=1.694, w2v_ctc_loss=1.286, task_loss=0.939, contrastive_loss=0.256, total=4117.01, n_correct=2412.18, ppl=3.24, accuracy=58.591, wps=14173.4, ups=1.15, wpb=12300.4, bsz=448.4, num_updates=10300, lr=0.000139347, gnorm=0.609, clip=0, loss_scale=2, train_wall=86, gb_free=15.6, wall=8721
2023-08-20 03:53:31 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:4')
2023-08-20 03:54:03 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.223 | trans_loss 5.434 | nll_loss 2.768 | w2v_ctc_loss 1.601 | task_loss 4.431 | contrastive_loss 0.328 | total 4003.4 | n_correct 2487.8 | ppl 6.81 | accuracy 62.142 | uer 23.431 | wer 25.309 | raw_wer 25.309 | bleu 18.48 | wps 1659.3 | wpb 4003.4 | bsz 141.8 | num_updates 10308 | best_bleu 18.69
2023-08-20 03:54:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10308 updates
2023-08-20 03:54:03 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_18.4809.pt
2023-08-20 03:54:06 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_18.4809.pt
2023-08-20 03:54:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_18.4809.pt (epoch 7 @ 10308 updates, score 18.48) (writing took 8.092347990022972 seconds)
2023-08-20 03:54:12 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-08-20 03:54:12 | INFO | train | epoch 007 | loss 2.248 | trans_loss 3.5 | nll_loss 1.708 | w2v_ctc_loss 1.295 | task_loss 0.884 | contrastive_loss 0.22 | total 4138.65 | n_correct 2408.51 | ppl 3.27 | accuracy 58.196 | wps 13382.9 | ups 1.08 | wpb 12355.8 | bsz 458.5 | num_updates 10308 | lr 0.000139293 | gnorm 0.553 | clip 0 | loss_scale 2 | train_wall 1252 | gb_free 12.7 | wall 8769
2023-08-20 03:54:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 03:54:12 | INFO | fairseq.trainer | begin training epoch 8
2023-08-20 03:54:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 03:55:38 | INFO | train_inner | epoch 008:     92 / 1474 loss=2.182, trans_loss=3.477, nll_loss=1.676, w2v_ctc_loss=1.25, task_loss=0.952, contrastive_loss=0.144, total=4097.76, n_correct=2429.88, ppl=3.2, accuracy=59.298, wps=9156.7, ups=0.75, wpb=12216.9, bsz=437, num_updates=10400, lr=0.000138675, gnorm=0.491, clip=0, loss_scale=2, train_wall=84, gb_free=15.4, wall=8854
2023-08-20 03:57:03 | INFO | train_inner | epoch 008:    192 / 1474 loss=2.171, trans_loss=3.464, nll_loss=1.659, w2v_ctc_loss=1.236, task_loss=0.963, contrastive_loss=0.161, total=4039, n_correct=2407.93, ppl=3.16, accuracy=59.617, wps=14066.2, ups=1.17, wpb=12046.3, bsz=428.9, num_updates=10500, lr=0.000138013, gnorm=0.482, clip=0, loss_scale=2, train_wall=85, gb_free=14.4, wall=8940
2023-08-20 03:58:29 | INFO | train_inner | epoch 008:    292 / 1474 loss=2.168, trans_loss=3.456, nll_loss=1.651, w2v_ctc_loss=1.233, task_loss=0.835, contrastive_loss=0.163, total=4210.6, n_correct=2518.91, ppl=3.14, accuracy=59.823, wps=14736.7, ups=1.17, wpb=12566.5, bsz=486.9, num_updates=10600, lr=0.000137361, gnorm=0.442, clip=0, loss_scale=2, train_wall=85, gb_free=15.5, wall=9025
2023-08-20 03:59:55 | INFO | train_inner | epoch 008:    392 / 1474 loss=2.179, trans_loss=3.463, nll_loss=1.659, w2v_ctc_loss=1.245, task_loss=0.922, contrastive_loss=0.182, total=4139.64, n_correct=2468.05, ppl=3.16, accuracy=59.62, wps=14325.3, ups=1.16, wpb=12350.6, bsz=447.4, num_updates=10700, lr=0.000136717, gnorm=0.455, clip=0, loss_scale=2, train_wall=85, gb_free=14.7, wall=9112
2023-08-20 04:01:22 | INFO | train_inner | epoch 008:    492 / 1474 loss=2.243, trans_loss=3.462, nll_loss=1.661, w2v_ctc_loss=1.238, task_loss=0.807, contrastive_loss=0.45, total=4189.5, n_correct=2496.32, ppl=3.16, accuracy=59.585, wps=14419.5, ups=1.15, wpb=12508.9, bsz=499.4, num_updates=10800, lr=0.000136083, gnorm=0.494, clip=0, loss_scale=2, train_wall=86, gb_free=12.2, wall=9198
2023-08-20 04:02:47 | INFO | train_inner | epoch 008:    592 / 1474 loss=2.175, trans_loss=3.461, nll_loss=1.662, w2v_ctc_loss=1.26, task_loss=0.963, contrastive_loss=0.12, total=4061.09, n_correct=2411.54, ppl=3.17, accuracy=59.382, wps=14224.2, ups=1.17, wpb=12140.1, bsz=427.7, num_updates=10900, lr=0.000135457, gnorm=0.497, clip=0, loss_scale=2, train_wall=85, gb_free=16.2, wall=9284
2023-08-20 04:04:13 | INFO | train_inner | epoch 008:    692 / 1474 loss=2.159, trans_loss=3.454, nll_loss=1.649, w2v_ctc_loss=1.241, task_loss=0.906, contrastive_loss=0.131, total=4144.17, n_correct=2483.56, ppl=3.14, accuracy=59.929, wps=14415.4, ups=1.17, wpb=12370.7, bsz=450.4, num_updates=11000, lr=0.00013484, gnorm=0.486, clip=0, loss_scale=2, train_wall=85, gb_free=12.9, wall=9370
2023-08-20 04:05:39 | INFO | train_inner | epoch 008:    792 / 1474 loss=2.189, trans_loss=3.455, nll_loss=1.654, w2v_ctc_loss=1.251, task_loss=0.908, contrastive_loss=0.219, total=4117.36, n_correct=2457.3, ppl=3.15, accuracy=59.681, wps=14320.6, ups=1.16, wpb=12305.6, bsz=448.1, num_updates=11100, lr=0.000134231, gnorm=0.794, clip=1, loss_scale=2, train_wall=85, gb_free=15.8, wall=9455
2023-08-20 04:07:05 | INFO | train_inner | epoch 008:    892 / 1474 loss=2.222, trans_loss=3.471, nll_loss=1.673, w2v_ctc_loss=1.274, task_loss=0.835, contrastive_loss=0.233, total=4182.5, n_correct=2477.18, ppl=3.19, accuracy=59.227, wps=14549.9, ups=1.16, wpb=12489.6, bsz=478.7, num_updates=11200, lr=0.000133631, gnorm=0.73, clip=0, loss_scale=2, train_wall=85, gb_free=17.1, wall=9541
2023-08-20 04:08:30 | INFO | train_inner | epoch 008:    992 / 1474 loss=2.163, trans_loss=3.457, nll_loss=1.654, w2v_ctc_loss=1.244, task_loss=0.849, contrastive_loss=0.131, total=4155.49, n_correct=2490.51, ppl=3.15, accuracy=59.933, wps=14534.9, ups=1.17, wpb=12408.4, bsz=464, num_updates=11300, lr=0.000133038, gnorm=0.576, clip=1, loss_scale=4, train_wall=85, gb_free=16.7, wall=9627
2023-08-20 04:09:56 | INFO | train_inner | epoch 008:   1092 / 1474 loss=2.181, trans_loss=3.453, nll_loss=1.648, w2v_ctc_loss=1.216, task_loss=0.892, contrastive_loss=0.341, total=4186.39, n_correct=2512.76, ppl=3.13, accuracy=60.022, wps=14473.2, ups=1.16, wpb=12493.6, bsz=461.7, num_updates=11400, lr=0.000132453, gnorm=0.428, clip=0, loss_scale=4, train_wall=86, gb_free=14.8, wall=9713
2023-08-20 04:11:22 | INFO | train_inner | epoch 008:   1192 / 1474 loss=2.151, trans_loss=3.448, nll_loss=1.644, w2v_ctc_loss=1.228, task_loss=0.84, contrastive_loss=0.135, total=4178.48, n_correct=2509.85, ppl=3.13, accuracy=60.066, wps=14577.4, ups=1.17, wpb=12482, bsz=472.1, num_updates=11500, lr=0.000131876, gnorm=0.45, clip=0, loss_scale=4, train_wall=85, gb_free=15.2, wall=9799
2023-08-20 04:12:48 | INFO | train_inner | epoch 008:   1292 / 1474 loss=2.153, trans_loss=3.45, nll_loss=1.646, w2v_ctc_loss=1.229, task_loss=0.934, contrastive_loss=0.155, total=4064.4, n_correct=2433.76, ppl=3.13, accuracy=59.88, wps=14151.5, ups=1.17, wpb=12140.7, bsz=436, num_updates=11600, lr=0.000131306, gnorm=0.478, clip=0, loss_scale=4, train_wall=85, gb_free=15.6, wall=9884
2023-08-20 04:14:13 | INFO | train_inner | epoch 008:   1392 / 1474 loss=2.189, trans_loss=3.458, nll_loss=1.656, w2v_ctc_loss=1.246, task_loss=0.843, contrastive_loss=0.23, total=4163.91, n_correct=2496.12, ppl=3.15, accuracy=59.947, wps=14606.6, ups=1.17, wpb=12433, bsz=472.8, num_updates=11700, lr=0.000130744, gnorm=0.642, clip=1, loss_scale=4, train_wall=84, gb_free=16, wall=9970
2023-08-20 04:15:22 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 04:15:56 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 4.145 | trans_loss 5.363 | nll_loss 2.678 | w2v_ctc_loss 1.519 | task_loss 4.434 | contrastive_loss 0.305 | total 4003.4 | n_correct 2530.3 | ppl 6.4 | accuracy 63.204 | uer 22.271 | wer 23.985 | raw_wer 23.985 | bleu 18.91 | wps 1586.7 | wpb 4003.4 | bsz 141.8 | num_updates 11782 | best_bleu 18.91
2023-08-20 04:15:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11782 updates
2023-08-20 04:15:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 04:16:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 04:16:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt (epoch 8 @ 11782 updates, score 18.91) (writing took 13.567798058968037 seconds)
2023-08-20 04:16:10 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-08-20 04:16:10 | INFO | train | epoch 008 | loss 2.179 | trans_loss 3.458 | nll_loss 1.656 | w2v_ctc_loss 1.24 | task_loss 0.885 | contrastive_loss 0.205 | total 4138.65 | n_correct 2472.97 | ppl 3.15 | accuracy 59.753 | wps 13820.5 | ups 1.12 | wpb 12355.8 | bsz 458.5 | num_updates 11782 | lr 0.000130288 | gnorm 0.526 | clip 0.2 | loss_scale 4 | train_wall 1252 | gb_free 16.4 | wall 10087
2023-08-20 04:16:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 04:16:10 | INFO | fairseq.trainer | begin training epoch 9
2023-08-20 04:16:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 04:16:33 | INFO | train_inner | epoch 009:     18 / 1474 loss=2.161, trans_loss=3.446, nll_loss=1.64, w2v_ctc_loss=1.205, task_loss=0.879, contrastive_loss=0.312, total=4111.15, n_correct=2481.09, ppl=3.12, accuracy=60.35, wps=8762.7, ups=0.71, wpb=12269.8, bsz=460.9, num_updates=11800, lr=0.000130189, gnorm=0.423, clip=0, loss_scale=4, train_wall=84, gb_free=16.6, wall=10110
2023-08-20 04:17:57 | INFO | train_inner | epoch 009:    118 / 1474 loss=2.108, trans_loss=3.419, nll_loss=1.607, w2v_ctc_loss=1.178, task_loss=0.825, contrastive_loss=0.155, total=4186.63, n_correct=2557.52, ppl=3.05, accuracy=61.088, wps=14774.5, ups=1.18, wpb=12503, bsz=481.5, num_updates=11900, lr=0.000129641, gnorm=0.428, clip=0, loss_scale=4, train_wall=84, gb_free=16.1, wall=10194
2023-08-20 04:19:24 | INFO | train_inner | epoch 009:    218 / 1474 loss=2.097, trans_loss=3.425, nll_loss=1.613, w2v_ctc_loss=1.183, task_loss=0.956, contrastive_loss=0.109, total=4071.82, n_correct=2479.33, ppl=3.06, accuracy=60.89, wps=14120.3, ups=1.16, wpb=12157.1, bsz=430.9, num_updates=12000, lr=0.000129099, gnorm=0.427, clip=0, loss_scale=4, train_wall=85, gb_free=16.1, wall=10280
2023-08-20 04:19:24 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 04:19:57 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.132 | trans_loss 5.351 | nll_loss 2.658 | w2v_ctc_loss 1.506 | task_loss 4.437 | contrastive_loss 0.307 | total 4003.4 | n_correct 2539.3 | ppl 6.31 | accuracy 63.429 | uer 21.992 | wer 23.866 | raw_wer 23.866 | bleu 19.47 | wps 1609.6 | wpb 4003.4 | bsz 141.8 | num_updates 12000 | best_bleu 19.47
2023-08-20 04:19:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12000 updates
2023-08-20 04:19:57 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_9_12000.pt
2023-08-20 04:19:59 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_9_12000.pt
2023-08-20 04:20:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_9_12000.pt (epoch 9 @ 12000 updates, score 19.47) (writing took 14.051885227963794 seconds)
2023-08-20 04:21:37 | INFO | train_inner | epoch 009:    318 / 1474 loss=2.101, trans_loss=3.415, nll_loss=1.602, w2v_ctc_loss=1.169, task_loss=0.81, contrastive_loss=0.164, total=4167.06, n_correct=2551.23, ppl=3.04, accuracy=61.224, wps=9343.8, ups=0.75, wpb=12450.2, bsz=485.4, num_updates=12100, lr=0.000128565, gnorm=0.415, clip=0, loss_scale=4, train_wall=84, gb_free=14.6, wall=10414
2023-08-20 04:23:03 | INFO | train_inner | epoch 009:    418 / 1474 loss=2.105, trans_loss=3.429, nll_loss=1.619, w2v_ctc_loss=1.187, task_loss=0.889, contrastive_loss=0.122, total=4175.09, n_correct=2538.52, ppl=3.07, accuracy=60.802, wps=14516.9, ups=1.16, wpb=12466.4, bsz=457.5, num_updates=12200, lr=0.000128037, gnorm=0.441, clip=0, loss_scale=4, train_wall=85, gb_free=17, wall=10499
2023-08-20 04:24:28 | INFO | train_inner | epoch 009:    518 / 1474 loss=2.137, trans_loss=3.432, nll_loss=1.622, w2v_ctc_loss=1.209, task_loss=0.925, contrastive_loss=0.179, total=4118.47, n_correct=2498.68, ppl=3.08, accuracy=60.67, wps=14440.8, ups=1.17, wpb=12292.7, bsz=439.7, num_updates=12300, lr=0.000127515, gnorm=0.565, clip=0, loss_scale=4, train_wall=84, gb_free=17.1, wall=10585
2023-08-20 04:25:54 | INFO | train_inner | epoch 009:    618 / 1474 loss=2.131, trans_loss=3.423, nll_loss=1.614, w2v_ctc_loss=1.189, task_loss=0.887, contrastive_loss=0.233, total=4141.76, n_correct=2521.87, ppl=3.06, accuracy=60.889, wps=14408.9, ups=1.16, wpb=12378.7, bsz=462.3, num_updates=12400, lr=0.000127, gnorm=0.49, clip=0, loss_scale=4, train_wall=85, gb_free=16.6, wall=10670
2023-08-20 04:27:18 | INFO | train_inner | epoch 009:    718 / 1474 loss=2.117, trans_loss=3.43, nll_loss=1.621, w2v_ctc_loss=1.205, task_loss=0.92, contrastive_loss=0.125, total=4075.02, n_correct=2471.94, ppl=3.08, accuracy=60.661, wps=14395.1, ups=1.18, wpb=12176, bsz=443.4, num_updates=12500, lr=0.000126491, gnorm=0.479, clip=0, loss_scale=4, train_wall=84, gb_free=17.2, wall=10755
2023-08-20 04:28:45 | INFO | train_inner | epoch 009:    818 / 1474 loss=2.181, trans_loss=3.425, nll_loss=1.616, w2v_ctc_loss=1.2, task_loss=0.802, contrastive_loss=0.371, total=4214.28, n_correct=2564.83, ppl=3.06, accuracy=60.86, wps=14551.5, ups=1.16, wpb=12593.1, bsz=499.2, num_updates=12600, lr=0.000125988, gnorm=0.544, clip=0, loss_scale=4, train_wall=86, gb_free=16.4, wall=10842
2023-08-20 04:30:12 | INFO | train_inner | epoch 009:    918 / 1474 loss=2.152, trans_loss=3.431, nll_loss=1.618, w2v_ctc_loss=1.194, task_loss=0.908, contrastive_loss=0.344, total=4157.54, n_correct=2530.36, ppl=3.07, accuracy=60.862, wps=14289.2, ups=1.15, wpb=12400.4, bsz=452, num_updates=12700, lr=0.000125491, gnorm=0.477, clip=0, loss_scale=4, train_wall=86, gb_free=15.8, wall=10928
2023-08-20 04:31:37 | INFO | train_inner | epoch 009:   1018 / 1474 loss=2.108, trans_loss=3.432, nll_loss=1.622, w2v_ctc_loss=1.191, task_loss=0.992, contrastive_loss=0.127, total=4093.77, n_correct=2487.54, ppl=3.08, accuracy=60.764, wps=14271.7, ups=1.17, wpb=12221, bsz=423.7, num_updates=12800, lr=0.000125, gnorm=0.466, clip=0, loss_scale=4, train_wall=85, gb_free=17, wall=11014
2023-08-20 04:33:03 | INFO | train_inner | epoch 009:   1118 / 1474 loss=2.102, trans_loss=3.428, nll_loss=1.613, w2v_ctc_loss=1.175, task_loss=0.842, contrastive_loss=0.15, total=4173.97, n_correct=2554.8, ppl=3.06, accuracy=61.208, wps=14582.3, ups=1.17, wpb=12442.6, bsz=471.4, num_updates=12900, lr=0.000124515, gnorm=0.469, clip=0, loss_scale=4, train_wall=85, gb_free=15.8, wall=11099
2023-08-20 04:34:29 | INFO | train_inner | epoch 009:   1218 / 1474 loss=2.115, trans_loss=3.429, nll_loss=1.619, w2v_ctc_loss=1.2, task_loss=0.93, contrastive_loss=0.132, total=4144.88, n_correct=2520.64, ppl=3.07, accuracy=60.813, wps=14272.5, ups=1.15, wpb=12376.1, bsz=450, num_updates=13000, lr=0.000124035, gnorm=0.455, clip=0, loss_scale=4, train_wall=86, gb_free=16, wall=11186
2023-08-20 04:35:55 | INFO | train_inner | epoch 009:   1318 / 1474 loss=2.139, trans_loss=3.417, nll_loss=1.602, w2v_ctc_loss=1.173, task_loss=0.808, contrastive_loss=0.327, total=4200.61, n_correct=2576.83, ppl=3.04, accuracy=61.344, wps=14632.7, ups=1.17, wpb=12534.4, bsz=490.7, num_updates=13100, lr=0.00012356, gnorm=0.449, clip=0, loss_scale=4, train_wall=85, gb_free=15.1, wall=11272
2023-08-20 04:37:20 | INFO | train_inner | epoch 009:   1418 / 1474 loss=2.103, trans_loss=3.432, nll_loss=1.621, w2v_ctc_loss=1.192, task_loss=0.951, contrastive_loss=0.109, total=4075.96, n_correct=2482.47, ppl=3.08, accuracy=60.905, wps=14304.8, ups=1.18, wpb=12161.1, bsz=430.6, num_updates=13200, lr=0.000123091, gnorm=0.526, clip=0, loss_scale=4, train_wall=84, gb_free=17.3, wall=11357
2023-08-20 04:38:07 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 04:38:41 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.1 | trans_loss 5.315 | nll_loss 2.619 | w2v_ctc_loss 1.489 | task_loss 4.472 | contrastive_loss 0.296 | total 4003.4 | n_correct 2554.7 | ppl 6.14 | accuracy 63.813 | uer 21.692 | wer 23.634 | raw_wer 23.634 | bleu 19.83 | wps 1581.3 | wpb 4003.4 | bsz 141.8 | num_updates 13256 | best_bleu 19.83
2023-08-20 04:38:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 13256 updates
2023-08-20 04:38:41 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 04:38:47 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 04:38:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt (epoch 9 @ 13256 updates, score 19.83) (writing took 11.903618076001294 seconds)
2023-08-20 04:38:53 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-08-20 04:38:53 | INFO | train | epoch 009 | loss 2.121 | trans_loss 3.426 | nll_loss 1.614 | w2v_ctc_loss 1.188 | task_loss 0.885 | contrastive_loss 0.195 | total 4138.65 | n_correct 2522.41 | ppl 3.06 | accuracy 60.948 | wps 13358.1 | ups 1.08 | wpb 12355.8 | bsz 458.5 | num_updates 13256 | lr 0.000122831 | gnorm 0.47 | clip 0 | loss_scale 4 | train_wall 1250 | gb_free 11.1 | wall 11450
2023-08-20 04:38:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 04:38:54 | INFO | fairseq.trainer | begin training epoch 10
2023-08-20 04:38:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 04:39:38 | INFO | train_inner | epoch 010:     44 / 1474 loss=2.092, trans_loss=3.409, nll_loss=1.592, w2v_ctc_loss=1.151, task_loss=0.826, contrastive_loss=0.208, total=4112.83, n_correct=2538.98, ppl=3.02, accuracy=61.733, wps=8890.1, ups=0.72, wpb=12275.4, bsz=475.1, num_updates=13300, lr=0.000122628, gnorm=0.412, clip=0, loss_scale=8, train_wall=83, gb_free=15.9, wall=11495
2023-08-20 04:41:04 | INFO | train_inner | epoch 010:    144 / 1474 loss=2.053, trans_loss=3.397, nll_loss=1.578, w2v_ctc_loss=1.136, task_loss=0.853, contrastive_loss=0.123, total=4230.29, n_correct=2620.21, ppl=2.98, accuracy=61.939, wps=14738.2, ups=1.17, wpb=12634.2, bsz=472.7, num_updates=13400, lr=0.000122169, gnorm=0.443, clip=0, loss_scale=8, train_wall=85, gb_free=16.2, wall=11581
2023-08-20 04:42:30 | INFO | train_inner | epoch 010:    244 / 1474 loss=2.087, trans_loss=3.395, nll_loss=1.573, w2v_ctc_loss=1.147, task_loss=0.868, contrastive_loss=0.25, total=4131.59, n_correct=2560.41, ppl=2.98, accuracy=61.972, wps=14391.9, ups=1.17, wpb=12331.3, bsz=463.6, num_updates=13500, lr=0.000121716, gnorm=0.466, clip=0, loss_scale=8, train_wall=85, gb_free=9.5, wall=11666
2023-08-20 04:43:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-20 04:43:56 | INFO | train_inner | epoch 010:    345 / 1474 loss=2.067, trans_loss=3.397, nll_loss=1.58, w2v_ctc_loss=1.142, task_loss=0.889, contrastive_loss=0.162, total=4134.82, n_correct=2555.92, ppl=2.99, accuracy=61.815, wps=14215.2, ups=1.15, wpb=12359, bsz=455.5, num_updates=13600, lr=0.000121268, gnorm=0.584, clip=0, loss_scale=4, train_wall=86, gb_free=17.5, wall=11753
2023-08-20 04:45:22 | INFO | train_inner | epoch 010:    445 / 1474 loss=2.091, trans_loss=3.402, nll_loss=1.583, w2v_ctc_loss=1.13, task_loss=0.862, contrastive_loss=0.336, total=4185.75, n_correct=2585.8, ppl=3, accuracy=61.776, wps=14546.6, ups=1.16, wpb=12496.7, bsz=477.6, num_updates=13700, lr=0.000120824, gnorm=0.446, clip=0, loss_scale=4, train_wall=85, gb_free=16.3, wall=11839
2023-08-20 04:46:48 | INFO | train_inner | epoch 010:    545 / 1474 loss=2.066, trans_loss=3.41, nll_loss=1.59, w2v_ctc_loss=1.155, task_loss=0.966, contrastive_loss=0.113, total=4098.46, n_correct=2527.77, ppl=3.01, accuracy=61.676, wps=14223.5, ups=1.16, wpb=12220.2, bsz=434.8, num_updates=13800, lr=0.000120386, gnorm=0.449, clip=0, loss_scale=4, train_wall=85, gb_free=14.6, wall=11925
2023-08-20 04:48:14 | INFO | train_inner | epoch 010:    645 / 1474 loss=2.098, trans_loss=3.406, nll_loss=1.588, w2v_ctc_loss=1.157, task_loss=0.822, contrastive_loss=0.233, total=4200.52, n_correct=2595.15, ppl=3.01, accuracy=61.782, wps=14628.5, ups=1.17, wpb=12533.8, bsz=485.9, num_updates=13900, lr=0.000119952, gnorm=0.483, clip=0, loss_scale=4, train_wall=85, gb_free=17.2, wall=12011
2023-08-20 04:49:39 | INFO | train_inner | epoch 010:    745 / 1474 loss=2.066, trans_loss=3.398, nll_loss=1.58, w2v_ctc_loss=1.159, task_loss=0.9, contrastive_loss=0.108, total=4107.62, n_correct=2539.25, ppl=2.99, accuracy=61.818, wps=14409.4, ups=1.17, wpb=12265.2, bsz=447.2, num_updates=14000, lr=0.000119523, gnorm=0.419, clip=0, loss_scale=4, train_wall=84, gb_free=16.6, wall=12096
2023-08-20 04:49:39 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 04:50:12 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.109 | trans_loss 5.3 | nll_loss 2.598 | w2v_ctc_loss 1.556 | task_loss 4.503 | contrastive_loss 0.29 | total 4003.4 | n_correct 2570.9 | ppl 6.06 | accuracy 64.218 | uer 21.596 | wer 23.586 | raw_wer 23.586 | bleu 19.99 | wps 1654.5 | wpb 4003.4 | bsz 141.8 | num_updates 14000 | best_bleu 19.99
2023-08-20 04:50:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14000 updates
2023-08-20 04:50:12 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_10_14000.pt
2023-08-20 04:50:14 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_10_14000.pt
2023-08-20 04:50:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_10_14000.pt (epoch 10 @ 14000 updates, score 19.99) (writing took 17.133885463990737 seconds)
2023-08-20 04:51:55 | INFO | train_inner | epoch 010:    845 / 1474 loss=2.052, trans_loss=3.401, nll_loss=1.583, w2v_ctc_loss=1.137, task_loss=0.88, contrastive_loss=0.116, total=4133.58, n_correct=2557.08, ppl=3, accuracy=61.861, wps=9079.6, ups=0.74, wpb=12338.9, bsz=456.2, num_updates=14100, lr=0.000119098, gnorm=0.428, clip=0, loss_scale=4, train_wall=84, gb_free=12.3, wall=12232
2023-08-20 04:53:20 | INFO | train_inner | epoch 010:    945 / 1474 loss=2.067, trans_loss=3.398, nll_loss=1.577, w2v_ctc_loss=1.141, task_loss=0.844, contrastive_loss=0.158, total=4167.18, n_correct=2586.35, ppl=2.98, accuracy=62.065, wps=14646.1, ups=1.18, wpb=12431.2, bsz=473.1, num_updates=14200, lr=0.000118678, gnorm=0.42, clip=0, loss_scale=4, train_wall=84, gb_free=14.5, wall=12317
2023-08-20 04:54:45 | INFO | train_inner | epoch 010:   1045 / 1474 loss=2.057, trans_loss=3.397, nll_loss=1.578, w2v_ctc_loss=1.145, task_loss=0.966, contrastive_loss=0.125, total=4056.76, n_correct=2509.28, ppl=2.99, accuracy=61.854, wps=14161.6, ups=1.17, wpb=12112.7, bsz=430.4, num_updates=14300, lr=0.000118262, gnorm=0.423, clip=0, loss_scale=4, train_wall=85, gb_free=16.7, wall=12402
2023-08-20 04:56:10 | INFO | train_inner | epoch 010:   1145 / 1474 loss=2.065, trans_loss=3.406, nll_loss=1.589, w2v_ctc_loss=1.16, task_loss=0.997, contrastive_loss=0.103, total=4033.34, n_correct=2485.97, ppl=3.01, accuracy=61.636, wps=14184.8, ups=1.18, wpb=12041.8, bsz=418.7, num_updates=14400, lr=0.000117851, gnorm=0.425, clip=0, loss_scale=4, train_wall=84, gb_free=16.4, wall=12487
2023-08-20 04:57:36 | INFO | train_inner | epoch 010:   1245 / 1474 loss=2.054, trans_loss=3.39, nll_loss=1.573, w2v_ctc_loss=1.151, task_loss=0.91, contrastive_loss=0.105, total=4107.17, n_correct=2543.74, ppl=2.98, accuracy=61.934, wps=14333.4, ups=1.17, wpb=12283.9, bsz=445.4, num_updates=14500, lr=0.000117444, gnorm=0.419, clip=0, loss_scale=4, train_wall=85, gb_free=16.1, wall=12573
2023-08-20 04:59:02 | INFO | train_inner | epoch 010:   1345 / 1474 loss=2.059, trans_loss=3.399, nll_loss=1.582, w2v_ctc_loss=1.149, task_loss=0.882, contrastive_loss=0.12, total=4148.52, n_correct=2570.16, ppl=2.99, accuracy=61.954, wps=14465.2, ups=1.17, wpb=12389, bsz=458.7, num_updates=14600, lr=0.000117041, gnorm=0.493, clip=0, loss_scale=4, train_wall=85, gb_free=16.5, wall=12658
2023-08-20 05:00:28 | INFO | train_inner | epoch 010:   1445 / 1474 loss=2.118, trans_loss=3.41, nll_loss=1.592, w2v_ctc_loss=1.131, task_loss=0.838, contrastive_loss=0.372, total=4181.64, n_correct=2581.1, ppl=3.01, accuracy=61.725, wps=14488, ups=1.16, wpb=12471.6, bsz=479.6, num_updates=14700, lr=0.000116642, gnorm=0.529, clip=0, loss_scale=4, train_wall=85, gb_free=15.6, wall=12745
2023-08-20 05:00:52 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 05:01:25 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.089 | trans_loss 5.293 | nll_loss 2.59 | w2v_ctc_loss 1.498 | task_loss 4.457 | contrastive_loss 0.302 | total 4003.4 | n_correct 2571.7 | ppl 6.02 | accuracy 64.238 | uer 21.32 | wer 23.247 | raw_wer 23.247 | bleu 20.28 | wps 1661.5 | wpb 4003.4 | bsz 141.8 | num_updates 14729 | best_bleu 20.28
2023-08-20 05:01:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14729 updates
2023-08-20 05:01:25 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 05:01:34 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 05:01:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt (epoch 10 @ 14729 updates, score 20.28) (writing took 15.46580491703935 seconds)
2023-08-20 05:01:40 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-08-20 05:01:40 | INFO | train | epoch 010 | loss 2.073 | trans_loss 3.4 | nll_loss 1.582 | w2v_ctc_loss 1.144 | task_loss 0.886 | contrastive_loss 0.184 | total 4138.4 | n_correct 2559.63 | ppl 2.99 | accuracy 61.851 | wps 13312.5 | ups 1.08 | wpb 12355.1 | bsz 458.4 | num_updates 14729 | lr 0.000116527 | gnorm 0.46 | clip 0 | loss_scale 4 | train_wall 1249 | gb_free 16.9 | wall 12817
2023-08-20 05:01:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 05:01:41 | INFO | fairseq.trainer | begin training epoch 11
2023-08-20 05:01:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 05:02:48 | INFO | train_inner | epoch 011:     71 / 1474 loss=2.046, trans_loss=3.379, nll_loss=1.554, w2v_ctc_loss=1.119, task_loss=0.824, contrastive_loss=0.19, total=4161.32, n_correct=2604.61, ppl=2.94, accuracy=62.591, wps=8895.1, ups=0.72, wpb=12424.2, bsz=475.2, num_updates=14800, lr=0.000116248, gnorm=0.508, clip=0, loss_scale=4, train_wall=83, gb_free=11.2, wall=12884
2023-08-20 05:04:13 | INFO | train_inner | epoch 011:    171 / 1474 loss=2.029, trans_loss=3.381, nll_loss=1.559, w2v_ctc_loss=1.119, task_loss=0.907, contrastive_loss=0.118, total=4105.22, n_correct=2566.99, ppl=2.95, accuracy=62.53, wps=14302.8, ups=1.17, wpb=12264.3, bsz=450.5, num_updates=14900, lr=0.000115857, gnorm=0.446, clip=0, loss_scale=4, train_wall=85, gb_free=15.8, wall=12970
2023-08-20 05:05:40 | INFO | train_inner | epoch 011:    271 / 1474 loss=2.004, trans_loss=3.373, nll_loss=1.548, w2v_ctc_loss=1.099, task_loss=0.925, contrastive_loss=0.097, total=4109.58, n_correct=2579.93, ppl=2.92, accuracy=62.778, wps=14218, ups=1.16, wpb=12274.9, bsz=441.4, num_updates=15000, lr=0.00011547, gnorm=0.398, clip=0, loss_scale=4, train_wall=86, gb_free=16.5, wall=13056
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:0')
2023-08-20 05:06:41 | INFO | train_inner | epoch 011:    371 / 1474 loss=2.092, trans_loss=5.015, nll_loss=2.305, w2v_ctc_loss=0.827, task_loss=1.354, contrastive_loss=0.085, total=4103.77, n_correct=2568.42, ppl=4.94, accuracy=62.587, wps=13382.8, ups=1.62, wpb=8253.5, bsz=299, num_updates=15100, lr=0.000115087, gnorm=0.532, clip=0, loss_scale=4, train_wall=61, gb_free=13.7, wall=13118
2023-08-20 05:07:43 | INFO | train_inner | epoch 011:    471 / 1474 loss=2.111, trans_loss=5.052, nll_loss=2.331, w2v_ctc_loss=0.823, task_loss=1.368, contrastive_loss=0.204, total=4117.47, n_correct=2568.85, ppl=5.03, accuracy=62.389, wps=13265.9, ups=1.61, wpb=8234.9, bsz=303.7, num_updates=15200, lr=0.000114708, gnorm=0.574, clip=0, loss_scale=4, train_wall=61, gb_free=15.9, wall=13180
2023-08-20 05:08:46 | INFO | train_inner | epoch 011:    571 / 1474 loss=2.11, trans_loss=5.049, nll_loss=2.327, w2v_ctc_loss=0.833, task_loss=1.421, contrastive_loss=0.198, total=4070.21, n_correct=2541.41, ppl=5.02, accuracy=62.439, wps=13096.6, ups=1.61, wpb=8140.4, bsz=292.9, num_updates=15300, lr=0.000114332, gnorm=0.556, clip=0, loss_scale=4, train_wall=61, gb_free=12.8, wall=13242
2023-08-20 05:09:48 | INFO | train_inner | epoch 011:    671 / 1474 loss=2.116, trans_loss=5.044, nll_loss=2.321, w2v_ctc_loss=0.834, task_loss=1.3, contrastive_loss=0.261, total=4161.7, n_correct=2600.05, ppl=5, accuracy=62.476, wps=13430, ups=1.61, wpb=8323.4, bsz=311.8, num_updates=15400, lr=0.000113961, gnorm=0.628, clip=0, loss_scale=4, train_wall=61, gb_free=16.9, wall=13304
2023-08-20 05:10:50 | INFO | train_inner | epoch 011:    771 / 1474 loss=2.112, trans_loss=5.059, nll_loss=2.34, w2v_ctc_loss=0.855, task_loss=1.371, contrastive_loss=0.085, total=4151.24, n_correct=2588.41, ppl=5.06, accuracy=62.353, wps=13227.6, ups=1.59, wpb=8302.5, bsz=301, num_updates=15500, lr=0.000113592, gnorm=0.628, clip=0, loss_scale=4, train_wall=62, gb_free=15.5, wall=13367
2023-08-20 05:11:52 | INFO | train_inner | epoch 011:    871 / 1474 loss=2.104, trans_loss=5.053, nll_loss=2.331, w2v_ctc_loss=0.843, task_loss=1.39, contrastive_loss=0.075, total=4128.97, n_correct=2575.38, ppl=5.03, accuracy=62.373, wps=13303.9, ups=1.61, wpb=8257.9, bsz=294.6, num_updates=15600, lr=0.000113228, gnorm=0.657, clip=1, loss_scale=4, train_wall=61, gb_free=16.2, wall=13429
2023-08-20 05:12:54 | INFO | train_inner | epoch 011:    971 / 1474 loss=2.097, trans_loss=5.05, nll_loss=2.329, w2v_ctc_loss=0.833, task_loss=1.339, contrastive_loss=0.085, total=4144.6, n_correct=2590.98, ppl=5.02, accuracy=62.515, wps=13404.4, ups=1.62, wpb=8289.2, bsz=304.5, num_updates=15700, lr=0.000112867, gnorm=0.568, clip=0, loss_scale=8, train_wall=61, gb_free=17.6, wall=13491
2023-08-20 05:13:56 | INFO | train_inner | epoch 011:   1071 / 1474 loss=2.094, trans_loss=5.041, nll_loss=2.318, w2v_ctc_loss=0.832, task_loss=1.298, contrastive_loss=0.102, total=4153.33, n_correct=2605, ppl=4.99, accuracy=62.721, wps=13483.9, ups=1.62, wpb=8306.7, bsz=310.5, num_updates=15800, lr=0.000112509, gnorm=0.535, clip=0, loss_scale=8, train_wall=61, gb_free=15.5, wall=13553
2023-08-20 05:14:58 | INFO | train_inner | epoch 011:   1171 / 1474 loss=2.098, trans_loss=5.049, nll_loss=2.328, w2v_ctc_loss=0.832, task_loss=1.313, contrastive_loss=0.099, total=4177.49, n_correct=2610.98, ppl=5.02, accuracy=62.501, wps=13431, ups=1.61, wpb=8355, bsz=311.7, num_updates=15900, lr=0.000112154, gnorm=0.54, clip=0, loss_scale=8, train_wall=61, gb_free=16.6, wall=13615
2023-08-20 05:16:01 | INFO | train_inner | epoch 011:   1271 / 1474 loss=2.106, trans_loss=5.043, nll_loss=2.321, w2v_ctc_loss=0.843, task_loss=1.322, contrastive_loss=0.147, total=4150.45, n_correct=2598.93, ppl=5, accuracy=62.618, wps=13097, ups=1.58, wpb=8300.9, bsz=307.4, num_updates=16000, lr=0.000111803, gnorm=0.533, clip=0, loss_scale=8, train_wall=62, gb_free=16.1, wall=13678
2023-08-20 05:16:01 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:1')
2023-08-20 05:16:34 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.067 | trans_loss 5.277 | nll_loss 2.571 | w2v_ctc_loss 1.465 | task_loss 4.476 | contrastive_loss 0.294 | total 4003.4 | n_correct 2584.8 | ppl 5.94 | accuracy 64.565 | uer 20.699 | wer 22.456 | raw_wer 22.456 | bleu 19.66 | wps 1662.9 | wpb 4003.4 | bsz 141.8 | num_updates 16000 | best_bleu 20.28
2023-08-20 05:16:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16000 updates
2023-08-20 05:16:34 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_11_16000.pt
2023-08-20 05:16:36 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_11_16000.pt
2023-08-20 05:16:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_11_16000.pt (epoch 11 @ 16000 updates, score 19.66) (writing took 9.804338506015483 seconds)
2023-08-20 05:17:46 | INFO | train_inner | epoch 011:   1371 / 1474 loss=2.113, trans_loss=5.042, nll_loss=2.32, w2v_ctc_loss=0.822, task_loss=1.231, contrastive_loss=0.32, total=4191.95, n_correct=2623.18, ppl=4.99, accuracy=62.577, wps=8002.3, ups=0.95, wpb=8383.9, bsz=326.8, num_updates=16100, lr=0.000111456, gnorm=0.548, clip=0, loss_scale=8, train_wall=61, gb_free=15.3, wall=13783
2023-08-20 05:18:48 | INFO | train_inner | epoch 011:   1471 / 1474 loss=2.091, trans_loss=5.042, nll_loss=2.319, w2v_ctc_loss=0.831, task_loss=1.261, contrastive_loss=0.093, total=4172.29, n_correct=2615.78, ppl=4.99, accuracy=62.694, wps=13560.2, ups=1.63, wpb=8344.6, bsz=315.6, num_updates=16200, lr=0.000111111, gnorm=0.537, clip=0, loss_scale=8, train_wall=61, gb_free=16, wall=13844
2023-08-20 05:18:49 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 05:19:22 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.061 | trans_loss 5.268 | nll_loss 2.558 | w2v_ctc_loss 1.467 | task_loss 4.486 | contrastive_loss 0.293 | total 4003.4 | n_correct 2587.5 | ppl 5.89 | accuracy 64.633 | uer 21.211 | wer 23.191 | raw_wer 23.191 | bleu 20.17 | wps 1645.7 | wpb 4003.4 | bsz 141.8 | num_updates 16203 | best_bleu 20.28
2023-08-20 05:19:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16203 updates
2023-08-20 05:19:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_20.1708.pt
2023-08-20 05:19:25 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_20.1708.pt
2023-08-20 05:19:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_20.1708.pt (epoch 11 @ 16203 updates, score 20.17) (writing took 7.957396736019291 seconds)
2023-08-20 05:19:31 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-08-20 05:19:31 | INFO | train | epoch 011 | loss 2.082 | trans_loss 4.626 | nll_loss 2.13 | w2v_ctc_loss 0.903 | task_loss 1.218 | contrastive_loss 0.137 | total 4138.65 | n_correct 2588.89 | ppl 4.38 | accuracy 62.554 | wps 12434.9 | ups 1.38 | wpb 9029.1 | bsz 333.7 | num_updates 16203 | lr 0.000111101 | gnorm 0.545 | clip 0.1 | loss_scale 8 | train_wall 967 | gb_free 16.9 | wall 13887
2023-08-20 05:19:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 05:19:31 | INFO | fairseq.trainer | begin training epoch 12
2023-08-20 05:19:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 05:20:38 | INFO | train_inner | epoch 012:     97 / 1474 loss=2.076, trans_loss=5.007, nll_loss=2.273, w2v_ctc_loss=0.815, task_loss=1.274, contrastive_loss=0.124, total=4138.25, n_correct=2622.56, ppl=4.83, accuracy=63.374, wps=7508.6, ups=0.91, wpb=8276.5, bsz=312.6, num_updates=16300, lr=0.00011077, gnorm=0.569, clip=0, loss_scale=8, train_wall=61, gb_free=16.6, wall=13955
2023-08-20 05:21:40 | INFO | train_inner | epoch 012:    197 / 1474 loss=2.087, trans_loss=5.02, nll_loss=2.289, w2v_ctc_loss=0.834, task_loss=1.36, contrastive_loss=0.079, total=4126.75, n_correct=2598.35, ppl=4.89, accuracy=62.964, wps=13349.9, ups=1.62, wpb=8253.5, bsz=296.6, num_updates=16400, lr=0.000110432, gnorm=0.715, clip=0, loss_scale=8, train_wall=61, gb_free=16.2, wall=14017
2023-08-20 05:22:42 | INFO | train_inner | epoch 012:    297 / 1474 loss=2.086, trans_loss=5.022, nll_loss=2.293, w2v_ctc_loss=0.823, task_loss=1.225, contrastive_loss=0.119, total=4221.98, n_correct=2660.44, ppl=4.9, accuracy=63.014, wps=13493.2, ups=1.6, wpb=8444, bsz=326, num_updates=16500, lr=0.000110096, gnorm=0.772, clip=0, loss_scale=8, train_wall=62, gb_free=16.1, wall=14079
2023-08-20 05:23:44 | INFO | train_inner | epoch 012:    397 / 1474 loss=2.082, trans_loss=5.021, nll_loss=2.291, w2v_ctc_loss=0.822, task_loss=1.333, contrastive_loss=0.086, total=4136.43, n_correct=2608.84, ppl=4.89, accuracy=63.07, wps=13375.3, ups=1.62, wpb=8272.9, bsz=301.9, num_updates=16600, lr=0.000109764, gnorm=0.613, clip=0, loss_scale=8, train_wall=61, gb_free=15.4, wall=14141
2023-08-20 05:24:46 | INFO | train_inner | epoch 012:    497 / 1474 loss=2.09, trans_loss=5.03, nll_loss=2.303, w2v_ctc_loss=0.831, task_loss=1.354, contrastive_loss=0.098, total=4082.65, n_correct=2571.31, ppl=4.94, accuracy=62.981, wps=13280.2, ups=1.63, wpb=8165.3, bsz=297.7, num_updates=16700, lr=0.000109435, gnorm=0.593, clip=0, loss_scale=8, train_wall=61, gb_free=17, wall=14202
2023-08-20 05:25:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-20 05:25:49 | INFO | train_inner | epoch 012:    598 / 1474 loss=2.087, trans_loss=5.02, nll_loss=2.29, w2v_ctc_loss=0.821, task_loss=1.247, contrastive_loss=0.162, total=4216.06, n_correct=2659.13, ppl=4.89, accuracy=63.071, wps=13426, ups=1.59, wpb=8432.1, bsz=320.5, num_updates=16800, lr=0.000109109, gnorm=0.593, clip=0, loss_scale=4, train_wall=62, gb_free=16.7, wall=14265
2023-08-20 05:26:51 | INFO | train_inner | epoch 012:    698 / 1474 loss=2.086, trans_loss=5.012, nll_loss=2.281, w2v_ctc_loss=0.809, task_loss=1.219, contrastive_loss=0.249, total=4201.87, n_correct=2660.06, ppl=4.86, accuracy=63.307, wps=13523.7, ups=1.61, wpb=8403.7, bsz=324.3, num_updates=16900, lr=0.000108786, gnorm=0.595, clip=0, loss_scale=4, train_wall=61, gb_free=11.4, wall=14327
2023-08-20 05:27:53 | INFO | train_inner | epoch 012:    798 / 1474 loss=2.075, trans_loss=5.011, nll_loss=2.279, w2v_ctc_loss=0.818, task_loss=1.36, contrastive_loss=0.085, total=4085.33, n_correct=2581.63, ppl=4.85, accuracy=63.193, wps=13080.3, ups=1.6, wpb=8170.7, bsz=296.8, num_updates=17000, lr=0.000108465, gnorm=0.568, clip=0, loss_scale=4, train_wall=62, gb_free=17.1, wall=14390
2023-08-20 05:28:55 | INFO | train_inner | epoch 012:    898 / 1474 loss=2.08, trans_loss=5.013, nll_loss=2.282, w2v_ctc_loss=0.812, task_loss=1.367, contrastive_loss=0.138, total=4168.55, n_correct=2638.12, ppl=4.86, accuracy=63.286, wps=13448.4, ups=1.61, wpb=8337.1, bsz=305.7, num_updates=17100, lr=0.000108148, gnorm=0.527, clip=0, loss_scale=4, train_wall=61, gb_free=9.2, wall=14452
2023-08-20 05:29:57 | INFO | train_inner | epoch 012:    998 / 1474 loss=2.085, trans_loss=5.02, nll_loss=2.291, w2v_ctc_loss=0.822, task_loss=1.355, contrastive_loss=0.144, total=4118.45, n_correct=2597.42, ppl=4.89, accuracy=63.068, wps=13323.1, ups=1.62, wpb=8236.9, bsz=301.4, num_updates=17200, lr=0.000107833, gnorm=0.56, clip=0, loss_scale=4, train_wall=61, gb_free=11.3, wall=14514
2023-08-20 05:30:59 | INFO | train_inner | epoch 012:   1098 / 1474 loss=2.097, trans_loss=5.026, nll_loss=2.299, w2v_ctc_loss=0.828, task_loss=1.379, contrastive_loss=0.192, total=4068.9, n_correct=2565.2, ppl=4.92, accuracy=63.044, wps=13166, ups=1.62, wpb=8137.8, bsz=293, num_updates=17300, lr=0.000107521, gnorm=0.669, clip=0, loss_scale=4, train_wall=61, gb_free=15.5, wall=14576
2023-08-20 05:32:01 | INFO | train_inner | epoch 012:   1198 / 1474 loss=2.112, trans_loss=5.047, nll_loss=2.326, w2v_ctc_loss=0.856, task_loss=1.304, contrastive_loss=0.164, total=4184.7, n_correct=2616.35, ppl=5.01, accuracy=62.522, wps=13390.1, ups=1.6, wpb=8369.4, bsz=317.6, num_updates=17400, lr=0.000107211, gnorm=0.688, clip=0, loss_scale=4, train_wall=62, gb_free=13.2, wall=14638
2023-08-20 05:33:04 | INFO | train_inner | epoch 012:   1298 / 1474 loss=2.084, trans_loss=5.021, nll_loss=2.292, w2v_ctc_loss=0.831, task_loss=1.487, contrastive_loss=0.072, total=4070.11, n_correct=2562.94, ppl=4.9, accuracy=62.97, wps=13031.9, ups=1.6, wpb=8140.2, bsz=285.4, num_updates=17500, lr=0.000106904, gnorm=0.558, clip=0, loss_scale=4, train_wall=62, gb_free=13.9, wall=14700
2023-08-20 05:34:06 | INFO | train_inner | epoch 012:   1398 / 1474 loss=2.087, trans_loss=5.027, nll_loss=2.3, w2v_ctc_loss=0.814, task_loss=1.344, contrastive_loss=0.178, total=4135.31, n_correct=2607.7, ppl=4.92, accuracy=63.059, wps=13363.7, ups=1.62, wpb=8270.6, bsz=305.4, num_updates=17600, lr=0.0001066, gnorm=0.575, clip=0, loss_scale=4, train_wall=61, gb_free=15.7, wall=14762
2023-08-20 05:34:52 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 05:35:25 | INFO | dev_st | epoch 012 | valid on 'dev_st' subset | loss 4.052 | trans_loss 5.255 | nll_loss 2.541 | w2v_ctc_loss 1.468 | task_loss 4.533 | contrastive_loss 0.294 | total 4003.4 | n_correct 2597.7 | ppl 5.82 | accuracy 64.887 | uer 20.537 | wer 22.326 | raw_wer 22.326 | bleu 20.17 | wps 1601.7 | wpb 4003.4 | bsz 141.8 | num_updates 17676 | best_bleu 20.28
2023-08-20 05:35:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 17676 updates
2023-08-20 05:35:25 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_20.1708.pt
2023-08-20 05:35:33 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_20.1708.pt
2023-08-20 05:35:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_20.1708.pt (epoch 12 @ 17676 updates, score 20.17) (writing took 12.9032872680109 seconds)
2023-08-20 05:35:39 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-08-20 05:35:39 | INFO | train | epoch 012 | loss 2.086 | trans_loss 5.021 | nll_loss 2.292 | w2v_ctc_loss 0.824 | task_loss 1.328 | contrastive_loss 0.133 | total 4138.98 | n_correct 2610.35 | ppl 4.9 | accuracy 63.068 | wps 12593.8 | ups 1.52 | wpb 8278 | bsz 305.7 | num_updates 17676 | lr 0.000106371 | gnorm 0.611 | clip 0 | loss_scale 4 | train_wall 903 | gb_free 12.3 | wall 14856
2023-08-20 05:35:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 05:35:39 | INFO | fairseq.trainer | begin training epoch 13
2023-08-20 05:35:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 05:36:01 | INFO | train_inner | epoch 013:     24 / 1474 loss=2.079, trans_loss=5.021, nll_loss=2.292, w2v_ctc_loss=0.825, task_loss=1.36, contrastive_loss=0.082, total=4097.19, n_correct=2587.81, ppl=4.9, accuracy=63.161, wps=7106.6, ups=0.87, wpb=8194.4, bsz=297.7, num_updates=17700, lr=0.000106299, gnorm=0.546, clip=0, loss_scale=4, train_wall=60, gb_free=15.7, wall=14878
2023-08-20 05:37:03 | INFO | train_inner | epoch 013:    124 / 1474 loss=2.068, trans_loss=4.997, nll_loss=2.26, w2v_ctc_loss=0.813, task_loss=1.322, contrastive_loss=0.093, total=4172.67, n_correct=2652.58, ppl=4.79, accuracy=63.57, wps=13365.6, ups=1.6, wpb=8345.3, bsz=303.7, num_updates=17800, lr=0.000106, gnorm=0.585, clip=0, loss_scale=4, train_wall=62, gb_free=15.7, wall=14940
2023-08-20 05:38:06 | INFO | train_inner | epoch 013:    224 / 1474 loss=2.088, trans_loss=5.003, nll_loss=2.27, w2v_ctc_loss=0.809, task_loss=1.244, contrastive_loss=0.311, total=4191.23, n_correct=2662.51, ppl=4.82, accuracy=63.526, wps=13359.8, ups=1.59, wpb=8382.5, bsz=326.3, num_updates=17900, lr=0.000105703, gnorm=0.594, clip=0, loss_scale=4, train_wall=62, gb_free=16.6, wall=15003
2023-08-20 05:39:08 | INFO | train_inner | epoch 013:    324 / 1474 loss=2.055, trans_loss=4.985, nll_loss=2.245, w2v_ctc_loss=0.796, task_loss=1.396, contrastive_loss=0.075, total=4102.25, n_correct=2627.15, ppl=4.74, accuracy=64.042, wps=13162.9, ups=1.6, wpb=8204.5, bsz=292.6, num_updates=18000, lr=0.000105409, gnorm=0.525, clip=0, loss_scale=4, train_wall=61, gb_free=15.9, wall=15065
2023-08-20 05:39:08 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 05:39:43 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.039 | trans_loss 5.248 | nll_loss 2.534 | w2v_ctc_loss 1.441 | task_loss 4.496 | contrastive_loss 0.29 | total 4003.4 | n_correct 2605 | ppl 5.79 | accuracy 65.07 | uer 20.975 | wer 22.919 | raw_wer 22.919 | bleu 20.74 | wps 1553.6 | wpb 4003.4 | bsz 141.8 | num_updates 18000 | best_bleu 20.74
2023-08-20 05:39:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 18000 updates
2023-08-20 05:39:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_13_18000.pt
2023-08-20 05:39:45 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_13_18000.pt
2023-08-20 05:39:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_13_18000.pt (epoch 13 @ 18000 updates, score 20.74) (writing took 14.263526194961742 seconds)
2023-08-20 05:40:58 | INFO | train_inner | epoch 013:    424 / 1474 loss=2.063, trans_loss=4.992, nll_loss=2.254, w2v_ctc_loss=0.805, task_loss=1.23, contrastive_loss=0.13, total=4205.83, n_correct=2686.28, ppl=4.77, accuracy=63.87, wps=7660.2, ups=0.91, wpb=8411.7, bsz=324.4, num_updates=18100, lr=0.000105118, gnorm=0.52, clip=0, loss_scale=4, train_wall=60, gb_free=16.1, wall=15175
2023-08-20 05:42:00 | INFO | train_inner | epoch 013:    524 / 1474 loss=2.074, trans_loss=5.002, nll_loss=2.267, w2v_ctc_loss=0.809, task_loss=1.284, contrastive_loss=0.16, total=4186.08, n_correct=2659.62, ppl=4.81, accuracy=63.535, wps=13486.3, ups=1.61, wpb=8372.2, bsz=317, num_updates=18200, lr=0.000104828, gnorm=0.619, clip=0, loss_scale=4, train_wall=61, gb_free=16.7, wall=15237
2023-08-20 05:43:02 | INFO | train_inner | epoch 013:    624 / 1474 loss=2.062, trans_loss=4.996, nll_loss=2.26, w2v_ctc_loss=0.811, task_loss=1.314, contrastive_loss=0.075, total=4152.83, n_correct=2646.77, ppl=4.79, accuracy=63.734, wps=13384.1, ups=1.61, wpb=8305.7, bsz=304.5, num_updates=18300, lr=0.000104542, gnorm=0.555, clip=0, loss_scale=4, train_wall=61, gb_free=15.6, wall=15299
2023-08-20 05:44:05 | INFO | train_inner | epoch 013:    724 / 1474 loss=2.071, trans_loss=4.999, nll_loss=2.263, w2v_ctc_loss=0.82, task_loss=1.438, contrastive_loss=0.076, total=4109.56, n_correct=2609.26, ppl=4.8, accuracy=63.492, wps=13240, ups=1.61, wpb=8219.1, bsz=289.8, num_updates=18400, lr=0.000104257, gnorm=0.544, clip=0, loss_scale=4, train_wall=61, gb_free=14.5, wall=15361
2023-08-20 05:45:07 | INFO | train_inner | epoch 013:    824 / 1474 loss=2.073, trans_loss=5.004, nll_loss=2.27, w2v_ctc_loss=0.813, task_loss=1.368, contrastive_loss=0.119, total=4114.63, n_correct=2610.59, ppl=4.82, accuracy=63.447, wps=13099.1, ups=1.59, wpb=8229.3, bsz=302.5, num_updates=18500, lr=0.000103975, gnorm=0.552, clip=0, loss_scale=4, train_wall=62, gb_free=16.6, wall=15424
2023-08-20 05:46:09 | INFO | train_inner | epoch 013:    924 / 1474 loss=2.06, trans_loss=4.997, nll_loss=2.261, w2v_ctc_loss=0.801, task_loss=1.349, contrastive_loss=0.084, total=4102.25, n_correct=2613.49, ppl=4.79, accuracy=63.709, wps=13394.4, ups=1.63, wpb=8204.5, bsz=297.5, num_updates=18600, lr=0.000103695, gnorm=0.527, clip=0, loss_scale=4, train_wall=60, gb_free=15.4, wall=15485
2023-08-20 05:47:10 | INFO | train_inner | epoch 013:   1024 / 1474 loss=2.08, trans_loss=5.006, nll_loss=2.273, w2v_ctc_loss=0.824, task_loss=1.386, contrastive_loss=0.137, total=4097.02, n_correct=2594.43, ppl=4.83, accuracy=63.325, wps=13356.4, ups=1.63, wpb=8194, bsz=297.1, num_updates=18700, lr=0.000103418, gnorm=0.603, clip=0, loss_scale=4, train_wall=61, gb_free=16.4, wall=15547
2023-08-20 05:48:12 | INFO | train_inner | epoch 013:   1124 / 1474 loss=2.063, trans_loss=4.991, nll_loss=2.254, w2v_ctc_loss=0.804, task_loss=1.339, contrastive_loss=0.116, total=4082.89, n_correct=2604.53, ppl=4.77, accuracy=63.791, wps=13158.7, ups=1.61, wpb=8165.8, bsz=300.3, num_updates=18800, lr=0.000103142, gnorm=0.544, clip=0, loss_scale=8, train_wall=61, gb_free=15, wall=15609
2023-08-20 05:49:14 | INFO | train_inner | epoch 013:   1224 / 1474 loss=2.068, trans_loss=5.002, nll_loss=2.267, w2v_ctc_loss=0.817, task_loss=1.399, contrastive_loss=0.076, total=4122.93, n_correct=2620.71, ppl=4.81, accuracy=63.564, wps=13253.9, ups=1.61, wpb=8245.9, bsz=296.7, num_updates=18900, lr=0.000102869, gnorm=0.551, clip=0, loss_scale=8, train_wall=61, gb_free=16.2, wall=15671
2023-08-20 05:50:17 | INFO | train_inner | epoch 013:   1324 / 1474 loss=2.063, trans_loss=4.986, nll_loss=2.248, w2v_ctc_loss=0.799, task_loss=1.308, contrastive_loss=0.174, total=4113.08, n_correct=2632.26, ppl=4.75, accuracy=63.997, wps=13126.9, ups=1.6, wpb=8226.2, bsz=309.2, num_updates=19000, lr=0.000102598, gnorm=0.549, clip=0, loss_scale=8, train_wall=62, gb_free=16.6, wall=15734
2023-08-20 05:51:19 | INFO | train_inner | epoch 013:   1424 / 1474 loss=2.07, trans_loss=4.997, nll_loss=2.262, w2v_ctc_loss=0.801, task_loss=1.308, contrastive_loss=0.182, total=4174.18, n_correct=2658.52, ppl=4.8, accuracy=63.69, wps=13467, ups=1.61, wpb=8348.4, bsz=310.4, num_updates=19100, lr=0.000102329, gnorm=0.548, clip=0, loss_scale=8, train_wall=61, gb_free=16.1, wall=15796
2023-08-20 05:51:50 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 05:52:22 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.04 | trans_loss 5.242 | nll_loss 2.52 | w2v_ctc_loss 1.462 | task_loss 4.489 | contrastive_loss 0.279 | total 4003.4 | n_correct 2605.4 | ppl 5.73 | accuracy 65.08 | uer 20.728 | wer 22.699 | raw_wer 22.699 | bleu 20.66 | wps 1635.5 | wpb 4003.4 | bsz 141.8 | num_updates 19150 | best_bleu 20.74
2023-08-20 05:52:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 19150 updates
2023-08-20 05:52:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_20.6604.pt
2023-08-20 05:52:25 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_20.6604.pt
2023-08-20 05:52:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_20.6604.pt (epoch 13 @ 19150 updates, score 20.66) (writing took 8.529961013002321 seconds)
2023-08-20 05:52:31 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-08-20 05:52:31 | INFO | train | epoch 013 | loss 2.068 | trans_loss 4.996 | nll_loss 2.26 | w2v_ctc_loss 0.809 | task_loss 1.328 | contrastive_loss 0.129 | total 4138.65 | n_correct 2635.44 | ppl 4.79 | accuracy 63.679 | wps 12050.5 | ups 1.46 | wpb 8277.3 | bsz 305.7 | num_updates 19150 | lr 0.000102195 | gnorm 0.558 | clip 0 | loss_scale 8 | train_wall 903 | gb_free 17.3 | wall 15868
2023-08-20 05:52:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 05:52:32 | INFO | fairseq.trainer | begin training epoch 14
2023-08-20 05:52:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 05:53:09 | INFO | train_inner | epoch 014:     50 / 1474 loss=2.047, trans_loss=4.971, nll_loss=2.229, w2v_ctc_loss=0.795, task_loss=1.221, contrastive_loss=0.091, total=4177.38, n_correct=2684.53, ppl=4.69, accuracy=64.263, wps=7555.5, ups=0.9, wpb=8354.8, bsz=321.6, num_updates=19200, lr=0.000102062, gnorm=0.574, clip=0, loss_scale=8, train_wall=61, gb_free=16.8, wall=15906
2023-08-20 05:54:11 | INFO | train_inner | epoch 014:    150 / 1474 loss=2.036, trans_loss=4.957, nll_loss=2.209, w2v_ctc_loss=0.784, task_loss=1.318, contrastive_loss=0.072, total=4098.74, n_correct=2648.22, ppl=4.62, accuracy=64.611, wps=13349, ups=1.63, wpb=8197.5, bsz=303.4, num_updates=19300, lr=0.000101797, gnorm=0.553, clip=0, loss_scale=8, train_wall=61, gb_free=16.3, wall=15968
2023-08-20 05:55:13 | INFO | train_inner | epoch 014:    250 / 1474 loss=2.059, trans_loss=4.976, nll_loss=2.234, w2v_ctc_loss=0.796, task_loss=1.387, contrastive_loss=0.17, total=4099.27, n_correct=2628.13, ppl=4.7, accuracy=64.112, wps=13283, ups=1.62, wpb=8198.5, bsz=294.5, num_updates=19400, lr=0.000101535, gnorm=0.597, clip=0, loss_scale=8, train_wall=61, gb_free=15.9, wall=16029
2023-08-20 05:56:15 | INFO | train_inner | epoch 014:    350 / 1474 loss=2.054, trans_loss=4.98, nll_loss=2.239, w2v_ctc_loss=0.8, task_loss=1.261, contrastive_loss=0.112, total=4161.31, n_correct=2666.7, ppl=4.72, accuracy=64.083, wps=13411.4, ups=1.61, wpb=8322.6, bsz=316.7, num_updates=19500, lr=0.000101274, gnorm=0.689, clip=0, loss_scale=8, train_wall=61, gb_free=16.4, wall=16091
2023-08-20 05:57:17 | INFO | train_inner | epoch 014:    450 / 1474 loss=2.044, trans_loss=4.974, nll_loss=2.231, w2v_ctc_loss=0.787, task_loss=1.314, contrastive_loss=0.086, total=4153.74, n_correct=2670.51, ppl=4.7, accuracy=64.292, wps=13434.2, ups=1.62, wpb=8307.5, bsz=305.8, num_updates=19600, lr=0.000101015, gnorm=0.57, clip=0, loss_scale=8, train_wall=61, gb_free=15.3, wall=16153
2023-08-20 05:58:19 | INFO | train_inner | epoch 014:    550 / 1474 loss=2.055, trans_loss=4.976, nll_loss=2.233, w2v_ctc_loss=0.807, task_loss=1.441, contrastive_loss=0.082, total=4064.6, n_correct=2604.66, ppl=4.7, accuracy=64.082, wps=13083.9, ups=1.61, wpb=8129.2, bsz=288.5, num_updates=19700, lr=0.000100759, gnorm=0.57, clip=0, loss_scale=8, train_wall=61, gb_free=15.6, wall=16215
2023-08-20 05:59:21 | INFO | train_inner | epoch 014:    650 / 1474 loss=2.058, trans_loss=4.978, nll_loss=2.237, w2v_ctc_loss=0.799, task_loss=1.313, contrastive_loss=0.147, total=4170.44, n_correct=2672, ppl=4.71, accuracy=64.07, wps=13414, ups=1.61, wpb=8340.9, bsz=308.9, num_updates=19800, lr=0.000100504, gnorm=0.607, clip=0, loss_scale=8, train_wall=61, gb_free=14.3, wall=16278
2023-08-20 06:00:23 | INFO | train_inner | epoch 014:    750 / 1474 loss=2.052, trans_loss=4.971, nll_loss=2.228, w2v_ctc_loss=0.808, task_loss=1.313, contrastive_loss=0.082, total=4131.24, n_correct=2651.26, ppl=4.68, accuracy=64.176, wps=13292.5, ups=1.61, wpb=8262.5, bsz=306.7, num_updates=19900, lr=0.000100251, gnorm=0.701, clip=0, loss_scale=8, train_wall=61, gb_free=16.6, wall=16340
2023-08-20 06:00:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-20 06:01:26 | INFO | train_inner | epoch 014:    851 / 1474 loss=2.057, trans_loss=4.969, nll_loss=2.225, w2v_ctc_loss=0.794, task_loss=1.263, contrastive_loss=0.191, total=4190.29, n_correct=2690.23, ppl=4.68, accuracy=64.202, wps=13335, ups=1.59, wpb=8380.6, bsz=321.4, num_updates=20000, lr=0.0001, gnorm=0.599, clip=0, loss_scale=4, train_wall=62, gb_free=14.4, wall=16403
2023-08-20 06:01:26 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 06:01:59 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.037 | trans_loss 5.231 | nll_loss 2.51 | w2v_ctc_loss 1.484 | task_loss 4.5 | contrastive_loss 0.277 | total 4003.4 | n_correct 2612.4 | ppl 5.7 | accuracy 65.255 | uer 20.651 | wer 22.937 | raw_wer 22.937 | bleu 21.13 | wps 1588.1 | wpb 4003.4 | bsz 141.8 | num_updates 20000 | best_bleu 21.13
2023-08-20 06:01:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20000 updates
2023-08-20 06:01:59 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_14_20000.pt
2023-08-20 06:02:02 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_14_20000.pt
2023-08-20 06:02:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_14_20000.pt (epoch 14 @ 20000 updates, score 21.13) (writing took 14.876307463971898 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:0')
2023-08-20 06:03:16 | INFO | train_inner | epoch 014:    951 / 1474 loss=2.053, trans_loss=4.976, nll_loss=2.235, w2v_ctc_loss=0.793, task_loss=1.331, contrastive_loss=0.121, total=4163.97, n_correct=2667.74, ppl=4.71, accuracy=64.067, wps=7534.1, ups=0.9, wpb=8327.9, bsz=309.5, num_updates=20100, lr=9.97509e-05, gnorm=0.536, clip=0, loss_scale=4, train_wall=61, gb_free=16.7, wall=16513
2023-08-20 06:04:19 | INFO | train_inner | epoch 014:   1051 / 1474 loss=2.048, trans_loss=4.972, nll_loss=2.23, w2v_ctc_loss=0.792, task_loss=1.35, contrastive_loss=0.1, total=4148.16, n_correct=2666.86, ppl=4.69, accuracy=64.29, wps=13259.9, ups=1.6, wpb=8296.3, bsz=300.7, num_updates=20200, lr=9.95037e-05, gnorm=0.53, clip=0, loss_scale=4, train_wall=62, gb_free=15.2, wall=16576
2023-08-20 06:05:21 | INFO | train_inner | epoch 014:   1151 / 1474 loss=2.077, trans_loss=4.976, nll_loss=2.236, w2v_ctc_loss=0.8, task_loss=1.256, contrastive_loss=0.369, total=4223.98, n_correct=2706.13, ppl=4.71, accuracy=64.066, wps=13562.3, ups=1.61, wpb=8448, bsz=326.4, num_updates=20300, lr=9.92583e-05, gnorm=0.55, clip=0, loss_scale=4, train_wall=61, gb_free=16.6, wall=16638
2023-08-20 06:06:23 | INFO | train_inner | epoch 014:   1251 / 1474 loss=2.067, trans_loss=4.989, nll_loss=2.251, w2v_ctc_loss=0.824, task_loss=1.547, contrastive_loss=0.065, total=4028.78, n_correct=2569.92, ppl=4.76, accuracy=63.789, wps=13035.6, ups=1.62, wpb=8057.6, bsz=274.5, num_updates=20400, lr=9.90148e-05, gnorm=0.726, clip=0, loss_scale=4, train_wall=61, gb_free=15.4, wall=16700
2023-08-20 06:07:25 | INFO | train_inner | epoch 014:   1351 / 1474 loss=2.044, trans_loss=4.975, nll_loss=2.233, w2v_ctc_loss=0.789, task_loss=1.27, contrastive_loss=0.08, total=4193.2, n_correct=2697.43, ppl=4.7, accuracy=64.329, wps=13646.8, ups=1.63, wpb=8386.4, bsz=315.4, num_updates=20500, lr=9.8773e-05, gnorm=0.599, clip=0, loss_scale=4, train_wall=61, gb_free=16.7, wall=16761
2023-08-20 06:08:26 | INFO | train_inner | epoch 014:   1451 / 1474 loss=2.048, trans_loss=4.977, nll_loss=2.236, w2v_ctc_loss=0.784, task_loss=1.326, contrastive_loss=0.116, total=4132.58, n_correct=2654.13, ppl=4.71, accuracy=64.225, wps=13428.2, ups=1.62, wpb=8265.2, bsz=304.1, num_updates=20600, lr=9.85329e-05, gnorm=0.536, clip=0, loss_scale=4, train_wall=61, gb_free=16.2, wall=16823
2023-08-20 06:08:40 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:2')
2023-08-20 06:09:13 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.012 | trans_loss 5.226 | nll_loss 2.506 | w2v_ctc_loss 1.412 | task_loss 4.519 | contrastive_loss 0.274 | total 4003.4 | n_correct 2621.1 | ppl 5.68 | accuracy 65.472 | uer 20.261 | wer 22.337 | raw_wer 22.337 | bleu 20.81 | wps 1652.5 | wpb 4003.4 | bsz 141.8 | num_updates 20623 | best_bleu 21.13
2023-08-20 06:09:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20623 updates
2023-08-20 06:09:13 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_20.8104.pt
2023-08-20 06:09:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_20.8104.pt
2023-08-20 06:09:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_20.8104.pt (epoch 14 @ 20623 updates, score 20.81) (writing took 8.769766428973526 seconds)
2023-08-20 06:09:22 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-08-20 06:09:22 | INFO | train | epoch 014 | loss 2.053 | trans_loss 4.975 | nll_loss 2.233 | w2v_ctc_loss 0.797 | task_loss 1.33 | contrastive_loss 0.127 | total 4138.9 | n_correct 2655.87 | ppl 4.7 | accuracy 64.168 | wps 12067.3 | ups 1.46 | wpb 8277.8 | bsz 305.7 | num_updates 20623 | lr 9.8478e-05 | gnorm 0.596 | clip 0 | loss_scale 4 | train_wall 901 | gb_free 15.9 | wall 16879
2023-08-20 06:09:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 06:09:22 | INFO | fairseq.trainer | begin training epoch 15
2023-08-20 06:09:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 06:10:16 | INFO | train_inner | epoch 015:     77 / 1474 loss=2.046, trans_loss=4.962, nll_loss=2.215, w2v_ctc_loss=0.785, task_loss=1.325, contrastive_loss=0.165, total=4093.65, n_correct=2637.22, ppl=4.64, accuracy=64.422, wps=7424.2, ups=0.91, wpb=8187.3, bsz=302.1, num_updates=20700, lr=9.82946e-05, gnorm=0.536, clip=0, loss_scale=4, train_wall=60, gb_free=16, wall=16933
2023-08-20 06:11:19 | INFO | train_inner | epoch 015:    177 / 1474 loss=2.038, trans_loss=4.955, nll_loss=2.207, w2v_ctc_loss=0.791, task_loss=1.382, contrastive_loss=0.074, total=4112.43, n_correct=2657.56, ppl=4.62, accuracy=64.623, wps=13204.5, ups=1.61, wpb=8224.9, bsz=298, num_updates=20800, lr=9.80581e-05, gnorm=0.546, clip=0, loss_scale=4, train_wall=62, gb_free=15.6, wall=16995
2023-08-20 06:12:21 | INFO | train_inner | epoch 015:    277 / 1474 loss=2.033, trans_loss=4.956, nll_loss=2.208, w2v_ctc_loss=0.783, task_loss=1.296, contrastive_loss=0.069, total=4185.73, n_correct=2706.29, ppl=4.62, accuracy=64.655, wps=13543.7, ups=1.62, wpb=8371.5, bsz=311, num_updates=20900, lr=9.78232e-05, gnorm=0.539, clip=0, loss_scale=4, train_wall=61, gb_free=12.7, wall=17057
2023-08-20 06:13:22 | INFO | train_inner | epoch 015:    377 / 1474 loss=2.028, trans_loss=4.944, nll_loss=2.194, w2v_ctc_loss=0.772, task_loss=1.321, contrastive_loss=0.09, total=4173.44, n_correct=2702.08, ppl=4.58, accuracy=64.745, wps=13501.9, ups=1.62, wpb=8346.9, bsz=307.4, num_updates=21000, lr=9.759e-05, gnorm=0.537, clip=0, loss_scale=4, train_wall=61, gb_free=14.5, wall=17119
2023-08-20 06:14:24 | INFO | train_inner | epoch 015:    477 / 1474 loss=2.05, trans_loss=4.959, nll_loss=2.212, w2v_ctc_loss=0.787, task_loss=1.393, contrastive_loss=0.18, total=4076.89, n_correct=2625.25, ppl=4.63, accuracy=64.393, wps=13156.8, ups=1.61, wpb=8153.8, bsz=294.4, num_updates=21100, lr=9.73585e-05, gnorm=0.575, clip=0, loss_scale=4, train_wall=61, gb_free=16.3, wall=17181
2023-08-20 06:15:27 | INFO | train_inner | epoch 015:    577 / 1474 loss=2.037, trans_loss=4.953, nll_loss=2.205, w2v_ctc_loss=0.785, task_loss=1.372, contrastive_loss=0.095, total=4145.22, n_correct=2676.47, ppl=4.61, accuracy=64.568, wps=13321.8, ups=1.61, wpb=8290.4, bsz=300.4, num_updates=21200, lr=9.71286e-05, gnorm=0.562, clip=0, loss_scale=4, train_wall=62, gb_free=16.5, wall=17243
2023-08-20 06:16:28 | INFO | train_inner | epoch 015:    677 / 1474 loss=2.044, trans_loss=4.952, nll_loss=2.203, w2v_ctc_loss=0.788, task_loss=1.353, contrastive_loss=0.138, total=4127.9, n_correct=2666.56, ppl=4.61, accuracy=64.598, wps=13338.3, ups=1.62, wpb=8255.8, bsz=304.9, num_updates=21300, lr=9.69003e-05, gnorm=0.544, clip=0, loss_scale=4, train_wall=61, gb_free=16.6, wall=17305
2023-08-20 06:17:31 | INFO | train_inner | epoch 015:    777 / 1474 loss=2.04, trans_loss=4.961, nll_loss=2.215, w2v_ctc_loss=0.791, task_loss=1.332, contrastive_loss=0.079, total=4182.81, n_correct=2696.65, ppl=4.64, accuracy=64.47, wps=13408.8, ups=1.6, wpb=8365.6, bsz=306.5, num_updates=21400, lr=9.66736e-05, gnorm=0.55, clip=0, loss_scale=4, train_wall=62, gb_free=12.6, wall=17368
2023-08-20 06:18:32 | INFO | train_inner | epoch 015:    877 / 1474 loss=2.038, trans_loss=4.957, nll_loss=2.211, w2v_ctc_loss=0.789, task_loss=1.437, contrastive_loss=0.07, total=4047.26, n_correct=2614.1, ppl=4.63, accuracy=64.589, wps=13173.8, ups=1.63, wpb=8094.5, bsz=286.2, num_updates=21500, lr=9.64486e-05, gnorm=0.559, clip=0, loss_scale=4, train_wall=61, gb_free=14.7, wall=17429
2023-08-20 06:19:34 | INFO | train_inner | epoch 015:    977 / 1474 loss=2.042, trans_loss=4.96, nll_loss=2.214, w2v_ctc_loss=0.782, task_loss=1.323, contrastive_loss=0.153, total=4140.07, n_correct=2673.32, ppl=4.64, accuracy=64.572, wps=13494.8, ups=1.63, wpb=8280.1, bsz=304.7, num_updates=21600, lr=9.6225e-05, gnorm=0.534, clip=0, loss_scale=4, train_wall=61, gb_free=15, wall=17490
2023-08-20 06:20:37 | INFO | train_inner | epoch 015:   1077 / 1474 loss=2.058, trans_loss=4.962, nll_loss=2.217, w2v_ctc_loss=0.784, task_loss=1.242, contrastive_loss=0.313, total=4182.98, n_correct=2695.46, ppl=4.65, accuracy=64.439, wps=13267.6, ups=1.59, wpb=8366, bsz=325, num_updates=21700, lr=9.60031e-05, gnorm=0.593, clip=0, loss_scale=4, train_wall=62, gb_free=12.9, wall=17553
2023-08-20 06:21:39 | INFO | train_inner | epoch 015:   1177 / 1474 loss=2.031, trans_loss=4.954, nll_loss=2.208, w2v_ctc_loss=0.775, task_loss=1.193, contrastive_loss=0.121, total=4187.88, n_correct=2711.19, ppl=4.62, accuracy=64.739, wps=13487.5, ups=1.61, wpb=8375.8, bsz=329.1, num_updates=21800, lr=9.57826e-05, gnorm=0.563, clip=0, loss_scale=4, train_wall=61, gb_free=16.5, wall=17616
2023-08-20 06:22:41 | INFO | train_inner | epoch 015:   1277 / 1474 loss=2.035, trans_loss=4.951, nll_loss=2.202, w2v_ctc_loss=0.789, task_loss=1.372, contrastive_loss=0.073, total=4136.5, n_correct=2674.14, ppl=4.6, accuracy=64.647, wps=13308.1, ups=1.61, wpb=8273, bsz=300.6, num_updates=21900, lr=9.55637e-05, gnorm=0.551, clip=0, loss_scale=4, train_wall=61, gb_free=11.8, wall=17678
2023-08-20 06:23:43 | INFO | train_inner | epoch 015:   1377 / 1474 loss=2.04, trans_loss=4.958, nll_loss=2.211, w2v_ctc_loss=0.796, task_loss=1.376, contrastive_loss=0.064, total=4103.74, n_correct=2648.68, ppl=4.63, accuracy=64.543, wps=13206.6, ups=1.61, wpb=8207.5, bsz=294.1, num_updates=22000, lr=9.53463e-05, gnorm=0.814, clip=0, loss_scale=8, train_wall=61, gb_free=16, wall=17740
2023-08-20 06:23:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 06:24:17 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 4.033 | trans_loss 5.228 | nll_loss 2.506 | w2v_ctc_loss 1.465 | task_loss 4.551 | contrastive_loss 0.289 | total 4003.4 | n_correct 2624.2 | ppl 5.68 | accuracy 65.549 | uer 20.495 | wer 22.624 | raw_wer 22.624 | bleu 21.09 | wps 1610.7 | wpb 4003.4 | bsz 141.8 | num_updates 22000 | best_bleu 21.13
2023-08-20 06:24:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22000 updates
2023-08-20 06:24:17 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_15_22000.pt
2023-08-20 06:24:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_15_22000.pt
2023-08-20 06:24:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_15_22000.pt (epoch 15 @ 22000 updates, score 21.09) (writing took 10.408397438004613 seconds)
2023-08-20 06:25:28 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 06:26:01 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 4.023 | trans_loss 5.218 | nll_loss 2.496 | w2v_ctc_loss 1.466 | task_loss 4.507 | contrastive_loss 0.279 | total 4003.4 | n_correct 2622.1 | ppl 5.64 | accuracy 65.497 | uer 20.025 | wer 22.076 | raw_wer 22.076 | bleu 21.05 | wps 1635.3 | wpb 4003.4 | bsz 141.8 | num_updates 22097 | best_bleu 21.13
2023-08-20 06:26:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22097 updates
2023-08-20 06:26:01 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.0504.pt
2023-08-20 06:26:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.0504.pt
2023-08-20 06:26:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.0504.pt (epoch 15 @ 22097 updates, score 21.05) (writing took 8.06794280098984 seconds)
2023-08-20 06:26:09 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-08-20 06:26:09 | INFO | train | epoch 015 | loss 2.04 | trans_loss 4.956 | nll_loss 2.209 | w2v_ctc_loss 0.786 | task_loss 1.329 | contrastive_loss 0.124 | total 4138.65 | n_correct 2672.53 | ppl 4.62 | accuracy 64.575 | wps 12110.5 | ups 1.46 | wpb 8277.3 | bsz 305.7 | num_updates 22097 | lr 9.51368e-05 | gnorm 0.574 | clip 0 | loss_scale 8 | train_wall 902 | gb_free 16.6 | wall 17886
2023-08-20 06:26:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 06:26:09 | INFO | fairseq.trainer | begin training epoch 16
2023-08-20 06:26:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 06:26:18 | INFO | train_inner | epoch 016:      3 / 1474 loss=2.048, trans_loss=4.966, nll_loss=2.223, w2v_ctc_loss=0.792, task_loss=1.267, contrastive_loss=0.153, total=4154.85, n_correct=2676.74, ppl=4.67, accuracy=64.424, wps=5352.9, ups=0.64, wpb=8309.7, bsz=316.5, num_updates=22100, lr=9.51303e-05, gnorm=0.592, clip=0, loss_scale=8, train_wall=61, gb_free=16.3, wall=17895
2023-08-20 06:27:19 | INFO | train_inner | epoch 016:    103 / 1474 loss=2.017, trans_loss=4.932, nll_loss=2.178, w2v_ctc_loss=0.764, task_loss=1.274, contrastive_loss=0.093, total=4116.45, n_correct=2680.43, ppl=4.53, accuracy=65.115, wps=13476.3, ups=1.64, wpb=8232.9, bsz=314.3, num_updates=22200, lr=9.49158e-05, gnorm=0.628, clip=0, loss_scale=8, train_wall=60, gb_free=16.7, wall=17956
2023-08-20 06:28:22 | INFO | train_inner | epoch 016:    203 / 1474 loss=2.013, trans_loss=4.926, nll_loss=2.17, w2v_ctc_loss=0.762, task_loss=1.367, contrastive_loss=0.067, total=4112.07, n_correct=2684.73, ppl=4.5, accuracy=65.289, wps=13197.8, ups=1.6, wpb=8224.1, bsz=297.4, num_updates=22300, lr=9.47027e-05, gnorm=0.562, clip=0, loss_scale=8, train_wall=61, gb_free=16.6, wall=18019
2023-08-20 06:29:24 | INFO | train_inner | epoch 016:    303 / 1474 loss=2.036, trans_loss=4.941, nll_loss=2.19, w2v_ctc_loss=0.784, task_loss=1.332, contrastive_loss=0.144, total=4160.84, n_correct=2697.75, ppl=4.56, accuracy=64.837, wps=13395.1, ups=1.61, wpb=8321.7, bsz=308.1, num_updates=22400, lr=9.44911e-05, gnorm=0.611, clip=0, loss_scale=8, train_wall=61, gb_free=16.6, wall=18081
2023-08-20 06:30:25 | INFO | train_inner | epoch 016:    403 / 1474 loss=2.033, trans_loss=4.934, nll_loss=2.179, w2v_ctc_loss=0.78, task_loss=1.424, contrastive_loss=0.151, total=4066.97, n_correct=2644.05, ppl=4.53, accuracy=65.013, wps=13211.7, ups=1.62, wpb=8133.9, bsz=287, num_updates=22500, lr=9.42809e-05, gnorm=0.567, clip=0, loss_scale=8, train_wall=61, gb_free=14.4, wall=18142
2023-08-20 06:31:28 | INFO | train_inner | epoch 016:    503 / 1474 loss=2.021, trans_loss=4.937, nll_loss=2.185, w2v_ctc_loss=0.769, task_loss=1.281, contrastive_loss=0.1, total=4168.86, n_correct=2718.14, ppl=4.55, accuracy=65.201, wps=13382.8, ups=1.61, wpb=8337.7, bsz=318.4, num_updates=22600, lr=9.40721e-05, gnorm=0.646, clip=0, loss_scale=8, train_wall=62, gb_free=16, wall=18205
2023-08-20 06:32:29 | INFO | train_inner | epoch 016:    603 / 1474 loss=2.029, trans_loss=4.945, nll_loss=2.194, w2v_ctc_loss=0.782, task_loss=1.32, contrastive_loss=0.066, total=4132.12, n_correct=2678.16, ppl=4.58, accuracy=64.813, wps=13442.6, ups=1.63, wpb=8264.2, bsz=300.3, num_updates=22700, lr=9.38647e-05, gnorm=0.67, clip=0, loss_scale=8, train_wall=61, gb_free=15.7, wall=18266
2023-08-20 06:33:31 | INFO | train_inner | epoch 016:    703 / 1474 loss=2.021, trans_loss=4.937, nll_loss=2.183, w2v_ctc_loss=0.774, task_loss=1.355, contrastive_loss=0.066, total=4102.33, n_correct=2668.17, ppl=4.54, accuracy=65.04, wps=13343.5, ups=1.63, wpb=8204.7, bsz=298.1, num_updates=22800, lr=9.36586e-05, gnorm=0.542, clip=0, loss_scale=8, train_wall=61, gb_free=15.8, wall=18328
2023-08-20 06:34:33 | INFO | train_inner | epoch 016:    803 / 1474 loss=2.018, trans_loss=4.932, nll_loss=2.178, w2v_ctc_loss=0.754, task_loss=1.275, contrastive_loss=0.125, total=4176.5, n_correct=2719.57, ppl=4.53, accuracy=65.116, wps=13453.5, ups=1.61, wpb=8353, bsz=311.3, num_updates=22900, lr=9.34539e-05, gnorm=0.561, clip=0, loss_scale=8, train_wall=61, gb_free=16.7, wall=18390
2023-08-20 06:35:35 | INFO | train_inner | epoch 016:    903 / 1474 loss=2.023, trans_loss=4.935, nll_loss=2.182, w2v_ctc_loss=0.766, task_loss=1.316, contrastive_loss=0.116, total=4150.45, n_correct=2702.96, ppl=4.54, accuracy=65.125, wps=13427.3, ups=1.62, wpb=8300.9, bsz=305.8, num_updates=23000, lr=9.32505e-05, gnorm=0.544, clip=0, loss_scale=8, train_wall=61, gb_free=11.3, wall=18451
2023-08-20 06:36:37 | INFO | train_inner | epoch 016:   1003 / 1474 loss=2.026, trans_loss=4.937, nll_loss=2.185, w2v_ctc_loss=0.774, task_loss=1.366, contrastive_loss=0.114, total=4118.26, n_correct=2676.6, ppl=4.55, accuracy=64.993, wps=13319.3, ups=1.62, wpb=8236.5, bsz=301.7, num_updates=23100, lr=9.30484e-05, gnorm=0.518, clip=0, loss_scale=8, train_wall=61, gb_free=16.1, wall=18513
2023-08-20 06:37:39 | INFO | train_inner | epoch 016:   1103 / 1474 loss=2.036, trans_loss=4.949, nll_loss=2.2, w2v_ctc_loss=0.788, task_loss=1.409, contrastive_loss=0.094, total=4113.57, n_correct=2661.34, ppl=4.6, accuracy=64.697, wps=13228.8, ups=1.61, wpb=8227.1, bsz=296.2, num_updates=23200, lr=9.28477e-05, gnorm=0.581, clip=0, loss_scale=8, train_wall=61, gb_free=16.6, wall=18575
2023-08-20 06:38:42 | INFO | train_inner | epoch 016:   1203 / 1474 loss=2.032, trans_loss=4.941, nll_loss=2.19, w2v_ctc_loss=0.764, task_loss=1.36, contrastive_loss=0.187, total=4157.18, n_correct=2698.74, ppl=4.56, accuracy=64.918, wps=13202.8, ups=1.59, wpb=8314.4, bsz=306.3, num_updates=23300, lr=9.26482e-05, gnorm=0.518, clip=0, loss_scale=8, train_wall=62, gb_free=16, wall=18638
2023-08-20 06:39:44 | INFO | train_inner | epoch 016:   1303 / 1474 loss=2.036, trans_loss=4.942, nll_loss=2.191, w2v_ctc_loss=0.784, task_loss=1.301, contrastive_loss=0.168, total=4150.54, n_correct=2694.25, ppl=4.57, accuracy=64.913, wps=13425.4, ups=1.62, wpb=8301.1, bsz=312.3, num_updates=23400, lr=9.245e-05, gnorm=0.513, clip=0, loss_scale=8, train_wall=61, gb_free=12.4, wall=18700
2023-08-20 06:40:46 | INFO | train_inner | epoch 016:   1403 / 1474 loss=2.023, trans_loss=4.94, nll_loss=2.188, w2v_ctc_loss=0.773, task_loss=1.257, contrastive_loss=0.098, total=4198.78, n_correct=2728.56, ppl=4.56, accuracy=64.985, wps=13462, ups=1.6, wpb=8397.6, bsz=322.4, num_updates=23500, lr=9.22531e-05, gnorm=0.557, clip=0, loss_scale=8, train_wall=62, gb_free=17.4, wall=18763
2023-08-20 06:41:30 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 06:42:04 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 4 | trans_loss 5.208 | nll_loss 2.48 | w2v_ctc_loss 1.411 | task_loss 4.511 | contrastive_loss 0.276 | total 4003.4 | n_correct 2626.1 | ppl 5.58 | accuracy 65.597 | uer 19.507 | wer 21.528 | raw_wer 21.528 | bleu 21.16 | wps 1592.5 | wpb 4003.4 | bsz 141.8 | num_updates 23571 | best_bleu 21.16
2023-08-20 06:42:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 23571 updates
2023-08-20 06:42:04 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 06:42:11 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 06:42:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt (epoch 16 @ 23571 updates, score 21.16) (writing took 13.512475778989028 seconds)
2023-08-20 06:42:17 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-08-20 06:42:17 | INFO | train | epoch 016 | loss 2.026 | trans_loss 4.938 | nll_loss 2.185 | w2v_ctc_loss 0.772 | task_loss 1.33 | contrastive_loss 0.122 | total 4138.65 | n_correct 2690.3 | ppl 4.55 | accuracy 65.004 | wps 12602.7 | ups 1.52 | wpb 8277.3 | bsz 305.7 | num_updates 23571 | lr 9.21141e-05 | gnorm 0.572 | clip 0 | loss_scale 8 | train_wall 903 | gb_free 15.1 | wall 18854
2023-08-20 06:42:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 06:42:18 | INFO | fairseq.trainer | begin training epoch 17
2023-08-20 06:42:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 06:42:43 | INFO | train_inner | epoch 017:     29 / 1474 loss=2.026, trans_loss=4.924, nll_loss=2.168, w2v_ctc_loss=0.758, task_loss=1.378, contrastive_loss=0.232, total=4138.06, n_correct=2698.21, ppl=4.49, accuracy=65.205, wps=7069.9, ups=0.85, wpb=8276.1, bsz=300.4, num_updates=23600, lr=9.20575e-05, gnorm=0.552, clip=0, loss_scale=8, train_wall=62, gb_free=17.4, wall=18880
2023-08-20 06:43:45 | INFO | train_inner | epoch 017:    129 / 1474 loss=2.007, trans_loss=4.912, nll_loss=2.152, w2v_ctc_loss=0.763, task_loss=1.369, contrastive_loss=0.065, total=4110.37, n_correct=2693.74, ppl=4.44, accuracy=65.535, wps=13233.6, ups=1.61, wpb=8220.7, bsz=295.6, num_updates=23700, lr=9.1863e-05, gnorm=0.562, clip=0, loss_scale=8, train_wall=61, gb_free=16.2, wall=18942
2023-08-20 06:44:47 | INFO | train_inner | epoch 017:    229 / 1474 loss=2.022, trans_loss=4.916, nll_loss=2.158, w2v_ctc_loss=0.75, task_loss=1.239, contrastive_loss=0.233, total=4181.59, n_correct=2734.57, ppl=4.46, accuracy=65.395, wps=13454.4, ups=1.61, wpb=8363.2, bsz=322.2, num_updates=23800, lr=9.16698e-05, gnorm=0.578, clip=0, loss_scale=8, train_wall=61, gb_free=15.4, wall=19004
2023-08-20 06:45:49 | INFO | train_inner | epoch 017:    329 / 1474 loss=2.023, trans_loss=4.92, nll_loss=2.163, w2v_ctc_loss=0.757, task_loss=1.332, contrastive_loss=0.235, total=4157.97, n_correct=2720.17, ppl=4.48, accuracy=65.421, wps=13403.6, ups=1.61, wpb=8315.9, bsz=304, num_updates=23900, lr=9.14779e-05, gnorm=0.546, clip=0, loss_scale=8, train_wall=61, gb_free=15.5, wall=19066
2023-08-20 06:46:52 | INFO | train_inner | epoch 017:    429 / 1474 loss=2.006, trans_loss=4.918, nll_loss=2.16, w2v_ctc_loss=0.755, task_loss=1.327, contrastive_loss=0.066, total=4135.12, n_correct=2709.39, ppl=4.47, accuracy=65.521, wps=13249.4, ups=1.6, wpb=8270.2, bsz=306.1, num_updates=24000, lr=9.12871e-05, gnorm=0.563, clip=0, loss_scale=8, train_wall=62, gb_free=12.2, wall=19128
2023-08-20 06:46:52 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 06:47:26 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 4.005 | trans_loss 5.21 | nll_loss 2.482 | w2v_ctc_loss 1.424 | task_loss 4.532 | contrastive_loss 0.277 | total 4003.4 | n_correct 2633.6 | ppl 5.59 | accuracy 65.784 | uer 19.526 | wer 21.472 | raw_wer 21.472 | bleu 21.14 | wps 1529.2 | wpb 4003.4 | bsz 141.8 | num_updates 24000 | best_bleu 21.16
2023-08-20 06:47:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 24000 updates
2023-08-20 06:47:26 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_17_24000.pt
2023-08-20 06:47:29 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_17_24000.pt
2023-08-20 06:47:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_17_24000.pt (epoch 17 @ 24000 updates, score 21.14) (writing took 7.881876356026623 seconds)
2023-08-20 06:48:37 | INFO | train_inner | epoch 017:    529 / 1474 loss=2.022, trans_loss=4.928, nll_loss=2.173, w2v_ctc_loss=0.772, task_loss=1.377, contrastive_loss=0.116, total=4185.81, n_correct=2731.93, ppl=4.51, accuracy=65.266, wps=7949.5, ups=0.95, wpb=8371.6, bsz=308.6, num_updates=24100, lr=9.10975e-05, gnorm=0.573, clip=0, loss_scale=16, train_wall=62, gb_free=15.7, wall=19234
2023-08-20 06:49:39 | INFO | train_inner | epoch 017:    629 / 1474 loss=2.01, trans_loss=4.925, nll_loss=2.169, w2v_ctc_loss=0.758, task_loss=1.329, contrastive_loss=0.063, total=4168.62, n_correct=2722.63, ppl=4.5, accuracy=65.313, wps=13444.5, ups=1.61, wpb=8337.2, bsz=303.2, num_updates=24200, lr=9.09091e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=61, gb_free=13.9, wall=19296
2023-08-20 06:50:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-20 06:50:42 | INFO | train_inner | epoch 017:    730 / 1474 loss=2.021, trans_loss=4.925, nll_loss=2.169, w2v_ctc_loss=0.775, task_loss=1.315, contrastive_loss=0.108, total=4167.15, n_correct=2718.58, ppl=4.5, accuracy=65.238, wps=13280.2, ups=1.59, wpb=8334.3, bsz=307.7, num_updates=24300, lr=9.07218e-05, gnorm=0.554, clip=0, loss_scale=8, train_wall=62, gb_free=11.3, wall=19359
2023-08-20 06:51:43 | INFO | train_inner | epoch 017:    830 / 1474 loss=2.012, trans_loss=4.923, nll_loss=2.166, w2v_ctc_loss=0.762, task_loss=1.351, contrastive_loss=0.074, total=4083.91, n_correct=2668.74, ppl=4.49, accuracy=65.348, wps=13253.1, ups=1.62, wpb=8167.8, bsz=294.5, num_updates=24400, lr=9.05357e-05, gnorm=0.59, clip=0, loss_scale=8, train_wall=61, gb_free=16.7, wall=19420
2023-08-20 06:52:45 | INFO | train_inner | epoch 017:    930 / 1474 loss=2.008, trans_loss=4.922, nll_loss=2.165, w2v_ctc_loss=0.757, task_loss=1.294, contrastive_loss=0.076, total=4114.87, n_correct=2688.21, ppl=4.49, accuracy=65.329, wps=13451.2, ups=1.63, wpb=8229.7, bsz=307.5, num_updates=24500, lr=9.03508e-05, gnorm=0.555, clip=0, loss_scale=8, train_wall=61, gb_free=17.4, wall=19481
2023-08-20 06:53:46 | INFO | train_inner | epoch 017:   1030 / 1474 loss=2.009, trans_loss=4.92, nll_loss=2.163, w2v_ctc_loss=0.76, task_loss=1.354, contrastive_loss=0.073, total=4091.03, n_correct=2677.88, ppl=4.48, accuracy=65.457, wps=13268.4, ups=1.62, wpb=8182.1, bsz=298.8, num_updates=24600, lr=9.0167e-05, gnorm=0.583, clip=0, loss_scale=8, train_wall=61, gb_free=15.4, wall=19543
2023-08-20 06:54:48 | INFO | train_inner | epoch 017:   1130 / 1474 loss=2.01, trans_loss=4.923, nll_loss=2.166, w2v_ctc_loss=0.759, task_loss=1.332, contrastive_loss=0.07, total=4112.54, n_correct=2686.53, ppl=4.49, accuracy=65.325, wps=13311.2, ups=1.62, wpb=8225.1, bsz=301.7, num_updates=24700, lr=8.99843e-05, gnorm=0.649, clip=0, loss_scale=8, train_wall=61, gb_free=15.2, wall=19605
2023-08-20 06:55:51 | INFO | train_inner | epoch 017:   1230 / 1474 loss=2.041, trans_loss=4.931, nll_loss=2.178, w2v_ctc_loss=0.752, task_loss=1.287, contrastive_loss=0.365, total=4171.58, n_correct=2713.51, ppl=4.53, accuracy=65.048, wps=13236.9, ups=1.59, wpb=8343.2, bsz=325.2, num_updates=24800, lr=8.98027e-05, gnorm=0.543, clip=0, loss_scale=8, train_wall=62, gb_free=16.1, wall=19668
2023-08-20 06:56:53 | INFO | train_inner | epoch 017:   1330 / 1474 loss=2.007, trans_loss=4.921, nll_loss=2.164, w2v_ctc_loss=0.753, task_loss=1.348, contrastive_loss=0.076, total=4138.5, n_correct=2709.22, ppl=4.48, accuracy=65.464, wps=13403.8, ups=1.62, wpb=8277, bsz=301.4, num_updates=24900, lr=8.96221e-05, gnorm=0.601, clip=0, loss_scale=8, train_wall=61, gb_free=15.5, wall=19730
2023-08-20 06:57:55 | INFO | train_inner | epoch 017:   1430 / 1474 loss=2.006, trans_loss=4.923, nll_loss=2.166, w2v_ctc_loss=0.755, task_loss=1.345, contrastive_loss=0.069, total=4118.28, n_correct=2695.89, ppl=4.49, accuracy=65.462, wps=13250.6, ups=1.61, wpb=8236.6, bsz=303.9, num_updates=25000, lr=8.94427e-05, gnorm=0.523, clip=0, loss_scale=8, train_wall=62, gb_free=15.7, wall=19792
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:0')
2023-08-20 06:58:22 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:2')
2023-08-20 06:58:56 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 3.981 | trans_loss 5.204 | nll_loss 2.476 | w2v_ctc_loss 1.362 | task_loss 4.513 | contrastive_loss 0.277 | total 4003.4 | n_correct 2639.1 | ppl 5.56 | accuracy 65.921 | uer 19.157 | wer 21.122 | raw_wer 21.122 | bleu 21.52 | wps 1578.1 | wpb 4003.4 | bsz 141.8 | num_updates 25044 | best_bleu 21.52
2023-08-20 06:58:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 25044 updates
2023-08-20 06:58:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 06:59:02 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 06:59:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt (epoch 17 @ 25044 updates, score 21.52) (writing took 12.057072002033237 seconds)
2023-08-20 06:59:08 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-08-20 06:59:08 | INFO | train | epoch 017 | loss 2.014 | trans_loss 4.921 | nll_loss 2.165 | w2v_ctc_loss 0.759 | task_loss 1.33 | contrastive_loss 0.12 | total 4138.17 | n_correct 2705.47 | ppl 4.48 | accuracy 65.378 | wps 12060.8 | ups 1.46 | wpb 8276.3 | bsz 305.5 | num_updates 25044 | lr 8.93641e-05 | gnorm 0.565 | clip 0 | loss_scale 8 | train_wall 903 | gb_free 15.9 | wall 19865
2023-08-20 06:59:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 06:59:08 | INFO | fairseq.trainer | begin training epoch 18
2023-08-20 06:59:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 06:59:51 | INFO | train_inner | epoch 018:     56 / 1474 loss=2.008, trans_loss=4.915, nll_loss=2.157, w2v_ctc_loss=0.762, task_loss=1.365, contrastive_loss=0.075, total=4131.1, n_correct=2706.59, ppl=4.46, accuracy=65.517, wps=7126, ups=0.86, wpb=8262.2, bsz=301.5, num_updates=25100, lr=8.92644e-05, gnorm=0.531, clip=0, loss_scale=8, train_wall=61, gb_free=16.5, wall=19908
2023-08-20 07:00:53 | INFO | train_inner | epoch 018:    156 / 1474 loss=2.003, trans_loss=4.894, nll_loss=2.129, w2v_ctc_loss=0.735, task_loss=1.252, contrastive_loss=0.199, total=4161.38, n_correct=2745.47, ppl=4.37, accuracy=65.975, wps=13412.1, ups=1.61, wpb=8322.8, bsz=315, num_updates=25200, lr=8.90871e-05, gnorm=0.605, clip=0, loss_scale=8, train_wall=61, gb_free=16.6, wall=19970
2023-08-20 07:01:55 | INFO | train_inner | epoch 018:    256 / 1474 loss=1.997, trans_loss=4.901, nll_loss=2.138, w2v_ctc_loss=0.749, task_loss=1.298, contrastive_loss=0.071, total=4153.17, n_correct=2731.68, ppl=4.4, accuracy=65.773, wps=13380, ups=1.61, wpb=8306.3, bsz=311.2, num_updates=25300, lr=8.89108e-05, gnorm=0.638, clip=0, loss_scale=8, train_wall=61, gb_free=16.5, wall=20032
2023-08-20 07:02:57 | INFO | train_inner | epoch 018:    356 / 1474 loss=2.004, trans_loss=4.911, nll_loss=2.15, w2v_ctc_loss=0.749, task_loss=1.35, contrastive_loss=0.083, total=4179.02, n_correct=2740.04, ppl=4.44, accuracy=65.567, wps=13485.6, ups=1.61, wpb=8358, bsz=302.1, num_updates=25400, lr=8.87357e-05, gnorm=0.649, clip=0, loss_scale=8, train_wall=61, gb_free=16.2, wall=20094
2023-08-20 07:04:00 | INFO | train_inner | epoch 018:    456 / 1474 loss=2.012, trans_loss=4.909, nll_loss=2.148, w2v_ctc_loss=0.75, task_loss=1.434, contrastive_loss=0.174, total=4069.37, n_correct=2667.85, ppl=4.43, accuracy=65.559, wps=13003.7, ups=1.6, wpb=8138.7, bsz=293.6, num_updates=25500, lr=8.85615e-05, gnorm=0.552, clip=0, loss_scale=8, train_wall=62, gb_free=15.2, wall=20156
2023-08-20 07:05:01 | INFO | train_inner | epoch 018:    556 / 1474 loss=1.99, trans_loss=4.894, nll_loss=2.13, w2v_ctc_loss=0.741, task_loss=1.181, contrastive_loss=0.082, total=4224.88, n_correct=2788.7, ppl=4.38, accuracy=66.007, wps=13713.2, ups=1.62, wpb=8449.8, bsz=330.4, num_updates=25600, lr=8.83883e-05, gnorm=0.588, clip=0, loss_scale=8, train_wall=61, gb_free=16.5, wall=20218
2023-08-20 07:06:03 | INFO | train_inner | epoch 018:    656 / 1474 loss=2.016, trans_loss=4.916, nll_loss=2.158, w2v_ctc_loss=0.765, task_loss=1.377, contrastive_loss=0.149, total=4087.72, n_correct=2676.12, ppl=4.46, accuracy=65.467, wps=13271.9, ups=1.62, wpb=8175.4, bsz=298.1, num_updates=25700, lr=8.82162e-05, gnorm=0.576, clip=0, loss_scale=8, train_wall=61, gb_free=16.2, wall=20280
2023-08-20 07:07:05 | INFO | train_inner | epoch 018:    756 / 1474 loss=2.017, trans_loss=4.911, nll_loss=2.152, w2v_ctc_loss=0.755, task_loss=1.267, contrastive_loss=0.239, total=4202.56, n_correct=2755.72, ppl=4.44, accuracy=65.572, wps=13501.2, ups=1.61, wpb=8405.1, bsz=322.9, num_updates=25800, lr=8.80451e-05, gnorm=0.529, clip=0, loss_scale=8, train_wall=62, gb_free=17.5, wall=20342
2023-08-20 07:08:07 | INFO | train_inner | epoch 018:    856 / 1474 loss=1.996, trans_loss=4.906, nll_loss=2.144, w2v_ctc_loss=0.745, task_loss=1.328, contrastive_loss=0.066, total=4181.64, n_correct=2748.86, ppl=4.42, accuracy=65.736, wps=13517.3, ups=1.62, wpb=8363.3, bsz=306.1, num_updates=25900, lr=8.7875e-05, gnorm=0.523, clip=0, loss_scale=8, train_wall=61, gb_free=15.2, wall=20404
2023-08-20 07:09:08 | INFO | train_inner | epoch 018:    956 / 1474 loss=1.995, trans_loss=4.904, nll_loss=2.142, w2v_ctc_loss=0.742, task_loss=1.248, contrastive_loss=0.072, total=4133.45, n_correct=2717.22, ppl=4.41, accuracy=65.737, wps=13617.8, ups=1.65, wpb=8266.9, bsz=312.7, num_updates=26000, lr=8.77058e-05, gnorm=0.68, clip=0, loss_scale=8, train_wall=60, gb_free=14.2, wall=20465
2023-08-20 07:09:08 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 07:09:41 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 3.991 | trans_loss 5.201 | nll_loss 2.471 | w2v_ctc_loss 1.398 | task_loss 4.511 | contrastive_loss 0.275 | total 4003.4 | n_correct 2639.1 | ppl 5.55 | accuracy 65.921 | uer 19.465 | wer 21.502 | raw_wer 21.502 | bleu 21.52 | wps 1608.4 | wpb 4003.4 | bsz 141.8 | num_updates 26000 | best_bleu 21.52
2023-08-20 07:09:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26000 updates
2023-08-20 07:09:41 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_18_26000.pt
2023-08-20 07:09:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_18_26000.pt
2023-08-20 07:09:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_18_26000.pt (epoch 18 @ 26000 updates, score 21.52) (writing took 12.389123875007499 seconds)
2023-08-20 07:10:56 | INFO | train_inner | epoch 018:   1056 / 1474 loss=2.002, trans_loss=4.908, nll_loss=2.148, w2v_ctc_loss=0.751, task_loss=1.402, contrastive_loss=0.07, total=4133.66, n_correct=2712.27, ppl=4.43, accuracy=65.614, wps=7624.6, ups=0.92, wpb=8267.3, bsz=298.3, num_updates=26100, lr=8.75376e-05, gnorm=0.613, clip=0, loss_scale=8, train_wall=61, gb_free=16.5, wall=20573
2023-08-20 07:11:58 | INFO | train_inner | epoch 018:   1156 / 1474 loss=2.012, trans_loss=4.903, nll_loss=2.142, w2v_ctc_loss=0.758, task_loss=1.25, contrastive_loss=0.175, total=4156.35, n_correct=2734.27, ppl=4.41, accuracy=65.785, wps=13456.4, ups=1.62, wpb=8312.7, bsz=315.8, num_updates=26200, lr=8.73704e-05, gnorm=0.546, clip=0, loss_scale=8, train_wall=61, gb_free=13.3, wall=20635
2023-08-20 07:13:00 | INFO | train_inner | epoch 018:   1256 / 1474 loss=2.001, trans_loss=4.914, nll_loss=2.155, w2v_ctc_loss=0.749, task_loss=1.424, contrastive_loss=0.063, total=4093.35, n_correct=2685.63, ppl=4.45, accuracy=65.61, wps=13289.5, ups=1.62, wpb=8186.7, bsz=288, num_updates=26300, lr=8.72041e-05, gnorm=0.543, clip=0, loss_scale=8, train_wall=61, gb_free=15.8, wall=20696
2023-08-20 07:14:01 | INFO | train_inner | epoch 018:   1356 / 1474 loss=2.012, trans_loss=4.916, nll_loss=2.158, w2v_ctc_loss=0.767, task_loss=1.442, contrastive_loss=0.086, total=4056.71, n_correct=2656.41, ppl=4.46, accuracy=65.482, wps=13123.1, ups=1.62, wpb=8113.4, bsz=289.2, num_updates=26400, lr=8.70388e-05, gnorm=0.544, clip=0, loss_scale=16, train_wall=61, gb_free=16.9, wall=20758
2023-08-20 07:14:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-20 07:15:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-20 07:15:04 | INFO | train_inner | epoch 018:   1458 / 1474 loss=2.007, trans_loss=4.915, nll_loss=2.157, w2v_ctc_loss=0.761, task_loss=1.384, contrastive_loss=0.074, total=4131.4, n_correct=2707.28, ppl=4.46, accuracy=65.529, wps=13142.5, ups=1.59, wpb=8262.8, bsz=300.6, num_updates=26500, lr=8.68744e-05, gnorm=0.586, clip=0, loss_scale=4, train_wall=62, gb_free=14.4, wall=20821
2023-08-20 07:15:14 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 07:15:47 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 3.988 | trans_loss 5.192 | nll_loss 2.461 | w2v_ctc_loss 1.408 | task_loss 4.525 | contrastive_loss 0.276 | total 4003.4 | n_correct 2645.4 | ppl 5.51 | accuracy 66.079 | uer 19.186 | wer 21.189 | raw_wer 21.189 | bleu 21.66 | wps 1609.3 | wpb 4003.4 | bsz 141.8 | num_updates 26516 | best_bleu 21.66
2023-08-20 07:15:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26516 updates
2023-08-20 07:15:47 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 07:15:53 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 07:15:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt (epoch 18 @ 26516 updates, score 21.66) (writing took 11.925575976027176 seconds)
2023-08-20 07:16:00 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-08-20 07:16:00 | INFO | train | epoch 018 | loss 2.005 | trans_loss 4.908 | nll_loss 2.147 | w2v_ctc_loss 0.751 | task_loss 1.329 | contrastive_loss 0.118 | total 4138.97 | n_correct 2717.83 | ppl 4.43 | accuracy 65.664 | wps 12047 | ups 1.46 | wpb 8277.9 | bsz 305.8 | num_updates 26516 | lr 8.68482e-05 | gnorm 0.582 | clip 0 | loss_scale 4 | train_wall 900 | gb_free 15.5 | wall 20876
2023-08-20 07:16:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 07:16:00 | INFO | fairseq.trainer | begin training epoch 19
2023-08-20 07:16:00 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 07:16:59 | INFO | train_inner | epoch 019:     84 / 1474 loss=1.996, trans_loss=4.89, nll_loss=2.124, w2v_ctc_loss=0.743, task_loss=1.331, contrastive_loss=0.123, total=4105.84, n_correct=2710.67, ppl=4.36, accuracy=66.02, wps=7174.7, ups=0.87, wpb=8211.7, bsz=297.1, num_updates=26600, lr=8.6711e-05, gnorm=0.562, clip=0, loss_scale=4, train_wall=61, gb_free=13.5, wall=20936
2023-08-20 07:18:02 | INFO | train_inner | epoch 019:    184 / 1474 loss=2, trans_loss=4.889, nll_loss=2.123, w2v_ctc_loss=0.761, task_loss=1.248, contrastive_loss=0.116, total=4213.06, n_correct=2781.95, ppl=4.36, accuracy=66.032, wps=13401.4, ups=1.59, wpb=8426.1, bsz=322.8, num_updates=26700, lr=8.65485e-05, gnorm=0.722, clip=0, loss_scale=4, train_wall=62, gb_free=16, wall=20998
2023-08-20 07:19:04 | INFO | train_inner | epoch 019:    284 / 1474 loss=1.994, trans_loss=4.889, nll_loss=2.122, w2v_ctc_loss=0.747, task_loss=1.282, contrastive_loss=0.102, total=4202.31, n_correct=2775.11, ppl=4.35, accuracy=66.038, wps=13577.9, ups=1.62, wpb=8404.6, bsz=311.7, num_updates=26800, lr=8.63868e-05, gnorm=0.639, clip=0, loss_scale=4, train_wall=61, gb_free=15.8, wall=21060
2023-08-20 07:20:06 | INFO | train_inner | epoch 019:    384 / 1474 loss=1.988, trans_loss=4.882, nll_loss=2.114, w2v_ctc_loss=0.732, task_loss=1.315, contrastive_loss=0.125, total=4166.62, n_correct=2758.68, ppl=4.33, accuracy=66.209, wps=13445.9, ups=1.61, wpb=8333.2, bsz=310.1, num_updates=26900, lr=8.62261e-05, gnorm=0.55, clip=0, loss_scale=4, train_wall=61, gb_free=16.7, wall=21122
2023-08-20 07:21:07 | INFO | train_inner | epoch 019:    484 / 1474 loss=1.99, trans_loss=4.893, nll_loss=2.127, w2v_ctc_loss=0.743, task_loss=1.382, contrastive_loss=0.069, total=4100.4, n_correct=2709.95, ppl=4.37, accuracy=66.09, wps=13341.6, ups=1.63, wpb=8200.8, bsz=297.6, num_updates=27000, lr=8.60663e-05, gnorm=0.536, clip=0, loss_scale=4, train_wall=61, gb_free=14.8, wall=21184
2023-08-20 07:22:09 | INFO | train_inner | epoch 019:    584 / 1474 loss=1.993, trans_loss=4.889, nll_loss=2.122, w2v_ctc_loss=0.738, task_loss=1.3, contrastive_loss=0.135, total=4132.2, n_correct=2727.15, ppl=4.35, accuracy=65.998, wps=13428, ups=1.62, wpb=8264.4, bsz=306, num_updates=27100, lr=8.59074e-05, gnorm=0.592, clip=0, loss_scale=4, train_wall=61, gb_free=16.6, wall=21245
2023-08-20 07:23:10 | INFO | train_inner | epoch 019:    684 / 1474 loss=1.984, trans_loss=4.897, nll_loss=2.134, w2v_ctc_loss=0.729, task_loss=1.196, contrastive_loss=0.067, total=4211.87, n_correct=2783.65, ppl=4.39, accuracy=66.091, wps=13668.1, ups=1.62, wpb=8423.7, bsz=323.5, num_updates=27200, lr=8.57493e-05, gnorm=0.585, clip=0, loss_scale=4, train_wall=61, gb_free=16.4, wall=21307
2023-08-20 07:24:12 | INFO | train_inner | epoch 019:    784 / 1474 loss=1.997, trans_loss=4.894, nll_loss=2.129, w2v_ctc_loss=0.754, task_loss=1.354, contrastive_loss=0.079, total=4132.27, n_correct=2724.86, ppl=4.37, accuracy=65.941, wps=13319, ups=1.61, wpb=8264.5, bsz=303.3, num_updates=27300, lr=8.55921e-05, gnorm=0.579, clip=0, loss_scale=4, train_wall=61, gb_free=16.4, wall=21369
2023-08-20 07:24:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-20 07:25:15 | INFO | train_inner | epoch 019:    885 / 1474 loss=1.993, trans_loss=4.896, nll_loss=2.131, w2v_ctc_loss=0.748, task_loss=1.369, contrastive_loss=0.062, total=4147.5, n_correct=2734.34, ppl=4.38, accuracy=65.927, wps=13280.8, ups=1.6, wpb=8295, bsz=302.4, num_updates=27400, lr=8.54358e-05, gnorm=0.565, clip=0, loss_scale=2, train_wall=62, gb_free=11.7, wall=21431
2023-08-20 07:26:17 | INFO | train_inner | epoch 019:    985 / 1474 loss=2.013, trans_loss=4.903, nll_loss=2.142, w2v_ctc_loss=0.737, task_loss=1.336, contrastive_loss=0.295, total=4101.29, n_correct=2698.78, ppl=4.41, accuracy=65.803, wps=13203.3, ups=1.61, wpb=8202.6, bsz=309, num_updates=27500, lr=8.52803e-05, gnorm=0.564, clip=0, loss_scale=2, train_wall=61, gb_free=17.2, wall=21494
2023-08-20 07:27:19 | INFO | train_inner | epoch 019:   1085 / 1474 loss=1.996, trans_loss=4.901, nll_loss=2.138, w2v_ctc_loss=0.738, task_loss=1.408, contrastive_loss=0.105, total=4041.68, n_correct=2662.61, ppl=4.4, accuracy=65.879, wps=13106.8, ups=1.62, wpb=8083.4, bsz=292.8, num_updates=27600, lr=8.51257e-05, gnorm=0.544, clip=0, loss_scale=2, train_wall=61, gb_free=16, wall=21555
2023-08-20 07:28:21 | INFO | train_inner | epoch 019:   1185 / 1474 loss=2.005, trans_loss=4.897, nll_loss=2.134, w2v_ctc_loss=0.739, task_loss=1.37, contrastive_loss=0.187, total=4128.85, n_correct=2717.82, ppl=4.39, accuracy=65.825, wps=13201.9, ups=1.6, wpb=8257.7, bsz=304.9, num_updates=27700, lr=8.49719e-05, gnorm=0.553, clip=0, loss_scale=2, train_wall=62, gb_free=16.1, wall=21618
2023-08-20 07:29:23 | INFO | train_inner | epoch 019:   1285 / 1474 loss=1.994, trans_loss=4.9, nll_loss=2.137, w2v_ctc_loss=0.74, task_loss=1.324, contrastive_loss=0.087, total=4157.6, n_correct=2742.68, ppl=4.4, accuracy=65.968, wps=13486, ups=1.62, wpb=8315.2, bsz=304.4, num_updates=27800, lr=8.48189e-05, gnorm=0.578, clip=0, loss_scale=2, train_wall=61, gb_free=11.6, wall=21679
2023-08-20 07:30:24 | INFO | train_inner | epoch 019:   1385 / 1474 loss=1.992, trans_loss=4.896, nll_loss=2.131, w2v_ctc_loss=0.743, task_loss=1.358, contrastive_loss=0.074, total=4119.9, n_correct=2718.09, ppl=4.38, accuracy=65.975, wps=13360.3, ups=1.62, wpb=8239.8, bsz=300.9, num_updates=27900, lr=8.46668e-05, gnorm=0.536, clip=0, loss_scale=2, train_wall=61, gb_free=17.2, wall=21741
2023-08-20 07:31:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 07:31:52 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 3.991 | trans_loss 5.196 | nll_loss 2.466 | w2v_ctc_loss 1.414 | task_loss 4.511 | contrastive_loss 0.273 | total 4003.4 | n_correct 2641 | ppl 5.53 | accuracy 65.969 | uer 19.436 | wer 21.293 | raw_wer 21.293 | bleu 21.43 | wps 1641.6 | wpb 4003.4 | bsz 141.8 | num_updates 27989 | best_bleu 21.66
2023-08-20 07:31:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 27989 updates
2023-08-20 07:31:52 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.4307.pt
2023-08-20 07:31:55 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.4307.pt
2023-08-20 07:32:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.4307.pt (epoch 19 @ 27989 updates, score 21.43) (writing took 8.864436061005108 seconds)
2023-08-20 07:32:01 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-08-20 07:32:01 | INFO | train | epoch 019 | loss 1.995 | trans_loss 4.894 | nll_loss 2.129 | w2v_ctc_loss 0.743 | task_loss 1.33 | contrastive_loss 0.115 | total 4138.55 | n_correct 2731.05 | ppl 4.37 | accuracy 65.99 | wps 12679.8 | ups 1.53 | wpb 8277.1 | bsz 305.7 | num_updates 27989 | lr 8.4532e-05 | gnorm 0.58 | clip 0 | loss_scale 2 | train_wall 901 | gb_free 17 | wall 21838
2023-08-20 07:32:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 07:32:01 | INFO | fairseq.trainer | begin training epoch 20
2023-08-20 07:32:01 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 07:32:16 | INFO | train_inner | epoch 020:     11 / 1474 loss=1.996, trans_loss=4.889, nll_loss=2.123, w2v_ctc_loss=0.738, task_loss=1.341, contrastive_loss=0.156, total=4125.77, n_correct=2725.03, ppl=4.36, accuracy=66.049, wps=7425.4, ups=0.9, wpb=8251.5, bsz=304.9, num_updates=28000, lr=8.45154e-05, gnorm=0.595, clip=0, loss_scale=2, train_wall=61, gb_free=9.5, wall=21852
2023-08-20 07:32:16 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 07:32:48 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 3.989 | trans_loss 5.201 | nll_loss 2.47 | w2v_ctc_loss 1.392 | task_loss 4.516 | contrastive_loss 0.28 | total 4003.4 | n_correct 2642.8 | ppl 5.54 | accuracy 66.014 | uer 19.25 | wer 21.028 | raw_wer 21.028 | bleu 21.59 | wps 1642.5 | wpb 4003.4 | bsz 141.8 | num_updates 28000 | best_bleu 21.66
2023-08-20 07:32:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 28000 updates
2023-08-20 07:32:48 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_20_28000.pt
2023-08-20 07:32:51 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_20_28000.pt
2023-08-20 07:32:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_20_28000.pt (epoch 20 @ 28000 updates, score 21.59) (writing took 9.367301364953164 seconds)
2023-08-20 07:34:01 | INFO | train_inner | epoch 020:    111 / 1474 loss=1.974, trans_loss=4.871, nll_loss=2.099, w2v_ctc_loss=0.724, task_loss=1.293, contrastive_loss=0.074, total=4195.07, n_correct=2789.96, ppl=4.28, accuracy=66.506, wps=7968.2, ups=0.95, wpb=8390.1, bsz=311.4, num_updates=28100, lr=8.43649e-05, gnorm=0.524, clip=0, loss_scale=2, train_wall=61, gb_free=15.6, wall=21958
2023-08-20 07:35:04 | INFO | train_inner | epoch 020:    211 / 1474 loss=1.983, trans_loss=4.875, nll_loss=2.104, w2v_ctc_loss=0.73, task_loss=1.384, contrastive_loss=0.128, total=4151.42, n_correct=2753.81, ppl=4.3, accuracy=66.334, wps=13230.3, ups=1.59, wpb=8302.8, bsz=301, num_updates=28200, lr=8.42152e-05, gnorm=0.578, clip=0, loss_scale=2, train_wall=62, gb_free=16.1, wall=22020
2023-08-20 07:36:06 | INFO | train_inner | epoch 020:    311 / 1474 loss=1.972, trans_loss=4.87, nll_loss=2.099, w2v_ctc_loss=0.728, task_loss=1.206, contrastive_loss=0.069, total=4195.39, n_correct=2790.75, ppl=4.28, accuracy=66.519, wps=13538.2, ups=1.61, wpb=8390.8, bsz=325.4, num_updates=28300, lr=8.40663e-05, gnorm=0.551, clip=0, loss_scale=2, train_wall=61, gb_free=15.1, wall=22082
2023-08-20 07:37:07 | INFO | train_inner | epoch 020:    411 / 1474 loss=1.973, trans_loss=4.866, nll_loss=2.092, w2v_ctc_loss=0.723, task_loss=1.353, contrastive_loss=0.066, total=4101.39, n_correct=2730.46, ppl=4.26, accuracy=66.574, wps=13318.3, ups=1.62, wpb=8202.8, bsz=296.2, num_updates=28400, lr=8.39181e-05, gnorm=0.584, clip=0, loss_scale=2, train_wall=61, gb_free=15.6, wall=22144
2023-08-20 07:38:09 | INFO | train_inner | epoch 020:    511 / 1474 loss=1.99, trans_loss=4.883, nll_loss=2.115, w2v_ctc_loss=0.73, task_loss=1.339, contrastive_loss=0.154, total=4125.09, n_correct=2730.49, ppl=4.33, accuracy=66.192, wps=13322.6, ups=1.61, wpb=8250.2, bsz=303.8, num_updates=28500, lr=8.37708e-05, gnorm=0.534, clip=0, loss_scale=2, train_wall=61, gb_free=16.2, wall=22206
2023-08-20 07:39:11 | INFO | train_inner | epoch 020:    611 / 1474 loss=1.994, trans_loss=4.884, nll_loss=2.115, w2v_ctc_loss=0.732, task_loss=1.425, contrastive_loss=0.158, total=4078.71, n_correct=2695.24, ppl=4.33, accuracy=66.081, wps=13186.8, ups=1.62, wpb=8157.4, bsz=292.3, num_updates=28600, lr=8.36242e-05, gnorm=0.563, clip=0, loss_scale=2, train_wall=61, gb_free=15.9, wall=22268
2023-08-20 07:40:13 | INFO | train_inner | epoch 020:    711 / 1474 loss=1.986, trans_loss=4.885, nll_loss=2.117, w2v_ctc_loss=0.743, task_loss=1.33, contrastive_loss=0.059, total=4142.11, n_correct=2740.17, ppl=4.34, accuracy=66.154, wps=13450, ups=1.62, wpb=8284.2, bsz=301.3, num_updates=28700, lr=8.34784e-05, gnorm=0.623, clip=0, loss_scale=2, train_wall=61, gb_free=16, wall=22329
2023-08-20 07:41:14 | INFO | train_inner | epoch 020:    811 / 1474 loss=1.984, trans_loss=4.887, nll_loss=2.121, w2v_ctc_loss=0.736, task_loss=1.289, contrastive_loss=0.068, total=4154.59, n_correct=2750.57, ppl=4.35, accuracy=66.206, wps=13509.3, ups=1.63, wpb=8309.2, bsz=311, num_updates=28800, lr=8.33333e-05, gnorm=0.631, clip=0, loss_scale=2, train_wall=61, gb_free=15.4, wall=22391
2023-08-20 07:42:17 | INFO | train_inner | epoch 020:    911 / 1474 loss=2.017, trans_loss=4.89, nll_loss=2.125, w2v_ctc_loss=0.74, task_loss=1.275, contrastive_loss=0.355, total=4156.71, n_correct=2738.95, ppl=4.36, accuracy=65.892, wps=13251.4, ups=1.59, wpb=8313.4, bsz=320.5, num_updates=28900, lr=8.3189e-05, gnorm=0.668, clip=0, loss_scale=2, train_wall=62, gb_free=16.2, wall=22454
2023-08-20 07:43:19 | INFO | train_inner | epoch 020:   1011 / 1474 loss=1.985, trans_loss=4.884, nll_loss=2.117, w2v_ctc_loss=0.725, task_loss=1.289, contrastive_loss=0.133, total=4191.9, n_correct=2777.71, ppl=4.34, accuracy=66.264, wps=13463.3, ups=1.61, wpb=8383.8, bsz=314.6, num_updates=29000, lr=8.30455e-05, gnorm=0.551, clip=0, loss_scale=2, train_wall=61, gb_free=16.3, wall=22516
2023-08-20 07:44:21 | INFO | train_inner | epoch 020:   1111 / 1474 loss=1.995, trans_loss=4.885, nll_loss=2.118, w2v_ctc_loss=0.741, task_loss=1.321, contrastive_loss=0.14, total=4134, n_correct=2736.28, ppl=4.34, accuracy=66.19, wps=13407.9, ups=1.62, wpb=8268, bsz=306, num_updates=29100, lr=8.29027e-05, gnorm=0.76, clip=1, loss_scale=2, train_wall=61, gb_free=15.2, wall=22578
2023-08-20 07:45:23 | INFO | train_inner | epoch 020:   1211 / 1474 loss=1.988, trans_loss=4.878, nll_loss=2.108, w2v_ctc_loss=0.75, task_loss=1.486, contrastive_loss=0.058, total=4027.67, n_correct=2665.16, ppl=4.31, accuracy=66.171, wps=12997.3, ups=1.61, wpb=8055.3, bsz=283.1, num_updates=29200, lr=8.27606e-05, gnorm=0.582, clip=0, loss_scale=2, train_wall=61, gb_free=16.5, wall=22640
2023-08-20 07:46:25 | INFO | train_inner | epoch 020:   1311 / 1474 loss=1.98, trans_loss=4.883, nll_loss=2.116, w2v_ctc_loss=0.73, task_loss=1.365, contrastive_loss=0.062, total=4140.59, n_correct=2745.97, ppl=4.33, accuracy=66.318, wps=13341.9, ups=1.61, wpb=8281.2, bsz=302.8, num_updates=29300, lr=8.26192e-05, gnorm=0.556, clip=0, loss_scale=2, train_wall=61, gb_free=16.9, wall=22702
2023-08-20 07:47:27 | INFO | train_inner | epoch 020:   1411 / 1474 loss=1.984, trans_loss=4.883, nll_loss=2.115, w2v_ctc_loss=0.735, task_loss=1.432, contrastive_loss=0.058, total=4114.68, n_correct=2724.38, ppl=4.33, accuracy=66.211, wps=13324, ups=1.62, wpb=8229.4, bsz=291, num_updates=29400, lr=8.24786e-05, gnorm=0.558, clip=0, loss_scale=4, train_wall=61, gb_free=11.3, wall=22763
2023-08-20 07:48:05 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 07:48:38 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 3.972 | trans_loss 5.193 | nll_loss 2.459 | w2v_ctc_loss 1.356 | task_loss 4.492 | contrastive_loss 0.273 | total 4003.4 | n_correct 2647.8 | ppl 5.5 | accuracy 66.139 | uer 18.809 | wer 20.659 | raw_wer 20.659 | bleu 21.36 | wps 1626 | wpb 4003.4 | bsz 141.8 | num_updates 29463 | best_bleu 21.66
2023-08-20 07:48:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 29463 updates
2023-08-20 07:48:38 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.3607.pt
2023-08-20 07:48:41 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.3607.pt
2023-08-20 07:48:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.3607.pt (epoch 20 @ 29463 updates, score 21.36) (writing took 8.15613246097928 seconds)
2023-08-20 07:48:47 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-08-20 07:48:47 | INFO | train | epoch 020 | loss 1.986 | trans_loss 4.88 | nll_loss 2.112 | w2v_ctc_loss 0.733 | task_loss 1.33 | contrastive_loss 0.114 | total 4138.65 | n_correct 2742.25 | ppl 4.32 | accuracy 66.259 | wps 12132.3 | ups 1.47 | wpb 8277.3 | bsz 305.7 | num_updates 29463 | lr 8.23904e-05 | gnorm 0.587 | clip 0.1 | loss_scale 4 | train_wall 901 | gb_free 15.7 | wall 22844
2023-08-20 07:48:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 07:48:47 | INFO | fairseq.trainer | begin training epoch 21
2023-08-20 07:48:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 07:49:18 | INFO | train_inner | epoch 021:     37 / 1474 loss=1.988, trans_loss=4.881, nll_loss=2.113, w2v_ctc_loss=0.725, task_loss=1.258, contrastive_loss=0.18, total=4150.5, n_correct=2751.29, ppl=4.32, accuracy=66.288, wps=7481, ups=0.9, wpb=8301, bsz=315.8, num_updates=29500, lr=8.23387e-05, gnorm=0.546, clip=0, loss_scale=4, train_wall=61, gb_free=15.1, wall=22874
2023-08-20 07:50:20 | INFO | train_inner | epoch 021:    137 / 1474 loss=1.98, trans_loss=4.861, nll_loss=2.086, w2v_ctc_loss=0.724, task_loss=1.245, contrastive_loss=0.172, total=4185.93, n_correct=2786.96, ppl=4.25, accuracy=66.579, wps=13568.1, ups=1.62, wpb=8371.9, bsz=319.2, num_updates=29600, lr=8.21995e-05, gnorm=0.624, clip=0, loss_scale=4, train_wall=61, gb_free=15.4, wall=22937
2023-08-20 07:51:22 | INFO | train_inner | epoch 021:    237 / 1474 loss=1.967, trans_loss=4.862, nll_loss=2.087, w2v_ctc_loss=0.706, task_loss=1.282, contrastive_loss=0.125, total=4152.28, n_correct=2770.15, ppl=4.25, accuracy=66.714, wps=13329.5, ups=1.61, wpb=8304.6, bsz=311.5, num_updates=29700, lr=8.2061e-05, gnorm=0.609, clip=0, loss_scale=4, train_wall=61, gb_free=16, wall=22999
2023-08-20 07:52:24 | INFO | train_inner | epoch 021:    337 / 1474 loss=1.988, trans_loss=4.873, nll_loss=2.103, w2v_ctc_loss=0.743, task_loss=1.306, contrastive_loss=0.131, total=4160.9, n_correct=2758.41, ppl=4.3, accuracy=66.294, wps=13399, ups=1.61, wpb=8321.8, bsz=312.1, num_updates=29800, lr=8.19232e-05, gnorm=0.706, clip=0, loss_scale=4, train_wall=61, gb_free=17.5, wall=23061
2023-08-20 07:53:26 | INFO | train_inner | epoch 021:    437 / 1474 loss=1.966, trans_loss=4.862, nll_loss=2.088, w2v_ctc_loss=0.718, task_loss=1.284, contrastive_loss=0.055, total=4175.77, n_correct=2784.98, ppl=4.25, accuracy=66.694, wps=13659.4, ups=1.64, wpb=8351.5, bsz=308.1, num_updates=29900, lr=8.17861e-05, gnorm=0.592, clip=0, loss_scale=4, train_wall=60, gb_free=17.3, wall=23122
2023-08-20 07:54:27 | INFO | train_inner | epoch 021:    537 / 1474 loss=1.967, trans_loss=4.857, nll_loss=2.081, w2v_ctc_loss=0.725, task_loss=1.381, contrastive_loss=0.052, total=4085.88, n_correct=2725.87, ppl=4.23, accuracy=66.714, wps=13241.2, ups=1.62, wpb=8171.8, bsz=294.4, num_updates=30000, lr=8.16497e-05, gnorm=0.56, clip=0, loss_scale=4, train_wall=61, gb_free=16.5, wall=23184
2023-08-20 07:54:27 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 07:55:01 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 3.979 | trans_loss 5.19 | nll_loss 2.457 | w2v_ctc_loss 1.387 | task_loss 4.508 | contrastive_loss 0.27 | total 4003.4 | n_correct 2654.6 | ppl 5.49 | accuracy 66.309 | uer 18.578 | wer 20.503 | raw_wer 20.503 | bleu 21.7 | wps 1623.9 | wpb 4003.4 | bsz 141.8 | num_updates 30000 | best_bleu 21.7
2023-08-20 07:55:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30000 updates
2023-08-20 07:55:01 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_21_30000.pt
2023-08-20 07:55:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_21_30000.pt
2023-08-20 07:55:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_21_30000.pt (epoch 21 @ 30000 updates, score 21.7) (writing took 15.304431098978966 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:0')
2023-08-20 07:56:18 | INFO | train_inner | epoch 021:    637 / 1474 loss=1.986, trans_loss=4.866, nll_loss=2.093, w2v_ctc_loss=0.725, task_loss=1.304, contrastive_loss=0.224, total=4211.14, n_correct=2801.21, ppl=4.27, accuracy=66.519, wps=7574.5, ups=0.9, wpb=8422.3, bsz=316.7, num_updates=30100, lr=8.15139e-05, gnorm=0.538, clip=0, loss_scale=4, train_wall=61, gb_free=16.9, wall=23295
2023-08-20 07:57:21 | INFO | train_inner | epoch 021:    737 / 1474 loss=1.978, trans_loss=4.875, nll_loss=2.104, w2v_ctc_loss=0.725, task_loss=1.337, contrastive_loss=0.086, total=4149.91, n_correct=2757.75, ppl=4.3, accuracy=66.453, wps=13310, ups=1.6, wpb=8299.8, bsz=306.8, num_updates=30200, lr=8.13788e-05, gnorm=0.604, clip=0, loss_scale=4, train_wall=62, gb_free=16.9, wall=23358
2023-08-20 07:58:23 | INFO | train_inner | epoch 021:    837 / 1474 loss=1.984, trans_loss=4.88, nll_loss=2.111, w2v_ctc_loss=0.728, task_loss=1.389, contrastive_loss=0.1, total=4075.22, n_correct=2702.32, ppl=4.32, accuracy=66.311, wps=13151.3, ups=1.61, wpb=8150.4, bsz=296.9, num_updates=30300, lr=8.12444e-05, gnorm=0.668, clip=1, loss_scale=4, train_wall=61, gb_free=16.5, wall=23420
2023-08-20 07:59:24 | INFO | train_inner | epoch 021:    937 / 1474 loss=1.976, trans_loss=4.869, nll_loss=2.097, w2v_ctc_loss=0.732, task_loss=1.333, contrastive_loss=0.074, total=4092.55, n_correct=2719.36, ppl=4.28, accuracy=66.447, wps=13366.1, ups=1.63, wpb=8185.1, bsz=300.3, num_updates=30400, lr=8.11107e-05, gnorm=0.547, clip=0, loss_scale=4, train_wall=60, gb_free=12.5, wall=23481
2023-08-20 08:00:26 | INFO | train_inner | epoch 021:   1037 / 1474 loss=1.973, trans_loss=4.873, nll_loss=2.103, w2v_ctc_loss=0.72, task_loss=1.349, contrastive_loss=0.07, total=4113.67, n_correct=2736.77, ppl=4.29, accuracy=66.529, wps=13270.5, ups=1.61, wpb=8227.3, bsz=300, num_updates=30500, lr=8.09776e-05, gnorm=0.526, clip=0, loss_scale=4, train_wall=61, gb_free=15.1, wall=23543
2023-08-20 08:01:27 | INFO | train_inner | epoch 021:   1137 / 1474 loss=1.98, trans_loss=4.871, nll_loss=2.099, w2v_ctc_loss=0.734, task_loss=1.434, contrastive_loss=0.076, total=4108.87, n_correct=2732.04, ppl=4.28, accuracy=66.491, wps=13380.1, ups=1.63, wpb=8217.7, bsz=292.2, num_updates=30600, lr=8.08452e-05, gnorm=0.628, clip=0, loss_scale=4, train_wall=61, gb_free=16.2, wall=23604
2023-08-20 08:02:30 | INFO | train_inner | epoch 021:   1237 / 1474 loss=1.983, trans_loss=4.871, nll_loss=2.101, w2v_ctc_loss=0.733, task_loss=1.24, contrastive_loss=0.129, total=4174.95, n_correct=2772.77, ppl=4.29, accuracy=66.414, wps=13316.8, ups=1.59, wpb=8349.9, bsz=316.8, num_updates=30700, lr=8.07134e-05, gnorm=0.586, clip=0, loss_scale=4, train_wall=62, gb_free=14.9, wall=23667
2023-08-20 08:03:32 | INFO | train_inner | epoch 021:   1337 / 1474 loss=1.971, trans_loss=4.868, nll_loss=2.097, w2v_ctc_loss=0.718, task_loss=1.328, contrastive_loss=0.077, total=4125.79, n_correct=2750.76, ppl=4.28, accuracy=66.672, wps=13396.6, ups=1.62, wpb=8251.6, bsz=304.4, num_updates=30800, lr=8.05823e-05, gnorm=0.589, clip=0, loss_scale=4, train_wall=61, gb_free=10.5, wall=23729
2023-08-20 08:04:34 | INFO | train_inner | epoch 021:   1437 / 1474 loss=1.993, trans_loss=4.88, nll_loss=2.111, w2v_ctc_loss=0.741, task_loss=1.377, contrastive_loss=0.138, total=4143.19, n_correct=2743.55, ppl=4.32, accuracy=66.218, wps=13325.4, ups=1.61, wpb=8286.4, bsz=307.2, num_updates=30900, lr=8.04518e-05, gnorm=0.654, clip=0, loss_scale=4, train_wall=61, gb_free=15.8, wall=23791
2023-08-20 08:04:57 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:2')
2023-08-20 08:05:30 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 3.977 | trans_loss 5.197 | nll_loss 2.464 | w2v_ctc_loss 1.365 | task_loss 4.517 | contrastive_loss 0.275 | total 4003.4 | n_correct 2640.5 | ppl 5.52 | accuracy 65.956 | uer 18.892 | wer 20.808 | raw_wer 20.808 | bleu 21.59 | wps 1614.1 | wpb 4003.4 | bsz 141.8 | num_updates 30937 | best_bleu 21.7
2023-08-20 08:05:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30937 updates
2023-08-20 08:05:30 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.5907.pt
2023-08-20 08:05:33 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.5907.pt
2023-08-20 08:05:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.5907.pt (epoch 21 @ 30937 updates, score 21.59) (writing took 8.482869127008598 seconds)
2023-08-20 08:05:39 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-08-20 08:05:39 | INFO | train | epoch 021 | loss 1.978 | trans_loss 4.869 | nll_loss 2.097 | w2v_ctc_loss 0.726 | task_loss 1.329 | contrastive_loss 0.112 | total 4138.65 | n_correct 2752.22 | ppl 4.28 | accuracy 66.501 | wps 12057.7 | ups 1.46 | wpb 8277.3 | bsz 305.7 | num_updates 30937 | lr 8.04037e-05 | gnorm 0.6 | clip 0.1 | loss_scale 4 | train_wall 900 | gb_free 15.1 | wall 23855
2023-08-20 08:05:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 08:05:39 | INFO | fairseq.trainer | begin training epoch 22
2023-08-20 08:05:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 08:06:25 | INFO | train_inner | epoch 022:     63 / 1474 loss=1.963, trans_loss=4.852, nll_loss=2.075, w2v_ctc_loss=0.719, task_loss=1.341, contrastive_loss=0.057, total=4135.51, n_correct=2766.62, ppl=4.21, accuracy=66.899, wps=7472.1, ups=0.9, wpb=8271, bsz=299.9, num_updates=31000, lr=8.03219e-05, gnorm=0.552, clip=0, loss_scale=4, train_wall=61, gb_free=16.1, wall=23901
2023-08-20 08:07:27 | INFO | train_inner | epoch 022:    163 / 1474 loss=1.976, trans_loss=4.855, nll_loss=2.079, w2v_ctc_loss=0.728, task_loss=1.354, contrastive_loss=0.135, total=4107.71, n_correct=2743.04, ppl=4.23, accuracy=66.778, wps=13216.7, ups=1.61, wpb=8215.4, bsz=306.6, num_updates=31100, lr=8.01927e-05, gnorm=0.773, clip=0, loss_scale=4, train_wall=61, gb_free=14.3, wall=23964
2023-08-20 08:08:29 | INFO | train_inner | epoch 022:    263 / 1474 loss=1.959, trans_loss=4.849, nll_loss=2.072, w2v_ctc_loss=0.712, task_loss=1.162, contrastive_loss=0.079, total=4275.18, n_correct=2863.11, ppl=4.2, accuracy=66.971, wps=13737.1, ups=1.61, wpb=8550.4, bsz=330.4, num_updates=31200, lr=8.00641e-05, gnorm=0.667, clip=0, loss_scale=4, train_wall=61, gb_free=16.6, wall=24026
2023-08-20 08:09:32 | INFO | train_inner | epoch 022:    363 / 1474 loss=1.99, trans_loss=4.861, nll_loss=2.086, w2v_ctc_loss=0.727, task_loss=1.338, contrastive_loss=0.232, total=4180.65, n_correct=2782.89, ppl=4.25, accuracy=66.566, wps=13311.2, ups=1.59, wpb=8361.3, bsz=311.7, num_updates=31300, lr=7.99361e-05, gnorm=0.581, clip=0, loss_scale=4, train_wall=62, gb_free=16.8, wall=24089
2023-08-20 08:10:34 | INFO | train_inner | epoch 022:    463 / 1474 loss=1.976, trans_loss=4.859, nll_loss=2.084, w2v_ctc_loss=0.726, task_loss=1.385, contrastive_loss=0.118, total=4146.77, n_correct=2767.66, ppl=4.24, accuracy=66.743, wps=13283.6, ups=1.6, wpb=8293.5, bsz=300.2, num_updates=31400, lr=7.98087e-05, gnorm=0.547, clip=0, loss_scale=4, train_wall=62, gb_free=15, wall=24151
2023-08-20 08:11:37 | INFO | train_inner | epoch 022:    563 / 1474 loss=1.964, trans_loss=4.854, nll_loss=2.077, w2v_ctc_loss=0.719, task_loss=1.354, contrastive_loss=0.062, total=4136.18, n_correct=2768.21, ppl=4.22, accuracy=66.927, wps=13286.7, ups=1.61, wpb=8272.4, bsz=302.8, num_updates=31500, lr=7.96819e-05, gnorm=0.517, clip=0, loss_scale=8, train_wall=62, gb_free=16.1, wall=24213
2023-08-20 08:12:38 | INFO | train_inner | epoch 022:    663 / 1474 loss=1.966, trans_loss=4.849, nll_loss=2.071, w2v_ctc_loss=0.711, task_loss=1.266, contrastive_loss=0.151, total=4145.05, n_correct=2776.8, ppl=4.2, accuracy=66.991, wps=13525.5, ups=1.63, wpb=8290.1, bsz=311.7, num_updates=31600, lr=7.95557e-05, gnorm=0.694, clip=0, loss_scale=8, train_wall=61, gb_free=12, wall=24275
2023-08-20 08:13:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-20 08:13:41 | INFO | train_inner | epoch 022:    764 / 1474 loss=1.97, trans_loss=4.858, nll_loss=2.082, w2v_ctc_loss=0.727, task_loss=1.337, contrastive_loss=0.07, total=4178.1, n_correct=2786.92, ppl=4.23, accuracy=66.703, wps=13344.3, ups=1.6, wpb=8356.2, bsz=307.6, num_updates=31700, lr=7.94301e-05, gnorm=0.867, clip=1, loss_scale=4, train_wall=62, gb_free=15.3, wall=24337
2023-08-20 08:14:42 | INFO | train_inner | epoch 022:    864 / 1474 loss=1.965, trans_loss=4.859, nll_loss=2.084, w2v_ctc_loss=0.714, task_loss=1.424, contrastive_loss=0.054, total=4082.3, n_correct=2721.6, ppl=4.24, accuracy=66.668, wps=13183.1, ups=1.61, wpb=8164.6, bsz=290.5, num_updates=31800, lr=7.93052e-05, gnorm=0.558, clip=0, loss_scale=4, train_wall=61, gb_free=15.5, wall=24399
2023-08-20 08:15:44 | INFO | train_inner | epoch 022:    964 / 1474 loss=1.962, trans_loss=4.853, nll_loss=2.077, w2v_ctc_loss=0.712, task_loss=1.361, contrastive_loss=0.063, total=4124.76, n_correct=2756.47, ppl=4.22, accuracy=66.827, wps=13363.8, ups=1.62, wpb=8249.5, bsz=301.4, num_updates=31900, lr=7.91808e-05, gnorm=0.57, clip=0, loss_scale=4, train_wall=61, gb_free=16, wall=24461
2023-08-20 08:16:46 | INFO | train_inner | epoch 022:   1064 / 1474 loss=1.97, trans_loss=4.852, nll_loss=2.076, w2v_ctc_loss=0.704, task_loss=1.26, contrastive_loss=0.216, total=4161.32, n_correct=2786.48, ppl=4.22, accuracy=66.961, wps=13489.4, ups=1.62, wpb=8322.6, bsz=316.3, num_updates=32000, lr=7.90569e-05, gnorm=0.59, clip=0, loss_scale=4, train_wall=61, gb_free=16.1, wall=24523
2023-08-20 08:16:46 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 08:17:19 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 3.978 | trans_loss 5.195 | nll_loss 2.466 | w2v_ctc_loss 1.374 | task_loss 4.487 | contrastive_loss 0.273 | total 4003.4 | n_correct 2648.3 | ppl 5.53 | accuracy 66.151 | uer 18.902 | wer 20.79 | raw_wer 20.79 | bleu 21.7 | wps 1597.7 | wpb 4003.4 | bsz 141.8 | num_updates 32000 | best_bleu 21.7
2023-08-20 08:17:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32000 updates
2023-08-20 08:17:19 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_22_32000.pt
2023-08-20 08:17:22 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_22_32000.pt
2023-08-20 08:17:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_22_32000.pt (epoch 22 @ 32000 updates, score 21.7) (writing took 14.574147197999991 seconds)
2023-08-20 08:18:36 | INFO | train_inner | epoch 022:   1164 / 1474 loss=1.984, trans_loss=4.876, nll_loss=2.106, w2v_ctc_loss=0.732, task_loss=1.367, contrastive_loss=0.113, total=4103.05, n_correct=2724.06, ppl=4.3, accuracy=66.391, wps=7449.2, ups=0.91, wpb=8206.1, bsz=297.5, num_updates=32100, lr=7.89337e-05, gnorm=0.644, clip=0, loss_scale=4, train_wall=61, gb_free=16.3, wall=24633
2023-08-20 08:19:38 | INFO | train_inner | epoch 022:   1264 / 1474 loss=1.972, trans_loss=4.869, nll_loss=2.098, w2v_ctc_loss=0.719, task_loss=1.248, contrastive_loss=0.096, total=4157.43, n_correct=2770.21, ppl=4.28, accuracy=66.633, wps=13418.7, ups=1.61, wpb=8314.9, bsz=317.1, num_updates=32200, lr=7.8811e-05, gnorm=0.623, clip=0, loss_scale=4, train_wall=61, gb_free=11.3, wall=24695
2023-08-20 08:20:39 | INFO | train_inner | epoch 022:   1364 / 1474 loss=1.965, trans_loss=4.853, nll_loss=2.077, w2v_ctc_loss=0.709, task_loss=1.326, contrastive_loss=0.121, total=4072.1, n_correct=2722.17, ppl=4.22, accuracy=66.849, wps=13325.6, ups=1.64, wpb=8144.2, bsz=299.3, num_updates=32300, lr=7.86889e-05, gnorm=0.655, clip=0, loss_scale=4, train_wall=60, gb_free=16.4, wall=24756
2023-08-20 08:21:41 | INFO | train_inner | epoch 022:   1464 / 1474 loss=1.984, trans_loss=4.874, nll_loss=2.103, w2v_ctc_loss=0.745, task_loss=1.392, contrastive_loss=0.073, total=4094.95, n_correct=2717.98, ppl=4.3, accuracy=66.374, wps=13237.5, ups=1.62, wpb=8189.9, bsz=293.1, num_updates=32400, lr=7.85674e-05, gnorm=0.867, clip=1, loss_scale=4, train_wall=61, gb_free=14.4, wall=24818
2023-08-20 08:21:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 08:22:23 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 3.979 | trans_loss 5.194 | nll_loss 2.46 | w2v_ctc_loss 1.381 | task_loss 4.47 | contrastive_loss 0.271 | total 4003.4 | n_correct 2649.9 | ppl 5.5 | accuracy 66.191 | uer 18.942 | wer 20.846 | raw_wer 20.846 | bleu 21.67 | wps 1536.8 | wpb 4003.4 | bsz 141.8 | num_updates 32410 | best_bleu 21.7
2023-08-20 08:22:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32410 updates
2023-08-20 08:22:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.6709.pt
2023-08-20 08:22:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.6709.pt
2023-08-20 08:22:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.6709.pt (epoch 22 @ 32410 updates, score 21.67) (writing took 8.07329342601588 seconds)
2023-08-20 08:22:32 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-08-20 08:22:32 | INFO | train | epoch 022 | loss 1.971 | trans_loss 4.858 | nll_loss 2.083 | w2v_ctc_loss 0.72 | task_loss 1.326 | contrastive_loss 0.111 | total 4138.84 | n_correct 2762.93 | ppl 4.24 | accuracy 66.756 | wps 12034.8 | ups 1.45 | wpb 8277.7 | bsz 305.7 | num_updates 32410 | lr 7.85553e-05 | gnorm 0.65 | clip 0.1 | loss_scale 4 | train_wall 901 | gb_free 11.3 | wall 24869
2023-08-20 08:22:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 08:22:32 | INFO | fairseq.trainer | begin training epoch 23
2023-08-20 08:22:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 08:23:35 | INFO | train_inner | epoch 023:     90 / 1474 loss=1.963, trans_loss=4.845, nll_loss=2.066, w2v_ctc_loss=0.725, task_loss=1.367, contrastive_loss=0.062, total=4083.88, n_correct=2736.54, ppl=4.19, accuracy=67.008, wps=7139.5, ups=0.87, wpb=8167.8, bsz=298.7, num_updates=32500, lr=7.84465e-05, gnorm=0.632, clip=0, loss_scale=4, train_wall=61, gb_free=16.5, wall=24932
2023-08-20 08:24:38 | INFO | train_inner | epoch 023:    190 / 1474 loss=1.956, trans_loss=4.837, nll_loss=2.056, w2v_ctc_loss=0.71, task_loss=1.41, contrastive_loss=0.064, total=4126.02, n_correct=2769.95, ppl=4.16, accuracy=67.134, wps=13252.4, ups=1.61, wpb=8252, bsz=297, num_updates=32600, lr=7.8326e-05, gnorm=0.711, clip=0, loss_scale=4, train_wall=61, gb_free=17.3, wall=24994
2023-08-20 08:25:40 | INFO | train_inner | epoch 023:    290 / 1474 loss=1.964, trans_loss=4.848, nll_loss=2.07, w2v_ctc_loss=0.705, task_loss=1.356, contrastive_loss=0.133, total=4139.67, n_correct=2773.15, ppl=4.2, accuracy=66.99, wps=13250.1, ups=1.6, wpb=8279.3, bsz=302.3, num_updates=32700, lr=7.82062e-05, gnorm=0.552, clip=0, loss_scale=4, train_wall=62, gb_free=16.1, wall=25057
2023-08-20 08:26:41 | INFO | train_inner | epoch 023:    390 / 1474 loss=1.953, trans_loss=4.835, nll_loss=2.052, w2v_ctc_loss=0.709, task_loss=1.365, contrastive_loss=0.053, total=4117.55, n_correct=2767.77, ppl=4.15, accuracy=67.219, wps=13436.4, ups=1.63, wpb=8235.1, bsz=295.5, num_updates=32800, lr=7.80869e-05, gnorm=0.634, clip=0, loss_scale=4, train_wall=61, gb_free=13.4, wall=25118
2023-08-20 08:27:43 | INFO | train_inner | epoch 023:    490 / 1474 loss=1.962, trans_loss=4.845, nll_loss=2.066, w2v_ctc_loss=0.713, task_loss=1.306, contrastive_loss=0.109, total=4158.64, n_correct=2789.73, ppl=4.19, accuracy=67.083, wps=13497.3, ups=1.62, wpb=8317.3, bsz=311.6, num_updates=32900, lr=7.79681e-05, gnorm=0.577, clip=0, loss_scale=4, train_wall=61, gb_free=15.2, wall=25180
2023-08-20 08:28:45 | INFO | train_inner | epoch 023:    590 / 1474 loss=1.949, trans_loss=4.836, nll_loss=2.055, w2v_ctc_loss=0.706, task_loss=1.251, contrastive_loss=0.058, total=4174.2, n_correct=2809.73, ppl=4.15, accuracy=67.312, wps=13509.7, ups=1.62, wpb=8348.4, bsz=317.2, num_updates=33000, lr=7.78499e-05, gnorm=0.542, clip=0, loss_scale=4, train_wall=61, gb_free=16.1, wall=25242
2023-08-20 08:29:46 | INFO | train_inner | epoch 023:    690 / 1474 loss=1.964, trans_loss=4.849, nll_loss=2.071, w2v_ctc_loss=0.713, task_loss=1.343, contrastive_loss=0.095, total=4132.13, n_correct=2769.24, ppl=4.2, accuracy=67.017, wps=13423, ups=1.62, wpb=8264.3, bsz=300.5, num_updates=33100, lr=7.77322e-05, gnorm=0.567, clip=0, loss_scale=4, train_wall=61, gb_free=17.5, wall=25303
2023-08-20 08:30:48 | INFO | train_inner | epoch 023:    790 / 1474 loss=1.96, trans_loss=4.847, nll_loss=2.068, w2v_ctc_loss=0.713, task_loss=1.327, contrastive_loss=0.077, total=4160.29, n_correct=2787.1, ppl=4.19, accuracy=66.993, wps=13460.2, ups=1.62, wpb=8320.6, bsz=308.8, num_updates=33200, lr=7.76151e-05, gnorm=0.556, clip=0, loss_scale=4, train_wall=61, gb_free=15.8, wall=25365
2023-08-20 08:31:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-20 08:31:51 | INFO | train_inner | epoch 023:    891 / 1474 loss=1.962, trans_loss=4.842, nll_loss=2.062, w2v_ctc_loss=0.709, task_loss=1.205, contrastive_loss=0.152, total=4183.35, n_correct=2808.24, ppl=4.18, accuracy=67.129, wps=13448.5, ups=1.61, wpb=8366.7, bsz=324.8, num_updates=33300, lr=7.74984e-05, gnorm=0.603, clip=0, loss_scale=2, train_wall=61, gb_free=15.7, wall=25427
2023-08-20 08:32:53 | INFO | train_inner | epoch 023:    991 / 1474 loss=1.982, trans_loss=4.847, nll_loss=2.068, w2v_ctc_loss=0.714, task_loss=1.346, contrastive_loss=0.312, total=4158.37, n_correct=2782.79, ppl=4.19, accuracy=66.92, wps=13290.7, ups=1.6, wpb=8316.7, bsz=307.6, num_updates=33400, lr=7.73823e-05, gnorm=0.549, clip=0, loss_scale=2, train_wall=62, gb_free=16.7, wall=25490
2023-08-20 08:33:55 | INFO | train_inner | epoch 023:   1091 / 1474 loss=1.967, trans_loss=4.854, nll_loss=2.078, w2v_ctc_loss=0.722, task_loss=1.413, contrastive_loss=0.064, total=4090.33, n_correct=2732.68, ppl=4.22, accuracy=66.808, wps=13172.1, ups=1.61, wpb=8180.7, bsz=290.8, num_updates=33500, lr=7.72667e-05, gnorm=0.564, clip=0, loss_scale=2, train_wall=61, gb_free=16.1, wall=25552
2023-08-20 08:34:57 | INFO | train_inner | epoch 023:   1191 / 1474 loss=1.956, trans_loss=4.849, nll_loss=2.072, w2v_ctc_loss=0.711, task_loss=1.317, contrastive_loss=0.058, total=4157.9, n_correct=2786.41, ppl=4.21, accuracy=67.015, wps=13424.8, ups=1.61, wpb=8315.8, bsz=309, num_updates=33600, lr=7.71517e-05, gnorm=0.574, clip=0, loss_scale=2, train_wall=61, gb_free=16.3, wall=25614
2023-08-20 08:35:59 | INFO | train_inner | epoch 023:   1291 / 1474 loss=1.953, trans_loss=4.845, nll_loss=2.066, w2v_ctc_loss=0.703, task_loss=1.284, contrastive_loss=0.068, total=4147.48, n_correct=2783.22, ppl=4.19, accuracy=67.106, wps=13425.1, ups=1.62, wpb=8295, bsz=311.7, num_updates=33700, lr=7.70371e-05, gnorm=0.601, clip=0, loss_scale=2, train_wall=61, gb_free=15.8, wall=25676
2023-08-20 08:37:01 | INFO | train_inner | epoch 023:   1391 / 1474 loss=1.974, trans_loss=4.86, nll_loss=2.086, w2v_ctc_loss=0.717, task_loss=1.308, contrastive_loss=0.145, total=4165.43, n_correct=2783.82, ppl=4.25, accuracy=66.832, wps=13409.5, ups=1.61, wpb=8330.9, bsz=311, num_updates=33800, lr=7.69231e-05, gnorm=0.65, clip=0, loss_scale=2, train_wall=61, gb_free=16.2, wall=25738
2023-08-20 08:37:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-08-20 08:37:52 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 08:38:26 | INFO | dev_st | epoch 023 | valid on 'dev_st' subset | loss 3.988 | trans_loss 5.19 | nll_loss 2.455 | w2v_ctc_loss 1.42 | task_loss 4.513 | contrastive_loss 0.27 | total 4003.4 | n_correct 2657.3 | ppl 5.48 | accuracy 66.376 | uer 18.679 | wer 20.529 | raw_wer 20.529 | bleu 22.19 | wps 1558.1 | wpb 4003.4 | bsz 141.8 | num_updates 33882 | best_bleu 22.19
2023-08-20 08:38:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 33882 updates
2023-08-20 08:38:26 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 08:38:34 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 08:38:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt (epoch 23 @ 33882 updates, score 22.19) (writing took 14.186049957992509 seconds)
2023-08-20 08:38:41 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-08-20 08:38:41 | INFO | train | epoch 023 | loss 1.963 | trans_loss 4.846 | nll_loss 2.068 | w2v_ctc_loss 0.712 | task_loss 1.33 | contrastive_loss 0.11 | total 4138.69 | n_correct 2774.01 | ppl 4.19 | accuracy 67.026 | wps 12575.7 | ups 1.52 | wpb 8277.4 | bsz 305.7 | num_updates 33882 | lr 7.68299e-05 | gnorm 0.603 | clip 0.1 | loss_scale 1 | train_wall 901 | gb_free 13.3 | wall 25837
2023-08-20 08:38:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 08:38:41 | INFO | fairseq.trainer | begin training epoch 24
2023-08-20 08:38:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 08:39:00 | INFO | train_inner | epoch 024:     18 / 1474 loss=1.993, trans_loss=4.86, nll_loss=2.086, w2v_ctc_loss=0.713, task_loss=1.318, contrastive_loss=0.333, total=4100.44, n_correct=2732.7, ppl=4.25, accuracy=66.644, wps=6900.3, ups=0.84, wpb=8200.9, bsz=309.8, num_updates=33900, lr=7.68095e-05, gnorm=0.732, clip=1, loss_scale=1, train_wall=62, gb_free=15.7, wall=25857
2023-08-20 08:40:02 | INFO | train_inner | epoch 024:    118 / 1474 loss=1.944, trans_loss=4.826, nll_loss=2.04, w2v_ctc_loss=0.7, task_loss=1.276, contrastive_loss=0.069, total=4132.45, n_correct=2789.2, ppl=4.11, accuracy=67.495, wps=13381.3, ups=1.62, wpb=8264.9, bsz=312.3, num_updates=34000, lr=7.66965e-05, gnorm=0.577, clip=0, loss_scale=1, train_wall=61, gb_free=11.9, wall=25918
2023-08-20 08:40:02 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 08:40:35 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 3.984 | trans_loss 5.192 | nll_loss 2.457 | w2v_ctc_loss 1.401 | task_loss 4.518 | contrastive_loss 0.268 | total 4003.4 | n_correct 2651.4 | ppl 5.49 | accuracy 66.229 | uer 18.753 | wer 20.693 | raw_wer 20.693 | bleu 21.92 | wps 1633.7 | wpb 4003.4 | bsz 141.8 | num_updates 34000 | best_bleu 22.19
2023-08-20 08:40:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 34000 updates
2023-08-20 08:40:35 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_24_34000.pt
2023-08-20 08:40:37 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_24_34000.pt
2023-08-20 08:40:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_24_34000.pt (epoch 24 @ 34000 updates, score 21.92) (writing took 8.565238791983575 seconds)
2023-08-20 08:41:46 | INFO | train_inner | epoch 024:    218 / 1474 loss=1.966, trans_loss=4.833, nll_loss=2.051, w2v_ctc_loss=0.693, task_loss=1.15, contrastive_loss=0.273, total=4253.95, n_correct=2861.35, ppl=4.15, accuracy=67.263, wps=8155.5, ups=0.96, wpb=8507.9, bsz=341.5, num_updates=34100, lr=7.6584e-05, gnorm=1.358, clip=1, loss_scale=1, train_wall=61, gb_free=15.6, wall=26023
2023-08-20 08:42:48 | INFO | train_inner | epoch 024:    318 / 1474 loss=1.946, trans_loss=4.832, nll_loss=2.049, w2v_ctc_loss=0.701, task_loss=1.286, contrastive_loss=0.055, total=4135.06, n_correct=2784.74, ppl=4.14, accuracy=67.345, wps=13400.3, ups=1.62, wpb=8270.1, bsz=307.5, num_updates=34200, lr=7.64719e-05, gnorm=0.538, clip=0, loss_scale=1, train_wall=61, gb_free=16.1, wall=26084
2023-08-20 08:43:50 | INFO | train_inner | epoch 024:    418 / 1474 loss=1.974, trans_loss=4.836, nll_loss=2.054, w2v_ctc_loss=0.716, task_loss=1.418, contrastive_loss=0.195, total=4151.36, n_correct=2784.9, ppl=4.15, accuracy=67.084, wps=13324.7, ups=1.6, wpb=8302.7, bsz=296.7, num_updates=34300, lr=7.63604e-05, gnorm=0.602, clip=0, loss_scale=1, train_wall=62, gb_free=16.3, wall=26147
2023-08-20 08:44:53 | INFO | train_inner | epoch 024:    518 / 1474 loss=1.961, trans_loss=4.835, nll_loss=2.053, w2v_ctc_loss=0.709, task_loss=1.334, contrastive_loss=0.14, total=4165.72, n_correct=2799.75, ppl=4.15, accuracy=67.209, wps=13325, ups=1.6, wpb=8331.4, bsz=309, num_updates=34400, lr=7.62493e-05, gnorm=0.632, clip=0, loss_scale=1, train_wall=62, gb_free=16.5, wall=26209
2023-08-20 08:45:54 | INFO | train_inner | epoch 024:    618 / 1474 loss=1.948, trans_loss=4.833, nll_loss=2.05, w2v_ctc_loss=0.699, task_loss=1.342, contrastive_loss=0.065, total=4151.26, n_correct=2793.72, ppl=4.14, accuracy=67.298, wps=13487.3, ups=1.62, wpb=8302.5, bsz=303.6, num_updates=34500, lr=7.61387e-05, gnorm=0.635, clip=0, loss_scale=1, train_wall=61, gb_free=16.1, wall=26271
2023-08-20 08:46:56 | INFO | train_inner | epoch 024:    718 / 1474 loss=1.961, trans_loss=4.841, nll_loss=2.061, w2v_ctc_loss=0.713, task_loss=1.377, contrastive_loss=0.096, total=4084.41, n_correct=2739.65, ppl=4.17, accuracy=67.076, wps=13224.2, ups=1.62, wpb=8168.8, bsz=293.6, num_updates=34600, lr=7.60286e-05, gnorm=0.648, clip=0, loss_scale=1, train_wall=61, gb_free=16.2, wall=26333
2023-08-20 08:47:59 | INFO | train_inner | epoch 024:    818 / 1474 loss=1.951, trans_loss=4.842, nll_loss=2.063, w2v_ctc_loss=0.699, task_loss=1.337, contrastive_loss=0.072, total=4132.46, n_correct=2781.87, ppl=4.18, accuracy=67.318, wps=13197.9, ups=1.6, wpb=8264.9, bsz=307, num_updates=34700, lr=7.5919e-05, gnorm=0.537, clip=0, loss_scale=1, train_wall=62, gb_free=15.3, wall=26395
2023-08-20 08:49:00 | INFO | train_inner | epoch 024:    918 / 1474 loss=1.96, trans_loss=4.842, nll_loss=2.061, w2v_ctc_loss=0.718, task_loss=1.498, contrastive_loss=0.048, total=4031.91, n_correct=2703.2, ppl=4.17, accuracy=67.045, wps=13081.7, ups=1.62, wpb=8063.8, bsz=277.3, num_updates=34800, lr=7.58098e-05, gnorm=0.55, clip=0, loss_scale=1, train_wall=61, gb_free=13.6, wall=26457
2023-08-20 08:50:03 | INFO | train_inner | epoch 024:   1018 / 1474 loss=1.952, trans_loss=4.842, nll_loss=2.062, w2v_ctc_loss=0.704, task_loss=1.358, contrastive_loss=0.055, total=4132.55, n_correct=2774, ppl=4.18, accuracy=67.126, wps=13254.4, ups=1.6, wpb=8265.1, bsz=301, num_updates=34900, lr=7.57011e-05, gnorm=0.563, clip=0, loss_scale=1, train_wall=62, gb_free=16, wall=26519
2023-08-20 08:51:04 | INFO | train_inner | epoch 024:   1118 / 1474 loss=1.955, trans_loss=4.828, nll_loss=2.045, w2v_ctc_loss=0.714, task_loss=1.282, contrastive_loss=0.096, total=4134.34, n_correct=2782.42, ppl=4.13, accuracy=67.3, wps=13388.1, ups=1.62, wpb=8268.7, bsz=309.3, num_updates=35000, lr=7.55929e-05, gnorm=0.672, clip=1, loss_scale=1, train_wall=61, gb_free=14.7, wall=26581
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:0')
2023-08-20 08:52:06 | INFO | train_inner | epoch 024:   1218 / 1474 loss=1.954, trans_loss=4.84, nll_loss=2.06, w2v_ctc_loss=0.702, task_loss=1.323, contrastive_loss=0.087, total=4150.78, n_correct=2788.56, ppl=4.17, accuracy=67.182, wps=13369.7, ups=1.61, wpb=8301.6, bsz=309.3, num_updates=35100, lr=7.54851e-05, gnorm=0.627, clip=0, loss_scale=1, train_wall=61, gb_free=16.2, wall=26643
2023-08-20 08:53:09 | INFO | train_inner | epoch 024:   1318 / 1474 loss=1.956, trans_loss=4.841, nll_loss=2.06, w2v_ctc_loss=0.713, task_loss=1.409, contrastive_loss=0.06, total=4114.93, n_correct=2764.63, ppl=4.17, accuracy=67.185, wps=13223.1, ups=1.61, wpb=8229.9, bsz=295.6, num_updates=35200, lr=7.53778e-05, gnorm=0.571, clip=0, loss_scale=1, train_wall=61, gb_free=13.9, wall=26705
2023-08-20 08:54:10 | INFO | train_inner | epoch 024:   1418 / 1474 loss=1.956, trans_loss=4.842, nll_loss=2.062, w2v_ctc_loss=0.714, task_loss=1.413, contrastive_loss=0.053, total=4084.73, n_correct=2743.53, ppl=4.18, accuracy=67.166, wps=13266.2, ups=1.62, wpb=8169.5, bsz=290.3, num_updates=35300, lr=7.5271e-05, gnorm=0.624, clip=0, loss_scale=1, train_wall=61, gb_free=15.4, wall=26767
2023-08-20 08:54:45 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:5')
2023-08-20 08:55:17 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 3.967 | trans_loss 5.183 | nll_loss 2.448 | w2v_ctc_loss 1.366 | task_loss 4.527 | contrastive_loss 0.266 | total 4003.4 | n_correct 2659.2 | ppl 5.46 | accuracy 66.424 | uer 18.607 | wer 20.473 | raw_wer 20.473 | bleu 21.76 | wps 1644.6 | wpb 4003.4 | bsz 141.8 | num_updates 35356 | best_bleu 22.19
2023-08-20 08:55:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 35356 updates
2023-08-20 08:55:17 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.7600.pt
2023-08-20 08:55:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.7600.pt
2023-08-20 08:55:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.7600.pt (epoch 24 @ 35356 updates, score 21.76) (writing took 8.756911981035955 seconds)
2023-08-20 08:55:26 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-08-20 08:55:26 | INFO | train | epoch 024 | loss 1.957 | trans_loss 4.837 | nll_loss 2.055 | w2v_ctc_loss 0.706 | task_loss 1.329 | contrastive_loss 0.109 | total 4138.65 | n_correct 2781.97 | ppl 4.16 | accuracy 67.219 | wps 12130.6 | ups 1.47 | wpb 8277.3 | bsz 305.7 | num_updates 35356 | lr 7.52114e-05 | gnorm 0.648 | clip 0.1 | loss_scale 1 | train_wall 903 | gb_free 15.8 | wall 26843
2023-08-20 08:55:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 08:55:27 | INFO | fairseq.trainer | begin training epoch 25
2023-08-20 08:55:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 08:56:01 | INFO | train_inner | epoch 025:     44 / 1474 loss=1.943, trans_loss=4.827, nll_loss=2.044, w2v_ctc_loss=0.698, task_loss=1.278, contrastive_loss=0.063, total=4165.73, n_correct=2812.8, ppl=4.12, accuracy=67.522, wps=7537.9, ups=0.9, wpb=8331.5, bsz=311.2, num_updates=35400, lr=7.51646e-05, gnorm=0.565, clip=0, loss_scale=1, train_wall=60, gb_free=16.9, wall=26878
2023-08-20 08:57:02 | INFO | train_inner | epoch 025:    144 / 1474 loss=1.933, trans_loss=4.813, nll_loss=2.024, w2v_ctc_loss=0.687, task_loss=1.264, contrastive_loss=0.064, total=4145.32, n_correct=2810.68, ppl=4.07, accuracy=67.804, wps=13469.8, ups=1.62, wpb=8290.6, bsz=313.9, num_updates=35500, lr=7.50587e-05, gnorm=0.6, clip=0, loss_scale=1, train_wall=61, gb_free=15.9, wall=26939
2023-08-20 08:58:05 | INFO | train_inner | epoch 025:    244 / 1474 loss=1.939, trans_loss=4.816, nll_loss=2.028, w2v_ctc_loss=0.692, task_loss=1.385, contrastive_loss=0.062, total=4120.65, n_correct=2786.66, ppl=4.08, accuracy=67.627, wps=13191, ups=1.6, wpb=8241.3, bsz=299.5, num_updates=35600, lr=7.49532e-05, gnorm=0.546, clip=0, loss_scale=1, train_wall=62, gb_free=15.7, wall=27002
2023-08-20 08:59:07 | INFO | train_inner | epoch 025:    344 / 1474 loss=1.952, trans_loss=4.825, nll_loss=2.039, w2v_ctc_loss=0.702, task_loss=1.421, contrastive_loss=0.095, total=4130.69, n_correct=2781.29, ppl=4.11, accuracy=67.332, wps=13313.8, ups=1.61, wpb=8261.4, bsz=293.1, num_updates=35700, lr=7.48481e-05, gnorm=0.635, clip=0, loss_scale=1, train_wall=61, gb_free=15.7, wall=27064
2023-08-20 09:00:09 | INFO | train_inner | epoch 025:    444 / 1474 loss=1.963, trans_loss=4.824, nll_loss=2.039, w2v_ctc_loss=0.71, task_loss=1.392, contrastive_loss=0.167, total=4168.92, n_correct=2809.24, ppl=4.11, accuracy=67.385, wps=13383.1, ups=1.61, wpb=8337.8, bsz=297.5, num_updates=35800, lr=7.47435e-05, gnorm=0.565, clip=0, loss_scale=1, train_wall=62, gb_free=15.6, wall=27126
2023-08-20 09:01:12 | INFO | train_inner | epoch 025:    544 / 1474 loss=1.943, trans_loss=4.828, nll_loss=2.044, w2v_ctc_loss=0.698, task_loss=1.294, contrastive_loss=0.066, total=4153.43, n_correct=2804.81, ppl=4.12, accuracy=67.53, wps=13292.5, ups=1.6, wpb=8306.9, bsz=314.2, num_updates=35900, lr=7.46393e-05, gnorm=0.576, clip=0, loss_scale=2, train_wall=62, gb_free=15.7, wall=27188
2023-08-20 09:02:13 | INFO | train_inner | epoch 025:    644 / 1474 loss=1.945, trans_loss=4.814, nll_loss=2.026, w2v_ctc_loss=0.697, task_loss=1.317, contrastive_loss=0.129, total=4158.93, n_correct=2813.75, ppl=4.07, accuracy=67.656, wps=13493.8, ups=1.62, wpb=8317.9, bsz=309.4, num_updates=36000, lr=7.45356e-05, gnorm=0.545, clip=0, loss_scale=2, train_wall=61, gb_free=16.5, wall=27250
2023-08-20 09:02:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 09:02:47 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 3.972 | trans_loss 5.183 | nll_loss 2.446 | w2v_ctc_loss 1.387 | task_loss 4.494 | contrastive_loss 0.265 | total 4003.4 | n_correct 2658.9 | ppl 5.45 | accuracy 66.416 | uer 18.395 | wer 20.331 | raw_wer 20.331 | bleu 21.99 | wps 1627.9 | wpb 4003.4 | bsz 141.8 | num_updates 36000 | best_bleu 22.19
2023-08-20 09:02:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36000 updates
2023-08-20 09:02:47 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_25_36000.pt
2023-08-20 09:02:49 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_25_36000.pt
2023-08-20 09:02:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_25_36000.pt (epoch 25 @ 36000 updates, score 21.99) (writing took 9.737725731974933 seconds)
2023-08-20 09:03:59 | INFO | train_inner | epoch 025:    744 / 1474 loss=1.947, trans_loss=4.819, nll_loss=2.032, w2v_ctc_loss=0.693, task_loss=1.297, contrastive_loss=0.127, total=4159.33, n_correct=2812.33, ppl=4.09, accuracy=67.615, wps=7901.5, ups=0.95, wpb=8318.7, bsz=310.7, num_updates=36100, lr=7.44323e-05, gnorm=0.605, clip=0, loss_scale=2, train_wall=61, gb_free=16.9, wall=27355
2023-08-20 09:05:00 | INFO | train_inner | epoch 025:    844 / 1474 loss=1.944, trans_loss=4.826, nll_loss=2.042, w2v_ctc_loss=0.699, task_loss=1.275, contrastive_loss=0.073, total=4143.3, n_correct=2796.5, ppl=4.12, accuracy=67.495, wps=13456.4, ups=1.62, wpb=8286.6, bsz=316.2, num_updates=36200, lr=7.43294e-05, gnorm=0.557, clip=0, loss_scale=2, train_wall=61, gb_free=16.4, wall=27417
2023-08-20 09:06:03 | INFO | train_inner | epoch 025:    944 / 1474 loss=1.954, trans_loss=4.829, nll_loss=2.046, w2v_ctc_loss=0.708, task_loss=1.268, contrastive_loss=0.129, total=4151.66, n_correct=2800.25, ppl=4.13, accuracy=67.449, wps=13291.8, ups=1.6, wpb=8303.3, bsz=315.8, num_updates=36300, lr=7.4227e-05, gnorm=0.764, clip=1, loss_scale=2, train_wall=62, gb_free=15.4, wall=27479
2023-08-20 09:07:05 | INFO | train_inner | epoch 025:   1044 / 1474 loss=1.958, trans_loss=4.832, nll_loss=2.05, w2v_ctc_loss=0.688, task_loss=1.304, contrastive_loss=0.236, total=4192.7, n_correct=2825.05, ppl=4.14, accuracy=67.38, wps=13436.5, ups=1.6, wpb=8385.4, bsz=313.5, num_updates=36400, lr=7.41249e-05, gnorm=0.529, clip=0, loss_scale=2, train_wall=62, gb_free=16.3, wall=27542
2023-08-20 09:08:07 | INFO | train_inner | epoch 025:   1144 / 1474 loss=1.949, trans_loss=4.83, nll_loss=2.046, w2v_ctc_loss=0.704, task_loss=1.434, contrastive_loss=0.049, total=4035.97, n_correct=2718.69, ppl=4.13, accuracy=67.362, wps=13093.8, ups=1.62, wpb=8071.9, bsz=284.5, num_updates=36500, lr=7.40233e-05, gnorm=0.664, clip=0, loss_scale=2, train_wall=61, gb_free=15.5, wall=27603
2023-08-20 09:09:08 | INFO | train_inner | epoch 025:   1244 / 1474 loss=1.942, trans_loss=4.831, nll_loss=2.048, w2v_ctc_loss=0.69, task_loss=1.368, contrastive_loss=0.057, total=4078.4, n_correct=2750.01, ppl=4.14, accuracy=67.429, wps=13375.7, ups=1.64, wpb=8156.8, bsz=293.5, num_updates=36600, lr=7.39221e-05, gnorm=0.543, clip=0, loss_scale=2, train_wall=60, gb_free=15.6, wall=27664
2023-08-20 09:10:10 | INFO | train_inner | epoch 025:   1344 / 1474 loss=1.948, trans_loss=4.824, nll_loss=2.039, w2v_ctc_loss=0.69, task_loss=1.294, contrastive_loss=0.149, total=4178.95, n_correct=2822.72, ppl=4.11, accuracy=67.546, wps=13441.2, ups=1.61, wpb=8357.9, bsz=310.6, num_updates=36700, lr=7.38213e-05, gnorm=0.518, clip=0, loss_scale=2, train_wall=62, gb_free=13.1, wall=27727
2023-08-20 09:11:13 | INFO | train_inner | epoch 025:   1444 / 1474 loss=1.96, trans_loss=4.843, nll_loss=2.064, w2v_ctc_loss=0.703, task_loss=1.317, contrastive_loss=0.119, total=4130.09, n_correct=2770.72, ppl=4.18, accuracy=67.086, wps=13163.3, ups=1.59, wpb=8260.2, bsz=310.3, num_updates=36800, lr=7.3721e-05, gnorm=0.658, clip=0, loss_scale=2, train_wall=62, gb_free=15, wall=27789
2023-08-20 09:11:31 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 09:12:05 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 3.952 | trans_loss 5.177 | nll_loss 2.439 | w2v_ctc_loss 1.326 | task_loss 4.503 | contrastive_loss 0.274 | total 4003.4 | n_correct 2662.9 | ppl 5.42 | accuracy 66.516 | uer 18.249 | wer 20.066 | raw_wer 20.066 | bleu 22.18 | wps 1607.4 | wpb 4003.4 | bsz 141.8 | num_updates 36830 | best_bleu 22.19
2023-08-20 09:12:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36830 updates
2023-08-20 09:12:05 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_22.1803.pt
2023-08-20 09:12:07 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_22.1803.pt
2023-08-20 09:12:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_22.1803.pt (epoch 25 @ 36830 updates, score 22.18) (writing took 8.58870029100217 seconds)
2023-08-20 09:12:14 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-08-20 09:12:14 | INFO | train | epoch 025 | loss 1.948 | trans_loss 4.825 | nll_loss 2.04 | w2v_ctc_loss 0.697 | task_loss 1.329 | contrastive_loss 0.106 | total 4138.65 | n_correct 2792.85 | ppl 4.11 | accuracy 67.482 | wps 12110.4 | ups 1.46 | wpb 8277.3 | bsz 305.7 | num_updates 36830 | lr 7.36909e-05 | gnorm 0.591 | clip 0.1 | loss_scale 2 | train_wall 902 | gb_free 13.8 | wall 27851
2023-08-20 09:12:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 09:12:14 | INFO | fairseq.trainer | begin training epoch 26
2023-08-20 09:12:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 09:13:05 | INFO | train_inner | epoch 026:     70 / 1474 loss=1.936, trans_loss=4.811, nll_loss=2.022, w2v_ctc_loss=0.689, task_loss=1.292, contrastive_loss=0.08, total=4149.02, n_correct=2810.45, ppl=4.06, accuracy=67.738, wps=7420.4, ups=0.89, wpb=8298, bsz=311.3, num_updates=36900, lr=7.3621e-05, gnorm=0.538, clip=0, loss_scale=2, train_wall=61, gb_free=11, wall=27901
2023-08-20 09:14:07 | INFO | train_inner | epoch 026:    170 / 1474 loss=1.945, trans_loss=4.81, nll_loss=2.022, w2v_ctc_loss=0.673, task_loss=1.157, contrastive_loss=0.257, total=4278.73, n_correct=2906.77, ppl=4.06, accuracy=67.935, wps=13676, ups=1.6, wpb=8557.5, bsz=340.6, num_updates=37000, lr=7.35215e-05, gnorm=0.554, clip=0, loss_scale=2, train_wall=62, gb_free=16.5, wall=27964
2023-08-20 09:15:09 | INFO | train_inner | epoch 026:    270 / 1474 loss=1.944, trans_loss=4.809, nll_loss=2.019, w2v_ctc_loss=0.692, task_loss=1.332, contrastive_loss=0.145, total=4116.54, n_correct=2790.16, ppl=4.05, accuracy=67.779, wps=13202.8, ups=1.6, wpb=8233.1, bsz=304.7, num_updates=37100, lr=7.34223e-05, gnorm=0.607, clip=0, loss_scale=2, train_wall=62, gb_free=17.3, wall=28026
2023-08-20 09:16:11 | INFO | train_inner | epoch 026:    370 / 1474 loss=1.943, trans_loss=4.815, nll_loss=2.027, w2v_ctc_loss=0.696, task_loss=1.268, contrastive_loss=0.104, total=4166.08, n_correct=2819.25, ppl=4.08, accuracy=67.672, wps=13558.5, ups=1.63, wpb=8332.2, bsz=315.2, num_updates=37200, lr=7.33236e-05, gnorm=0.63, clip=0, loss_scale=2, train_wall=61, gb_free=13.9, wall=28088
2023-08-20 09:17:12 | INFO | train_inner | epoch 026:    470 / 1474 loss=1.942, trans_loss=4.803, nll_loss=2.012, w2v_ctc_loss=0.694, task_loss=1.291, contrastive_loss=0.149, total=4158.94, n_correct=2826.98, ppl=4.03, accuracy=67.974, wps=13542.9, ups=1.63, wpb=8317.9, bsz=312.3, num_updates=37300, lr=7.32252e-05, gnorm=0.571, clip=0, loss_scale=2, train_wall=61, gb_free=12.3, wall=28149
2023-08-20 09:18:14 | INFO | train_inner | epoch 026:    570 / 1474 loss=1.944, trans_loss=4.818, nll_loss=2.03, w2v_ctc_loss=0.705, task_loss=1.322, contrastive_loss=0.069, total=4161.02, n_correct=2813.69, ppl=4.09, accuracy=67.62, wps=13420.6, ups=1.61, wpb=8322, bsz=305.3, num_updates=37400, lr=7.31272e-05, gnorm=0.631, clip=0, loss_scale=2, train_wall=61, gb_free=15.8, wall=28211
2023-08-20 09:19:16 | INFO | train_inner | epoch 026:    670 / 1474 loss=1.936, trans_loss=4.815, nll_loss=2.027, w2v_ctc_loss=0.687, task_loss=1.363, contrastive_loss=0.054, total=4132.2, n_correct=2796.36, ppl=4.07, accuracy=67.672, wps=13312.3, ups=1.61, wpb=8264.4, bsz=297.9, num_updates=37500, lr=7.30297e-05, gnorm=0.573, clip=0, loss_scale=2, train_wall=61, gb_free=15.3, wall=28273
2023-08-20 09:20:19 | INFO | train_inner | epoch 026:    770 / 1474 loss=1.953, trans_loss=4.822, nll_loss=2.036, w2v_ctc_loss=0.695, task_loss=1.358, contrastive_loss=0.166, total=4091.8, n_correct=2758.79, ppl=4.1, accuracy=67.422, wps=13183, ups=1.61, wpb=8183.6, bsz=298.7, num_updates=37600, lr=7.29325e-05, gnorm=0.575, clip=0, loss_scale=2, train_wall=61, gb_free=16.5, wall=28335
2023-08-20 09:21:20 | INFO | train_inner | epoch 026:    870 / 1474 loss=1.94, trans_loss=4.816, nll_loss=2.028, w2v_ctc_loss=0.696, task_loss=1.33, contrastive_loss=0.069, total=4177.9, n_correct=2825.75, ppl=4.08, accuracy=67.636, wps=13541.2, ups=1.62, wpb=8355.8, bsz=306.9, num_updates=37700, lr=7.28357e-05, gnorm=0.586, clip=0, loss_scale=2, train_wall=61, gb_free=15.8, wall=28397
2023-08-20 09:22:22 | INFO | train_inner | epoch 026:    970 / 1474 loss=1.944, trans_loss=4.821, nll_loss=2.035, w2v_ctc_loss=0.686, task_loss=1.354, contrastive_loss=0.121, total=4145.98, n_correct=2798.5, ppl=4.1, accuracy=67.499, wps=13389.7, ups=1.61, wpb=8292, bsz=302.5, num_updates=37800, lr=7.27393e-05, gnorm=0.574, clip=0, loss_scale=2, train_wall=61, gb_free=16, wall=28459
2023-08-20 09:23:24 | INFO | train_inner | epoch 026:   1070 / 1474 loss=1.936, trans_loss=4.815, nll_loss=2.026, w2v_ctc_loss=0.692, task_loss=1.398, contrastive_loss=0.053, total=4118.58, n_correct=2792.17, ppl=4.07, accuracy=67.794, wps=13309.2, ups=1.62, wpb=8237.2, bsz=293.8, num_updates=37900, lr=7.26433e-05, gnorm=0.531, clip=0, loss_scale=2, train_wall=61, gb_free=16.5, wall=28521
2023-08-20 09:24:26 | INFO | train_inner | epoch 026:   1170 / 1474 loss=1.949, trans_loss=4.828, nll_loss=2.043, w2v_ctc_loss=0.699, task_loss=1.404, contrastive_loss=0.093, total=4113.01, n_correct=2776.26, ppl=4.12, accuracy=67.499, wps=13201.7, ups=1.6, wpb=8226, bsz=298.1, num_updates=38000, lr=7.25476e-05, gnorm=0.605, clip=0, loss_scale=4, train_wall=62, gb_free=12.8, wall=28583
2023-08-20 09:24:26 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 09:24:59 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 3.975 | trans_loss 5.183 | nll_loss 2.447 | w2v_ctc_loss 1.395 | task_loss 4.504 | contrastive_loss 0.261 | total 4003.4 | n_correct 2659.3 | ppl 5.45 | accuracy 66.426 | uer 18.684 | wer 20.611 | raw_wer 20.611 | bleu 21.8 | wps 1642.3 | wpb 4003.4 | bsz 141.8 | num_updates 38000 | best_bleu 22.19
2023-08-20 09:24:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38000 updates
2023-08-20 09:24:59 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_26_38000.pt
2023-08-20 09:25:02 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_26_38000.pt
2023-08-20 09:25:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_26_38000.pt (epoch 26 @ 38000 updates, score 21.8) (writing took 9.597668641014025 seconds)
2023-08-20 09:26:11 | INFO | train_inner | epoch 026:   1270 / 1474 loss=1.95, trans_loss=4.832, nll_loss=2.048, w2v_ctc_loss=0.707, task_loss=1.463, contrastive_loss=0.056, total=4007.19, n_correct=2701.34, ppl=4.14, accuracy=67.412, wps=7649, ups=0.95, wpb=8014.4, bsz=281.1, num_updates=38100, lr=7.24524e-05, gnorm=0.571, clip=0, loss_scale=4, train_wall=61, gb_free=16.3, wall=28688
2023-08-20 09:27:14 | INFO | train_inner | epoch 026:   1370 / 1474 loss=1.94, trans_loss=4.825, nll_loss=2.041, w2v_ctc_loss=0.689, task_loss=1.321, contrastive_loss=0.071, total=4157.41, n_correct=2810.79, ppl=4.12, accuracy=67.609, wps=13299.9, ups=1.6, wpb=8314.8, bsz=312.4, num_updates=38200, lr=7.23575e-05, gnorm=0.766, clip=1, loss_scale=4, train_wall=62, gb_free=16.1, wall=28750
2023-08-20 09:28:15 | INFO | train_inner | epoch 026:   1470 / 1474 loss=1.936, trans_loss=4.82, nll_loss=2.034, w2v_ctc_loss=0.687, task_loss=1.298, contrastive_loss=0.063, total=4141.4, n_correct=2804.39, ppl=4.09, accuracy=67.716, wps=13424.6, ups=1.62, wpb=8282.8, bsz=311.3, num_updates=38300, lr=7.22629e-05, gnorm=0.567, clip=0, loss_scale=4, train_wall=61, gb_free=16.8, wall=28812
2023-08-20 09:28:18 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 09:28:51 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 3.962 | trans_loss 5.178 | nll_loss 2.441 | w2v_ctc_loss 1.361 | task_loss 4.52 | contrastive_loss 0.271 | total 4003.4 | n_correct 2662.8 | ppl 5.43 | accuracy 66.513 | uer 18.257 | wer 20.137 | raw_wer 20.137 | bleu 21.84 | wps 1609.5 | wpb 4003.4 | bsz 141.8 | num_updates 38304 | best_bleu 22.19
2023-08-20 09:28:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38304 updates
2023-08-20 09:28:51 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.8403.pt
2023-08-20 09:28:54 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.8403.pt
2023-08-20 09:29:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.8403.pt (epoch 26 @ 38304 updates, score 21.84) (writing took 8.826680985977873 seconds)
2023-08-20 09:29:01 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-08-20 09:29:01 | INFO | train | epoch 026 | loss 1.942 | trans_loss 4.817 | nll_loss 2.029 | w2v_ctc_loss 0.692 | task_loss 1.33 | contrastive_loss 0.106 | total 4138.65 | n_correct 2800.8 | ppl 4.08 | accuracy 67.674 | wps 12117.6 | ups 1.46 | wpb 8277.3 | bsz 305.7 | num_updates 38304 | lr 7.22592e-05 | gnorm 0.594 | clip 0.1 | loss_scale 4 | train_wall 902 | gb_free 15.6 | wall 28858
2023-08-20 09:29:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 09:29:01 | INFO | fairseq.trainer | begin training epoch 27
2023-08-20 09:29:01 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 09:30:07 | INFO | train_inner | epoch 027:     96 / 1474 loss=1.925, trans_loss=4.788, nll_loss=1.992, w2v_ctc_loss=0.681, task_loss=1.413, contrastive_loss=0.044, total=4078.13, n_correct=2781.41, ppl=3.98, accuracy=68.203, wps=7324.2, ups=0.9, wpb=8156.3, bsz=285.1, num_updates=38400, lr=7.21688e-05, gnorm=0.592, clip=0, loss_scale=4, train_wall=60, gb_free=14.7, wall=28923
2023-08-20 09:31:09 | INFO | train_inner | epoch 027:    196 / 1474 loss=1.924, trans_loss=4.797, nll_loss=2.003, w2v_ctc_loss=0.68, task_loss=1.258, contrastive_loss=0.072, total=4185.69, n_correct=2852.32, ppl=4.01, accuracy=68.145, wps=13499.4, ups=1.61, wpb=8371.4, bsz=323.7, num_updates=38500, lr=7.2075e-05, gnorm=0.553, clip=0, loss_scale=4, train_wall=61, gb_free=16.8, wall=28985
2023-08-20 09:31:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-20 09:32:12 | INFO | train_inner | epoch 027:    297 / 1474 loss=1.936, trans_loss=4.807, nll_loss=2.016, w2v_ctc_loss=0.695, task_loss=1.334, contrastive_loss=0.056, total=4162.53, n_correct=2825.57, ppl=4.04, accuracy=67.881, wps=13181.4, ups=1.58, wpb=8325.1, bsz=305.1, num_updates=38600, lr=7.19816e-05, gnorm=0.54, clip=0, loss_scale=2, train_wall=62, gb_free=16.4, wall=29049
2023-08-20 09:33:15 | INFO | train_inner | epoch 027:    397 / 1474 loss=1.953, trans_loss=4.812, nll_loss=2.022, w2v_ctc_loss=0.688, task_loss=1.386, contrastive_loss=0.235, total=4081.39, n_correct=2763.3, ppl=4.06, accuracy=67.705, wps=12967.6, ups=1.59, wpb=8162.8, bsz=298.3, num_updates=38700, lr=7.18885e-05, gnorm=0.679, clip=1, loss_scale=2, train_wall=62, gb_free=11.7, wall=29112
2023-08-20 09:34:17 | INFO | train_inner | epoch 027:    497 / 1474 loss=1.943, trans_loss=4.816, nll_loss=2.028, w2v_ctc_loss=0.682, task_loss=1.21, contrastive_loss=0.172, total=4248.17, n_correct=2876.53, ppl=4.08, accuracy=67.712, wps=13616.1, ups=1.6, wpb=8496.3, bsz=332, num_updates=38800, lr=7.17958e-05, gnorm=0.564, clip=0, loss_scale=2, train_wall=62, gb_free=15.8, wall=29174
2023-08-20 09:35:19 | INFO | train_inner | epoch 027:    597 / 1474 loss=1.933, trans_loss=4.802, nll_loss=2.01, w2v_ctc_loss=0.683, task_loss=1.311, contrastive_loss=0.111, total=4134.4, n_correct=2809.41, ppl=4.03, accuracy=67.952, wps=13434.7, ups=1.62, wpb=8268.8, bsz=310.6, num_updates=38900, lr=7.17035e-05, gnorm=0.523, clip=0, loss_scale=2, train_wall=61, gb_free=16.5, wall=29236
2023-08-20 09:36:21 | INFO | train_inner | epoch 027:    697 / 1474 loss=1.938, trans_loss=4.809, nll_loss=2.019, w2v_ctc_loss=0.688, task_loss=1.319, contrastive_loss=0.092, total=4163.46, n_correct=2821.81, ppl=4.05, accuracy=67.776, wps=13349.7, ups=1.6, wpb=8326.9, bsz=306.6, num_updates=39000, lr=7.16115e-05, gnorm=0.574, clip=0, loss_scale=2, train_wall=62, gb_free=15.4, wall=29298
2023-08-20 09:37:23 | INFO | train_inner | epoch 027:    797 / 1474 loss=1.929, trans_loss=4.803, nll_loss=2.011, w2v_ctc_loss=0.682, task_loss=1.391, contrastive_loss=0.056, total=4110, n_correct=2795.39, ppl=4.03, accuracy=68.014, wps=13331.2, ups=1.62, wpb=8220, bsz=294.2, num_updates=39100, lr=7.15199e-05, gnorm=0.53, clip=0, loss_scale=2, train_wall=61, gb_free=17, wall=29360
2023-08-20 09:38:24 | INFO | train_inner | epoch 027:    897 / 1474 loss=1.933, trans_loss=4.814, nll_loss=2.026, w2v_ctc_loss=0.682, task_loss=1.394, contrastive_loss=0.049, total=4100.4, n_correct=2780.22, ppl=4.07, accuracy=67.804, wps=13341.8, ups=1.63, wpb=8200.8, bsz=292.4, num_updates=39200, lr=7.14286e-05, gnorm=0.573, clip=0, loss_scale=2, train_wall=61, gb_free=16.5, wall=29421
2023-08-20 09:39:27 | INFO | train_inner | epoch 027:    997 / 1474 loss=1.95, trans_loss=4.813, nll_loss=2.024, w2v_ctc_loss=0.687, task_loss=1.304, contrastive_loss=0.233, total=4187.56, n_correct=2835.53, ppl=4.07, accuracy=67.713, wps=13421.3, ups=1.6, wpb=8375.1, bsz=313.9, num_updates=39300, lr=7.13376e-05, gnorm=0.531, clip=0, loss_scale=2, train_wall=62, gb_free=15.5, wall=29483
2023-08-20 09:40:28 | INFO | train_inner | epoch 027:   1097 / 1474 loss=1.93, trans_loss=4.808, nll_loss=2.019, w2v_ctc_loss=0.679, task_loss=1.312, contrastive_loss=0.071, total=4165.48, n_correct=2827.76, ppl=4.05, accuracy=67.886, wps=13530.7, ups=1.62, wpb=8331, bsz=310.7, num_updates=39400, lr=7.1247e-05, gnorm=0.537, clip=0, loss_scale=2, train_wall=61, gb_free=16.9, wall=29545
2023-08-20 09:41:30 | INFO | train_inner | epoch 027:   1197 / 1474 loss=1.935, trans_loss=4.81, nll_loss=2.02, w2v_ctc_loss=0.687, task_loss=1.422, contrastive_loss=0.064, total=4091.68, n_correct=2772.92, ppl=4.06, accuracy=67.77, wps=13181.5, ups=1.61, wpb=8183.4, bsz=292.5, num_updates=39500, lr=7.11568e-05, gnorm=0.557, clip=0, loss_scale=2, train_wall=61, gb_free=17.2, wall=29607
2023-08-20 09:42:32 | INFO | train_inner | epoch 027:   1297 / 1474 loss=1.942, trans_loss=4.813, nll_loss=2.025, w2v_ctc_loss=0.686, task_loss=1.409, contrastive_loss=0.119, total=4057.13, n_correct=2746.06, ppl=4.07, accuracy=67.685, wps=13147.3, ups=1.62, wpb=8114.3, bsz=292.7, num_updates=39600, lr=7.10669e-05, gnorm=0.549, clip=0, loss_scale=2, train_wall=61, gb_free=14.3, wall=29669
2023-08-20 09:43:34 | INFO | train_inner | epoch 027:   1397 / 1474 loss=1.937, trans_loss=4.813, nll_loss=2.025, w2v_ctc_loss=0.685, task_loss=1.25, contrastive_loss=0.107, total=4158.28, n_correct=2819.72, ppl=4.07, accuracy=67.81, wps=13546, ups=1.63, wpb=8316.6, bsz=313.5, num_updates=39700, lr=7.09773e-05, gnorm=0.529, clip=0, loss_scale=2, train_wall=60, gb_free=16, wall=29730
2023-08-20 09:44:21 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 09:44:54 | INFO | dev_st | epoch 027 | valid on 'dev_st' subset | loss 3.959 | trans_loss 5.181 | nll_loss 2.446 | w2v_ctc_loss 1.35 | task_loss 4.502 | contrastive_loss 0.26 | total 4003.4 | n_correct 2656.4 | ppl 5.45 | accuracy 66.354 | uer 18.533 | wer 20.462 | raw_wer 20.462 | bleu 21.95 | wps 1640.7 | wpb 4003.4 | bsz 141.8 | num_updates 39777 | best_bleu 22.19
2023-08-20 09:44:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 39777 updates
2023-08-20 09:44:54 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.9501.pt
2023-08-20 09:44:57 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.9501.pt
2023-08-20 09:45:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.9501.pt (epoch 27 @ 39777 updates, score 21.95) (writing took 9.495541443990078 seconds)
2023-08-20 09:45:04 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-08-20 09:45:04 | INFO | train | epoch 027 | loss 1.936 | trans_loss 4.807 | nll_loss 2.017 | w2v_ctc_loss 0.684 | task_loss 1.329 | contrastive_loss 0.104 | total 4138.71 | n_correct 2808.64 | ppl 4.05 | accuracy 67.863 | wps 12655.3 | ups 1.53 | wpb 8277.4 | bsz 305.7 | num_updates 39777 | lr 7.09086e-05 | gnorm 0.56 | clip 0.1 | loss_scale 2 | train_wall 901 | gb_free 17.4 | wall 29821
2023-08-20 09:45:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 09:45:04 | INFO | fairseq.trainer | begin training epoch 28
2023-08-20 09:45:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 09:45:26 | INFO | train_inner | epoch 028:     23 / 1474 loss=1.923, trans_loss=4.802, nll_loss=2.01, w2v_ctc_loss=0.675, task_loss=1.281, contrastive_loss=0.056, total=4116.54, n_correct=2802.42, ppl=4.03, accuracy=68.077, wps=7329, ups=0.89, wpb=8233.1, bsz=307.5, num_updates=39800, lr=7.08881e-05, gnorm=0.794, clip=1, loss_scale=2, train_wall=61, gb_free=15.6, wall=29843
2023-08-20 09:46:27 | INFO | train_inner | epoch 028:    123 / 1474 loss=1.924, trans_loss=4.785, nll_loss=1.988, w2v_ctc_loss=0.685, task_loss=1.373, contrastive_loss=0.051, total=4115.01, n_correct=2809.53, ppl=3.97, accuracy=68.275, wps=13391.9, ups=1.63, wpb=8230, bsz=294, num_updates=39900, lr=7.07992e-05, gnorm=0.554, clip=0, loss_scale=2, train_wall=61, gb_free=17, wall=29904
2023-08-20 09:47:29 | INFO | train_inner | epoch 028:    223 / 1474 loss=1.917, trans_loss=4.792, nll_loss=1.997, w2v_ctc_loss=0.667, task_loss=1.266, contrastive_loss=0.06, total=4187.31, n_correct=2859.5, ppl=3.99, accuracy=68.29, wps=13550.7, ups=1.62, wpb=8374.6, bsz=314.5, num_updates=40000, lr=7.07107e-05, gnorm=0.566, clip=0, loss_scale=2, train_wall=61, gb_free=15, wall=29966
2023-08-20 09:47:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 09:48:02 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 3.966 | trans_loss 5.183 | nll_loss 2.444 | w2v_ctc_loss 1.368 | task_loss 4.518 | contrastive_loss 0.262 | total 4003.4 | n_correct 2662.4 | ppl 5.44 | accuracy 66.503 | uer 18.392 | wer 20.57 | raw_wer 20.57 | bleu 21.8 | wps 1654 | wpb 4003.4 | bsz 141.8 | num_updates 40000 | best_bleu 22.19
2023-08-20 09:48:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 40000 updates
2023-08-20 09:48:02 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_28_40000.pt
2023-08-20 09:48:04 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_28_40000.pt
2023-08-20 09:48:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_28_40000.pt (epoch 28 @ 40000 updates, score 21.8) (writing took 9.642484868993051 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:0')
2023-08-20 09:49:15 | INFO | train_inner | epoch 028:    323 / 1474 loss=1.956, trans_loss=4.8, nll_loss=2.008, w2v_ctc_loss=0.67, task_loss=1.333, contrastive_loss=0.39, total=4136.97, n_correct=2807.66, ppl=4.02, accuracy=67.868, wps=7837.6, ups=0.95, wpb=8273.9, bsz=313.5, num_updates=40100, lr=7.06225e-05, gnorm=0.541, clip=0, loss_scale=2, train_wall=62, gb_free=17.1, wall=30071
2023-08-20 09:50:16 | INFO | train_inner | epoch 028:    423 / 1474 loss=1.926, trans_loss=4.793, nll_loss=1.998, w2v_ctc_loss=0.686, task_loss=1.393, contrastive_loss=0.048, total=4084.26, n_correct=2783.35, ppl=3.99, accuracy=68.148, wps=13232.5, ups=1.62, wpb=8168.5, bsz=294, num_updates=40200, lr=7.05346e-05, gnorm=0.649, clip=0, loss_scale=2, train_wall=61, gb_free=16.4, wall=30133
2023-08-20 09:51:18 | INFO | train_inner | epoch 028:    523 / 1474 loss=1.926, trans_loss=4.797, nll_loss=2.004, w2v_ctc_loss=0.678, task_loss=1.348, contrastive_loss=0.064, total=4118.66, n_correct=2800.55, ppl=4.01, accuracy=67.997, wps=13338.1, ups=1.62, wpb=8237.3, bsz=301, num_updates=40300, lr=7.0447e-05, gnorm=0.617, clip=0, loss_scale=2, train_wall=61, gb_free=16.7, wall=30195
2023-08-20 09:52:20 | INFO | train_inner | epoch 028:    623 / 1474 loss=1.926, trans_loss=4.803, nll_loss=2.011, w2v_ctc_loss=0.679, task_loss=1.335, contrastive_loss=0.052, total=4180.5, n_correct=2844.17, ppl=4.03, accuracy=68.034, wps=13576.4, ups=1.62, wpb=8361, bsz=305.7, num_updates=40400, lr=7.03598e-05, gnorm=0.534, clip=0, loss_scale=2, train_wall=61, gb_free=16.2, wall=30257
2023-08-20 09:53:22 | INFO | train_inner | epoch 028:    723 / 1474 loss=1.933, trans_loss=4.804, nll_loss=2.013, w2v_ctc_loss=0.672, task_loss=1.22, contrastive_loss=0.172, total=4175.56, n_correct=2840.93, ppl=4.04, accuracy=68.037, wps=13402.1, ups=1.6, wpb=8351.1, bsz=325.1, num_updates=40500, lr=7.02728e-05, gnorm=0.534, clip=0, loss_scale=2, train_wall=62, gb_free=16.1, wall=30319
2023-08-20 09:54:24 | INFO | train_inner | epoch 028:    823 / 1474 loss=1.919, trans_loss=4.795, nll_loss=2.001, w2v_ctc_loss=0.673, task_loss=1.309, contrastive_loss=0.051, total=4085.95, n_correct=2789.38, ppl=4, accuracy=68.268, wps=13197.5, ups=1.61, wpb=8171.9, bsz=305.3, num_updates=40600, lr=7.01862e-05, gnorm=0.63, clip=0, loss_scale=4, train_wall=61, gb_free=16.9, wall=30381
2023-08-20 09:55:27 | INFO | train_inner | epoch 028:    923 / 1474 loss=1.937, trans_loss=4.804, nll_loss=2.013, w2v_ctc_loss=0.683, task_loss=1.373, contrastive_loss=0.114, total=4129.1, n_correct=2801.5, ppl=4.04, accuracy=67.848, wps=13193.6, ups=1.6, wpb=8258.2, bsz=300.9, num_updates=40700, lr=7.01e-05, gnorm=0.6, clip=0, loss_scale=4, train_wall=62, gb_free=16.4, wall=30443
2023-08-20 09:56:29 | INFO | train_inner | epoch 028:   1023 / 1474 loss=1.936, trans_loss=4.8, nll_loss=2.008, w2v_ctc_loss=0.676, task_loss=1.313, contrastive_loss=0.162, total=4171.45, n_correct=2835.8, ppl=4.02, accuracy=67.981, wps=13398.7, ups=1.61, wpb=8342.9, bsz=308.8, num_updates=40800, lr=7.0014e-05, gnorm=0.532, clip=0, loss_scale=4, train_wall=61, gb_free=16.5, wall=30506
2023-08-20 09:57:31 | INFO | train_inner | epoch 028:   1123 / 1474 loss=1.921, trans_loss=4.795, nll_loss=2.001, w2v_ctc_loss=0.673, task_loss=1.283, contrastive_loss=0.073, total=4208.27, n_correct=2866.36, ppl=4, accuracy=68.113, wps=13554, ups=1.61, wpb=8416.5, bsz=320.2, num_updates=40900, lr=6.99284e-05, gnorm=0.53, clip=0, loss_scale=4, train_wall=61, gb_free=14.6, wall=30568
2023-08-20 09:58:33 | INFO | train_inner | epoch 028:   1223 / 1474 loss=1.919, trans_loss=4.8, nll_loss=2.008, w2v_ctc_loss=0.666, task_loss=1.299, contrastive_loss=0.058, total=4125.42, n_correct=2812.42, ppl=4.02, accuracy=68.173, wps=13378.3, ups=1.62, wpb=8250.8, bsz=308, num_updates=41000, lr=6.9843e-05, gnorm=0.6, clip=0, loss_scale=4, train_wall=61, gb_free=14.5, wall=30629
2023-08-20 09:59:35 | INFO | train_inner | epoch 028:   1323 / 1474 loss=1.94, trans_loss=4.807, nll_loss=2.016, w2v_ctc_loss=0.696, task_loss=1.493, contrastive_loss=0.074, total=4060.94, n_correct=2751.67, ppl=4.04, accuracy=67.759, wps=12950.8, ups=1.59, wpb=8121.9, bsz=281.1, num_updates=41100, lr=6.9758e-05, gnorm=0.705, clip=0, loss_scale=4, train_wall=62, gb_free=15.4, wall=30692
2023-08-20 10:00:37 | INFO | train_inner | epoch 028:   1423 / 1474 loss=1.933, trans_loss=4.802, nll_loss=2.01, w2v_ctc_loss=0.681, task_loss=1.379, contrastive_loss=0.096, total=4158.96, n_correct=2826.54, ppl=4.03, accuracy=67.963, wps=13419.7, ups=1.61, wpb=8317.9, bsz=300.3, num_updates=41200, lr=6.96733e-05, gnorm=0.566, clip=0, loss_scale=4, train_wall=61, gb_free=16.5, wall=30754
2023-08-20 10:01:09 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:3')
2023-08-20 10:01:41 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 3.957 | trans_loss 5.183 | nll_loss 2.444 | w2v_ctc_loss 1.337 | task_loss 4.523 | contrastive_loss 0.262 | total 4003.4 | n_correct 2662.4 | ppl 5.44 | accuracy 66.503 | uer 18.276 | wer 20.141 | raw_wer 20.141 | bleu 22.03 | wps 1637.2 | wpb 4003.4 | bsz 141.8 | num_updates 41251 | best_bleu 22.19
2023-08-20 10:01:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 41251 updates
2023-08-20 10:01:41 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_22.0306.pt
2023-08-20 10:01:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_22.0306.pt
2023-08-20 10:01:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_22.0306.pt (epoch 28 @ 41251 updates, score 22.03) (writing took 8.265927358006593 seconds)
2023-08-20 10:01:50 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-08-20 10:01:50 | INFO | train | epoch 028 | loss 1.929 | trans_loss 4.798 | nll_loss 2.005 | w2v_ctc_loss 0.677 | task_loss 1.331 | contrastive_loss 0.103 | total 4138.65 | n_correct 2817.24 | ppl 4.01 | accuracy 68.071 | wps 12127.6 | ups 1.47 | wpb 8277.3 | bsz 305.7 | num_updates 41251 | lr 6.96302e-05 | gnorm 0.598 | clip 0.1 | loss_scale 4 | train_wall 902 | gb_free 16.1 | wall 30827
2023-08-20 10:01:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 10:01:51 | INFO | fairseq.trainer | begin training epoch 29
2023-08-20 10:01:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 10:02:28 | INFO | train_inner | epoch 029:     49 / 1474 loss=1.915, trans_loss=4.784, nll_loss=1.988, w2v_ctc_loss=0.668, task_loss=1.283, contrastive_loss=0.068, total=4165.26, n_correct=2852.71, ppl=3.97, accuracy=68.488, wps=7526.4, ups=0.9, wpb=8330.5, bsz=316, num_updates=41300, lr=6.95889e-05, gnorm=0.575, clip=0, loss_scale=4, train_wall=61, gb_free=16, wall=30865
2023-08-20 10:03:30 | INFO | train_inner | epoch 029:    149 / 1474 loss=1.921, trans_loss=4.786, nll_loss=1.99, w2v_ctc_loss=0.672, task_loss=1.316, contrastive_loss=0.088, total=4118.11, n_correct=2813.78, ppl=3.97, accuracy=68.327, wps=13330.8, ups=1.62, wpb=8236.2, bsz=306.8, num_updates=41400, lr=6.95048e-05, gnorm=0.579, clip=0, loss_scale=4, train_wall=61, gb_free=14.9, wall=30927
2023-08-20 10:04:32 | INFO | train_inner | epoch 029:    249 / 1474 loss=1.921, trans_loss=4.781, nll_loss=1.983, w2v_ctc_loss=0.661, task_loss=1.212, contrastive_loss=0.169, total=4199.43, n_correct=2875.41, ppl=3.95, accuracy=68.471, wps=13443.4, ups=1.6, wpb=8398.9, bsz=329, num_updates=41500, lr=6.9421e-05, gnorm=0.528, clip=0, loss_scale=4, train_wall=62, gb_free=11.1, wall=30989
2023-08-20 10:05:35 | INFO | train_inner | epoch 029:    349 / 1474 loss=1.925, trans_loss=4.797, nll_loss=2.003, w2v_ctc_loss=0.679, task_loss=1.442, contrastive_loss=0.054, total=4088.08, n_correct=2784.28, ppl=4.01, accuracy=68.107, wps=13128.9, ups=1.61, wpb=8176.2, bsz=289.8, num_updates=41600, lr=6.93375e-05, gnorm=0.546, clip=0, loss_scale=4, train_wall=61, gb_free=14.7, wall=31051
2023-08-20 10:06:36 | INFO | train_inner | epoch 029:    449 / 1474 loss=1.905, trans_loss=4.768, nll_loss=1.966, w2v_ctc_loss=0.66, task_loss=1.277, contrastive_loss=0.048, total=4155.15, n_correct=2855.02, ppl=3.91, accuracy=68.71, wps=13475.8, ups=1.62, wpb=8310.3, bsz=307.8, num_updates=41700, lr=6.92543e-05, gnorm=0.524, clip=0, loss_scale=4, train_wall=61, gb_free=11.7, wall=31113
2023-08-20 10:07:39 | INFO | train_inner | epoch 029:    549 / 1474 loss=1.94, trans_loss=4.799, nll_loss=2.006, w2v_ctc_loss=0.683, task_loss=1.411, contrastive_loss=0.145, total=4166.22, n_correct=2831.33, ppl=4.02, accuracy=67.959, wps=13343.3, ups=1.6, wpb=8332.4, bsz=296.4, num_updates=41800, lr=6.91714e-05, gnorm=0.573, clip=0, loss_scale=4, train_wall=62, gb_free=15.2, wall=31175
2023-08-20 10:08:41 | INFO | train_inner | epoch 029:    649 / 1474 loss=1.926, trans_loss=4.783, nll_loss=1.986, w2v_ctc_loss=0.667, task_loss=1.245, contrastive_loss=0.211, total=4139.47, n_correct=2830.82, ppl=3.96, accuracy=68.386, wps=13375.9, ups=1.62, wpb=8278.9, bsz=319.4, num_updates=41900, lr=6.90889e-05, gnorm=0.587, clip=0, loss_scale=4, train_wall=61, gb_free=11.1, wall=31237
2023-08-20 10:09:43 | INFO | train_inner | epoch 029:    749 / 1474 loss=1.924, trans_loss=4.787, nll_loss=1.99, w2v_ctc_loss=0.672, task_loss=1.243, contrastive_loss=0.13, total=4231.88, n_correct=2890.02, ppl=3.97, accuracy=68.292, wps=13491.4, ups=1.59, wpb=8463.8, bsz=327.1, num_updates=42000, lr=6.90066e-05, gnorm=0.563, clip=0, loss_scale=4, train_wall=62, gb_free=17.2, wall=31300
2023-08-20 10:09:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 10:10:16 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 3.963 | trans_loss 5.177 | nll_loss 2.438 | w2v_ctc_loss 1.373 | task_loss 4.497 | contrastive_loss 0.261 | total 4003.4 | n_correct 2661.2 | ppl 5.42 | accuracy 66.473 | uer 18.223 | wer 20.141 | raw_wer 20.141 | bleu 21.72 | wps 1643.7 | wpb 4003.4 | bsz 141.8 | num_updates 42000 | best_bleu 22.19
2023-08-20 10:10:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42000 updates
2023-08-20 10:10:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_29_42000.pt
2023-08-20 10:10:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_29_42000.pt
2023-08-20 10:10:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_29_42000.pt (epoch 29 @ 42000 updates, score 21.72) (writing took 7.276963123003952 seconds)
2023-08-20 10:11:26 | INFO | train_inner | epoch 029:    849 / 1474 loss=1.926, trans_loss=4.803, nll_loss=2.011, w2v_ctc_loss=0.676, task_loss=1.457, contrastive_loss=0.049, total=4037.23, n_correct=2743.06, ppl=4.03, accuracy=67.944, wps=7895, ups=0.98, wpb=8074.5, bsz=283.7, num_updates=42100, lr=6.89246e-05, gnorm=0.562, clip=0, loss_scale=4, train_wall=61, gb_free=11.8, wall=31402
2023-08-20 10:12:27 | INFO | train_inner | epoch 029:    949 / 1474 loss=1.927, trans_loss=4.796, nll_loss=2.002, w2v_ctc_loss=0.684, task_loss=1.39, contrastive_loss=0.055, total=4079.11, n_correct=2779.18, ppl=4.01, accuracy=68.132, wps=13309.6, ups=1.63, wpb=8158.2, bsz=292.4, num_updates=42200, lr=6.88428e-05, gnorm=0.546, clip=0, loss_scale=4, train_wall=61, gb_free=13.8, wall=31464
2023-08-20 10:13:29 | INFO | train_inner | epoch 029:   1049 / 1474 loss=1.92, trans_loss=4.785, nll_loss=1.988, w2v_ctc_loss=0.662, task_loss=1.301, contrastive_loss=0.131, total=4141.41, n_correct=2829.81, ppl=3.97, accuracy=68.33, wps=13434.3, ups=1.62, wpb=8282.8, bsz=309.2, num_updates=42300, lr=6.87614e-05, gnorm=0.568, clip=0, loss_scale=4, train_wall=61, gb_free=16.2, wall=31525
2023-08-20 10:14:31 | INFO | train_inner | epoch 029:   1149 / 1474 loss=1.927, trans_loss=4.801, nll_loss=2.009, w2v_ctc_loss=0.683, task_loss=1.45, contrastive_loss=0.046, total=4079.37, n_correct=2773.7, ppl=4.02, accuracy=67.993, wps=13182.1, ups=1.62, wpb=8158.7, bsz=285.6, num_updates=42400, lr=6.86803e-05, gnorm=0.538, clip=0, loss_scale=4, train_wall=61, gb_free=15.6, wall=31587
2023-08-20 10:15:32 | INFO | train_inner | epoch 029:   1249 / 1474 loss=1.924, trans_loss=4.799, nll_loss=2.007, w2v_ctc_loss=0.676, task_loss=1.348, contrastive_loss=0.05, total=4156.24, n_correct=2828.91, ppl=4.02, accuracy=68.064, wps=13491.4, ups=1.62, wpb=8312.5, bsz=300.7, num_updates=42500, lr=6.85994e-05, gnorm=0.519, clip=0, loss_scale=4, train_wall=61, gb_free=16.7, wall=31649
2023-08-20 10:16:35 | INFO | train_inner | epoch 029:   1349 / 1474 loss=1.927, trans_loss=4.79, nll_loss=1.995, w2v_ctc_loss=0.675, task_loss=1.321, contrastive_loss=0.114, total=4160.63, n_correct=2836.32, ppl=3.99, accuracy=68.17, wps=13320.6, ups=1.6, wpb=8321.3, bsz=309, num_updates=42600, lr=6.85189e-05, gnorm=0.609, clip=0, loss_scale=4, train_wall=62, gb_free=13.9, wall=31711
2023-08-20 10:17:36 | INFO | train_inner | epoch 029:   1449 / 1474 loss=1.925, trans_loss=4.787, nll_loss=1.991, w2v_ctc_loss=0.67, task_loss=1.287, contrastive_loss=0.144, total=4167.72, n_correct=2845.85, ppl=3.98, accuracy=68.283, wps=13558.1, ups=1.63, wpb=8335.4, bsz=314.2, num_updates=42700, lr=6.84386e-05, gnorm=0.573, clip=0, loss_scale=8, train_wall=61, gb_free=16.7, wall=31773
2023-08-20 10:17:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 10:18:24 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 3.957 | trans_loss 5.178 | nll_loss 2.438 | w2v_ctc_loss 1.348 | task_loss 4.527 | contrastive_loss 0.268 | total 4003.4 | n_correct 2662 | ppl 5.42 | accuracy 66.493 | uer 17.83 | wer 19.6 | raw_wer 19.6 | bleu 21.88 | wps 1638.2 | wpb 4003.4 | bsz 141.8 | num_updates 42725 | best_bleu 22.19
2023-08-20 10:18:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42725 updates
2023-08-20 10:18:24 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.8804.pt
2023-08-20 10:18:27 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.8804.pt
2023-08-20 10:18:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.8804.pt (epoch 29 @ 42725 updates, score 21.88) (writing took 6.87130099197384 seconds)
2023-08-20 10:18:32 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-08-20 10:18:32 | INFO | train | epoch 029 | loss 1.923 | trans_loss 4.79 | nll_loss 1.994 | w2v_ctc_loss 0.672 | task_loss 1.33 | contrastive_loss 0.102 | total 4138.65 | n_correct 2824.49 | ppl 3.98 | accuracy 68.247 | wps 12182.6 | ups 1.47 | wpb 8277.3 | bsz 305.7 | num_updates 42725 | lr 6.84186e-05 | gnorm 0.558 | clip 0 | loss_scale 8 | train_wall 901 | gb_free 15.6 | wall 31829
2023-08-20 10:18:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 10:18:32 | INFO | fairseq.trainer | begin training epoch 30
2023-08-20 10:18:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 10:19:26 | INFO | train_inner | epoch 030:     75 / 1474 loss=1.921, trans_loss=4.781, nll_loss=1.983, w2v_ctc_loss=0.661, task_loss=1.278, contrastive_loss=0.159, total=4168.99, n_correct=2853.25, ppl=3.95, accuracy=68.44, wps=7590.6, ups=0.91, wpb=8338, bsz=316.7, num_updates=42800, lr=6.83586e-05, gnorm=0.652, clip=0, loss_scale=8, train_wall=61, gb_free=16.1, wall=31883
2023-08-20 10:20:28 | INFO | train_inner | epoch 030:    175 / 1474 loss=1.914, trans_loss=4.769, nll_loss=1.968, w2v_ctc_loss=0.666, task_loss=1.234, contrastive_loss=0.093, total=4215.08, n_correct=2894.63, ppl=3.91, accuracy=68.673, wps=13576.9, ups=1.61, wpb=8430.2, bsz=321, num_updates=42900, lr=6.82789e-05, gnorm=0.679, clip=0, loss_scale=8, train_wall=61, gb_free=16.6, wall=31945
2023-08-20 10:21:30 | INFO | train_inner | epoch 030:    275 / 1474 loss=1.922, trans_loss=4.787, nll_loss=1.991, w2v_ctc_loss=0.68, task_loss=1.394, contrastive_loss=0.046, total=4110.97, n_correct=2808.23, ppl=3.97, accuracy=68.311, wps=13195.6, ups=1.6, wpb=8221.9, bsz=292.3, num_updates=43000, lr=6.81994e-05, gnorm=0.621, clip=0, loss_scale=8, train_wall=62, gb_free=15.8, wall=32007
2023-08-20 10:22:32 | INFO | train_inner | epoch 030:    375 / 1474 loss=1.909, trans_loss=4.774, nll_loss=1.973, w2v_ctc_loss=0.663, task_loss=1.302, contrastive_loss=0.052, total=4191.07, n_correct=2875.83, ppl=3.93, accuracy=68.618, wps=13490.4, ups=1.61, wpb=8382.1, bsz=311, num_updates=43100, lr=6.81203e-05, gnorm=0.636, clip=0, loss_scale=8, train_wall=61, gb_free=16.2, wall=32069
2023-08-20 10:23:34 | INFO | train_inner | epoch 030:    475 / 1474 loss=1.919, trans_loss=4.782, nll_loss=1.984, w2v_ctc_loss=0.665, task_loss=1.288, contrastive_loss=0.115, total=4121.27, n_correct=2817.27, ppl=3.96, accuracy=68.359, wps=13456.4, ups=1.63, wpb=8242.5, bsz=311.3, num_updates=43200, lr=6.80414e-05, gnorm=0.659, clip=0, loss_scale=8, train_wall=61, gb_free=15.3, wall=32130
2023-08-20 10:24:35 | INFO | train_inner | epoch 030:    575 / 1474 loss=1.915, trans_loss=4.78, nll_loss=1.982, w2v_ctc_loss=0.667, task_loss=1.313, contrastive_loss=0.077, total=4157.58, n_correct=2846.08, ppl=3.95, accuracy=68.455, wps=13489, ups=1.62, wpb=8315.2, bsz=310, num_updates=43300, lr=6.79628e-05, gnorm=0.662, clip=0, loss_scale=8, train_wall=61, gb_free=15.6, wall=32192
2023-08-20 10:25:37 | INFO | train_inner | epoch 030:    675 / 1474 loss=1.916, trans_loss=4.779, nll_loss=1.98, w2v_ctc_loss=0.668, task_loss=1.307, contrastive_loss=0.089, total=4193.34, n_correct=2871.21, ppl=3.94, accuracy=68.471, wps=13513.2, ups=1.61, wpb=8386.7, bsz=316.4, num_updates=43400, lr=6.78844e-05, gnorm=0.614, clip=0, loss_scale=8, train_wall=61, gb_free=14.5, wall=32254
2023-08-20 10:26:40 | INFO | train_inner | epoch 030:    775 / 1474 loss=1.932, trans_loss=4.787, nll_loss=1.991, w2v_ctc_loss=0.676, task_loss=1.354, contrastive_loss=0.168, total=4101.46, n_correct=2799.28, ppl=3.97, accuracy=68.251, wps=13195, ups=1.61, wpb=8202.9, bsz=302.5, num_updates=43500, lr=6.78064e-05, gnorm=0.525, clip=0, loss_scale=8, train_wall=61, gb_free=16.4, wall=32316
2023-08-20 10:27:41 | INFO | train_inner | epoch 030:    875 / 1474 loss=1.916, trans_loss=4.784, nll_loss=1.986, w2v_ctc_loss=0.666, task_loss=1.352, contrastive_loss=0.061, total=4116.1, n_correct=2816.93, ppl=3.96, accuracy=68.437, wps=13378.3, ups=1.63, wpb=8232.2, bsz=298.9, num_updates=43600, lr=6.77285e-05, gnorm=0.592, clip=0, loss_scale=8, train_wall=61, gb_free=15.8, wall=32378
2023-08-20 10:28:43 | INFO | train_inner | epoch 030:    975 / 1474 loss=1.917, trans_loss=4.786, nll_loss=1.989, w2v_ctc_loss=0.669, task_loss=1.378, contrastive_loss=0.061, total=4121.44, n_correct=2818.15, ppl=3.97, accuracy=68.378, wps=13314.1, ups=1.62, wpb=8242.9, bsz=297.4, num_updates=43700, lr=6.7651e-05, gnorm=0.52, clip=0, loss_scale=8, train_wall=61, gb_free=15.6, wall=32440
2023-08-20 10:29:46 | INFO | train_inner | epoch 030:   1075 / 1474 loss=1.927, trans_loss=4.788, nll_loss=1.991, w2v_ctc_loss=0.667, task_loss=1.501, contrastive_loss=0.139, total=4092.65, n_correct=2791.06, ppl=3.97, accuracy=68.197, wps=13070.8, ups=1.6, wpb=8185.3, bsz=280.2, num_updates=43800, lr=6.75737e-05, gnorm=0.544, clip=0, loss_scale=8, train_wall=62, gb_free=13.9, wall=32502
2023-08-20 10:30:48 | INFO | train_inner | epoch 030:   1175 / 1474 loss=1.913, trans_loss=4.781, nll_loss=1.983, w2v_ctc_loss=0.654, task_loss=1.275, contrastive_loss=0.119, total=4174.4, n_correct=2862.21, ppl=3.95, accuracy=68.566, wps=13451.2, ups=1.61, wpb=8348.8, bsz=315.6, num_updates=43900, lr=6.74967e-05, gnorm=0.534, clip=0, loss_scale=8, train_wall=61, gb_free=16.6, wall=32565
2023-08-20 10:31:50 | INFO | train_inner | epoch 030:   1275 / 1474 loss=1.922, trans_loss=4.788, nll_loss=1.992, w2v_ctc_loss=0.678, task_loss=1.467, contrastive_loss=0.055, total=4039.35, n_correct=2755.46, ppl=3.98, accuracy=68.215, wps=12907.2, ups=1.6, wpb=8078.7, bsz=284.7, num_updates=44000, lr=6.742e-05, gnorm=0.586, clip=0, loss_scale=8, train_wall=62, gb_free=16.1, wall=32627
2023-08-20 10:31:50 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 10:32:23 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 3.959 | trans_loss 5.173 | nll_loss 2.43 | w2v_ctc_loss 1.371 | task_loss 4.535 | contrastive_loss 0.257 | total 4003.4 | n_correct 2664.5 | ppl 5.39 | accuracy 66.556 | uer 17.893 | wer 19.831 | raw_wer 19.831 | bleu 21.7 | wps 1620.2 | wpb 4003.4 | bsz 141.8 | num_updates 44000 | best_bleu 22.19
2023-08-20 10:32:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44000 updates
2023-08-20 10:32:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_30_44000.pt
2023-08-20 10:32:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_30_44000.pt
2023-08-20 10:32:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_30_44000.pt (epoch 30 @ 44000 updates, score 21.7) (writing took 7.9786956399912015 seconds)
2023-08-20 10:33:33 | INFO | train_inner | epoch 030:   1375 / 1474 loss=1.906, trans_loss=4.778, nll_loss=1.979, w2v_ctc_loss=0.659, task_loss=1.253, contrastive_loss=0.064, total=4164.49, n_correct=2854.64, ppl=3.94, accuracy=68.547, wps=8086.9, ups=0.97, wpb=8329, bsz=321.7, num_updates=44100, lr=6.73435e-05, gnorm=0.553, clip=0, loss_scale=8, train_wall=60, gb_free=14.9, wall=32730
2023-08-20 10:34:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 10:35:07 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 3.961 | trans_loss 5.175 | nll_loss 2.434 | w2v_ctc_loss 1.365 | task_loss 4.532 | contrastive_loss 0.271 | total 4003.4 | n_correct 2664 | ppl 5.4 | accuracy 66.543 | uer 18.366 | wer 20.383 | raw_wer 20.383 | bleu 22.02 | wps 1630.5 | wpb 4003.4 | bsz 141.8 | num_updates 44199 | best_bleu 22.19
2023-08-20 10:35:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44199 updates
2023-08-20 10:35:07 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_22.0207.pt
2023-08-20 10:35:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_22.0207.pt
2023-08-20 10:35:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_22.0207.pt (epoch 30 @ 44199 updates, score 22.02) (writing took 7.686619104002602 seconds)
2023-08-20 10:35:16 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-08-20 10:35:16 | INFO | train | epoch 030 | loss 1.918 | trans_loss 4.782 | nll_loss 1.984 | w2v_ctc_loss 0.666 | task_loss 1.33 | contrastive_loss 0.101 | total 4138.65 | n_correct 2831.49 | ppl 3.96 | accuracy 68.416 | wps 12150.5 | ups 1.47 | wpb 8277.3 | bsz 305.7 | num_updates 44199 | lr 6.7268e-05 | gnorm 0.596 | clip 0 | loss_scale 8 | train_wall 902 | gb_free 16.8 | wall 32833
2023-08-20 10:35:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 10:35:16 | INFO | fairseq.trainer | begin training epoch 31
2023-08-20 10:35:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 10:35:24 | INFO | train_inner | epoch 031:      1 / 1474 loss=1.926, trans_loss=4.789, nll_loss=1.995, w2v_ctc_loss=0.66, task_loss=1.244, contrastive_loss=0.213, total=4125.07, n_correct=2817.07, ppl=3.98, accuracy=68.291, wps=7444.1, ups=0.9, wpb=8250.1, bsz=315.2, num_updates=44200, lr=6.72673e-05, gnorm=0.556, clip=0, loss_scale=8, train_wall=61, gb_free=15, wall=32841
2023-08-20 10:36:26 | INFO | train_inner | epoch 031:    101 / 1474 loss=1.912, trans_loss=4.771, nll_loss=1.969, w2v_ctc_loss=0.671, task_loss=1.41, contrastive_loss=0.054, total=4087.45, n_correct=2805.07, ppl=3.92, accuracy=68.626, wps=13278.5, ups=1.62, wpb=8174.9, bsz=291.9, num_updates=44300, lr=6.71913e-05, gnorm=0.566, clip=0, loss_scale=8, train_wall=61, gb_free=15.7, wall=32903
2023-08-20 10:37:28 | INFO | train_inner | epoch 031:    201 / 1474 loss=1.908, trans_loss=4.77, nll_loss=1.968, w2v_ctc_loss=0.66, task_loss=1.377, contrastive_loss=0.068, total=4137.8, n_correct=2841.11, ppl=3.91, accuracy=68.662, wps=13277.5, ups=1.6, wpb=8275.6, bsz=298.1, num_updates=44400, lr=6.71156e-05, gnorm=0.569, clip=0, loss_scale=8, train_wall=62, gb_free=14.9, wall=32965
2023-08-20 10:38:31 | INFO | train_inner | epoch 031:    301 / 1474 loss=1.914, trans_loss=4.768, nll_loss=1.965, w2v_ctc_loss=0.663, task_loss=1.375, contrastive_loss=0.116, total=4138.27, n_correct=2842.62, ppl=3.91, accuracy=68.691, wps=13236.6, ups=1.6, wpb=8276.5, bsz=300.1, num_updates=44500, lr=6.70402e-05, gnorm=0.593, clip=0, loss_scale=8, train_wall=62, gb_free=14.9, wall=33027
2023-08-20 10:39:32 | INFO | train_inner | epoch 031:    401 / 1474 loss=1.908, trans_loss=4.776, nll_loss=1.976, w2v_ctc_loss=0.657, task_loss=1.443, contrastive_loss=0.052, total=4098.88, n_correct=2813.06, ppl=3.93, accuracy=68.63, wps=13289.6, ups=1.62, wpb=8197.8, bsz=286.9, num_updates=44600, lr=6.6965e-05, gnorm=0.549, clip=0, loss_scale=8, train_wall=61, gb_free=16.7, wall=33089
2023-08-20 10:40:35 | INFO | train_inner | epoch 031:    501 / 1474 loss=1.911, trans_loss=4.771, nll_loss=1.969, w2v_ctc_loss=0.669, task_loss=1.369, contrastive_loss=0.062, total=4123.26, n_correct=2829.86, ppl=3.91, accuracy=68.632, wps=13258.1, ups=1.61, wpb=8246.5, bsz=302.9, num_updates=44700, lr=6.689e-05, gnorm=0.652, clip=0, loss_scale=16, train_wall=62, gb_free=16.4, wall=33151
2023-08-20 10:41:37 | INFO | train_inner | epoch 031:    601 / 1474 loss=1.908, trans_loss=4.77, nll_loss=1.969, w2v_ctc_loss=0.661, task_loss=1.413, contrastive_loss=0.052, total=4069.38, n_correct=2796.32, ppl=3.91, accuracy=68.716, wps=13072.9, ups=1.61, wpb=8138.8, bsz=291.2, num_updates=44800, lr=6.68153e-05, gnorm=0.573, clip=0, loss_scale=16, train_wall=62, gb_free=17.1, wall=33214
2023-08-20 10:41:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-20 10:42:39 | INFO | train_inner | epoch 031:    702 / 1474 loss=1.904, trans_loss=4.77, nll_loss=1.969, w2v_ctc_loss=0.656, task_loss=1.249, contrastive_loss=0.054, total=4211.34, n_correct=2894.4, ppl=3.92, accuracy=68.729, wps=13447.4, ups=1.6, wpb=8422.7, bsz=316.6, num_updates=44900, lr=6.67409e-05, gnorm=0.635, clip=0, loss_scale=8, train_wall=62, gb_free=16.8, wall=33276
2023-08-20 10:43:42 | INFO | train_inner | epoch 031:    802 / 1474 loss=1.918, trans_loss=4.777, nll_loss=1.978, w2v_ctc_loss=0.661, task_loss=1.378, contrastive_loss=0.123, total=4104.39, n_correct=2809.47, ppl=3.94, accuracy=68.45, wps=13211, ups=1.61, wpb=8208.8, bsz=297.1, num_updates=45000, lr=6.66667e-05, gnorm=0.582, clip=0, loss_scale=8, train_wall=61, gb_free=16, wall=33338
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:0')
2023-08-20 10:44:44 | INFO | train_inner | epoch 031:    902 / 1474 loss=1.911, trans_loss=4.769, nll_loss=1.966, w2v_ctc_loss=0.664, task_loss=1.416, contrastive_loss=0.067, total=4094.66, n_correct=2809.44, ppl=3.91, accuracy=68.612, wps=13164.9, ups=1.61, wpb=8189.3, bsz=292.5, num_updates=45100, lr=6.65927e-05, gnorm=0.715, clip=1, loss_scale=8, train_wall=62, gb_free=16.7, wall=33401
2023-08-20 10:45:45 | INFO | train_inner | epoch 031:   1002 / 1474 loss=1.916, trans_loss=4.78, nll_loss=1.983, w2v_ctc_loss=0.658, task_loss=1.242, contrastive_loss=0.149, total=4186.13, n_correct=2869.89, ppl=3.95, accuracy=68.557, wps=13573.6, ups=1.62, wpb=8372.3, bsz=320.7, num_updates=45200, lr=6.6519e-05, gnorm=0.548, clip=0, loss_scale=8, train_wall=61, gb_free=15.8, wall=33462
2023-08-20 10:45:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-20 10:46:48 | INFO | train_inner | epoch 031:   1103 / 1474 loss=1.911, trans_loss=4.775, nll_loss=1.976, w2v_ctc_loss=0.657, task_loss=1.279, contrastive_loss=0.098, total=4164.01, n_correct=2855.37, ppl=3.93, accuracy=68.573, wps=13410.2, ups=1.61, wpb=8328, bsz=318.9, num_updates=45300, lr=6.64455e-05, gnorm=0.556, clip=0, loss_scale=4, train_wall=61, gb_free=16.5, wall=33524
2023-08-20 10:47:49 | INFO | train_inner | epoch 031:   1203 / 1474 loss=1.922, trans_loss=4.778, nll_loss=1.98, w2v_ctc_loss=0.66, task_loss=1.244, contrastive_loss=0.213, total=4186.26, n_correct=2868.23, ppl=3.94, accuracy=68.515, wps=13537.8, ups=1.62, wpb=8372.5, bsz=321.9, num_updates=45400, lr=6.63723e-05, gnorm=0.597, clip=0, loss_scale=4, train_wall=61, gb_free=15.6, wall=33586
2023-08-20 10:48:51 | INFO | train_inner | epoch 031:   1303 / 1474 loss=1.909, trans_loss=4.781, nll_loss=1.984, w2v_ctc_loss=0.663, task_loss=1.208, contrastive_loss=0.058, total=4214.93, n_correct=2887.92, ppl=3.96, accuracy=68.516, wps=13655.9, ups=1.62, wpb=8429.9, bsz=323.2, num_updates=45500, lr=6.62994e-05, gnorm=0.552, clip=0, loss_scale=4, train_wall=61, gb_free=15.7, wall=33648
2023-08-20 10:49:54 | INFO | train_inner | epoch 031:   1403 / 1474 loss=1.926, trans_loss=4.775, nll_loss=1.976, w2v_ctc_loss=0.654, task_loss=1.217, contrastive_loss=0.26, total=4198.91, n_correct=2881.62, ppl=3.93, accuracy=68.628, wps=13422.7, ups=1.6, wpb=8397.8, bsz=328.4, num_updates=45600, lr=6.62266e-05, gnorm=0.545, clip=0, loss_scale=4, train_wall=62, gb_free=16.7, wall=33710
2023-08-20 10:50:38 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2340, device='cuda:2')
2023-08-20 10:51:11 | INFO | dev_st | epoch 031 | valid on 'dev_st' subset | loss 3.961 | trans_loss 5.17 | nll_loss 2.429 | w2v_ctc_loss 1.385 | task_loss 4.546 | contrastive_loss 0.258 | total 4003.4 | n_correct 2667.7 | ppl 5.39 | accuracy 66.636 | uer 17.949 | wer 19.768 | raw_wer 19.768 | bleu 22.33 | wps 1595.3 | wpb 4003.4 | bsz 141.8 | num_updates 45671 | best_bleu 22.33
2023-08-20 10:51:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 45671 updates
2023-08-20 10:51:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 10:51:18 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 10:51:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt (epoch 31 @ 45671 updates, score 22.33) (writing took 12.706434714025818 seconds)
2023-08-20 10:51:24 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-08-20 10:51:24 | INFO | train | epoch 031 | loss 1.913 | trans_loss 4.774 | nll_loss 1.974 | w2v_ctc_loss 0.661 | task_loss 1.329 | contrastive_loss 0.101 | total 4139.04 | n_correct 2839.55 | ppl 3.93 | accuracy 68.604 | wps 12584 | ups 1.52 | wpb 8278.1 | bsz 305.8 | num_updates 45671 | lr 6.61751e-05 | gnorm 0.586 | clip 0.1 | loss_scale 4 | train_wall 903 | gb_free 11.6 | wall 33801
2023-08-20 10:51:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 10:51:24 | INFO | fairseq.trainer | begin training epoch 32
2023-08-20 10:51:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 10:51:50 | INFO | train_inner | epoch 032:     29 / 1474 loss=1.905, trans_loss=4.769, nll_loss=1.968, w2v_ctc_loss=0.659, task_loss=1.371, contrastive_loss=0.048, total=4063.98, n_correct=2792.17, ppl=3.91, accuracy=68.705, wps=7019.8, ups=0.86, wpb=8128, bsz=294.7, num_updates=45700, lr=6.61541e-05, gnorm=0.555, clip=0, loss_scale=4, train_wall=61, gb_free=16.8, wall=33826
2023-08-20 10:52:52 | INFO | train_inner | epoch 032:    129 / 1474 loss=1.892, trans_loss=4.751, nll_loss=1.944, w2v_ctc_loss=0.646, task_loss=1.262, contrastive_loss=0.057, total=4194.92, n_correct=2897.73, ppl=3.85, accuracy=69.077, wps=13501.2, ups=1.61, wpb=8389.8, bsz=315.1, num_updates=45800, lr=6.60819e-05, gnorm=0.544, clip=0, loss_scale=4, train_wall=61, gb_free=15.7, wall=33888
2023-08-20 10:53:54 | INFO | train_inner | epoch 032:    229 / 1474 loss=1.901, trans_loss=4.767, nll_loss=1.966, w2v_ctc_loss=0.655, task_loss=1.259, contrastive_loss=0.067, total=4158.74, n_correct=2861.33, ppl=3.91, accuracy=68.803, wps=13312.2, ups=1.6, wpb=8317.5, bsz=321.8, num_updates=45900, lr=6.60098e-05, gnorm=0.544, clip=0, loss_scale=4, train_wall=62, gb_free=16.6, wall=33951
2023-08-20 10:54:56 | INFO | train_inner | epoch 032:    329 / 1474 loss=1.892, trans_loss=4.751, nll_loss=1.944, w2v_ctc_loss=0.643, task_loss=1.271, contrastive_loss=0.059, total=4179.94, n_correct=2891.04, ppl=3.85, accuracy=69.165, wps=13483.4, ups=1.61, wpb=8359.9, bsz=311.9, num_updates=46000, lr=6.5938e-05, gnorm=0.565, clip=0, loss_scale=4, train_wall=61, gb_free=16.7, wall=34013
2023-08-20 10:54:56 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 10:55:29 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 3.978 | trans_loss 5.176 | nll_loss 2.437 | w2v_ctc_loss 1.425 | task_loss 4.512 | contrastive_loss 0.261 | total 4003.4 | n_correct 2665.9 | ppl 5.42 | accuracy 66.591 | uer 18.212 | wer 20.294 | raw_wer 20.294 | bleu 22.07 | wps 1618.5 | wpb 4003.4 | bsz 141.8 | num_updates 46000 | best_bleu 22.33
2023-08-20 10:55:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 46000 updates
2023-08-20 10:55:29 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_32_46000.pt
2023-08-20 10:55:32 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_32_46000.pt
2023-08-20 10:55:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_32_46000.pt (epoch 32 @ 46000 updates, score 22.07) (writing took 8.016483165032696 seconds)
2023-08-20 10:56:39 | INFO | train_inner | epoch 032:    429 / 1474 loss=1.902, trans_loss=4.758, nll_loss=1.954, w2v_ctc_loss=0.66, task_loss=1.285, contrastive_loss=0.06, total=4177.5, n_correct=2881.28, ppl=3.87, accuracy=68.971, wps=8105.2, ups=0.97, wpb=8355, bsz=312.1, num_updates=46100, lr=6.58665e-05, gnorm=0.597, clip=0, loss_scale=4, train_wall=60, gb_free=15.8, wall=34116
2023-08-20 10:57:42 | INFO | train_inner | epoch 032:    529 / 1474 loss=1.912, trans_loss=4.768, nll_loss=1.966, w2v_ctc_loss=0.657, task_loss=1.293, contrastive_loss=0.136, total=4193.38, n_correct=2882.16, ppl=3.91, accuracy=68.731, wps=13378.3, ups=1.6, wpb=8386.8, bsz=316.1, num_updates=46200, lr=6.57952e-05, gnorm=0.559, clip=0, loss_scale=4, train_wall=62, gb_free=17.2, wall=34179
2023-08-20 10:58:44 | INFO | train_inner | epoch 032:    629 / 1474 loss=1.912, trans_loss=4.772, nll_loss=1.971, w2v_ctc_loss=0.666, task_loss=1.408, contrastive_loss=0.067, total=4127.83, n_correct=2831.05, ppl=3.92, accuracy=68.584, wps=13244.3, ups=1.6, wpb=8255.7, bsz=297.2, num_updates=46300, lr=6.57241e-05, gnorm=0.725, clip=1, loss_scale=4, train_wall=61, gb_free=15.6, wall=34241
2023-08-20 10:59:46 | INFO | train_inner | epoch 032:    729 / 1474 loss=1.91, trans_loss=4.771, nll_loss=1.97, w2v_ctc_loss=0.671, task_loss=1.348, contrastive_loss=0.05, total=4166.29, n_correct=2862.65, ppl=3.92, accuracy=68.71, wps=13400.2, ups=1.61, wpb=8332.6, bsz=304.3, num_updates=46400, lr=6.56532e-05, gnorm=0.638, clip=0, loss_scale=4, train_wall=61, gb_free=13, wall=34303
2023-08-20 11:00:48 | INFO | train_inner | epoch 032:    829 / 1474 loss=1.903, trans_loss=4.766, nll_loss=1.964, w2v_ctc_loss=0.655, task_loss=1.373, contrastive_loss=0.048, total=4111.78, n_correct=2829.06, ppl=3.9, accuracy=68.804, wps=13379.6, ups=1.63, wpb=8223.6, bsz=293.6, num_updates=46500, lr=6.55826e-05, gnorm=0.7, clip=0, loss_scale=4, train_wall=61, gb_free=15.6, wall=34365
2023-08-20 11:01:50 | INFO | train_inner | epoch 032:    929 / 1474 loss=1.907, trans_loss=4.771, nll_loss=1.97, w2v_ctc_loss=0.659, task_loss=1.372, contrastive_loss=0.048, total=4143.54, n_correct=2842.62, ppl=3.92, accuracy=68.604, wps=13329.7, ups=1.61, wpb=8287.1, bsz=298.9, num_updates=46600, lr=6.55122e-05, gnorm=0.7, clip=0, loss_scale=4, train_wall=61, gb_free=15.7, wall=34427
2023-08-20 11:02:52 | INFO | train_inner | epoch 032:   1029 / 1474 loss=1.915, trans_loss=4.772, nll_loss=1.971, w2v_ctc_loss=0.661, task_loss=1.323, contrastive_loss=0.143, total=4108.34, n_correct=2820.29, ppl=3.92, accuracy=68.648, wps=13308.3, ups=1.62, wpb=8216.7, bsz=303, num_updates=46700, lr=6.5442e-05, gnorm=0.631, clip=0, loss_scale=4, train_wall=61, gb_free=11.4, wall=34489
2023-08-20 11:03:54 | INFO | train_inner | epoch 032:   1129 / 1474 loss=1.917, trans_loss=4.773, nll_loss=1.972, w2v_ctc_loss=0.666, task_loss=1.572, contrastive_loss=0.082, total=4016.76, n_correct=2749.57, ppl=3.92, accuracy=68.452, wps=12928.8, ups=1.61, wpb=8033.5, bsz=271.3, num_updates=46800, lr=6.5372e-05, gnorm=0.576, clip=0, loss_scale=4, train_wall=61, gb_free=15.5, wall=34551
2023-08-20 11:04:56 | INFO | train_inner | epoch 032:   1229 / 1474 loss=1.925, trans_loss=4.779, nll_loss=1.981, w2v_ctc_loss=0.663, task_loss=1.299, contrastive_loss=0.185, total=4166.97, n_correct=2855.31, ppl=3.95, accuracy=68.522, wps=13340, ups=1.6, wpb=8333.9, bsz=312.9, num_updates=46900, lr=6.53023e-05, gnorm=0.579, clip=0, loss_scale=4, train_wall=62, gb_free=15.2, wall=34613
2023-08-20 11:05:58 | INFO | train_inner | epoch 032:   1329 / 1474 loss=1.903, trans_loss=4.766, nll_loss=1.963, w2v_ctc_loss=0.659, task_loss=1.355, contrastive_loss=0.047, total=4063.47, n_correct=2796.15, ppl=3.9, accuracy=68.812, wps=13258.6, ups=1.63, wpb=8126.9, bsz=297, num_updates=47000, lr=6.52328e-05, gnorm=0.551, clip=0, loss_scale=4, train_wall=61, gb_free=17, wall=34674
2023-08-20 11:07:00 | INFO | train_inner | epoch 032:   1429 / 1474 loss=1.936, trans_loss=4.775, nll_loss=1.976, w2v_ctc_loss=0.664, task_loss=1.329, contrastive_loss=0.316, total=4124.87, n_correct=2826.45, ppl=3.93, accuracy=68.522, wps=13336.3, ups=1.62, wpb=8249.7, bsz=308.5, num_updates=47100, lr=6.51635e-05, gnorm=0.898, clip=1, loss_scale=4, train_wall=61, gb_free=16.2, wall=34736
2023-08-20 11:07:27 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 11:08:00 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 3.954 | trans_loss 5.178 | nll_loss 2.44 | w2v_ctc_loss 1.337 | task_loss 4.519 | contrastive_loss 0.265 | total 4003.4 | n_correct 2661.4 | ppl 5.43 | accuracy 66.478 | uer 18.366 | wer 20.342 | raw_wer 20.342 | bleu 21.92 | wps 1654.3 | wpb 4003.4 | bsz 141.8 | num_updates 47145 | best_bleu 22.33
2023-08-20 11:08:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 47145 updates
2023-08-20 11:08:00 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.9201.pt
2023-08-20 11:08:02 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.9201.pt
2023-08-20 11:08:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint.best_bleu_21.9201.pt (epoch 32 @ 47145 updates, score 21.92) (writing took 7.024000971985515 seconds)
2023-08-20 11:08:07 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-08-20 11:08:07 | INFO | train | epoch 032 | loss 1.909 | trans_loss 4.767 | nll_loss 1.965 | w2v_ctc_loss 0.658 | task_loss 1.329 | contrastive_loss 0.1 | total 4138.65 | n_correct 2845.51 | ppl 3.9 | accuracy 68.755 | wps 12161.5 | ups 1.47 | wpb 8277.3 | bsz 305.7 | num_updates 47145 | lr 6.51324e-05 | gnorm 0.636 | clip 0.1 | loss_scale 4 | train_wall 901 | gb_free 16 | wall 34804
2023-08-20 11:08:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 11:08:08 | INFO | fairseq.trainer | begin training epoch 33
2023-08-20 11:08:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 11:08:49 | INFO | train_inner | epoch 033:     55 / 1474 loss=1.91, trans_loss=4.767, nll_loss=1.965, w2v_ctc_loss=0.66, task_loss=1.274, contrastive_loss=0.113, total=4141.34, n_correct=2849.27, ppl=3.9, accuracy=68.801, wps=7562.5, ups=0.91, wpb=8282.7, bsz=316.2, num_updates=47200, lr=6.50945e-05, gnorm=0.734, clip=0, loss_scale=4, train_wall=61, gb_free=15.7, wall=34846
2023-08-20 11:09:51 | INFO | train_inner | epoch 033:    155 / 1474 loss=1.893, trans_loss=4.75, nll_loss=1.942, w2v_ctc_loss=0.642, task_loss=1.419, contrastive_loss=0.039, total=4077.99, n_correct=2818.32, ppl=3.84, accuracy=69.111, wps=13162.8, ups=1.61, wpb=8156, bsz=285.4, num_updates=47300, lr=6.50256e-05, gnorm=0.621, clip=0, loss_scale=8, train_wall=61, gb_free=16, wall=34908
2023-08-20 11:10:54 | INFO | train_inner | epoch 033:    255 / 1474 loss=1.908, trans_loss=4.752, nll_loss=1.946, w2v_ctc_loss=0.641, task_loss=1.125, contrastive_loss=0.211, total=4280.69, n_correct=2956.05, ppl=3.85, accuracy=69.055, wps=13679.3, ups=1.6, wpb=8561.4, bsz=347.1, num_updates=47400, lr=6.4957e-05, gnorm=0.571, clip=0, loss_scale=8, train_wall=62, gb_free=16.2, wall=34970
2023-08-20 11:11:56 | INFO | train_inner | epoch 033:    355 / 1474 loss=1.903, trans_loss=4.759, nll_loss=1.955, w2v_ctc_loss=0.659, task_loss=1.346, contrastive_loss=0.066, total=4131.92, n_correct=2844.2, ppl=3.88, accuracy=68.835, wps=13320.7, ups=1.61, wpb=8263.8, bsz=303.2, num_updates=47500, lr=6.48886e-05, gnorm=0.563, clip=0, loss_scale=8, train_wall=61, gb_free=16, wall=35032
2023-08-20 11:12:57 | INFO | train_inner | epoch 033:    455 / 1474 loss=1.888, trans_loss=4.746, nll_loss=1.936, w2v_ctc_loss=0.644, task_loss=1.278, contrastive_loss=0.047, total=4124.62, n_correct=2857.42, ppl=3.83, accuracy=69.277, wps=13476.3, ups=1.63, wpb=8249.2, bsz=307.5, num_updates=47600, lr=6.48204e-05, gnorm=0.669, clip=1, loss_scale=8, train_wall=60, gb_free=16.4, wall=35094
2023-08-20 11:13:59 | INFO | train_inner | epoch 033:    555 / 1474 loss=1.903, trans_loss=4.762, nll_loss=1.957, w2v_ctc_loss=0.654, task_loss=1.37, contrastive_loss=0.067, total=4147.63, n_correct=2856.64, ppl=3.88, accuracy=68.874, wps=13385.3, ups=1.61, wpb=8295.3, bsz=296.9, num_updates=47700, lr=6.47524e-05, gnorm=0.547, clip=0, loss_scale=8, train_wall=61, gb_free=16.7, wall=35156
2023-08-20 11:15:01 | INFO | train_inner | epoch 033:    655 / 1474 loss=1.912, trans_loss=4.772, nll_loss=1.971, w2v_ctc_loss=0.659, task_loss=1.388, contrastive_loss=0.1, total=4142.79, n_correct=2844.79, ppl=3.92, accuracy=68.668, wps=13281.2, ups=1.6, wpb=8285.6, bsz=298.3, num_updates=47800, lr=6.46846e-05, gnorm=0.542, clip=0, loss_scale=8, train_wall=61, gb_free=15.9, wall=35218
2023-08-20 11:15:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-20 11:16:04 | INFO | train_inner | epoch 033:    756 / 1474 loss=1.912, trans_loss=4.768, nll_loss=1.965, w2v_ctc_loss=0.673, task_loss=1.402, contrastive_loss=0.048, total=4090.27, n_correct=2807.53, ppl=3.9, accuracy=68.639, wps=13093.9, ups=1.6, wpb=8180.5, bsz=293.9, num_updates=47900, lr=6.46171e-05, gnorm=0.611, clip=0, loss_scale=4, train_wall=62, gb_free=14.5, wall=35281
2023-08-20 11:17:05 | INFO | train_inner | epoch 033:    856 / 1474 loss=1.898, trans_loss=4.757, nll_loss=1.953, w2v_ctc_loss=0.642, task_loss=1.295, contrastive_loss=0.115, total=4121.84, n_correct=2846.41, ppl=3.87, accuracy=69.057, wps=13385.1, ups=1.62, wpb=8243.7, bsz=312.2, num_updates=48000, lr=6.45497e-05, gnorm=0.686, clip=1, loss_scale=4, train_wall=60, gb_free=14.3, wall=35342
2023-08-20 11:17:05 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 11:17:38 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 3.965 | trans_loss 5.18 | nll_loss 2.439 | w2v_ctc_loss 1.375 | task_loss 4.547 | contrastive_loss 0.255 | total 4003.4 | n_correct 2669 | ppl 5.42 | accuracy 66.668 | uer 17.774 | wer 19.567 | raw_wer 19.567 | bleu 21.91 | wps 1671 | wpb 4003.4 | bsz 141.8 | num_updates 48000 | best_bleu 22.33
2023-08-20 11:17:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48000 updates
2023-08-20 11:17:38 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_33_48000.pt
2023-08-20 11:17:40 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_33_48000.pt
2023-08-20 11:17:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_33_48000.pt (epoch 33 @ 48000 updates, score 21.91) (writing took 7.125174231012352 seconds)
2023-08-20 11:18:47 | INFO | train_inner | epoch 033:    956 / 1474 loss=1.9, trans_loss=4.759, nll_loss=1.954, w2v_ctc_loss=0.657, task_loss=1.298, contrastive_loss=0.061, total=4170.72, n_correct=2879.09, ppl=3.88, accuracy=69.031, wps=8213.7, ups=0.98, wpb=8341.4, bsz=313.2, num_updates=48100, lr=6.44826e-05, gnorm=0.545, clip=0, loss_scale=4, train_wall=61, gb_free=15.7, wall=35444
2023-08-20 11:19:49 | INFO | train_inner | epoch 033:   1056 / 1474 loss=1.913, trans_loss=4.758, nll_loss=1.953, w2v_ctc_loss=0.657, task_loss=1.364, contrastive_loss=0.161, total=4122.22, n_correct=2837.72, ppl=3.87, accuracy=68.84, wps=13217.9, ups=1.6, wpb=8244.4, bsz=302.3, num_updates=48200, lr=6.44157e-05, gnorm=0.565, clip=0, loss_scale=4, train_wall=62, gb_free=16.4, wall=35506
2023-08-20 11:20:52 | INFO | train_inner | epoch 033:   1156 / 1474 loss=1.908, trans_loss=4.767, nll_loss=1.965, w2v_ctc_loss=0.645, task_loss=1.333, contrastive_loss=0.151, total=4181.69, n_correct=2876.83, ppl=3.91, accuracy=68.796, wps=13362.1, ups=1.6, wpb=8363.4, bsz=309.9, num_updates=48300, lr=6.43489e-05, gnorm=0.543, clip=0, loss_scale=4, train_wall=62, gb_free=16.1, wall=35569
2023-08-20 11:21:54 | INFO | train_inner | epoch 033:   1256 / 1474 loss=1.903, trans_loss=4.761, nll_loss=1.957, w2v_ctc_loss=0.66, task_loss=1.395, contrastive_loss=0.051, total=4112.96, n_correct=2832.97, ppl=3.88, accuracy=68.879, wps=13230.8, ups=1.61, wpb=8225.9, bsz=294.9, num_updates=48400, lr=6.42824e-05, gnorm=0.565, clip=0, loss_scale=4, train_wall=61, gb_free=15.3, wall=35631
2023-08-20 11:22:56 | INFO | train_inner | epoch 033:   1356 / 1474 loss=1.904, trans_loss=4.765, nll_loss=1.963, w2v_ctc_loss=0.658, task_loss=1.31, contrastive_loss=0.072, total=4121.63, n_correct=2837.71, ppl=3.9, accuracy=68.849, wps=13250.7, ups=1.61, wpb=8243.3, bsz=311.1, num_updates=48500, lr=6.42161e-05, gnorm=0.658, clip=0, loss_scale=4, train_wall=62, gb_free=16.6, wall=35693
2023-08-20 11:23:59 | INFO | train_inner | epoch 033:   1456 / 1474 loss=1.914, trans_loss=4.763, nll_loss=1.96, w2v_ctc_loss=0.651, task_loss=1.322, contrastive_loss=0.215, total=4133.76, n_correct=2845.27, ppl=3.89, accuracy=68.83, wps=13269, ups=1.6, wpb=8267.5, bsz=310.2, num_updates=48600, lr=6.415e-05, gnorm=0.74, clip=1, loss_scale=4, train_wall=61, gb_free=14.6, wall=35755
2023-08-20 11:24:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 11:24:43 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 3.959 | trans_loss 5.178 | nll_loss 2.436 | w2v_ctc_loss 1.361 | task_loss 4.547 | contrastive_loss 0.256 | total 4003.4 | n_correct 2666.8 | ppl 5.41 | accuracy 66.613 | uer 17.782 | wer 19.694 | raw_wer 19.694 | bleu 22.46 | wps 1630.9 | wpb 4003.4 | bsz 141.8 | num_updates 48618 | best_bleu 22.46
2023-08-20 11:24:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48618 updates
2023-08-20 11:24:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 11:24:49 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-20 11:24:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_best.pt (epoch 33 @ 48618 updates, score 22.46) (writing took 12.209539687028155 seconds)
2023-08-20 11:24:55 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-08-20 11:24:55 | INFO | train | epoch 033 | loss 1.904 | trans_loss 4.76 | nll_loss 1.956 | w2v_ctc_loss 0.653 | task_loss 1.33 | contrastive_loss 0.099 | total 4138.88 | n_correct 2852 | ppl 3.88 | accuracy 68.907 | wps 12098.7 | ups 1.46 | wpb 8277.8 | bsz 305.7 | num_updates 48618 | lr 6.41382e-05 | gnorm 0.601 | clip 0.2 | loss_scale 4 | train_wall 902 | gb_free 17.5 | wall 35812
2023-08-20 11:24:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-20 11:24:56 | INFO | fairseq.trainer | begin training epoch 34
2023-08-20 11:24:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-20 11:25:54 | INFO | train_inner | epoch 034:     82 / 1474 loss=1.893, trans_loss=4.747, nll_loss=1.938, w2v_ctc_loss=0.648, task_loss=1.333, contrastive_loss=0.053, total=4114.8, n_correct=2846.81, ppl=3.83, accuracy=69.185, wps=7134.9, ups=0.87, wpb=8229.6, bsz=299.3, num_updates=48700, lr=6.40841e-05, gnorm=0.585, clip=0, loss_scale=4, train_wall=61, gb_free=16.4, wall=35871
2023-08-20 11:26:56 | INFO | train_inner | epoch 034:    182 / 1474 loss=1.895, trans_loss=4.742, nll_loss=1.932, w2v_ctc_loss=0.655, task_loss=1.387, contrastive_loss=0.055, total=4068.7, n_correct=2815.96, ppl=3.82, accuracy=69.21, wps=13185.9, ups=1.62, wpb=8137.4, bsz=295.3, num_updates=48800, lr=6.40184e-05, gnorm=0.654, clip=0, loss_scale=4, train_wall=61, gb_free=16.5, wall=35933
2023-08-20 11:27:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-20 11:27:59 | INFO | train_inner | epoch 034:    283 / 1474 loss=1.919, trans_loss=4.761, nll_loss=1.957, w2v_ctc_loss=0.648, task_loss=1.227, contrastive_loss=0.26, total=4251.07, n_correct=2928.88, ppl=3.88, accuracy=68.897, wps=13472.7, ups=1.58, wpb=8502.1, bsz=331.9, num_updates=48900, lr=6.39529e-05, gnorm=0.805, clip=1, loss_scale=2, train_wall=62, gb_free=12.1, wall=35996
2023-08-20 11:29:01 | INFO | train_inner | epoch 034:    383 / 1474 loss=1.898, trans_loss=4.742, nll_loss=1.932, w2v_ctc_loss=0.644, task_loss=1.26, contrastive_loss=0.152, total=4164.91, n_correct=2882.1, ppl=3.82, accuracy=69.2, wps=13367.6, ups=1.6, wpb=8329.8, bsz=318, num_updates=49000, lr=6.38877e-05, gnorm=0.785, clip=1, loss_scale=2, train_wall=62, gb_free=15.4, wall=36058
2023-08-20 11:30:03 | INFO | train_inner | epoch 034:    483 / 1474 loss=1.901, trans_loss=4.753, nll_loss=1.945, w2v_ctc_loss=0.66, task_loss=1.469, contrastive_loss=0.045, total=4057.95, n_correct=2798.6, ppl=3.85, accuracy=68.966, wps=13203.8, ups=1.63, wpb=8115.9, bsz=282.2, num_updates=49100, lr=6.38226e-05, gnorm=0.575, clip=0, loss_scale=2, train_wall=61, gb_free=16.6, wall=36119
2023-08-20 11:31:04 | INFO | train_inner | epoch 034:    583 / 1474 loss=1.888, trans_loss=4.742, nll_loss=1.932, w2v_ctc_loss=0.642, task_loss=1.328, contrastive_loss=0.049, total=4146.28, n_correct=2872.26, ppl=3.82, accuracy=69.273, wps=13470.8, ups=1.62, wpb=8292.6, bsz=304, num_updates=49200, lr=6.37577e-05, gnorm=0.603, clip=0, loss_scale=2, train_wall=61, gb_free=16.7, wall=36181
2023-08-20 11:32:06 | INFO | train_inner | epoch 034:    683 / 1474 loss=1.892, trans_loss=4.749, nll_loss=1.941, w2v_ctc_loss=0.645, task_loss=1.396, contrastive_loss=0.045, total=4097.4, n_correct=2831.06, ppl=3.84, accuracy=69.094, wps=13213.8, ups=1.61, wpb=8194.8, bsz=294, num_updates=49300, lr=6.3693e-05, gnorm=0.616, clip=0, loss_scale=2, train_wall=61, gb_free=16.5, wall=36243
2023-08-20 11:33:08 | INFO | train_inner | epoch 034:    783 / 1474 loss=1.905, trans_loss=4.768, nll_loss=1.966, w2v_ctc_loss=0.644, task_loss=1.377, contrastive_loss=0.112, total=4078.95, n_correct=2806.68, ppl=3.91, accuracy=68.809, wps=13171.1, ups=1.61, wpb=8157.9, bsz=296.1, num_updates=49400, lr=6.36285e-05, gnorm=0.601, clip=0, loss_scale=2, train_wall=61, gb_free=15.8, wall=36305
2023-08-20 11:34:10 | INFO | train_inner | epoch 034:    883 / 1474 loss=1.898, trans_loss=4.756, nll_loss=1.951, w2v_ctc_loss=0.646, task_loss=1.392, contrastive_loss=0.072, total=4104.4, n_correct=2833.53, ppl=3.87, accuracy=69.036, wps=13179.3, ups=1.61, wpb=8208.8, bsz=297.8, num_updates=49500, lr=6.35642e-05, gnorm=0.555, clip=0, loss_scale=2, train_wall=62, gb_free=16, wall=36367
2023-08-20 11:35:12 | INFO | train_inner | epoch 034:    983 / 1474 loss=1.9, trans_loss=4.756, nll_loss=1.951, w2v_ctc_loss=0.656, task_loss=1.295, contrastive_loss=0.069, total=4180.16, n_correct=2883.33, ppl=3.87, accuracy=68.977, wps=13577, ups=1.62, wpb=8360.3, bsz=314, num_updates=49600, lr=6.35001e-05, gnorm=0.597, clip=0, loss_scale=2, train_wall=61, gb_free=17.1, wall=36429
2023-08-20 11:36:14 | INFO | train_inner | epoch 034:   1083 / 1474 loss=1.9, trans_loss=4.758, nll_loss=1.952, w2v_ctc_loss=0.659, task_loss=1.285, contrastive_loss=0.05, total=4152.49, n_correct=2865.6, ppl=3.87, accuracy=69.009, wps=13476.7, ups=1.62, wpb=8305, bsz=308.8, num_updates=49700, lr=6.34361e-05, gnorm=0.636, clip=0, loss_scale=2, train_wall=61, gb_free=15.7, wall=36490
2023-08-20 11:37:16 | INFO | train_inner | epoch 034:   1183 / 1474 loss=1.902, trans_loss=4.759, nll_loss=1.954, w2v_ctc_loss=0.655, task_loss=1.379, contrastive_loss=0.062, total=4090.71, n_correct=2820.47, ppl=3.87, accuracy=68.948, wps=13187.6, ups=1.61, wpb=8181.4, bsz=295, num_updates=49800, lr=6.33724e-05, gnorm=0.555, clip=0, loss_scale=2, train_wall=61, gb_free=13.3, wall=36552
2023-08-20 11:38:18 | INFO | train_inner | epoch 034:   1283 / 1474 loss=1.893, trans_loss=4.75, nll_loss=1.942, w2v_ctc_loss=0.649, task_loss=1.318, contrastive_loss=0.047, total=4165.26, n_correct=2880.06, ppl=3.84, accuracy=69.145, wps=13479.5, ups=1.62, wpb=8330.5, bsz=304.1, num_updates=49900, lr=6.33089e-05, gnorm=0.576, clip=0, loss_scale=2, train_wall=61, gb_free=17.4, wall=36614
2023-08-20 11:39:20 | INFO | train_inner | epoch 034:   1383 / 1474 loss=1.904, trans_loss=4.759, nll_loss=1.954, w2v_ctc_loss=0.653, task_loss=1.293, contrastive_loss=0.11, total=4184.03, n_correct=2885.37, ppl=3.87, accuracy=68.962, wps=13360.6, ups=1.6, wpb=8368.1, bsz=318.1, num_updates=50000, lr=6.32456e-05, gnorm=0.665, clip=0, loss_scale=2, train_wall=62, gb_free=17.2, wall=36677
2023-08-20 11:39:20 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2023-08-20 11:39:20 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-20 11:39:53 | INFO | dev_st | epoch 034 | valid on 'dev_st' subset | loss 3.96 | trans_loss 5.176 | nll_loss 2.431 | w2v_ctc_loss 1.372 | task_loss 4.545 | contrastive_loss 0.251 | total 4003.4 | n_correct 2666.6 | ppl 5.39 | accuracy 66.608 | uer 17.578 | wer 19.552 | raw_wer 19.552 | bleu 22.16 | wps 1640 | wpb 4003.4 | bsz 141.8 | num_updates 50000 | best_bleu 22.46
2023-08-20 11:39:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 50000 updates
2023-08-20 11:39:53 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_34_50000.pt
2023-08-20 11:39:55 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_34_50000.pt
2023-08-20 11:40:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v2_merge_large_0819_AT_sentence_mixup0105_mt0102_changeid_alpha1.5_mt0.5/checkpoint_34_50000.pt (epoch 34 @ 50000 updates, score 22.16) (writing took 8.137531137035694 seconds)
2023-08-20 11:40:02 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2023-08-20 11:40:02 | INFO | train | epoch 034 | loss 1.899 | trans_loss 4.753 | nll_loss 1.946 | w2v_ctc_loss 0.65 | task_loss 1.337 | contrastive_loss 0.087 | total 4133.24 | n_correct 2854.06 | ppl 3.85 | accuracy 69.051 | wps 12599.8 | ups 1.52 | wpb 8266.5 | bsz 304.3 | num_updates 50000 | lr 6.32456e-05 | gnorm 0.63 | clip 0.1 | loss_scale 2 | train_wall 846 | gb_free 17.2 | wall 36719
2023-08-20 11:40:02 | INFO | fairseq_cli.train | done training in 36672.8 seconds
Exception in thread Thread-7:
Traceback (most recent call last):
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-6:
Traceback (most recent call last):
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    res = self._recv_bytes()
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    self.run()
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    buf = self._recv_bytes(maxlength)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    data = self._queue.get(True, queue_wait_duration)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    self.run()
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    raise EOFError
EOFError
    res = self._recv_bytes()
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
    res = self._recv_bytes()
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1472 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
