2023-08-01 22:06:50 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:13778
2023-08-01 22:06:50 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:13778
2023-08-01 22:06:50 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:13778
2023-08-01 22:06:50 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:13778
2023-08-01 22:06:51 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:13778
2023-08-01 22:06:51 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:13778
2023-08-01 22:06:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-01 22:06:51 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:13778
2023-08-01 22:06:51 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:13778
2023-08-01 22:06:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-01 22:06:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-01 22:06:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-01 22:06:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-01 22:06:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-01 22:06:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-01 22:06:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-01 22:06:51 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-01 22:06:51 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-01 22:06:51 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-01 22:06:51 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-01 22:06:51 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-01 22:06:51 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-01 22:06:51 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-01 22:06:51 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-01 22:06:51 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-01 22:06:51 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-01 22:06:51 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-01 22:06:51 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-01 22:06:51 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-01 22:06:51 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-01 22:06:51 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-01 22:06:51 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-01 22:06:56 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13778', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=False, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=False, min_loss_scale=0.0001, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=False, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=False, min_loss_scale=0.0001, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=False, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=False, min_loss_scale=0.0001, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-01 22:06:56 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-08-01 22:06:56 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-08-01 22:06:56 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-01 22:06:56 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-08-01 22:06:56 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-08-01 22:07:00 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-01 22:07:00 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-01 22:07:00 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-01 22:07:02 | INFO | root | load pretrained hubert
2023-08-01 22:07:04 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-08-01 22:07:05 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-08-01 22:07:06 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-08-01 22:07:06 | INFO | root | share the sematic adapter and textual encoder
2023-08-01 22:07:06 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-01 22:07:06 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-08-01 22:07:06 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-01 22:07:06 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-08-01 22:07:06 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-08-01 22:07:06 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-01 22:07:06 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-01 22:07:06 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-01 22:07:06 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-01 22:07:06 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-01 22:07:06 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-01 22:07:09 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-01 22:07:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-01 22:07:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-01 22:07:09 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-01 22:07:09 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-01 22:07:09 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-01 22:07:09 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-01 22:07:09 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-01 22:07:09 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-01 22:07:09 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-01 22:07:09 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-01 22:07:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-01 22:07:09 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-01 22:07:09 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-01 22:07:09 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_last.pt
2023-08-01 22:07:09 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_last.pt
2023-08-01 22:07:09 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-01 22:07:09 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-01 22:07:09 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-01 22:07:09 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-01 22:07:11 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-01 22:07:13 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-01 22:08:00 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-01 22:08:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-01 22:08:00 | INFO | fairseq.trainer | begin training epoch 1
2023-08-01 22:08:00 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-01 22:09:12 | INFO | train_inner | epoch 001:    100 / 1474 loss=29.508, trans_loss=5.979, nll_loss=5.446, w2v_ctc_loss=34.667, task_loss=0, contrastive_loss=4.988, total=4207.04, n_correct=208.04, ppl=43.59, accuracy=4.945, wps=14790.9, ups=1.77, wpb=8344, bsz=314.3, num_updates=100, lr=4.098e-06, gnorm=1.359, clip=0, loss_scale=128, train_wall=64, gb_free=19.5, wall=123
2023-08-01 22:10:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-01 22:10:08 | INFO | train_inner | epoch 001:    201 / 1474 loss=26.156, trans_loss=5.817, nll_loss=5.248, w2v_ctc_loss=29.781, task_loss=0, contrastive_loss=4.923, total=4121.36, n_correct=219.97, ppl=37.99, accuracy=5.337, wps=14535.5, ups=1.78, wpb=8184.2, bsz=307.6, num_updates=200, lr=8.096e-06, gnorm=5.618, clip=13, loss_scale=64, train_wall=56, gb_free=19.3, wall=179
2023-08-01 22:11:04 | INFO | train_inner | epoch 001:    301 / 1474 loss=15.476, trans_loss=5.86, nll_loss=5.302, w2v_ctc_loss=13.411, task_loss=0, contrastive_loss=4.805, total=4079.62, n_correct=205, ppl=39.46, accuracy=5.025, wps=14578.4, ups=1.8, wpb=8107, bsz=292.2, num_updates=300, lr=1.2094e-05, gnorm=7.045, clip=8, loss_scale=64, train_wall=55, gb_free=20, wall=234
2023-08-01 22:12:00 | INFO | train_inner | epoch 001:    401 / 1474 loss=13.59, trans_loss=5.909, nll_loss=5.37, w2v_ctc_loss=10.411, task_loss=0, contrastive_loss=4.853, total=4174.14, n_correct=196.04, ppl=41.36, accuracy=4.697, wps=14720.6, ups=1.78, wpb=8289.4, bsz=307, num_updates=400, lr=1.6092e-05, gnorm=4.476, clip=0, loss_scale=64, train_wall=56, gb_free=19, wall=291
2023-08-01 22:12:56 | INFO | train_inner | epoch 001:    501 / 1474 loss=12.936, trans_loss=5.864, nll_loss=5.327, w2v_ctc_loss=9.468, task_loss=0, contrastive_loss=4.838, total=4176.18, n_correct=193.97, ppl=40.14, accuracy=4.645, wps=14966.8, ups=1.8, wpb=8303.5, bsz=318.2, num_updates=500, lr=2.009e-05, gnorm=2.129, clip=0, loss_scale=64, train_wall=55, gb_free=19.2, wall=346
2023-08-01 22:13:51 | INFO | train_inner | epoch 001:    601 / 1474 loss=12.558, trans_loss=5.9, nll_loss=5.37, w2v_ctc_loss=8.919, task_loss=0, contrastive_loss=4.913, total=4147.79, n_correct=193.87, ppl=41.36, accuracy=4.674, wps=14789.1, ups=1.8, wpb=8223.8, bsz=322.8, num_updates=600, lr=2.4088e-05, gnorm=1.085, clip=0, loss_scale=64, train_wall=55, gb_free=19, wall=402
2023-08-01 22:14:46 | INFO | train_inner | epoch 001:    701 / 1474 loss=12.27, trans_loss=5.874, nll_loss=5.345, w2v_ctc_loss=8.716, task_loss=0, contrastive_loss=4.534, total=4152.1, n_correct=216.33, ppl=40.64, accuracy=5.21, wps=14921.4, ups=1.81, wpb=8243.4, bsz=304.2, num_updates=700, lr=2.8086e-05, gnorm=0.872, clip=0, loss_scale=64, train_wall=55, gb_free=19.6, wall=457
2023-08-01 22:15:42 | INFO | train_inner | epoch 001:    801 / 1474 loss=11.852, trans_loss=5.785, nll_loss=5.242, w2v_ctc_loss=8.38, task_loss=0, contrastive_loss=4.383, total=4123.83, n_correct=260.28, ppl=37.85, accuracy=6.312, wps=14817.3, ups=1.81, wpb=8182.2, bsz=309.4, num_updates=800, lr=3.2084e-05, gnorm=1.245, clip=0, loss_scale=64, train_wall=55, gb_free=19.2, wall=512
2023-08-01 22:16:37 | INFO | train_inner | epoch 001:    901 / 1474 loss=11.445, trans_loss=5.741, nll_loss=5.202, w2v_ctc_loss=8.094, task_loss=0, contrastive_loss=4.012, total=4163.61, n_correct=276.59, ppl=36.8, accuracy=6.643, wps=15000, ups=1.81, wpb=8270.3, bsz=304.7, num_updates=900, lr=3.6082e-05, gnorm=2.038, clip=0, loss_scale=64, train_wall=55, gb_free=18.9, wall=567
2023-08-01 22:17:32 | INFO | train_inner | epoch 001:   1001 / 1474 loss=11.023, trans_loss=5.692, nll_loss=5.151, w2v_ctc_loss=7.761, task_loss=0, contrastive_loss=3.784, total=4135.34, n_correct=304.18, ppl=35.54, accuracy=7.356, wps=14806.1, ups=1.8, wpb=8217.9, bsz=304.5, num_updates=1000, lr=4.008e-05, gnorm=2.271, clip=0, loss_scale=64, train_wall=55, gb_free=19.1, wall=623
2023-08-01 22:18:28 | INFO | train_inner | epoch 001:   1101 / 1474 loss=10.611, trans_loss=5.67, nll_loss=5.129, w2v_ctc_loss=7.454, task_loss=0, contrastive_loss=3.441, total=4147.38, n_correct=325.03, ppl=35, accuracy=7.837, wps=14907.6, ups=1.81, wpb=8219.6, bsz=303.2, num_updates=1100, lr=4.4078e-05, gnorm=2.484, clip=0, loss_scale=64, train_wall=55, gb_free=18.9, wall=678
2023-08-01 22:19:23 | INFO | train_inner | epoch 001:   1201 / 1474 loss=10.241, trans_loss=5.643, nll_loss=5.105, w2v_ctc_loss=7.17, task_loss=0, contrastive_loss=3.12, total=4139.9, n_correct=331.89, ppl=34.42, accuracy=8.017, wps=14945.5, ups=1.82, wpb=8226.6, bsz=293.4, num_updates=1200, lr=4.8076e-05, gnorm=2.561, clip=0, loss_scale=64, train_wall=55, gb_free=19, wall=733
2023-08-01 22:20:17 | INFO | train_inner | epoch 001:   1301 / 1474 loss=9.907, trans_loss=5.645, nll_loss=5.109, w2v_ctc_loss=6.858, task_loss=0, contrastive_loss=2.845, total=4046.58, n_correct=329.77, ppl=34.52, accuracy=8.149, wps=14671.3, ups=1.83, wpb=8035, bsz=292.8, num_updates=1300, lr=5.2074e-05, gnorm=2.753, clip=0, loss_scale=64, train_wall=54, gb_free=19.8, wall=788
2023-08-01 22:21:12 | INFO | train_inner | epoch 001:   1401 / 1474 loss=9.576, trans_loss=5.628, nll_loss=5.092, w2v_ctc_loss=6.537, task_loss=0, contrastive_loss=2.936, total=4133.18, n_correct=339.96, ppl=34.11, accuracy=8.225, wps=14948.7, ups=1.82, wpb=8216.8, bsz=303.3, num_updates=1400, lr=5.6072e-05, gnorm=2.436, clip=0, loss_scale=64, train_wall=54, gb_free=20, wall=843
2023-08-01 22:21:52 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-01 22:22:31 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 9.846 | trans_loss 10.928 | nll_loss 9.772 | w2v_ctc_loss 5.68 | task_loss 0 | contrastive_loss 2.261 | total 4003.4 | n_correct 379.7 | ppl 874.31 | accuracy 9.484 | uer 71.194 | wer 69.043 | raw_wer 69.043 | bleu 0.02 | wps 1214.5 | wpb 4003.4 | bsz 141.8 | num_updates 1473
2023-08-01 22:22:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1473 updates
2023-08-01 22:22:31 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-01 22:22:35 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-01 22:22:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt (epoch 1 @ 1473 updates, score 0.02) (writing took 7.257832119241357 seconds)
2023-08-01 22:22:38 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-08-01 22:22:38 | INFO | train | epoch 001 | loss 13.863 | trans_loss 5.779 | nll_loss 5.238 | w2v_ctc_loss 11.714 | task_loss 0 | contrastive_loss 4.106 | total 4138.36 | n_correct 261.34 | ppl 37.74 | accuracy 6.315 | wps 14024.1 | ups 1.71 | wpb 8216.6 | bsz 305.6 | num_updates 1473 | lr 5.89905e-05 | gnorm 2.724 | clip 1.4 | loss_scale 64 | train_wall 817 | gb_free 19.2 | wall 929
2023-08-01 22:22:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-01 22:22:39 | INFO | fairseq.trainer | begin training epoch 2
2023-08-01 22:22:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-01 22:23:02 | INFO | train_inner | epoch 002:     27 / 1474 loss=9.282, trans_loss=5.627, nll_loss=5.08, w2v_ctc_loss=6.231, task_loss=0, contrastive_loss=2.691, total=4162.95, n_correct=343.54, ppl=33.83, accuracy=8.252, wps=7541.1, ups=0.91, wpb=8253.9, bsz=313.9, num_updates=1500, lr=6.007e-05, gnorm=2.415, clip=0, loss_scale=64, train_wall=55, gb_free=19.7, wall=952
2023-08-01 22:23:57 | INFO | train_inner | epoch 002:    127 / 1474 loss=9.007, trans_loss=5.617, nll_loss=5.069, w2v_ctc_loss=6.033, task_loss=0, contrastive_loss=2.373, total=4155.98, n_correct=348.92, ppl=33.57, accuracy=8.396, wps=14932.1, ups=1.81, wpb=8238.9, bsz=301.1, num_updates=1600, lr=6.4068e-05, gnorm=2.507, clip=0, loss_scale=64, train_wall=55, gb_free=18.9, wall=1008
2023-08-01 22:24:52 | INFO | train_inner | epoch 002:    227 / 1474 loss=8.756, trans_loss=5.598, nll_loss=5.046, w2v_ctc_loss=5.724, task_loss=0, contrastive_loss=2.404, total=4179.21, n_correct=351.73, ppl=33.03, accuracy=8.416, wps=15145.8, ups=1.82, wpb=8305.4, bsz=325.9, num_updates=1700, lr=6.8066e-05, gnorm=2.21, clip=0, loss_scale=64, train_wall=54, gb_free=19, wall=1062
2023-08-01 22:25:47 | INFO | train_inner | epoch 002:    327 / 1474 loss=8.499, trans_loss=5.604, nll_loss=5.048, w2v_ctc_loss=5.568, task_loss=0, contrastive_loss=1.968, total=4146.1, n_correct=351.9, ppl=33.08, accuracy=8.487, wps=14873.1, ups=1.81, wpb=8228, bsz=298.5, num_updates=1800, lr=7.2064e-05, gnorm=2.099, clip=0, loss_scale=64, train_wall=55, gb_free=18.8, wall=1118
2023-08-01 22:26:42 | INFO | train_inner | epoch 002:    427 / 1474 loss=8.26, trans_loss=5.595, nll_loss=5.041, w2v_ctc_loss=5.404, task_loss=0, contrastive_loss=1.685, total=4037.99, n_correct=336.13, ppl=32.93, accuracy=8.324, wps=14707.3, ups=1.83, wpb=8031.2, bsz=276.9, num_updates=1900, lr=7.6062e-05, gnorm=2.037, clip=0, loss_scale=64, train_wall=54, gb_free=19, wall=1172
2023-08-01 22:27:37 | INFO | train_inner | epoch 002:    527 / 1474 loss=8.125, trans_loss=5.597, nll_loss=5.034, w2v_ctc_loss=5.173, task_loss=0, contrastive_loss=1.823, total=4176.97, n_correct=354.67, ppl=32.77, accuracy=8.491, wps=15094.4, ups=1.82, wpb=8286.5, bsz=312.4, num_updates=2000, lr=8.006e-05, gnorm=1.879, clip=0, loss_scale=64, train_wall=54, gb_free=19.7, wall=1227
2023-08-01 22:27:37 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-01 22:28:16 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 9.25 | trans_loss 10.848 | nll_loss 9.625 | w2v_ctc_loss 4.54 | task_loss 0 | contrastive_loss 1.477 | total 4003.4 | n_correct 414 | ppl 789.7 | accuracy 10.341 | uer 60.624 | wer 58.92 | raw_wer 58.92 | bleu 0.04 | wps 1165.4 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_bleu 0.04
2023-08-01 22:28:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-08-01 22:28:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_2_2000.pt
2023-08-01 22:28:20 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_2_2000.pt
2023-08-01 22:28:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.04) (writing took 41.24676704593003 seconds)
2023-08-01 22:29:52 | INFO | train_inner | epoch 002:    627 / 1474 loss=7.913, trans_loss=5.587, nll_loss=5.02, w2v_ctc_loss=5.007, task_loss=0, contrastive_loss=1.506, total=4126.49, n_correct=351.78, ppl=32.44, accuracy=8.525, wps=6044.1, ups=0.74, wpb=8188.4, bsz=297, num_updates=2100, lr=8.4058e-05, gnorm=1.65, clip=0, loss_scale=64, train_wall=54, gb_free=19.2, wall=1363
2023-08-01 22:30:47 | INFO | train_inner | epoch 002:    727 / 1474 loss=7.825, trans_loss=5.581, nll_loss=5.018, w2v_ctc_loss=4.888, task_loss=0, contrastive_loss=1.655, total=4149.06, n_correct=353.78, ppl=32.41, accuracy=8.527, wps=15058.1, ups=1.83, wpb=8237.3, bsz=310.2, num_updates=2200, lr=8.8056e-05, gnorm=1.64, clip=0, loss_scale=64, train_wall=54, gb_free=19.3, wall=1417
2023-08-01 22:31:42 | INFO | train_inner | epoch 002:    827 / 1474 loss=7.695, trans_loss=5.569, nll_loss=5.001, w2v_ctc_loss=4.783, task_loss=0, contrastive_loss=1.57, total=4175.4, n_correct=358.16, ppl=32.03, accuracy=8.578, wps=15003.5, ups=1.81, wpb=8296.5, bsz=307.2, num_updates=2300, lr=9.2054e-05, gnorm=1.5, clip=0, loss_scale=128, train_wall=55, gb_free=19.9, wall=1473
2023-08-01 22:32:37 | INFO | train_inner | epoch 002:    927 / 1474 loss=7.555, trans_loss=5.561, nll_loss=4.991, w2v_ctc_loss=4.641, task_loss=0, contrastive_loss=1.542, total=4104.2, n_correct=355.82, ppl=31.81, accuracy=8.67, wps=14870.2, ups=1.82, wpb=8148.9, bsz=297.3, num_updates=2400, lr=9.6052e-05, gnorm=1.409, clip=0, loss_scale=128, train_wall=54, gb_free=19, wall=1528
2023-08-01 22:33:32 | INFO | train_inner | epoch 002:   1027 / 1474 loss=7.427, trans_loss=5.557, nll_loss=4.984, w2v_ctc_loss=4.543, task_loss=0, contrastive_loss=1.306, total=4102.5, n_correct=356.66, ppl=31.65, accuracy=8.694, wps=14897.7, ups=1.83, wpb=8148.7, bsz=304.2, num_updates=2500, lr=0.00010005, gnorm=1.284, clip=0, loss_scale=128, train_wall=54, gb_free=19.3, wall=1582
2023-08-01 22:34:27 | INFO | train_inner | epoch 002:   1127 / 1474 loss=7.383, trans_loss=5.552, nll_loss=4.977, w2v_ctc_loss=4.418, task_loss=0, contrastive_loss=1.626, total=4187.61, n_correct=360.87, ppl=31.49, accuracy=8.618, wps=15099, ups=1.82, wpb=8308.1, bsz=324.8, num_updates=2600, lr=0.000104048, gnorm=1.252, clip=0, loss_scale=128, train_wall=55, gb_free=19.6, wall=1637
2023-08-01 22:35:22 | INFO | train_inner | epoch 002:   1227 / 1474 loss=7.299, trans_loss=5.539, nll_loss=4.96, w2v_ctc_loss=4.36, task_loss=0, contrastive_loss=1.501, total=4221.06, n_correct=373.01, ppl=31.12, accuracy=8.837, wps=15134.3, ups=1.81, wpb=8375, bsz=328.5, num_updates=2700, lr=0.000108046, gnorm=1.166, clip=0, loss_scale=128, train_wall=55, gb_free=19.5, wall=1693
2023-08-01 22:36:17 | INFO | train_inner | epoch 002:   1327 / 1474 loss=7.138, trans_loss=5.514, nll_loss=4.932, w2v_ctc_loss=4.296, task_loss=0, contrastive_loss=1.059, total=4157.86, n_correct=373.66, ppl=30.53, accuracy=8.987, wps=15111.8, ups=1.83, wpb=8267.7, bsz=307.1, num_updates=2800, lr=0.000112044, gnorm=1.085, clip=0, loss_scale=128, train_wall=54, gb_free=19.6, wall=1747
2023-08-01 22:37:12 | INFO | train_inner | epoch 002:   1427 / 1474 loss=7.09, trans_loss=5.516, nll_loss=4.935, w2v_ctc_loss=4.24, task_loss=0, contrastive_loss=1.199, total=4054.34, n_correct=359.7, ppl=30.59, accuracy=8.872, wps=14581.3, ups=1.81, wpb=8052.6, bsz=292.5, num_updates=2900, lr=0.000116042, gnorm=1.048, clip=0, loss_scale=128, train_wall=55, gb_free=19.4, wall=1803
2023-08-01 22:37:38 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-01 22:38:16 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 8.634 | trans_loss 10.562 | nll_loss 9.274 | w2v_ctc_loss 3.652 | task_loss 0 | contrastive_loss 0.833 | total 4003.4 | n_correct 451.3 | ppl 618.96 | accuracy 11.273 | uer 50.787 | wer 49.834 | raw_wer 49.834 | bleu 0.1 | wps 1192.7 | wpb 4003.4 | bsz 141.8 | num_updates 2947 | best_bleu 0.1
2023-08-01 22:38:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2947 updates
2023-08-01 22:38:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-01 22:38:29 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-01 22:38:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt (epoch 2 @ 2947 updates, score 0.1) (writing took 24.179572861641645 seconds)
2023-08-01 22:38:41 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-08-01 22:38:41 | INFO | train | epoch 002 | loss 7.854 | trans_loss 5.57 | nll_loss 5.003 | w2v_ctc_loss 4.933 | task_loss 0 | contrastive_loss 1.662 | total 4138.65 | n_correct 356.262 | ppl 32.07 | accuracy 8.608 | wps 12588.2 | ups 1.53 | wpb 8217.2 | bsz 305.7 | num_updates 2947 | lr 0.000117921 | gnorm 1.621 | clip 0 | loss_scale 128 | train_wall 803 | gb_free 19.3 | wall 1891
2023-08-01 22:38:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-01 22:38:41 | INFO | fairseq.trainer | begin training epoch 3
2023-08-01 22:38:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-01 22:39:19 | INFO | train_inner | epoch 003:     53 / 1474 loss=6.976, trans_loss=5.502, nll_loss=4.915, w2v_ctc_loss=4.146, task_loss=0, contrastive_loss=1.04, total=4071.2, n_correct=371.43, ppl=30.17, accuracy=9.123, wps=6359.7, ups=0.79, wpb=8082.9, bsz=295, num_updates=3000, lr=0.00012004, gnorm=0.966, clip=0, loss_scale=128, train_wall=55, gb_free=19.1, wall=1930
2023-08-01 22:39:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-01 22:39:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-01 22:39:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-01 22:39:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-01 22:39:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-01 22:40:36 | INFO | train_inner | epoch 003:    158 / 1474 loss=5.716, trans_loss=4.203, nll_loss=3.291, w2v_ctc_loss=3.642, task_loss=0, contrastive_loss=0.948, total=4145.36, n_correct=1106.18, ppl=9.79, accuracy=26.685, wps=10703.5, ups=1.3, wpb=8232.1, bsz=304.9, num_updates=3100, lr=0.000124038, gnorm=2.807, clip=2, loss_scale=4, train_wall=76, gb_free=16.8, wall=2007
2023-08-01 22:41:50 | INFO | train_inner | epoch 003:    258 / 1474 loss=5.074, trans_loss=3.764, nll_loss=2.719, w2v_ctc_loss=3.281, task_loss=0, contrastive_loss=0.813, total=4161.13, n_correct=1447.01, ppl=6.59, accuracy=34.774, wps=11099.2, ups=1.34, wpb=8270.4, bsz=311.3, num_updates=3200, lr=0.000128036, gnorm=1.964, clip=0, loss_scale=4, train_wall=74, gb_free=17.3, wall=2081
2023-08-01 22:43:04 | INFO | train_inner | epoch 003:    358 / 1474 loss=4.89, trans_loss=3.667, nll_loss=2.59, w2v_ctc_loss=3.124, task_loss=0, contrastive_loss=0.878, total=4150.02, n_correct=1540.59, ppl=6.02, accuracy=37.122, wps=11198.8, ups=1.36, wpb=8234.9, bsz=307.8, num_updates=3300, lr=0.000132034, gnorm=1.811, clip=0, loss_scale=4, train_wall=73, gb_free=17.3, wall=2155
2023-08-01 22:44:19 | INFO | train_inner | epoch 003:    458 / 1474 loss=4.687, trans_loss=3.58, nll_loss=2.476, w2v_ctc_loss=2.986, task_loss=0, contrastive_loss=0.653, total=4209.57, n_correct=1653.26, ppl=5.56, accuracy=39.274, wps=11183.9, ups=1.34, wpb=8356.4, bsz=317.9, num_updates=3400, lr=0.000136032, gnorm=1.622, clip=0, loss_scale=4, train_wall=74, gb_free=16.3, wall=2229
2023-08-01 22:45:33 | INFO | train_inner | epoch 003:    558 / 1474 loss=4.546, trans_loss=3.526, nll_loss=2.407, w2v_ctc_loss=2.872, task_loss=0, contrastive_loss=0.621, total=4088.48, n_correct=1660.75, ppl=5.3, accuracy=40.62, wps=10927.1, ups=1.35, wpb=8124, bsz=293.1, num_updates=3500, lr=0.00014003, gnorm=1.616, clip=0, loss_scale=4, train_wall=74, gb_free=17.9, wall=2304
2023-08-01 22:46:48 | INFO | train_inner | epoch 003:    658 / 1474 loss=4.469, trans_loss=3.473, nll_loss=2.331, w2v_ctc_loss=2.769, task_loss=0, contrastive_loss=0.795, total=4221.58, n_correct=1779.65, ppl=5.03, accuracy=42.156, wps=11168.7, ups=1.33, wpb=8366.3, bsz=321.3, num_updates=3600, lr=0.000144028, gnorm=1.519, clip=0, loss_scale=4, train_wall=74, gb_free=16.6, wall=2379
2023-08-01 22:48:02 | INFO | train_inner | epoch 003:    758 / 1474 loss=4.362, trans_loss=3.43, nll_loss=2.28, w2v_ctc_loss=2.732, task_loss=0, contrastive_loss=0.447, total=4167.41, n_correct=1792.2, ppl=4.86, accuracy=43.005, wps=11245.4, ups=1.36, wpb=8280.2, bsz=315, num_updates=3700, lr=0.000148026, gnorm=1.596, clip=0, loss_scale=4, train_wall=73, gb_free=16.6, wall=2452
2023-08-01 22:49:16 | INFO | train_inner | epoch 003:    858 / 1474 loss=4.263, trans_loss=3.397, nll_loss=2.234, w2v_ctc_loss=2.654, task_loss=0, contrastive_loss=0.39, total=4165.53, n_correct=1827.02, ppl=4.71, accuracy=43.86, wps=11145.2, ups=1.35, wpb=8272.3, bsz=304.1, num_updates=3800, lr=0.000152024, gnorm=1.409, clip=0, loss_scale=4, train_wall=74, gb_free=17.3, wall=2526
2023-08-01 22:50:30 | INFO | train_inner | epoch 003:    958 / 1474 loss=4.223, trans_loss=3.363, nll_loss=2.187, w2v_ctc_loss=2.618, task_loss=0, contrastive_loss=0.437, total=4162.3, n_correct=1874.21, ppl=4.55, accuracy=45.028, wps=11127, ups=1.35, wpb=8254.7, bsz=312.8, num_updates=3900, lr=0.000156022, gnorm=1.387, clip=0, loss_scale=4, train_wall=74, gb_free=17, wall=2601
2023-08-01 22:51:44 | INFO | train_inner | epoch 003:   1058 / 1474 loss=4.172, trans_loss=3.34, nll_loss=2.161, w2v_ctc_loss=2.602, task_loss=0, contrastive_loss=0.376, total=4069.95, n_correct=1849.29, ppl=4.47, accuracy=45.438, wps=10951.6, ups=1.35, wpb=8083.8, bsz=295.8, num_updates=4000, lr=0.00016002, gnorm=1.39, clip=0, loss_scale=4, train_wall=73, gb_free=16.5, wall=2674
2023-08-01 22:51:44 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-01 22:52:17 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.15 | trans_loss 6.406 | nll_loss 3.934 | w2v_ctc_loss 2.032 | task_loss 0 | contrastive_loss 0.371 | total 4003.4 | n_correct 1970.1 | ppl 15.28 | accuracy 49.211 | uer 29.212 | wer 30.215 | raw_wer 30.215 | bleu 11.2 | wps 1455.2 | wpb 4003.4 | bsz 141.8 | num_updates 4000 | best_bleu 11.2
2023-08-01 22:52:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-08-01 22:52:17 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_3_4000.pt
2023-08-01 22:52:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_3_4000.pt
2023-08-01 22:52:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 11.2) (writing took 41.818287106230855 seconds)
2023-08-01 22:54:12 | INFO | train_inner | epoch 003:   1158 / 1474 loss=4.11, trans_loss=3.319, nll_loss=2.131, w2v_ctc_loss=2.534, task_loss=0, contrastive_loss=0.354, total=4038.49, n_correct=1854.75, ppl=4.38, accuracy=45.927, wps=5410.3, ups=0.67, wpb=8016.3, bsz=288.3, num_updates=4100, lr=0.000164018, gnorm=1.384, clip=0, loss_scale=4, train_wall=73, gb_free=16.6, wall=2823
2023-08-01 22:55:25 | INFO | train_inner | epoch 003:   1258 / 1474 loss=4.042, trans_loss=3.287, nll_loss=2.09, w2v_ctc_loss=2.477, task_loss=0, contrastive_loss=0.326, total=4064.31, n_correct=1902.43, ppl=4.26, accuracy=46.808, wps=11030, ups=1.37, wpb=8072.4, bsz=289.2, num_updates=4200, lr=0.000168016, gnorm=1.298, clip=0, loss_scale=4, train_wall=73, gb_free=17.5, wall=2896
2023-08-01 22:56:40 | INFO | train_inner | epoch 003:   1358 / 1474 loss=4.023, trans_loss=3.267, nll_loss=2.064, w2v_ctc_loss=2.43, task_loss=0, contrastive_loss=0.489, total=4134.58, n_correct=1956.26, ppl=4.18, accuracy=47.315, wps=11010, ups=1.34, wpb=8209.2, bsz=307.2, num_updates=4300, lr=0.000172014, gnorm=1.33, clip=0, loss_scale=4, train_wall=74, gb_free=18, wall=2970
2023-08-01 22:57:54 | INFO | train_inner | epoch 003:   1458 / 1474 loss=3.989, trans_loss=3.24, nll_loss=2.03, w2v_ctc_loss=2.4, task_loss=0, contrastive_loss=0.469, total=4209.94, n_correct=2023.79, ppl=4.08, accuracy=48.072, wps=11287.4, ups=1.35, wpb=8363.5, bsz=318.3, num_updates=4400, lr=0.000176012, gnorm=1.311, clip=0, loss_scale=4, train_wall=74, gb_free=17.3, wall=3044
2023-08-01 22:58:06 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-01 22:58:34 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 4.989 | trans_loss 6.237 | nll_loss 3.7 | w2v_ctc_loss 1.902 | task_loss 0 | contrastive_loss 0.35 | total 4003.4 | n_correct 2080.9 | ppl 13 | accuracy 51.978 | uer 28.654 | wer 29.253 | raw_wer 29.253 | bleu 13.72 | wps 1609.8 | wpb 4003.4 | bsz 141.8 | num_updates 4416 | best_bleu 13.72
2023-08-01 22:58:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4416 updates
2023-08-01 22:58:34 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-01 22:58:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-01 22:59:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt (epoch 3 @ 4416 updates, score 13.72) (writing took 25.665850576013327 seconds)
2023-08-01 22:59:00 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-08-01 22:59:00 | INFO | train | epoch 003 | loss 4.555 | trans_loss 3.56 | nll_loss 2.446 | w2v_ctc_loss 2.839 | task_loss 0 | contrastive_loss 0.592 | total 4140.13 | n_correct 1686.41 | ppl 5.45 | accuracy 40.733 | wps 9903.3 | ups 1.2 | wpb 8220.1 | bsz 305.9 | num_updates 4416 | lr 0.000176652 | gnorm 1.575 | clip 0.1 | loss_scale 4 | train_wall 1074 | gb_free 16.9 | wall 3111
2023-08-01 22:59:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-01 22:59:00 | INFO | fairseq.trainer | begin training epoch 4
2023-08-01 22:59:00 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-01 23:00:09 | INFO | train_inner | epoch 004:     84 / 1474 loss=3.866, trans_loss=3.21, nll_loss=1.987, w2v_ctc_loss=2.324, task_loss=0, contrastive_loss=0.253, total=4099.41, n_correct=2003.28, ppl=3.97, accuracy=48.868, wps=6004.2, ups=0.74, wpb=8137.6, bsz=293, num_updates=4500, lr=0.00018001, gnorm=1.252, clip=0, loss_scale=4, train_wall=72, gb_free=16.7, wall=3180
2023-08-01 23:01:22 | INFO | train_inner | epoch 004:    184 / 1474 loss=3.843, trans_loss=3.185, nll_loss=1.956, w2v_ctc_loss=2.298, task_loss=0, contrastive_loss=0.29, total=4175.15, n_correct=2071.84, ppl=3.88, accuracy=49.623, wps=11359.3, ups=1.37, wpb=8289.8, bsz=312.2, num_updates=4600, lr=0.000184008, gnorm=1.269, clip=0, loss_scale=4, train_wall=72, gb_free=16.9, wall=3253
2023-08-01 23:02:37 | INFO | train_inner | epoch 004:    284 / 1474 loss=3.871, trans_loss=3.183, nll_loss=1.955, w2v_ctc_loss=2.302, task_loss=0, contrastive_loss=0.482, total=4145.23, n_correct=2058.67, ppl=3.88, accuracy=49.664, wps=11106.6, ups=1.35, wpb=8237.2, bsz=308.6, num_updates=4700, lr=0.000188006, gnorm=1.275, clip=0, loss_scale=4, train_wall=74, gb_free=16.2, wall=3327
2023-08-01 23:03:50 | INFO | train_inner | epoch 004:    384 / 1474 loss=3.807, trans_loss=3.178, nll_loss=1.944, w2v_ctc_loss=2.268, task_loss=0, contrastive_loss=0.25, total=4127.66, n_correct=2055.34, ppl=3.85, accuracy=49.794, wps=11105, ups=1.36, wpb=8186.9, bsz=295.6, num_updates=4800, lr=0.000192004, gnorm=1.224, clip=0, loss_scale=4, train_wall=73, gb_free=17.6, wall=3401
2023-08-01 23:05:04 | INFO | train_inner | epoch 004:    484 / 1474 loss=3.871, trans_loss=3.167, nll_loss=1.933, w2v_ctc_loss=2.229, task_loss=0, contrastive_loss=0.847, total=4218.78, n_correct=2118.47, ppl=3.82, accuracy=50.215, wps=11309.5, ups=1.35, wpb=8373.6, bsz=331.8, num_updates=4900, lr=0.000196002, gnorm=1.292, clip=0, loss_scale=4, train_wall=74, gb_free=16.7, wall=3475
2023-08-01 23:06:18 | INFO | train_inner | epoch 004:    584 / 1474 loss=3.795, trans_loss=3.145, nll_loss=1.905, w2v_ctc_loss=2.249, task_loss=0, contrastive_loss=0.357, total=4217.52, n_correct=2142.49, ppl=3.75, accuracy=50.8, wps=11340.4, ups=1.35, wpb=8373.6, bsz=323.9, num_updates=5000, lr=0.0002, gnorm=1.235, clip=0, loss_scale=4, train_wall=73, gb_free=16.3, wall=3549
Exception in thread Thread-6:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-7:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
    raise EOFError
EOFError
    raise EOFError
EOFError
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq/trainer.py", line 863, in train_step
    raise e
  File "/mnt/zhangyh/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 660, in train_step
    dist.all_reduce(tmp_mt_weight)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1534, in all_reduce
    work = default_pg.allreduce([tensor], opts)
RuntimeError: Tensors must be CUDA and dense

/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 144 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 27, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
2023-08-02 09:44:03 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:12009
2023-08-02 09:44:03 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:12009
2023-08-02 09:44:04 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:12009
2023-08-02 09:44:04 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:12009
2023-08-02 09:44:04 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:12009
2023-08-02 09:44:04 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:12009
2023-08-02 09:44:04 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:12009
2023-08-02 09:44:04 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:12009
2023-08-02 09:44:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-02 09:44:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-02 09:44:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-02 09:44:05 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-02 09:44:05 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-02 09:44:05 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-02 09:44:05 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-02 09:44:05 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-02 09:44:05 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-02 09:44:05 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-02 09:44:05 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-02 09:44:05 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-02 09:44:05 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-02 09:44:05 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-02 09:44:05 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-02 09:44:05 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-02 09:44:05 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-02 09:44:05 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-02 09:44:05 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-02 09:44:05 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-02 09:44:05 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-02 09:44:05 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-02 09:44:05 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-02 09:44:05 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-02 09:44:09 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:12009', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=False, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=False, min_loss_scale=0.0001, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=False, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=False, min_loss_scale=0.0001, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=False, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=False, min_loss_scale=0.0001, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-02 09:44:09 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-08-02 09:44:09 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-08-02 09:44:09 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-02 09:44:09 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-08-02 09:44:09 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-08-02 09:44:14 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-02 09:44:14 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-02 09:44:14 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-02 09:44:15 | INFO | root | load pretrained hubert
2023-08-02 09:44:17 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-08-02 09:44:18 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-08-02 09:44:21 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-08-02 09:44:21 | INFO | root | share the sematic adapter and textual encoder
2023-08-02 09:44:21 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-02 09:44:21 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-08-02 09:44:21 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-02 09:44:21 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-08-02 09:44:21 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-08-02 09:44:21 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-02 09:44:21 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-02 09:44:21 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-02 09:44:21 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-02 09:44:21 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-02 09:44:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-02 09:44:30 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-02 09:44:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-02 09:44:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-02 09:44:30 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-02 09:44:30 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-02 09:44:30 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-02 09:44:30 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-02 09:44:30 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-02 09:44:30 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-02 09:44:30 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-02 09:44:30 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-02 09:44:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-02 09:44:30 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-02 09:44:30 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-02 09:44:30 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_last.pt
2023-08-02 09:44:33 | INFO | fairseq.trainer | load the task parameters
asr_weight tensor(1.)
mt_weight tensor(0.5000)
2023-08-02 09:44:34 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-02 09:44:34 | INFO | fairseq.trainer | Loaded checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_last.pt (epoch 4 @ 4416 updates)
2023-08-02 09:44:34 | INFO | fairseq.trainer | loading train data for epoch 4
2023-08-02 09:44:34 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-02 09:44:34 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-02 09:44:34 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-02 09:44:39 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-02 09:44:43 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-02 09:45:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 09:45:30 | INFO | fairseq.trainer | begin training epoch 4
2023-08-02 09:45:30 | INFO | fairseq_cli.train | Start iterating over samples
asr_weight tensor(1.)
mt_weight tensor(0.5000)
asr_weight tensor(1.)
mt_weight tensor(0.5000)
asr_weight tensor(1.)
mt_weight tensor(0.5000)
asr_weight tensor(1.)
mt_weight tensor(0.5000)
asr_weight tensor(1.)
mt_weight tensor(0.5000)
asr_weight tensor(1.)
mt_weight tensor(0.5000)
asr_weight tensor(1.)
mt_weight tensor(0.5000)
2023-08-02 09:46:50 | INFO | train_inner | epoch 004:     84 / 1474 loss=3.843, trans_loss=3.201, nll_loss=1.976, w2v_ctc_loss=2.305, task_loss=0, contrastive_loss=0.254, total=4124.8, n_correct=2025.32, ppl=3.93, accuracy=49.101, wps=10927.2, ups=1.33, wpb=8192.4, bsz=296.3, num_updates=4500, lr=0.00018001, gnorm=1.232, clip=0, loss_scale=4, train_wall=72, gb_free=16.7, wall=140
2023-08-02 09:48:04 | INFO | train_inner | epoch 004:    184 / 1474 loss=3.856, trans_loss=3.188, nll_loss=1.959, w2v_ctc_loss=2.314, task_loss=0, contrastive_loss=0.294, total=4175.15, n_correct=2069.33, ppl=3.89, accuracy=49.563, wps=11230, ups=1.35, wpb=8289.8, bsz=312.2, num_updates=4600, lr=0.000184008, gnorm=1.289, clip=0, loss_scale=4, train_wall=73, gb_free=16.9, wall=214
2023-08-02 09:49:19 | INFO | train_inner | epoch 004:    284 / 1474 loss=3.866, trans_loss=3.183, nll_loss=1.955, w2v_ctc_loss=2.296, task_loss=0, contrastive_loss=0.484, total=4145.23, n_correct=2059.64, ppl=3.88, accuracy=49.687, wps=11062.4, ups=1.34, wpb=8237.2, bsz=308.6, num_updates=4700, lr=0.000188006, gnorm=1.31, clip=0, loss_scale=4, train_wall=74, gb_free=16.2, wall=288
2023-08-02 09:50:33 | INFO | train_inner | epoch 004:    384 / 1474 loss=3.812, trans_loss=3.178, nll_loss=1.944, w2v_ctc_loss=2.273, task_loss=0, contrastive_loss=0.251, total=4127.66, n_correct=2056.63, ppl=3.85, accuracy=49.826, wps=11060.2, ups=1.35, wpb=8186.9, bsz=295.6, num_updates=4800, lr=0.000192004, gnorm=1.221, clip=0, loss_scale=4, train_wall=74, gb_free=17.6, wall=362
2023-08-02 09:51:48 | INFO | train_inner | epoch 004:    484 / 1474 loss=3.854, trans_loss=3.155, nll_loss=1.917, w2v_ctc_loss=2.217, task_loss=0, contrastive_loss=0.841, total=4218.78, n_correct=2128.15, ppl=3.78, accuracy=50.445, wps=11145.5, ups=1.33, wpb=8373.6, bsz=331.8, num_updates=4900, lr=0.000196002, gnorm=1.241, clip=0, loss_scale=4, train_wall=75, gb_free=16.7, wall=437
2023-08-02 09:53:02 | INFO | train_inner | epoch 004:    584 / 1474 loss=3.794, trans_loss=3.144, nll_loss=1.903, w2v_ctc_loss=2.248, task_loss=0, contrastive_loss=0.352, total=4217.52, n_correct=2144.01, ppl=3.74, accuracy=50.836, wps=11222.2, ups=1.34, wpb=8373.6, bsz=323.9, num_updates=5000, lr=0.0002, gnorm=1.225, clip=0, loss_scale=4, train_wall=74, gb_free=16.3, wall=512
mt_weight tensor(0.5000)
asr_weight tensor(1.1948, device='cuda:0')
2023-08-02 09:54:18 | INFO | train_inner | epoch 004:    684 / 1474 loss=3.774, trans_loss=3.153, nll_loss=1.909, w2v_ctc_loss=2.212, task_loss=0, contrastive_loss=0.436, total=4176.39, n_correct=2117.65, ppl=3.75, accuracy=50.705, wps=10894.2, ups=1.32, wpb=8272.5, bsz=303.8, num_updates=5100, lr=0.00019803, gnorm=1.375, clip=0, loss_scale=4, train_wall=75, gb_free=17.3, wall=588
2023-08-02 09:55:32 | INFO | train_inner | epoch 004:    784 / 1474 loss=3.731, trans_loss=3.138, nll_loss=1.897, w2v_ctc_loss=2.213, task_loss=0, contrastive_loss=0.24, total=4026.63, n_correct=2050.26, ppl=3.72, accuracy=50.918, wps=10777.3, ups=1.35, wpb=7998.4, bsz=280.4, num_updates=5200, lr=0.000196116, gnorm=1.379, clip=0, loss_scale=4, train_wall=74, gb_free=13.6, wall=662
2023-08-02 09:56:47 | INFO | train_inner | epoch 004:    884 / 1474 loss=3.761, trans_loss=3.122, nll_loss=1.876, w2v_ctc_loss=2.201, task_loss=0, contrastive_loss=0.494, total=4186.04, n_correct=2143.17, ppl=3.67, accuracy=51.198, wps=11087.3, ups=1.33, wpb=8315.3, bsz=310.9, num_updates=5300, lr=0.000194257, gnorm=1.375, clip=0, loss_scale=4, train_wall=74, gb_free=17.9, wall=737
2023-08-02 09:58:03 | INFO | train_inner | epoch 004:    984 / 1474 loss=3.691, trans_loss=3.107, nll_loss=1.857, w2v_ctc_loss=2.162, task_loss=0, contrastive_loss=0.296, total=4125.02, n_correct=2134.38, ppl=3.62, accuracy=51.742, wps=10883, ups=1.33, wpb=8196, bsz=304.7, num_updates=5400, lr=0.00019245, gnorm=1.337, clip=0, loss_scale=4, train_wall=75, gb_free=13.2, wall=812
2023-08-02 09:59:17 | INFO | train_inner | epoch 004:   1084 / 1474 loss=3.692, trans_loss=3.119, nll_loss=1.87, w2v_ctc_loss=2.17, task_loss=0, contrastive_loss=0.266, total=4075.6, n_correct=2099.81, ppl=3.66, accuracy=51.521, wps=10851.7, ups=1.34, wpb=8087.8, bsz=290.5, num_updates=5500, lr=0.000190693, gnorm=1.357, clip=0, loss_scale=4, train_wall=74, gb_free=16.3, wall=887
2023-08-02 10:00:32 | INFO | train_inner | epoch 004:   1184 / 1474 loss=3.704, trans_loss=3.101, nll_loss=1.851, w2v_ctc_loss=2.15, task_loss=0, contrastive_loss=0.434, total=4161.18, n_correct=2160.47, ppl=3.61, accuracy=51.92, wps=11148.5, ups=1.35, wpb=8270.6, bsz=322.3, num_updates=5600, lr=0.000188982, gnorm=1.36, clip=0, loss_scale=4, train_wall=74, gb_free=17, wall=961
2023-08-02 10:01:45 | INFO | train_inner | epoch 004:   1284 / 1474 loss=3.669, trans_loss=3.088, nll_loss=1.832, w2v_ctc_loss=2.124, task_loss=0, contrastive_loss=0.376, total=4156.53, n_correct=2174.81, ppl=3.56, accuracy=52.323, wps=11163, ups=1.35, wpb=8254.9, bsz=315.1, num_updates=5700, lr=0.000187317, gnorm=1.334, clip=0, loss_scale=4, train_wall=74, gb_free=16.3, wall=1035
2023-08-02 10:03:00 | INFO | train_inner | epoch 004:   1384 / 1474 loss=3.626, trans_loss=3.086, nll_loss=1.829, w2v_ctc_loss=2.122, task_loss=0, contrastive_loss=0.204, total=4101.23, n_correct=2145.82, ppl=3.55, accuracy=52.321, wps=11001.9, ups=1.35, wpb=8147.7, bsz=291.7, num_updates=5800, lr=0.000185695, gnorm=1.3, clip=0, loss_scale=4, train_wall=74, gb_free=15.9, wall=1109
2023-08-02 10:04:06 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(1.1948, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(1.1948, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(1.1948, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(1.1948, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(1.1948, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(1.1948, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(1.1948, device='cuda:3')
2023-08-02 10:04:32 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 4.662 | trans_loss 5.925 | nll_loss 3.292 | w2v_ctc_loss 1.582 | task_loss 0 | contrastive_loss 0.285 | total 4003.4 | n_correct 2255 | ppl 9.79 | accuracy 56.327 | uer 23.303 | wer 24.839 | raw_wer 24.839 | bleu 16.27 | wps 1926.1 | wpb 4003.4 | bsz 141.8 | num_updates 5890 | best_bleu 16.27
2023-08-02 10:04:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5890 updates
2023-08-02 10:04:32 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 10:04:46 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 10:04:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt (epoch 4 @ 5890 updates, score 16.27) (writing took 26.835422737523913 seconds)
2023-08-02 10:04:59 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-08-02 10:04:59 | INFO | train | epoch 004 | loss 3.753 | trans_loss 3.136 | nll_loss 1.893 | w2v_ctc_loss 2.207 | task_loss 0 | contrastive_loss 0.371 | total 4138.65 | n_correct 2109.63 | ppl 3.71 | accuracy 50.974 | wps 10519 | ups 1.28 | wpb 8217.2 | bsz 305.7 | num_updates 5890 | lr 0.000184271 | gnorm 1.312 | clip 0 | loss_scale 4 | train_wall 1101 | gb_free 15.1 | wall 1228
2023-08-02 10:04:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 10:04:59 | INFO | fairseq.trainer | begin training epoch 5
2023-08-02 10:04:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 10:05:14 | INFO | train_inner | epoch 005:     10 / 1474 loss=3.601, trans_loss=3.079, nll_loss=1.82, w2v_ctc_loss=2.089, task_loss=0, contrastive_loss=0.23, total=4037.7, n_correct=2121.43, ppl=3.53, accuracy=52.541, wps=5942.6, ups=0.74, wpb=8018.2, bsz=292.9, num_updates=5900, lr=0.000184115, gnorm=1.331, clip=0, loss_scale=4, train_wall=73, gb_free=17, wall=1244
2023-08-02 10:06:29 | INFO | train_inner | epoch 005:    110 / 1474 loss=3.488, trans_loss=3.023, nll_loss=1.747, w2v_ctc_loss=1.963, task_loss=0, contrastive_loss=0.236, total=4247.37, n_correct=2308.11, ppl=3.36, accuracy=54.342, wps=11336.4, ups=1.34, wpb=8436, bsz=330, num_updates=6000, lr=0.000182574, gnorm=1.275, clip=0, loss_scale=4, train_wall=74, gb_free=16.9, wall=1319
2023-08-02 10:06:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 10:06:54 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.66 | trans_loss 5.92 | nll_loss 3.283 | w2v_ctc_loss 1.582 | task_loss 0 | contrastive_loss 0.292 | total 4003.4 | n_correct 2258.5 | ppl 9.74 | accuracy 56.415 | uer 23.359 | wer 24.686 | raw_wer 24.686 | bleu 16.43 | wps 2021.8 | wpb 4003.4 | bsz 141.8 | num_updates 6000 | best_bleu 16.43
2023-08-02 10:06:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6000 updates
2023-08-02 10:06:54 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_5_6000.pt
2023-08-02 10:06:58 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_5_6000.pt
2023-08-02 10:07:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_5_6000.pt (epoch 5 @ 6000 updates, score 16.43) (writing took 49.71759455092251 seconds)
2023-08-02 10:08:57 | INFO | train_inner | epoch 005:    210 / 1474 loss=3.561, trans_loss=3.036, nll_loss=1.761, w2v_ctc_loss=1.994, task_loss=0, contrastive_loss=0.576, total=4189.85, n_correct=2259.68, ppl=3.39, accuracy=53.932, wps=5608.2, ups=0.67, wpb=8310.6, bsz=325.5, num_updates=6100, lr=0.000181071, gnorm=1.323, clip=0, loss_scale=4, train_wall=73, gb_free=17.9, wall=1467
2023-08-02 10:10:11 | INFO | train_inner | epoch 005:    310 / 1474 loss=3.542, trans_loss=3.032, nll_loss=1.762, w2v_ctc_loss=2.022, task_loss=0, contrastive_loss=0.353, total=4090.1, n_correct=2195.51, ppl=3.39, accuracy=53.679, wps=10960.1, ups=1.35, wpb=8137.9, bsz=295.9, num_updates=6200, lr=0.000179605, gnorm=1.334, clip=0, loss_scale=4, train_wall=74, gb_free=16.5, wall=1541
2023-08-02 10:11:25 | INFO | train_inner | epoch 005:    410 / 1474 loss=3.522, trans_loss=3.019, nll_loss=1.745, w2v_ctc_loss=1.969, task_loss=0, contrastive_loss=0.464, total=4147.17, n_correct=2247.58, ppl=3.35, accuracy=54.196, wps=11129.5, ups=1.35, wpb=8248, bsz=315, num_updates=6300, lr=0.000178174, gnorm=1.303, clip=0, loss_scale=4, train_wall=74, gb_free=15.1, wall=1615
2023-08-02 10:12:39 | INFO | train_inner | epoch 005:    510 / 1474 loss=3.475, trans_loss=3.029, nll_loss=1.755, w2v_ctc_loss=1.973, task_loss=0, contrastive_loss=0.167, total=4026.81, n_correct=2168.58, ppl=3.38, accuracy=53.854, wps=10860.7, ups=1.36, wpb=8002.9, bsz=277.7, num_updates=6400, lr=0.000176777, gnorm=1.286, clip=0, loss_scale=4, train_wall=73, gb_free=17.5, wall=1689
2023-08-02 10:13:53 | INFO | train_inner | epoch 005:    610 / 1474 loss=3.502, trans_loss=3.032, nll_loss=1.755, w2v_ctc_loss=1.951, task_loss=0, contrastive_loss=0.411, total=4107.75, n_correct=2219.18, ppl=3.38, accuracy=54.024, wps=10984.4, ups=1.35, wpb=8146.1, bsz=300.8, num_updates=6500, lr=0.000175412, gnorm=1.295, clip=0, loss_scale=8, train_wall=74, gb_free=16.4, wall=1763
2023-08-02 10:15:08 | INFO | train_inner | epoch 005:    710 / 1474 loss=3.507, trans_loss=3.022, nll_loss=1.745, w2v_ctc_loss=1.956, task_loss=0, contrastive_loss=0.394, total=4178.85, n_correct=2271.31, ppl=3.35, accuracy=54.353, wps=11092.6, ups=1.34, wpb=8294.2, bsz=320.6, num_updates=6600, lr=0.000174078, gnorm=1.299, clip=0, loss_scale=8, train_wall=74, gb_free=17.8, wall=1838
2023-08-02 10:16:23 | INFO | train_inner | epoch 005:    810 / 1474 loss=3.47, trans_loss=3.02, nll_loss=1.742, w2v_ctc_loss=1.944, task_loss=0, contrastive_loss=0.275, total=4127.73, n_correct=2242.79, ppl=3.35, accuracy=54.335, wps=10943.8, ups=1.34, wpb=8192.7, bsz=299.4, num_updates=6700, lr=0.000172774, gnorm=1.265, clip=0, loss_scale=8, train_wall=74, gb_free=15.4, wall=1913
2023-08-02 10:17:37 | INFO | train_inner | epoch 005:    910 / 1474 loss=3.451, trans_loss=3.012, nll_loss=1.733, w2v_ctc_loss=1.942, task_loss=0, contrastive_loss=0.214, total=4095.48, n_correct=2227.93, ppl=3.33, accuracy=54.4, wps=10999.7, ups=1.35, wpb=8134, bsz=296.9, num_updates=6800, lr=0.000171499, gnorm=1.274, clip=0, loss_scale=8, train_wall=74, gb_free=15.8, wall=1987
2023-08-02 10:18:50 | INFO | train_inner | epoch 005:   1010 / 1474 loss=3.469, trans_loss=3.012, nll_loss=1.732, w2v_ctc_loss=1.938, task_loss=0, contrastive_loss=0.341, total=4165.12, n_correct=2271.01, ppl=3.32, accuracy=54.524, wps=11270.4, ups=1.36, wpb=8268.5, bsz=309, num_updates=6900, lr=0.000170251, gnorm=1.258, clip=0, loss_scale=8, train_wall=73, gb_free=15.9, wall=2060
2023-08-02 10:20:05 | INFO | train_inner | epoch 005:   1110 / 1474 loss=3.479, trans_loss=3.013, nll_loss=1.733, w2v_ctc_loss=1.944, task_loss=0, contrastive_loss=0.342, total=4176.72, n_correct=2284.63, ppl=3.32, accuracy=54.699, wps=11036.8, ups=1.33, wpb=8282.5, bsz=310.8, num_updates=7000, lr=0.000169031, gnorm=1.277, clip=0, loss_scale=8, train_wall=75, gb_free=17, wall=2135
2023-08-02 10:21:19 | INFO | train_inner | epoch 005:   1210 / 1474 loss=3.433, trans_loss=3.008, nll_loss=1.726, w2v_ctc_loss=1.922, task_loss=0, contrastive_loss=0.198, total=4164.13, n_correct=2281.84, ppl=3.31, accuracy=54.798, wps=11154.8, ups=1.35, wpb=8256.7, bsz=302.6, num_updates=7100, lr=0.000167836, gnorm=1.266, clip=0, loss_scale=8, train_wall=74, gb_free=17.1, wall=2209
2023-08-02 10:22:34 | INFO | train_inner | epoch 005:   1310 / 1474 loss=3.408, trans_loss=3.004, nll_loss=1.723, w2v_ctc_loss=1.898, task_loss=0, contrastive_loss=0.163, total=4134.91, n_correct=2264.57, ppl=3.3, accuracy=54.767, wps=11037.7, ups=1.35, wpb=8206.5, bsz=297.1, num_updates=7200, lr=0.000166667, gnorm=1.255, clip=0, loss_scale=8, train_wall=74, gb_free=16.6, wall=2283
2023-08-02 10:23:47 | INFO | train_inner | epoch 005:   1410 / 1474 loss=3.416, trans_loss=2.997, nll_loss=1.716, w2v_ctc_loss=1.891, task_loss=0, contrastive_loss=0.245, total=4134.37, n_correct=2271.63, ppl=3.29, accuracy=54.945, wps=11164.7, ups=1.36, wpb=8213.2, bsz=305.6, num_updates=7300, lr=0.000165521, gnorm=1.26, clip=0, loss_scale=8, train_wall=73, gb_free=17.9, wall=2357
2023-08-02 10:24:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 10:24:58 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.554 | trans_loss 5.833 | nll_loss 3.18 | w2v_ctc_loss 1.43 | task_loss 0 | contrastive_loss 0.295 | total 4003.4 | n_correct 2315.6 | ppl 9.07 | accuracy 57.841 | uer 21.673 | wer 23.31 | raw_wer 23.31 | bleu 16.97 | wps 2296.7 | wpb 4003.4 | bsz 141.8 | num_updates 7364 | best_bleu 16.97
2023-08-02 10:24:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7364 updates
2023-08-02 10:24:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 10:25:14 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 10:25:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt (epoch 5 @ 7364 updates, score 16.97) (writing took 28.134190764278173 seconds)
2023-08-02 10:25:26 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-08-02 10:25:26 | INFO | train | epoch 005 | loss 3.479 | trans_loss 3.018 | nll_loss 1.74 | w2v_ctc_loss 1.95 | task_loss 0 | contrastive_loss 0.313 | total 4138.65 | n_correct 2250.14 | ppl 3.34 | accuracy 54.369 | wps 9868.9 | ups 1.2 | wpb 8217.2 | bsz 305.7 | num_updates 7364 | lr 0.0001648 | gnorm 1.285 | clip 0 | loss_scale 8 | train_wall 1086 | gb_free 16.4 | wall 2456
2023-08-02 10:25:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 10:25:26 | INFO | fairseq.trainer | begin training epoch 6
2023-08-02 10:25:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 10:26:01 | INFO | train_inner | epoch 006:     36 / 1474 loss=3.396, trans_loss=2.984, nll_loss=1.695, w2v_ctc_loss=1.881, task_loss=0, contrastive_loss=0.242, total=4115.45, n_correct=2280.18, ppl=3.24, accuracy=55.405, wps=6100.8, ups=0.75, wpb=8165.8, bsz=298.6, num_updates=7400, lr=0.000164399, gnorm=1.285, clip=0, loss_scale=8, train_wall=74, gb_free=16.6, wall=2491
2023-08-02 10:27:15 | INFO | train_inner | epoch 006:    136 / 1474 loss=3.328, trans_loss=2.952, nll_loss=1.656, w2v_ctc_loss=1.794, task_loss=0, contrastive_loss=0.315, total=4154.25, n_correct=2333.15, ppl=3.15, accuracy=56.163, wps=11180.9, ups=1.35, wpb=8253.2, bsz=304.1, num_updates=7500, lr=0.000163299, gnorm=1.249, clip=0, loss_scale=8, train_wall=73, gb_free=15.8, wall=2565
2023-08-02 10:28:29 | INFO | train_inner | epoch 006:    236 / 1474 loss=3.345, trans_loss=2.966, nll_loss=1.674, w2v_ctc_loss=1.85, task_loss=0, contrastive_loss=0.173, total=4112.66, n_correct=2287.8, ppl=3.19, accuracy=55.628, wps=11064.2, ups=1.35, wpb=8174.6, bsz=291.5, num_updates=7600, lr=0.000162221, gnorm=1.247, clip=0, loss_scale=8, train_wall=73, gb_free=16.3, wall=2638
2023-08-02 10:29:44 | INFO | train_inner | epoch 006:    336 / 1474 loss=3.365, trans_loss=2.951, nll_loss=1.655, w2v_ctc_loss=1.774, task_loss=0, contrastive_loss=0.621, total=4177.51, n_correct=2357.12, ppl=3.15, accuracy=56.424, wps=11037.8, ups=1.33, wpb=8296.3, bsz=327.5, num_updates=7700, lr=0.000161165, gnorm=1.26, clip=0, loss_scale=8, train_wall=75, gb_free=16.3, wall=2714
2023-08-02 10:30:57 | INFO | train_inner | epoch 006:    436 / 1474 loss=3.305, trans_loss=2.951, nll_loss=1.654, w2v_ctc_loss=1.792, task_loss=0, contrastive_loss=0.194, total=4154.57, n_correct=2338.89, ppl=3.15, accuracy=56.297, wps=11226.9, ups=1.36, wpb=8250.9, bsz=313.4, num_updates=7800, lr=0.000160128, gnorm=1.247, clip=0, loss_scale=8, train_wall=73, gb_free=16.4, wall=2787
2023-08-02 10:32:12 | INFO | train_inner | epoch 006:    536 / 1474 loss=3.32, trans_loss=2.959, nll_loss=1.664, w2v_ctc_loss=1.818, task_loss=0, contrastive_loss=0.179, total=4167.79, n_correct=2338.64, ppl=3.17, accuracy=56.112, wps=11160.5, ups=1.35, wpb=8270.7, bsz=303.4, num_updates=7900, lr=0.000159111, gnorm=1.236, clip=0, loss_scale=8, train_wall=74, gb_free=16, wall=2861
2023-08-02 10:33:25 | INFO | train_inner | epoch 006:    636 / 1474 loss=3.324, trans_loss=2.957, nll_loss=1.662, w2v_ctc_loss=1.794, task_loss=0, contrastive_loss=0.272, total=4146.17, n_correct=2329.59, ppl=3.16, accuracy=56.187, wps=11167, ups=1.36, wpb=8230.4, bsz=314.4, num_updates=8000, lr=0.000158114, gnorm=1.268, clip=0, loss_scale=8, train_wall=73, gb_free=16.8, wall=2935
2023-08-02 10:33:25 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 10:33:47 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.522 | trans_loss 5.786 | nll_loss 3.111 | w2v_ctc_loss 1.443 | task_loss 0 | contrastive_loss 0.278 | total 4003.4 | n_correct 2338.3 | ppl 8.64 | accuracy 58.408 | uer 20.654 | wer 22.303 | raw_wer 22.303 | bleu 17.87 | wps 2396.3 | wpb 4003.4 | bsz 141.8 | num_updates 8000 | best_bleu 17.87
2023-08-02 10:33:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8000 updates
2023-08-02 10:33:47 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_6_8000.pt
2023-08-02 10:33:51 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_6_8000.pt
2023-08-02 10:34:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_6_8000.pt (epoch 6 @ 8000 updates, score 17.87) (writing took 43.26603898778558 seconds)
2023-08-02 10:35:45 | INFO | train_inner | epoch 006:    736 / 1474 loss=3.334, trans_loss=2.96, nll_loss=1.667, w2v_ctc_loss=1.825, task_loss=0, contrastive_loss=0.198, total=4148.65, n_correct=2321.9, ppl=3.18, accuracy=55.968, wps=5904.5, ups=0.72, wpb=8239.4, bsz=302.5, num_updates=8100, lr=0.000157135, gnorm=1.233, clip=0, loss_scale=8, train_wall=73, gb_free=15.8, wall=3074
2023-08-02 10:36:59 | INFO | train_inner | epoch 006:    836 / 1474 loss=3.313, trans_loss=2.964, nll_loss=1.672, w2v_ctc_loss=1.802, task_loss=0, contrastive_loss=0.172, total=4114.34, n_correct=2296.79, ppl=3.19, accuracy=55.824, wps=11048.8, ups=1.35, wpb=8167.8, bsz=294.4, num_updates=8200, lr=0.000156174, gnorm=1.25, clip=0, loss_scale=8, train_wall=73, gb_free=15.3, wall=3148
2023-08-02 10:38:13 | INFO | train_inner | epoch 006:    936 / 1474 loss=3.347, trans_loss=2.964, nll_loss=1.67, w2v_ctc_loss=1.812, task_loss=0, contrastive_loss=0.317, total=4081.53, n_correct=2282.27, ppl=3.18, accuracy=55.917, wps=10940.8, ups=1.35, wpb=8099.8, bsz=296.4, num_updates=8300, lr=0.00015523, gnorm=1.268, clip=0, loss_scale=8, train_wall=74, gb_free=18, wall=3222
2023-08-02 10:39:27 | INFO | train_inner | epoch 006:   1036 / 1474 loss=3.337, trans_loss=2.952, nll_loss=1.657, w2v_ctc_loss=1.783, task_loss=0, contrastive_loss=0.426, total=4165.84, n_correct=2346.58, ppl=3.15, accuracy=56.329, wps=11180.6, ups=1.35, wpb=8269.8, bsz=318.1, num_updates=8400, lr=0.000154303, gnorm=1.272, clip=0, loss_scale=8, train_wall=74, gb_free=17, wall=3296
2023-08-02 10:40:40 | INFO | train_inner | epoch 006:   1136 / 1474 loss=3.303, trans_loss=2.956, nll_loss=1.661, w2v_ctc_loss=1.793, task_loss=0, contrastive_loss=0.177, total=4072.29, n_correct=2285.55, ppl=3.16, accuracy=56.124, wps=10970.8, ups=1.36, wpb=8085.3, bsz=285.3, num_updates=8500, lr=0.000153393, gnorm=1.252, clip=0, loss_scale=8, train_wall=73, gb_free=17.2, wall=3370
2023-08-02 10:41:55 | INFO | train_inner | epoch 006:   1236 / 1474 loss=3.357, trans_loss=2.945, nll_loss=1.65, w2v_ctc_loss=1.778, task_loss=0, contrastive_loss=0.641, total=4141.55, n_correct=2334.78, ppl=3.14, accuracy=56.375, wps=11046.6, ups=1.34, wpb=8229.3, bsz=316.5, num_updates=8600, lr=0.000152499, gnorm=1.26, clip=0, loss_scale=16, train_wall=74, gb_free=13.6, wall=3445
2023-08-02 10:43:09 | INFO | train_inner | epoch 006:   1336 / 1474 loss=3.291, trans_loss=2.951, nll_loss=1.653, w2v_ctc_loss=1.777, task_loss=0, contrastive_loss=0.159, total=4125.31, n_correct=2330.52, ppl=3.14, accuracy=56.493, wps=11108.8, ups=1.36, wpb=8179.7, bsz=301.8, num_updates=8700, lr=0.00015162, gnorm=1.242, clip=0, loss_scale=16, train_wall=73, gb_free=17.9, wall=3518
2023-08-02 10:44:23 | INFO | train_inner | epoch 006:   1436 / 1474 loss=3.281, trans_loss=2.941, nll_loss=1.642, w2v_ctc_loss=1.777, task_loss=0, contrastive_loss=0.163, total=4196.2, n_correct=2378.15, ppl=3.12, accuracy=56.674, wps=11201.5, ups=1.34, wpb=8329, bsz=307.7, num_updates=8800, lr=0.000150756, gnorm=1.224, clip=0, loss_scale=16, train_wall=74, gb_free=11.9, wall=3593
2023-08-02 10:44:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 10:45:14 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.467 | trans_loss 5.738 | nll_loss 3.053 | w2v_ctc_loss 1.386 | task_loss 0 | contrastive_loss 0.256 | total 4003.4 | n_correct 2362.1 | ppl 8.3 | accuracy 59.002 | uer 19.446 | wer 21.196 | raw_wer 21.196 | bleu 18.22 | wps 2136.6 | wpb 4003.4 | bsz 141.8 | num_updates 8838 | best_bleu 18.22
2023-08-02 10:45:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8838 updates
2023-08-02 10:45:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 10:45:29 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 10:45:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt (epoch 6 @ 8838 updates, score 18.22) (writing took 26.993916315957904 seconds)
2023-08-02 10:45:41 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-08-02 10:45:41 | INFO | train | epoch 006 | loss 3.323 | trans_loss 2.954 | nll_loss 1.659 | w2v_ctc_loss 1.796 | task_loss 0 | contrastive_loss 0.285 | total 4138.65 | n_correct 2325.97 | ppl 3.16 | accuracy 56.201 | wps 9968.4 | ups 1.21 | wpb 8217.2 | bsz 305.7 | num_updates 8838 | lr 0.000150431 | gnorm 1.25 | clip 0 | loss_scale 16 | train_wall 1084 | gb_free 15.4 | wall 3671
2023-08-02 10:45:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 10:45:41 | INFO | fairseq.trainer | begin training epoch 7
2023-08-02 10:45:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 10:46:36 | INFO | train_inner | epoch 007:     62 / 1474 loss=3.233, trans_loss=2.919, nll_loss=1.614, w2v_ctc_loss=1.717, task_loss=0, contrastive_loss=0.185, total=4108.19, n_correct=2354.05, ppl=3.06, accuracy=57.301, wps=6139.2, ups=0.75, wpb=8158.4, bsz=307.9, num_updates=8900, lr=0.000149906, gnorm=1.229, clip=0, loss_scale=16, train_wall=73, gb_free=17.3, wall=3726
2023-08-02 10:47:49 | INFO | train_inner | epoch 007:    162 / 1474 loss=3.227, trans_loss=2.916, nll_loss=1.609, w2v_ctc_loss=1.701, task_loss=0, contrastive_loss=0.3, total=4106.05, n_correct=2358.43, ppl=3.05, accuracy=57.438, wps=11114.5, ups=1.36, wpb=8152.7, bsz=303.6, num_updates=9000, lr=0.000149071, gnorm=1.231, clip=0, loss_scale=16, train_wall=73, gb_free=16.9, wall=3799
2023-08-02 10:49:04 | INFO | train_inner | epoch 007:    262 / 1474 loss=3.209, trans_loss=2.909, nll_loss=1.598, w2v_ctc_loss=1.709, task_loss=0, contrastive_loss=0.157, total=4129.3, n_correct=2380.19, ppl=3.03, accuracy=57.641, wps=10927.2, ups=1.33, wpb=8193.5, bsz=301.2, num_updates=9100, lr=0.00014825, gnorm=1.227, clip=0, loss_scale=16, train_wall=75, gb_free=17.5, wall=3874
2023-08-02 10:50:19 | INFO | train_inner | epoch 007:    362 / 1474 loss=3.267, trans_loss=2.916, nll_loss=1.608, w2v_ctc_loss=1.698, task_loss=0, contrastive_loss=0.547, total=4201.67, n_correct=2409.06, ppl=3.05, accuracy=57.336, wps=11075.8, ups=1.33, wpb=8338.1, bsz=319.8, num_updates=9200, lr=0.000147442, gnorm=1.251, clip=0, loss_scale=16, train_wall=75, gb_free=15.6, wall=3949
2023-08-02 10:51:33 | INFO | train_inner | epoch 007:    462 / 1474 loss=3.236, trans_loss=2.913, nll_loss=1.608, w2v_ctc_loss=1.679, task_loss=0, contrastive_loss=0.438, total=4155.31, n_correct=2381.3, ppl=3.05, accuracy=57.307, wps=11214.4, ups=1.36, wpb=8255.6, bsz=310.3, num_updates=9300, lr=0.000146647, gnorm=1.227, clip=0, loss_scale=16, train_wall=73, gb_free=17, wall=4023
2023-08-02 10:52:46 | INFO | train_inner | epoch 007:    562 / 1474 loss=3.212, trans_loss=2.912, nll_loss=1.602, w2v_ctc_loss=1.7, task_loss=0, contrastive_loss=0.168, total=4165.88, n_correct=2398.4, ppl=3.04, accuracy=57.572, wps=11280.6, ups=1.37, wpb=8260.5, bsz=306, num_updates=9400, lr=0.000145865, gnorm=1.23, clip=0, loss_scale=16, train_wall=73, gb_free=17.4, wall=4096
2023-08-02 10:54:00 | INFO | train_inner | epoch 007:    662 / 1474 loss=3.192, trans_loss=2.911, nll_loss=1.601, w2v_ctc_loss=1.681, task_loss=0, contrastive_loss=0.151, total=4149.29, n_correct=2387.97, ppl=3.03, accuracy=57.551, wps=11142.6, ups=1.35, wpb=8232, bsz=301.1, num_updates=9500, lr=0.000145095, gnorm=1.223, clip=0, loss_scale=16, train_wall=73, gb_free=17.2, wall=4170
2023-08-02 10:55:15 | INFO | train_inner | epoch 007:    762 / 1474 loss=3.205, trans_loss=2.91, nll_loss=1.603, w2v_ctc_loss=1.703, task_loss=0, contrastive_loss=0.153, total=4134.54, n_correct=2376.11, ppl=3.04, accuracy=57.47, wps=11043.3, ups=1.34, wpb=8210.8, bsz=299.9, num_updates=9600, lr=0.000144338, gnorm=1.224, clip=0, loss_scale=16, train_wall=74, gb_free=14.2, wall=4244
2023-08-02 10:56:29 | INFO | train_inner | epoch 007:    862 / 1474 loss=3.196, trans_loss=2.911, nll_loss=1.604, w2v_ctc_loss=1.679, task_loss=0, contrastive_loss=0.176, total=4151.77, n_correct=2385.94, ppl=3.04, accuracy=57.468, wps=11084.3, ups=1.35, wpb=8239.8, bsz=307.9, num_updates=9700, lr=0.000143592, gnorm=1.232, clip=0, loss_scale=16, train_wall=74, gb_free=15.1, wall=4319
2023-08-02 10:57:43 | INFO | train_inner | epoch 007:    962 / 1474 loss=3.215, trans_loss=2.907, nll_loss=1.599, w2v_ctc_loss=1.672, task_loss=0, contrastive_loss=0.314, total=4124.8, n_correct=2376.29, ppl=3.03, accuracy=57.61, wps=11049.9, ups=1.35, wpb=8188.5, bsz=314.1, num_updates=9800, lr=0.000142857, gnorm=1.237, clip=0, loss_scale=16, train_wall=74, gb_free=16.9, wall=4393
2023-08-02 10:58:56 | INFO | train_inner | epoch 007:   1062 / 1474 loss=3.189, trans_loss=2.913, nll_loss=1.606, w2v_ctc_loss=1.681, task_loss=0, contrastive_loss=0.13, total=4113.08, n_correct=2363.92, ppl=3.04, accuracy=57.473, wps=11141.8, ups=1.36, wpb=8166.5, bsz=292.9, num_updates=9900, lr=0.000142134, gnorm=1.223, clip=0, loss_scale=16, train_wall=73, gb_free=15, wall=4466
2023-08-02 11:00:10 | INFO | train_inner | epoch 007:   1162 / 1474 loss=3.257, trans_loss=2.905, nll_loss=1.601, w2v_ctc_loss=1.688, task_loss=0, contrastive_loss=0.517, total=4134.15, n_correct=2379.27, ppl=3.03, accuracy=57.552, wps=11111.7, ups=1.35, wpb=8219.6, bsz=313.5, num_updates=10000, lr=0.000141421, gnorm=1.265, clip=0, loss_scale=16, train_wall=73, gb_free=16.3, wall=4540
2023-08-02 11:00:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 11:00:33 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.45 | trans_loss 5.716 | nll_loss 3.024 | w2v_ctc_loss 1.382 | task_loss 0 | contrastive_loss 0.255 | total 4003.4 | n_correct 2379.1 | ppl 8.13 | accuracy 59.427 | uer 19.051 | wer 20.827 | raw_wer 20.827 | bleu 18.31 | wps 2373.9 | wpb 4003.4 | bsz 141.8 | num_updates 10000 | best_bleu 18.31
2023-08-02 11:00:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10000 updates
2023-08-02 11:00:33 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_7_10000.pt
2023-08-02 11:00:36 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_7_10000.pt
2023-08-02 11:00:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_7_10000.pt (epoch 7 @ 10000 updates, score 18.31) (writing took 25.71622103638947 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:0')
2023-08-02 11:02:13 | INFO | train_inner | epoch 007:   1262 / 1474 loss=3.197, trans_loss=2.909, nll_loss=1.603, w2v_ctc_loss=1.686, task_loss=0, contrastive_loss=0.169, total=4133.98, n_correct=2377.07, ppl=3.04, accuracy=57.501, wps=6707, ups=0.82, wpb=8209.9, bsz=300.6, num_updates=10100, lr=0.00014072, gnorm=1.553, clip=0, loss_scale=16, train_wall=73, gb_free=16.9, wall=4662
2023-08-02 11:03:26 | INFO | train_inner | epoch 007:   1362 / 1474 loss=3.209, trans_loss=2.904, nll_loss=1.596, w2v_ctc_loss=1.69, task_loss=0, contrastive_loss=0.217, total=4171.46, n_correct=2411.7, ppl=3.02, accuracy=57.814, wps=11279.6, ups=1.36, wpb=8281.7, bsz=317, num_updates=10200, lr=0.000140028, gnorm=1.53, clip=0, loss_scale=16, train_wall=73, gb_free=17.9, wall=4736
2023-08-02 11:04:41 | INFO | train_inner | epoch 007:   1462 / 1474 loss=3.22, trans_loss=2.91, nll_loss=1.606, w2v_ctc_loss=1.686, task_loss=0, contrastive_loss=0.319, total=4106.94, n_correct=2358.27, ppl=3.04, accuracy=57.422, wps=10929.3, ups=1.34, wpb=8164.3, bsz=295.8, num_updates=10300, lr=0.000139347, gnorm=1.563, clip=0, loss_scale=16, train_wall=74, gb_free=15.9, wall=4810
2023-08-02 11:04:49 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:4')
2023-08-02 11:05:12 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.43 | trans_loss 5.705 | nll_loss 3.011 | w2v_ctc_loss 1.337 | task_loss 0 | contrastive_loss 0.259 | total 4003.4 | n_correct 2389.4 | ppl 8.06 | accuracy 59.684 | uer 18.947 | wer 20.775 | raw_wer 20.775 | bleu 19.08 | wps 2329.5 | wpb 4003.4 | bsz 141.8 | num_updates 10312 | best_bleu 19.08
2023-08-02 11:05:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10312 updates
2023-08-02 11:05:12 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 11:05:24 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 11:05:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt (epoch 7 @ 10312 updates, score 19.08) (writing took 23.857062408700585 seconds)
2023-08-02 11:05:36 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-08-02 11:05:36 | INFO | train | epoch 007 | loss 3.216 | trans_loss 2.91 | nll_loss 1.603 | w2v_ctc_loss 1.69 | task_loss 0 | contrastive_loss 0.266 | total 4138.65 | n_correct 2380.4 | ppl 3.04 | accuracy 57.516 | wps 10134.4 | ups 1.23 | wpb 8217.2 | bsz 305.7 | num_updates 10312 | lr 0.000139266 | gnorm 1.3 | clip 0 | loss_scale 16 | train_wall 1084 | gb_free 13.5 | wall 4866
2023-08-02 11:05:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 11:05:37 | INFO | fairseq.trainer | begin training epoch 8
2023-08-02 11:05:37 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 11:06:50 | INFO | train_inner | epoch 008:     88 / 1474 loss=3.133, trans_loss=2.89, nll_loss=1.573, w2v_ctc_loss=1.614, task_loss=0, contrastive_loss=0.158, total=4120.74, n_correct=2403.13, ppl=2.97, accuracy=58.318, wps=6339.6, ups=0.78, wpb=8166.6, bsz=296.3, num_updates=10400, lr=0.000138675, gnorm=1.491, clip=0, loss_scale=16, train_wall=73, gb_free=17.4, wall=4939
2023-08-02 11:08:03 | INFO | train_inner | epoch 008:    188 / 1474 loss=3.131, trans_loss=2.886, nll_loss=1.568, w2v_ctc_loss=1.613, task_loss=0, contrastive_loss=0.189, total=4028.45, n_correct=2351.01, ppl=2.96, accuracy=58.36, wps=10828.8, ups=1.36, wpb=7986.6, bsz=284.5, num_updates=10500, lr=0.000138013, gnorm=1.547, clip=0, loss_scale=16, train_wall=73, gb_free=16.6, wall=5013
2023-08-02 11:09:17 | INFO | train_inner | epoch 008:    288 / 1474 loss=3.121, trans_loss=2.877, nll_loss=1.558, w2v_ctc_loss=1.602, task_loss=0, contrastive_loss=0.183, total=4212.3, n_correct=2469.7, ppl=2.95, accuracy=58.631, wps=11339.5, ups=1.36, wpb=8356.7, bsz=326.4, num_updates=10600, lr=0.000137361, gnorm=1.501, clip=0, loss_scale=32, train_wall=73, gb_free=17, wall=5087
2023-08-02 11:10:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-02 11:10:32 | INFO | train_inner | epoch 008:    389 / 1474 loss=3.155, trans_loss=2.888, nll_loss=1.572, w2v_ctc_loss=1.637, task_loss=0, contrastive_loss=0.223, total=4122.94, n_correct=2396.02, ppl=2.97, accuracy=58.114, wps=10868.9, ups=1.33, wpb=8182.1, bsz=294.2, num_updates=10700, lr=0.000136717, gnorm=1.529, clip=0, loss_scale=16, train_wall=75, gb_free=12.2, wall=5162
2023-08-02 11:11:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-02 11:11:48 | INFO | train_inner | epoch 008:    490 / 1474 loss=3.182, trans_loss=2.881, nll_loss=1.566, w2v_ctc_loss=1.617, task_loss=0, contrastive_loss=0.431, total=4188.76, n_correct=2446.42, ppl=2.96, accuracy=58.404, wps=11059.3, ups=1.33, wpb=8315.2, bsz=329.5, num_updates=10800, lr=0.000136083, gnorm=1.533, clip=0, loss_scale=8, train_wall=75, gb_free=13.2, wall=5237
2023-08-02 11:13:02 | INFO | train_inner | epoch 008:    590 / 1474 loss=3.132, trans_loss=2.883, nll_loss=1.572, w2v_ctc_loss=1.635, task_loss=0, contrastive_loss=0.125, total=4065.55, n_correct=2360.32, ppl=2.97, accuracy=58.057, wps=10872.8, ups=1.34, wpb=8087.9, bsz=285.5, num_updates=10900, lr=0.000135457, gnorm=1.525, clip=0, loss_scale=8, train_wall=74, gb_free=16.3, wall=5312
2023-08-02 11:14:16 | INFO | train_inner | epoch 008:    690 / 1474 loss=3.136, trans_loss=2.882, nll_loss=1.567, w2v_ctc_loss=1.635, task_loss=0, contrastive_loss=0.14, total=4135.41, n_correct=2420.3, ppl=2.96, accuracy=58.526, wps=11054.8, ups=1.35, wpb=8208.4, bsz=298.1, num_updates=11000, lr=0.00013484, gnorm=1.522, clip=0, loss_scale=8, train_wall=74, gb_free=16.2, wall=5386
2023-08-02 11:15:30 | INFO | train_inner | epoch 008:    790 / 1474 loss=3.136, trans_loss=2.873, nll_loss=1.559, w2v_ctc_loss=1.61, task_loss=0, contrastive_loss=0.278, total=4128.86, n_correct=2413.3, ppl=2.95, accuracy=58.45, wps=11168, ups=1.36, wpb=8210.1, bsz=301.4, num_updates=11100, lr=0.000134231, gnorm=1.537, clip=0, loss_scale=8, train_wall=73, gb_free=16.6, wall=5459
2023-08-02 11:16:44 | INFO | train_inner | epoch 008:    890 / 1474 loss=3.144, trans_loss=2.878, nll_loss=1.564, w2v_ctc_loss=1.607, task_loss=0, contrastive_loss=0.286, total=4166.92, n_correct=2438.78, ppl=2.96, accuracy=58.527, wps=11227.2, ups=1.36, wpb=8279.8, bsz=314.3, num_updates=11200, lr=0.000133631, gnorm=1.536, clip=0, loss_scale=8, train_wall=73, gb_free=14.8, wall=5533
2023-08-02 11:17:58 | INFO | train_inner | epoch 008:    990 / 1474 loss=3.112, trans_loss=2.88, nll_loss=1.565, w2v_ctc_loss=1.6, task_loss=0, contrastive_loss=0.136, total=4150.39, n_correct=2428.84, ppl=2.96, accuracy=58.521, wps=11130.9, ups=1.35, wpb=8239.8, bsz=308.5, num_updates=11300, lr=0.000133038, gnorm=1.538, clip=0, loss_scale=8, train_wall=74, gb_free=17.5, wall=5607
2023-08-02 11:19:12 | INFO | train_inner | epoch 008:   1090 / 1474 loss=3.164, trans_loss=2.884, nll_loss=1.57, w2v_ctc_loss=1.603, task_loss=0, contrastive_loss=0.474, total=4197.39, n_correct=2445.35, ppl=2.97, accuracy=58.259, wps=11182.8, ups=1.34, wpb=8332.1, bsz=310.9, num_updates=11400, lr=0.000132453, gnorm=1.514, clip=0, loss_scale=8, train_wall=74, gb_free=17, wall=5682
2023-08-02 11:20:26 | INFO | train_inner | epoch 008:   1190 / 1474 loss=3.117, trans_loss=2.876, nll_loss=1.562, w2v_ctc_loss=1.606, task_loss=0, contrastive_loss=0.151, total=4180.55, n_correct=2445.02, ppl=2.95, accuracy=58.486, wps=11267.3, ups=1.36, wpb=8306.7, bsz=315.1, num_updates=11500, lr=0.000131876, gnorm=1.517, clip=0, loss_scale=8, train_wall=73, gb_free=17.4, wall=5755
2023-08-02 11:21:39 | INFO | train_inner | epoch 008:   1290 / 1474 loss=3.134, trans_loss=2.884, nll_loss=1.572, w2v_ctc_loss=1.62, task_loss=0, contrastive_loss=0.184, total=4062.6, n_correct=2362.33, ppl=2.97, accuracy=58.148, wps=11080.1, ups=1.37, wpb=8072.7, bsz=291.9, num_updates=11600, lr=0.000131306, gnorm=1.545, clip=0, loss_scale=8, train_wall=72, gb_free=13.4, wall=5828
2023-08-02 11:22:52 | INFO | train_inner | epoch 008:   1390 / 1474 loss=3.136, trans_loss=2.879, nll_loss=1.565, w2v_ctc_loss=1.596, task_loss=0, contrastive_loss=0.277, total=4159.11, n_correct=2431.19, ppl=2.96, accuracy=58.455, wps=11286.1, ups=1.37, wpb=8259.9, bsz=312.5, num_updates=11700, lr=0.000130744, gnorm=1.52, clip=0, loss_scale=8, train_wall=73, gb_free=13.6, wall=5901
2023-08-02 11:23:54 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 11:24:16 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 4.409 | trans_loss 5.677 | nll_loss 2.976 | w2v_ctc_loss 1.339 | task_loss 0 | contrastive_loss 0.242 | total 4003.4 | n_correct 2403.6 | ppl 7.87 | accuracy 60.039 | uer 18.586 | wer 20.111 | raw_wer 20.111 | bleu 18.78 | wps 2318.8 | wpb 4003.4 | bsz 141.8 | num_updates 11784 | best_bleu 19.08
2023-08-02 11:24:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11784 updates
2023-08-02 11:24:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_18.7805.pt
2023-08-02 11:24:20 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_18.7805.pt
2023-08-02 11:24:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_18.7805.pt (epoch 8 @ 11784 updates, score 18.78) (writing took 18.15819812193513 seconds)
2023-08-02 11:24:35 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-08-02 11:24:35 | INFO | train | epoch 008 | loss 3.137 | trans_loss 2.881 | nll_loss 1.566 | w2v_ctc_loss 1.612 | task_loss 0 | contrastive_loss 0.239 | total 4137.57 | n_correct 2415.8 | ppl 2.96 | accuracy 58.387 | wps 10621.7 | ups 1.29 | wpb 8215.1 | bsz 305.3 | num_updates 11784 | lr 0.000130277 | gnorm 1.526 | clip 0 | loss_scale 8 | train_wall 1082 | gb_free 17.1 | wall 6004
2023-08-02 11:24:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 11:24:35 | INFO | fairseq.trainer | begin training epoch 9
2023-08-02 11:24:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 11:24:54 | INFO | train_inner | epoch 009:     16 / 1474 loss=3.132, trans_loss=2.876, nll_loss=1.559, w2v_ctc_loss=1.574, task_loss=0, contrastive_loss=0.428, total=4121.25, n_correct=2415.76, ppl=2.95, accuracy=58.617, wps=6667.8, ups=0.82, wpb=8177.2, bsz=310.7, num_updates=11800, lr=0.000130189, gnorm=1.531, clip=0, loss_scale=8, train_wall=73, gb_free=17.9, wall=6024
2023-08-02 11:26:08 | INFO | train_inner | epoch 009:    116 / 1474 loss=3.059, trans_loss=2.848, nll_loss=1.523, w2v_ctc_loss=1.54, task_loss=0, contrastive_loss=0.177, total=4191.82, n_correct=2486.47, ppl=2.87, accuracy=59.317, wps=11263.7, ups=1.35, wpb=8326.3, bsz=320, num_updates=11900, lr=0.000129641, gnorm=1.503, clip=0, loss_scale=8, train_wall=73, gb_free=16.2, wall=6098
2023-08-02 11:27:22 | INFO | train_inner | epoch 009:    216 / 1474 loss=3.053, trans_loss=2.855, nll_loss=1.532, w2v_ctc_loss=1.544, task_loss=0, contrastive_loss=0.121, total=4061.27, n_correct=2407.92, ppl=2.89, accuracy=59.29, wps=10950.6, ups=1.36, wpb=8065.2, bsz=287.6, num_updates=12000, lr=0.000129099, gnorm=1.507, clip=0, loss_scale=8, train_wall=73, gb_free=17.8, wall=6172
2023-08-02 11:27:22 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 11:27:44 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.418 | trans_loss 5.686 | nll_loss 2.982 | w2v_ctc_loss 1.348 | task_loss 0 | contrastive_loss 0.248 | total 4003.4 | n_correct 2401.4 | ppl 7.9 | accuracy 59.984 | uer 18.631 | wer 20.29 | raw_wer 20.29 | bleu 18.53 | wps 2432.9 | wpb 4003.4 | bsz 141.8 | num_updates 12000 | best_bleu 19.08
2023-08-02 11:27:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12000 updates
2023-08-02 11:27:44 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_9_12000.pt
2023-08-02 11:27:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_9_12000.pt
2023-08-02 11:28:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_9_12000.pt (epoch 9 @ 12000 updates, score 18.53) (writing took 31.743914518505335 seconds)
2023-08-02 11:29:29 | INFO | train_inner | epoch 009:    316 / 1474 loss=3.046, trans_loss=2.842, nll_loss=1.518, w2v_ctc_loss=1.523, task_loss=0, contrastive_loss=0.187, total=4146.43, n_correct=2468.05, ppl=2.86, accuracy=59.522, wps=6471.9, ups=0.79, wpb=8243.5, bsz=316.4, num_updates=12100, lr=0.000128565, gnorm=1.5, clip=0, loss_scale=8, train_wall=72, gb_free=16.8, wall=6299
2023-08-02 11:30:44 | INFO | train_inner | epoch 009:    416 / 1474 loss=3.061, trans_loss=2.857, nll_loss=1.535, w2v_ctc_loss=1.546, task_loss=0, contrastive_loss=0.142, total=4194.84, n_correct=2474.34, ppl=2.9, accuracy=58.985, wps=11129.4, ups=1.34, wpb=8329.8, bsz=311.2, num_updates=12200, lr=0.000128037, gnorm=1.514, clip=0, loss_scale=8, train_wall=74, gb_free=16.4, wall=6374
2023-08-02 11:31:58 | INFO | train_inner | epoch 009:    516 / 1474 loss=3.099, trans_loss=2.865, nll_loss=1.544, w2v_ctc_loss=1.575, task_loss=0, contrastive_loss=0.218, total=4124.3, n_correct=2428.59, ppl=2.92, accuracy=58.885, wps=11068.6, ups=1.35, wpb=8186, bsz=293, num_updates=12300, lr=0.000127515, gnorm=1.544, clip=0, loss_scale=8, train_wall=74, gb_free=12.1, wall=6448
2023-08-02 11:33:12 | INFO | train_inner | epoch 009:    616 / 1474 loss=3.044, trans_loss=2.848, nll_loss=1.527, w2v_ctc_loss=1.528, task_loss=0, contrastive_loss=0.153, total=4120.96, n_correct=2440.63, ppl=2.88, accuracy=59.225, wps=11138.6, ups=1.36, wpb=8195.5, bsz=302.3, num_updates=12400, lr=0.000127, gnorm=1.51, clip=0, loss_scale=8, train_wall=73, gb_free=16.4, wall=6521
2023-08-02 11:34:25 | INFO | train_inner | epoch 009:    716 / 1474 loss=3.102, trans_loss=2.86, nll_loss=1.541, w2v_ctc_loss=1.571, task_loss=0, contrastive_loss=0.282, total=4088.53, n_correct=2408.39, ppl=2.91, accuracy=58.906, wps=11082.8, ups=1.36, wpb=8125.2, bsz=300.9, num_updates=12500, lr=0.000126491, gnorm=1.553, clip=0, loss_scale=8, train_wall=73, gb_free=17.1, wall=6595
2023-08-02 11:35:39 | INFO | train_inner | epoch 009:    816 / 1474 loss=3.139, trans_loss=2.858, nll_loss=1.541, w2v_ctc_loss=1.561, task_loss=0, contrastive_loss=0.495, total=4220.43, n_correct=2491.22, ppl=2.91, accuracy=59.028, wps=11288.8, ups=1.35, wpb=8390.7, bsz=334.1, num_updates=12600, lr=0.000125988, gnorm=1.562, clip=0, loss_scale=8, train_wall=74, gb_free=14.6, wall=6669
2023-08-02 11:36:54 | INFO | train_inner | epoch 009:    916 / 1474 loss=3.103, trans_loss=2.858, nll_loss=1.534, w2v_ctc_loss=1.543, task_loss=0, contrastive_loss=0.473, total=4146.05, n_correct=2452.22, ppl=2.9, accuracy=59.146, wps=10966.3, ups=1.33, wpb=8225.5, bsz=300.2, num_updates=12700, lr=0.000125491, gnorm=1.526, clip=0, loss_scale=8, train_wall=75, gb_free=17.9, wall=6744
2023-08-02 11:38:08 | INFO | train_inner | epoch 009:   1016 / 1474 loss=3.076, trans_loss=2.868, nll_loss=1.549, w2v_ctc_loss=1.562, task_loss=0, contrastive_loss=0.142, total=4101.48, n_correct=2407.46, ppl=2.93, accuracy=58.697, wps=11075.5, ups=1.36, wpb=8140.2, bsz=282.9, num_updates=12800, lr=0.000125, gnorm=1.509, clip=0, loss_scale=8, train_wall=73, gb_free=16.1, wall=6818
2023-08-02 11:39:22 | INFO | train_inner | epoch 009:   1116 / 1474 loss=3.071, trans_loss=2.864, nll_loss=1.54, w2v_ctc_loss=1.547, task_loss=0, contrastive_loss=0.173, total=4179.09, n_correct=2471.8, ppl=2.91, accuracy=59.147, wps=11227.9, ups=1.36, wpb=8278.6, bsz=316.5, num_updates=12900, lr=0.000124515, gnorm=1.524, clip=0, loss_scale=16, train_wall=73, gb_free=15.5, wall=6891
2023-08-02 11:40:36 | INFO | train_inner | epoch 009:   1216 / 1474 loss=3.071, trans_loss=2.86, nll_loss=1.54, w2v_ctc_loss=1.559, task_loss=0, contrastive_loss=0.148, total=4140.66, n_correct=2440.17, ppl=2.91, accuracy=58.932, wps=11010.6, ups=1.34, wpb=8222.8, bsz=298.7, num_updates=13000, lr=0.000124035, gnorm=1.514, clip=0, loss_scale=16, train_wall=74, gb_free=17.2, wall=6966
2023-08-02 11:41:50 | INFO | train_inner | epoch 009:   1316 / 1474 loss=3.091, trans_loss=2.857, nll_loss=1.535, w2v_ctc_loss=1.523, task_loss=0, contrastive_loss=0.43, total=4204.43, n_correct=2492.71, ppl=2.9, accuracy=59.288, wps=11362.3, ups=1.36, wpb=8340.5, bsz=328.3, num_updates=13100, lr=0.00012356, gnorm=1.505, clip=0, loss_scale=16, train_wall=73, gb_free=17.9, wall=7039
2023-08-02 11:43:04 | INFO | train_inner | epoch 009:   1416 / 1474 loss=3.076, trans_loss=2.871, nll_loss=1.553, w2v_ctc_loss=1.564, task_loss=0, contrastive_loss=0.121, total=4069.19, n_correct=2387.88, ppl=2.93, accuracy=58.682, wps=10939.4, ups=1.35, wpb=8074, bsz=285.2, num_updates=13200, lr=0.000123091, gnorm=1.527, clip=0, loss_scale=16, train_wall=73, gb_free=16.9, wall=7113
2023-08-02 11:43:45 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2023-08-02 11:44:07 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.389 | trans_loss 5.659 | nll_loss 2.95 | w2v_ctc_loss 1.318 | task_loss 0 | contrastive_loss 0.241 | total 4003.4 | n_correct 2418.7 | ppl 7.73 | accuracy 60.416 | uer 17.798 | wer 19.537 | raw_wer 19.537 | bleu 18.89 | wps 2414.8 | wpb 4003.4 | bsz 141.8 | num_updates 13258 | best_bleu 19.08
2023-08-02 11:44:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 13258 updates
2023-08-02 11:44:07 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_18.8908.pt
2023-08-02 11:44:11 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_18.8908.pt
2023-08-02 11:44:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_18.8908.pt (epoch 9 @ 13258 updates, score 18.89) (writing took 17.51734489016235 seconds)
2023-08-02 11:44:25 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-08-02 11:44:25 | INFO | train | epoch 009 | loss 3.079 | trans_loss 2.858 | nll_loss 1.537 | w2v_ctc_loss 1.549 | task_loss 0 | contrastive_loss 0.242 | total 4138.65 | n_correct 2445.21 | ppl 2.9 | accuracy 59.082 | wps 10176.1 | ups 1.24 | wpb 8217.2 | bsz 305.7 | num_updates 13258 | lr 0.000122822 | gnorm 1.523 | clip 0 | loss_scale 16 | train_wall 1081 | gb_free 12 | wall 7195
2023-08-02 11:44:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 11:44:25 | INFO | fairseq.trainer | begin training epoch 10
2023-08-02 11:44:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 11:45:04 | INFO | train_inner | epoch 010:     42 / 1474 loss=3.06, trans_loss=2.848, nll_loss=1.523, w2v_ctc_loss=1.521, task_loss=0, contrastive_loss=0.259, total=4100.8, n_correct=2442.5, ppl=2.87, accuracy=59.562, wps=6754.8, ups=0.83, wpb=8137.4, bsz=313, num_updates=13300, lr=0.000122628, gnorm=1.551, clip=0, loss_scale=16, train_wall=72, gb_free=16.5, wall=7234
2023-08-02 11:46:18 | INFO | train_inner | epoch 010:    142 / 1474 loss=2.985, trans_loss=2.827, nll_loss=1.497, w2v_ctc_loss=1.466, task_loss=0, contrastive_loss=0.144, total=4247.35, n_correct=2548.57, ppl=2.82, accuracy=60.004, wps=11386.8, ups=1.35, wpb=8437.1, bsz=319.7, num_updates=13400, lr=0.000122169, gnorm=1.476, clip=0, loss_scale=16, train_wall=74, gb_free=12.1, wall=7308
2023-08-02 11:47:31 | INFO | train_inner | epoch 010:    242 / 1474 loss=3.028, trans_loss=2.833, nll_loss=1.502, w2v_ctc_loss=1.486, task_loss=0, contrastive_loss=0.331, total=4122.82, n_correct=2471.85, ppl=2.83, accuracy=59.955, wps=11194.7, ups=1.37, wpb=8180.5, bsz=307.6, num_updates=13500, lr=0.000121716, gnorm=1.547, clip=0, loss_scale=16, train_wall=73, gb_free=16.5, wall=7381
2023-08-02 11:47:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-02 11:48:46 | INFO | train_inner | epoch 010:    343 / 1474 loss=2.987, trans_loss=2.823, nll_loss=1.496, w2v_ctc_loss=1.479, task_loss=0, contrastive_loss=0.112, total=4136.39, n_correct=2483.5, ppl=2.82, accuracy=60.04, wps=10965.5, ups=1.33, wpb=8228.4, bsz=300.9, num_updates=13600, lr=0.000121268, gnorm=1.504, clip=0, loss_scale=8, train_wall=75, gb_free=15.8, wall=7456
2023-08-02 11:49:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-02 11:50:02 | INFO | train_inner | epoch 010:    444 / 1474 loss=2.997, trans_loss=2.836, nll_loss=1.508, w2v_ctc_loss=1.478, task_loss=0, contrastive_loss=0.137, total=4169.37, n_correct=2493.66, ppl=2.84, accuracy=59.809, wps=10963.1, ups=1.32, wpb=8277.8, bsz=309.4, num_updates=13700, lr=0.000120824, gnorm=1.498, clip=0, loss_scale=4, train_wall=75, gb_free=15.7, wall=7531
2023-08-02 11:51:16 | INFO | train_inner | epoch 010:    544 / 1474 loss=3.034, trans_loss=2.85, nll_loss=1.522, w2v_ctc_loss=1.519, task_loss=0, contrastive_loss=0.129, total=4097.61, n_correct=2437.44, ppl=2.87, accuracy=59.484, wps=10987.5, ups=1.35, wpb=8121.6, bsz=289.7, num_updates=13800, lr=0.000120386, gnorm=1.529, clip=0, loss_scale=4, train_wall=73, gb_free=18, wall=7605
2023-08-02 11:52:30 | INFO | train_inner | epoch 010:    644 / 1474 loss=3.052, trans_loss=2.841, nll_loss=1.515, w2v_ctc_loss=1.51, task_loss=0, contrastive_loss=0.301, total=4187.04, n_correct=2502.8, ppl=2.86, accuracy=59.775, wps=11253.1, ups=1.35, wpb=8307.9, bsz=322.4, num_updates=13900, lr=0.000119952, gnorm=1.535, clip=0, loss_scale=4, train_wall=73, gb_free=16.7, wall=7679
2023-08-02 11:53:43 | INFO | train_inner | epoch 010:    744 / 1474 loss=3.033, trans_loss=2.843, nll_loss=1.517, w2v_ctc_loss=1.525, task_loss=0, contrastive_loss=0.127, total=4112.31, n_correct=2450.77, ppl=2.86, accuracy=59.596, wps=11172.7, ups=1.37, wpb=8165.1, bsz=298.9, num_updates=14000, lr=0.000119523, gnorm=1.545, clip=0, loss_scale=4, train_wall=73, gb_free=12.8, wall=7752
2023-08-02 11:53:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 11:54:05 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.389 | trans_loss 5.65 | nll_loss 2.938 | w2v_ctc_loss 1.341 | task_loss 0 | contrastive_loss 0.237 | total 4003.4 | n_correct 2422.1 | ppl 7.66 | accuracy 60.501 | uer 18.1 | wer 19.664 | raw_wer 19.664 | bleu 19.01 | wps 2349.3 | wpb 4003.4 | bsz 141.8 | num_updates 14000 | best_bleu 19.08
2023-08-02 11:54:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14000 updates
2023-08-02 11:54:05 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_10_14000.pt
2023-08-02 11:54:09 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_10_14000.pt
2023-08-02 11:54:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_10_14000.pt (epoch 10 @ 14000 updates, score 19.01) (writing took 32.80232532508671 seconds)
2023-08-02 11:55:52 | INFO | train_inner | epoch 010:    844 / 1474 loss=3, trans_loss=2.832, nll_loss=1.504, w2v_ctc_loss=1.481, task_loss=0, contrastive_loss=0.133, total=4135.95, n_correct=2476.5, ppl=2.84, accuracy=59.877, wps=6360.5, ups=0.77, wpb=8210.8, bsz=304.6, num_updates=14100, lr=0.000119098, gnorm=1.495, clip=0, loss_scale=4, train_wall=73, gb_free=16.1, wall=7881
2023-08-02 11:57:05 | INFO | train_inner | epoch 010:    944 / 1474 loss=3.035, trans_loss=2.84, nll_loss=1.513, w2v_ctc_loss=1.508, task_loss=0, contrastive_loss=0.19, total=4169.57, n_correct=2492.84, ppl=2.85, accuracy=59.787, wps=11320.5, ups=1.37, wpb=8269.4, bsz=315.5, num_updates=14200, lr=0.000118678, gnorm=1.526, clip=0, loss_scale=4, train_wall=72, gb_free=13.9, wall=7954
2023-08-02 11:58:18 | INFO | train_inner | epoch 010:   1044 / 1474 loss=3.022, trans_loss=2.844, nll_loss=1.519, w2v_ctc_loss=1.506, task_loss=0, contrastive_loss=0.145, total=4058.1, n_correct=2410.52, ppl=2.87, accuracy=59.4, wps=10948.9, ups=1.36, wpb=8057.1, bsz=286.9, num_updates=14300, lr=0.000118262, gnorm=1.528, clip=0, loss_scale=4, train_wall=73, gb_free=16.3, wall=8028
2023-08-02 11:59:32 | INFO | train_inner | epoch 010:   1144 / 1474 loss=3.031, trans_loss=2.85, nll_loss=1.528, w2v_ctc_loss=1.524, task_loss=0, contrastive_loss=0.122, total=4034.12, n_correct=2390.73, ppl=2.88, accuracy=59.263, wps=10921.6, ups=1.36, wpb=8010.7, bsz=278.8, num_updates=14400, lr=0.000117851, gnorm=1.52, clip=0, loss_scale=4, train_wall=73, gb_free=15.8, wall=8101
2023-08-02 12:00:45 | INFO | train_inner | epoch 010:   1244 / 1474 loss=3.015, trans_loss=2.833, nll_loss=1.511, w2v_ctc_loss=1.511, task_loss=0, contrastive_loss=0.121, total=4107.11, n_correct=2446.96, ppl=2.85, accuracy=59.579, wps=11096.8, ups=1.36, wpb=8176.4, bsz=297.6, num_updates=14500, lr=0.000117444, gnorm=1.538, clip=0, loss_scale=4, train_wall=73, gb_free=15.4, wall=8175
2023-08-02 12:01:59 | INFO | train_inner | epoch 010:   1344 / 1474 loss=3.02, trans_loss=2.84, nll_loss=1.516, w2v_ctc_loss=1.509, task_loss=0, contrastive_loss=0.137, total=4143.63, n_correct=2468.68, ppl=2.86, accuracy=59.578, wps=11171.1, ups=1.36, wpb=8229.5, bsz=304.7, num_updates=14600, lr=0.000117041, gnorm=1.537, clip=0, loss_scale=4, train_wall=73, gb_free=16.9, wall=8249
2023-08-02 12:03:13 | INFO | train_inner | epoch 010:   1444 / 1474 loss=3.075, trans_loss=2.846, nll_loss=1.52, w2v_ctc_loss=1.482, task_loss=0, contrastive_loss=0.508, total=4183.87, n_correct=2491.89, ppl=2.87, accuracy=59.559, wps=11210.5, ups=1.35, wpb=8296.3, bsz=320.5, num_updates=14700, lr=0.000116642, gnorm=1.583, clip=0, loss_scale=4, train_wall=74, gb_free=13.6, wall=8323
2023-08-02 12:03:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 12:03:57 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.392 | trans_loss 5.64 | nll_loss 2.927 | w2v_ctc_loss 1.369 | task_loss 0 | contrastive_loss 0.248 | total 4003.4 | n_correct 2423.2 | ppl 7.61 | accuracy 60.529 | uer 17.657 | wer 19.455 | raw_wer 19.455 | bleu 19.23 | wps 2397 | wpb 4003.4 | bsz 141.8 | num_updates 14730 | best_bleu 19.23
2023-08-02 12:03:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14730 updates
2023-08-02 12:03:57 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 12:04:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 12:04:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt (epoch 10 @ 14730 updates, score 19.23) (writing took 24.40391101129353 seconds)
2023-08-02 12:04:22 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-08-02 12:04:22 | INFO | train | epoch 010 | loss 3.023 | trans_loss 2.838 | nll_loss 1.512 | w2v_ctc_loss 1.497 | task_loss 0 | contrastive_loss 0.205 | total 4136.42 | n_correct 2469.6 | ppl 2.85 | accuracy 59.704 | wps 10102.8 | ups 1.23 | wpb 8212.8 | bsz 304.7 | num_updates 14730 | lr 0.000116524 | gnorm 1.527 | clip 0 | loss_scale 4 | train_wall 1079 | gb_free 17.4 | wall 8391
2023-08-02 12:04:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 12:04:22 | INFO | fairseq.trainer | begin training epoch 11
2023-08-02 12:04:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 12:05:21 | INFO | train_inner | epoch 011:     70 / 1474 loss=2.991, trans_loss=2.82, nll_loss=1.488, w2v_ctc_loss=1.461, task_loss=0, contrastive_loss=0.25, total=4162.14, n_correct=2507.5, ppl=2.81, accuracy=60.245, wps=6479, ups=0.78, wpb=8263.2, bsz=316.2, num_updates=14800, lr=0.000116248, gnorm=1.52, clip=0, loss_scale=4, train_wall=72, gb_free=16.7, wall=8450
2023-08-02 12:06:35 | INFO | train_inner | epoch 011:    170 / 1474 loss=2.96, trans_loss=2.814, nll_loss=1.482, w2v_ctc_loss=1.446, task_loss=0, contrastive_loss=0.133, total=4103.74, n_correct=2477.47, ppl=2.79, accuracy=60.371, wps=11043, ups=1.35, wpb=8157.6, bsz=300.3, num_updates=14900, lr=0.000115857, gnorm=1.505, clip=0, loss_scale=4, train_wall=73, gb_free=17, wall=8524
2023-08-02 12:07:48 | INFO | train_inner | epoch 011:    270 / 1474 loss=2.961, trans_loss=2.819, nll_loss=1.487, w2v_ctc_loss=1.45, task_loss=0, contrastive_loss=0.115, total=4114.56, n_correct=2476.43, ppl=2.8, accuracy=60.187, wps=11183.3, ups=1.37, wpb=8173.7, bsz=295.4, num_updates=15000, lr=0.00011547, gnorm=1.51, clip=0, loss_scale=4, train_wall=73, gb_free=15.8, wall=8597
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:0')
2023-08-02 12:08:39 | INFO | train_inner | epoch 011:    370 / 1474 loss=4.341, trans_loss=5.544, nll_loss=2.918, w2v_ctc_loss=1.429, task_loss=0, contrastive_loss=0.126, total=4094.93, n_correct=2473.42, ppl=7.56, accuracy=60.402, wps=8037.7, ups=1.95, wpb=4128.5, bsz=148.9, num_updates=15100, lr=0.000115087, gnorm=0.955, clip=0, loss_scale=4, train_wall=51, gb_free=12.8, wall=8649
2023-08-02 12:09:30 | INFO | train_inner | epoch 011:    470 / 1474 loss=4.399, trans_loss=5.608, nll_loss=2.967, w2v_ctc_loss=1.44, task_loss=0, contrastive_loss=0.372, total=4113.98, n_correct=2473.88, ppl=7.82, accuracy=60.133, wps=8021.6, ups=1.95, wpb=4114, bsz=151.2, num_updates=15200, lr=0.000114708, gnorm=0.99, clip=0, loss_scale=4, train_wall=51, gb_free=10.8, wall=8700
2023-08-02 12:10:22 | INFO | train_inner | epoch 011:    570 / 1474 loss=4.393, trans_loss=5.598, nll_loss=2.954, w2v_ctc_loss=1.455, task_loss=0, contrastive_loss=0.36, total=4074.83, n_correct=2453.47, ppl=7.75, accuracy=60.21, wps=7943.4, ups=1.95, wpb=4074.8, bsz=146.6, num_updates=15300, lr=0.000114332, gnorm=0.972, clip=0, loss_scale=4, train_wall=51, gb_free=15.7, wall=8751
2023-08-02 12:11:13 | INFO | train_inner | epoch 011:    670 / 1474 loss=4.408, trans_loss=5.605, nll_loss=2.965, w2v_ctc_loss=1.456, task_loss=0, contrastive_loss=0.482, total=4161.4, n_correct=2502.75, ppl=7.81, accuracy=60.142, wps=8039.2, ups=1.93, wpb=4161.4, bsz=156, num_updates=15400, lr=0.000113961, gnorm=0.984, clip=0, loss_scale=4, train_wall=51, gb_free=17, wall=8803
2023-08-02 12:12:05 | INFO | train_inner | epoch 011:    770 / 1474 loss=4.396, trans_loss=5.609, nll_loss=2.97, w2v_ctc_loss=1.486, task_loss=0, contrastive_loss=0.128, total=4153.23, n_correct=2495.52, ppl=7.83, accuracy=60.086, wps=8065.2, ups=1.94, wpb=4153.2, bsz=150.7, num_updates=15500, lr=0.000113592, gnorm=1.018, clip=0, loss_scale=4, train_wall=51, gb_free=15.4, wall=8854
2023-08-02 12:12:56 | INFO | train_inner | epoch 011:    870 / 1474 loss=4.393, trans_loss=5.608, nll_loss=2.968, w2v_ctc_loss=1.479, task_loss=0, contrastive_loss=0.116, total=4122.58, n_correct=2470.84, ppl=7.83, accuracy=59.934, wps=8097.6, ups=1.96, wpb=4122.6, bsz=146.9, num_updates=15600, lr=0.000113228, gnorm=0.98, clip=0, loss_scale=4, train_wall=50, gb_free=16.2, wall=8905
2023-08-02 12:13:47 | INFO | train_inner | epoch 011:    970 / 1474 loss=4.379, trans_loss=5.591, nll_loss=2.948, w2v_ctc_loss=1.468, task_loss=0, contrastive_loss=0.137, total=4150.11, n_correct=2502.29, ppl=7.72, accuracy=60.295, wps=8090, ups=1.95, wpb=4150.1, bsz=152.4, num_updates=15700, lr=0.000112867, gnorm=0.975, clip=0, loss_scale=8, train_wall=51, gb_free=16, wall=8957
2023-08-02 12:14:38 | INFO | train_inner | epoch 011:   1070 / 1474 loss=4.389, trans_loss=5.593, nll_loss=2.953, w2v_ctc_loss=1.484, task_loss=0, contrastive_loss=0.168, total=4146.14, n_correct=2501.4, ppl=7.74, accuracy=60.331, wps=8154, ups=1.97, wpb=4146.1, bsz=154.9, num_updates=15800, lr=0.000112509, gnorm=0.971, clip=0, loss_scale=8, train_wall=50, gb_free=17.1, wall=9008
2023-08-02 12:15:29 | INFO | train_inner | epoch 011:   1170 / 1474 loss=4.395, trans_loss=5.601, nll_loss=2.962, w2v_ctc_loss=1.488, task_loss=0, contrastive_loss=0.162, total=4182.57, n_correct=2514.41, ppl=7.79, accuracy=60.116, wps=8151.6, ups=1.95, wpb=4182.6, bsz=155.7, num_updates=15900, lr=0.000112154, gnorm=0.979, clip=0, loss_scale=8, train_wall=51, gb_free=17.4, wall=9059
2023-08-02 12:16:21 | INFO | train_inner | epoch 011:   1270 / 1474 loss=4.404, trans_loss=5.599, nll_loss=2.96, w2v_ctc_loss=1.497, task_loss=0, contrastive_loss=0.257, total=4157.11, n_correct=2503.86, ppl=7.78, accuracy=60.231, wps=7977.9, ups=1.92, wpb=4157.1, bsz=154.3, num_updates=16000, lr=0.000111803, gnorm=0.98, clip=0, loss_scale=8, train_wall=52, gb_free=17.1, wall=9111
2023-08-02 12:16:21 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:5')
2023-08-02 12:16:43 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.385 | trans_loss 5.621 | nll_loss 2.91 | w2v_ctc_loss 1.391 | task_loss 0 | contrastive_loss 0.249 | total 4003.4 | n_correct 2436.3 | ppl 7.52 | accuracy 60.856 | uer 17.678 | wer 19.328 | raw_wer 19.328 | bleu 19.1 | wps 2374.6 | wpb 4003.4 | bsz 141.8 | num_updates 16000 | best_bleu 19.23
2023-08-02 12:16:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16000 updates
2023-08-02 12:16:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_11_16000.pt
2023-08-02 12:16:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_11_16000.pt
2023-08-02 12:17:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_11_16000.pt (epoch 11 @ 16000 updates, score 19.1) (writing took 29.276407681405544 seconds)
2023-08-02 12:18:05 | INFO | train_inner | epoch 011:   1370 / 1474 loss=4.408, trans_loss=5.59, nll_loss=2.95, w2v_ctc_loss=1.457, task_loss=0, contrastive_loss=0.602, total=4192.31, n_correct=2526.5, ppl=7.73, accuracy=60.265, wps=4039.9, ups=0.96, wpb=4192.3, bsz=164.2, num_updates=16100, lr=0.000111456, gnorm=0.971, clip=0, loss_scale=8, train_wall=51, gb_free=17.3, wall=9215
2023-08-02 12:18:56 | INFO | train_inner | epoch 011:   1470 / 1474 loss=4.379, trans_loss=5.59, nll_loss=2.949, w2v_ctc_loss=1.465, task_loss=0, contrastive_loss=0.153, total=4162.97, n_correct=2511.74, ppl=7.72, accuracy=60.335, wps=8124.9, ups=1.95, wpb=4163, bsz=156.5, num_updates=16200, lr=0.000111111, gnorm=0.969, clip=0, loss_scale=8, train_wall=51, gb_free=15.3, wall=9266
2023-08-02 12:18:58 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 12:19:21 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.354 | trans_loss 5.619 | nll_loss 2.905 | w2v_ctc_loss 1.29 | task_loss 0 | contrastive_loss 0.251 | total 4003.4 | n_correct 2432.2 | ppl 7.49 | accuracy 60.753 | uer 17.846 | wer 19.526 | raw_wer 19.526 | bleu 19.34 | wps 2314.9 | wpb 4003.4 | bsz 141.8 | num_updates 16204 | best_bleu 19.34
2023-08-02 12:19:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16204 updates
2023-08-02 12:19:21 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 12:19:34 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 12:19:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt (epoch 11 @ 16204 updates, score 19.34) (writing took 23.940637661144137 seconds)
2023-08-02 12:19:46 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-08-02 12:19:46 | INFO | train | epoch 011 | loss 3.951 | trans_loss 4.74 | nll_loss 2.502 | w2v_ctc_loss 1.462 | task_loss 0 | contrastive_loss 0.216 | total 4138.65 | n_correct 2492.65 | ppl 5.67 | accuracy 60.229 | wps 7797.8 | ups 1.6 | wpb 4887.3 | bsz 180.7 | num_updates 16204 | lr 0.000111097 | gnorm 1.076 | clip 0 | loss_scale 8 | train_wall 809 | gb_free 17.6 | wall 9315
2023-08-02 12:19:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 12:19:46 | INFO | fairseq.trainer | begin training epoch 12
2023-08-02 12:19:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 12:20:43 | INFO | train_inner | epoch 012:     96 / 1474 loss=4.333, trans_loss=5.531, nll_loss=2.871, w2v_ctc_loss=1.434, task_loss=0, contrastive_loss=0.218, total=4145.21, n_correct=2541.63, ppl=7.31, accuracy=61.315, wps=3900.1, ups=0.94, wpb=4145.2, bsz=157, num_updates=16300, lr=0.00011077, gnorm=0.966, clip=0, loss_scale=8, train_wall=50, gb_free=17.4, wall=9372
2023-08-02 12:21:33 | INFO | train_inner | epoch 012:    196 / 1474 loss=4.335, trans_loss=5.54, nll_loss=2.881, w2v_ctc_loss=1.444, task_loss=0, contrastive_loss=0.126, total=4124.1, n_correct=2520.02, ppl=7.37, accuracy=61.105, wps=8111.7, ups=1.97, wpb=4124.1, bsz=148.1, num_updates=16400, lr=0.000110432, gnorm=0.968, clip=0, loss_scale=8, train_wall=50, gb_free=15.9, wall=9423
2023-08-02 12:22:25 | INFO | train_inner | epoch 012:    296 / 1474 loss=4.331, trans_loss=5.534, nll_loss=2.876, w2v_ctc_loss=1.423, task_loss=0, contrastive_loss=0.18, total=4208.07, n_correct=2573.85, ppl=7.34, accuracy=61.165, wps=8134.2, ups=1.93, wpb=4208.1, bsz=160.9, num_updates=16500, lr=0.000110096, gnorm=0.964, clip=0, loss_scale=8, train_wall=51, gb_free=12.9, wall=9475
2023-08-02 12:23:17 | INFO | train_inner | epoch 012:    396 / 1474 loss=4.337, trans_loss=5.542, nll_loss=2.885, w2v_ctc_loss=1.44, task_loss=0, contrastive_loss=0.148, total=4144.42, n_correct=2530, ppl=7.39, accuracy=61.046, wps=8001.8, ups=1.93, wpb=4144.4, bsz=152.7, num_updates=16600, lr=0.000109764, gnorm=0.97, clip=0, loss_scale=8, train_wall=51, gb_free=16.9, wall=9527
2023-08-02 12:24:08 | INFO | train_inner | epoch 012:    496 / 1474 loss=4.367, trans_loss=5.566, nll_loss=2.917, w2v_ctc_loss=1.477, task_loss=0, contrastive_loss=0.166, total=4095.26, n_correct=2484.7, ppl=7.56, accuracy=60.673, wps=8053.3, ups=1.97, wpb=4095.3, bsz=149.9, num_updates=16700, lr=0.000109435, gnorm=0.976, clip=0, loss_scale=8, train_wall=50, gb_free=18, wall=9578
2023-08-02 12:24:59 | INFO | train_inner | epoch 012:    596 / 1474 loss=4.35, trans_loss=5.542, nll_loss=2.888, w2v_ctc_loss=1.443, task_loss=0, contrastive_loss=0.29, total=4204.6, n_correct=2563.52, ppl=7.4, accuracy=60.969, wps=8180.1, ups=1.95, wpb=4204.6, bsz=159.6, num_updates=16800, lr=0.000109109, gnorm=0.974, clip=0, loss_scale=8, train_wall=51, gb_free=16.8, wall=9629
2023-08-02 12:25:50 | INFO | train_inner | epoch 012:    696 / 1474 loss=4.348, trans_loss=5.535, nll_loss=2.879, w2v_ctc_loss=1.42, task_loss=0, contrastive_loss=0.47, total=4197.19, n_correct=2567.85, ppl=7.36, accuracy=61.18, wps=8225.4, ups=1.96, wpb=4197.2, bsz=161.6, num_updates=16900, lr=0.000108786, gnorm=0.959, clip=0, loss_scale=8, train_wall=51, gb_free=17, wall=9680
2023-08-02 12:26:42 | INFO | train_inner | epoch 012:    796 / 1474 loss=4.336, trans_loss=5.538, nll_loss=2.882, w2v_ctc_loss=1.445, task_loss=0, contrastive_loss=0.146, total=4094.06, n_correct=2503.83, ppl=7.37, accuracy=61.158, wps=7996, ups=1.95, wpb=4094.1, bsz=149.1, num_updates=17000, lr=0.000108465, gnorm=0.981, clip=0, loss_scale=8, train_wall=51, gb_free=15.2, wall=9731
2023-08-02 12:27:33 | INFO | train_inner | epoch 012:    896 / 1474 loss=4.357, trans_loss=5.552, nll_loss=2.9, w2v_ctc_loss=1.455, task_loss=0, contrastive_loss=0.243, total=4163.5, n_correct=2536.38, ppl=7.46, accuracy=60.919, wps=8038.6, ups=1.93, wpb=4163.5, bsz=152.5, num_updates=17100, lr=0.000108148, gnorm=0.976, clip=0, loss_scale=8, train_wall=51, gb_free=13.4, wall=9783
2023-08-02 12:28:24 | INFO | train_inner | epoch 012:    996 / 1474 loss=4.359, trans_loss=5.555, nll_loss=2.906, w2v_ctc_loss=1.456, task_loss=0, contrastive_loss=0.271, total=4124.99, n_correct=2510.62, ppl=7.49, accuracy=60.864, wps=8077.3, ups=1.96, wpb=4125, bsz=151.3, num_updates=17200, lr=0.000107833, gnorm=0.988, clip=0, loss_scale=8, train_wall=51, gb_free=11.8, wall=9834
2023-08-02 12:29:16 | INFO | train_inner | epoch 012:   1096 / 1474 loss=4.373, trans_loss=5.562, nll_loss=2.914, w2v_ctc_loss=1.466, task_loss=0, contrastive_loss=0.356, total=4046.6, n_correct=2461.51, ppl=7.54, accuracy=60.829, wps=7910.4, ups=1.95, wpb=4046.6, bsz=144.9, num_updates=17300, lr=0.000107521, gnorm=0.99, clip=0, loss_scale=8, train_wall=51, gb_free=17, wall=9885
2023-08-02 12:30:07 | INFO | train_inner | epoch 012:   1196 / 1474 loss=4.387, trans_loss=5.573, nll_loss=2.931, w2v_ctc_loss=1.488, task_loss=0, contrastive_loss=0.291, total=4196.85, n_correct=2543.88, ppl=7.62, accuracy=60.614, wps=8185.7, ups=1.95, wpb=4196.9, bsz=159.5, num_updates=17400, lr=0.000107211, gnorm=0.99, clip=0, loss_scale=8, train_wall=51, gb_free=17, wall=9936
2023-08-02 12:30:58 | INFO | train_inner | epoch 012:   1296 / 1474 loss=4.36, trans_loss=5.559, nll_loss=2.912, w2v_ctc_loss=1.482, task_loss=0, contrastive_loss=0.123, total=4067.78, n_correct=2473.8, ppl=7.53, accuracy=60.814, wps=7946.8, ups=1.95, wpb=4067.8, bsz=142.7, num_updates=17500, lr=0.000106904, gnorm=0.978, clip=0, loss_scale=8, train_wall=51, gb_free=15.7, wall=9988
2023-08-02 12:31:50 | INFO | train_inner | epoch 012:   1396 / 1474 loss=4.361, trans_loss=5.557, nll_loss=2.91, w2v_ctc_loss=1.441, task_loss=0, contrastive_loss=0.33, total=4142.88, n_correct=2519.02, ppl=7.52, accuracy=60.804, wps=8007.6, ups=1.93, wpb=4142.9, bsz=153.1, num_updates=17600, lr=0.0001066, gnorm=0.967, clip=0, loss_scale=8, train_wall=51, gb_free=16, wall=10039
2023-08-02 12:32:30 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 12:32:51 | INFO | dev_st | epoch 012 | valid on 'dev_st' subset | loss 4.356 | trans_loss 5.601 | nll_loss 2.885 | w2v_ctc_loss 1.337 | task_loss 0 | contrastive_loss 0.257 | total 4003.4 | n_correct 2448.4 | ppl 7.39 | accuracy 61.158 | uer 17.601 | wer 19.097 | raw_wer 19.097 | bleu 19.6 | wps 2468.1 | wpb 4003.4 | bsz 141.8 | num_updates 17678 | best_bleu 19.6
2023-08-02 12:32:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 17678 updates
2023-08-02 12:32:51 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 12:33:04 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 12:33:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt (epoch 12 @ 17678 updates, score 19.6) (writing took 24.15160182118416 seconds)
2023-08-02 12:33:16 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-08-02 12:33:16 | INFO | train | epoch 012 | loss 4.352 | trans_loss 5.549 | nll_loss 2.897 | w2v_ctc_loss 1.452 | task_loss 0 | contrastive_loss 0.236 | total 4138.65 | n_correct 2522.84 | ppl 7.45 | accuracy 60.958 | wps 7525.8 | ups 1.82 | wpb 4138.6 | bsz 152.8 | num_updates 17678 | lr 0.000106365 | gnorm 0.975 | clip 0 | loss_scale 8 | train_wall 749 | gb_free 13.4 | wall 10126
2023-08-02 12:33:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 12:33:16 | INFO | fairseq.trainer | begin training epoch 13
2023-08-02 12:33:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 12:33:36 | INFO | train_inner | epoch 013:     22 / 1474 loss=4.356, trans_loss=5.553, nll_loss=2.904, w2v_ctc_loss=1.479, task_loss=0, contrastive_loss=0.135, total=4097.08, n_correct=2495.29, ppl=7.49, accuracy=60.904, wps=3859.5, ups=0.94, wpb=4097.1, bsz=148.3, num_updates=17700, lr=0.000106299, gnorm=0.968, clip=0, loss_scale=8, train_wall=51, gb_free=16.8, wall=10146
2023-08-02 12:34:27 | INFO | train_inner | epoch 013:    122 / 1474 loss=4.303, trans_loss=5.501, nll_loss=2.834, w2v_ctc_loss=1.415, task_loss=0, contrastive_loss=0.165, total=4164.24, n_correct=2568.6, ppl=7.13, accuracy=61.682, wps=8171.1, ups=1.96, wpb=4164.2, bsz=151, num_updates=17800, lr=0.000106, gnorm=0.962, clip=0, loss_scale=16, train_wall=51, gb_free=17.3, wall=10197
2023-08-02 12:35:18 | INFO | train_inner | epoch 013:    222 / 1474 loss=4.342, trans_loss=5.514, nll_loss=2.854, w2v_ctc_loss=1.417, task_loss=0, contrastive_loss=0.592, total=4201.52, n_correct=2583.47, ppl=7.23, accuracy=61.489, wps=8226.3, ups=1.96, wpb=4201.5, bsz=164.3, num_updates=17900, lr=0.000105703, gnorm=0.981, clip=0, loss_scale=16, train_wall=51, gb_free=13.6, wall=10248
2023-08-02 12:36:09 | INFO | train_inner | epoch 013:    322 / 1474 loss=4.301, trans_loss=5.499, nll_loss=2.833, w2v_ctc_loss=1.423, task_loss=0, contrastive_loss=0.137, total=4102.53, n_correct=2535.42, ppl=7.12, accuracy=61.801, wps=8025.9, ups=1.96, wpb=4102.5, bsz=147, num_updates=18000, lr=0.000105409, gnorm=0.981, clip=0, loss_scale=16, train_wall=51, gb_free=16.5, wall=10299
2023-08-02 12:36:09 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 12:36:32 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.354 | trans_loss 5.606 | nll_loss 2.887 | w2v_ctc_loss 1.319 | task_loss 0 | contrastive_loss 0.254 | total 4003.4 | n_correct 2442.5 | ppl 7.4 | accuracy 61.011 | uer 17.814 | wer 19.593 | raw_wer 19.593 | bleu 19.47 | wps 2333.2 | wpb 4003.4 | bsz 141.8 | num_updates 18000 | best_bleu 19.6
2023-08-02 12:36:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 18000 updates
2023-08-02 12:36:32 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_13_18000.pt
2023-08-02 12:36:36 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_13_18000.pt
2023-08-02 12:37:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_13_18000.pt (epoch 13 @ 18000 updates, score 19.47) (writing took 31.642651207745075 seconds)
2023-08-02 12:37:55 | INFO | train_inner | epoch 013:    422 / 1474 loss=4.309, trans_loss=5.497, nll_loss=2.83, w2v_ctc_loss=1.429, task_loss=0, contrastive_loss=0.221, total=4190.45, n_correct=2590.91, ppl=7.11, accuracy=61.829, wps=3947.9, ups=0.94, wpb=4190.4, bsz=160.2, num_updates=18100, lr=0.000105118, gnorm=0.957, clip=0, loss_scale=16, train_wall=51, gb_free=16.3, wall=10405
2023-08-02 12:38:47 | INFO | train_inner | epoch 013:    522 / 1474 loss=4.329, trans_loss=5.514, nll_loss=2.854, w2v_ctc_loss=1.441, task_loss=0, contrastive_loss=0.298, total=4194.45, n_correct=2579.74, ppl=7.23, accuracy=61.504, wps=8173.7, ups=1.95, wpb=4194.4, bsz=159.5, num_updates=18200, lr=0.000104828, gnorm=0.972, clip=0, loss_scale=16, train_wall=51, gb_free=17.7, wall=10456
2023-08-02 12:39:38 | INFO | train_inner | epoch 013:    622 / 1474 loss=4.299, trans_loss=5.498, nll_loss=2.834, w2v_ctc_loss=1.419, task_loss=0, contrastive_loss=0.127, total=4158.04, n_correct=2563.08, ppl=7.13, accuracy=61.642, wps=8111, ups=1.95, wpb=4158, bsz=153.3, num_updates=18300, lr=0.000104542, gnorm=0.966, clip=0, loss_scale=16, train_wall=51, gb_free=14, wall=10507
2023-08-02 12:40:29 | INFO | train_inner | epoch 013:    722 / 1474 loss=4.328, trans_loss=5.521, nll_loss=2.862, w2v_ctc_loss=1.465, task_loss=0, contrastive_loss=0.127, total=4099.91, n_correct=2514.62, ppl=7.27, accuracy=61.334, wps=8035, ups=1.96, wpb=4099.9, bsz=142.8, num_updates=18400, lr=0.000104257, gnorm=0.984, clip=0, loss_scale=16, train_wall=51, gb_free=17.1, wall=10558
2023-08-02 12:41:20 | INFO | train_inner | epoch 013:    822 / 1474 loss=4.324, trans_loss=5.517, nll_loss=2.858, w2v_ctc_loss=1.432, task_loss=0, contrastive_loss=0.216, total=4122.78, n_correct=2530.93, ppl=7.25, accuracy=61.389, wps=8032.3, ups=1.95, wpb=4122.8, bsz=153, num_updates=18500, lr=0.000103975, gnorm=0.979, clip=0, loss_scale=16, train_wall=51, gb_free=15.4, wall=10610
2023-08-02 12:42:11 | INFO | train_inner | epoch 013:    922 / 1474 loss=4.314, trans_loss=5.512, nll_loss=2.852, w2v_ctc_loss=1.433, task_loss=0, contrastive_loss=0.145, total=4102.59, n_correct=2524.14, ppl=7.22, accuracy=61.526, wps=8011, ups=1.95, wpb=4102.6, bsz=148.3, num_updates=18600, lr=0.000103695, gnorm=0.983, clip=0, loss_scale=16, train_wall=51, gb_free=17.7, wall=10661
2023-08-02 12:43:03 | INFO | train_inner | epoch 013:   1022 / 1474 loss=4.34, trans_loss=5.526, nll_loss=2.87, w2v_ctc_loss=1.46, task_loss=0, contrastive_loss=0.246, total=4087.8, n_correct=2502.96, ppl=7.31, accuracy=61.23, wps=7996.6, ups=1.96, wpb=4087.8, bsz=146.8, num_updates=18700, lr=0.000103418, gnorm=1.003, clip=0, loss_scale=16, train_wall=51, gb_free=17.1, wall=10712
2023-08-02 12:43:54 | INFO | train_inner | epoch 013:   1122 / 1474 loss=4.318, trans_loss=5.509, nll_loss=2.849, w2v_ctc_loss=1.435, task_loss=0, contrastive_loss=0.214, total=4098.77, n_correct=2522.45, ppl=7.2, accuracy=61.542, wps=7996.4, ups=1.95, wpb=4098.8, bsz=152.4, num_updates=18800, lr=0.000103142, gnorm=0.964, clip=0, loss_scale=16, train_wall=51, gb_free=14.2, wall=10763
2023-08-02 12:44:45 | INFO | train_inner | epoch 013:   1222 / 1474 loss=4.328, trans_loss=5.523, nll_loss=2.867, w2v_ctc_loss=1.458, task_loss=0, contrastive_loss=0.132, total=4115.57, n_correct=2524.19, ppl=7.29, accuracy=61.333, wps=8047.5, ups=1.96, wpb=4115.6, bsz=147.9, num_updates=18900, lr=0.000102869, gnorm=0.978, clip=0, loss_scale=16, train_wall=51, gb_free=15.6, wall=10815
2023-08-02 12:45:36 | INFO | train_inner | epoch 013:   1322 / 1474 loss=4.318, trans_loss=5.504, nll_loss=2.844, w2v_ctc_loss=1.426, task_loss=0, contrastive_loss=0.323, total=4111.02, n_correct=2535.16, ppl=7.18, accuracy=61.667, wps=8059, ups=1.96, wpb=4111, bsz=153.9, num_updates=19000, lr=0.000102598, gnorm=0.976, clip=0, loss_scale=16, train_wall=51, gb_free=18.2, wall=10866
2023-08-02 12:46:27 | INFO | train_inner | epoch 013:   1422 / 1474 loss=4.333, trans_loss=5.519, nll_loss=2.863, w2v_ctc_loss=1.426, task_loss=0, contrastive_loss=0.35, total=4179.06, n_correct=2564.41, ppl=7.28, accuracy=61.363, wps=8134.1, ups=1.95, wpb=4179.1, bsz=156, num_updates=19100, lr=0.000102329, gnorm=0.989, clip=0, loss_scale=16, train_wall=51, gb_free=16.5, wall=10917
2023-08-02 12:46:54 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 12:47:16 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.362 | trans_loss 5.594 | nll_loss 2.877 | w2v_ctc_loss 1.374 | task_loss 0 | contrastive_loss 0.251 | total 4003.4 | n_correct 2451.4 | ppl 7.35 | accuracy 61.233 | uer 17.952 | wer 19.802 | raw_wer 19.802 | bleu 19.41 | wps 2282.1 | wpb 4003.4 | bsz 141.8 | num_updates 19152 | best_bleu 19.6
2023-08-02 12:47:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 19152 updates
2023-08-02 12:47:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_19.4106.pt
2023-08-02 12:47:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_19.4106.pt
2023-08-02 12:47:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_19.4106.pt (epoch 13 @ 19152 updates, score 19.41) (writing took 23.312579918652773 seconds)
2023-08-02 12:47:40 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-08-02 12:47:40 | INFO | train | epoch 013 | loss 4.32 | trans_loss 5.511 | nll_loss 2.85 | w2v_ctc_loss 1.435 | task_loss 0 | contrastive_loss 0.235 | total 4138.65 | n_correct 2546.57 | ppl 7.21 | accuracy 61.532 | wps 7061.3 | ups 1.71 | wpb 4138.6 | bsz 152.8 | num_updates 19152 | lr 0.00010219 | gnorm 0.977 | clip 0 | loss_scale 16 | train_wall 748 | gb_free 18 | wall 10990
2023-08-02 12:47:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 12:47:40 | INFO | fairseq.trainer | begin training epoch 14
2023-08-02 12:47:40 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 12:48:13 | INFO | train_inner | epoch 014:     48 / 1474 loss=4.278, trans_loss=5.467, nll_loss=2.797, w2v_ctc_loss=1.411, task_loss=0, contrastive_loss=0.155, total=4179.66, n_correct=2603.68, ppl=6.95, accuracy=62.294, wps=3969.1, ups=0.95, wpb=4179.7, bsz=161, num_updates=19200, lr=0.000102062, gnorm=0.972, clip=0, loss_scale=16, train_wall=51, gb_free=11.3, wall=11022
2023-08-02 12:49:03 | INFO | train_inner | epoch 014:    148 / 1474 loss=4.267, trans_loss=5.456, nll_loss=2.779, w2v_ctc_loss=1.412, task_loss=0, contrastive_loss=0.126, total=4081.01, n_correct=2549.95, ppl=6.87, accuracy=62.483, wps=8069, ups=1.98, wpb=4081, bsz=150.3, num_updates=19300, lr=0.000101797, gnorm=0.976, clip=0, loss_scale=16, train_wall=50, gb_free=16.4, wall=11073
2023-08-02 12:49:54 | INFO | train_inner | epoch 014:    248 / 1474 loss=4.292, trans_loss=5.476, nll_loss=2.806, w2v_ctc_loss=1.408, task_loss=0, contrastive_loss=0.323, total=4109.83, n_correct=2548.69, ppl=6.99, accuracy=62.014, wps=8072.5, ups=1.96, wpb=4109.8, bsz=147.6, num_updates=19400, lr=0.000101535, gnorm=0.967, clip=0, loss_scale=16, train_wall=50, gb_free=16.2, wall=11124
2023-08-02 12:50:45 | INFO | train_inner | epoch 014:    348 / 1474 loss=4.27, trans_loss=5.461, nll_loss=2.788, w2v_ctc_loss=1.388, task_loss=0, contrastive_loss=0.192, total=4171.83, n_correct=2598.6, ppl=6.91, accuracy=62.289, wps=8143.5, ups=1.95, wpb=4171.8, bsz=159.8, num_updates=19500, lr=0.000101274, gnorm=0.966, clip=0, loss_scale=16, train_wall=51, gb_free=17.1, wall=11175
2023-08-02 12:51:36 | INFO | train_inner | epoch 014:    448 / 1474 loss=4.28, trans_loss=5.477, nll_loss=2.808, w2v_ctc_loss=1.401, task_loss=0, contrastive_loss=0.143, total=4142.75, n_correct=2571.03, ppl=7, accuracy=62.061, wps=8198.2, ups=1.98, wpb=4142.8, bsz=151.1, num_updates=19600, lr=0.000101015, gnorm=0.977, clip=0, loss_scale=16, train_wall=50, gb_free=17, wall=11226
2023-08-02 12:52:27 | INFO | train_inner | epoch 014:    548 / 1474 loss=4.308, trans_loss=5.493, nll_loss=2.827, w2v_ctc_loss=1.451, task_loss=0, contrastive_loss=0.161, total=4073.76, n_correct=2516.15, ppl=7.09, accuracy=61.765, wps=7900.8, ups=1.94, wpb=4073.8, bsz=145.4, num_updates=19700, lr=0.000100759, gnorm=0.996, clip=0, loss_scale=16, train_wall=51, gb_free=15.9, wall=11277
2023-08-02 12:53:18 | INFO | train_inner | epoch 014:    648 / 1474 loss=4.305, trans_loss=5.488, nll_loss=2.823, w2v_ctc_loss=1.427, task_loss=0, contrastive_loss=0.272, total=4158.79, n_correct=2570.59, ppl=7.08, accuracy=61.811, wps=8203.4, ups=1.97, wpb=4158.8, bsz=153.4, num_updates=19800, lr=0.000100504, gnorm=0.978, clip=0, loss_scale=32, train_wall=50, gb_free=17.6, wall=11328
2023-08-02 12:54:09 | INFO | train_inner | epoch 014:    748 / 1474 loss=4.283, trans_loss=5.471, nll_loss=2.801, w2v_ctc_loss=1.424, task_loss=0, contrastive_loss=0.14, total=4145.47, n_correct=2575.77, ppl=6.97, accuracy=62.135, wps=8128.9, ups=1.96, wpb=4145.5, bsz=154.8, num_updates=19900, lr=0.000100251, gnorm=0.958, clip=0, loss_scale=32, train_wall=51, gb_free=16.7, wall=11379
2023-08-02 12:55:01 | INFO | train_inner | epoch 014:    848 / 1474 loss=4.298, trans_loss=5.471, nll_loss=2.802, w2v_ctc_loss=1.414, task_loss=0, contrastive_loss=0.365, total=4171.1, n_correct=2591.3, ppl=6.98, accuracy=62.125, wps=8116.1, ups=1.95, wpb=4171.1, bsz=159.8, num_updates=20000, lr=0.0001, gnorm=0.987, clip=0, loss_scale=32, train_wall=51, gb_free=17.2, wall=11430
2023-08-02 12:55:01 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 12:55:22 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.36 | trans_loss 5.584 | nll_loss 2.862 | w2v_ctc_loss 1.391 | task_loss 0 | contrastive_loss 0.246 | total 4003.4 | n_correct 2458 | ppl 7.27 | accuracy 61.398 | uer 17.501 | wer 19.373 | raw_wer 19.373 | bleu 19.48 | wps 2384.2 | wpb 4003.4 | bsz 141.8 | num_updates 20000 | best_bleu 19.6
2023-08-02 12:55:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20000 updates
2023-08-02 12:55:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_14_20000.pt
2023-08-02 12:55:27 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_14_20000.pt
2023-08-02 12:55:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_14_20000.pt (epoch 14 @ 20000 updates, score 19.48) (writing took 34.49322948791087 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:0')
2023-08-02 12:56:51 | INFO | train_inner | epoch 014:    948 / 1474 loss=4.291, trans_loss=5.48, nll_loss=2.812, w2v_ctc_loss=1.409, task_loss=0, contrastive_loss=0.229, total=4167.75, n_correct=2575.6, ppl=7.02, accuracy=61.798, wps=3756.6, ups=0.9, wpb=4167.8, bsz=155.1, num_updates=20100, lr=9.97509e-05, gnorm=0.963, clip=0, loss_scale=32, train_wall=51, gb_free=17, wall=11541
2023-08-02 12:57:43 | INFO | train_inner | epoch 014:   1048 / 1474 loss=4.289, trans_loss=5.483, nll_loss=2.819, w2v_ctc_loss=1.405, task_loss=0, contrastive_loss=0.181, total=4143.92, n_correct=2566.25, ppl=7.05, accuracy=61.928, wps=8008.4, ups=1.93, wpb=4143.9, bsz=150.4, num_updates=20200, lr=9.95037e-05, gnorm=0.971, clip=0, loss_scale=32, train_wall=51, gb_free=16.4, wall=11593
2023-08-02 12:58:35 | INFO | train_inner | epoch 014:   1148 / 1474 loss=4.335, trans_loss=5.49, nll_loss=2.827, w2v_ctc_loss=1.423, task_loss=0, contrastive_loss=0.722, total=4228.69, n_correct=2611.88, ppl=7.1, accuracy=61.766, wps=8242.5, ups=1.95, wpb=4228.7, bsz=163.6, num_updates=20300, lr=9.92583e-05, gnorm=0.969, clip=0, loss_scale=32, train_wall=51, gb_free=16.2, wall=11644
2023-08-02 12:59:25 | INFO | train_inner | epoch 014:   1248 / 1474 loss=4.31, trans_loss=5.504, nll_loss=2.843, w2v_ctc_loss=1.45, task_loss=0, contrastive_loss=0.108, total=4021.19, n_correct=2476.29, ppl=7.18, accuracy=61.581, wps=7934.9, ups=1.97, wpb=4021.2, bsz=135.8, num_updates=20400, lr=9.90148e-05, gnorm=0.991, clip=0, loss_scale=32, train_wall=50, gb_free=16.8, wall=11695
2023-08-02 13:00:17 | INFO | train_inner | epoch 014:   1348 / 1474 loss=4.284, trans_loss=5.482, nll_loss=2.817, w2v_ctc_loss=1.403, task_loss=0, contrastive_loss=0.14, total=4213.9, n_correct=2611.25, ppl=7.05, accuracy=61.968, wps=8198.6, ups=1.95, wpb=4213.9, bsz=159.7, num_updates=20500, lr=9.8773e-05, gnorm=0.959, clip=0, loss_scale=32, train_wall=51, gb_free=16.6, wall=11746
2023-08-02 13:01:08 | INFO | train_inner | epoch 014:   1448 / 1474 loss=4.303, trans_loss=5.494, nll_loss=2.833, w2v_ctc_loss=1.418, task_loss=0, contrastive_loss=0.215, total=4130.28, n_correct=2552.1, ppl=7.12, accuracy=61.79, wps=8101.4, ups=1.96, wpb=4130.3, bsz=152, num_updates=20600, lr=9.85329e-05, gnorm=0.988, clip=0, loss_scale=32, train_wall=51, gb_free=15.8, wall=11797
2023-08-02 13:01:21 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:1')
2023-08-02 13:01:43 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.356 | trans_loss 5.577 | nll_loss 2.854 | w2v_ctc_loss 1.391 | task_loss 0 | contrastive_loss 0.256 | total 4003.4 | n_correct 2463.1 | ppl 7.23 | accuracy 61.525 | uer 17.575 | wer 19.358 | raw_wer 19.358 | bleu 19.48 | wps 2295.7 | wpb 4003.4 | bsz 141.8 | num_updates 20626 | best_bleu 19.6
2023-08-02 13:01:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20626 updates
2023-08-02 13:01:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_19.4804.pt
2023-08-02 13:01:58 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_19.4804.pt
2023-08-02 13:02:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_19.4804.pt (epoch 14 @ 20626 updates, score 19.48) (writing took 24.449984742328525 seconds)
2023-08-02 13:02:08 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-08-02 13:02:08 | INFO | train | epoch 014 | loss 4.293 | trans_loss 5.48 | nll_loss 2.812 | w2v_ctc_loss 1.416 | task_loss 0 | contrastive_loss 0.236 | total 4138.65 | n_correct 2565.15 | ppl 7.02 | accuracy 61.98 | wps 7029.3 | ups 1.7 | wpb 4138.6 | bsz 152.8 | num_updates 20626 | lr 9.84708e-05 | gnorm 0.975 | clip 0 | loss_scale 32 | train_wall 746 | gb_free 16.8 | wall 11858
2023-08-02 13:02:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 13:02:08 | INFO | fairseq.trainer | begin training epoch 15
2023-08-02 13:02:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 13:02:54 | INFO | train_inner | epoch 015:     74 / 1474 loss=4.279, trans_loss=5.462, nll_loss=2.79, w2v_ctc_loss=1.397, task_loss=0, contrastive_loss=0.321, total=4083.88, n_correct=2542.38, ppl=6.92, accuracy=62.254, wps=3836.7, ups=0.94, wpb=4083.9, bsz=150.1, num_updates=20700, lr=9.82946e-05, gnorm=0.982, clip=0, loss_scale=32, train_wall=50, gb_free=16.6, wall=11904
2023-08-02 13:03:45 | INFO | train_inner | epoch 015:    174 / 1474 loss=4.257, trans_loss=5.444, nll_loss=2.766, w2v_ctc_loss=1.404, task_loss=0, contrastive_loss=0.134, total=4115.73, n_correct=2574.21, ppl=6.8, accuracy=62.546, wps=8034.8, ups=1.95, wpb=4115.7, bsz=148.9, num_updates=20800, lr=9.80581e-05, gnorm=0.958, clip=0, loss_scale=32, train_wall=51, gb_free=17.2, wall=11955
2023-08-02 13:04:36 | INFO | train_inner | epoch 015:    274 / 1474 loss=4.25, trans_loss=5.442, nll_loss=2.764, w2v_ctc_loss=1.386, task_loss=0, contrastive_loss=0.121, total=4193.15, n_correct=2628.62, ppl=6.79, accuracy=62.688, wps=8273.7, ups=1.97, wpb=4193.1, bsz=156.3, num_updates=20900, lr=9.78232e-05, gnorm=0.968, clip=0, loss_scale=32, train_wall=50, gb_free=13.4, wall=12006
2023-08-02 13:05:27 | INFO | train_inner | epoch 015:    374 / 1474 loss=4.256, trans_loss=5.442, nll_loss=2.763, w2v_ctc_loss=1.396, task_loss=0, contrastive_loss=0.168, total=4167.66, n_correct=2605.82, ppl=6.79, accuracy=62.525, wps=8141, ups=1.95, wpb=4167.7, bsz=153, num_updates=21000, lr=9.759e-05, gnorm=0.973, clip=0, loss_scale=32, train_wall=51, gb_free=16.5, wall=12057
2023-08-02 13:06:18 | INFO | train_inner | epoch 015:    474 / 1474 loss=4.276, trans_loss=5.458, nll_loss=2.784, w2v_ctc_loss=1.385, task_loss=0, contrastive_loss=0.347, total=4074.53, n_correct=2534.73, ppl=6.89, accuracy=62.209, wps=7990.2, ups=1.96, wpb=4074.5, bsz=147.1, num_updates=21100, lr=9.73585e-05, gnorm=0.992, clip=0, loss_scale=32, train_wall=51, gb_free=16.2, wall=12108
2023-08-02 13:07:09 | INFO | train_inner | epoch 015:    574 / 1474 loss=4.253, trans_loss=5.444, nll_loss=2.767, w2v_ctc_loss=1.393, task_loss=0, contrastive_loss=0.132, total=4140.59, n_correct=2586.36, ppl=6.81, accuracy=62.464, wps=8086.6, ups=1.95, wpb=4140.6, bsz=149.4, num_updates=21200, lr=9.71286e-05, gnorm=0.972, clip=0, loss_scale=32, train_wall=51, gb_free=12.8, wall=12159
2023-08-02 13:08:01 | INFO | train_inner | epoch 015:    674 / 1474 loss=4.28, trans_loss=5.453, nll_loss=2.779, w2v_ctc_loss=1.414, task_loss=0, contrastive_loss=0.296, total=4134.99, n_correct=2584.84, ppl=6.86, accuracy=62.511, wps=8077.1, ups=1.95, wpb=4135, bsz=153.5, num_updates=21300, lr=9.69003e-05, gnorm=0.966, clip=0, loss_scale=32, train_wall=51, gb_free=11.4, wall=12210
2023-08-02 13:08:52 | INFO | train_inner | epoch 015:    774 / 1474 loss=4.271, trans_loss=5.461, nll_loss=2.789, w2v_ctc_loss=1.412, task_loss=0, contrastive_loss=0.137, total=4173.66, n_correct=2597.08, ppl=6.91, accuracy=62.225, wps=8141.2, ups=1.95, wpb=4173.7, bsz=152.5, num_updates=21400, lr=9.66736e-05, gnorm=0.977, clip=0, loss_scale=32, train_wall=51, gb_free=17.2, wall=12262
2023-08-02 13:09:43 | INFO | train_inner | epoch 015:    874 / 1474 loss=4.268, trans_loss=5.46, nll_loss=2.788, w2v_ctc_loss=1.407, task_loss=0, contrastive_loss=0.13, total=4059.35, n_correct=2528.32, ppl=6.91, accuracy=62.284, wps=7993, ups=1.97, wpb=4059.3, bsz=144.1, num_updates=21500, lr=9.64486e-05, gnorm=0.985, clip=0, loss_scale=32, train_wall=50, gb_free=16.1, wall=12312
2023-08-02 13:10:34 | INFO | train_inner | epoch 015:    974 / 1474 loss=4.27, trans_loss=5.454, nll_loss=2.78, w2v_ctc_loss=1.391, task_loss=0, contrastive_loss=0.301, total=4122.87, n_correct=2570.59, ppl=6.87, accuracy=62.35, wps=8082.9, ups=1.96, wpb=4122.9, bsz=150.8, num_updates=21600, lr=9.6225e-05, gnorm=0.97, clip=0, loss_scale=32, train_wall=51, gb_free=17.9, wall=12363
2023-08-02 13:11:25 | INFO | train_inner | epoch 015:   1074 / 1474 loss=4.302, trans_loss=5.465, nll_loss=2.797, w2v_ctc_loss=1.394, task_loss=0, contrastive_loss=0.612, total=4192.24, n_correct=2607.12, ppl=6.95, accuracy=62.189, wps=8102.6, ups=1.93, wpb=4192.2, bsz=162.6, num_updates=21700, lr=9.60031e-05, gnorm=0.971, clip=0, loss_scale=32, train_wall=51, gb_free=17.6, wall=12415
2023-08-02 13:12:17 | INFO | train_inner | epoch 015:   1174 / 1474 loss=4.251, trans_loss=5.438, nll_loss=2.764, w2v_ctc_loss=1.368, task_loss=0, contrastive_loss=0.23, total=4185, n_correct=2623.62, ppl=6.79, accuracy=62.691, wps=8151.2, ups=1.95, wpb=4185, bsz=164.6, num_updates=21800, lr=9.57826e-05, gnorm=0.968, clip=0, loss_scale=32, train_wall=51, gb_free=16.7, wall=12466
2023-08-02 13:13:08 | INFO | train_inner | epoch 015:   1274 / 1474 loss=4.271, trans_loss=5.457, nll_loss=2.787, w2v_ctc_loss=1.416, task_loss=0, contrastive_loss=0.141, total=4152.04, n_correct=2589.51, ppl=6.9, accuracy=62.367, wps=8050.3, ups=1.94, wpb=4152, bsz=151.8, num_updates=21900, lr=9.55637e-05, gnorm=0.988, clip=0, loss_scale=64, train_wall=51, gb_free=17, wall=12518
2023-08-02 13:13:59 | INFO | train_inner | epoch 015:   1374 / 1474 loss=4.261, trans_loss=5.455, nll_loss=2.782, w2v_ctc_loss=1.398, task_loss=0, contrastive_loss=0.114, total=4100.21, n_correct=2557.54, ppl=6.88, accuracy=62.376, wps=8063.3, ups=1.97, wpb=4100.2, bsz=146.8, num_updates=22000, lr=9.53463e-05, gnorm=0.975, clip=0, loss_scale=64, train_wall=50, gb_free=17.9, wall=12569
2023-08-02 13:13:59 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 13:14:21 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 4.322 | trans_loss 5.572 | nll_loss 2.849 | w2v_ctc_loss 1.293 | task_loss 0 | contrastive_loss 0.247 | total 4003.4 | n_correct 2470 | ppl 7.2 | accuracy 61.698 | uer 17.264 | wer 19.108 | raw_wer 19.108 | bleu 19.78 | wps 2383.6 | wpb 4003.4 | bsz 141.8 | num_updates 22000 | best_bleu 19.78
2023-08-02 13:14:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22000 updates
2023-08-02 13:14:21 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_15_22000.pt
2023-08-02 13:14:25 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_15_22000.pt
2023-08-02 13:14:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_15_22000.pt (epoch 15 @ 22000 updates, score 19.78) (writing took 37.70619557052851 seconds)
2023-08-02 13:15:51 | INFO | train_inner | epoch 015:   1474 / 1474 loss=4.286, trans_loss=5.463, nll_loss=2.796, w2v_ctc_loss=1.413, task_loss=0, contrastive_loss=0.287, total=4141.17, n_correct=2578.35, ppl=6.94, accuracy=62.261, wps=3692.6, ups=0.89, wpb=4141.2, bsz=157.2, num_updates=22100, lr=9.51303e-05, gnorm=0.989, clip=0, loss_scale=64, train_wall=51, gb_free=17.4, wall=12681
2023-08-02 13:15:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 13:16:13 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 4.333 | trans_loss 5.569 | nll_loss 2.849 | w2v_ctc_loss 1.333 | task_loss 0 | contrastive_loss 0.261 | total 4003.4 | n_correct 2471.1 | ppl 7.2 | accuracy 61.725 | uer 17.27 | wer 19.063 | raw_wer 19.063 | bleu 19.66 | wps 2386.2 | wpb 4003.4 | bsz 141.8 | num_updates 22100 | best_bleu 19.78
2023-08-02 13:16:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22100 updates
2023-08-02 13:16:13 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_19.6604.pt
2023-08-02 13:16:17 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_19.6604.pt
2023-08-02 13:16:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_19.6604.pt (epoch 15 @ 22100 updates, score 19.66) (writing took 20.142499981448054 seconds)
2023-08-02 13:16:34 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-08-02 13:16:34 | INFO | train | epoch 015 | loss 4.268 | trans_loss 5.452 | nll_loss 2.778 | w2v_ctc_loss 1.397 | task_loss 0 | contrastive_loss 0.235 | total 4138.65 | n_correct 2583.15 | ppl 6.86 | accuracy 62.415 | wps 7047.9 | ups 1.7 | wpb 4138.6 | bsz 152.8 | num_updates 22100 | lr 9.51303e-05 | gnorm 0.975 | clip 0 | loss_scale 64 | train_wall 748 | gb_free 17.4 | wall 12723
2023-08-02 13:16:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 13:16:34 | INFO | fairseq.trainer | begin training epoch 16
2023-08-02 13:16:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 13:17:33 | INFO | train_inner | epoch 016:    100 / 1474 loss=4.226, trans_loss=5.411, nll_loss=2.726, w2v_ctc_loss=1.365, task_loss=0, contrastive_loss=0.168, total=4126.22, n_correct=2604.2, ppl=6.62, accuracy=63.113, wps=4045.2, ups=0.98, wpb=4126.2, bsz=157.8, num_updates=22200, lr=9.49158e-05, gnorm=0.972, clip=0, loss_scale=64, train_wall=51, gb_free=16.5, wall=12783
2023-08-02 13:18:24 | INFO | train_inner | epoch 016:    200 / 1474 loss=4.21, trans_loss=5.403, nll_loss=2.714, w2v_ctc_loss=1.347, task_loss=0, contrastive_loss=0.124, total=4100.6, n_correct=2590, ppl=6.56, accuracy=63.161, wps=8018, ups=1.96, wpb=4100.6, bsz=148.4, num_updates=22300, lr=9.47027e-05, gnorm=0.972, clip=0, loss_scale=64, train_wall=51, gb_free=13.3, wall=12834
2023-08-02 13:19:15 | INFO | train_inner | epoch 016:    300 / 1474 loss=4.254, trans_loss=5.43, nll_loss=2.75, w2v_ctc_loss=1.393, task_loss=0, contrastive_loss=0.27, total=4166.94, n_correct=2616.98, ppl=6.73, accuracy=62.803, wps=8175.4, ups=1.96, wpb=4166.9, bsz=154.5, num_updates=22400, lr=9.44911e-05, gnorm=0.963, clip=0, loss_scale=64, train_wall=51, gb_free=17.6, wall=12885
2023-08-02 13:20:06 | INFO | train_inner | epoch 016:    400 / 1474 loss=4.244, trans_loss=5.423, nll_loss=2.739, w2v_ctc_loss=1.38, task_loss=0, contrastive_loss=0.301, total=4073.3, n_correct=2558.28, ppl=6.68, accuracy=62.806, wps=7982.9, ups=1.96, wpb=4073.3, bsz=144, num_updates=22500, lr=9.42809e-05, gnorm=0.976, clip=0, loss_scale=64, train_wall=51, gb_free=17.4, wall=12936
2023-08-02 13:20:58 | INFO | train_inner | epoch 016:    500 / 1474 loss=4.235, trans_loss=5.419, nll_loss=2.736, w2v_ctc_loss=1.373, task_loss=0, contrastive_loss=0.183, total=4174.67, n_correct=2630.77, ppl=6.66, accuracy=63.017, wps=8099.4, ups=1.94, wpb=4174.7, bsz=159.5, num_updates=22600, lr=9.40721e-05, gnorm=0.983, clip=0, loss_scale=64, train_wall=51, gb_free=16.4, wall=12988
2023-08-02 13:21:49 | INFO | train_inner | epoch 016:    600 / 1474 loss=4.226, trans_loss=5.418, nll_loss=2.734, w2v_ctc_loss=1.365, task_loss=0, contrastive_loss=0.115, total=4124.65, n_correct=2596.93, ppl=6.65, accuracy=62.961, wps=8085.1, ups=1.96, wpb=4124.6, bsz=148.8, num_updates=22700, lr=9.38647e-05, gnorm=0.961, clip=0, loss_scale=64, train_wall=51, gb_free=16.7, wall=13039
2023-08-02 13:22:40 | INFO | train_inner | epoch 016:    700 / 1474 loss=4.232, trans_loss=5.423, nll_loss=2.741, w2v_ctc_loss=1.371, task_loss=0, contrastive_loss=0.122, total=4095.49, n_correct=2576.28, ppl=6.69, accuracy=62.905, wps=8059.9, ups=1.97, wpb=4095.5, bsz=148.2, num_updates=22800, lr=9.36586e-05, gnorm=0.981, clip=0, loss_scale=64, train_wall=50, gb_free=16.6, wall=13090
2023-08-02 13:23:31 | INFO | train_inner | epoch 016:    800 / 1474 loss=4.234, trans_loss=5.421, nll_loss=2.739, w2v_ctc_loss=1.353, task_loss=0, contrastive_loss=0.239, total=4174.94, n_correct=2620.85, ppl=6.68, accuracy=62.776, wps=8149.2, ups=1.95, wpb=4174.9, bsz=155.4, num_updates=22900, lr=9.34539e-05, gnorm=0.965, clip=0, loss_scale=64, train_wall=51, gb_free=16.8, wall=13141
2023-08-02 13:24:22 | INFO | train_inner | epoch 016:    900 / 1474 loss=4.238, trans_loss=5.421, nll_loss=2.741, w2v_ctc_loss=1.369, task_loss=0, contrastive_loss=0.225, total=4163.19, n_correct=2619.92, ppl=6.69, accuracy=62.931, wps=8148.1, ups=1.96, wpb=4163.2, bsz=155.3, num_updates=23000, lr=9.32505e-05, gnorm=0.966, clip=0, loss_scale=64, train_wall=51, gb_free=17.3, wall=13192
2023-08-02 13:25:13 | INFO | train_inner | epoch 016:   1000 / 1474 loss=4.256, trans_loss=5.436, nll_loss=2.76, w2v_ctc_loss=1.398, task_loss=0, contrastive_loss=0.218, total=4103.45, n_correct=2567.21, ppl=6.77, accuracy=62.562, wps=8035.6, ups=1.96, wpb=4103.4, bsz=149, num_updates=23100, lr=9.30484e-05, gnorm=0.98, clip=0, loss_scale=64, train_wall=51, gb_free=15.3, wall=13243
2023-08-02 13:26:05 | INFO | train_inner | epoch 016:   1100 / 1474 loss=4.26, trans_loss=5.445, nll_loss=2.771, w2v_ctc_loss=1.402, task_loss=0, contrastive_loss=0.173, total=4119.27, n_correct=2574.41, ppl=6.83, accuracy=62.497, wps=7976.3, ups=1.94, wpb=4119.3, bsz=147.7, num_updates=23200, lr=9.28477e-05, gnorm=0.986, clip=0, loss_scale=64, train_wall=51, gb_free=18.1, wall=13295
2023-08-02 13:26:56 | INFO | train_inner | epoch 016:   1200 / 1474 loss=4.258, trans_loss=5.437, nll_loss=2.762, w2v_ctc_loss=1.366, task_loss=0, contrastive_loss=0.358, total=4165.11, n_correct=2606.48, ppl=6.78, accuracy=62.579, wps=8090.6, ups=1.94, wpb=4165.1, bsz=154.3, num_updates=23300, lr=9.26482e-05, gnorm=0.978, clip=0, loss_scale=64, train_wall=51, gb_free=17.2, wall=13346
2023-08-02 13:27:48 | INFO | train_inner | epoch 016:   1300 / 1474 loss=4.269, trans_loss=5.441, nll_loss=2.767, w2v_ctc_loss=1.402, task_loss=0, contrastive_loss=0.329, total=4134.61, n_correct=2589.57, ppl=6.81, accuracy=62.632, wps=8084.8, ups=1.96, wpb=4134.6, bsz=155.4, num_updates=23400, lr=9.245e-05, gnorm=0.982, clip=0, loss_scale=64, train_wall=51, gb_free=17.2, wall=13397
2023-08-02 13:28:39 | INFO | train_inner | epoch 016:   1400 / 1474 loss=4.25, trans_loss=5.433, nll_loss=2.757, w2v_ctc_loss=1.385, task_loss=0, contrastive_loss=0.185, total=4206.33, n_correct=2636.51, ppl=6.76, accuracy=62.68, wps=8178.4, ups=1.94, wpb=4206.3, bsz=161.1, num_updates=23500, lr=9.22531e-05, gnorm=0.976, clip=0, loss_scale=64, train_wall=51, gb_free=15.9, wall=13449
2023-08-02 13:29:17 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 13:29:40 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 4.325 | trans_loss 5.56 | nll_loss 2.833 | w2v_ctc_loss 1.328 | task_loss 0 | contrastive_loss 0.25 | total 4003.4 | n_correct 2475.3 | ppl 7.13 | accuracy 61.83 | uer 17.047 | wer 18.817 | raw_wer 18.817 | bleu 20.24 | wps 2248 | wpb 4003.4 | bsz 141.8 | num_updates 23574 | best_bleu 20.24
2023-08-02 13:29:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 23574 updates
2023-08-02 13:29:40 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 13:29:53 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 13:30:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt (epoch 16 @ 23574 updates, score 20.24) (writing took 24.773216793313622 seconds)
2023-08-02 13:30:05 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-08-02 13:30:05 | INFO | train | epoch 016 | loss 4.243 | trans_loss 5.426 | nll_loss 2.746 | w2v_ctc_loss 1.376 | task_loss 0 | contrastive_loss 0.233 | total 4138.65 | n_correct 2599.39 | ppl 6.71 | accuracy 62.808 | wps 7514.4 | ups 1.82 | wpb 4138.6 | bsz 152.8 | num_updates 23574 | lr 9.21082e-05 | gnorm 0.975 | clip 0 | loss_scale 64 | train_wall 748 | gb_free 15.9 | wall 13535
2023-08-02 13:30:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 13:30:06 | INFO | fairseq.trainer | begin training epoch 17
2023-08-02 13:30:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 13:30:27 | INFO | train_inner | epoch 017:     26 / 1474 loss=4.247, trans_loss=5.42, nll_loss=2.74, w2v_ctc_loss=1.365, task_loss=0, contrastive_loss=0.453, total=4152.31, n_correct=2612.27, ppl=6.68, accuracy=62.911, wps=3851.1, ups=0.93, wpb=4152.3, bsz=152.3, num_updates=23600, lr=9.20575e-05, gnorm=0.978, clip=0, loss_scale=64, train_wall=51, gb_free=14.4, wall=13556
2023-08-02 13:31:18 | INFO | train_inner | epoch 017:    126 / 1474 loss=4.207, trans_loss=5.392, nll_loss=2.701, w2v_ctc_loss=1.361, task_loss=0, contrastive_loss=0.127, total=4118.91, n_correct=2610.06, ppl=6.5, accuracy=63.368, wps=8080.6, ups=1.96, wpb=4118.9, bsz=147.9, num_updates=23700, lr=9.1863e-05, gnorm=0.968, clip=0, loss_scale=64, train_wall=51, gb_free=16.9, wall=13607
2023-08-02 13:31:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-02 13:32:10 | INFO | train_inner | epoch 017:    227 / 1474 loss=4.214, trans_loss=5.391, nll_loss=2.701, w2v_ctc_loss=1.356, task_loss=0, contrastive_loss=0.236, total=4123.81, n_correct=2612.26, ppl=6.5, accuracy=63.346, wps=7967.1, ups=1.93, wpb=4123.8, bsz=154.1, num_updates=23800, lr=9.16698e-05, gnorm=0.99, clip=0, loss_scale=32, train_wall=51, gb_free=17.4, wall=13659
2023-08-02 13:33:00 | INFO | train_inner | epoch 017:    327 / 1474 loss=4.233, trans_loss=5.401, nll_loss=2.714, w2v_ctc_loss=1.359, task_loss=0, contrastive_loss=0.463, total=4156.91, n_correct=2624.69, ppl=6.56, accuracy=63.14, wps=8175.5, ups=1.97, wpb=4156.9, bsz=152.9, num_updates=23900, lr=9.14779e-05, gnorm=0.962, clip=0, loss_scale=32, train_wall=50, gb_free=17.9, wall=13710
2023-08-02 13:33:51 | INFO | train_inner | epoch 017:    427 / 1474 loss=4.207, trans_loss=5.393, nll_loss=2.704, w2v_ctc_loss=1.355, task_loss=0, contrastive_loss=0.128, total=4146.43, n_correct=2628.82, ppl=6.52, accuracy=63.4, wps=8116.1, ups=1.96, wpb=4146.4, bsz=154, num_updates=24000, lr=9.12871e-05, gnorm=0.98, clip=0, loss_scale=32, train_wall=51, gb_free=16.9, wall=13761
2023-08-02 13:33:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 13:34:14 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 4.345 | trans_loss 5.564 | nll_loss 2.838 | w2v_ctc_loss 1.386 | task_loss 0 | contrastive_loss 0.252 | total 4003.4 | n_correct 2470.8 | ppl 7.15 | accuracy 61.718 | uer 17.535 | wer 19.462 | raw_wer 19.462 | bleu 19.81 | wps 2255.4 | wpb 4003.4 | bsz 141.8 | num_updates 24000 | best_bleu 20.24
2023-08-02 13:34:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 24000 updates
2023-08-02 13:34:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_17_24000.pt
2023-08-02 13:34:18 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_17_24000.pt
2023-08-02 13:34:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_17_24000.pt (epoch 17 @ 24000 updates, score 19.81) (writing took 32.509712267667055 seconds)
2023-08-02 13:35:39 | INFO | train_inner | epoch 017:    527 / 1474 loss=4.224, trans_loss=5.402, nll_loss=2.716, w2v_ctc_loss=1.371, task_loss=0, contrastive_loss=0.214, total=4182.1, n_correct=2640.46, ppl=6.57, accuracy=63.137, wps=3890.3, ups=0.93, wpb=4182.1, bsz=153.9, num_updates=24100, lr=9.10975e-05, gnorm=0.975, clip=0, loss_scale=32, train_wall=51, gb_free=17.3, wall=13869
2023-08-02 13:36:31 | INFO | train_inner | epoch 017:    627 / 1474 loss=4.207, trans_loss=5.398, nll_loss=2.71, w2v_ctc_loss=1.349, task_loss=0, contrastive_loss=0.118, total=4167.27, n_correct=2634.9, ppl=6.54, accuracy=63.228, wps=8089.9, ups=1.94, wpb=4167.3, bsz=151.1, num_updates=24200, lr=9.09091e-05, gnorm=0.955, clip=0, loss_scale=32, train_wall=51, gb_free=11.6, wall=13920
2023-08-02 13:37:22 | INFO | train_inner | epoch 017:    727 / 1474 loss=4.247, trans_loss=5.42, nll_loss=2.739, w2v_ctc_loss=1.402, task_loss=0, contrastive_loss=0.21, total=4166.12, n_correct=2619.4, ppl=6.68, accuracy=62.874, wps=8143.7, ups=1.95, wpb=4166.1, bsz=154.1, num_updates=24300, lr=9.07218e-05, gnorm=1.003, clip=0, loss_scale=32, train_wall=51, gb_free=16.7, wall=13971
2023-08-02 13:38:12 | INFO | train_inner | epoch 017:    827 / 1474 loss=4.221, trans_loss=5.409, nll_loss=2.724, w2v_ctc_loss=1.366, task_loss=0, contrastive_loss=0.14, total=4091.64, n_correct=2581.71, ppl=6.61, accuracy=63.097, wps=8114.3, ups=1.98, wpb=4091.6, bsz=147.7, num_updates=24400, lr=9.05357e-05, gnorm=0.987, clip=0, loss_scale=32, train_wall=50, gb_free=17.7, wall=14022
2023-08-02 13:39:03 | INFO | train_inner | epoch 017:    927 / 1474 loss=4.207, trans_loss=5.397, nll_loss=2.709, w2v_ctc_loss=1.343, task_loss=0, contrastive_loss=0.139, total=4106.83, n_correct=2600.07, ppl=6.54, accuracy=63.311, wps=8065.9, ups=1.96, wpb=4106.8, bsz=152.3, num_updates=24500, lr=9.03508e-05, gnorm=0.971, clip=0, loss_scale=32, train_wall=50, gb_free=16.3, wall=14073
2023-08-02 13:39:54 | INFO | train_inner | epoch 017:   1027 / 1474 loss=4.225, trans_loss=5.406, nll_loss=2.722, w2v_ctc_loss=1.378, task_loss=0, contrastive_loss=0.147, total=4115.49, n_correct=2597.28, ppl=6.6, accuracy=63.11, wps=8113, ups=1.97, wpb=4115.5, bsz=152.9, num_updates=24600, lr=9.0167e-05, gnorm=0.987, clip=0, loss_scale=32, train_wall=50, gb_free=17, wall=14123
2023-08-02 13:40:44 | INFO | train_inner | epoch 017:   1127 / 1474 loss=4.214, trans_loss=5.403, nll_loss=2.717, w2v_ctc_loss=1.36, task_loss=0, contrastive_loss=0.121, total=4078.39, n_correct=2573.48, ppl=6.58, accuracy=63.1, wps=8039.6, ups=1.97, wpb=4078.4, bsz=146.9, num_updates=24700, lr=8.99843e-05, gnorm=0.992, clip=0, loss_scale=32, train_wall=50, gb_free=16, wall=14174
2023-08-02 13:41:36 | INFO | train_inner | epoch 017:   1227 / 1474 loss=4.261, trans_loss=5.422, nll_loss=2.744, w2v_ctc_loss=1.357, task_loss=0, contrastive_loss=0.602, total=4173.49, n_correct=2620.03, ppl=6.7, accuracy=62.778, wps=8028.5, ups=1.92, wpb=4173.5, bsz=161.9, num_updates=24800, lr=8.98027e-05, gnorm=0.979, clip=0, loss_scale=32, train_wall=52, gb_free=16.5, wall=14226
2023-08-02 13:42:28 | INFO | train_inner | epoch 017:   1327 / 1474 loss=4.235, trans_loss=5.417, nll_loss=2.737, w2v_ctc_loss=1.353, task_loss=0, contrastive_loss=0.287, total=4156.28, n_correct=2612.21, ppl=6.67, accuracy=62.85, wps=8141.7, ups=1.96, wpb=4156.3, bsz=154, num_updates=24900, lr=8.96221e-05, gnorm=0.974, clip=0, loss_scale=32, train_wall=51, gb_free=18.2, wall=14277
2023-08-02 13:43:18 | INFO | train_inner | epoch 017:   1427 / 1474 loss=4.222, trans_loss=5.411, nll_loss=2.73, w2v_ctc_loss=1.361, task_loss=0, contrastive_loss=0.13, total=4112.95, n_correct=2590.07, ppl=6.63, accuracy=62.974, wps=8073.5, ups=1.96, wpb=4112.9, bsz=151.6, num_updates=25000, lr=8.94427e-05, gnorm=0.982, clip=0, loss_scale=32, train_wall=51, gb_free=17.3, wall=14328
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:0')
2023-08-02 13:43:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:4')
2023-08-02 13:44:05 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 4.335 | trans_loss 5.557 | nll_loss 2.833 | w2v_ctc_loss 1.366 | task_loss 0 | contrastive_loss 0.254 | total 4003.4 | n_correct 2476.1 | ppl 7.13 | accuracy 61.85 | uer 17.079 | wer 18.974 | raw_wer 18.974 | bleu 19.75 | wps 2017.7 | wpb 4003.4 | bsz 141.8 | num_updates 25047 | best_bleu 20.24
2023-08-02 13:44:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 25047 updates
2023-08-02 13:44:05 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_19.7506.pt
2023-08-02 13:44:09 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_19.7506.pt
2023-08-02 13:44:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_19.7506.pt (epoch 17 @ 25047 updates, score 19.75) (writing took 22.20052787847817 seconds)
2023-08-02 13:44:28 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-08-02 13:44:28 | INFO | train | epoch 017 | loss 4.223 | trans_loss 5.405 | nll_loss 2.719 | w2v_ctc_loss 1.363 | task_loss 0 | contrastive_loss 0.217 | total 4136.62 | n_correct 2611.18 | ppl 6.59 | accuracy 63.123 | wps 7065.5 | ups 1.71 | wpb 4136.6 | bsz 152.5 | num_updates 25047 | lr 8.93588e-05 | gnorm 0.979 | clip 0 | loss_scale 32 | train_wall 747 | gb_free 16.8 | wall 14398
2023-08-02 13:44:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 13:44:28 | INFO | fairseq.trainer | begin training epoch 18
2023-08-02 13:44:28 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 13:45:03 | INFO | train_inner | epoch 018:     53 / 1474 loss=4.217, trans_loss=5.398, nll_loss=2.711, w2v_ctc_loss=1.372, task_loss=0, contrastive_loss=0.148, total=4139.04, n_correct=2613.76, ppl=6.55, accuracy=63.149, wps=3947.8, ups=0.95, wpb=4139, bsz=151.7, num_updates=25100, lr=8.92644e-05, gnorm=0.977, clip=0, loss_scale=32, train_wall=51, gb_free=17.5, wall=14433
2023-08-02 13:45:54 | INFO | train_inner | epoch 018:    153 / 1474 loss=4.201, trans_loss=5.37, nll_loss=2.674, w2v_ctc_loss=1.325, task_loss=0, contrastive_loss=0.386, total=4154.85, n_correct=2645.59, ppl=6.38, accuracy=63.675, wps=8158.2, ups=1.96, wpb=4154.9, bsz=156.4, num_updates=25200, lr=8.90871e-05, gnorm=0.984, clip=0, loss_scale=32, train_wall=50, gb_free=17.1, wall=14484
2023-08-02 13:46:46 | INFO | train_inner | epoch 018:    253 / 1474 loss=4.177, trans_loss=5.361, nll_loss=2.664, w2v_ctc_loss=1.328, task_loss=0, contrastive_loss=0.132, total=4162.72, n_correct=2661.79, ppl=6.34, accuracy=63.944, wps=8110.6, ups=1.95, wpb=4162.7, bsz=156.5, num_updates=25300, lr=8.89108e-05, gnorm=0.975, clip=0, loss_scale=32, train_wall=51, gb_free=16.7, wall=14535
2023-08-02 13:47:37 | INFO | train_inner | epoch 018:    353 / 1474 loss=4.197, trans_loss=5.38, nll_loss=2.686, w2v_ctc_loss=1.346, task_loss=0, contrastive_loss=0.158, total=4161.22, n_correct=2643.5, ppl=6.44, accuracy=63.527, wps=8125.2, ups=1.95, wpb=4161.2, bsz=150.7, num_updates=25400, lr=8.87357e-05, gnorm=0.976, clip=0, loss_scale=32, train_wall=51, gb_free=14.9, wall=14586
2023-08-02 13:48:28 | INFO | train_inner | epoch 018:    453 / 1474 loss=4.209, trans_loss=5.385, nll_loss=2.694, w2v_ctc_loss=1.34, task_loss=0, contrastive_loss=0.338, total=4092.36, n_correct=2597.93, ppl=6.47, accuracy=63.482, wps=8013.3, ups=1.96, wpb=4092.4, bsz=147.7, num_updates=25500, lr=8.85615e-05, gnorm=0.986, clip=0, loss_scale=32, train_wall=51, gb_free=17.1, wall=14638
2023-08-02 13:49:19 | INFO | train_inner | epoch 018:    553 / 1474 loss=4.183, trans_loss=5.365, nll_loss=2.669, w2v_ctc_loss=1.332, task_loss=0, contrastive_loss=0.155, total=4206.45, n_correct=2682.14, ppl=6.36, accuracy=63.763, wps=8274, ups=1.97, wpb=4206.4, bsz=164.5, num_updates=25600, lr=8.83883e-05, gnorm=0.96, clip=0, loss_scale=32, train_wall=50, gb_free=18.1, wall=14688
2023-08-02 13:50:10 | INFO | train_inner | epoch 018:    653 / 1474 loss=4.222, trans_loss=5.401, nll_loss=2.715, w2v_ctc_loss=1.358, task_loss=0, contrastive_loss=0.292, total=4097.96, n_correct=2591.15, ppl=6.57, accuracy=63.23, wps=7997.9, ups=1.95, wpb=4098, bsz=149.3, num_updates=25700, lr=8.82162e-05, gnorm=0.999, clip=0, loss_scale=32, train_wall=51, gb_free=12.9, wall=14740
2023-08-02 13:51:01 | INFO | train_inner | epoch 018:    753 / 1474 loss=4.229, trans_loss=5.39, nll_loss=2.703, w2v_ctc_loss=1.36, task_loss=0, contrastive_loss=0.476, total=4208.5, n_correct=2665.12, ppl=6.51, accuracy=63.327, wps=8216.4, ups=1.95, wpb=4208.5, bsz=161.3, num_updates=25800, lr=8.80451e-05, gnorm=0.973, clip=0, loss_scale=64, train_wall=51, gb_free=16.6, wall=14791
2023-08-02 13:51:52 | INFO | train_inner | epoch 018:    853 / 1474 loss=4.193, trans_loss=5.381, nll_loss=2.69, w2v_ctc_loss=1.34, task_loss=0, contrastive_loss=0.113, total=4166.07, n_correct=2645.43, ppl=6.45, accuracy=63.499, wps=8178.6, ups=1.96, wpb=4166.1, bsz=151.2, num_updates=25900, lr=8.7875e-05, gnorm=0.962, clip=0, loss_scale=64, train_wall=51, gb_free=16.7, wall=14842
2023-08-02 13:52:43 | INFO | train_inner | epoch 018:    953 / 1474 loss=4.186, trans_loss=5.373, nll_loss=2.68, w2v_ctc_loss=1.323, task_loss=0, contrastive_loss=0.155, total=4141.27, n_correct=2635.6, ppl=6.41, accuracy=63.642, wps=8206.1, ups=1.98, wpb=4141.3, bsz=158, num_updates=26000, lr=8.77058e-05, gnorm=0.977, clip=0, loss_scale=64, train_wall=50, gb_free=16.5, wall=14892
2023-08-02 13:52:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 13:53:06 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 4.321 | trans_loss 5.555 | nll_loss 2.826 | w2v_ctc_loss 1.328 | task_loss 0 | contrastive_loss 0.248 | total 4003.4 | n_correct 2476.8 | ppl 7.09 | accuracy 61.867 | uer 16.917 | wer 18.832 | raw_wer 18.832 | bleu 19.87 | wps 2159.2 | wpb 4003.4 | bsz 141.8 | num_updates 26000 | best_bleu 20.24
2023-08-02 13:53:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26000 updates
2023-08-02 13:53:06 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_18_26000.pt
2023-08-02 13:53:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_18_26000.pt
2023-08-02 13:53:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_18_26000.pt (epoch 18 @ 26000 updates, score 19.87) (writing took 29.35534456372261 seconds)
2023-08-02 13:54:27 | INFO | train_inner | epoch 018:   1053 / 1474 loss=4.2, trans_loss=5.388, nll_loss=2.699, w2v_ctc_loss=1.343, task_loss=0, contrastive_loss=0.135, total=4134.55, n_correct=2621.1, ppl=6.49, accuracy=63.395, wps=3949.8, ups=0.96, wpb=4134.6, bsz=150.4, num_updates=26100, lr=8.75376e-05, gnorm=0.986, clip=0, loss_scale=64, train_wall=51, gb_free=16.9, wall=14997
2023-08-02 13:55:18 | INFO | train_inner | epoch 018:   1153 / 1474 loss=4.219, trans_loss=5.386, nll_loss=2.697, w2v_ctc_loss=1.357, task_loss=0, contrastive_loss=0.349, total=4157.63, n_correct=2637.16, ppl=6.48, accuracy=63.429, wps=8132.7, ups=1.96, wpb=4157.6, bsz=157, num_updates=26200, lr=8.73704e-05, gnorm=0.975, clip=0, loss_scale=64, train_wall=51, gb_free=17.5, wall=15048
2023-08-02 13:56:09 | INFO | train_inner | epoch 018:   1253 / 1474 loss=4.202, trans_loss=5.394, nll_loss=2.707, w2v_ctc_loss=1.342, task_loss=0, contrastive_loss=0.122, total=4085.66, n_correct=2582.93, ppl=6.53, accuracy=63.219, wps=8027.8, ups=1.96, wpb=4085.7, bsz=143.3, num_updates=26300, lr=8.72041e-05, gnorm=0.976, clip=0, loss_scale=64, train_wall=50, gb_free=17.9, wall=15099
2023-08-02 13:57:00 | INFO | train_inner | epoch 018:   1353 / 1474 loss=4.231, trans_loss=5.409, nll_loss=2.727, w2v_ctc_loss=1.389, task_loss=0, contrastive_loss=0.171, total=4065.6, n_correct=2564.37, ppl=6.62, accuracy=63.075, wps=8061.6, ups=1.98, wpb=4065.6, bsz=145.6, num_updates=26400, lr=8.70388e-05, gnorm=0.994, clip=0, loss_scale=64, train_wall=50, gb_free=13.7, wall=15149
2023-08-02 13:57:52 | INFO | train_inner | epoch 018:   1453 / 1474 loss=4.21, trans_loss=5.395, nll_loss=2.71, w2v_ctc_loss=1.357, task_loss=0, contrastive_loss=0.142, total=4122.48, n_correct=2604.45, ppl=6.54, accuracy=63.177, wps=7954.6, ups=1.93, wpb=4122.5, bsz=149.7, num_updates=26500, lr=8.68744e-05, gnorm=0.981, clip=0, loss_scale=64, train_wall=51, gb_free=17.5, wall=15201
2023-08-02 13:58:02 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 13:58:25 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 4.315 | trans_loss 5.551 | nll_loss 2.827 | w2v_ctc_loss 1.314 | task_loss 0 | contrastive_loss 0.251 | total 4003.4 | n_correct 2485.3 | ppl 7.1 | accuracy 62.08 | uer 16.81 | wer 18.631 | raw_wer 18.631 | bleu 19.93 | wps 2256.5 | wpb 4003.4 | bsz 141.8 | num_updates 26521 | best_bleu 20.24
2023-08-02 13:58:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26521 updates
2023-08-02 13:58:25 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_19.9307.pt
2023-08-02 13:58:28 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_19.9307.pt
2023-08-02 13:58:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_19.9307.pt (epoch 18 @ 26521 updates, score 19.93) (writing took 16.079812860116363 seconds)
2023-08-02 13:58:41 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-08-02 13:58:41 | INFO | train | epoch 018 | loss 4.204 | trans_loss 5.384 | nll_loss 2.694 | w2v_ctc_loss 1.346 | task_loss 0 | contrastive_loss 0.23 | total 4138.65 | n_correct 2625.98 | ppl 6.47 | accuracy 63.45 | wps 7148 | ups 1.73 | wpb 4138.6 | bsz 152.8 | num_updates 26521 | lr 8.684e-05 | gnorm 0.979 | clip 0 | loss_scale 64 | train_wall 746 | gb_free 16.4 | wall 15251
2023-08-02 13:58:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 13:58:41 | INFO | fairseq.trainer | begin training epoch 19
2023-08-02 13:58:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 13:59:30 | INFO | train_inner | epoch 019:     79 / 1474 loss=4.18, trans_loss=5.356, nll_loss=2.658, w2v_ctc_loss=1.325, task_loss=0, contrastive_loss=0.246, total=4101.48, n_correct=2617.4, ppl=6.31, accuracy=63.816, wps=4181.1, ups=1.02, wpb=4101.5, bsz=148.5, num_updates=26600, lr=8.6711e-05, gnorm=0.98, clip=0, loss_scale=64, train_wall=50, gb_free=17.5, wall=15299
2023-08-02 14:00:21 | INFO | train_inner | epoch 019:    179 / 1474 loss=4.18, trans_loss=5.348, nll_loss=2.648, w2v_ctc_loss=1.341, task_loss=0, contrastive_loss=0.222, total=4227.39, n_correct=2707.88, ppl=6.27, accuracy=64.056, wps=8253.2, ups=1.95, wpb=4227.4, bsz=162.4, num_updates=26700, lr=8.65485e-05, gnorm=0.967, clip=0, loss_scale=64, train_wall=51, gb_free=16, wall=15351
2023-08-02 14:01:12 | INFO | train_inner | epoch 019:    279 / 1474 loss=4.158, trans_loss=5.341, nll_loss=2.638, w2v_ctc_loss=1.318, task_loss=0, contrastive_loss=0.115, total=4186.65, n_correct=2689.19, ppl=6.22, accuracy=64.233, wps=8196.5, ups=1.96, wpb=4186.6, bsz=153.2, num_updates=26800, lr=8.63868e-05, gnorm=0.96, clip=0, loss_scale=64, train_wall=51, gb_free=14.3, wall=15402
2023-08-02 14:01:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-02 14:02:03 | INFO | train_inner | epoch 019:    380 / 1474 loss=4.171, trans_loss=5.35, nll_loss=2.649, w2v_ctc_loss=1.319, task_loss=0, contrastive_loss=0.212, total=4150.15, n_correct=2658.28, ppl=6.27, accuracy=64.053, wps=8100.9, ups=1.95, wpb=4150.1, bsz=152.3, num_updates=26900, lr=8.62261e-05, gnorm=0.978, clip=0, loss_scale=32, train_wall=51, gb_free=16.7, wall=15453
2023-08-02 14:02:55 | INFO | train_inner | epoch 019:    480 / 1474 loss=4.177, trans_loss=5.36, nll_loss=2.662, w2v_ctc_loss=1.333, task_loss=0, contrastive_loss=0.145, total=4113.89, n_correct=2625.25, ppl=6.33, accuracy=63.814, wps=8005.7, ups=1.95, wpb=4113.9, bsz=150.8, num_updates=27000, lr=8.60663e-05, gnorm=0.978, clip=0, loss_scale=32, train_wall=51, gb_free=17.5, wall=15504
2023-08-02 14:03:45 | INFO | train_inner | epoch 019:    580 / 1474 loss=4.185, trans_loss=5.363, nll_loss=2.668, w2v_ctc_loss=1.323, task_loss=0, contrastive_loss=0.277, total=4128.58, n_correct=2636.56, ppl=6.36, accuracy=63.861, wps=8127.2, ups=1.97, wpb=4128.6, bsz=153.1, num_updates=27100, lr=8.59074e-05, gnorm=0.977, clip=0, loss_scale=32, train_wall=50, gb_free=17, wall=15555
2023-08-02 14:04:36 | INFO | train_inner | epoch 019:    680 / 1474 loss=4.163, trans_loss=5.354, nll_loss=2.656, w2v_ctc_loss=1.3, task_loss=0, contrastive_loss=0.13, total=4201.56, n_correct=2684.77, ppl=6.3, accuracy=63.899, wps=8306.1, ups=1.98, wpb=4201.6, bsz=160.7, num_updates=27200, lr=8.57493e-05, gnorm=0.973, clip=0, loss_scale=32, train_wall=50, gb_free=18.1, wall=15606
2023-08-02 14:05:27 | INFO | train_inner | epoch 019:    780 / 1474 loss=4.179, trans_loss=5.359, nll_loss=2.662, w2v_ctc_loss=1.34, task_loss=0, contrastive_loss=0.139, total=4124.03, n_correct=2632.51, ppl=6.33, accuracy=63.833, wps=8035, ups=1.95, wpb=4124, bsz=149.5, num_updates=27300, lr=8.55921e-05, gnorm=0.988, clip=0, loss_scale=32, train_wall=51, gb_free=17.9, wall=15657
2023-08-02 14:06:18 | INFO | train_inner | epoch 019:    880 / 1474 loss=4.185, trans_loss=5.37, nll_loss=2.676, w2v_ctc_loss=1.336, task_loss=0, contrastive_loss=0.135, total=4177.8, n_correct=2657.95, ppl=6.39, accuracy=63.621, wps=8170.3, ups=1.96, wpb=4177.8, bsz=154.8, num_updates=27400, lr=8.54358e-05, gnorm=0.971, clip=0, loss_scale=32, train_wall=51, gb_free=15.3, wall=15708
2023-08-02 14:07:10 | INFO | train_inner | epoch 019:    980 / 1474 loss=4.228, trans_loss=5.386, nll_loss=2.699, w2v_ctc_loss=1.342, task_loss=0, contrastive_loss=0.597, total=4084.26, n_correct=2588.48, ppl=6.49, accuracy=63.377, wps=7973.8, ups=1.95, wpb=4084.3, bsz=152.9, num_updates=27500, lr=8.52803e-05, gnorm=1.002, clip=0, loss_scale=32, train_wall=51, gb_free=16.6, wall=15759
2023-08-02 14:08:02 | INFO | train_inner | epoch 019:   1080 / 1474 loss=4.203, trans_loss=5.387, nll_loss=2.699, w2v_ctc_loss=1.337, task_loss=0, contrastive_loss=0.209, total=4042.73, n_correct=2567.3, ppl=6.49, accuracy=63.504, wps=7797.5, ups=1.93, wpb=4042.7, bsz=147, num_updates=27600, lr=8.51257e-05, gnorm=1.005, clip=0, loss_scale=32, train_wall=51, gb_free=17.6, wall=15811
2023-08-02 14:08:53 | INFO | train_inner | epoch 019:   1180 / 1474 loss=4.213, trans_loss=5.382, nll_loss=2.693, w2v_ctc_loss=1.341, task_loss=0, contrastive_loss=0.373, total=4140.95, n_correct=2626, ppl=6.47, accuracy=63.415, wps=8085.6, ups=1.95, wpb=4140.9, bsz=154, num_updates=27700, lr=8.49719e-05, gnorm=0.981, clip=0, loss_scale=32, train_wall=51, gb_free=13.4, wall=15862
2023-08-02 14:09:43 | INFO | train_inner | epoch 019:   1280 / 1474 loss=4.193, trans_loss=5.383, nll_loss=2.693, w2v_ctc_loss=1.325, task_loss=0, contrastive_loss=0.167, total=4135.79, n_correct=2624.66, ppl=6.47, accuracy=63.462, wps=8169.8, ups=1.98, wpb=4135.8, bsz=149.8, num_updates=27800, lr=8.48189e-05, gnorm=0.977, clip=0, loss_scale=32, train_wall=50, gb_free=18.2, wall=15913
2023-08-02 14:10:35 | INFO | train_inner | epoch 019:   1380 / 1474 loss=4.184, trans_loss=5.372, nll_loss=2.68, w2v_ctc_loss=1.329, task_loss=0, contrastive_loss=0.137, total=4138.67, n_correct=2634.72, ppl=6.41, accuracy=63.661, wps=8079.6, ups=1.95, wpb=4138.7, bsz=150.8, num_updates=27900, lr=8.46668e-05, gnorm=0.974, clip=0, loss_scale=32, train_wall=51, gb_free=16.9, wall=15964
2023-08-02 14:11:22 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 14:11:45 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 4.315 | trans_loss 5.544 | nll_loss 2.817 | w2v_ctc_loss 1.334 | task_loss 0 | contrastive_loss 0.249 | total 4003.4 | n_correct 2490.4 | ppl 7.05 | accuracy 62.207 | uer 16.946 | wer 18.765 | raw_wer 18.765 | bleu 20.19 | wps 2276.8 | wpb 4003.4 | bsz 141.8 | num_updates 27994 | best_bleu 20.24
2023-08-02 14:11:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 27994 updates
2023-08-02 14:11:45 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.1908.pt
2023-08-02 14:11:49 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.1908.pt
2023-08-02 14:12:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.1908.pt (epoch 19 @ 27994 updates, score 20.19) (writing took 18.75298111140728 seconds)
2023-08-02 14:12:04 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-08-02 14:12:04 | INFO | train | epoch 019 | loss 4.186 | trans_loss 5.365 | nll_loss 2.67 | w2v_ctc_loss 1.331 | task_loss 0 | contrastive_loss 0.221 | total 4137.51 | n_correct 2637.93 | ppl 6.37 | accuracy 63.756 | wps 7590.8 | ups 1.83 | wpb 4137.5 | bsz 152.7 | num_updates 27994 | lr 8.45245e-05 | gnorm 0.981 | clip 0 | loss_scale 32 | train_wall 746 | gb_free 17.7 | wall 16054
2023-08-02 14:12:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 14:12:04 | INFO | fairseq.trainer | begin training epoch 20
2023-08-02 14:12:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 14:12:16 | INFO | train_inner | epoch 020:      6 / 1474 loss=4.199, trans_loss=5.372, nll_loss=2.68, w2v_ctc_loss=1.341, task_loss=0, contrastive_loss=0.313, total=4117.61, n_correct=2620.91, ppl=6.41, accuracy=63.651, wps=4078.9, ups=0.99, wpb=4117.6, bsz=151.5, num_updates=28000, lr=8.45154e-05, gnorm=1, clip=0, loss_scale=32, train_wall=50, gb_free=16.8, wall=16065
2023-08-02 14:12:16 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 14:12:39 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 4.309 | trans_loss 5.547 | nll_loss 2.817 | w2v_ctc_loss 1.304 | task_loss 0 | contrastive_loss 0.252 | total 4003.4 | n_correct 2486.7 | ppl 7.05 | accuracy 62.115 | uer 16.861 | wer 18.817 | raw_wer 18.817 | bleu 19.93 | wps 2155.7 | wpb 4003.4 | bsz 141.8 | num_updates 28000 | best_bleu 20.24
2023-08-02 14:12:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 28000 updates
2023-08-02 14:12:39 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_20_28000.pt
2023-08-02 14:12:43 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_20_28000.pt
2023-08-02 14:13:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_20_28000.pt (epoch 20 @ 28000 updates, score 19.93) (writing took 27.654764030128717 seconds)
2023-08-02 14:14:02 | INFO | train_inner | epoch 020:    106 / 1474 loss=4.139, trans_loss=5.322, nll_loss=2.613, w2v_ctc_loss=1.288, task_loss=0, contrastive_loss=0.155, total=4192.82, n_correct=2704.4, ppl=6.12, accuracy=64.501, wps=3948.2, ups=0.94, wpb=4192.8, bsz=156.4, num_updates=28100, lr=8.43649e-05, gnorm=0.966, clip=0, loss_scale=32, train_wall=51, gb_free=16.7, wall=16171
2023-08-02 14:14:53 | INFO | train_inner | epoch 020:    206 / 1474 loss=4.16, trans_loss=5.337, nll_loss=2.633, w2v_ctc_loss=1.307, task_loss=0, contrastive_loss=0.263, total=4155.9, n_correct=2667.05, ppl=6.2, accuracy=64.175, wps=8081.1, ups=1.94, wpb=4155.9, bsz=151.1, num_updates=28200, lr=8.42152e-05, gnorm=0.978, clip=0, loss_scale=32, train_wall=51, gb_free=12.4, wall=16223
2023-08-02 14:15:44 | INFO | train_inner | epoch 020:    306 / 1474 loss=4.144, trans_loss=5.323, nll_loss=2.617, w2v_ctc_loss=1.303, task_loss=0, contrastive_loss=0.138, total=4192.69, n_correct=2700.76, ppl=6.13, accuracy=64.416, wps=8236.1, ups=1.96, wpb=4192.7, bsz=163.8, num_updates=28300, lr=8.40663e-05, gnorm=0.982, clip=0, loss_scale=32, train_wall=50, gb_free=17.4, wall=16274
2023-08-02 14:16:35 | INFO | train_inner | epoch 020:    406 / 1474 loss=4.148, trans_loss=5.333, nll_loss=2.628, w2v_ctc_loss=1.301, task_loss=0, contrastive_loss=0.133, total=4116.96, n_correct=2647.14, ppl=6.18, accuracy=64.298, wps=8122.8, ups=1.97, wpb=4117, bsz=148.4, num_updates=28400, lr=8.39181e-05, gnorm=0.989, clip=0, loss_scale=32, train_wall=50, gb_free=13.2, wall=16324
2023-08-02 14:17:26 | INFO | train_inner | epoch 020:    506 / 1474 loss=4.172, trans_loss=5.351, nll_loss=2.651, w2v_ctc_loss=1.303, task_loss=0, contrastive_loss=0.313, total=4100.73, n_correct=2622.34, ppl=6.28, accuracy=63.948, wps=7990.3, ups=1.95, wpb=4100.7, bsz=149.2, num_updates=28500, lr=8.37708e-05, gnorm=0.99, clip=0, loss_scale=32, train_wall=51, gb_free=16.7, wall=16376
2023-08-02 14:18:17 | INFO | train_inner | epoch 020:    606 / 1474 loss=4.183, trans_loss=5.353, nll_loss=2.654, w2v_ctc_loss=1.323, task_loss=0, contrastive_loss=0.322, total=4101.99, n_correct=2621.37, ppl=6.3, accuracy=63.905, wps=8088.7, ups=1.97, wpb=4102, bsz=149.2, num_updates=28600, lr=8.36242e-05, gnorm=1.014, clip=0, loss_scale=32, train_wall=50, gb_free=14.1, wall=16426
2023-08-02 14:19:08 | INFO | train_inner | epoch 020:    706 / 1474 loss=4.168, trans_loss=5.351, nll_loss=2.653, w2v_ctc_loss=1.326, task_loss=0, contrastive_loss=0.118, total=4124.25, n_correct=2636.83, ppl=6.29, accuracy=63.935, wps=8104.3, ups=1.97, wpb=4124.2, bsz=148.6, num_updates=28700, lr=8.34784e-05, gnorm=0.986, clip=0, loss_scale=32, train_wall=50, gb_free=17.1, wall=16477
2023-08-02 14:19:59 | INFO | train_inner | epoch 020:    806 / 1474 loss=4.161, trans_loss=5.342, nll_loss=2.641, w2v_ctc_loss=1.32, task_loss=0, contrastive_loss=0.131, total=4153.23, n_correct=2664.51, ppl=6.24, accuracy=64.155, wps=8145.6, ups=1.96, wpb=4153.2, bsz=154.3, num_updates=28800, lr=8.33333e-05, gnorm=0.992, clip=0, loss_scale=32, train_wall=51, gb_free=18, wall=16528
2023-08-02 14:20:50 | INFO | train_inner | epoch 020:    906 / 1474 loss=4.22, trans_loss=5.367, nll_loss=2.675, w2v_ctc_loss=1.322, task_loss=0, contrastive_loss=0.717, total=4153.72, n_correct=2647.16, ppl=6.38, accuracy=63.73, wps=8052.8, ups=1.94, wpb=4153.7, bsz=160.3, num_updates=28900, lr=8.3189e-05, gnorm=0.996, clip=0, loss_scale=64, train_wall=51, gb_free=16.5, wall=16580
2023-08-02 14:21:42 | INFO | train_inner | epoch 020:   1006 / 1474 loss=4.163, trans_loss=5.35, nll_loss=2.651, w2v_ctc_loss=1.308, task_loss=0, contrastive_loss=0.14, total=4156.05, n_correct=2657.23, ppl=6.28, accuracy=63.936, wps=8103.7, ups=1.95, wpb=4156.1, bsz=152.6, num_updates=29000, lr=8.30455e-05, gnorm=0.98, clip=0, loss_scale=64, train_wall=51, gb_free=15.5, wall=16631
2023-08-02 14:22:32 | INFO | train_inner | epoch 020:   1106 / 1474 loss=4.187, trans_loss=5.355, nll_loss=2.659, w2v_ctc_loss=1.307, task_loss=0, contrastive_loss=0.406, total=4181.53, n_correct=2674.27, ppl=6.31, accuracy=63.954, wps=8229.4, ups=1.97, wpb=4181.5, bsz=160.1, num_updates=29100, lr=8.29027e-05, gnorm=0.987, clip=0, loss_scale=64, train_wall=50, gb_free=17.5, wall=16682
2023-08-02 14:23:23 | INFO | train_inner | epoch 020:   1206 / 1474 loss=4.168, trans_loss=5.348, nll_loss=2.649, w2v_ctc_loss=1.339, task_loss=0, contrastive_loss=0.109, total=4029.26, n_correct=2575.87, ppl=6.27, accuracy=63.929, wps=7930.4, ups=1.97, wpb=4029.3, bsz=141.2, num_updates=29200, lr=8.27606e-05, gnorm=1.001, clip=0, loss_scale=64, train_wall=50, gb_free=17.1, wall=16733
2023-08-02 14:24:15 | INFO | train_inner | epoch 020:   1306 / 1474 loss=4.18, trans_loss=5.363, nll_loss=2.67, w2v_ctc_loss=1.334, task_loss=0, contrastive_loss=0.134, total=4127.21, n_correct=2631.87, ppl=6.36, accuracy=63.769, wps=8011.4, ups=1.94, wpb=4127.2, bsz=150, num_updates=29300, lr=8.26192e-05, gnorm=0.991, clip=0, loss_scale=64, train_wall=51, gb_free=15.3, wall=16784
2023-08-02 14:25:06 | INFO | train_inner | epoch 020:   1406 / 1474 loss=4.182, trans_loss=5.368, nll_loss=2.675, w2v_ctc_loss=1.336, task_loss=0, contrastive_loss=0.123, total=4110.89, n_correct=2617.61, ppl=6.39, accuracy=63.675, wps=8067.3, ups=1.96, wpb=4110.9, bsz=145.8, num_updates=29400, lr=8.24786e-05, gnorm=0.997, clip=0, loss_scale=64, train_wall=50, gb_free=13.9, wall=16835
2023-08-02 14:25:40 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 14:26:02 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 4.304 | trans_loss 5.541 | nll_loss 2.814 | w2v_ctc_loss 1.303 | task_loss 0 | contrastive_loss 0.246 | total 4003.4 | n_correct 2476.9 | ppl 7.03 | accuracy 61.87 | uer 17.017 | wer 18.855 | raw_wer 18.855 | bleu 20.45 | wps 2341.9 | wpb 4003.4 | bsz 141.8 | num_updates 29468 | best_bleu 20.45
2023-08-02 14:26:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 29468 updates
2023-08-02 14:26:02 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 14:26:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 14:26:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt (epoch 20 @ 29468 updates, score 20.45) (writing took 23.72041222639382 seconds)
2023-08-02 14:26:27 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-08-02 14:26:27 | INFO | train | epoch 020 | loss 4.17 | trans_loss 5.348 | nll_loss 2.648 | w2v_ctc_loss 1.315 | task_loss 0 | contrastive_loss 0.23 | total 4138.65 | n_correct 2649.67 | ppl 6.27 | accuracy 64.022 | wps 7074.5 | ups 1.71 | wpb 4138.6 | bsz 152.8 | num_updates 29468 | lr 8.23834e-05 | gnorm 0.989 | clip 0 | loss_scale 64 | train_wall 746 | gb_free 17 | wall 16916
2023-08-02 14:26:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 14:26:27 | INFO | fairseq.trainer | begin training epoch 21
2023-08-02 14:26:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 14:26:51 | INFO | train_inner | epoch 021:     32 / 1474 loss=4.181, trans_loss=5.352, nll_loss=2.656, w2v_ctc_loss=1.307, task_loss=0, contrastive_loss=0.359, total=4166.35, n_correct=2665.18, ppl=6.3, accuracy=63.969, wps=3954.6, ups=0.95, wpb=4166.4, bsz=159.7, num_updates=29500, lr=8.23387e-05, gnorm=0.977, clip=0, loss_scale=64, train_wall=50, gb_free=11.9, wall=16941
2023-08-02 14:27:42 | INFO | train_inner | epoch 021:    132 / 1474 loss=4.145, trans_loss=5.31, nll_loss=2.599, w2v_ctc_loss=1.287, task_loss=0, contrastive_loss=0.345, total=4181.45, n_correct=2702.82, ppl=6.06, accuracy=64.638, wps=8175.9, ups=1.96, wpb=4181.4, bsz=158.8, num_updates=29600, lr=8.21995e-05, gnorm=0.979, clip=0, loss_scale=64, train_wall=51, gb_free=15.6, wall=16992
2023-08-02 14:28:33 | INFO | train_inner | epoch 021:    232 / 1474 loss=4.137, trans_loss=5.315, nll_loss=2.606, w2v_ctc_loss=1.281, task_loss=0, contrastive_loss=0.261, total=4167.12, n_correct=2688.63, ppl=6.09, accuracy=64.52, wps=8204.3, ups=1.97, wpb=4167.1, bsz=157.6, num_updates=29700, lr=8.2061e-05, gnorm=0.967, clip=0, loss_scale=64, train_wall=50, gb_free=12.4, wall=17043
2023-08-02 14:29:24 | INFO | train_inner | epoch 021:    332 / 1474 loss=4.158, trans_loss=5.327, nll_loss=2.621, w2v_ctc_loss=1.317, task_loss=0, contrastive_loss=0.26, total=4130.24, n_correct=2657.97, ppl=6.15, accuracy=64.354, wps=8026.7, ups=1.94, wpb=4130.2, bsz=151.8, num_updates=29800, lr=8.19232e-05, gnorm=0.978, clip=0, loss_scale=64, train_wall=51, gb_free=17.6, wall=17094
2023-08-02 14:30:15 | INFO | train_inner | epoch 021:    432 / 1474 loss=4.132, trans_loss=5.316, nll_loss=2.607, w2v_ctc_loss=1.288, task_loss=0, contrastive_loss=0.122, total=4186.28, n_correct=2702.58, ppl=6.09, accuracy=64.558, wps=8212.3, ups=1.96, wpb=4186.3, bsz=155.1, num_updates=29900, lr=8.17861e-05, gnorm=0.972, clip=0, loss_scale=64, train_wall=51, gb_free=17.7, wall=17145
2023-08-02 14:31:06 | INFO | train_inner | epoch 021:    532 / 1474 loss=4.139, trans_loss=5.317, nll_loss=2.608, w2v_ctc_loss=1.311, task_loss=0, contrastive_loss=0.115, total=4096.33, n_correct=2643.81, ppl=6.1, accuracy=64.541, wps=8072.4, ups=1.97, wpb=4096.3, bsz=149, num_updates=30000, lr=8.16497e-05, gnorm=0.988, clip=0, loss_scale=64, train_wall=50, gb_free=17.6, wall=17196
2023-08-02 14:31:06 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 14:31:29 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 4.326 | trans_loss 5.552 | nll_loss 2.825 | w2v_ctc_loss 1.346 | task_loss 0 | contrastive_loss 0.259 | total 4003.4 | n_correct 2483.7 | ppl 7.08 | accuracy 62.04 | uer 17.283 | wer 19.123 | raw_wer 19.123 | bleu 20.23 | wps 2248.5 | wpb 4003.4 | bsz 141.8 | num_updates 30000 | best_bleu 20.45
2023-08-02 14:31:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30000 updates
2023-08-02 14:31:29 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_21_30000.pt
2023-08-02 14:31:33 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_21_30000.pt
2023-08-02 14:32:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_21_30000.pt (epoch 21 @ 30000 updates, score 20.23) (writing took 32.25764653645456 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:0')
2023-08-02 14:32:54 | INFO | train_inner | epoch 021:    632 / 1474 loss=4.157, trans_loss=5.326, nll_loss=2.621, w2v_ctc_loss=1.279, task_loss=0, contrastive_loss=0.456, total=4215.02, n_correct=2713.82, ppl=6.15, accuracy=64.385, wps=3924.4, ups=0.93, wpb=4215, bsz=157.3, num_updates=30100, lr=8.15139e-05, gnorm=0.975, clip=0, loss_scale=64, train_wall=51, gb_free=16.9, wall=17303
2023-08-02 14:33:45 | INFO | train_inner | epoch 021:    732 / 1474 loss=4.151, trans_loss=5.334, nll_loss=2.632, w2v_ctc_loss=1.294, task_loss=0, contrastive_loss=0.177, total=4147.25, n_correct=2664.6, ppl=6.2, accuracy=64.25, wps=8096.6, ups=1.95, wpb=4147.2, bsz=154.3, num_updates=30200, lr=8.13788e-05, gnorm=0.979, clip=0, loss_scale=64, train_wall=51, gb_free=18.1, wall=17354
2023-08-02 14:34:36 | INFO | train_inner | epoch 021:    832 / 1474 loss=4.169, trans_loss=5.348, nll_loss=2.649, w2v_ctc_loss=1.315, task_loss=0, contrastive_loss=0.202, total=4071.46, n_correct=2607.48, ppl=6.27, accuracy=64.043, wps=7931.1, ups=1.95, wpb=4071.5, bsz=147.1, num_updates=30300, lr=8.12444e-05, gnorm=0.999, clip=0, loss_scale=64, train_wall=51, gb_free=17.6, wall=17406
2023-08-02 14:35:27 | INFO | train_inner | epoch 021:    932 / 1474 loss=4.144, trans_loss=5.326, nll_loss=2.62, w2v_ctc_loss=1.298, task_loss=0, contrastive_loss=0.149, total=4098.4, n_correct=2635.82, ppl=6.15, accuracy=64.313, wps=8003.1, ups=1.95, wpb=4098.4, bsz=150.7, num_updates=30400, lr=8.11107e-05, gnorm=0.98, clip=0, loss_scale=64, train_wall=51, gb_free=16.3, wall=17457
2023-08-02 14:36:19 | INFO | train_inner | epoch 021:   1032 / 1474 loss=4.17, trans_loss=5.353, nll_loss=2.656, w2v_ctc_loss=1.322, task_loss=0, contrastive_loss=0.143, total=4099.06, n_correct=2620.05, ppl=6.3, accuracy=63.918, wps=7991.6, ups=1.95, wpb=4099.1, bsz=148.8, num_updates=30500, lr=8.09776e-05, gnorm=0.983, clip=0, loss_scale=64, train_wall=51, gb_free=16.8, wall=17508
2023-08-02 14:37:09 | INFO | train_inner | epoch 021:   1132 / 1474 loss=4.158, trans_loss=5.34, nll_loss=2.638, w2v_ctc_loss=1.314, task_loss=0, contrastive_loss=0.151, total=4127.16, n_correct=2647.59, ppl=6.23, accuracy=64.15, wps=8126.4, ups=1.97, wpb=4127.2, bsz=147.8, num_updates=30600, lr=8.08452e-05, gnorm=0.996, clip=0, loss_scale=64, train_wall=50, gb_free=16.1, wall=17559
2023-08-02 14:38:00 | INFO | train_inner | epoch 021:   1232 / 1474 loss=4.161, trans_loss=5.336, nll_loss=2.634, w2v_ctc_loss=1.303, task_loss=0, contrastive_loss=0.254, total=4159.29, n_correct=2670.95, ppl=6.21, accuracy=64.216, wps=8161.4, ups=1.96, wpb=4159.3, bsz=156.2, num_updates=30700, lr=8.07134e-05, gnorm=0.978, clip=0, loss_scale=64, train_wall=51, gb_free=18.1, wall=17610
2023-08-02 14:38:52 | INFO | train_inner | epoch 021:   1332 / 1474 loss=4.151, trans_loss=5.332, nll_loss=2.63, w2v_ctc_loss=1.3, task_loss=0, contrastive_loss=0.173, total=4139.42, n_correct=2662.28, ppl=6.19, accuracy=64.315, wps=8058.9, ups=1.95, wpb=4139.4, bsz=155.9, num_updates=30800, lr=8.05823e-05, gnorm=0.988, clip=0, loss_scale=64, train_wall=51, gb_free=14.8, wall=17661
2023-08-02 14:39:43 | INFO | train_inner | epoch 021:   1432 / 1474 loss=4.191, trans_loss=5.358, nll_loss=2.663, w2v_ctc_loss=1.347, task_loss=0, contrastive_loss=0.272, total=4123.35, n_correct=2632.27, ppl=6.33, accuracy=63.838, wps=8017.4, ups=1.94, wpb=4123.4, bsz=150.7, num_updates=30900, lr=8.04518e-05, gnorm=1, clip=0, loss_scale=64, train_wall=51, gb_free=16.8, wall=17713
2023-08-02 14:40:05 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:7')
2023-08-02 14:40:27 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 4.321 | trans_loss 5.552 | nll_loss 2.825 | w2v_ctc_loss 1.327 | task_loss 0 | contrastive_loss 0.265 | total 4003.4 | n_correct 2490.3 | ppl 7.08 | accuracy 62.205 | uer 17.105 | wer 19.093 | raw_wer 19.093 | bleu 20.46 | wps 2413.4 | wpb 4003.4 | bsz 141.8 | num_updates 30942 | best_bleu 20.46
2023-08-02 14:40:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30942 updates
2023-08-02 14:40:27 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 14:40:40 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 14:40:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt (epoch 21 @ 30942 updates, score 20.46) (writing took 26.10998446121812 seconds)
2023-08-02 14:40:53 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-08-02 14:40:53 | INFO | train | epoch 021 | loss 4.155 | trans_loss 5.331 | nll_loss 2.628 | w2v_ctc_loss 1.303 | task_loss 0 | contrastive_loss 0.229 | total 4138.65 | n_correct 2660.6 | ppl 6.18 | accuracy 64.287 | wps 7038.9 | ups 1.7 | wpb 4138.6 | bsz 152.8 | num_updates 30942 | lr 8.03972e-05 | gnorm 0.982 | clip 0 | loss_scale 128 | train_wall 748 | gb_free 15.9 | wall 17783
2023-08-02 14:40:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 14:40:53 | INFO | fairseq.trainer | begin training epoch 22
2023-08-02 14:40:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 14:41:31 | INFO | train_inner | epoch 022:     58 / 1474 loss=4.128, trans_loss=5.311, nll_loss=2.602, w2v_ctc_loss=1.287, task_loss=0, contrastive_loss=0.116, total=4141.35, n_correct=2675.43, ppl=6.07, accuracy=64.603, wps=3825.1, ups=0.92, wpb=4141.4, bsz=150, num_updates=31000, lr=8.03219e-05, gnorm=0.979, clip=0, loss_scale=128, train_wall=51, gb_free=16.8, wall=17821
2023-08-02 14:42:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-02 14:42:24 | INFO | train_inner | epoch 022:    159 / 1474 loss=4.123, trans_loss=5.301, nll_loss=2.588, w2v_ctc_loss=1.289, task_loss=0, contrastive_loss=0.142, total=4104.33, n_correct=2656.1, ppl=6.01, accuracy=64.715, wps=7875.7, ups=1.92, wpb=4104.3, bsz=152.5, num_updates=31100, lr=8.01927e-05, gnorm=0.978, clip=0, loss_scale=64, train_wall=52, gb_free=17.7, wall=17873
2023-08-02 14:43:15 | INFO | train_inner | epoch 022:    259 / 1474 loss=4.108, trans_loss=5.29, nll_loss=2.575, w2v_ctc_loss=1.262, task_loss=0, contrastive_loss=0.138, total=4247.73, n_correct=2762.86, ppl=5.96, accuracy=65.043, wps=8271.1, ups=1.95, wpb=4247.7, bsz=161.6, num_updates=31200, lr=8.00641e-05, gnorm=0.967, clip=0, loss_scale=64, train_wall=51, gb_free=14.4, wall=17925
2023-08-02 14:43:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-02 14:44:07 | INFO | train_inner | epoch 022:    360 / 1474 loss=4.16, trans_loss=5.322, nll_loss=2.616, w2v_ctc_loss=1.304, task_loss=0, contrastive_loss=0.371, total=4186, n_correct=2700.04, ppl=6.13, accuracy=64.502, wps=7982.3, ups=1.91, wpb=4186, bsz=155, num_updates=31300, lr=7.99361e-05, gnorm=0.995, clip=0, loss_scale=32, train_wall=52, gb_free=17.9, wall=17977
2023-08-02 14:44:59 | INFO | train_inner | epoch 022:    460 / 1474 loss=4.144, trans_loss=5.319, nll_loss=2.611, w2v_ctc_loss=1.29, task_loss=0, contrastive_loss=0.236, total=4132.62, n_correct=2665.86, ppl=6.11, accuracy=64.508, wps=8090.5, ups=1.96, wpb=4132.6, bsz=148.6, num_updates=31400, lr=7.98087e-05, gnorm=0.986, clip=0, loss_scale=32, train_wall=51, gb_free=17, wall=18028
2023-08-02 14:45:50 | INFO | train_inner | epoch 022:    560 / 1474 loss=4.142, trans_loss=5.32, nll_loss=2.612, w2v_ctc_loss=1.309, task_loss=0, contrastive_loss=0.138, total=4155.5, n_correct=2679.17, ppl=6.12, accuracy=64.473, wps=8054.3, ups=1.94, wpb=4155.5, bsz=153.7, num_updates=31500, lr=7.96819e-05, gnorm=0.985, clip=0, loss_scale=32, train_wall=51, gb_free=16.9, wall=18080
2023-08-02 14:46:41 | INFO | train_inner | epoch 022:    660 / 1474 loss=4.128, trans_loss=5.302, nll_loss=2.59, w2v_ctc_loss=1.267, task_loss=0, contrastive_loss=0.307, total=4147.84, n_correct=2686.4, ppl=6.02, accuracy=64.766, wps=8163.1, ups=1.97, wpb=4147.8, bsz=156.6, num_updates=31600, lr=7.95557e-05, gnorm=0.984, clip=0, loss_scale=32, train_wall=50, gb_free=13.7, wall=18131
2023-08-02 14:47:32 | INFO | train_inner | epoch 022:    760 / 1474 loss=4.131, trans_loss=5.31, nll_loss=2.6, w2v_ctc_loss=1.295, task_loss=0, contrastive_loss=0.14, total=4166.89, n_correct=2691.56, ppl=6.06, accuracy=64.594, wps=8112.3, ups=1.95, wpb=4166.9, bsz=152.2, num_updates=31700, lr=7.94301e-05, gnorm=0.986, clip=0, loss_scale=32, train_wall=51, gb_free=16.3, wall=18182
2023-08-02 14:48:24 | INFO | train_inner | epoch 022:    860 / 1474 loss=4.142, trans_loss=5.328, nll_loss=2.623, w2v_ctc_loss=1.299, task_loss=0, contrastive_loss=0.115, total=4074.75, n_correct=2617.84, ppl=6.16, accuracy=64.245, wps=7919.8, ups=1.94, wpb=4074.8, bsz=144.2, num_updates=31800, lr=7.93052e-05, gnorm=0.995, clip=0, loss_scale=32, train_wall=51, gb_free=17.9, wall=18233
2023-08-02 14:49:15 | INFO | train_inner | epoch 022:    960 / 1474 loss=4.132, trans_loss=5.317, nll_loss=2.611, w2v_ctc_loss=1.286, task_loss=0, contrastive_loss=0.119, total=4136.34, n_correct=2668.88, ppl=6.11, accuracy=64.523, wps=8091.7, ups=1.96, wpb=4136.3, bsz=151.8, num_updates=31900, lr=7.91808e-05, gnorm=0.984, clip=0, loss_scale=32, train_wall=51, gb_free=17, wall=18285
2023-08-02 14:50:06 | INFO | train_inner | epoch 022:   1060 / 1474 loss=4.149, trans_loss=5.314, nll_loss=2.607, w2v_ctc_loss=1.278, task_loss=0, contrastive_loss=0.459, total=4157.21, n_correct=2685.17, ppl=6.09, accuracy=64.591, wps=8176.5, ups=1.97, wpb=4157.2, bsz=157.7, num_updates=32000, lr=7.90569e-05, gnorm=0.974, clip=0, loss_scale=32, train_wall=50, gb_free=12.7, wall=18335
2023-08-02 14:50:06 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 14:50:29 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 4.317 | trans_loss 5.541 | nll_loss 2.817 | w2v_ctc_loss 1.343 | task_loss 0 | contrastive_loss 0.251 | total 4003.4 | n_correct 2489.4 | ppl 7.05 | accuracy 62.182 | uer 16.941 | wer 18.866 | raw_wer 18.866 | bleu 20.37 | wps 2251 | wpb 4003.4 | bsz 141.8 | num_updates 32000 | best_bleu 20.46
2023-08-02 14:50:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32000 updates
2023-08-02 14:50:29 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_22_32000.pt
2023-08-02 14:50:33 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_22_32000.pt
2023-08-02 14:51:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_22_32000.pt (epoch 22 @ 32000 updates, score 20.37) (writing took 32.72688912600279 seconds)
2023-08-02 14:51:53 | INFO | train_inner | epoch 022:   1160 / 1474 loss=4.163, trans_loss=5.341, nll_loss=2.641, w2v_ctc_loss=1.31, task_loss=0, contrastive_loss=0.219, total=4092.91, n_correct=2626.4, ppl=6.24, accuracy=64.17, wps=3808.7, ups=0.93, wpb=4092.9, bsz=147.2, num_updates=32100, lr=7.89337e-05, gnorm=0.993, clip=0, loss_scale=32, train_wall=50, gb_free=16.5, wall=18443
2023-08-02 14:52:45 | INFO | train_inner | epoch 022:   1260 / 1474 loss=4.149, trans_loss=5.326, nll_loss=2.623, w2v_ctc_loss=1.292, task_loss=0, contrastive_loss=0.206, total=4182.65, n_correct=2686.44, ppl=6.16, accuracy=64.228, wps=8145.1, ups=1.95, wpb=4182.6, bsz=161.8, num_updates=32200, lr=7.8811e-05, gnorm=0.985, clip=0, loss_scale=32, train_wall=51, gb_free=17, wall=18494
2023-08-02 14:53:36 | INFO | train_inner | epoch 022:   1360 / 1474 loss=4.136, trans_loss=5.317, nll_loss=2.611, w2v_ctc_loss=1.275, task_loss=0, contrastive_loss=0.254, total=4071.58, n_correct=2627.08, ppl=6.11, accuracy=64.522, wps=7921.2, ups=1.95, wpb=4071.6, bsz=150.3, num_updates=32300, lr=7.86889e-05, gnorm=1, clip=0, loss_scale=32, train_wall=51, gb_free=17.1, wall=18546
2023-08-02 14:54:27 | INFO | train_inner | epoch 022:   1460 / 1474 loss=4.154, trans_loss=5.337, nll_loss=2.636, w2v_ctc_loss=1.308, task_loss=0, contrastive_loss=0.144, total=4077.83, n_correct=2617.25, ppl=6.21, accuracy=64.182, wps=8037.6, ups=1.97, wpb=4077.8, bsz=144, num_updates=32400, lr=7.85674e-05, gnorm=0.993, clip=0, loss_scale=32, train_wall=50, gb_free=16.5, wall=18596
2023-08-02 14:54:34 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 14:54:56 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 4.308 | trans_loss 5.537 | nll_loss 2.809 | w2v_ctc_loss 1.32 | task_loss 0 | contrastive_loss 0.257 | total 4003.4 | n_correct 2487.1 | ppl 7.01 | accuracy 62.125 | uer 17.214 | wer 19.078 | raw_wer 19.078 | bleu 20.48 | wps 2390.5 | wpb 4003.4 | bsz 141.8 | num_updates 32414 | best_bleu 20.48
2023-08-02 14:54:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32414 updates
2023-08-02 14:54:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 14:55:09 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 14:55:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt (epoch 22 @ 32414 updates, score 20.48) (writing took 24.868762131780386 seconds)
2023-08-02 14:55:22 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-08-02 14:55:22 | INFO | train | epoch 022 | loss 4.139 | trans_loss 5.317 | nll_loss 2.609 | w2v_ctc_loss 1.291 | task_loss 0 | contrastive_loss 0.21 | total 4136.68 | n_correct 2668.85 | ppl 6.1 | accuracy 64.517 | wps 7011.9 | ups 1.7 | wpb 4136.7 | bsz 152.5 | num_updates 32414 | lr 7.85505e-05 | gnorm 0.986 | clip 0 | loss_scale 32 | train_wall 748 | gb_free 12.4 | wall 18651
2023-08-02 14:55:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 14:55:22 | INFO | fairseq.trainer | begin training epoch 23
2023-08-02 14:55:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 14:56:14 | INFO | train_inner | epoch 023:     86 / 1474 loss=4.123, trans_loss=5.297, nll_loss=2.584, w2v_ctc_loss=1.3, task_loss=0, contrastive_loss=0.13, total=4089.8, n_correct=2655.12, ppl=5.99, accuracy=64.921, wps=3817, ups=0.93, wpb=4089.8, bsz=149.7, num_updates=32500, lr=7.84465e-05, gnorm=0.992, clip=0, loss_scale=32, train_wall=51, gb_free=17.1, wall=18703
2023-08-02 14:57:05 | INFO | train_inner | epoch 023:    186 / 1474 loss=4.105, trans_loss=5.286, nll_loss=2.57, w2v_ctc_loss=1.269, task_loss=0, contrastive_loss=0.125, total=4117.76, n_correct=2675.54, ppl=5.94, accuracy=64.976, wps=8073.7, ups=1.96, wpb=4117.8, bsz=148, num_updates=32600, lr=7.8326e-05, gnorm=0.993, clip=0, loss_scale=32, train_wall=50, gb_free=16, wall=18754
2023-08-02 14:57:56 | INFO | train_inner | epoch 023:    286 / 1474 loss=4.126, trans_loss=5.301, nll_loss=2.589, w2v_ctc_loss=1.267, task_loss=0, contrastive_loss=0.277, total=4144.73, n_correct=2683.15, ppl=6.02, accuracy=64.736, wps=8039.2, ups=1.94, wpb=4144.7, bsz=152, num_updates=32700, lr=7.82062e-05, gnorm=0.991, clip=0, loss_scale=32, train_wall=51, gb_free=17.9, wall=18806
2023-08-02 14:58:47 | INFO | train_inner | epoch 023:    386 / 1474 loss=4.104, trans_loss=5.289, nll_loss=2.573, w2v_ctc_loss=1.264, task_loss=0, contrastive_loss=0.11, total=4126.79, n_correct=2683.68, ppl=5.95, accuracy=65.031, wps=8082.3, ups=1.96, wpb=4126.8, bsz=148.2, num_updates=32800, lr=7.80869e-05, gnorm=0.975, clip=0, loss_scale=32, train_wall=51, gb_free=17.5, wall=18857
2023-08-02 14:59:38 | INFO | train_inner | epoch 023:    486 / 1474 loss=4.122, trans_loss=5.298, nll_loss=2.585, w2v_ctc_loss=1.267, task_loss=0, contrastive_loss=0.226, total=4150.15, n_correct=2688.21, ppl=6, accuracy=64.774, wps=8178.3, ups=1.97, wpb=4150.1, bsz=156, num_updates=32900, lr=7.79681e-05, gnorm=0.987, clip=0, loss_scale=32, train_wall=50, gb_free=16.7, wall=18908
2023-08-02 15:00:29 | INFO | train_inner | epoch 023:    586 / 1474 loss=4.095, trans_loss=5.276, nll_loss=2.557, w2v_ctc_loss=1.254, task_loss=0, contrastive_loss=0.123, total=4174.6, n_correct=2721.23, ppl=5.88, accuracy=65.185, wps=8189.7, ups=1.96, wpb=4174.6, bsz=158.2, num_updates=33000, lr=7.78499e-05, gnorm=0.97, clip=0, loss_scale=32, train_wall=51, gb_free=16.9, wall=18959
2023-08-02 15:01:20 | INFO | train_inner | epoch 023:    686 / 1474 loss=4.122, trans_loss=5.296, nll_loss=2.583, w2v_ctc_loss=1.281, task_loss=0, contrastive_loss=0.198, total=4136.6, n_correct=2681.62, ppl=5.99, accuracy=64.827, wps=8104.7, ups=1.96, wpb=4136.6, bsz=150.6, num_updates=33100, lr=7.77322e-05, gnorm=0.98, clip=0, loss_scale=32, train_wall=51, gb_free=18.1, wall=19010
2023-08-02 15:02:11 | INFO | train_inner | epoch 023:    786 / 1474 loss=4.122, trans_loss=5.302, nll_loss=2.59, w2v_ctc_loss=1.279, task_loss=0, contrastive_loss=0.156, total=4147.22, n_correct=2686.56, ppl=6.02, accuracy=64.78, wps=8184.3, ups=1.97, wpb=4147.2, bsz=152.6, num_updates=33200, lr=7.76151e-05, gnorm=0.992, clip=0, loss_scale=32, train_wall=50, gb_free=17.7, wall=19061
2023-08-02 15:03:02 | INFO | train_inner | epoch 023:    886 / 1474 loss=4.13, trans_loss=5.297, nll_loss=2.585, w2v_ctc_loss=1.275, task_loss=0, contrastive_loss=0.32, total=4193.16, n_correct=2720.75, ppl=6, accuracy=64.885, wps=8202.6, ups=1.96, wpb=4193.2, bsz=163.6, num_updates=33300, lr=7.74984e-05, gnorm=0.989, clip=0, loss_scale=32, train_wall=51, gb_free=16.4, wall=19112
2023-08-02 15:03:53 | INFO | train_inner | epoch 023:    986 / 1474 loss=4.142, trans_loss=5.3, nll_loss=2.588, w2v_ctc_loss=1.255, task_loss=0, contrastive_loss=0.628, total=4164.33, n_correct=2697.59, ppl=6.01, accuracy=64.778, wps=8150.5, ups=1.96, wpb=4164.3, bsz=155.1, num_updates=33400, lr=7.73823e-05, gnorm=0.975, clip=0, loss_scale=64, train_wall=51, gb_free=18, wall=19163
2023-08-02 15:04:44 | INFO | train_inner | epoch 023:   1086 / 1474 loss=4.127, trans_loss=5.306, nll_loss=2.596, w2v_ctc_loss=1.292, task_loss=0, contrastive_loss=0.136, total=4088.37, n_correct=2641.55, ppl=6.05, accuracy=64.611, wps=8008.2, ups=1.96, wpb=4088.4, bsz=144.8, num_updates=33500, lr=7.72667e-05, gnorm=0.985, clip=0, loss_scale=64, train_wall=51, gb_free=16, wall=19214
2023-08-02 15:05:35 | INFO | train_inner | epoch 023:   1186 / 1474 loss=4.123, trans_loss=5.304, nll_loss=2.594, w2v_ctc_loss=1.285, task_loss=0, contrastive_loss=0.123, total=4162.3, n_correct=2695.25, ppl=6.04, accuracy=64.754, wps=8125.8, ups=1.95, wpb=4162.3, bsz=154.5, num_updates=33600, lr=7.71517e-05, gnorm=0.987, clip=0, loss_scale=64, train_wall=51, gb_free=16.1, wall=19265
2023-08-02 15:06:26 | INFO | train_inner | epoch 023:   1286 / 1474 loss=4.125, trans_loss=5.306, nll_loss=2.596, w2v_ctc_loss=1.283, task_loss=0, contrastive_loss=0.142, total=4131.74, n_correct=2671.98, ppl=6.05, accuracy=64.67, wps=8108.6, ups=1.96, wpb=4131.7, bsz=154.4, num_updates=33700, lr=7.70371e-05, gnorm=0.985, clip=0, loss_scale=64, train_wall=51, gb_free=17.4, wall=19316
2023-08-02 15:07:18 | INFO | train_inner | epoch 023:   1386 / 1474 loss=4.151, trans_loss=5.327, nll_loss=2.625, w2v_ctc_loss=1.289, task_loss=0, contrastive_loss=0.254, total=4141.25, n_correct=2664.65, ppl=6.17, accuracy=64.344, wps=8072.8, ups=1.95, wpb=4141.2, bsz=152.4, num_updates=33800, lr=7.69231e-05, gnorm=0.995, clip=0, loss_scale=64, train_wall=51, gb_free=17.2, wall=19367
2023-08-02 15:08:03 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 15:08:26 | INFO | dev_st | epoch 023 | valid on 'dev_st' subset | loss 4.314 | trans_loss 5.54 | nll_loss 2.812 | w2v_ctc_loss 1.337 | task_loss 0 | contrastive_loss 0.259 | total 4003.4 | n_correct 2496.2 | ppl 7.02 | accuracy 62.352 | uer 16.784 | wer 18.564 | raw_wer 18.564 | bleu 20.33 | wps 2209.8 | wpb 4003.4 | bsz 141.8 | num_updates 33888 | best_bleu 20.48
2023-08-02 15:08:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 33888 updates
2023-08-02 15:08:26 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.3300.pt
2023-08-02 15:08:29 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.3300.pt
2023-08-02 15:08:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.3300.pt (epoch 23 @ 33888 updates, score 20.33) (writing took 14.423690777271986 seconds)
2023-08-02 15:08:41 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-08-02 15:08:41 | INFO | train | epoch 023 | loss 4.125 | trans_loss 5.301 | nll_loss 2.589 | w2v_ctc_loss 1.275 | task_loss 0 | contrastive_loss 0.227 | total 4138.65 | n_correct 2681.14 | ppl 6.02 | accuracy 64.783 | wps 7635.3 | ups 1.84 | wpb 4138.6 | bsz 152.8 | num_updates 33888 | lr 7.68231e-05 | gnorm 0.986 | clip 0 | loss_scale 64 | train_wall 746 | gb_free 14.3 | wall 19450
2023-08-02 15:08:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 15:08:41 | INFO | fairseq.trainer | begin training epoch 24
2023-08-02 15:08:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 15:08:55 | INFO | train_inner | epoch 024:     12 / 1474 loss=4.153, trans_loss=5.321, nll_loss=2.617, w2v_ctc_loss=1.272, task_loss=0, contrastive_loss=0.407, total=4095.53, n_correct=2640.97, ppl=6.13, accuracy=64.484, wps=4209.7, ups=1.03, wpb=4095.5, bsz=153.1, num_updates=33900, lr=7.68095e-05, gnorm=0.999, clip=0, loss_scale=64, train_wall=51, gb_free=17.7, wall=19465
2023-08-02 15:08:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-02 15:09:46 | INFO | train_inner | epoch 024:    113 / 1474 loss=4.097, trans_loss=5.267, nll_loss=2.545, w2v_ctc_loss=1.247, task_loss=0, contrastive_loss=0.276, total=4153.34, n_correct=2713.51, ppl=5.83, accuracy=65.333, wps=8090.8, ups=1.95, wpb=4153.3, bsz=158.6, num_updates=34000, lr=7.66965e-05, gnorm=0.968, clip=0, loss_scale=32, train_wall=51, gb_free=16.8, wall=19516
2023-08-02 15:09:46 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 15:10:10 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 4.314 | trans_loss 5.546 | nll_loss 2.814 | w2v_ctc_loss 1.321 | task_loss 0 | contrastive_loss 0.255 | total 4003.4 | n_correct 2486.9 | ppl 7.03 | accuracy 62.12 | uer 16.728 | wer 18.59 | raw_wer 18.59 | bleu 20.28 | wps 2236.1 | wpb 4003.4 | bsz 141.8 | num_updates 34000 | best_bleu 20.48
2023-08-02 15:10:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 34000 updates
2023-08-02 15:10:10 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_24_34000.pt
2023-08-02 15:10:14 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_24_34000.pt
2023-08-02 15:10:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_24_34000.pt (epoch 24 @ 34000 updates, score 20.28) (writing took 32.71350806392729 seconds)
2023-08-02 15:11:35 | INFO | train_inner | epoch 024:    213 / 1474 loss=4.122, trans_loss=5.277, nll_loss=2.56, w2v_ctc_loss=1.239, task_loss=0, contrastive_loss=0.554, total=4251.29, n_correct=2771.56, ppl=5.9, accuracy=65.193, wps=3923.8, ups=0.92, wpb=4251.3, bsz=170.4, num_updates=34100, lr=7.6584e-05, gnorm=0.969, clip=0, loss_scale=32, train_wall=51, gb_free=17.2, wall=19624
2023-08-02 15:12:26 | INFO | train_inner | epoch 024:    313 / 1474 loss=4.085, trans_loss=5.268, nll_loss=2.546, w2v_ctc_loss=1.246, task_loss=0, contrastive_loss=0.114, total=4128.18, n_correct=2694.51, ppl=5.84, accuracy=65.271, wps=8116.4, ups=1.97, wpb=4128.2, bsz=152.8, num_updates=34200, lr=7.64719e-05, gnorm=0.985, clip=0, loss_scale=32, train_wall=50, gb_free=16.6, wall=19675
2023-08-02 15:13:17 | INFO | train_inner | epoch 024:    413 / 1474 loss=4.131, trans_loss=5.293, nll_loss=2.579, w2v_ctc_loss=1.272, task_loss=0, contrastive_loss=0.396, total=4158.92, n_correct=2699.84, ppl=5.98, accuracy=64.917, wps=8102.9, ups=1.95, wpb=4158.9, bsz=149.9, num_updates=34300, lr=7.63604e-05, gnorm=1.002, clip=0, loss_scale=32, train_wall=51, gb_free=17, wall=19726
2023-08-02 15:14:08 | INFO | train_inner | epoch 024:    513 / 1474 loss=4.108, trans_loss=5.281, nll_loss=2.563, w2v_ctc_loss=1.265, task_loss=0, contrastive_loss=0.254, total=4144.91, n_correct=2696.88, ppl=5.91, accuracy=65.065, wps=8042.5, ups=1.94, wpb=4144.9, bsz=151.6, num_updates=34400, lr=7.62493e-05, gnorm=0.977, clip=0, loss_scale=32, train_wall=51, gb_free=17.4, wall=19778
2023-08-02 15:15:00 | INFO | train_inner | epoch 024:    613 / 1474 loss=4.102, trans_loss=5.282, nll_loss=2.565, w2v_ctc_loss=1.253, task_loss=0, contrastive_loss=0.177, total=4165.3, n_correct=2708.72, ppl=5.92, accuracy=65.031, wps=8104.9, ups=1.95, wpb=4165.3, bsz=153.8, num_updates=34500, lr=7.61387e-05, gnorm=0.984, clip=0, loss_scale=32, train_wall=51, gb_free=16.2, wall=19829
2023-08-02 15:15:51 | INFO | train_inner | epoch 024:    713 / 1474 loss=4.113, trans_loss=5.293, nll_loss=2.579, w2v_ctc_loss=1.261, task_loss=0, contrastive_loss=0.204, total=4102.21, n_correct=2661.92, ppl=5.98, accuracy=64.89, wps=8026, ups=1.96, wpb=4102.2, bsz=147.5, num_updates=34600, lr=7.60286e-05, gnorm=0.989, clip=0, loss_scale=32, train_wall=51, gb_free=17.4, wall=19881
2023-08-02 15:16:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-02 15:16:43 | INFO | train_inner | epoch 024:    814 / 1474 loss=4.105, trans_loss=5.288, nll_loss=2.573, w2v_ctc_loss=1.255, task_loss=0, contrastive_loss=0.156, total=4119.44, n_correct=2676.89, ppl=5.95, accuracy=64.982, wps=7922.2, ups=1.92, wpb=4119.4, bsz=154.1, num_updates=34700, lr=7.5919e-05, gnorm=0.982, clip=0, loss_scale=16, train_wall=52, gb_free=16.8, wall=19933
2023-08-02 15:17:34 | INFO | train_inner | epoch 024:    914 / 1474 loss=4.116, trans_loss=5.299, nll_loss=2.585, w2v_ctc_loss=1.281, task_loss=0, contrastive_loss=0.11, total=4041.44, n_correct=2617.82, ppl=6, accuracy=64.774, wps=7947.4, ups=1.97, wpb=4041.4, bsz=140.3, num_updates=34800, lr=7.58098e-05, gnorm=0.994, clip=0, loss_scale=16, train_wall=50, gb_free=16.7, wall=19983
2023-08-02 15:18:25 | INFO | train_inner | epoch 024:   1014 / 1474 loss=4.102, trans_loss=5.291, nll_loss=2.576, w2v_ctc_loss=1.249, task_loss=0, contrastive_loss=0.114, total=4128.8, n_correct=2680.53, ppl=5.96, accuracy=64.923, wps=8076.8, ups=1.96, wpb=4128.8, bsz=148.2, num_updates=34900, lr=7.57011e-05, gnorm=0.998, clip=0, loss_scale=16, train_wall=51, gb_free=15.5, wall=20035
2023-08-02 15:19:16 | INFO | train_inner | epoch 024:   1114 / 1474 loss=4.104, trans_loss=5.279, nll_loss=2.562, w2v_ctc_loss=1.26, task_loss=0, contrastive_loss=0.199, total=4130.49, n_correct=2687.68, ppl=5.9, accuracy=65.069, wps=8090.2, ups=1.96, wpb=4130.5, bsz=154.4, num_updates=35000, lr=7.55929e-05, gnorm=0.983, clip=0, loss_scale=16, train_wall=51, gb_free=16.5, wall=20086
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:0')
2023-08-02 15:20:08 | INFO | train_inner | epoch 024:   1214 / 1474 loss=4.111, trans_loss=5.294, nll_loss=2.582, w2v_ctc_loss=1.253, task_loss=0, contrastive_loss=0.18, total=4157.47, n_correct=2698.57, ppl=5.99, accuracy=64.909, wps=8036.6, ups=1.93, wpb=4157.5, bsz=155.6, num_updates=35100, lr=7.54851e-05, gnorm=0.994, clip=0, loss_scale=16, train_wall=51, gb_free=13.3, wall=20137
2023-08-02 15:20:59 | INFO | train_inner | epoch 024:   1314 / 1474 loss=4.12, trans_loss=5.299, nll_loss=2.587, w2v_ctc_loss=1.286, task_loss=0, contrastive_loss=0.129, total=4107.23, n_correct=2663.63, ppl=6.01, accuracy=64.852, wps=7980.9, ups=1.94, wpb=4107.2, bsz=147.2, num_updates=35200, lr=7.53778e-05, gnorm=0.99, clip=0, loss_scale=16, train_wall=51, gb_free=17.9, wall=20189
2023-08-02 15:21:50 | INFO | train_inner | epoch 024:   1414 / 1474 loss=4.122, trans_loss=5.304, nll_loss=2.595, w2v_ctc_loss=1.282, task_loss=0, contrastive_loss=0.122, total=4094.39, n_correct=2648.48, ppl=6.04, accuracy=64.686, wps=8032.1, ups=1.96, wpb=4094.4, bsz=146.4, num_updates=35300, lr=7.5271e-05, gnorm=0.992, clip=0, loss_scale=16, train_wall=51, gb_free=16.7, wall=20240
2023-08-02 15:22:21 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:1')
2023-08-02 15:22:44 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 4.321 | trans_loss 5.534 | nll_loss 2.803 | w2v_ctc_loss 1.372 | task_loss 0 | contrastive_loss 0.261 | total 4003.4 | n_correct 2495.4 | ppl 6.98 | accuracy 62.332 | uer 16.805 | wer 18.709 | raw_wer 18.709 | bleu 20.52 | wps 2191.6 | wpb 4003.4 | bsz 141.8 | num_updates 35360 | best_bleu 20.52
2023-08-02 15:22:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 35360 updates
2023-08-02 15:22:44 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 15:22:57 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 15:23:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt (epoch 24 @ 35360 updates, score 20.52) (writing took 23.96882607974112 seconds)
2023-08-02 15:23:09 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-08-02 15:23:09 | INFO | train | epoch 024 | loss 4.109 | trans_loss 5.286 | nll_loss 2.571 | w2v_ctc_loss 1.26 | task_loss 0 | contrastive_loss 0.213 | total 4137.91 | n_correct 2689.63 | ppl 5.94 | accuracy 65 | wps 7017.9 | ups 1.7 | wpb 4137.9 | bsz 152.7 | num_updates 35360 | lr 7.52071e-05 | gnorm 0.986 | clip 0 | loss_scale 16 | train_wall 749 | gb_free 16.6 | wall 20318
2023-08-02 15:23:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 15:23:09 | INFO | fairseq.trainer | begin training epoch 25
2023-08-02 15:23:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 15:23:36 | INFO | train_inner | epoch 025:     40 / 1474 loss=4.086, trans_loss=5.268, nll_loss=2.548, w2v_ctc_loss=1.244, task_loss=0, contrastive_loss=0.135, total=4165.57, n_correct=2719.7, ppl=5.85, accuracy=65.29, wps=3924.7, ups=0.94, wpb=4165.6, bsz=155.6, num_updates=35400, lr=7.51646e-05, gnorm=0.984, clip=0, loss_scale=16, train_wall=51, gb_free=13.7, wall=20346
2023-08-02 15:24:27 | INFO | train_inner | epoch 025:    140 / 1474 loss=4.068, trans_loss=5.249, nll_loss=2.522, w2v_ctc_loss=1.23, task_loss=0, contrastive_loss=0.131, total=4135.43, n_correct=2715.97, ppl=5.74, accuracy=65.676, wps=8092.4, ups=1.96, wpb=4135.4, bsz=154.5, num_updates=35500, lr=7.50587e-05, gnorm=0.969, clip=0, loss_scale=16, train_wall=51, gb_free=18.1, wall=20397
2023-08-02 15:25:19 | INFO | train_inner | epoch 025:    240 / 1474 loss=4.08, trans_loss=5.258, nll_loss=2.534, w2v_ctc_loss=1.245, task_loss=0, contrastive_loss=0.139, total=4116.13, n_correct=2691.8, ppl=5.79, accuracy=65.396, wps=8044.4, ups=1.95, wpb=4116.1, bsz=151.5, num_updates=35600, lr=7.49532e-05, gnorm=0.974, clip=0, loss_scale=16, train_wall=51, gb_free=17.7, wall=20448
2023-08-02 15:26:10 | INFO | train_inner | epoch 025:    340 / 1474 loss=4.09, trans_loss=5.267, nll_loss=2.544, w2v_ctc_loss=1.245, task_loss=0, contrastive_loss=0.2, total=4141.49, n_correct=2701.32, ppl=5.83, accuracy=65.226, wps=8070.2, ups=1.95, wpb=4141.5, bsz=147.1, num_updates=35700, lr=7.48481e-05, gnorm=0.991, clip=0, loss_scale=16, train_wall=51, gb_free=15.7, wall=20500
2023-08-02 15:27:01 | INFO | train_inner | epoch 025:    440 / 1474 loss=4.121, trans_loss=5.28, nll_loss=2.562, w2v_ctc_loss=1.282, task_loss=0, contrastive_loss=0.351, total=4167.4, n_correct=2713.59, ppl=5.91, accuracy=65.115, wps=8147.2, ups=1.95, wpb=4167.4, bsz=148.8, num_updates=35800, lr=7.47435e-05, gnorm=1.001, clip=0, loss_scale=16, train_wall=51, gb_free=16.3, wall=20551
2023-08-02 15:27:52 | INFO | train_inner | epoch 025:    540 / 1474 loss=4.098, trans_loss=5.278, nll_loss=2.561, w2v_ctc_loss=1.257, task_loss=0, contrastive_loss=0.141, total=4160.61, n_correct=2713.52, ppl=5.9, accuracy=65.219, wps=8157.1, ups=1.96, wpb=4160.6, bsz=157, num_updates=35900, lr=7.46393e-05, gnorm=0.99, clip=0, loss_scale=16, train_wall=51, gb_free=17.9, wall=20602
2023-08-02 15:28:43 | INFO | train_inner | epoch 025:    640 / 1474 loss=4.1, trans_loss=5.27, nll_loss=2.551, w2v_ctc_loss=1.257, task_loss=0, contrastive_loss=0.277, total=4153.68, n_correct=2711.23, ppl=5.86, accuracy=65.273, wps=8171.2, ups=1.97, wpb=4153.7, bsz=154.8, num_updates=36000, lr=7.45356e-05, gnorm=0.992, clip=0, loss_scale=16, train_wall=50, gb_free=16.8, wall=20653
2023-08-02 15:28:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 15:29:05 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 4.33 | trans_loss 5.536 | nll_loss 2.805 | w2v_ctc_loss 1.398 | task_loss 0 | contrastive_loss 0.256 | total 4003.4 | n_correct 2501.1 | ppl 6.99 | accuracy 62.474 | uer 16.911 | wer 18.769 | raw_wer 18.769 | bleu 20.25 | wps 2376.9 | wpb 4003.4 | bsz 141.8 | num_updates 36000 | best_bleu 20.52
2023-08-02 15:29:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36000 updates
2023-08-02 15:29:05 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_25_36000.pt
2023-08-02 15:29:09 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_25_36000.pt
2023-08-02 15:29:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_25_36000.pt (epoch 25 @ 36000 updates, score 20.25) (writing took 20.229471910744905 seconds)
2023-08-02 15:30:17 | INFO | train_inner | epoch 025:    740 / 1474 loss=4.101, trans_loss=5.272, nll_loss=2.553, w2v_ctc_loss=1.25, task_loss=0, contrastive_loss=0.264, total=4128.34, n_correct=2690.9, ppl=5.87, accuracy=65.181, wps=4402.9, ups=1.07, wpb=4128.3, bsz=150.6, num_updates=36100, lr=7.44323e-05, gnorm=0.989, clip=0, loss_scale=16, train_wall=51, gb_free=17.4, wall=20746
2023-08-02 15:31:08 | INFO | train_inner | epoch 025:    840 / 1474 loss=4.094, trans_loss=5.273, nll_loss=2.554, w2v_ctc_loss=1.247, task_loss=0, contrastive_loss=0.16, total=4182.4, n_correct=2729.06, ppl=5.87, accuracy=65.251, wps=8209.4, ups=1.96, wpb=4182.4, bsz=163, num_updates=36200, lr=7.43294e-05, gnorm=0.987, clip=0, loss_scale=16, train_wall=51, gb_free=18.1, wall=20797
2023-08-02 15:31:59 | INFO | train_inner | epoch 025:    940 / 1474 loss=4.101, trans_loss=5.272, nll_loss=2.555, w2v_ctc_loss=1.247, task_loss=0, contrastive_loss=0.271, total=4155.21, n_correct=2709.91, ppl=5.87, accuracy=65.217, wps=8148.6, ups=1.96, wpb=4155.2, bsz=158.5, num_updates=36300, lr=7.4227e-05, gnorm=0.991, clip=0, loss_scale=16, train_wall=51, gb_free=14.7, wall=20848
2023-08-02 15:32:50 | INFO | train_inner | epoch 025:   1040 / 1474 loss=4.12, trans_loss=5.287, nll_loss=2.572, w2v_ctc_loss=1.238, task_loss=0, contrastive_loss=0.493, total=4177.7, n_correct=2713.61, ppl=5.95, accuracy=64.955, wps=8098.5, ups=1.94, wpb=4177.7, bsz=154.9, num_updates=36400, lr=7.41249e-05, gnorm=0.977, clip=0, loss_scale=16, train_wall=51, gb_free=16.1, wall=20900
2023-08-02 15:33:41 | INFO | train_inner | epoch 025:   1140 / 1474 loss=4.088, trans_loss=5.276, nll_loss=2.558, w2v_ctc_loss=1.241, task_loss=0, contrastive_loss=0.109, total=4039.24, n_correct=2633.65, ppl=5.89, accuracy=65.202, wps=8007.9, ups=1.98, wpb=4039.2, bsz=142.6, num_updates=36500, lr=7.40233e-05, gnorm=1.012, clip=0, loss_scale=16, train_wall=50, gb_free=16.8, wall=20950
2023-08-02 15:34:31 | INFO | train_inner | epoch 025:   1240 / 1474 loss=4.098, trans_loss=5.283, nll_loss=2.566, w2v_ctc_loss=1.25, task_loss=0, contrastive_loss=0.129, total=4090.59, n_correct=2659.58, ppl=5.92, accuracy=65.017, wps=8109, ups=1.98, wpb=4090.6, bsz=147.8, num_updates=36600, lr=7.39221e-05, gnorm=0.988, clip=0, loss_scale=16, train_wall=50, gb_free=17.7, wall=21001
2023-08-02 15:35:22 | INFO | train_inner | epoch 025:   1340 / 1474 loss=4.105, trans_loss=5.276, nll_loss=2.559, w2v_ctc_loss=1.248, task_loss=0, contrastive_loss=0.311, total=4164.34, n_correct=2714.66, ppl=5.89, accuracy=65.188, wps=8212.8, ups=1.97, wpb=4164.3, bsz=155.1, num_updates=36700, lr=7.38213e-05, gnorm=0.976, clip=0, loss_scale=16, train_wall=50, gb_free=17.3, wall=21051
2023-08-02 15:36:13 | INFO | train_inner | epoch 025:   1440 / 1474 loss=4.117, trans_loss=5.298, nll_loss=2.586, w2v_ctc_loss=1.258, task_loss=0, contrastive_loss=0.212, total=4099.11, n_correct=2657.75, ppl=6, accuracy=64.837, wps=7978.7, ups=1.95, wpb=4099.1, bsz=149.7, num_updates=36800, lr=7.3721e-05, gnorm=1.001, clip=0, loss_scale=32, train_wall=51, gb_free=12.9, wall=21103
2023-08-02 15:36:31 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 15:36:53 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 4.312 | trans_loss 5.537 | nll_loss 2.809 | w2v_ctc_loss 1.332 | task_loss 0 | contrastive_loss 0.26 | total 4003.4 | n_correct 2496 | ppl 7.01 | accuracy 62.347 | uer 16.951 | wer 19.037 | raw_wer 19.037 | bleu 20.17 | wps 2303.1 | wpb 4003.4 | bsz 141.8 | num_updates 36834 | best_bleu 20.52
2023-08-02 15:36:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36834 updates
2023-08-02 15:36:53 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_last.pt
2023-08-02 15:37:06 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_last.pt
2023-08-02 15:37:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_last.pt (epoch 25 @ 36834 updates, score 20.17) (writing took 12.894760992377996 seconds)
2023-08-02 15:37:06 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-08-02 15:37:06 | INFO | train | epoch 025 | loss 4.098 | trans_loss 5.274 | nll_loss 2.555 | w2v_ctc_loss 1.249 | task_loss 0 | contrastive_loss 0.225 | total 4138.65 | n_correct 2698.51 | ppl 5.88 | accuracy 65.203 | wps 7281.6 | ups 1.76 | wpb 4138.6 | bsz 152.8 | num_updates 36834 | lr 7.36869e-05 | gnorm 0.988 | clip 0 | loss_scale 32 | train_wall 746 | gb_free 14.8 | wall 21156
2023-08-02 15:37:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 15:37:07 | INFO | fairseq.trainer | begin training epoch 26
2023-08-02 15:37:07 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 15:37:47 | INFO | train_inner | epoch 026:     66 / 1474 loss=4.077, trans_loss=5.253, nll_loss=2.529, w2v_ctc_loss=1.232, task_loss=0, contrastive_loss=0.181, total=4180.21, n_correct=2739.2, ppl=5.77, accuracy=65.528, wps=4432.6, ups=1.06, wpb=4180.2, bsz=159.2, num_updates=36900, lr=7.3621e-05, gnorm=1, clip=0, loss_scale=32, train_wall=51, gb_free=17.4, wall=21197
2023-08-02 15:38:39 | INFO | train_inner | epoch 026:    166 / 1474 loss=4.091, trans_loss=5.248, nll_loss=2.522, w2v_ctc_loss=1.206, task_loss=0, contrastive_loss=0.557, total=4270.78, n_correct=2806.1, ppl=5.74, accuracy=65.705, wps=8349.8, ups=1.96, wpb=4270.8, bsz=170.2, num_updates=37000, lr=7.35215e-05, gnorm=0.984, clip=0, loss_scale=32, train_wall=51, gb_free=15.7, wall=21248
2023-08-02 15:39:29 | INFO | train_inner | epoch 026:    266 / 1474 loss=4.085, trans_loss=5.251, nll_loss=2.526, w2v_ctc_loss=1.24, task_loss=0, contrastive_loss=0.306, total=4125.04, n_correct=2708.49, ppl=5.76, accuracy=65.66, wps=8122.9, ups=1.97, wpb=4125, bsz=153.6, num_updates=37100, lr=7.34223e-05, gnorm=1.004, clip=0, loss_scale=32, train_wall=50, gb_free=15.7, wall=21299
2023-08-02 15:40:20 | INFO | train_inner | epoch 026:    366 / 1474 loss=4.075, trans_loss=5.248, nll_loss=2.522, w2v_ctc_loss=1.229, task_loss=0, contrastive_loss=0.218, total=4165.74, n_correct=2733.3, ppl=5.74, accuracy=65.614, wps=8197.7, ups=1.97, wpb=4165.7, bsz=157.3, num_updates=37200, lr=7.33236e-05, gnorm=0.985, clip=0, loss_scale=32, train_wall=50, gb_free=17.2, wall=21350
2023-08-02 15:41:11 | INFO | train_inner | epoch 026:    466 / 1474 loss=4.078, trans_loss=5.244, nll_loss=2.516, w2v_ctc_loss=1.233, task_loss=0, contrastive_loss=0.314, total=4170.23, n_correct=2742.13, ppl=5.72, accuracy=65.755, wps=8252.5, ups=1.98, wpb=4170.2, bsz=157.7, num_updates=37300, lr=7.32252e-05, gnorm=0.989, clip=0, loss_scale=32, train_wall=50, gb_free=18.1, wall=21400
2023-08-02 15:42:02 | INFO | train_inner | epoch 026:    566 / 1474 loss=4.095, trans_loss=5.267, nll_loss=2.546, w2v_ctc_loss=1.269, task_loss=0, contrastive_loss=0.151, total=4155.02, n_correct=2713.03, ppl=5.84, accuracy=65.295, wps=8046, ups=1.94, wpb=4155, bsz=151.9, num_updates=37400, lr=7.31272e-05, gnorm=1.017, clip=0, loss_scale=32, train_wall=51, gb_free=18.2, wall=21452
2023-08-02 15:42:54 | INFO | train_inner | epoch 026:    666 / 1474 loss=4.072, trans_loss=5.257, nll_loss=2.534, w2v_ctc_loss=1.225, task_loss=0, contrastive_loss=0.123, total=4136.96, n_correct=2708.7, ppl=5.79, accuracy=65.476, wps=7986.6, ups=1.93, wpb=4137, bsz=149.6, num_updates=37500, lr=7.30297e-05, gnorm=0.993, clip=0, loss_scale=32, train_wall=51, gb_free=15.8, wall=21504
2023-08-02 15:43:45 | INFO | train_inner | epoch 026:    766 / 1474 loss=4.1, trans_loss=5.266, nll_loss=2.545, w2v_ctc_loss=1.243, task_loss=0, contrastive_loss=0.347, total=4086.28, n_correct=2668.14, ppl=5.83, accuracy=65.295, wps=8065.3, ups=1.97, wpb=4086.3, bsz=149.2, num_updates=37600, lr=7.29325e-05, gnorm=0.999, clip=0, loss_scale=32, train_wall=50, gb_free=15.5, wall=21555
2023-08-02 15:44:36 | INFO | train_inner | epoch 026:    866 / 1474 loss=4.089, trans_loss=5.265, nll_loss=2.544, w2v_ctc_loss=1.253, task_loss=0, contrastive_loss=0.152, total=4183.26, n_correct=2729.81, ppl=5.83, accuracy=65.256, wps=8212.9, ups=1.96, wpb=4183.3, bsz=154.1, num_updates=37700, lr=7.28357e-05, gnorm=0.988, clip=0, loss_scale=32, train_wall=51, gb_free=17.7, wall=21605
2023-08-02 15:45:27 | INFO | train_inner | epoch 026:    966 / 1474 loss=4.091, trans_loss=5.271, nll_loss=2.551, w2v_ctc_loss=1.227, task_loss=0, contrastive_loss=0.26, total=4137.96, n_correct=2695.46, ppl=5.86, accuracy=65.14, wps=8086.2, ups=1.95, wpb=4138, bsz=149.5, num_updates=37800, lr=7.27393e-05, gnorm=1, clip=0, loss_scale=32, train_wall=51, gb_free=17.3, wall=21657
2023-08-02 15:46:18 | INFO | train_inner | epoch 026:   1066 / 1474 loss=4.081, trans_loss=5.263, nll_loss=2.541, w2v_ctc_loss=1.242, task_loss=0, contrastive_loss=0.123, total=4120.53, n_correct=2692.8, ppl=5.82, accuracy=65.351, wps=8034.5, ups=1.95, wpb=4120.5, bsz=147.1, num_updates=37900, lr=7.26433e-05, gnorm=0.987, clip=0, loss_scale=32, train_wall=51, gb_free=17.1, wall=21708
2023-08-02 15:47:10 | INFO | train_inner | epoch 026:   1166 / 1474 loss=4.103, trans_loss=5.278, nll_loss=2.56, w2v_ctc_loss=1.259, task_loss=0, contrastive_loss=0.203, total=4113.86, n_correct=2679.49, ppl=5.9, accuracy=65.133, wps=8029.3, ups=1.95, wpb=4113.9, bsz=149.2, num_updates=38000, lr=7.25476e-05, gnorm=0.997, clip=0, loss_scale=32, train_wall=51, gb_free=17, wall=21759
2023-08-02 15:47:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 15:47:33 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 4.341 | trans_loss 5.536 | nll_loss 2.808 | w2v_ctc_loss 1.433 | task_loss 0 | contrastive_loss 0.257 | total 4003.4 | n_correct 2489.8 | ppl 7 | accuracy 62.192 | uer 17.017 | wer 18.817 | raw_wer 18.817 | bleu 19.97 | wps 2144.8 | wpb 4003.4 | bsz 141.8 | num_updates 38000 | best_bleu 20.52
2023-08-02 15:47:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38000 updates
2023-08-02 15:47:33 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_26_38000.pt
2023-08-02 15:47:37 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_26_38000.pt
2023-08-02 15:48:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_26_38000.pt (epoch 26 @ 38000 updates, score 19.97) (writing took 30.165543399751186 seconds)
2023-08-02 15:48:55 | INFO | train_inner | epoch 026:   1266 / 1474 loss=4.11, trans_loss=5.291, nll_loss=2.578, w2v_ctc_loss=1.273, task_loss=0, contrastive_loss=0.127, total=3996.19, n_correct=2591.32, ppl=5.97, accuracy=64.845, wps=3795.9, ups=0.95, wpb=3996.2, bsz=139.6, num_updates=38100, lr=7.24524e-05, gnorm=1.007, clip=0, loss_scale=32, train_wall=51, gb_free=18.1, wall=21864
2023-08-02 15:49:46 | INFO | train_inner | epoch 026:   1366 / 1474 loss=4.089, trans_loss=5.271, nll_loss=2.552, w2v_ctc_loss=1.235, task_loss=0, contrastive_loss=0.156, total=4159.74, n_correct=2718.06, ppl=5.87, accuracy=65.342, wps=8071.1, ups=1.94, wpb=4159.7, bsz=155.7, num_updates=38200, lr=7.23575e-05, gnorm=0.997, clip=0, loss_scale=32, train_wall=51, gb_free=17.6, wall=21916
2023-08-02 15:50:37 | INFO | train_inner | epoch 026:   1466 / 1474 loss=4.077, trans_loss=5.261, nll_loss=2.54, w2v_ctc_loss=1.222, task_loss=0, contrastive_loss=0.143, total=4165.66, n_correct=2726.75, ppl=5.82, accuracy=65.458, wps=8211.2, ups=1.97, wpb=4165.7, bsz=158.7, num_updates=38300, lr=7.22629e-05, gnorm=0.98, clip=0, loss_scale=32, train_wall=50, gb_free=16.9, wall=21967
2023-08-02 15:50:41 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 15:51:03 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 4.295 | trans_loss 5.531 | nll_loss 2.802 | w2v_ctc_loss 1.291 | task_loss 0 | contrastive_loss 0.256 | total 4003.4 | n_correct 2494.7 | ppl 6.97 | accuracy 62.315 | uer 16.622 | wer 18.568 | raw_wer 18.568 | bleu 19.97 | wps 2366.8 | wpb 4003.4 | bsz 141.8 | num_updates 38308 | best_bleu 20.52
2023-08-02 15:51:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38308 updates
2023-08-02 15:51:03 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_last.pt
2023-08-02 15:51:16 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_last.pt
2023-08-02 15:51:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_last.pt (epoch 26 @ 38308 updates, score 19.97) (writing took 12.458412412554026 seconds)
2023-08-02 15:51:16 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-08-02 15:51:16 | INFO | train | epoch 026 | loss 4.087 | trans_loss 5.262 | nll_loss 2.54 | w2v_ctc_loss 1.239 | task_loss 0 | contrastive_loss 0.227 | total 4138.65 | n_correct 2706.72 | ppl 5.81 | accuracy 65.401 | wps 7182.7 | ups 1.74 | wpb 4138.6 | bsz 152.8 | num_updates 38308 | lr 7.22554e-05 | gnorm 0.996 | clip 0 | loss_scale 32 | train_wall 746 | gb_free 16.5 | wall 22005
2023-08-02 15:51:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 15:51:16 | INFO | fairseq.trainer | begin training epoch 27
2023-08-02 15:51:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 15:52:10 | INFO | train_inner | epoch 027:     92 / 1474 loss=4.045, trans_loss=5.226, nll_loss=2.492, w2v_ctc_loss=1.216, task_loss=0, contrastive_loss=0.104, total=4054.57, n_correct=2677.47, ppl=5.62, accuracy=66.036, wps=4363, ups=1.08, wpb=4054.6, bsz=141.2, num_updates=38400, lr=7.21688e-05, gnorm=1, clip=0, loss_scale=32, train_wall=50, gb_free=16.8, wall=22060
2023-08-02 15:53:01 | INFO | train_inner | epoch 027:    192 / 1474 loss=4.048, trans_loss=5.222, nll_loss=2.488, w2v_ctc_loss=1.215, task_loss=0, contrastive_loss=0.159, total=4195.2, n_correct=2772.21, ppl=5.61, accuracy=66.081, wps=8195.2, ups=1.95, wpb=4195.2, bsz=161.6, num_updates=38500, lr=7.2075e-05, gnorm=0.993, clip=0, loss_scale=32, train_wall=51, gb_free=17.5, wall=22111
2023-08-02 15:53:52 | INFO | train_inner | epoch 027:    292 / 1474 loss=4.058, trans_loss=5.237, nll_loss=2.507, w2v_ctc_loss=1.225, task_loss=0, contrastive_loss=0.125, total=4162.23, n_correct=2740.26, ppl=5.69, accuracy=65.836, wps=8143.2, ups=1.96, wpb=4162.2, bsz=152.7, num_updates=38600, lr=7.19816e-05, gnorm=0.989, clip=0, loss_scale=32, train_wall=51, gb_free=17.6, wall=22162
2023-08-02 15:53:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-02 15:54:45 | INFO | train_inner | epoch 027:    393 / 1474 loss=4.07, trans_loss=5.246, nll_loss=2.518, w2v_ctc_loss=1.222, task_loss=0, contrastive_loss=0.263, total=4052.96, n_correct=2662.45, ppl=5.73, accuracy=65.691, wps=7742.4, ups=1.91, wpb=4053, bsz=144.2, num_updates=38700, lr=7.18885e-05, gnorm=0.992, clip=0, loss_scale=16, train_wall=52, gb_free=18, wall=22214
2023-08-02 15:55:36 | INFO | train_inner | epoch 027:    493 / 1474 loss=4.086, trans_loss=5.251, nll_loss=2.526, w2v_ctc_loss=1.224, task_loss=0, contrastive_loss=0.358, total=4249.35, n_correct=2785.04, ppl=5.76, accuracy=65.54, wps=8309.6, ups=1.96, wpb=4249.4, bsz=166, num_updates=38800, lr=7.17958e-05, gnorm=0.979, clip=0, loss_scale=16, train_wall=51, gb_free=12.7, wall=22265
2023-08-02 15:56:27 | INFO | train_inner | epoch 027:    593 / 1474 loss=4.083, trans_loss=5.253, nll_loss=2.529, w2v_ctc_loss=1.238, task_loss=0, contrastive_loss=0.243, total=4133.39, n_correct=2709.7, ppl=5.77, accuracy=65.556, wps=8092.4, ups=1.96, wpb=4133.4, bsz=156, num_updates=38900, lr=7.17035e-05, gnorm=1.005, clip=0, loss_scale=16, train_wall=51, gb_free=12.8, wall=22317
2023-08-02 15:57:18 | INFO | train_inner | epoch 027:    693 / 1474 loss=4.081, trans_loss=5.254, nll_loss=2.53, w2v_ctc_loss=1.24, task_loss=0, contrastive_loss=0.198, total=4162.71, n_correct=2726.26, ppl=5.77, accuracy=65.492, wps=8086.6, ups=1.94, wpb=4162.7, bsz=152.7, num_updates=39000, lr=7.16115e-05, gnorm=0.984, clip=0, loss_scale=16, train_wall=51, gb_free=16.7, wall=22368
2023-08-02 15:58:09 | INFO | train_inner | epoch 027:    793 / 1474 loss=4.074, trans_loss=5.252, nll_loss=2.527, w2v_ctc_loss=1.241, task_loss=0, contrastive_loss=0.127, total=4103.81, n_correct=2691.58, ppl=5.76, accuracy=65.587, wps=8116.3, ups=1.98, wpb=4103.8, bsz=147.1, num_updates=39100, lr=7.15199e-05, gnorm=1.002, clip=0, loss_scale=16, train_wall=50, gb_free=16.9, wall=22419
2023-08-02 15:59:00 | INFO | train_inner | epoch 027:    893 / 1474 loss=4.069, trans_loss=5.256, nll_loss=2.532, w2v_ctc_loss=1.22, task_loss=0, contrastive_loss=0.112, total=4101.56, n_correct=2687.05, ppl=5.78, accuracy=65.513, wps=8086.8, ups=1.97, wpb=4101.6, bsz=146.1, num_updates=39200, lr=7.14286e-05, gnorm=1.033, clip=0, loss_scale=16, train_wall=50, gb_free=18.1, wall=22469
2023-08-02 15:59:51 | INFO | train_inner | epoch 027:    993 / 1474 loss=4.096, trans_loss=5.257, nll_loss=2.535, w2v_ctc_loss=1.226, task_loss=0, contrastive_loss=0.482, total=4199.56, n_correct=2748.34, ppl=5.8, accuracy=65.444, wps=8178.1, ups=1.95, wpb=4199.6, bsz=158.4, num_updates=39300, lr=7.13376e-05, gnorm=1.004, clip=0, loss_scale=16, train_wall=51, gb_free=12.3, wall=22521
2023-08-02 16:00:42 | INFO | train_inner | epoch 027:   1093 / 1474 loss=4.071, trans_loss=5.253, nll_loss=2.528, w2v_ctc_loss=1.225, task_loss=0, contrastive_loss=0.148, total=4150.97, n_correct=2717.04, ppl=5.77, accuracy=65.456, wps=8171.1, ups=1.97, wpb=4151, bsz=152.5, num_updates=39400, lr=7.1247e-05, gnorm=1.024, clip=0, loss_scale=16, train_wall=50, gb_free=12.7, wall=22571
2023-08-02 16:01:33 | INFO | train_inner | epoch 027:   1193 / 1474 loss=4.083, trans_loss=5.261, nll_loss=2.539, w2v_ctc_loss=1.243, task_loss=0, contrastive_loss=0.157, total=4103.06, n_correct=2683.39, ppl=5.81, accuracy=65.4, wps=8053.4, ups=1.96, wpb=4103.1, bsz=148.8, num_updates=39500, lr=7.11568e-05, gnorm=1.021, clip=0, loss_scale=16, train_wall=50, gb_free=17, wall=22622
2023-08-02 16:02:24 | INFO | train_inner | epoch 027:   1293 / 1474 loss=4.093, trans_loss=5.269, nll_loss=2.549, w2v_ctc_loss=1.236, task_loss=0, contrastive_loss=0.261, total=4062.52, n_correct=2652.6, ppl=5.85, accuracy=65.294, wps=7888.1, ups=1.94, wpb=4062.5, bsz=146.1, num_updates=39600, lr=7.10669e-05, gnorm=1.017, clip=0, loss_scale=16, train_wall=51, gb_free=17, wall=22674
2023-08-02 16:03:15 | INFO | train_inner | epoch 027:   1393 / 1474 loss=4.095, trans_loss=5.266, nll_loss=2.547, w2v_ctc_loss=1.249, task_loss=0, contrastive_loss=0.226, total=4152, n_correct=2710.94, ppl=5.84, accuracy=65.292, wps=8244.8, ups=1.99, wpb=4152, bsz=156.2, num_updates=39700, lr=7.09773e-05, gnorm=1.004, clip=0, loss_scale=16, train_wall=50, gb_free=17, wall=22724
2023-08-02 16:03:56 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 16:04:18 | INFO | dev_st | epoch 027 | valid on 'dev_st' subset | loss 4.314 | trans_loss 5.53 | nll_loss 2.801 | w2v_ctc_loss 1.353 | task_loss 0 | contrastive_loss 0.265 | total 4003.4 | n_correct 2502.3 | ppl 6.97 | accuracy 62.504 | uer 16.739 | wer 18.687 | raw_wer 18.687 | bleu 20.35 | wps 2081.4 | wpb 4003.4 | bsz 141.8 | num_updates 39781 | best_bleu 20.52
2023-08-02 16:04:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 39781 updates
2023-08-02 16:04:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.3505.pt
2023-08-02 16:04:22 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.3505.pt
2023-08-02 16:04:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.3505.pt (epoch 27 @ 39781 updates, score 20.35) (writing took 35.9198482632637 seconds)
2023-08-02 16:04:56 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-08-02 16:04:56 | INFO | train | epoch 027 | loss 4.075 | trans_loss 5.25 | nll_loss 2.525 | w2v_ctc_loss 1.23 | task_loss 0 | contrastive_loss 0.21 | total 4136.97 | n_correct 2713.27 | ppl 5.75 | accuracy 65.586 | wps 7427.6 | ups 1.8 | wpb 4137 | bsz 152.6 | num_updates 39781 | lr 7.0905e-05 | gnorm 1.002 | clip 0 | loss_scale 16 | train_wall 746 | gb_free 18.2 | wall 22826
2023-08-02 16:04:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 16:04:56 | INFO | fairseq.trainer | begin training epoch 28
2023-08-02 16:04:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 16:05:15 | INFO | train_inner | epoch 028:     19 / 1474 loss=4.064, trans_loss=5.246, nll_loss=2.52, w2v_ctc_loss=1.22, task_loss=0, contrastive_loss=0.132, total=4108.43, n_correct=2696.89, ppl=5.74, accuracy=65.643, wps=3416.3, ups=0.83, wpb=4108.4, bsz=152.6, num_updates=39800, lr=7.08881e-05, gnorm=0.991, clip=0, loss_scale=16, train_wall=51, gb_free=16.7, wall=22845
2023-08-02 16:06:05 | INFO | train_inner | epoch 028:    119 / 1474 loss=4.037, trans_loss=5.217, nll_loss=2.481, w2v_ctc_loss=1.205, task_loss=0, contrastive_loss=0.12, total=4113.41, n_correct=2720.03, ppl=5.58, accuracy=66.126, wps=8137.9, ups=1.98, wpb=4113.4, bsz=147, num_updates=39900, lr=7.07992e-05, gnorm=0.996, clip=0, loss_scale=16, train_wall=50, gb_free=17.4, wall=22895
2023-08-02 16:06:56 | INFO | train_inner | epoch 028:    219 / 1474 loss=4.044, trans_loss=5.222, nll_loss=2.489, w2v_ctc_loss=1.208, task_loss=0, contrastive_loss=0.137, total=4191.56, n_correct=2771.79, ppl=5.61, accuracy=66.128, wps=8254, ups=1.97, wpb=4191.6, bsz=157.6, num_updates=40000, lr=7.07107e-05, gnorm=0.993, clip=0, loss_scale=16, train_wall=50, gb_free=15.6, wall=22946
2023-08-02 16:06:56 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 16:07:18 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 4.324 | trans_loss 5.536 | nll_loss 2.806 | w2v_ctc_loss 1.369 | task_loss 0 | contrastive_loss 0.27 | total 4003.4 | n_correct 2493.3 | ppl 7 | accuracy 62.28 | uer 16.749 | wer 18.664 | raw_wer 18.664 | bleu 20.4 | wps 2396.8 | wpb 4003.4 | bsz 141.8 | num_updates 40000 | best_bleu 20.52
2023-08-02 16:07:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 40000 updates
2023-08-02 16:07:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_28_40000.pt
2023-08-02 16:07:22 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_28_40000.pt
2023-08-02 16:07:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_28_40000.pt (epoch 28 @ 40000 updates, score 20.4) (writing took 38.222239116206765 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:0')
2023-08-02 16:08:52 | INFO | train_inner | epoch 028:    319 / 1474 loss=4.103, trans_loss=5.246, nll_loss=2.519, w2v_ctc_loss=1.205, task_loss=0, contrastive_loss=0.796, total=4145.32, n_correct=2720.94, ppl=5.73, accuracy=65.639, wps=3565.8, ups=0.86, wpb=4145.3, bsz=158.1, num_updates=40100, lr=7.06225e-05, gnorm=1.002, clip=0, loss_scale=16, train_wall=51, gb_free=16.2, wall=23062
2023-08-02 16:09:44 | INFO | train_inner | epoch 028:    419 / 1474 loss=4.054, trans_loss=5.233, nll_loss=2.501, w2v_ctc_loss=1.224, task_loss=0, contrastive_loss=0.112, total=4092.14, n_correct=2694.05, ppl=5.66, accuracy=65.835, wps=7957.6, ups=1.94, wpb=4092.1, bsz=147.8, num_updates=40200, lr=7.05346e-05, gnorm=0.999, clip=0, loss_scale=16, train_wall=51, gb_free=16.8, wall=23114
2023-08-02 16:10:35 | INFO | train_inner | epoch 028:    519 / 1474 loss=4.057, trans_loss=5.235, nll_loss=2.505, w2v_ctc_loss=1.223, task_loss=0, contrastive_loss=0.134, total=4096.35, n_correct=2697.64, ppl=5.68, accuracy=65.855, wps=8047.7, ups=1.96, wpb=4096.4, bsz=147.8, num_updates=40300, lr=7.0447e-05, gnorm=1.008, clip=0, loss_scale=16, train_wall=50, gb_free=16.4, wall=23164
2023-08-02 16:11:26 | INFO | train_inner | epoch 028:    619 / 1474 loss=4.062, trans_loss=5.243, nll_loss=2.515, w2v_ctc_loss=1.22, task_loss=0, contrastive_loss=0.134, total=4178.12, n_correct=2748.08, ppl=5.71, accuracy=65.773, wps=8174.9, ups=1.96, wpb=4178.1, bsz=152.7, num_updates=40400, lr=7.03598e-05, gnorm=0.998, clip=0, loss_scale=16, train_wall=51, gb_free=16.4, wall=23216
2023-08-02 16:12:17 | INFO | train_inner | epoch 028:    719 / 1474 loss=4.082, trans_loss=5.245, nll_loss=2.521, w2v_ctc_loss=1.225, task_loss=0, contrastive_loss=0.357, total=4185.82, n_correct=2752.95, ppl=5.74, accuracy=65.768, wps=8235.8, ups=1.97, wpb=4185.8, bsz=163.2, num_updates=40500, lr=7.02728e-05, gnorm=1.005, clip=0, loss_scale=16, train_wall=50, gb_free=16.3, wall=23266
2023-08-02 16:13:08 | INFO | train_inner | epoch 028:    819 / 1474 loss=4.053, trans_loss=5.234, nll_loss=2.504, w2v_ctc_loss=1.213, task_loss=0, contrastive_loss=0.123, total=4096.2, n_correct=2697.71, ppl=5.67, accuracy=65.859, wps=8040.4, ups=1.96, wpb=4096.2, bsz=153.5, num_updates=40600, lr=7.01862e-05, gnorm=1.001, clip=0, loss_scale=16, train_wall=50, gb_free=16.2, wall=23317
2023-08-02 16:13:59 | INFO | train_inner | epoch 028:    919 / 1474 loss=4.071, trans_loss=5.247, nll_loss=2.52, w2v_ctc_loss=1.215, task_loss=0, contrastive_loss=0.243, total=4120.27, n_correct=2704.97, ppl=5.74, accuracy=65.65, wps=8030.8, ups=1.95, wpb=4120.3, bsz=150.4, num_updates=40700, lr=7.01e-05, gnorm=1.001, clip=0, loss_scale=32, train_wall=51, gb_free=17.8, wall=23369
2023-08-02 16:14:50 | INFO | train_inner | epoch 028:   1019 / 1474 loss=4.084, trans_loss=5.248, nll_loss=2.523, w2v_ctc_loss=1.23, task_loss=0, contrastive_loss=0.344, total=4177.86, n_correct=2740.6, ppl=5.75, accuracy=65.598, wps=8193.7, ups=1.96, wpb=4177.9, bsz=155.5, num_updates=40800, lr=7.0014e-05, gnorm=0.989, clip=0, loss_scale=32, train_wall=51, gb_free=16.9, wall=23420
2023-08-02 16:15:41 | INFO | train_inner | epoch 028:   1119 / 1474 loss=4.063, trans_loss=5.239, nll_loss=2.512, w2v_ctc_loss=1.222, task_loss=0, contrastive_loss=0.162, total=4210.86, n_correct=2766.12, ppl=5.71, accuracy=65.69, wps=8235.5, ups=1.96, wpb=4210.9, bsz=159.4, num_updates=40900, lr=6.99284e-05, gnorm=0.984, clip=0, loss_scale=32, train_wall=51, gb_free=17.9, wall=23471
2023-08-02 16:16:32 | INFO | train_inner | epoch 028:   1219 / 1474 loss=4.062, trans_loss=5.246, nll_loss=2.521, w2v_ctc_loss=1.211, task_loss=0, contrastive_loss=0.136, total=4104.61, n_correct=2693.9, ppl=5.74, accuracy=65.631, wps=8103.9, ups=1.97, wpb=4104.6, bsz=152.8, num_updates=41000, lr=6.9843e-05, gnorm=1.005, clip=0, loss_scale=32, train_wall=50, gb_free=16.9, wall=23521
2023-08-02 16:17:23 | INFO | train_inner | epoch 028:   1319 / 1474 loss=4.076, trans_loss=5.254, nll_loss=2.529, w2v_ctc_loss=1.238, task_loss=0, contrastive_loss=0.16, total=4087.78, n_correct=2676.55, ppl=5.77, accuracy=65.477, wps=8041.5, ups=1.97, wpb=4087.8, bsz=142.6, num_updates=41100, lr=6.9758e-05, gnorm=1.029, clip=0, loss_scale=32, train_wall=50, gb_free=15.5, wall=23572
2023-08-02 16:18:14 | INFO | train_inner | epoch 028:   1419 / 1474 loss=4.079, trans_loss=5.256, nll_loss=2.532, w2v_ctc_loss=1.229, task_loss=0, contrastive_loss=0.208, total=4145.03, n_correct=2710.77, ppl=5.78, accuracy=65.398, wps=8069, ups=1.95, wpb=4145, bsz=148.8, num_updates=41200, lr=6.96733e-05, gnorm=1.007, clip=0, loss_scale=32, train_wall=51, gb_free=18, wall=23624
2023-08-02 16:18:42 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:5')
2023-08-02 16:19:04 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 4.321 | trans_loss 5.532 | nll_loss 2.8 | w2v_ctc_loss 1.372 | task_loss 0 | contrastive_loss 0.265 | total 4003.4 | n_correct 2501.7 | ppl 6.96 | accuracy 62.489 | uer 16.595 | wer 18.441 | raw_wer 18.441 | bleu 20.76 | wps 2419.8 | wpb 4003.4 | bsz 141.8 | num_updates 41255 | best_bleu 20.76
2023-08-02 16:19:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 41255 updates
2023-08-02 16:19:04 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 16:19:16 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt
2023-08-02 16:19:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_best.pt (epoch 28 @ 41255 updates, score 20.76) (writing took 24.31939241103828 seconds)
2023-08-02 16:19:28 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-08-02 16:19:28 | INFO | train | epoch 028 | loss 4.065 | trans_loss 5.239 | nll_loss 2.511 | w2v_ctc_loss 1.219 | task_loss 0 | contrastive_loss 0.225 | total 4138.65 | n_correct 2721.61 | ppl 5.7 | accuracy 65.761 | wps 6993.1 | ups 1.69 | wpb 4138.6 | bsz 152.8 | num_updates 41255 | lr 6.96268e-05 | gnorm 1.001 | clip 0 | loss_scale 32 | train_wall 745 | gb_free 17 | wall 23698
2023-08-02 16:19:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 16:19:29 | INFO | fairseq.trainer | begin training epoch 29
2023-08-02 16:19:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 16:20:00 | INFO | train_inner | epoch 029:     45 / 1474 loss=4.043, trans_loss=5.218, nll_loss=2.485, w2v_ctc_loss=1.211, task_loss=0, contrastive_loss=0.142, total=4163.06, n_correct=2753.11, ppl=5.6, accuracy=66.132, wps=3926.4, ups=0.94, wpb=4163.1, bsz=157, num_updates=41300, lr=6.95889e-05, gnorm=0.995, clip=0, loss_scale=32, train_wall=50, gb_free=16.8, wall=23730
2023-08-02 16:20:51 | INFO | train_inner | epoch 029:    145 / 1474 loss=4.048, trans_loss=5.219, nll_loss=2.485, w2v_ctc_loss=1.209, task_loss=0, contrastive_loss=0.206, total=4116.29, n_correct=2723.13, ppl=5.6, accuracy=66.155, wps=8022.7, ups=1.95, wpb=4116.3, bsz=154.3, num_updates=41400, lr=6.95048e-05, gnorm=1.007, clip=0, loss_scale=32, train_wall=51, gb_free=14.6, wall=23781
2023-08-02 16:21:43 | INFO | train_inner | epoch 029:    245 / 1474 loss=4.053, trans_loss=5.214, nll_loss=2.479, w2v_ctc_loss=1.194, task_loss=0, contrastive_loss=0.363, total=4197.24, n_correct=2775.58, ppl=5.57, accuracy=66.129, wps=8183.9, ups=1.95, wpb=4197.2, bsz=165, num_updates=41500, lr=6.9421e-05, gnorm=0.999, clip=0, loss_scale=32, train_wall=51, gb_free=17.5, wall=23832
2023-08-02 16:22:34 | INFO | train_inner | epoch 029:    345 / 1474 loss=4.055, trans_loss=5.232, nll_loss=2.501, w2v_ctc_loss=1.228, task_loss=0, contrastive_loss=0.124, total=4092.21, n_correct=2700.13, ppl=5.66, accuracy=65.982, wps=7993.5, ups=1.95, wpb=4092.2, bsz=145.3, num_updates=41600, lr=6.93375e-05, gnorm=1.002, clip=0, loss_scale=32, train_wall=51, gb_free=16.1, wall=23884
2023-08-02 16:23:25 | INFO | train_inner | epoch 029:    445 / 1474 loss=4.028, trans_loss=5.208, nll_loss=2.469, w2v_ctc_loss=1.197, task_loss=0, contrastive_loss=0.112, total=4161.27, n_correct=2754.57, ppl=5.54, accuracy=66.195, wps=8097.6, ups=1.95, wpb=4161.3, bsz=153.9, num_updates=41700, lr=6.92543e-05, gnorm=0.997, clip=0, loss_scale=32, train_wall=51, gb_free=16.6, wall=23935
2023-08-02 16:24:16 | INFO | train_inner | epoch 029:    545 / 1474 loss=4.063, trans_loss=5.236, nll_loss=2.507, w2v_ctc_loss=1.201, task_loss=0, contrastive_loss=0.302, total=4159.68, n_correct=2738.75, ppl=5.68, accuracy=65.84, wps=8126.1, ups=1.95, wpb=4159.7, bsz=148.2, num_updates=41800, lr=6.91714e-05, gnorm=0.985, clip=0, loss_scale=32, train_wall=51, gb_free=17, wall=23986
2023-08-02 16:25:08 | INFO | train_inner | epoch 029:    645 / 1474 loss=4.068, trans_loss=5.227, nll_loss=2.497, w2v_ctc_loss=1.21, task_loss=0, contrastive_loss=0.447, total=4143.76, n_correct=2732.54, ppl=5.64, accuracy=65.943, wps=8072.5, ups=1.95, wpb=4143.8, bsz=159.4, num_updates=41900, lr=6.90889e-05, gnorm=1.016, clip=0, loss_scale=32, train_wall=51, gb_free=17.7, wall=24037
2023-08-02 16:26:00 | INFO | train_inner | epoch 029:    745 / 1474 loss=4.055, trans_loss=5.222, nll_loss=2.489, w2v_ctc_loss=1.206, task_loss=0, contrastive_loss=0.279, total=4234.8, n_correct=2798.01, ppl=5.61, accuracy=66.072, wps=8151.3, ups=1.92, wpb=4234.8, bsz=164.1, num_updates=42000, lr=6.90066e-05, gnorm=1, clip=0, loss_scale=32, train_wall=52, gb_free=17.2, wall=24089
2023-08-02 16:26:00 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 16:26:22 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 4.32 | trans_loss 5.537 | nll_loss 2.807 | w2v_ctc_loss 1.36 | task_loss 0 | contrastive_loss 0.262 | total 4003.4 | n_correct 2495.4 | ppl 7 | accuracy 62.332 | uer 16.877 | wer 18.694 | raw_wer 18.694 | bleu 20.14 | wps 2312.9 | wpb 4003.4 | bsz 141.8 | num_updates 42000 | best_bleu 20.76
2023-08-02 16:26:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42000 updates
2023-08-02 16:26:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_29_42000.pt
2023-08-02 16:26:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_29_42000.pt
2023-08-02 16:26:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_29_42000.pt (epoch 29 @ 42000 updates, score 20.14) (writing took 16.51818408817053 seconds)
2023-08-02 16:27:30 | INFO | train_inner | epoch 029:    845 / 1474 loss=4.056, trans_loss=5.243, nll_loss=2.516, w2v_ctc_loss=1.208, task_loss=0, contrastive_loss=0.115, total=4033.21, n_correct=2648.78, ppl=5.72, accuracy=65.674, wps=4465.7, ups=1.11, wpb=4033.2, bsz=140.8, num_updates=42100, lr=6.89246e-05, gnorm=1.009, clip=0, loss_scale=32, train_wall=51, gb_free=17, wall=24180
2023-08-02 16:28:21 | INFO | train_inner | epoch 029:    945 / 1474 loss=4.053, trans_loss=5.235, nll_loss=2.505, w2v_ctc_loss=1.21, task_loss=0, contrastive_loss=0.133, total=4085.97, n_correct=2691.18, ppl=5.68, accuracy=65.864, wps=8072.5, ups=1.98, wpb=4086, bsz=148.4, num_updates=42200, lr=6.88428e-05, gnorm=1.001, clip=0, loss_scale=32, train_wall=50, gb_free=17.6, wall=24230
2023-08-02 16:29:12 | INFO | train_inner | epoch 029:   1045 / 1474 loss=4.055, trans_loss=5.227, nll_loss=2.496, w2v_ctc_loss=1.201, task_loss=0, contrastive_loss=0.288, total=4140.84, n_correct=2728.24, ppl=5.64, accuracy=65.886, wps=8109.1, ups=1.96, wpb=4140.8, bsz=153.4, num_updates=42300, lr=6.87614e-05, gnorm=1.005, clip=0, loss_scale=32, train_wall=51, gb_free=17.1, wall=24281
2023-08-02 16:30:02 | INFO | train_inner | epoch 029:   1145 / 1474 loss=4.05, trans_loss=5.234, nll_loss=2.503, w2v_ctc_loss=1.212, task_loss=0, contrastive_loss=0.107, total=4068.4, n_correct=2675.89, ppl=5.67, accuracy=65.773, wps=8017.6, ups=1.97, wpb=4068.4, bsz=142.1, num_updates=42400, lr=6.86803e-05, gnorm=0.99, clip=0, loss_scale=32, train_wall=50, gb_free=17.9, wall=24332
2023-08-02 16:30:54 | INFO | train_inner | epoch 029:   1245 / 1474 loss=4.06, trans_loss=5.241, nll_loss=2.514, w2v_ctc_loss=1.22, task_loss=0, contrastive_loss=0.122, total=4154.79, n_correct=2728.39, ppl=5.71, accuracy=65.669, wps=8110.1, ups=1.95, wpb=4154.8, bsz=149.8, num_updates=42500, lr=6.85994e-05, gnorm=0.99, clip=0, loss_scale=32, train_wall=51, gb_free=17.3, wall=24383
2023-08-02 16:31:45 | INFO | train_inner | epoch 029:   1345 / 1474 loss=4.057, trans_loss=5.23, nll_loss=2.5, w2v_ctc_loss=1.202, task_loss=0, contrastive_loss=0.252, total=4166.4, n_correct=2746.41, ppl=5.66, accuracy=65.918, wps=8109.6, ups=1.95, wpb=4166.4, bsz=155.8, num_updates=42600, lr=6.85189e-05, gnorm=1.001, clip=0, loss_scale=32, train_wall=51, gb_free=17, wall=24435
2023-08-02 16:32:36 | INFO | train_inner | epoch 029:   1445 / 1474 loss=4.071, trans_loss=5.24, nll_loss=2.514, w2v_ctc_loss=1.219, task_loss=0, contrastive_loss=0.309, total=4169.4, n_correct=2739.24, ppl=5.71, accuracy=65.699, wps=8175.5, ups=1.96, wpb=4169.4, bsz=156, num_updates=42700, lr=6.84386e-05, gnorm=0.996, clip=0, loss_scale=32, train_wall=51, gb_free=16.3, wall=24486
2023-08-02 16:32:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 16:33:14 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 4.307 | trans_loss 5.534 | nll_loss 2.804 | w2v_ctc_loss 1.32 | task_loss 0 | contrastive_loss 0.267 | total 4003.4 | n_correct 2501.4 | ppl 6.98 | accuracy 62.482 | uer 16.755 | wer 18.586 | raw_wer 18.586 | bleu 20.54 | wps 2249.3 | wpb 4003.4 | bsz 141.8 | num_updates 42729 | best_bleu 20.76
2023-08-02 16:33:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42729 updates
2023-08-02 16:33:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.5409.pt
2023-08-02 16:33:18 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.5409.pt
2023-08-02 16:33:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.5409.pt (epoch 29 @ 42729 updates, score 20.54) (writing took 19.051999382674694 seconds)
2023-08-02 16:33:33 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-08-02 16:33:33 | INFO | train | epoch 029 | loss 4.054 | trans_loss 5.228 | nll_loss 2.497 | w2v_ctc_loss 1.208 | task_loss 0 | contrastive_loss 0.224 | total 4138.65 | n_correct 2728.57 | ppl 5.65 | accuracy 65.929 | wps 7223.8 | ups 1.75 | wpb 4138.6 | bsz 152.8 | num_updates 42729 | lr 6.84154e-05 | gnorm 1 | clip 0 | loss_scale 64 | train_wall 748 | gb_free 16.7 | wall 24543
2023-08-02 16:33:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 16:33:33 | INFO | fairseq.trainer | begin training epoch 30
2023-08-02 16:33:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 16:34:17 | INFO | train_inner | epoch 030:     71 / 1474 loss=4.044, trans_loss=5.212, nll_loss=2.476, w2v_ctc_loss=1.186, task_loss=0, contrastive_loss=0.343, total=4176.73, n_correct=2765.18, ppl=5.56, accuracy=66.204, wps=4136.7, ups=0.99, wpb=4176.7, bsz=159.5, num_updates=42800, lr=6.83586e-05, gnorm=0.993, clip=0, loss_scale=64, train_wall=51, gb_free=17.5, wall=24587
2023-08-02 16:35:08 | INFO | train_inner | epoch 030:    171 / 1474 loss=4.021, trans_loss=5.189, nll_loss=2.446, w2v_ctc_loss=1.187, task_loss=0, contrastive_loss=0.203, total=4202.84, n_correct=2803.21, ppl=5.45, accuracy=66.698, wps=8249.1, ups=1.96, wpb=4202.8, bsz=159.3, num_updates=42900, lr=6.82789e-05, gnorm=0.99, clip=0, loss_scale=64, train_wall=51, gb_free=13.3, wall=24638
2023-08-02 16:35:59 | INFO | train_inner | epoch 030:    271 / 1474 loss=4.034, trans_loss=5.208, nll_loss=2.47, w2v_ctc_loss=1.21, task_loss=0, contrastive_loss=0.115, total=4120.08, n_correct=2732, ppl=5.54, accuracy=66.309, wps=8038.5, ups=1.95, wpb=4120.1, bsz=148.2, num_updates=43000, lr=6.81994e-05, gnorm=0.987, clip=0, loss_scale=64, train_wall=51, gb_free=17.2, wall=24689
2023-08-02 16:36:51 | INFO | train_inner | epoch 030:    371 / 1474 loss=4.019, trans_loss=5.201, nll_loss=2.461, w2v_ctc_loss=1.181, task_loss=0, contrastive_loss=0.119, total=4175.82, n_correct=2775.22, ppl=5.5, accuracy=66.459, wps=8116.5, ups=1.94, wpb=4175.8, bsz=153.1, num_updates=43100, lr=6.81203e-05, gnorm=0.981, clip=0, loss_scale=64, train_wall=51, gb_free=16.1, wall=24740
2023-08-02 16:37:42 | INFO | train_inner | epoch 030:    471 / 1474 loss=4.038, trans_loss=5.209, nll_loss=2.472, w2v_ctc_loss=1.19, task_loss=0, contrastive_loss=0.245, total=4128.9, n_correct=2736.93, ppl=5.55, accuracy=66.287, wps=8110.4, ups=1.96, wpb=4128.9, bsz=156.2, num_updates=43200, lr=6.80414e-05, gnorm=1.009, clip=0, loss_scale=64, train_wall=50, gb_free=17.2, wall=24791
2023-08-02 16:38:33 | INFO | train_inner | epoch 030:    571 / 1474 loss=4.038, trans_loss=5.217, nll_loss=2.481, w2v_ctc_loss=1.192, task_loss=0, contrastive_loss=0.171, total=4162.83, n_correct=2753.8, ppl=5.58, accuracy=66.152, wps=8174.7, ups=1.96, wpb=4162.8, bsz=155.7, num_updates=43300, lr=6.79628e-05, gnorm=1.011, clip=0, loss_scale=64, train_wall=50, gb_free=15.5, wall=24842
2023-08-02 16:39:24 | INFO | train_inner | epoch 030:    671 / 1474 loss=4.04, trans_loss=5.212, nll_loss=2.476, w2v_ctc_loss=1.199, task_loss=0, contrastive_loss=0.195, total=4197.56, n_correct=2778.8, ppl=5.56, accuracy=66.2, wps=8190.5, ups=1.95, wpb=4197.6, bsz=158.8, num_updates=43400, lr=6.78844e-05, gnorm=0.988, clip=0, loss_scale=64, train_wall=51, gb_free=17, wall=24893
2023-08-02 16:40:15 | INFO | train_inner | epoch 030:    771 / 1474 loss=4.073, trans_loss=5.236, nll_loss=2.507, w2v_ctc_loss=1.219, task_loss=0, contrastive_loss=0.365, total=4097.27, n_correct=2693.79, ppl=5.69, accuracy=65.746, wps=8002.9, ups=1.95, wpb=4097.3, bsz=150.6, num_updates=43500, lr=6.78064e-05, gnorm=1.016, clip=0, loss_scale=64, train_wall=51, gb_free=15.4, wall=24945
2023-08-02 16:41:06 | INFO | train_inner | epoch 030:    871 / 1474 loss=4.045, trans_loss=5.227, nll_loss=2.496, w2v_ctc_loss=1.205, task_loss=0, contrastive_loss=0.125, total=4097.18, n_correct=2702.15, ppl=5.64, accuracy=65.951, wps=7977.6, ups=1.95, wpb=4097.2, bsz=146.1, num_updates=43600, lr=6.77285e-05, gnorm=1.006, clip=0, loss_scale=64, train_wall=51, gb_free=17.2, wall=24996
2023-08-02 16:41:58 | INFO | train_inner | epoch 030:    971 / 1474 loss=4.057, trans_loss=5.233, nll_loss=2.503, w2v_ctc_loss=1.218, task_loss=0, contrastive_loss=0.161, total=4140.12, n_correct=2725.61, ppl=5.67, accuracy=65.834, wps=8087.1, ups=1.95, wpb=4140.1, bsz=152.3, num_updates=43700, lr=6.7651e-05, gnorm=1.01, clip=0, loss_scale=64, train_wall=51, gb_free=15.8, wall=25047
2023-08-02 16:42:49 | INFO | train_inner | epoch 030:   1071 / 1474 loss=4.064, trans_loss=5.24, nll_loss=2.511, w2v_ctc_loss=1.206, task_loss=0, contrastive_loss=0.304, total=4099.61, n_correct=2688.82, ppl=5.7, accuracy=65.587, wps=7911.2, ups=1.93, wpb=4099.6, bsz=140.7, num_updates=43800, lr=6.75737e-05, gnorm=1.008, clip=0, loss_scale=64, train_wall=51, gb_free=17.8, wall=25099
2023-08-02 16:43:41 | INFO | train_inner | epoch 030:   1171 / 1474 loss=4.049, trans_loss=5.221, nll_loss=2.489, w2v_ctc_loss=1.196, task_loss=0, contrastive_loss=0.26, total=4164.38, n_correct=2751.45, ppl=5.61, accuracy=66.071, wps=8122.5, ups=1.95, wpb=4164.4, bsz=157.4, num_updates=43900, lr=6.74967e-05, gnorm=0.993, clip=0, loss_scale=64, train_wall=51, gb_free=17.8, wall=25150
2023-08-02 16:44:32 | INFO | train_inner | epoch 030:   1271 / 1474 loss=4.048, trans_loss=5.231, nll_loss=2.5, w2v_ctc_loss=1.208, task_loss=0, contrastive_loss=0.126, total=4030.53, n_correct=2655.45, ppl=5.66, accuracy=65.883, wps=7870.3, ups=1.95, wpb=4030.5, bsz=140.7, num_updates=44000, lr=6.742e-05, gnorm=1.033, clip=0, loss_scale=64, train_wall=51, gb_free=17.6, wall=25202
2023-08-02 16:44:32 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 16:44:56 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 4.319 | trans_loss 5.534 | nll_loss 2.801 | w2v_ctc_loss 1.36 | task_loss 0 | contrastive_loss 0.266 | total 4003.4 | n_correct 2499.8 | ppl 6.97 | accuracy 62.442 | uer 16.871 | wer 18.612 | raw_wer 18.612 | bleu 20.4 | wps 2134.8 | wpb 4003.4 | bsz 141.8 | num_updates 44000 | best_bleu 20.76
2023-08-02 16:44:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44000 updates
2023-08-02 16:44:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_30_44000.pt
2023-08-02 16:45:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_30_44000.pt
2023-08-02 16:45:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_30_44000.pt (epoch 30 @ 44000 updates, score 20.4) (writing took 42.26143692433834 seconds)
2023-08-02 16:46:31 | INFO | train_inner | epoch 030:   1371 / 1474 loss=4.028, trans_loss=5.211, nll_loss=2.476, w2v_ctc_loss=1.176, task_loss=0, contrastive_loss=0.145, total=4163.24, n_correct=2757.19, ppl=5.56, accuracy=66.227, wps=3494.4, ups=0.84, wpb=4163.2, bsz=160.3, num_updates=44100, lr=6.73435e-05, gnorm=0.989, clip=0, loss_scale=64, train_wall=51, gb_free=17.6, wall=25321
2023-08-02 16:47:22 | INFO | train_inner | epoch 030:   1471 / 1474 loss=4.056, trans_loss=5.224, nll_loss=2.493, w2v_ctc_loss=1.183, task_loss=0, contrastive_loss=0.448, total=4130.55, n_correct=2725.59, ppl=5.63, accuracy=65.986, wps=8081.8, ups=1.96, wpb=4130.6, bsz=155.8, num_updates=44200, lr=6.72673e-05, gnorm=0.992, clip=0, loss_scale=64, train_wall=51, gb_free=17, wall=25372
2023-08-02 16:47:24 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 16:47:47 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 4.315 | trans_loss 5.535 | nll_loss 2.802 | w2v_ctc_loss 1.343 | task_loss 0 | contrastive_loss 0.269 | total 4003.4 | n_correct 2501.1 | ppl 6.98 | accuracy 62.474 | uer 16.808 | wer 18.668 | raw_wer 18.668 | bleu 20.16 | wps 2203 | wpb 4003.4 | bsz 141.8 | num_updates 44203 | best_bleu 20.76
2023-08-02 16:47:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44203 updates
2023-08-02 16:47:47 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_last.pt
2023-08-02 16:48:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_last.pt
2023-08-02 16:48:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_last.pt (epoch 30 @ 44203 updates, score 20.16) (writing took 13.270158883184195 seconds)
2023-08-02 16:48:00 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-08-02 16:48:00 | INFO | train | epoch 030 | loss 4.044 | trans_loss 5.218 | nll_loss 2.484 | w2v_ctc_loss 1.197 | task_loss 0 | contrastive_loss 0.224 | total 4138.65 | n_correct 2736.09 | ppl 5.59 | accuracy 66.111 | wps 7033.7 | ups 1.7 | wpb 4138.6 | bsz 152.8 | num_updates 44203 | lr 6.7265e-05 | gnorm 1.001 | clip 0 | loss_scale 64 | train_wall 749 | gb_free 17.5 | wall 25410
2023-08-02 16:48:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 16:48:01 | INFO | fairseq.trainer | begin training epoch 31
2023-08-02 16:48:01 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 16:48:57 | INFO | train_inner | epoch 031:     97 / 1474 loss=4.022, trans_loss=5.198, nll_loss=2.457, w2v_ctc_loss=1.193, task_loss=0, contrastive_loss=0.126, total=4081.82, n_correct=2716.17, ppl=5.49, accuracy=66.543, wps=4316.7, ups=1.06, wpb=4081.8, bsz=147.8, num_updates=44300, lr=6.71913e-05, gnorm=1.002, clip=0, loss_scale=64, train_wall=50, gb_free=14.1, wall=25466
2023-08-02 16:49:48 | INFO | train_inner | epoch 031:    197 / 1474 loss=4.031, trans_loss=5.203, nll_loss=2.463, w2v_ctc_loss=1.201, task_loss=0, contrastive_loss=0.172, total=4143.18, n_correct=2749.26, ppl=5.51, accuracy=66.356, wps=8079.8, ups=1.95, wpb=4143.2, bsz=150.5, num_updates=44400, lr=6.71156e-05, gnorm=1.008, clip=0, loss_scale=64, train_wall=51, gb_free=17.6, wall=25518
2023-08-02 16:50:39 | INFO | train_inner | epoch 031:    297 / 1474 loss=4.032, trans_loss=5.203, nll_loss=2.463, w2v_ctc_loss=1.189, task_loss=0, contrastive_loss=0.252, total=4154.03, n_correct=2756.59, ppl=5.51, accuracy=66.359, wps=8104.6, ups=1.95, wpb=4154, bsz=150.7, num_updates=44500, lr=6.70402e-05, gnorm=1, clip=0, loss_scale=64, train_wall=51, gb_free=16.8, wall=25569
2023-08-02 16:51:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-02 16:51:31 | INFO | train_inner | epoch 031:    398 / 1474 loss=4.031, trans_loss=5.212, nll_loss=2.475, w2v_ctc_loss=1.192, task_loss=0, contrastive_loss=0.129, total=4089.52, n_correct=2705.98, ppl=5.56, accuracy=66.169, wps=7928.6, ups=1.94, wpb=4089.5, bsz=142.9, num_updates=44600, lr=6.6965e-05, gnorm=1.012, clip=0, loss_scale=32, train_wall=51, gb_free=16.6, wall=25621
2023-08-02 16:52:22 | INFO | train_inner | epoch 031:    498 / 1474 loss=4.026, trans_loss=5.199, nll_loss=2.459, w2v_ctc_loss=1.199, task_loss=0, contrastive_loss=0.145, total=4114.41, n_correct=2730.75, ppl=5.5, accuracy=66.37, wps=7984, ups=1.94, wpb=4114.4, bsz=150.4, num_updates=44700, lr=6.689e-05, gnorm=1.002, clip=0, loss_scale=32, train_wall=51, gb_free=16.5, wall=25672
2023-08-02 16:53:14 | INFO | train_inner | epoch 031:    598 / 1474 loss=4.029, trans_loss=5.207, nll_loss=2.468, w2v_ctc_loss=1.199, task_loss=0, contrastive_loss=0.127, total=4084.36, n_correct=2707.79, ppl=5.53, accuracy=66.297, wps=7983.4, ups=1.95, wpb=4084.4, bsz=147.5, num_updates=44800, lr=6.68153e-05, gnorm=1.019, clip=0, loss_scale=32, train_wall=51, gb_free=16.9, wall=25723
2023-08-02 16:54:04 | INFO | train_inner | epoch 031:    698 / 1474 loss=4.015, trans_loss=5.196, nll_loss=2.456, w2v_ctc_loss=1.172, task_loss=0, contrastive_loss=0.126, total=4210.09, n_correct=2797.88, ppl=5.49, accuracy=66.457, wps=8265.7, ups=1.96, wpb=4210.1, bsz=157.4, num_updates=44900, lr=6.67409e-05, gnorm=0.989, clip=0, loss_scale=32, train_wall=51, gb_free=15.3, wall=25774
2023-08-02 16:54:55 | INFO | train_inner | epoch 031:    798 / 1474 loss=4.043, trans_loss=5.214, nll_loss=2.478, w2v_ctc_loss=1.193, task_loss=0, contrastive_loss=0.266, total=4098.1, n_correct=2710.57, ppl=5.57, accuracy=66.142, wps=8047.1, ups=1.96, wpb=4098.1, bsz=147.8, num_updates=45000, lr=6.66667e-05, gnorm=1.013, clip=0, loss_scale=32, train_wall=50, gb_free=17.4, wall=25825
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:0')
2023-08-02 16:55:47 | INFO | train_inner | epoch 031:    898 / 1474 loss=4.022, trans_loss=5.202, nll_loss=2.463, w2v_ctc_loss=1.179, task_loss=0, contrastive_loss=0.155, total=4101.05, n_correct=2719.26, ppl=5.51, accuracy=66.306, wps=8029.2, ups=1.96, wpb=4101.1, bsz=148.4, num_updates=45100, lr=6.65927e-05, gnorm=1.006, clip=0, loss_scale=32, train_wall=51, gb_free=17.4, wall=25876
2023-08-02 16:56:37 | INFO | train_inner | epoch 031:    998 / 1474 loss=4.044, trans_loss=5.212, nll_loss=2.477, w2v_ctc_loss=1.188, task_loss=0, contrastive_loss=0.324, total=4186.3, n_correct=2775.28, ppl=5.57, accuracy=66.294, wps=8213.8, ups=1.96, wpb=4186.3, bsz=159.3, num_updates=45200, lr=6.6519e-05, gnorm=0.988, clip=0, loss_scale=32, train_wall=51, gb_free=16.7, wall=25927
2023-08-02 16:57:29 | INFO | train_inner | epoch 031:   1098 / 1474 loss=4.035, trans_loss=5.211, nll_loss=2.476, w2v_ctc_loss=1.185, task_loss=0, contrastive_loss=0.216, total=4147.34, n_correct=2749.36, ppl=5.56, accuracy=66.292, wps=8053, ups=1.94, wpb=4147.3, bsz=157.3, num_updates=45300, lr=6.64455e-05, gnorm=1.003, clip=0, loss_scale=32, train_wall=51, gb_free=17.5, wall=25979
2023-08-02 16:58:20 | INFO | train_inner | epoch 031:   1198 / 1474 loss=4.053, trans_loss=5.213, nll_loss=2.48, w2v_ctc_loss=1.189, task_loss=0, contrastive_loss=0.449, total=4185.34, n_correct=2769.7, ppl=5.58, accuracy=66.176, wps=8272.5, ups=1.98, wpb=4185.3, bsz=160.8, num_updates=45400, lr=6.63723e-05, gnorm=1.002, clip=0, loss_scale=32, train_wall=50, gb_free=17.4, wall=26029
2023-08-02 16:59:11 | INFO | train_inner | epoch 031:   1298 / 1474 loss=4.038, trans_loss=5.215, nll_loss=2.482, w2v_ctc_loss=1.2, task_loss=0, contrastive_loss=0.138, total=4223.54, n_correct=2792.91, ppl=5.59, accuracy=66.127, wps=8285.1, ups=1.96, wpb=4223.5, bsz=162.5, num_updates=45500, lr=6.62994e-05, gnorm=0.991, clip=0, loss_scale=32, train_wall=51, gb_free=14.1, wall=26080
2023-08-02 17:00:02 | INFO | train_inner | epoch 031:   1398 / 1474 loss=4.058, trans_loss=5.213, nll_loss=2.479, w2v_ctc_loss=1.176, task_loss=0, contrastive_loss=0.544, total=4195.76, n_correct=2781.66, ppl=5.58, accuracy=66.297, wps=8158, ups=1.94, wpb=4195.8, bsz=164.1, num_updates=45600, lr=6.62266e-05, gnorm=0.996, clip=0, loss_scale=32, train_wall=51, gb_free=17, wall=26132
2023-08-02 17:00:41 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(1.5610, device='cuda:1')
2023-08-02 17:01:02 | INFO | dev_st | epoch 031 | valid on 'dev_st' subset | loss 4.314 | trans_loss 5.524 | nll_loss 2.793 | w2v_ctc_loss 1.366 | task_loss 0 | contrastive_loss 0.269 | total 4003.4 | n_correct 2505 | ppl 6.93 | accuracy 62.572 | uer 16.508 | wer 18.28 | raw_wer 18.28 | bleu 20.64 | wps 2408.5 | wpb 4003.4 | bsz 141.8 | num_updates 45676 | best_bleu 20.76
2023-08-02 17:01:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 45676 updates
2023-08-02 17:01:02 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.6400.pt
2023-08-02 17:01:06 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.6400.pt
2023-08-02 17:01:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.6400.pt (epoch 31 @ 45676 updates, score 20.64) (writing took 31.290218766778708 seconds)
2023-08-02 17:01:34 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-08-02 17:01:34 | INFO | train | epoch 031 | loss 4.034 | trans_loss 5.208 | nll_loss 2.471 | w2v_ctc_loss 1.19 | task_loss 0 | contrastive_loss 0.224 | total 4138.88 | n_correct 2743.53 | ppl 5.54 | accuracy 66.287 | wps 7492.3 | ups 1.81 | wpb 4138.9 | bsz 152.9 | num_updates 45676 | lr 6.61715e-05 | gnorm 1.003 | clip 0 | loss_scale 32 | train_wall 746 | gb_free 12.8 | wall 26224
2023-08-02 17:01:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 17:01:34 | INFO | fairseq.trainer | begin training epoch 32
2023-08-02 17:01:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 17:01:54 | INFO | train_inner | epoch 032:     24 / 1474 loss=4.026, trans_loss=5.207, nll_loss=2.47, w2v_ctc_loss=1.193, task_loss=0, contrastive_loss=0.115, total=4039.04, n_correct=2676.95, ppl=5.54, accuracy=66.277, wps=3594.9, ups=0.89, wpb=4039, bsz=143.6, num_updates=45700, lr=6.61541e-05, gnorm=1.027, clip=0, loss_scale=32, train_wall=50, gb_free=18.1, wall=26244
2023-08-02 17:02:46 | INFO | train_inner | epoch 032:    124 / 1474 loss=3.987, trans_loss=5.163, nll_loss=2.413, w2v_ctc_loss=1.152, task_loss=0, contrastive_loss=0.142, total=4224.84, n_correct=2831.93, ppl=5.33, accuracy=67.03, wps=8184.3, ups=1.94, wpb=4224.8, bsz=161.2, num_updates=45800, lr=6.60819e-05, gnorm=0.986, clip=0, loss_scale=32, train_wall=51, gb_free=16.9, wall=26296
2023-08-02 17:03:37 | INFO | train_inner | epoch 032:    224 / 1474 loss=4.015, trans_loss=5.188, nll_loss=2.446, w2v_ctc_loss=1.179, task_loss=0, contrastive_loss=0.16, total=4163.01, n_correct=2772.74, ppl=5.45, accuracy=66.604, wps=8128.5, ups=1.95, wpb=4163, bsz=161.1, num_updates=45900, lr=6.60098e-05, gnorm=1.008, clip=0, loss_scale=32, train_wall=51, gb_free=17.5, wall=26347
2023-08-02 17:04:28 | INFO | train_inner | epoch 032:    324 / 1474 loss=3.99, trans_loss=5.17, nll_loss=2.421, w2v_ctc_loss=1.147, task_loss=0, contrastive_loss=0.146, total=4185.21, n_correct=2798.9, ppl=5.36, accuracy=66.876, wps=8254, ups=1.97, wpb=4185.2, bsz=157.5, num_updates=46000, lr=6.5938e-05, gnorm=0.997, clip=0, loss_scale=32, train_wall=50, gb_free=16.8, wall=26398
2023-08-02 17:04:28 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 17:04:51 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 4.328 | trans_loss 5.534 | nll_loss 2.804 | w2v_ctc_loss 1.388 | task_loss 0 | contrastive_loss 0.272 | total 4003.4 | n_correct 2503.4 | ppl 6.98 | accuracy 62.532 | uer 16.861 | wer 18.769 | raw_wer 18.769 | bleu 20.44 | wps 2231.3 | wpb 4003.4 | bsz 141.8 | num_updates 46000 | best_bleu 20.76
2023-08-02 17:04:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 46000 updates
2023-08-02 17:04:51 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_32_46000.pt
2023-08-02 17:04:55 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_32_46000.pt
2023-08-02 17:05:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_32_46000.pt (epoch 32 @ 46000 updates, score 20.44) (writing took 40.106772776693106 seconds)
2023-08-02 17:06:24 | INFO | train_inner | epoch 032:    424 / 1474 loss=4.009, trans_loss=5.186, nll_loss=2.443, w2v_ctc_loss=1.173, task_loss=0, contrastive_loss=0.137, total=4156.71, n_correct=2772.85, ppl=5.44, accuracy=66.708, wps=3568.1, ups=0.86, wpb=4156.7, bsz=152.8, num_updates=46100, lr=6.58665e-05, gnorm=1.006, clip=0, loss_scale=32, train_wall=51, gb_free=11.6, wall=26514
2023-08-02 17:07:16 | INFO | train_inner | epoch 032:    524 / 1474 loss=4.039, trans_loss=5.201, nll_loss=2.462, w2v_ctc_loss=1.194, task_loss=0, contrastive_loss=0.306, total=4195.32, n_correct=2787.06, ppl=5.51, accuracy=66.433, wps=8203.2, ups=1.96, wpb=4195.3, bsz=159.1, num_updates=46200, lr=6.57952e-05, gnorm=1.009, clip=0, loss_scale=32, train_wall=51, gb_free=17, wall=26565
2023-08-02 17:08:07 | INFO | train_inner | epoch 032:    624 / 1474 loss=4.025, trans_loss=5.202, nll_loss=2.463, w2v_ctc_loss=1.186, task_loss=0, contrastive_loss=0.157, total=4141.99, n_correct=2749.27, ppl=5.51, accuracy=66.376, wps=8065.9, ups=1.95, wpb=4142, bsz=150.5, num_updates=46300, lr=6.57241e-05, gnorm=1.005, clip=0, loss_scale=32, train_wall=51, gb_free=17, wall=26617
2023-08-02 17:08:59 | INFO | train_inner | epoch 032:    724 / 1474 loss=4.021, trans_loss=5.199, nll_loss=2.459, w2v_ctc_loss=1.189, task_loss=0, contrastive_loss=0.124, total=4153.97, n_correct=2759.03, ppl=5.5, accuracy=66.419, wps=8016, ups=1.93, wpb=4154, bsz=150.7, num_updates=46400, lr=6.56532e-05, gnorm=1.003, clip=0, loss_scale=32, train_wall=51, gb_free=17.5, wall=26668
2023-08-02 17:09:49 | INFO | train_inner | epoch 032:    824 / 1474 loss=4.011, trans_loss=5.194, nll_loss=2.453, w2v_ctc_loss=1.169, task_loss=0, contrastive_loss=0.119, total=4119.08, n_correct=2737.24, ppl=5.48, accuracy=66.453, wps=8138.4, ups=1.98, wpb=4119.1, bsz=147.6, num_updates=46500, lr=6.55826e-05, gnorm=0.999, clip=0, loss_scale=32, train_wall=50, gb_free=17.3, wall=26719
2023-08-02 17:10:41 | INFO | train_inner | epoch 032:    924 / 1474 loss=4.014, trans_loss=5.197, nll_loss=2.458, w2v_ctc_loss=1.17, task_loss=0, contrastive_loss=0.119, total=4142.37, n_correct=2751.26, ppl=5.49, accuracy=66.418, wps=8072.2, ups=1.95, wpb=4142.4, bsz=149.6, num_updates=46600, lr=6.55122e-05, gnorm=0.991, clip=0, loss_scale=32, train_wall=51, gb_free=16.2, wall=26770
2023-08-02 17:11:32 | INFO | train_inner | epoch 032:   1024 / 1474 loss=4.036, trans_loss=5.208, nll_loss=2.471, w2v_ctc_loss=1.18, task_loss=0, contrastive_loss=0.305, total=4112.12, n_correct=2724.2, ppl=5.54, accuracy=66.248, wps=8048.1, ups=1.96, wpb=4112.1, bsz=151.2, num_updates=46700, lr=6.5442e-05, gnorm=1.006, clip=0, loss_scale=64, train_wall=51, gb_free=17.2, wall=26821
2023-08-02 17:12:23 | INFO | train_inner | epoch 032:   1124 / 1474 loss=4.037, trans_loss=5.216, nll_loss=2.48, w2v_ctc_loss=1.191, task_loss=0, contrastive_loss=0.187, total=4022.64, n_correct=2657.35, ppl=5.58, accuracy=66.06, wps=7889.1, ups=1.96, wpb=4022.6, bsz=136.5, num_updates=46800, lr=6.5372e-05, gnorm=1.023, clip=0, loss_scale=64, train_wall=51, gb_free=11.9, wall=26872
2023-08-02 17:13:14 | INFO | train_inner | epoch 032:   1224 / 1474 loss=4.059, trans_loss=5.223, nll_loss=2.492, w2v_ctc_loss=1.186, task_loss=0, contrastive_loss=0.403, total=4145.44, n_correct=2735.72, ppl=5.62, accuracy=65.993, wps=8092.3, ups=1.95, wpb=4145.4, bsz=154.3, num_updates=46900, lr=6.53023e-05, gnorm=1.018, clip=0, loss_scale=64, train_wall=51, gb_free=17.5, wall=26924
2023-08-02 17:14:05 | INFO | train_inner | epoch 032:   1324 / 1474 loss=4.025, trans_loss=5.205, nll_loss=2.468, w2v_ctc_loss=1.187, task_loss=0, contrastive_loss=0.117, total=4083.72, n_correct=2707.49, ppl=5.53, accuracy=66.3, wps=8053.8, ups=1.97, wpb=4083.7, bsz=148.6, num_updates=47000, lr=6.52328e-05, gnorm=1.007, clip=0, loss_scale=64, train_wall=50, gb_free=16.3, wall=26974
2023-08-02 17:14:56 | INFO | train_inner | epoch 032:   1424 / 1474 loss=4.071, trans_loss=5.221, nll_loss=2.489, w2v_ctc_loss=1.211, task_loss=0, contrastive_loss=0.589, total=4105.33, n_correct=2713.48, ppl=5.61, accuracy=66.097, wps=8028.6, ups=1.96, wpb=4105.3, bsz=152.8, num_updates=47100, lr=6.51635e-05, gnorm=1.016, clip=0, loss_scale=64, train_wall=51, gb_free=17.1, wall=27025
2023-08-02 17:15:21 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 17:15:44 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 4.341 | trans_loss 5.533 | nll_loss 2.803 | w2v_ctc_loss 1.431 | task_loss 0 | contrastive_loss 0.27 | total 4003.4 | n_correct 2501.2 | ppl 6.98 | accuracy 62.477 | uer 16.983 | wer 18.836 | raw_wer 18.836 | bleu 20.45 | wps 2192 | wpb 4003.4 | bsz 141.8 | num_updates 47150 | best_bleu 20.76
2023-08-02 17:15:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 47150 updates
2023-08-02 17:15:44 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.4505.pt
2023-08-02 17:15:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.4505.pt
2023-08-02 17:16:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint.best_bleu_20.4505.pt (epoch 32 @ 47150 updates, score 20.45) (writing took 28.153554381802678 seconds)
2023-08-02 17:16:13 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-08-02 17:16:13 | INFO | train | epoch 032 | loss 4.025 | trans_loss 5.198 | nll_loss 2.458 | w2v_ctc_loss 1.179 | task_loss 0 | contrastive_loss 0.225 | total 4138.65 | n_correct 2749.75 | ppl 5.49 | accuracy 66.441 | wps 6943.2 | ups 1.68 | wpb 4138.6 | bsz 152.8 | num_updates 47150 | lr 6.5129e-05 | gnorm 1.006 | clip 0 | loss_scale 64 | train_wall 747 | gb_free 17 | wall 27102
2023-08-02 17:16:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 17:16:13 | INFO | fairseq.trainer | begin training epoch 33
2023-08-02 17:16:13 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 17:16:47 | INFO | train_inner | epoch 033:     50 / 1474 loss=4.027, trans_loss=5.192, nll_loss=2.451, w2v_ctc_loss=1.168, task_loss=0, contrastive_loss=0.325, total=4161.67, n_correct=2771.81, ppl=5.47, accuracy=66.603, wps=3747.1, ups=0.9, wpb=4161.7, bsz=161.3, num_updates=47200, lr=6.50945e-05, gnorm=1.009, clip=0, loss_scale=64, train_wall=51, gb_free=18.2, wall=27137
2023-08-02 17:17:38 | INFO | train_inner | epoch 033:    150 / 1474 loss=3.982, trans_loss=5.168, nll_loss=2.418, w2v_ctc_loss=1.139, task_loss=0, contrastive_loss=0.1, total=4067.33, n_correct=2723.77, ppl=5.34, accuracy=66.967, wps=7998.8, ups=1.97, wpb=4067.3, bsz=142.1, num_updates=47300, lr=6.50256e-05, gnorm=1.007, clip=0, loss_scale=64, train_wall=50, gb_free=17.7, wall=27187
2023-08-02 17:18:29 | INFO | train_inner | epoch 033:    250 / 1474 loss=4.025, trans_loss=5.176, nll_loss=2.431, w2v_ctc_loss=1.159, task_loss=0, contrastive_loss=0.451, total=4278.37, n_correct=2863.18, ppl=5.39, accuracy=66.922, wps=8346.1, ups=1.95, wpb=4278.4, bsz=172.6, num_updates=47400, lr=6.4957e-05, gnorm=0.997, clip=0, loss_scale=64, train_wall=51, gb_free=15.1, wall=27239
2023-08-02 17:19:20 | INFO | train_inner | epoch 033:    350 / 1474 loss=4.007, trans_loss=5.183, nll_loss=2.439, w2v_ctc_loss=1.171, task_loss=0, contrastive_loss=0.157, total=4121.62, n_correct=2743.65, ppl=5.42, accuracy=66.567, wps=8048.6, ups=1.95, wpb=4121.6, bsz=150.2, num_updates=47500, lr=6.48886e-05, gnorm=1.013, clip=0, loss_scale=64, train_wall=51, gb_free=12.9, wall=27290
2023-08-02 17:20:11 | INFO | train_inner | epoch 033:    450 / 1474 loss=3.985, trans_loss=5.166, nll_loss=2.416, w2v_ctc_loss=1.146, task_loss=0, contrastive_loss=0.123, total=4138.89, n_correct=2768.9, ppl=5.34, accuracy=66.9, wps=8146.3, ups=1.97, wpb=4138.9, bsz=155.9, num_updates=47600, lr=6.48204e-05, gnorm=0.996, clip=0, loss_scale=64, train_wall=50, gb_free=16.9, wall=27341
2023-08-02 17:21:02 | INFO | train_inner | epoch 033:    550 / 1474 loss=4.018, trans_loss=5.194, nll_loss=2.452, w2v_ctc_loss=1.18, task_loss=0, contrastive_loss=0.159, total=4129.77, n_correct=2744.53, ppl=5.47, accuracy=66.457, wps=8085.6, ups=1.96, wpb=4129.8, bsz=146.4, num_updates=47700, lr=6.47524e-05, gnorm=1.012, clip=0, loss_scale=64, train_wall=51, gb_free=14, wall=27392
2023-08-02 17:21:53 | INFO | train_inner | epoch 033:    650 / 1474 loss=4.023, trans_loss=5.199, nll_loss=2.458, w2v_ctc_loss=1.172, task_loss=0, contrastive_loss=0.225, total=4165.89, n_correct=2765.81, ppl=5.5, accuracy=66.392, wps=8156.4, ups=1.96, wpb=4165.9, bsz=151.8, num_updates=47800, lr=6.46846e-05, gnorm=1.023, clip=0, loss_scale=64, train_wall=51, gb_free=17.6, wall=27443
2023-08-02 17:22:44 | INFO | train_inner | epoch 033:    750 / 1474 loss=4.032, trans_loss=5.206, nll_loss=2.468, w2v_ctc_loss=1.212, task_loss=0, contrastive_loss=0.118, total=4069.2, n_correct=2697.36, ppl=5.53, accuracy=66.287, wps=7962.2, ups=1.96, wpb=4069.2, bsz=143.1, num_updates=47900, lr=6.46171e-05, gnorm=1.018, clip=0, loss_scale=64, train_wall=51, gb_free=15.4, wall=27494
2023-08-02 17:23:35 | INFO | train_inner | epoch 033:    850 / 1474 loss=4.006, trans_loss=5.183, nll_loss=2.439, w2v_ctc_loss=1.147, task_loss=0, contrastive_loss=0.257, total=4129.04, n_correct=2756.61, ppl=5.42, accuracy=66.762, wps=8125.2, ups=1.97, wpb=4129, bsz=157.7, num_updates=48000, lr=6.45497e-05, gnorm=0.994, clip=0, loss_scale=64, train_wall=50, gb_free=16.3, wall=27545
2023-08-02 17:23:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 17:23:58 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 4.335 | trans_loss 5.537 | nll_loss 2.805 | w2v_ctc_loss 1.403 | task_loss 0 | contrastive_loss 0.271 | total 4003.4 | n_correct 2501.4 | ppl 6.99 | accuracy 62.482 | uer 16.649 | wer 18.37 | raw_wer 18.37 | bleu 20.23 | wps 2298.4 | wpb 4003.4 | bsz 141.8 | num_updates 48000 | best_bleu 20.76
2023-08-02 17:23:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48000 updates
2023-08-02 17:23:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_33_48000.pt
2023-08-02 17:24:02 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_33_48000.pt
2023-08-02 17:24:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_33_48000.pt (epoch 33 @ 48000 updates, score 20.23) (writing took 27.92276394367218 seconds)
2023-08-02 17:25:17 | INFO | train_inner | epoch 033:    950 / 1474 loss=4.017, trans_loss=5.193, nll_loss=2.452, w2v_ctc_loss=1.181, task_loss=0, contrastive_loss=0.149, total=4159.38, n_correct=2764.4, ppl=5.47, accuracy=66.462, wps=4070.2, ups=0.98, wpb=4159.4, bsz=155.6, num_updates=48100, lr=6.44826e-05, gnorm=1.019, clip=0, loss_scale=64, train_wall=51, gb_free=15.1, wall=27647
2023-08-02 17:26:08 | INFO | train_inner | epoch 033:   1050 / 1474 loss=4.042, trans_loss=5.203, nll_loss=2.465, w2v_ctc_loss=1.19, task_loss=0, contrastive_loss=0.35, total=4132.38, n_correct=2740.32, ppl=5.52, accuracy=66.313, wps=8107.1, ups=1.96, wpb=4132.4, bsz=153.1, num_updates=48200, lr=6.44157e-05, gnorm=1.016, clip=0, loss_scale=64, train_wall=50, gb_free=15.9, wall=27698
2023-08-02 17:26:59 | INFO | train_inner | epoch 033:   1150 / 1474 loss=4.034, trans_loss=5.204, nll_loss=2.467, w2v_ctc_loss=1.174, task_loss=0, contrastive_loss=0.325, total=4182.36, n_correct=2772.89, ppl=5.53, accuracy=66.3, wps=8202, ups=1.96, wpb=4182.4, bsz=155.1, num_updates=48300, lr=6.43489e-05, gnorm=1.021, clip=0, loss_scale=64, train_wall=50, gb_free=11, wall=27749
2023-08-02 17:27:51 | INFO | train_inner | epoch 033:   1250 / 1474 loss=4.022, trans_loss=5.199, nll_loss=2.459, w2v_ctc_loss=1.192, task_loss=0, contrastive_loss=0.128, total=4107.66, n_correct=2729.19, ppl=5.5, accuracy=66.441, wps=8016.1, ups=1.95, wpb=4107.7, bsz=146.1, num_updates=48400, lr=6.42824e-05, gnorm=1.012, clip=0, loss_scale=64, train_wall=51, gb_free=16.3, wall=27800
2023-08-02 17:28:42 | INFO | train_inner | epoch 033:   1350 / 1474 loss=4.014, trans_loss=5.19, nll_loss=2.449, w2v_ctc_loss=1.173, task_loss=0, contrastive_loss=0.166, total=4133.5, n_correct=2752.89, ppl=5.46, accuracy=66.599, wps=8068.2, ups=1.95, wpb=4133.5, bsz=156.7, num_updates=48500, lr=6.42161e-05, gnorm=1.006, clip=0, loss_scale=64, train_wall=51, gb_free=15.9, wall=27851
2023-08-02 17:29:33 | INFO | train_inner | epoch 033:   1450 / 1474 loss=4.036, trans_loss=5.199, nll_loss=2.461, w2v_ctc_loss=1.17, task_loss=0, contrastive_loss=0.456, total=4127.53, n_correct=2741.17, ppl=5.51, accuracy=66.412, wps=8045.8, ups=1.95, wpb=4127.5, bsz=155.2, num_updates=48600, lr=6.415e-05, gnorm=1.008, clip=0, loss_scale=64, train_wall=51, gb_free=15.9, wall=27903
2023-08-02 17:29:45 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 17:30:08 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 4.327 | trans_loss 5.531 | nll_loss 2.799 | w2v_ctc_loss 1.392 | task_loss 0 | contrastive_loss 0.268 | total 4003.4 | n_correct 2503 | ppl 6.96 | accuracy 62.522 | uer 16.768 | wer 18.579 | raw_wer 18.579 | bleu 20.38 | wps 2208.1 | wpb 4003.4 | bsz 141.8 | num_updates 48624 | best_bleu 20.76
2023-08-02 17:30:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48624 updates
2023-08-02 17:30:08 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_last.pt
2023-08-02 17:30:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_last.pt
2023-08-02 17:30:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_last.pt (epoch 33 @ 48624 updates, score 20.38) (writing took 12.845792042091489 seconds)
2023-08-02 17:30:21 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-08-02 17:30:21 | INFO | train | epoch 033 | loss 4.017 | trans_loss 5.19 | nll_loss 2.448 | w2v_ctc_loss 1.172 | task_loss 0 | contrastive_loss 0.225 | total 4138.65 | n_correct 2754.69 | ppl 5.46 | accuracy 66.56 | wps 7188.7 | ups 1.74 | wpb 4138.6 | bsz 152.8 | num_updates 48624 | lr 6.41342e-05 | gnorm 1.01 | clip 0 | loss_scale 64 | train_wall 746 | gb_free 18.2 | wall 27951
2023-08-02 17:30:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-02 17:30:22 | INFO | fairseq.trainer | begin training epoch 34
2023-08-02 17:30:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-02 17:31:08 | INFO | train_inner | epoch 034:     76 / 1474 loss=3.992, trans_loss=5.169, nll_loss=2.42, w2v_ctc_loss=1.16, task_loss=0, contrastive_loss=0.131, total=4129.08, n_correct=2764.27, ppl=5.35, accuracy=66.946, wps=4331.7, ups=1.05, wpb=4129.1, bsz=151.3, num_updates=48700, lr=6.40841e-05, gnorm=1, clip=0, loss_scale=128, train_wall=51, gb_free=16, wall=27998
2023-08-02 17:31:59 | INFO | train_inner | epoch 034:    176 / 1474 loss=3.989, trans_loss=5.163, nll_loss=2.412, w2v_ctc_loss=1.166, task_loss=0, contrastive_loss=0.133, total=4070.46, n_correct=2729.98, ppl=5.32, accuracy=67.068, wps=8032.9, ups=1.97, wpb=4070.5, bsz=147.1, num_updates=48800, lr=6.40184e-05, gnorm=1.003, clip=0, loss_scale=128, train_wall=50, gb_free=13.8, wall=28049
2023-08-02 17:32:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-02 17:32:51 | INFO | train_inner | epoch 034:    277 / 1474 loss=4.008, trans_loss=5.176, nll_loss=2.43, w2v_ctc_loss=1.15, task_loss=0, contrastive_loss=0.339, total=4207.39, n_correct=2811.86, ppl=5.39, accuracy=66.831, wps=8041.1, ups=1.91, wpb=4207.4, bsz=158.6, num_updates=48900, lr=6.39529e-05, gnorm=0.991, clip=0, loss_scale=64, train_wall=52, gb_free=17, wall=28101
2023-08-02 17:33:43 | INFO | train_inner | epoch 034:    377 / 1474 loss=3.998, trans_loss=5.161, nll_loss=2.411, w2v_ctc_loss=1.149, task_loss=0, contrastive_loss=0.331, total=4173.35, n_correct=2799.8, ppl=5.32, accuracy=67.088, wps=8145.3, ups=1.95, wpb=4173.4, bsz=160.2, num_updates=49000, lr=6.38877e-05, gnorm=1.007, clip=0, loss_scale=64, train_wall=51, gb_free=16.8, wall=28152
2023-08-02 17:34:34 | INFO | train_inner | epoch 034:    477 / 1474 loss=4.012, trans_loss=5.185, nll_loss=2.44, w2v_ctc_loss=1.19, task_loss=0, contrastive_loss=0.123, total=4066.67, n_correct=2709.19, ppl=5.43, accuracy=66.619, wps=7962.7, ups=1.96, wpb=4066.7, bsz=142.3, num_updates=49100, lr=6.38226e-05, gnorm=1.023, clip=0, loss_scale=64, train_wall=51, gb_free=17.2, wall=28203
2023-08-02 17:35:24 | INFO | train_inner | epoch 034:    577 / 1474 loss=3.983, trans_loss=5.162, nll_loss=2.411, w2v_ctc_loss=1.147, task_loss=0, contrastive_loss=0.125, total=4115.49, n_correct=2759.64, ppl=5.32, accuracy=67.055, wps=8125.2, ups=1.97, wpb=4115.5, bsz=149.7, num_updates=49200, lr=6.37577e-05, gnorm=1.01, clip=0, loss_scale=64, train_wall=50, gb_free=15.5, wall=28254
2023-08-02 17:36:16 | INFO | train_inner | epoch 034:    677 / 1474 loss=3.988, trans_loss=5.167, nll_loss=2.418, w2v_ctc_loss=1.154, task_loss=0, contrastive_loss=0.116, total=4124.78, n_correct=2761.24, ppl=5.35, accuracy=66.943, wps=8048.9, ups=1.95, wpb=4124.8, bsz=150.1, num_updates=49300, lr=6.3693e-05, gnorm=0.994, clip=0, loss_scale=64, train_wall=51, gb_free=12.3, wall=28305
2023-08-02 17:37:07 | INFO | train_inner | epoch 034:    777 / 1474 loss=4.014, trans_loss=5.194, nll_loss=2.453, w2v_ctc_loss=1.148, task_loss=0, contrastive_loss=0.258, total=4082.25, n_correct=2715.81, ppl=5.48, accuracy=66.527, wps=8001.7, ups=1.96, wpb=4082.2, bsz=148, num_updates=49400, lr=6.36285e-05, gnorm=1, clip=0, loss_scale=64, train_wall=51, gb_free=16.8, wall=28356
2023-08-02 17:37:58 | INFO | train_inner | epoch 034:    877 / 1474 loss=4.015, trans_loss=5.191, nll_loss=2.45, w2v_ctc_loss=1.175, task_loss=0, contrastive_loss=0.167, total=4110.66, n_correct=2734.25, ppl=5.47, accuracy=66.516, wps=8009.8, ups=1.95, wpb=4110.7, bsz=148, num_updates=49500, lr=6.35642e-05, gnorm=1.023, clip=0, loss_scale=64, train_wall=51, gb_free=16.3, wall=28408
2023-08-02 17:38:49 | INFO | train_inner | epoch 034:    977 / 1474 loss=4.013, trans_loss=5.186, nll_loss=2.443, w2v_ctc_loss=1.178, task_loss=0, contrastive_loss=0.164, total=4162.47, n_correct=2773.46, ppl=5.44, accuracy=66.63, wps=8128, ups=1.95, wpb=4162.5, bsz=156.2, num_updates=49600, lr=6.35001e-05, gnorm=1.005, clip=0, loss_scale=64, train_wall=51, gb_free=13.7, wall=28459
2023-08-02 17:39:40 | INFO | train_inner | epoch 034:   1077 / 1474 loss=4.011, trans_loss=5.189, nll_loss=2.447, w2v_ctc_loss=1.176, task_loss=0, contrastive_loss=0.128, total=4151.58, n_correct=2762.94, ppl=5.45, accuracy=66.552, wps=8207.9, ups=1.98, wpb=4151.6, bsz=154, num_updates=49700, lr=6.34361e-05, gnorm=1.004, clip=0, loss_scale=64, train_wall=50, gb_free=17.7, wall=28509
2023-08-02 17:40:31 | INFO | train_inner | epoch 034:   1177 / 1474 loss=4.014, trans_loss=5.191, nll_loss=2.45, w2v_ctc_loss=1.175, task_loss=0, contrastive_loss=0.152, total=4091.98, n_correct=2720.45, ppl=5.46, accuracy=66.482, wps=8028.7, ups=1.96, wpb=4092, bsz=148.7, num_updates=49800, lr=6.33724e-05, gnorm=1.016, clip=0, loss_scale=64, train_wall=51, gb_free=17.5, wall=28560
2023-08-02 17:41:21 | INFO | train_inner | epoch 034:   1277 / 1474 loss=4.002, trans_loss=5.183, nll_loss=2.44, w2v_ctc_loss=1.164, task_loss=0, contrastive_loss=0.118, total=4162.83, n_correct=2771.89, ppl=5.42, accuracy=66.587, wps=8234.1, ups=1.98, wpb=4162.8, bsz=150.6, num_updates=49900, lr=6.33089e-05, gnorm=1.005, clip=0, loss_scale=64, train_wall=50, gb_free=16.6, wall=28611
2023-08-02 17:42:13 | INFO | train_inner | epoch 034:   1377 / 1474 loss=4.024, trans_loss=5.193, nll_loss=2.453, w2v_ctc_loss=1.176, task_loss=0, contrastive_loss=0.249, total=4187.24, n_correct=2784.5, ppl=5.48, accuracy=66.5, wps=8156.3, ups=1.95, wpb=4187.2, bsz=160.1, num_updates=50000, lr=6.32456e-05, gnorm=0.995, clip=0, loss_scale=64, train_wall=51, gb_free=17.5, wall=28662
2023-08-02 17:42:13 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2023-08-02 17:42:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-02 17:42:35 | INFO | dev_st | epoch 034 | valid on 'dev_st' subset | loss 4.325 | trans_loss 5.53 | nll_loss 2.798 | w2v_ctc_loss 1.387 | task_loss 0 | contrastive_loss 0.267 | total 4003.4 | n_correct 2503.2 | ppl 6.96 | accuracy 62.527 | uer 16.603 | wer 18.411 | raw_wer 18.411 | bleu 20.49 | wps 2335.1 | wpb 4003.4 | bsz 141.8 | num_updates 50000 | best_bleu 20.76
2023-08-02 17:42:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 50000 updates
2023-08-02 17:42:35 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_34_50000.pt
2023-08-02 17:42:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_34_50000.pt
2023-08-02 17:43:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0801_two_cl/checkpoint_34_50000.pt (epoch 34 @ 50000 updates, score 20.49) (writing took 32.453915510326624 seconds)
2023-08-02 17:43:08 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2023-08-02 17:43:08 | INFO | train | epoch 034 | loss 4.004 | trans_loss 5.179 | nll_loss 2.434 | w2v_ctc_loss 1.164 | task_loss 0 | contrastive_loss 0.184 | total 4131.59 | n_correct 2757.62 | ppl 5.4 | accuracy 66.745 | wps 7410.8 | ups 1.79 | wpb 4131.6 | bsz 151.8 | num_updates 50000 | lr 6.32456e-05 | gnorm 1.005 | clip 0 | loss_scale 64 | train_wall 697 | gb_free 17.5 | wall 28718
2023-08-02 17:43:08 | INFO | fairseq_cli.train | done training in 28658.7 seconds
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-6:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-7:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    raise EOFError
EOFError
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
    raise EOFError
EOFError
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1344 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
