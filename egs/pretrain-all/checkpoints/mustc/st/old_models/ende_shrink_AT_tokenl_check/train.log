2023-07-11 03:57:13 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:11123
2023-07-11 03:57:13 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:11123
2023-07-11 03:57:13 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:11123
2023-07-11 03:57:13 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:11123
2023-07-11 03:57:13 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:11123
2023-07-11 03:57:13 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:11123
2023-07-11 03:57:13 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:11123
2023-07-11 03:57:13 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:11123
2023-07-11 03:57:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 03:57:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 03:57:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 03:57:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 03:57:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 03:57:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 03:57:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 03:57:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 03:57:14 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 03:57:14 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 03:57:14 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 03:57:14 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 03:57:14 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 03:57:14 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 03:57:14 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 03:57:14 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 03:57:14 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 03:57:14 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 03:57:14 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 03:57:14 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 03:57:14 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 03:57:14 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 03:57:14 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 03:57:14 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 03:57:16 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11123', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 03:57:16 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 03:57:16 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 03:57:16 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 03:57:16 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 03:57:16 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 03:57:21 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 03:57:21 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 03:57:21 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 03:57:23 | INFO | root | load pretrained hubert
2023-07-11 03:57:26 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 03:57:26 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 03:57:29 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 03:57:29 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 03:57:29 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 03:57:29 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 03:57:29 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 03:57:29 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 03:57:29 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 03:57:29 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 03:57:29 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 03:57:29 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 03:57:29 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 03:57:29 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 03:57:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 03:57:36 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 03:57:36 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 03:57:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 03:57:37 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 03:57:37 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 03:57:37 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 03:57:37 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 03:57:37 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 03:57:37 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 03:57:37 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 03:57:37 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 03:57:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 03:57:37 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 03:57:37 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 03:57:37 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 03:57:37 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 03:57:37 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 03:57:37 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 03:57:37 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 03:57:37 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 03:57:39 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 03:57:40 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 03:57:42 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 03:58:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 03:58:47 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 03:58:47 | INFO | fairseq_cli.train | Start iterating over samples
### st1:  torch.Size([1264, 512])
### st2:  torch.Size([1264, 512])
### st3,  tensor([], device='cuda:0', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
2023-07-11 03:58:56 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/crash.pt
### st1:  torch.Size([1056, 512])
### st2:  torch.Size([1056, 512])
### st3,  tensor([], device='cuda:2', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
### st1:  torch.Size([1504, 512])
### st2:  torch.Size([1504, 512])
### st3,  tensor([], device='cuda:1', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
### st1:  torch.Size([1416, 512])
### st2:  torch.Size([1415, 512])
### st3,  tensor([[-5.9277e-01, -1.5684e+00,  2.2246e+00,  1.3721e+00, -2.7148e-01,
         -1.0371e+00, -9.4678e-01,  5.6299e-01,  1.8574e+00, -3.0249e-01,
         -3.0029e-01,  8.9941e-01, -1.5068e+00, -1.8740e+00,  1.3557e-02,
          1.4043e+00,  2.3105e+00, -8.9844e-01, -1.2152e-01, -5.0830e-01,
         -5.4962e-02, -8.3350e-01, -1.0146e+00, -2.6172e-01, -2.2354e-02,
         -1.7493e-01,  4.2139e-01,  1.3828e+00,  1.3828e+00, -1.7737e-01,
          2.1558e-01, -1.3389e+00, -5.2148e-01, -5.0830e-01,  2.7881e-01,
         -1.9121e+00,  7.0166e-01, -1.9666e-01,  7.3291e-01,  1.5283e+00,
         -1.4099e-01,  7.1094e-01,  2.4243e-01,  1.7617e+00,  6.8652e-01,
          1.2080e+00,  1.4600e+00,  5.5566e-01, -3.3301e-01, -1.1270e+00,
         -1.3027e-03, -1.4294e-01, -7.8516e-01,  1.9678e-01, -1.2373e+00,
          1.0908e+00, -7.9932e-01,  6.4746e-01, -1.4658e+00, -6.3086e-01,
         -4.5258e-02,  1.4526e-01,  1.4746e+00, -8.7109e-01,  4.7485e-01,
          1.8340e+00, -2.7588e-01,  7.5977e-01,  1.0635e+00,  1.2451e+00,
          1.6768e+00,  9.4678e-01,  4.5020e-01,  6.9678e-01,  1.2627e+00,
         -5.3174e-01, -2.9736e-01,  6.3242e+00,  5.0928e-01,  1.9893e+00,
          2.2129e+00,  6.1963e-01,  5.3857e-01, -2.0959e-01,  3.8965e-01,
         -3.9331e-01, -1.0381e+00, -6.5771e-01,  1.8096e+00,  1.1221e+00,
         -1.0938e+00,  1.2988e+00,  1.0156e+00,  2.1375e-01,  2.9224e-01,
          5.1074e-01, -1.1338e+00, -4.7241e-01,  8.0811e-01,  2.5659e-01,
          1.4697e+00, -6.8359e-01, -1.1523e+00, -2.4573e-01, -1.3513e-01,
          9.6741e-02, -2.6025e-01,  4.0430e-01,  6.8018e-01, -1.8857e+00,
         -5.6299e-01,  3.4424e-01,  1.8420e-01, -6.8604e-01, -1.3110e-01,
         -1.5869e+00,  2.4902e-01, -3.4058e-01, -2.1033e-01, -1.4610e-02,
          7.6074e-01, -1.2188e+00, -1.7542e-01, -7.6074e-01,  8.3008e-01,
         -6.0425e-02, -5.2441e-01, -6.7041e-01,  2.2656e-01, -3.6377e-02,
         -1.1084e+00, -1.1328e+00,  9.8999e-02, -1.0762e+00, -1.2559e+00,
          8.7891e-01, -1.6621e+00, -6.1719e-01,  2.1594e-01, -1.1328e+00,
         -5.9619e-01, -2.1619e-01, -4.0405e-01, -8.6230e-01, -1.1957e-01,
          3.8550e-01, -1.9324e-01, -4.0375e-02, -1.6992e+00, -6.1670e-01,
         -2.5830e-01, -1.7175e-01,  3.6108e-01, -5.0049e-01,  2.6147e-01,
         -2.2498e-01, -4.2627e-01, -3.6865e-01,  2.4796e-02,  3.3960e-01,
          9.5312e-01,  6.8506e-01, -2.8613e-01, -6.2939e-01, -8.0713e-01,
         -8.1250e-01, -9.3115e-01,  2.7075e-01, -2.2852e-01, -1.0020e+00,
          3.1982e-01, -9.2627e-01, -3.5181e-01, -9.3701e-01, -9.5898e-01,
         -4.7754e-01, -6.4014e-01, -6.0156e-01, -8.2227e-01,  4.1602e-01,
          6.4600e-01, -8.7524e-02, -6.6162e-01,  1.2408e-01, -3.3374e-01,
          1.0312e+00, -9.3457e-01,  9.3604e-01,  1.1055e+00, -3.0493e-01,
         -9.8511e-02,  8.1152e-01, -3.4106e-01,  1.0566e+00, -1.0547e-01,
          3.6816e-01, -5.2063e-02, -1.4229e+00,  1.5918e+00,  1.5491e-01,
          1.2432e+00,  1.1250e+00,  6.2695e-01, -4.1309e-01,  1.2634e-01,
          5.2148e-01, -2.5439e-01,  2.9565e-01, -1.7256e+00, -4.6509e-02,
         -2.8833e-01, -2.5684e-01,  1.0957e+00, -8.6523e-01, -3.8916e-01,
         -1.1719e+00, -2.8122e-02, -2.0435e-01, -7.9297e-01, -4.4409e-01,
         -1.5742e+00, -1.8789e+00,  3.8574e-01,  3.7598e-01,  9.1553e-01,
         -2.2278e-01, -6.6455e-01, -5.3076e-01, -5.5615e-01, -2.1602e+00,
          2.9236e-02, -2.8101e-01, -5.9180e-01, -2.7368e-01, -1.3599e-01,
         -1.6016e-01, -4.8779e-01,  1.0225e+00, -2.1387e-01, -2.4438e-01,
          2.0874e-01, -1.3418e+00,  3.1708e-02,  5.9277e-01, -3.9209e-01,
         -9.4922e-01,  9.7705e-01,  1.0547e+00, -1.3098e-01, -1.3750e+00,
         -1.4429e-01, -5.7861e-01,  7.9297e-01,  1.2354e+00, -3.6646e-01,
          8.2153e-02, -1.4404e-01, -1.1543e+00, -1.2598e+00,  1.1064e+00,
          4.3896e-01, -3.7280e-01, -1.1768e+00, -2.1855e+00, -1.5713e+00,
         -6.3184e-01,  2.0625e+00,  6.1084e-01, -4.9365e-01,  3.7134e-01,
         -1.1338e+00, -7.7734e-01, -1.3359e+00,  1.5752e+00,  1.4619e+00,
         -1.2656e+00,  4.4495e-02, -1.3994e+00, -1.1975e-01, -8.2764e-01,
          5.4492e-01, -5.0830e-01, -1.8631e-02, -5.0244e-01,  7.9395e-01,
          1.2732e-01,  7.5830e-01,  1.6768e+00, -2.1045e-01, -5.0391e-01,
          3.5938e-01, -1.1846e+00, -1.3789e+00, -1.5732e+00, -7.2949e-01,
          1.7285e-01, -6.2164e-02, -9.9121e-01, -5.4395e-01, -4.0210e-01,
          7.6611e-01,  1.6426e+00,  7.4512e-01,  1.9316e+00,  1.4902e+00,
          1.5000e+00, -2.6436e-03,  2.2871e+00,  1.6875e+00,  1.1279e+00,
          7.8223e-01, -2.0337e-01, -7.0312e-01, -1.1465e+00, -4.2725e-01,
         -1.3301e+00, -5.2686e-01, -7.4023e-01, -1.2832e+00,  4.7241e-01,
         -1.1162e+00, -1.6992e+00, -7.1953e+00, -3.1875e+00, -1.3926e+00,
         -1.2793e-01, -1.2188e+00,  8.6731e-02,  9.1553e-01, -4.3018e-01,
          2.0667e-01,  7.2363e-01,  2.5000e-01, -1.4912e+00,  4.8608e-01,
          1.1758e+00,  2.1912e-01, -1.6035e+00,  1.3887e+00,  7.7588e-01,
          1.2021e+00,  1.8525e+00, -5.9424e-01,  7.7295e-01,  1.4170e+00,
          2.7734e-01,  9.5166e-01,  1.0577e-01,  1.8125e+00, -5.4053e-01,
          1.0479e+00,  9.5068e-01, -3.9771e-01,  3.3789e-01, -2.1362e-01,
          1.7346e-01,  1.3408e+00, -3.3618e-01, -1.0229e-01,  1.4709e-01,
         -6.5186e-01,  6.1829e-02,  1.9678e-01, -6.0400e-01,  3.0908e-01,
          2.6459e-02,  1.3623e+00,  7.2693e-02, -6.0205e-01,  8.2715e-01,
          1.2051e+00, -3.0933e-01,  1.5039e+00, -8.2227e-01, -6.0254e-01,
         -3.3057e-01,  2.1790e-01, -3.9014e-01,  7.9248e-01, -1.1924e+00,
          2.7368e-01,  6.8848e-01, -7.1387e-01, -1.9861e-01,  8.3105e-01,
         -1.9980e+00,  4.0527e-01, -1.5342e+00, -1.9531e+00, -8.3313e-02,
          1.6785e-01,  1.0137e+00, -3.8208e-01, -1.4046e-02, -1.1201e+00,
          4.8364e-01, -4.6216e-01, -1.7129e+00, -8.7585e-02, -9.7754e-01,
         -3.8428e-01, -3.0298e-01,  6.6846e-01,  1.2715e+00,  1.0742e+00,
         -5.7129e-01, -1.2178e+00,  7.9639e-01, -8.1055e-01,  7.3828e-01,
          1.4832e-01,  8.4375e-01,  1.6279e+00,  1.8030e-01, -5.4248e-01,
         -8.4619e-01, -1.1826e-02,  4.6875e-01, -1.1445e+00,  5.6641e-01,
          3.6230e-01,  8.5254e-01,  5.5469e-01,  2.4072e-01, -3.2178e-01,
          8.1396e-01,  1.4619e+00, -1.4758e-01, -2.7298e-02,  1.6230e+00,
         -9.9915e-02,  2.2009e-01,  1.1108e-01,  1.6309e+00, -1.6885e+00,
          7.4121e-01, -6.2061e-01,  8.9307e-01,  6.1426e-01, -1.0645e+00,
         -2.8809e-01,  3.9868e-01, -3.4058e-01,  9.0771e-01, -5.7251e-02,
          1.8701e+00,  1.2920e+00, -8.5352e-01, -3.3887e-01,  2.0752e-01,
          5.2100e-01,  1.2197e+00, -4.0649e-01,  1.3262e+00, -2.0508e-01,
          2.5781e-01, -1.1221e+00,  9.4824e-01, -1.2461e+00, -4.8340e-01,
          3.7384e-02,  9.2285e-01,  2.2871e+00, -7.3779e-01,  9.9915e-02,
         -1.4346e+00,  7.5830e-01, -7.5195e-01,  4.8657e-01, -6.3525e-01,
         -3.0493e-01,  1.6589e-01,  6.9824e-01,  4.8828e-02, -5.9570e-01,
         -1.1749e-01,  8.1152e-01, -2.3804e-01,  4.1412e-02, -1.1420e-01,
          1.1084e+00, -4.1577e-01,  7.2021e-01, -8.1299e-01, -5.3076e-01,
          2.6929e-01, -2.7295e-01, -6.5332e-01,  4.7534e-01, -1.3105e+00,
          8.1543e-01,  1.0225e+00,  9.3115e-01,  1.5361e+00,  2.4756e-01,
         -9.0039e-01,  3.2715e-01,  1.0586e+00, -3.5303e-01, -5.8545e-01,
         -8.1885e-01,  3.5327e-01,  1.7646e+00, -1.8906e+00,  1.3848e+00,
          5.2002e-01, -1.3660e-01,  3.4204e-01, -3.3295e-02,  1.3398e+00,
         -5.6787e-01, -3.2373e-01]], device='cuda:6', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
### st1:  torch.Size([968, 512])
### st2:  torch.Size([968, 512])
### st3,  tensor([], device='cuda:7', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
### st1:  torch.Size([1600, 512])
### st2:  torch.Size([1600, 512])
### st3,  tensor([], device='cuda:5', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
### st1:  torch.Size([1296, 512])
### st2:  torch.Size([1296, 512])
### st3,  tensor([], device='cuda:4', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
2023-07-11 03:58:57 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/crash.pt
### st1:  torch.Size([1680, 512])
### st2:  torch.Size([1680, 512])
### st3,  tensor([], device='cuda:3', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 506, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 401, in _per_task_train_loss
    loss, sample_size, logging_output = criterion(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT.py", line 278, in forward
    assert False
AssertionError

/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 16 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-07-11 04:06:58 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:14360
2023-07-11 04:06:58 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:14360
2023-07-11 04:06:58 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:14360
2023-07-11 04:06:58 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:14360
2023-07-11 04:06:58 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:14360
2023-07-11 04:06:58 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:14360
2023-07-11 04:06:58 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:14360
2023-07-11 04:06:58 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:14360
2023-07-11 04:06:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 04:06:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 04:06:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 04:06:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 04:06:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 04:06:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 04:06:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 04:06:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 04:06:59 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:06:59 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 04:06:59 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:06:59 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:06:59 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:06:59 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:06:59 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 04:06:59 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 04:06:59 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 04:06:59 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 04:06:59 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:06:59 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 04:06:59 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:06:59 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 04:06:59 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:06:59 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 04:07:01 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14360', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 04:07:02 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 04:07:02 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 04:07:02 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 04:07:02 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 04:07:02 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 04:07:06 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 04:07:06 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 04:07:06 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 04:07:08 | INFO | root | load pretrained hubert
2023-07-11 04:07:11 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 04:07:12 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 04:07:12 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 04:07:12 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 04:07:12 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 04:07:12 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 04:07:12 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 04:07:12 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 04:07:12 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 04:07:12 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 04:07:12 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 04:07:12 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:07:12 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:07:12 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:07:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 04:07:21 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 04:07:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 04:07:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 04:07:22 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:07:22 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:07:22 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:07:22 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:07:22 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:07:22 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:07:22 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:07:22 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:07:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 04:07:22 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 04:07:22 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 04:07:22 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 04:07:22 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 04:07:22 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 04:07:22 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 04:07:22 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:07:22 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:07:24 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:07:26 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:07:28 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:08:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 04:08:32 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 04:08:32 | INFO | fairseq_cli.train | Start iterating over samples
---- 1:  torch.Size([8, 200, 512]) torch.Size([8, 200])
---- 2:  torch.Size([1600, 512]) torch.Size([1600])
---- 3:  torch.Size([1600, 512]) torch.Size([0, 512])
### st1:  torch.Size([1600, 512])
### st2:  torch.Size([1600, 512])
### st3,  tensor([], device='cuda:5', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
---- 1:  torch.Size([16, 81, 512]) torch.Size([16, 81])
---- 2:  torch.Size([1296, 512]) torch.Size([1296])
---- 3:  torch.Size([1296, 512]) torch.Size([0, 512])
### st1:  torch.Size([1296, 512])
### st2:  torch.Size([1296, 512])
### st3,  tensor([], device='cuda:4', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
---- 1:  torch.Size([8, 158, 512]) torch.Size([8, 158])
---- 2:  torch.Size([1264, 512]) torch.Size([1264])
---- 3:  torch.Size([1264, 512]) torch.Size([0, 512])
### st1:  torch.Size([1264, 512])
### st2:  torch.Size([1264, 512])
### st3,  tensor([], device='cuda:0', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
2023-07-11 04:08:41 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/crash.pt
---- 1:  torch.Size([6, 280, 512]) torch.Size([6, 280])
---- 2:  torch.Size([1680, 512]) torch.Size([1680])
---- 3:  torch.Size([1680, 512]) torch.Size([0, 512])
### st1:  torch.Size([1680, 512])
### st2:  torch.Size([1680, 512])
### st3,  tensor([], device='cuda:3', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
---- 1:  torch.Size([16, 94, 512]) torch.Size([16, 94])
---- 2:  torch.Size([1504, 512]) torch.Size([1504])
---- 3:  torch.Size([1504, 512]) torch.Size([0, 512])
### st1:  torch.Size([1504, 512])
### st2:  torch.Size([1504, 512])
### st3,  tensor([], device='cuda:1', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
---- 1:  torch.Size([24, 59, 512]) torch.Size([24, 59])
---- 2:  torch.Size([1416, 512]) torch.Size([1416])
---- 3:  torch.Size([1415, 512]) torch.Size([1, 512])
### st1:  torch.Size([1416, 512])
### st2:  torch.Size([1415, 512])
### st3,  tensor([[-5.9277e-01, -1.5684e+00,  2.2246e+00,  1.3721e+00, -2.7148e-01,
         -1.0371e+00, -9.4678e-01,  5.6299e-01,  1.8574e+00, -3.0249e-01,
         -3.0029e-01,  8.9941e-01, -1.5068e+00, -1.8740e+00,  1.3557e-02,
          1.4043e+00,  2.3105e+00, -8.9844e-01, -1.2152e-01, -5.0830e-01,
         -5.4962e-02, -8.3350e-01, -1.0146e+00, -2.6172e-01, -2.2354e-02,
         -1.7493e-01,  4.2139e-01,  1.3828e+00,  1.3828e+00, -1.7737e-01,
          2.1558e-01, -1.3389e+00, -5.2148e-01, -5.0830e-01,  2.7881e-01,
         -1.9121e+00,  7.0166e-01, -1.9666e-01,  7.3291e-01,  1.5283e+00,
         -1.4099e-01,  7.1094e-01,  2.4243e-01,  1.7617e+00,  6.8652e-01,
          1.2080e+00,  1.4600e+00,  5.5566e-01, -3.3301e-01, -1.1270e+00,
         -1.3027e-03, -1.4294e-01, -7.8516e-01,  1.9678e-01, -1.2373e+00,
          1.0908e+00, -7.9932e-01,  6.4746e-01, -1.4658e+00, -6.3086e-01,
         -4.5258e-02,  1.4526e-01,  1.4746e+00, -8.7109e-01,  4.7485e-01,
          1.8340e+00, -2.7588e-01,  7.5977e-01,  1.0635e+00,  1.2451e+00,
          1.6768e+00,  9.4678e-01,  4.5020e-01,  6.9678e-01,  1.2627e+00,
         -5.3174e-01, -2.9736e-01,  6.3242e+00,  5.0928e-01,  1.9893e+00,
          2.2129e+00,  6.1963e-01,  5.3857e-01, -2.0959e-01,  3.8965e-01,
         -3.9331e-01, -1.0381e+00, -6.5771e-01,  1.8096e+00,  1.1221e+00,
         -1.0938e+00,  1.2988e+00,  1.0156e+00,  2.1375e-01,  2.9224e-01,
          5.1074e-01, -1.1338e+00, -4.7241e-01,  8.0811e-01,  2.5659e-01,
          1.4697e+00, -6.8359e-01, -1.1523e+00, -2.4573e-01, -1.3513e-01,
          9.6741e-02, -2.6025e-01,  4.0430e-01,  6.8018e-01, -1.8857e+00,
         -5.6299e-01,  3.4424e-01,  1.8420e-01, -6.8604e-01, -1.3110e-01,
         -1.5869e+00,  2.4902e-01, -3.4058e-01, -2.1033e-01, -1.4610e-02,
          7.6074e-01, -1.2188e+00, -1.7542e-01, -7.6074e-01,  8.3008e-01,
         -6.0425e-02, -5.2441e-01, -6.7041e-01,  2.2656e-01, -3.6377e-02,
         -1.1084e+00, -1.1328e+00,  9.8999e-02, -1.0762e+00, -1.2559e+00,
          8.7891e-01, -1.6621e+00, -6.1719e-01,  2.1594e-01, -1.1328e+00,
         -5.9619e-01, -2.1619e-01, -4.0405e-01, -8.6230e-01, -1.1957e-01,
          3.8550e-01, -1.9324e-01, -4.0375e-02, -1.6992e+00, -6.1670e-01,
         -2.5830e-01, -1.7175e-01,  3.6108e-01, -5.0049e-01,  2.6147e-01,
         -2.2498e-01, -4.2627e-01, -3.6865e-01,  2.4796e-02,  3.3960e-01,
          9.5312e-01,  6.8506e-01, -2.8613e-01, -6.2939e-01, -8.0713e-01,
         -8.1250e-01, -9.3115e-01,  2.7075e-01, -2.2852e-01, -1.0020e+00,
          3.1982e-01, -9.2627e-01, -3.5181e-01, -9.3701e-01, -9.5898e-01,
         -4.7754e-01, -6.4014e-01, -6.0156e-01, -8.2227e-01,  4.1602e-01,
          6.4600e-01, -8.7524e-02, -6.6162e-01,  1.2408e-01, -3.3374e-01,
          1.0312e+00, -9.3457e-01,  9.3604e-01,  1.1055e+00, -3.0493e-01,
         -9.8511e-02,  8.1152e-01, -3.4106e-01,  1.0566e+00, -1.0547e-01,
          3.6816e-01, -5.2063e-02, -1.4229e+00,  1.5918e+00,  1.5491e-01,
          1.2432e+00,  1.1250e+00,  6.2695e-01, -4.1309e-01,  1.2634e-01,
          5.2148e-01, -2.5439e-01,  2.9565e-01, -1.7256e+00, -4.6509e-02,
         -2.8833e-01, -2.5684e-01,  1.0957e+00, -8.6523e-01, -3.8916e-01,
         -1.1719e+00, -2.8122e-02, -2.0435e-01, -7.9297e-01, -4.4409e-01,
         -1.5742e+00, -1.8789e+00,  3.8574e-01,  3.7598e-01,  9.1553e-01,
         -2.2278e-01, -6.6455e-01, -5.3076e-01, -5.5615e-01, -2.1602e+00,
          2.9236e-02, -2.8101e-01, -5.9180e-01, -2.7368e-01, -1.3599e-01,
         -1.6016e-01, -4.8779e-01,  1.0225e+00, -2.1387e-01, -2.4438e-01,
          2.0874e-01, -1.3418e+00,  3.1708e-02,  5.9277e-01, -3.9209e-01,
         -9.4922e-01,  9.7705e-01,  1.0547e+00, -1.3098e-01, -1.3750e+00,
         -1.4429e-01, -5.7861e-01,  7.9297e-01,  1.2354e+00, -3.6646e-01,
          8.2153e-02, -1.4404e-01, -1.1543e+00, -1.2598e+00,  1.1064e+00,
          4.3896e-01, -3.7280e-01, -1.1768e+00, -2.1855e+00, -1.5713e+00,
         -6.3184e-01,  2.0625e+00,  6.1084e-01, -4.9365e-01,  3.7134e-01,
         -1.1338e+00, -7.7734e-01, -1.3359e+00,  1.5752e+00,  1.4619e+00,
         -1.2656e+00,  4.4495e-02, -1.3994e+00, -1.1975e-01, -8.2764e-01,
          5.4492e-01, -5.0830e-01, -1.8631e-02, -5.0244e-01,  7.9395e-01,
          1.2732e-01,  7.5830e-01,  1.6768e+00, -2.1045e-01, -5.0391e-01,
          3.5938e-01, -1.1846e+00, -1.3789e+00, -1.5732e+00, -7.2949e-01,
          1.7285e-01, -6.2164e-02, -9.9121e-01, -5.4395e-01, -4.0210e-01,
          7.6611e-01,  1.6426e+00,  7.4512e-01,  1.9316e+00,  1.4902e+00,
          1.5000e+00, -2.6436e-03,  2.2871e+00,  1.6875e+00,  1.1279e+00,
          7.8223e-01, -2.0337e-01, -7.0312e-01, -1.1465e+00, -4.2725e-01,
         -1.3301e+00, -5.2686e-01, -7.4023e-01, -1.2832e+00,  4.7241e-01,
         -1.1162e+00, -1.6992e+00, -7.1953e+00, -3.1875e+00, -1.3926e+00,
         -1.2793e-01, -1.2188e+00,  8.6731e-02,  9.1553e-01, -4.3018e-01,
          2.0667e-01,  7.2363e-01,  2.5000e-01, -1.4912e+00,  4.8608e-01,
          1.1758e+00,  2.1912e-01, -1.6035e+00,  1.3887e+00,  7.7588e-01,
          1.2021e+00,  1.8525e+00, -5.9424e-01,  7.7295e-01,  1.4170e+00,
          2.7734e-01,  9.5166e-01,  1.0577e-01,  1.8125e+00, -5.4053e-01,
          1.0479e+00,  9.5068e-01, -3.9771e-01,  3.3789e-01, -2.1362e-01,
          1.7346e-01,  1.3408e+00, -3.3618e-01, -1.0229e-01,  1.4709e-01,
         -6.5186e-01,  6.1829e-02,  1.9678e-01, -6.0400e-01,  3.0908e-01,
          2.6459e-02,  1.3623e+00,  7.2693e-02, -6.0205e-01,  8.2715e-01,
          1.2051e+00, -3.0933e-01,  1.5039e+00, -8.2227e-01, -6.0254e-01,
         -3.3057e-01,  2.1790e-01, -3.9014e-01,  7.9248e-01, -1.1924e+00,
          2.7368e-01,  6.8848e-01, -7.1387e-01, -1.9861e-01,  8.3105e-01,
         -1.9980e+00,  4.0527e-01, -1.5342e+00, -1.9531e+00, -8.3313e-02,
          1.6785e-01,  1.0137e+00, -3.8208e-01, -1.4046e-02, -1.1201e+00,
          4.8364e-01, -4.6216e-01, -1.7129e+00, -8.7585e-02, -9.7754e-01,
         -3.8428e-01, -3.0298e-01,  6.6846e-01,  1.2715e+00,  1.0742e+00,
         -5.7129e-01, -1.2178e+00,  7.9639e-01, -8.1055e-01,  7.3828e-01,
          1.4832e-01,  8.4375e-01,  1.6279e+00,  1.8030e-01, -5.4248e-01,
         -8.4619e-01, -1.1826e-02,  4.6875e-01, -1.1445e+00,  5.6641e-01,
          3.6230e-01,  8.5254e-01,  5.5469e-01,  2.4072e-01, -3.2178e-01,
          8.1396e-01,  1.4619e+00, -1.4758e-01, -2.7298e-02,  1.6230e+00,
         -9.9915e-02,  2.2009e-01,  1.1108e-01,  1.6309e+00, -1.6885e+00,
          7.4121e-01, -6.2061e-01,  8.9307e-01,  6.1426e-01, -1.0645e+00,
         -2.8809e-01,  3.9868e-01, -3.4058e-01,  9.0771e-01, -5.7251e-02,
          1.8701e+00,  1.2920e+00, -8.5352e-01, -3.3887e-01,  2.0752e-01,
          5.2100e-01,  1.2197e+00, -4.0649e-01,  1.3262e+00, -2.0508e-01,
          2.5781e-01, -1.1221e+00,  9.4824e-01, -1.2461e+00, -4.8340e-01,
          3.7384e-02,  9.2285e-01,  2.2871e+00, -7.3779e-01,  9.9915e-02,
         -1.4346e+00,  7.5830e-01, -7.5195e-01,  4.8657e-01, -6.3525e-01,
         -3.0493e-01,  1.6589e-01,  6.9824e-01,  4.8828e-02, -5.9570e-01,
         -1.1749e-01,  8.1152e-01, -2.3804e-01,  4.1412e-02, -1.1420e-01,
          1.1084e+00, -4.1577e-01,  7.2021e-01, -8.1299e-01, -5.3076e-01,
          2.6929e-01, -2.7295e-01, -6.5332e-01,  4.7534e-01, -1.3105e+00,
          8.1543e-01,  1.0225e+00,  9.3115e-01,  1.5361e+00,  2.4756e-01,
         -9.0039e-01,  3.2715e-01,  1.0586e+00, -3.5303e-01, -5.8545e-01,
         -8.1885e-01,  3.5327e-01,  1.7646e+00, -1.8906e+00,  1.3848e+00,
          5.2002e-01, -1.3660e-01,  3.4204e-01, -3.3295e-02,  1.3398e+00,
         -5.6787e-01, -3.2373e-01]], device='cuda:6', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
---- 1:  torch.Size([8, 132, 512]) torch.Size([8, 132])
---- 2:  torch.Size([1056, 512]) torch.Size([1056])
---- 3:  torch.Size([1056, 512]) torch.Size([0, 512])
### st1:  torch.Size([1056, 512])
### st2:  torch.Size([1056, 512])
### st3,  tensor([], device='cuda:2', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
---- 1:  torch.Size([8, 121, 512]) torch.Size([8, 121])
---- 2:  torch.Size([968, 512]) torch.Size([968])
---- 3:  torch.Size([968, 512]) torch.Size([0, 512])
### st1:  torch.Size([968, 512])
### st2:  torch.Size([968, 512])
### st3,  tensor([], device='cuda:7', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 5 terminated with the following error:
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 506, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 401, in _per_task_train_loss
    loss, sample_size, logging_output = criterion(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT.py", line 278, in forward
    assert False
AssertionError

/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-07-11 04:10:11 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:13397
2023-07-11 04:10:11 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:13397
2023-07-11 04:10:11 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:13397
2023-07-11 04:10:11 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:13397
2023-07-11 04:10:11 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:13397
2023-07-11 04:10:11 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:13397
2023-07-11 04:10:11 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:13397
2023-07-11 04:10:11 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:13397
2023-07-11 04:10:11 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 04:10:11 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 04:10:11 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 04:10:12 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 04:10:12 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 04:10:12 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 04:10:12 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 04:10:12 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:10:12 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 04:10:12 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:10:12 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 04:10:12 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 04:10:12 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:10:12 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 04:10:12 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:10:12 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:10:12 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:10:12 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 04:10:12 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 04:10:12 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 04:10:12 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:10:12 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 04:10:12 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:10:12 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 04:10:14 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13397', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 04:10:14 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 04:10:14 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 04:10:14 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 04:10:14 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 04:10:14 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 04:10:19 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 04:10:19 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 04:10:19 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 04:10:21 | INFO | root | load pretrained hubert
2023-07-11 04:10:24 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 04:10:26 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 04:10:29 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 04:10:29 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 04:10:29 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 04:10:29 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 04:10:29 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 04:10:29 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 04:10:29 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 04:10:29 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 04:10:29 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 04:10:29 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:10:29 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:10:29 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:10:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 04:10:35 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 04:10:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 04:10:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 04:10:36 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:10:36 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:10:36 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:10:36 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:10:36 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:10:36 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:10:36 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:10:36 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:10:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 04:10:36 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 04:10:36 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 04:10:36 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 04:10:36 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 04:10:36 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 04:10:36 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 04:10:36 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:10:36 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:10:37 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:10:39 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:10:41 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:11:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 04:11:45 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 04:11:45 | INFO | fairseq_cli.train | Start iterating over samples
---- 1:  torch.Size([8, 121, 512]) torch.Size([8, 121])
---- 2:  torch.Size([968, 512]) torch.Size([968])
---- 3:  torch.Size([968, 512]) torch.Size([0, 512])
---- 4:  tensor([], device='cuda:7', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
### st1:  torch.Size([968, 512])
### st2:  torch.Size([968, 512])
### st3,  tensor([], device='cuda:7', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
---- 1:  torch.Size([8, 158, 512]) torch.Size([8, 158])
---- 2:  torch.Size([1264, 512]) torch.Size([1264])
---- 3:  torch.Size([1264, 512]) torch.Size([0, 512])
---- 4:  tensor([], device='cuda:0', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
### st1:  torch.Size([1264, 512])
### st2:  torch.Size([1264, 512])
### st3,  tensor([], device='cuda:0', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
2023-07-11 04:11:54 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/crash.pt
---- 1:  torch.Size([16, 94, 512]) torch.Size([16, 94])
---- 2:  torch.Size([1504, 512]) torch.Size([1504])
---- 3:  torch.Size([1504, 512]) torch.Size([0, 512])
---- 4:  tensor([], device='cuda:1', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
### st1:  torch.Size([1504, 512])
### st2:  torch.Size([1504, 512])
### st3,  tensor([], device='cuda:1', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
---- 1:  torch.Size([8, 200, 512]) torch.Size([8, 200])
---- 2:  torch.Size([1600, 512]) torch.Size([1600])
---- 3:  torch.Size([1600, 512]) torch.Size([0, 512])
---- 4:  tensor([], device='cuda:5', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
### st1:  torch.Size([1600, 512])
### st2:  torch.Size([1600, 512])
### st3,  tensor([], device='cuda:5', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
---- 1:  torch.Size([24, 59, 512]) torch.Size([24, 59])
---- 2:  torch.Size([1416, 512]) torch.Size([1416])
---- 3:  torch.Size([1415, 512]) torch.Size([1, 512])
---- 4:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:6',
       dtype=torch.float16, grad_fn=<IndexBackward0>)
### st1:  torch.Size([1416, 512])
### st2:  torch.Size([1415, 512])
### st3,  tensor([[-5.9277e-01, -1.5684e+00,  2.2246e+00,  1.3721e+00, -2.7148e-01,
         -1.0371e+00, -9.4678e-01,  5.6299e-01,  1.8574e+00, -3.0249e-01,
         -3.0029e-01,  8.9941e-01, -1.5068e+00, -1.8740e+00,  1.3557e-02,
          1.4043e+00,  2.3105e+00, -8.9844e-01, -1.2152e-01, -5.0830e-01,
         -5.4962e-02, -8.3350e-01, -1.0146e+00, -2.6172e-01, -2.2354e-02,
         -1.7493e-01,  4.2139e-01,  1.3828e+00,  1.3828e+00, -1.7737e-01,
          2.1558e-01, -1.3389e+00, -5.2148e-01, -5.0830e-01,  2.7881e-01,
         -1.9121e+00,  7.0166e-01, -1.9666e-01,  7.3291e-01,  1.5283e+00,
         -1.4099e-01,  7.1094e-01,  2.4243e-01,  1.7617e+00,  6.8652e-01,
          1.2080e+00,  1.4600e+00,  5.5566e-01, -3.3301e-01, -1.1270e+00,
         -1.3027e-03, -1.4294e-01, -7.8516e-01,  1.9678e-01, -1.2373e+00,
          1.0908e+00, -7.9932e-01,  6.4746e-01, -1.4658e+00, -6.3086e-01,
         -4.5258e-02,  1.4526e-01,  1.4746e+00, -8.7109e-01,  4.7485e-01,
          1.8340e+00, -2.7588e-01,  7.5977e-01,  1.0635e+00,  1.2451e+00,
          1.6768e+00,  9.4678e-01,  4.5020e-01,  6.9678e-01,  1.2627e+00,
         -5.3174e-01, -2.9736e-01,  6.3242e+00,  5.0928e-01,  1.9893e+00,
          2.2129e+00,  6.1963e-01,  5.3857e-01, -2.0959e-01,  3.8965e-01,
         -3.9331e-01, -1.0381e+00, -6.5771e-01,  1.8096e+00,  1.1221e+00,
         -1.0938e+00,  1.2988e+00,  1.0156e+00,  2.1375e-01,  2.9224e-01,
          5.1074e-01, -1.1338e+00, -4.7241e-01,  8.0811e-01,  2.5659e-01,
          1.4697e+00, -6.8359e-01, -1.1523e+00, -2.4573e-01, -1.3513e-01,
          9.6741e-02, -2.6025e-01,  4.0430e-01,  6.8018e-01, -1.8857e+00,
         -5.6299e-01,  3.4424e-01,  1.8420e-01, -6.8604e-01, -1.3110e-01,
         -1.5869e+00,  2.4902e-01, -3.4058e-01, -2.1033e-01, -1.4610e-02,
          7.6074e-01, -1.2188e+00, -1.7542e-01, -7.6074e-01,  8.3008e-01,
         -6.0425e-02, -5.2441e-01, -6.7041e-01,  2.2656e-01, -3.6377e-02,
         -1.1084e+00, -1.1328e+00,  9.8999e-02, -1.0762e+00, -1.2559e+00,
          8.7891e-01, -1.6621e+00, -6.1719e-01,  2.1594e-01, -1.1328e+00,
         -5.9619e-01, -2.1619e-01, -4.0405e-01, -8.6230e-01, -1.1957e-01,
          3.8550e-01, -1.9324e-01, -4.0375e-02, -1.6992e+00, -6.1670e-01,
         -2.5830e-01, -1.7175e-01,  3.6108e-01, -5.0049e-01,  2.6147e-01,
         -2.2498e-01, -4.2627e-01, -3.6865e-01,  2.4796e-02,  3.3960e-01,
          9.5312e-01,  6.8506e-01, -2.8613e-01, -6.2939e-01, -8.0713e-01,
         -8.1250e-01, -9.3115e-01,  2.7075e-01, -2.2852e-01, -1.0020e+00,
          3.1982e-01, -9.2627e-01, -3.5181e-01, -9.3701e-01, -9.5898e-01,
         -4.7754e-01, -6.4014e-01, -6.0156e-01, -8.2227e-01,  4.1602e-01,
          6.4600e-01, -8.7524e-02, -6.6162e-01,  1.2408e-01, -3.3374e-01,
          1.0312e+00, -9.3457e-01,  9.3604e-01,  1.1055e+00, -3.0493e-01,
         -9.8511e-02,  8.1152e-01, -3.4106e-01,  1.0566e+00, -1.0547e-01,
          3.6816e-01, -5.2063e-02, -1.4229e+00,  1.5918e+00,  1.5491e-01,
          1.2432e+00,  1.1250e+00,  6.2695e-01, -4.1309e-01,  1.2634e-01,
          5.2148e-01, -2.5439e-01,  2.9565e-01, -1.7256e+00, -4.6509e-02,
         -2.8833e-01, -2.5684e-01,  1.0957e+00, -8.6523e-01, -3.8916e-01,
         -1.1719e+00, -2.8122e-02, -2.0435e-01, -7.9297e-01, -4.4409e-01,
         -1.5742e+00, -1.8789e+00,  3.8574e-01,  3.7598e-01,  9.1553e-01,
         -2.2278e-01, -6.6455e-01, -5.3076e-01, -5.5615e-01, -2.1602e+00,
          2.9236e-02, -2.8101e-01, -5.9180e-01, -2.7368e-01, -1.3599e-01,
         -1.6016e-01, -4.8779e-01,  1.0225e+00, -2.1387e-01, -2.4438e-01,
          2.0874e-01, -1.3418e+00,  3.1708e-02,  5.9277e-01, -3.9209e-01,
         -9.4922e-01,  9.7705e-01,  1.0547e+00, -1.3098e-01, -1.3750e+00,
         -1.4429e-01, -5.7861e-01,  7.9297e-01,  1.2354e+00, -3.6646e-01,
          8.2153e-02, -1.4404e-01, -1.1543e+00, -1.2598e+00,  1.1064e+00,
          4.3896e-01, -3.7280e-01, -1.1768e+00, -2.1855e+00, -1.5713e+00,
         -6.3184e-01,  2.0625e+00,  6.1084e-01, -4.9365e-01,  3.7134e-01,
         -1.1338e+00, -7.7734e-01, -1.3359e+00,  1.5752e+00,  1.4619e+00,
         -1.2656e+00,  4.4495e-02, -1.3994e+00, -1.1975e-01, -8.2764e-01,
          5.4492e-01, -5.0830e-01, -1.8631e-02, -5.0244e-01,  7.9395e-01,
          1.2732e-01,  7.5830e-01,  1.6768e+00, -2.1045e-01, -5.0391e-01,
          3.5938e-01, -1.1846e+00, -1.3789e+00, -1.5732e+00, -7.2949e-01,
          1.7285e-01, -6.2164e-02, -9.9121e-01, -5.4395e-01, -4.0210e-01,
          7.6611e-01,  1.6426e+00,  7.4512e-01,  1.9316e+00,  1.4902e+00,
          1.5000e+00, -2.6436e-03,  2.2871e+00,  1.6875e+00,  1.1279e+00,
          7.8223e-01, -2.0337e-01, -7.0312e-01, -1.1465e+00, -4.2725e-01,
         -1.3301e+00, -5.2686e-01, -7.4023e-01, -1.2832e+00,  4.7241e-01,
         -1.1162e+00, -1.6992e+00, -7.1953e+00, -3.1875e+00, -1.3926e+00,
         -1.2793e-01, -1.2188e+00,  8.6731e-02,  9.1553e-01, -4.3018e-01,
          2.0667e-01,  7.2363e-01,  2.5000e-01, -1.4912e+00,  4.8608e-01,
          1.1758e+00,  2.1912e-01, -1.6035e+00,  1.3887e+00,  7.7588e-01,
          1.2021e+00,  1.8525e+00, -5.9424e-01,  7.7295e-01,  1.4170e+00,
          2.7734e-01,  9.5166e-01,  1.0577e-01,  1.8125e+00, -5.4053e-01,
          1.0479e+00,  9.5068e-01, -3.9771e-01,  3.3789e-01, -2.1362e-01,
          1.7346e-01,  1.3408e+00, -3.3618e-01, -1.0229e-01,  1.4709e-01,
         -6.5186e-01,  6.1829e-02,  1.9678e-01, -6.0400e-01,  3.0908e-01,
          2.6459e-02,  1.3623e+00,  7.2693e-02, -6.0205e-01,  8.2715e-01,
          1.2051e+00, -3.0933e-01,  1.5039e+00, -8.2227e-01, -6.0254e-01,
         -3.3057e-01,  2.1790e-01, -3.9014e-01,  7.9248e-01, -1.1924e+00,
          2.7368e-01,  6.8848e-01, -7.1387e-01, -1.9861e-01,  8.3105e-01,
         -1.9980e+00,  4.0527e-01, -1.5342e+00, -1.9531e+00, -8.3313e-02,
          1.6785e-01,  1.0137e+00, -3.8208e-01, -1.4046e-02, -1.1201e+00,
          4.8364e-01, -4.6216e-01, -1.7129e+00, -8.7585e-02, -9.7754e-01,
         -3.8428e-01, -3.0298e-01,  6.6846e-01,  1.2715e+00,  1.0742e+00,
         -5.7129e-01, -1.2178e+00,  7.9639e-01, -8.1055e-01,  7.3828e-01,
          1.4832e-01,  8.4375e-01,  1.6279e+00,  1.8030e-01, -5.4248e-01,
         -8.4619e-01, -1.1826e-02,  4.6875e-01, -1.1445e+00,  5.6641e-01,
          3.6230e-01,  8.5254e-01,  5.5469e-01,  2.4072e-01, -3.2178e-01,
          8.1396e-01,  1.4619e+00, -1.4758e-01, -2.7298e-02,  1.6230e+00,
         -9.9915e-02,  2.2009e-01,  1.1108e-01,  1.6309e+00, -1.6885e+00,
          7.4121e-01, -6.2061e-01,  8.9307e-01,  6.1426e-01, -1.0645e+00,
         -2.8809e-01,  3.9868e-01, -3.4058e-01,  9.0771e-01, -5.7251e-02,
          1.8701e+00,  1.2920e+00, -8.5352e-01, -3.3887e-01,  2.0752e-01,
          5.2100e-01,  1.2197e+00, -4.0649e-01,  1.3262e+00, -2.0508e-01,
          2.5781e-01, -1.1221e+00,  9.4824e-01, -1.2461e+00, -4.8340e-01,
          3.7384e-02,  9.2285e-01,  2.2871e+00, -7.3779e-01,  9.9915e-02,
         -1.4346e+00,  7.5830e-01, -7.5195e-01,  4.8657e-01, -6.3525e-01,
         -3.0493e-01,  1.6589e-01,  6.9824e-01,  4.8828e-02, -5.9570e-01,
         -1.1749e-01,  8.1152e-01, -2.3804e-01,  4.1412e-02, -1.1420e-01,
          1.1084e+00, -4.1577e-01,  7.2021e-01, -8.1299e-01, -5.3076e-01,
          2.6929e-01, -2.7295e-01, -6.5332e-01,  4.7534e-01, -1.3105e+00,
          8.1543e-01,  1.0225e+00,  9.3115e-01,  1.5361e+00,  2.4756e-01,
         -9.0039e-01,  3.2715e-01,  1.0586e+00, -3.5303e-01, -5.8545e-01,
         -8.1885e-01,  3.5327e-01,  1.7646e+00, -1.8906e+00,  1.3848e+00,
          5.2002e-01, -1.3660e-01,  3.4204e-01, -3.3295e-02,  1.3398e+00,
         -5.6787e-01, -3.2373e-01]], device='cuda:6', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
---- 1:  torch.Size([8, 132, 512]) torch.Size([8, 132])
---- 2:  torch.Size([1056, 512]) torch.Size([1056])
---- 3:  torch.Size([1056, 512]) torch.Size([0, 512])
---- 4:  tensor([], device='cuda:2', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
### st1:  torch.Size([1056, 512])
### st2:  torch.Size([1056, 512])
### st3,  tensor([], device='cuda:2', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
---- 1:  torch.Size([16, 81, 512]) torch.Size([16, 81])
---- 2:  torch.Size([1296, 512]) torch.Size([1296])
---- 3:  torch.Size([1296, 512]) torch.Size([0, 512])
---- 4:  tensor([], device='cuda:4', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
### st1:  torch.Size([1296, 512])
### st2:  torch.Size([1296, 512])
### st3,  tensor([], device='cuda:4', size=(0, 512), dtype=torch.float16,
       grad_fn=<IndexBackward0>)
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 7 terminated with the following error:
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 506, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 401, in _per_task_train_loss
    loss, sample_size, logging_output = criterion(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT.py", line 278, in forward
    assert False
AssertionError

/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 44 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-07-11 04:27:50 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:16036
2023-07-11 04:27:50 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:16036
2023-07-11 04:27:50 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:16036
2023-07-11 04:27:50 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:16036
2023-07-11 04:27:50 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:16036
2023-07-11 04:27:50 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:16036
2023-07-11 04:27:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 04:27:50 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:16036
2023-07-11 04:27:50 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:16036
2023-07-11 04:27:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 04:27:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 04:27:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 04:27:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 04:27:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 04:27:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 04:27:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 04:27:50 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:27:50 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 04:27:50 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:27:50 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 04:27:50 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:27:50 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 04:27:50 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:27:50 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 04:27:50 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:27:50 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 04:27:50 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:27:50 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:27:50 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:27:50 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 04:27:50 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 04:27:50 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 04:27:52 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16036', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 04:27:52 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 04:27:52 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 04:27:52 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 04:27:52 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 04:27:52 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 04:27:57 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 04:27:57 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 04:27:57 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 04:27:58 | INFO | root | load pretrained hubert
2023-07-11 04:28:02 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 04:28:03 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 04:28:05 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 04:28:05 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 04:28:05 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 04:28:05 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 04:28:05 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 04:28:05 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 04:28:05 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 04:28:05 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 04:28:05 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 04:28:05 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:28:05 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:28:05 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:28:12 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 04:28:15 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 04:28:15 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 04:28:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 04:28:16 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:28:16 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:28:16 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:28:16 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:28:16 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:28:16 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:28:16 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:28:16 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:28:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 04:28:16 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 04:28:16 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 04:28:16 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 04:28:16 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 04:28:16 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 04:28:16 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 04:28:16 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:28:16 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:28:17 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:28:19 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:28:21 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:29:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 04:29:26 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 04:29:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-11 04:29:34 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/crash.pt
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 6 terminated with the following error:
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 506, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 401, in _per_task_train_loss
    loss, sample_size, logging_output = criterion(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT.py", line 310, in forward
    assert False
AssertionError

/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-07-11 04:30:21 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:19690
2023-07-11 04:30:21 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:19690
2023-07-11 04:30:21 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:19690
2023-07-11 04:30:21 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:19690
2023-07-11 04:30:21 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:19690
2023-07-11 04:30:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 04:30:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 04:30:21 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:19690
2023-07-11 04:30:21 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:19690
2023-07-11 04:30:21 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:19690
2023-07-11 04:30:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 04:30:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 04:30:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 04:30:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 04:30:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 04:30:21 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:30:21 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 04:30:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 04:30:21 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:30:21 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 04:30:21 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:30:21 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 04:30:21 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:30:21 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 04:30:21 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:30:21 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 04:30:21 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:30:21 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 04:30:21 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:30:21 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 04:30:21 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:30:21 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 04:30:23 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19690', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 04:30:23 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 04:30:23 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 04:30:23 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 04:30:23 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 04:30:23 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 04:30:28 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 04:30:28 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 04:30:28 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 04:30:30 | INFO | root | load pretrained hubert
2023-07-11 04:30:30 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 04:30:32 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 04:30:33 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 04:30:33 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 04:30:33 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 04:30:33 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 04:30:33 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 04:30:33 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 04:30:33 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 04:30:33 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 04:30:33 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 04:30:33 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:30:33 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:30:33 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:30:33 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 04:30:41 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 04:30:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 04:30:42 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 04:30:42 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:30:42 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:30:42 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:30:42 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:30:42 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:30:42 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:30:42 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:30:42 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:30:42 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 04:30:42 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 04:30:42 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 04:30:42 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 04:30:42 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 04:30:42 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 04:30:42 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 04:30:42 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:30:42 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:30:43 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:30:45 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:30:47 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:31:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 04:31:51 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 04:31:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-11 04:32:00 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/crash.pt
2023-07-11 04:32:02 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/crash.pt
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 7 terminated with the following error:
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 506, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 401, in _per_task_train_loss
    loss, sample_size, logging_output = criterion(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT.py", line 341, in forward
    loss = F.binary_cross_entropy_with_logits(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py", line 3163, in binary_cross_entropy_with_logits
    raise ValueError("Target size ({}) must be the same as input size ({})".format(target.size(), input.size()))
ValueError: Target size (torch.Size([2, 1])) must be the same as input size (torch.Size([2]))

/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 16 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-07-11 04:33:44 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:16999
2023-07-11 04:33:44 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:16999
2023-07-11 04:33:44 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:16999
2023-07-11 04:33:44 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:16999
2023-07-11 04:33:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 04:33:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 04:33:44 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:16999
2023-07-11 04:33:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 04:33:44 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:16999
2023-07-11 04:33:44 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:16999
2023-07-11 04:33:44 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:16999
2023-07-11 04:33:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 04:33:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 04:33:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 04:33:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 04:33:44 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:33:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 04:33:44 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:33:44 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 04:33:44 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 04:33:44 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:33:44 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 04:33:44 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:33:44 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 04:33:44 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:33:44 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:33:44 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:33:44 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 04:33:44 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 04:33:44 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 04:33:44 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 04:33:44 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 04:33:46 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16999', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 04:33:46 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 04:33:46 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 04:33:46 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 04:33:46 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 04:33:46 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 04:33:51 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 04:33:51 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 04:33:51 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 04:33:53 | INFO | root | load pretrained hubert
2023-07-11 04:33:56 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 04:33:57 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 04:34:01 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 04:34:01 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 04:34:01 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 04:34:01 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 04:34:01 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 04:34:01 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 04:34:01 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 04:34:01 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 04:34:01 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 04:34:01 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:34:01 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:34:01 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:34:08 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 04:34:09 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 04:34:09 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 04:34:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 04:34:09 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:34:09 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:34:09 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:34:09 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:34:09 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:34:09 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:34:09 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:34:09 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 04:34:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 04:34:09 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 04:34:09 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 04:34:09 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 04:34:09 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 04:34:09 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 04:34:09 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 04:34:09 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:34:09 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 04:34:11 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:34:13 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:34:15 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 04:35:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 04:35:19 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 04:35:19 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-11 04:35:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-11 04:36:32 | INFO | train_inner | epoch 001:    101 / 1474 loss=8.858, trans_loss=5.64, nll_loss=4.216, w2v_ctc_loss=9.594, task_loss=1.761, contrastive_loss=3.31, total=4200.41, n_correct=212.24, ppl=18.58, accuracy=5.053, wps=20187.7, ups=1.61, wpb=12542.9, bsz=467.5, num_updates=100, lr=4.098e-06, gnorm=0.398, clip=0, loss_scale=64, train_wall=65, gb_free=19, wall=143
2023-07-11 04:37:34 | INFO | train_inner | epoch 001:    201 / 1474 loss=7.92, trans_loss=5.438, nll_loss=4.023, w2v_ctc_loss=8.265, task_loss=1.735, contrastive_loss=3.286, total=4127.38, n_correct=251.25, ppl=16.26, accuracy=6.087, wps=20050.4, ups=1.63, wpb=12327, bsz=463.1, num_updates=200, lr=8.096e-06, gnorm=1.55, clip=0, loss_scale=64, train_wall=61, gb_free=19.2, wall=204
2023-07-11 04:38:35 | INFO | train_inner | epoch 001:    301 / 1474 loss=4.941, trans_loss=5.405, nll_loss=4.04, w2v_ctc_loss=3.742, task_loss=1.819, contrastive_loss=3.201, total=4079.62, n_correct=249.92, ppl=16.45, accuracy=6.126, wps=19859.5, ups=1.63, wpb=12195.4, bsz=438.2, num_updates=300, lr=1.2094e-05, gnorm=1.974, clip=0, loss_scale=64, train_wall=61, gb_free=19.9, wall=266
2023-07-11 04:39:37 | INFO | train_inner | epoch 001:    401 / 1474 loss=4.425, trans_loss=5.453, nll_loss=4.121, w2v_ctc_loss=2.909, task_loss=1.571, contrastive_loss=3.236, total=4174.14, n_correct=233.08, ppl=17.4, accuracy=5.584, wps=20347.4, ups=1.63, wpb=12461.1, bsz=460.4, num_updates=400, lr=1.6092e-05, gnorm=1.279, clip=0, loss_scale=64, train_wall=61, gb_free=18.9, wall=327
2023-07-11 04:40:39 | INFO | train_inner | epoch 001:    501 / 1474 loss=4.238, trans_loss=5.453, nll_loss=4.129, w2v_ctc_loss=2.629, task_loss=1.423, contrastive_loss=3.23, total=4176.18, n_correct=217.8, ppl=17.49, accuracy=5.215, wps=20042.3, ups=1.6, wpb=12513, bsz=477.4, num_updates=500, lr=2.009e-05, gnorm=0.611, clip=0, loss_scale=64, train_wall=62, gb_free=19.2, wall=390
2023-07-11 04:41:40 | INFO | train_inner | epoch 001:    601 / 1474 loss=4.146, trans_loss=5.482, nll_loss=4.171, w2v_ctc_loss=2.485, task_loss=1.335, contrastive_loss=3.282, total=4147.79, n_correct=213.94, ppl=18.01, accuracy=5.158, wps=20173.9, ups=1.63, wpb=12368.1, bsz=484.2, num_updates=600, lr=2.4088e-05, gnorm=0.315, clip=0, loss_scale=64, train_wall=61, gb_free=18.9, wall=451
2023-07-11 04:42:41 | INFO | train_inner | epoch 001:    701 / 1474 loss=4.062, trans_loss=5.469, nll_loss=4.162, w2v_ctc_loss=2.428, task_loss=1.365, contrastive_loss=3.031, total=4152.1, n_correct=230.01, ppl=17.91, accuracy=5.54, wps=20387.5, ups=1.65, wpb=12373.8, bsz=456.2, num_updates=700, lr=2.8086e-05, gnorm=0.258, clip=0, loss_scale=64, train_wall=60, gb_free=19.5, wall=512
2023-07-11 04:43:42 | INFO | train_inner | epoch 001:    801 / 1474 loss=3.953, trans_loss=5.423, nll_loss=4.109, w2v_ctc_loss=2.334, task_loss=1.3, contrastive_loss=2.93, total=4123.83, n_correct=254.99, ppl=17.25, accuracy=6.183, wps=20136.1, ups=1.64, wpb=12311.9, bsz=464.1, num_updates=800, lr=3.2084e-05, gnorm=0.361, clip=0, loss_scale=64, train_wall=61, gb_free=19.1, wall=573
2023-07-11 04:44:43 | INFO | train_inner | epoch 001:    901 / 1474 loss=3.846, trans_loss=5.404, nll_loss=4.101, w2v_ctc_loss=2.263, task_loss=1.31, contrastive_loss=2.675, total=4163.61, n_correct=280.21, ppl=17.16, accuracy=6.73, wps=20429.9, ups=1.65, wpb=12409.7, bsz=457.1, num_updates=900, lr=3.6082e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=60, gb_free=18.8, wall=633
2023-07-11 04:45:44 | INFO | train_inner | epoch 001:   1001 / 1474 loss=3.725, trans_loss=5.386, nll_loss=4.083, w2v_ctc_loss=2.16, task_loss=1.316, contrastive_loss=2.533, total=4135.34, n_correct=299.99, ppl=16.95, accuracy=7.254, wps=20094.7, ups=1.63, wpb=12363.9, bsz=456.8, num_updates=1000, lr=4.008e-05, gnorm=0.635, clip=0, loss_scale=64, train_wall=61, gb_free=19, wall=695
2023-07-11 04:46:45 | INFO | train_inner | epoch 001:   1101 / 1474 loss=3.621, trans_loss=5.379, nll_loss=4.077, w2v_ctc_loss=2.081, task_loss=1.324, contrastive_loss=2.315, total=4147.38, n_correct=315.96, ppl=16.88, accuracy=7.618, wps=20362.8, ups=1.65, wpb=12356.9, bsz=454.9, num_updates=1100, lr=4.4078e-05, gnorm=0.736, clip=0, loss_scale=64, train_wall=60, gb_free=18.8, wall=756
2023-07-11 04:47:46 | INFO | train_inner | epoch 001:   1201 / 1474 loss=3.508, trans_loss=5.359, nll_loss=4.056, w2v_ctc_loss=1.996, task_loss=1.37, contrastive_loss=2.102, total=4139.9, n_correct=328.42, ppl=16.64, accuracy=7.933, wps=20249.8, ups=1.63, wpb=12386.7, bsz=440.1, num_updates=1200, lr=4.8076e-05, gnorm=0.718, clip=0, loss_scale=64, train_wall=61, gb_free=18.9, wall=817
2023-07-11 04:48:47 | INFO | train_inner | epoch 001:   1301 / 1474 loss=3.417, trans_loss=5.362, nll_loss=4.062, w2v_ctc_loss=1.915, task_loss=1.313, contrastive_loss=1.922, total=4046.58, n_correct=319.86, ppl=16.71, accuracy=7.904, wps=19938.5, ups=1.65, wpb=12092.5, bsz=439.3, num_updates=1300, lr=5.2074e-05, gnorm=0.734, clip=0, loss_scale=64, train_wall=60, gb_free=19.7, wall=877
2023-07-11 04:49:48 | INFO | train_inner | epoch 001:   1401 / 1474 loss=3.36, trans_loss=5.358, nll_loss=4.069, w2v_ctc_loss=1.849, task_loss=1.29, contrastive_loss=2.006, total=4133.18, n_correct=330.6, ppl=16.79, accuracy=7.999, wps=20276.9, ups=1.65, wpb=12300.7, bsz=455, num_updates=1400, lr=5.6072e-05, gnorm=0.806, clip=0, loss_scale=64, train_wall=60, gb_free=19.9, wall=938
2023-07-11 04:50:32 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2023-07-11 04:51:06 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 2.561 | trans_loss 10.96 | nll_loss 9.952 | w2v_ctc_loss 1.497 | task_loss 7.416 | contrastive_loss 2.325 | total 4003.4 | n_correct 372.8 | ppl 990.57 | accuracy 9.312 | uer 71.484 | wer 69.464 | raw_wer 69.464 | bleu 0.03 | wps 1458.2 | wpb 4003.4 | bsz 141.8 | num_updates 1473
2023-07-11 04:51:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1473 updates
2023-07-11 04:51:06 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_best.pt
2023-07-11 04:51:09 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_best.pt
2023-07-11 04:51:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_best.pt (epoch 1 @ 1473 updates, score 0.03) (writing took 5.29084413399687 seconds)
2023-07-11 04:51:11 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-07-11 04:51:11 | INFO | train | epoch 001 | loss 4.513 | trans_loss 5.426 | nll_loss 4.099 | w2v_ctc_loss 3.261 | task_loss 1.435 | contrastive_loss 2.749 | total 4138.32 | n_correct 270.636 | ppl 17.14 | accuracy 6.54 | wps 19341.9 | ups 1.57 | wpb 12354.6 | bsz 458.3 | num_updates 1473 | lr 5.89905e-05 | gnorm 0.773 | clip 0 | loss_scale 64 | train_wall 898 | gb_free 19.2 | wall 1022
2023-07-11 04:51:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 04:51:11 | INFO | fairseq.trainer | begin training epoch 2
2023-07-11 04:51:11 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-11 04:51:36 | INFO | train_inner | epoch 002:     27 / 1474 loss=3.277, trans_loss=5.352, nll_loss=4.052, w2v_ctc_loss=1.763, task_loss=1.229, contrastive_loss=1.838, total=4162.95, n_correct=340.55, ppl=16.59, accuracy=8.18, wps=11478.7, ups=0.92, wpb=12414, bsz=470.8, num_updates=1500, lr=6.007e-05, gnorm=0.682, clip=0, loss_scale=64, train_wall=61, gb_free=19.6, wall=1046
2023-07-11 04:52:37 | INFO | train_inner | epoch 002:    127 / 1474 loss=3.197, trans_loss=5.353, nll_loss=4.054, w2v_ctc_loss=1.706, task_loss=1.313, contrastive_loss=1.637, total=4155.98, n_correct=340.12, ppl=16.6, accuracy=8.184, wps=20258.6, ups=1.64, wpb=12390, bsz=451.6, num_updates=1600, lr=6.4068e-05, gnorm=0.694, clip=0, loss_scale=64, train_wall=61, gb_free=18.9, wall=1107
2023-07-11 04:53:38 | INFO | train_inner | epoch 002:    227 / 1474 loss=3.132, trans_loss=5.324, nll_loss=4.022, w2v_ctc_loss=1.626, task_loss=1.137, contrastive_loss=1.667, total=4179.21, n_correct=349.19, ppl=16.24, accuracy=8.355, wps=20305.9, ups=1.63, wpb=12488.1, bsz=488.9, num_updates=1700, lr=6.8066e-05, gnorm=0.639, clip=0, loss_scale=64, train_wall=61, gb_free=19, wall=1169
2023-07-11 04:54:40 | INFO | train_inner | epoch 002:    327 / 1474 loss=3.056, trans_loss=5.326, nll_loss=4.02, w2v_ctc_loss=1.58, task_loss=1.307, contrastive_loss=1.378, total=4146.1, n_correct=352.04, ppl=16.23, accuracy=8.491, wps=20163.7, ups=1.63, wpb=12381, bsz=447.8, num_updates=1800, lr=7.2064e-05, gnorm=0.603, clip=0, loss_scale=64, train_wall=61, gb_free=18.8, wall=1230
2023-07-11 04:55:41 | INFO | train_inner | epoch 002:    427 / 1474 loss=2.984, trans_loss=5.315, nll_loss=4.01, w2v_ctc_loss=1.532, task_loss=1.435, contrastive_loss=1.2, total=4037.99, n_correct=343.98, ppl=16.11, accuracy=8.519, wps=19871.2, ups=1.65, wpb=12079.4, bsz=415.4, num_updates=1900, lr=7.6062e-05, gnorm=0.606, clip=0, loss_scale=64, train_wall=60, gb_free=18.9, wall=1291
2023-07-11 04:56:41 | INFO | train_inner | epoch 002:    527 / 1474 loss=2.964, trans_loss=5.312, nll_loss=4.002, w2v_ctc_loss=1.476, task_loss=1.247, contrastive_loss=1.305, total=4176.97, n_correct=363.51, ppl=16.02, accuracy=8.703, wps=20516.9, ups=1.64, wpb=12475.1, bsz=468.6, num_updates=2000, lr=8.006e-05, gnorm=0.588, clip=0, loss_scale=64, train_wall=60, gb_free=19.6, wall=1352
2023-07-11 04:56:41 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-11 04:57:15 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 2.404 | trans_loss 10.817 | nll_loss 9.766 | w2v_ctc_loss 1.208 | task_loss 7.415 | contrastive_loss 1.647 | total 4003.4 | n_correct 397.4 | ppl 870.89 | accuracy 9.927 | uer 61.766 | wer 59.379 | raw_wer 59.379 | bleu 0.03 | wps 1450.4 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_bleu 0.03
2023-07-11 04:57:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-07-11 04:57:15 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_2_2000.pt
2023-07-11 04:57:18 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_2_2000.pt
2023-07-11 04:57:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.03) (writing took 9.225600878999103 seconds)
2023-07-11 04:58:25 | INFO | train_inner | epoch 002:    627 / 1474 loss=2.893, trans_loss=5.305, nll_loss=3.997, w2v_ctc_loss=1.426, task_loss=1.292, contrastive_loss=1.092, total=4126.49, n_correct=361.55, ppl=15.97, accuracy=8.762, wps=11843.1, ups=0.96, wpb=12299.1, bsz=445.5, num_updates=2100, lr=8.4058e-05, gnorm=0.489, clip=0, loss_scale=128, train_wall=60, gb_free=19.2, wall=1456
2023-07-11 04:59:26 | INFO | train_inner | epoch 002:    727 / 1474 loss=2.867, trans_loss=5.287, nll_loss=3.979, w2v_ctc_loss=1.391, task_loss=1.264, contrastive_loss=1.198, total=4149.06, n_correct=370.51, ppl=15.77, accuracy=8.93, wps=20411.5, ups=1.65, wpb=12372.9, bsz=465.4, num_updates=2200, lr=8.8056e-05, gnorm=0.494, clip=0, loss_scale=128, train_wall=60, gb_free=19.2, wall=1516
2023-07-11 05:00:27 | INFO | train_inner | epoch 002:    827 / 1474 loss=2.836, trans_loss=5.27, nll_loss=3.956, w2v_ctc_loss=1.364, task_loss=1.298, contrastive_loss=1.144, total=4175.4, n_correct=380.85, ppl=15.52, accuracy=9.121, wps=20316.7, ups=1.63, wpb=12477.6, bsz=460.9, num_updates=2300, lr=9.2054e-05, gnorm=0.437, clip=0, loss_scale=128, train_wall=61, gb_free=19.8, wall=1578
2023-07-11 05:01:28 | INFO | train_inner | epoch 002:    927 / 1474 loss=2.791, trans_loss=5.259, nll_loss=3.943, w2v_ctc_loss=1.321, task_loss=1.324, contrastive_loss=1.126, total=4104.2, n_correct=378.06, ppl=15.38, accuracy=9.212, wps=20211, ups=1.65, wpb=12243.8, bsz=445.9, num_updates=2400, lr=9.6052e-05, gnorm=0.428, clip=0, loss_scale=128, train_wall=60, gb_free=19, wall=1638
2023-07-11 05:02:28 | INFO | train_inner | epoch 002:   1027 / 1474 loss=2.758, trans_loss=5.255, nll_loss=3.937, w2v_ctc_loss=1.289, task_loss=1.287, contrastive_loss=0.982, total=4102.5, n_correct=379.72, ppl=15.31, accuracy=9.256, wps=20255.8, ups=1.65, wpb=12270.3, bsz=456.3, num_updates=2500, lr=0.00010005, gnorm=0.391, clip=0, loss_scale=128, train_wall=60, gb_free=19.2, wall=1699
2023-07-11 05:03:30 | INFO | train_inner | epoch 002:   1127 / 1474 loss=2.755, trans_loss=5.245, nll_loss=3.926, w2v_ctc_loss=1.262, task_loss=1.169, contrastive_loss=1.194, total=4187.61, n_correct=396.79, ppl=15.2, accuracy=9.475, wps=20400.8, ups=1.63, wpb=12484.3, bsz=487.1, num_updates=2600, lr=0.000104048, gnorm=0.358, clip=0, loss_scale=128, train_wall=61, gb_free=19.5, wall=1760
2023-07-11 05:04:31 | INFO | train_inner | epoch 002:   1227 / 1474 loss=2.725, trans_loss=5.238, nll_loss=3.917, w2v_ctc_loss=1.239, task_loss=1.178, contrastive_loss=1.116, total=4221.06, n_correct=406.25, ppl=15.1, accuracy=9.624, wps=20497.3, ups=1.63, wpb=12590.1, bsz=492.8, num_updates=2700, lr=0.000108046, gnorm=0.353, clip=0, loss_scale=128, train_wall=61, gb_free=19.4, wall=1822
2023-07-11 05:06:31 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:13193
2023-07-11 05:06:31 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:13193
2023-07-11 05:06:31 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:13193
2023-07-11 05:06:31 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:13193
2023-07-11 05:06:31 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:13193
2023-07-11 05:06:31 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:13193
2023-07-11 05:06:31 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:13193
2023-07-11 05:06:31 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:13193
2023-07-11 05:06:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 05:06:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 05:06:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 05:06:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 05:06:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 05:06:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 05:06:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 05:06:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 05:06:32 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 05:06:32 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 05:06:32 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 05:06:32 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 05:06:32 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 05:06:32 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 05:06:32 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 05:06:32 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 05:06:32 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 05:06:32 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 05:06:32 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 05:06:32 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 05:06:32 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 05:06:32 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 05:06:32 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 05:06:32 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 05:06:34 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13193', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 05:06:34 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 05:06:34 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 05:06:34 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 05:06:34 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 05:06:34 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 05:06:39 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 05:06:39 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 05:06:39 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 05:06:40 | INFO | root | load pretrained hubert
2023-07-11 05:06:44 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 05:06:44 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 05:06:47 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 05:06:47 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 05:06:47 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 05:06:47 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 05:06:47 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 05:06:47 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 05:06:47 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 05:06:47 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 05:06:47 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 05:06:47 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 05:06:47 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 05:06:47 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 05:06:51 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 05:06:55 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 05:06:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 05:06:55 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 05:06:55 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 05:06:55 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 05:06:55 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 05:06:55 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 05:06:55 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 05:06:55 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 05:06:55 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 05:06:55 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 05:06:55 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 05:06:55 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 05:06:55 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 05:06:55 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 05:06:55 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 05:06:55 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 05:06:55 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 05:06:55 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 05:06:55 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 05:06:57 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 05:06:59 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 05:07:00 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 05:08:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 05:08:06 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 05:08:06 | INFO | fairseq_cli.train | Start iterating over samples
torch.Size([59, 24, 512])
torch.Size([280, 6, 512])
torch.Size([132, 8, 512])
torch.Size([94, 16, 512])
torch.Size([200, 8, 512])
torch.Size([81, 16, 512])
torch.Size([121, 8, 512])
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 6 terminated with the following error:
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 506, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 401, in _per_task_train_loss
    loss, sample_size, logging_output = criterion(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT.py", line 261, in forward
    assert False
AssertionError

/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 30, in <module>
    from fairseq import checkpoint_utils, options, quantization_utils, tasks, utils
  File "/workspace/fairseq-AT/fairseq/__init__.py", line 32, in <module>
    import fairseq.criterions  # noqa
  File "/workspace/fairseq-AT/fairseq/criterions/__init__.py", line 35, in <module>
    importlib.import_module("fairseq.criterions." + file_name)
  File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/workspace/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT.py", line 332
    loss = bce_loss if loss is None else loss += bce_loss
                                              ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 30, in <module>
    from fairseq import checkpoint_utils, options, quantization_utils, tasks, utils
  File "/workspace/fairseq-AT/fairseq/__init__.py", line 32, in <module>
    import fairseq.criterions  # noqa
  File "/workspace/fairseq-AT/fairseq/criterions/__init__.py", line 35, in <module>
    importlib.import_module("fairseq.criterions." + file_name)
  File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/workspace/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT.py", line 332
    loss = bce_loss if loss is None else loss += bce_loss
                                              ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 30, in <module>
    from fairseq import checkpoint_utils, options, quantization_utils, tasks, utils
  File "/workspace/fairseq-AT/fairseq/__init__.py", line 32, in <module>
    import fairseq.criterions  # noqa
  File "/workspace/fairseq-AT/fairseq/criterions/__init__.py", line 35, in <module>
    importlib.import_module("fairseq.criterions." + file_name)
  File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/workspace/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT.py", line 332
    loss = bce_loss if loss is None else (loss += bce_loss)
                                               ^
SyntaxError: invalid syntax
2023-07-11 06:43:13 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:10694
2023-07-11 06:43:13 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:10694
2023-07-11 06:43:13 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:10694
2023-07-11 06:43:13 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:10694
2023-07-11 06:43:13 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:10694
2023-07-11 06:43:13 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:10694
2023-07-11 06:43:13 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:10694
2023-07-11 06:43:13 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:10694
2023-07-11 06:43:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 06:43:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 06:43:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 06:43:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 06:43:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 06:43:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 06:43:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 06:43:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 06:43:13 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 06:43:13 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 06:43:13 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 06:43:13 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 06:43:13 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 06:43:13 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 06:43:13 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 06:43:13 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 06:43:13 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 06:43:13 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 06:43:13 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 06:43:13 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 06:43:13 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 06:43:13 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 06:43:13 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 06:43:13 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 06:43:15 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10694', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 06:43:15 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 06:43:15 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 06:43:15 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 06:43:15 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 06:43:15 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 06:43:20 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 06:43:20 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 06:43:20 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 06:43:21 | INFO | root | load pretrained hubert
2023-07-11 06:43:22 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 06:43:24 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 06:43:25 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 06:43:25 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 06:43:25 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 06:43:25 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 06:43:25 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 06:43:25 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 06:43:25 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 06:43:25 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 06:43:25 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 06:43:25 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 06:43:25 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 06:43:25 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 06:43:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 06:43:35 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 06:43:35 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 06:43:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 06:43:36 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 06:43:36 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 06:43:36 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 06:43:36 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 06:43:36 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 06:43:36 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 06:43:36 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 06:43:36 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 06:43:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 06:43:36 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 06:43:36 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 06:43:36 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 06:43:36 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 06:43:36 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 06:43:36 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 06:43:36 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 06:43:36 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 06:43:38 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 06:43:39 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 06:43:41 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 06:44:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 06:44:45 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 06:44:45 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-11 06:44:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-11 06:45:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-07-11 06:46:28 | INFO | train_inner | epoch 001:    102 / 1474 loss=8.828, trans_loss=5.626, nll_loss=4.214, w2v_ctc_loss=9.557, task_loss=1.816, contrastive_loss=3.307, total=4208.4, n_correct=196.6, ppl=18.56, accuracy=4.672, wps=13843, ups=1.1, wpb=12567.1, bsz=469.7, num_updates=100, lr=4.098e-06, gnorm=0.646, clip=0, loss_scale=32, train_wall=95, gb_free=16.4, wall=172
2023-07-11 06:47:56 | INFO | train_inner | epoch 001:    202 / 1474 loss=6.226, trans_loss=5.657, nll_loss=4.326, w2v_ctc_loss=5.393, task_loss=1.764, contrastive_loss=3.285, total=4125.46, n_correct=147.61, ppl=20.05, accuracy=3.578, wps=14012.5, ups=1.14, wpb=12321.8, bsz=461.7, num_updates=200, lr=8.096e-06, gnorm=3.257, clip=0, loss_scale=32, train_wall=87, gb_free=13.9, wall=260
2023-07-11 06:49:24 | INFO | train_inner | epoch 001:    302 / 1474 loss=4.942, trans_loss=5.577, nll_loss=4.274, w2v_ctc_loss=3.348, task_loss=1.768, contrastive_loss=3.204, total=4077.62, n_correct=183.29, ppl=19.35, accuracy=4.495, wps=13856.8, ups=1.14, wpb=12184.2, bsz=437.4, num_updates=300, lr=1.2094e-05, gnorm=2.516, clip=0, loss_scale=32, train_wall=87, gb_free=17.4, wall=348
2023-07-11 06:50:51 | INFO | train_inner | epoch 001:    402 / 1474 loss=4.937, trans_loss=5.568, nll_loss=4.273, w2v_ctc_loss=3.08, task_loss=1.259, contrastive_loss=3.235, total=4177.45, n_correct=117.45, ppl=19.34, accuracy=2.812, wps=14309, ups=1.15, wpb=12472.6, bsz=462.8, num_updates=400, lr=1.6092e-05, gnorm=1.767, clip=0, loss_scale=32, train_wall=87, gb_free=16.4, wall=435
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 1 terminated with signal SIGTERM
/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 128 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-07-11 06:53:57 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:14978
2023-07-11 06:53:57 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:14978
2023-07-11 06:53:57 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:14978
2023-07-11 06:53:57 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:14978
2023-07-11 06:53:57 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:14978
2023-07-11 06:53:57 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:14978
2023-07-11 06:53:57 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:14978
2023-07-11 06:53:57 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:14978
2023-07-11 06:53:57 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 06:53:57 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 06:53:57 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 06:53:57 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 06:53:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 06:53:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 06:53:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 06:53:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 06:53:58 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 06:53:58 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 06:53:58 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 06:53:58 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 06:53:58 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 06:53:58 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 06:53:58 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 06:53:58 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 06:53:58 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 06:53:58 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 06:53:58 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 06:53:58 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 06:53:58 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 06:53:58 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 06:53:58 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 06:53:58 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 06:54:00 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14978', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 06:54:00 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 06:54:00 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 06:54:00 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 06:54:00 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 06:54:00 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 06:54:05 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 06:54:05 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 06:54:05 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 06:54:06 | INFO | root | load pretrained hubert
2023-07-11 06:54:10 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 06:54:10 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 06:54:14 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 06:54:14 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 06:54:14 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 06:54:14 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 06:54:14 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 06:54:14 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 06:54:14 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 06:54:14 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 06:54:14 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 06:54:14 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 06:54:14 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 06:54:14 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 06:54:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 06:54:21 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 06:54:21 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 06:54:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 06:54:22 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 06:54:22 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 06:54:22 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 06:54:22 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 06:54:22 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 06:54:22 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 06:54:22 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 06:54:22 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 06:54:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 06:54:22 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 06:54:22 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 06:54:22 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 06:54:22 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 06:54:22 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 06:54:22 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 06:54:22 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 06:54:22 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 06:54:23 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 06:54:25 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 06:54:27 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 06:55:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 06:55:32 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 06:55:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-11 06:55:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-11 06:55:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-07-11 06:57:12 | INFO | train_inner | epoch 001:    102 / 1474 loss=8.828, trans_loss=5.626, nll_loss=4.214, w2v_ctc_loss=9.557, task_loss=1.816, contrastive_loss=3.307, total=4208.4, n_correct=196.79, ppl=18.56, accuracy=4.676, wps=14009.5, ups=1.12, wpb=12567.1, bsz=469.7, num_updates=100, lr=4.098e-06, gnorm=0.646, clip=0, loss_scale=32, train_wall=92, gb_free=16.4, wall=170
2023-07-11 06:58:39 | INFO | train_inner | epoch 001:    202 / 1474 loss=6.226, trans_loss=5.657, nll_loss=4.326, w2v_ctc_loss=5.393, task_loss=1.764, contrastive_loss=3.285, total=4125.46, n_correct=147.07, ppl=20.05, accuracy=3.565, wps=14072.4, ups=1.14, wpb=12321.8, bsz=461.7, num_updates=200, lr=8.096e-06, gnorm=3.257, clip=0, loss_scale=32, train_wall=87, gb_free=13.9, wall=258
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 1 terminated with signal SIGTERM
/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 128 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-07-11 07:02:28 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:19052
2023-07-11 07:02:28 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:19052
2023-07-11 07:02:28 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:19052
2023-07-11 07:02:28 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:19052
2023-07-11 07:02:28 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:19052
2023-07-11 07:02:28 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:19052
2023-07-11 07:02:28 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:19052
2023-07-11 07:02:28 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:19052
2023-07-11 07:02:28 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 07:02:28 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 07:02:28 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 07:02:29 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 07:02:29 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 07:02:29 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 07:02:29 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 07:02:29 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 07:02:29 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 07:02:29 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 07:02:29 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 07:02:29 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 07:02:29 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 07:02:29 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 07:02:29 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 07:02:29 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 07:02:29 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 07:02:29 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 07:02:29 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 07:02:29 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 07:02:29 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 07:02:29 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 07:02:29 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 07:02:29 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 07:02:31 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19052', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='token', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 07:02:31 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 07:02:31 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 07:02:31 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 07:02:31 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 07:02:31 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 07:02:36 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 07:02:36 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 07:02:36 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 07:02:38 | INFO | root | load pretrained hubert
2023-07-11 07:02:42 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 07:02:43 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 07:02:46 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 07:02:46 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 07:02:46 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 07:02:46 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 07:02:46 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 07:02:46 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 07:02:46 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 07:02:46 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 07:02:46 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 07:02:46 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 07:02:46 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 07:02:46 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 07:02:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 07:02:52 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 07:02:52 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 07:02:53 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 07:02:53 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 07:02:53 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 07:02:53 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 07:02:53 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 07:02:53 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 07:02:53 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 07:02:53 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 07:02:53 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 07:02:53 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 07:02:53 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 07:02:53 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 07:02:53 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 07:02:53 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 07:02:53 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 07:02:53 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 07:02:53 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 07:02:53 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 07:02:55 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 07:02:56 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 07:02:58 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 07:04:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 07:04:04 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 07:04:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-11 07:04:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-11 07:04:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-07-11 07:05:43 | INFO | train_inner | epoch 001:    102 / 1474 loss=8.828, trans_loss=5.626, nll_loss=4.214, w2v_ctc_loss=9.557, task_loss=1.816, contrastive_loss=3.307, total=4208.4, n_correct=196.77, ppl=18.56, accuracy=4.676, wps=14058.4, ups=1.12, wpb=12567.1, bsz=469.7, num_updates=100, lr=4.098e-06, gnorm=0.646, clip=0, loss_scale=32, train_wall=91, gb_free=16.4, wall=170
2023-07-11 07:07:10 | INFO | train_inner | epoch 001:    202 / 1474 loss=6.227, trans_loss=5.657, nll_loss=4.326, w2v_ctc_loss=5.393, task_loss=1.764, contrastive_loss=3.285, total=4125.46, n_correct=147.42, ppl=20.06, accuracy=3.573, wps=14066.1, ups=1.14, wpb=12321.8, bsz=461.7, num_updates=200, lr=8.096e-06, gnorm=3.257, clip=0, loss_scale=32, train_wall=87, gb_free=13.9, wall=257
2023-07-11 08:08:40 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:17524
2023-07-11 08:08:40 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:17524
2023-07-11 08:08:40 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:17524
2023-07-11 08:08:40 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:17524
2023-07-11 08:08:40 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:17524
2023-07-11 08:08:40 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:17524
2023-07-11 08:08:40 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:17524
2023-07-11 08:08:40 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:17524
2023-07-11 08:08:40 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 08:08:40 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 08:08:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 08:08:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 08:08:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 08:08:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 08:08:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 08:08:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 08:08:41 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:08:41 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 08:08:41 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:08:41 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 08:08:41 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:08:41 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:08:41 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 08:08:41 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 08:08:41 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:08:41 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 08:08:41 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:08:41 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 08:08:41 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:08:41 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 08:08:41 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:08:41 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 08:08:43 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:17524', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 08:08:43 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:08:43 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:08:43 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 08:08:43 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 08:08:43 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:08:48 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 08:08:48 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 08:08:48 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 08:08:50 | INFO | root | load pretrained hubert
2023-07-11 08:08:54 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:08:55 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:08:59 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:08:59 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 08:08:59 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 08:08:59 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 08:08:59 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 08:08:59 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 08:08:59 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 08:08:59 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 08:08:59 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:08:59 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:08:59 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:08:59 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:09:06 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 08:09:06 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 08:09:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 08:09:07 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 08:09:07 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:09:07 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:09:07 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:09:07 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:09:07 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:09:07 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:09:07 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:09:07 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:09:07 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 08:09:07 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 08:09:07 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 08:09:07 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:09:07 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:09:07 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 08:09:07 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:09:07 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:09:07 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:09:08 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:09:10 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:09:12 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:10:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 08:10:17 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 08:10:17 | INFO | fairseq_cli.train | Start iterating over samples
> 1.  torch.Size([8, 132, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) [[False, False, False, True], [False, False, True, True]]
> 1.  torch.Size([8, 158, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) [[False, False, False, True], [False, False, True, True]]
2023-07-11 08:10:25 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/crash.pt
> 1.  torch.Size([8, 121, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) [[False, False, False, True], [False, False, True, True]]
> 1.  torch.Size([24, 59, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) [[False, False, False, True], [False, False, True, True]]
> 1.  torch.Size([6, 280, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) [[False, False, False, True], [False, False, True, True]]
> 1.  torch.Size([16, 94, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) [[False, False, False, True], [False, False, True, True]]
> 1.  torch.Size([16, 81, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) [[False, False, False, True], [False, False, True, True]]
2023-07-11 08:10:26 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/crash.pt
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 513, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 408, in _per_task_train_loss
    loss, sample_size, logging_output = criterion(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT.py", line 263, in forward
    st_prob = model.task_net(st_encoder_output, st_encoder_padding_mask if self.at_nopad else None)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/models/speech_to_text/s2t_joint.py", line 1018, in forward
    lengths = (~padding_mask).long.sum(dim=1)
TypeError: bad operand type for unary ~: 'list'

/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-07-11 08:11:41 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:19158
2023-07-11 08:11:41 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:19158
2023-07-11 08:11:41 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:19158
2023-07-11 08:11:41 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:19158
2023-07-11 08:11:41 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:19158
2023-07-11 08:11:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 08:11:41 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:19158
2023-07-11 08:11:41 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:19158
2023-07-11 08:11:41 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:19158
2023-07-11 08:11:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 08:11:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 08:11:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 08:11:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 08:11:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 08:11:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 08:11:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 08:11:41 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:11:41 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 08:11:41 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:11:41 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 08:11:41 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:11:41 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 08:11:41 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:11:41 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 08:11:41 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:11:41 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:11:41 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 08:11:41 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 08:11:41 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:11:41 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 08:11:41 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:11:41 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 08:11:43 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19158', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 08:11:43 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:11:43 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:11:43 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 08:11:43 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 08:11:43 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:11:48 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 08:11:48 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 08:11:48 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 08:11:49 | INFO | root | load pretrained hubert
2023-07-11 08:11:53 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:11:53 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:11:54 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:11:54 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 08:11:54 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 08:11:54 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 08:11:54 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 08:11:54 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 08:11:54 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 08:11:54 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 08:11:54 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:11:54 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:11:54 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:11:54 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:12:01 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 08:12:04 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 08:12:04 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 08:12:04 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 08:12:04 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:12:04 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:12:04 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:12:04 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:12:04 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:12:04 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:12:04 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:12:04 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:12:04 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 08:12:04 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 08:12:04 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 08:12:04 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:12:04 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:12:04 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 08:12:04 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:12:04 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:12:04 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:12:06 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:12:08 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:12:09 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:13:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 08:13:14 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 08:13:14 | INFO | fairseq_cli.train | Start iterating over samples
> 1.  torch.Size([24, 59, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[0., 0., 0., 1.],
        [0., 0., 1., 1.]])
> 1.  torch.Size([6, 280, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[0., 0., 0., 1.],
        [0., 0., 1., 1.]])
> 1.  torch.Size([8, 158, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[0., 0., 0., 1.],
        [0., 0., 1., 1.]])
2023-07-11 08:13:23 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/crash.pt
> 1.  torch.Size([16, 81, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[0., 0., 0., 1.],
        [0., 0., 1., 1.]])
> 1.  torch.Size([8, 200, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[0., 0., 0., 1.],
        [0., 0., 1., 1.]])
> 1.  torch.Size([16, 94, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[0., 0., 0., 1.],
        [0., 0., 1., 1.]])
> 1.  torch.Size([8, 132, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[0., 0., 0., 1.],
        [0., 0., 1., 1.]])
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 513, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 408, in _per_task_train_loss
    loss, sample_size, logging_output = criterion(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT.py", line 263, in forward
    st_prob = model.task_net(st_encoder_output, st_encoder_padding_mask if self.at_nopad else None)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/models/speech_to_text/s2t_joint.py", line 1018, in forward
    lengths = (~padding_mask).long.sum(dim=1)
TypeError: ~ (operator.invert) is only implemented on integer and Boolean-type tensors

/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 44 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-07-11 08:15:09 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:13365
2023-07-11 08:15:09 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:13365
2023-07-11 08:15:09 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:13365
2023-07-11 08:15:09 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:13365
2023-07-11 08:15:09 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:13365
2023-07-11 08:15:09 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:13365
2023-07-11 08:15:09 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:13365
2023-07-11 08:15:09 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:13365
2023-07-11 08:15:09 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 08:15:09 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 08:15:09 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 08:15:09 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 08:15:09 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 08:15:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 08:15:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 08:15:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 08:15:10 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:15:10 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 08:15:10 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:15:10 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:15:10 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:15:10 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:15:10 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:15:10 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 08:15:10 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 08:15:10 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 08:15:10 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 08:15:10 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 08:15:10 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:15:10 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:15:10 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 08:15:10 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 08:15:12 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13365', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 08:15:12 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:15:13 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:15:13 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 08:15:13 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 08:15:13 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:15:17 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 08:15:17 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 08:15:17 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 08:15:19 | INFO | root | load pretrained hubert
2023-07-11 08:15:23 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:15:24 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:15:25 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:15:25 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 08:15:25 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 08:15:25 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 08:15:25 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 08:15:25 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 08:15:25 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 08:15:25 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 08:15:25 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:15:25 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:15:25 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:15:25 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:15:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 08:15:34 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 08:15:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 08:15:35 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 08:15:35 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:15:35 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:15:35 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:15:35 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:15:35 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:15:35 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:15:35 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:15:35 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:15:35 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 08:15:35 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 08:15:35 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 08:15:35 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:15:35 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:15:35 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 08:15:35 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:15:35 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:15:35 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:15:36 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:15:38 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:15:40 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:16:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 08:16:45 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 08:16:45 | INFO | fairseq_cli.train | Start iterating over samples
> 1.  torch.Size([8, 132, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[False, False, False,  True],
        [False, False,  True,  True]])
> 1.  torch.Size([8, 158, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[False, False, False,  True],
        [False, False,  True,  True]])
2023-07-11 08:16:53 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/crash.pt
> 1.  torch.Size([8, 121, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[False, False, False,  True],
        [False, False,  True,  True]])
> 1.  torch.Size([24, 59, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[False, False, False,  True],
        [False, False,  True,  True]])
> 1.  torch.Size([6, 280, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[False, False, False,  True],
        [False, False,  True,  True]])
> 1.  torch.Size([16, 81, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[False, False, False,  True],
        [False, False,  True,  True]])
> 1.  torch.Size([16, 94, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[False, False, False,  True],
        [False, False,  True,  True]])
> 1.  torch.Size([8, 200, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[False, False, False,  True],
        [False, False,  True,  True]])
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 513, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 408, in _per_task_train_loss
    loss, sample_size, logging_output = criterion(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT.py", line 263, in forward
    st_prob = model.task_net(st_encoder_output, st_encoder_padding_mask if self.at_nopad else None)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/models/speech_to_text/s2t_joint.py", line 1018, in forward
    lengths = (~padding_mask).long.sum(dim=1)
AttributeError: 'builtin_function_or_method' object has no attribute 'sum'

/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-07-11 08:17:35 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:13314
2023-07-11 08:17:35 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:13314
2023-07-11 08:17:35 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:13314
2023-07-11 08:17:35 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:13314
2023-07-11 08:17:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 08:17:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 08:17:35 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:13314
2023-07-11 08:17:35 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:13314
2023-07-11 08:17:35 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:13314
2023-07-11 08:17:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 08:17:35 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:13314
2023-07-11 08:17:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 08:17:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 08:17:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 08:17:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 08:17:35 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:17:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 08:17:35 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:17:35 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 08:17:35 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 08:17:35 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:17:35 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 08:17:35 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:17:35 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 08:17:35 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:17:35 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:17:35 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 08:17:35 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 08:17:35 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:17:35 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 08:17:35 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:17:35 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 08:17:37 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13314', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 08:17:37 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:17:37 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:17:37 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 08:17:37 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 08:17:37 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:17:42 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 08:17:42 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 08:17:42 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 08:17:44 | INFO | root | load pretrained hubert
2023-07-11 08:17:48 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:17:50 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:17:53 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:17:53 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 08:17:53 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 08:17:53 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 08:17:53 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 08:17:53 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 08:17:53 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 08:17:53 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 08:17:53 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:17:53 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:17:53 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:17:53 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:18:00 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 08:18:00 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 08:18:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 08:18:00 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 08:18:00 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:18:00 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:18:00 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:18:00 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:18:00 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:18:00 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:18:00 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:18:00 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:18:00 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 08:18:00 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 08:18:00 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 08:18:00 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:18:00 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:18:00 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 08:18:00 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:18:00 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:18:00 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:18:02 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:18:04 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:18:06 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:19:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 08:19:10 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 08:19:10 | INFO | fairseq_cli.train | Start iterating over samples
> 1.  torch.Size([8, 158, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[False, False, False,  True],
        [False, False,  True,  True]])
2023-07-11 08:19:18 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/crash.pt
> 1.  torch.Size([16, 81, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[False, False, False,  True],
        [False, False,  True,  True]])
> 1.  torch.Size([8, 121, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[False, False, False,  True],
        [False, False,  True,  True]])
> 1.  torch.Size([8, 200, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[False, False, False,  True],
        [False, False,  True,  True]])
> 1.  torch.Size([6, 280, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[False, False, False,  True],
        [False, False,  True,  True]])
> 1.  torch.Size([24, 59, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[False, False, False,  True],
        [False, False,  True,  True]])
> 1.  torch.Size([16, 94, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[False, False, False,  True],
        [False, False,  True,  True]])
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 4 terminated with the following error:
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 513, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 408, in _per_task_train_loss
    loss, sample_size, logging_output = criterion(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT.py", line 263, in forward
    st_prob = model.task_net(st_encoder_output, st_encoder_padding_mask if self.at_nopad else None)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/models/speech_to_text/s2t_joint.py", line 1019, in forward
    clean = x[padding_mask.unsqueeze(-1)]=0.0
IndexError: The shape of the mask [2, 4, 1] at index 2 does not match the shape of the indexed tensor [2, 4, 5] at index 2

/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-07-11 08:21:07 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:17024
2023-07-11 08:21:07 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:17024
2023-07-11 08:21:07 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:17024
2023-07-11 08:21:07 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:17024
2023-07-11 08:21:07 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:17024
2023-07-11 08:21:07 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:17024
2023-07-11 08:21:07 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:17024
2023-07-11 08:21:07 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:17024
2023-07-11 08:21:07 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 08:21:07 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 08:21:07 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 08:21:07 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 08:21:07 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 08:21:07 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 08:21:08 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 08:21:08 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 08:21:08 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:21:08 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:21:08 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:21:08 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:21:08 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:21:08 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 08:21:08 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 08:21:08 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 08:21:08 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 08:21:08 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 08:21:08 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:21:08 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 08:21:08 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:21:08 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 08:21:08 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:21:08 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 08:21:10 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:17024', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 08:21:10 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:21:10 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:21:10 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 08:21:10 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 08:21:10 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:21:15 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 08:21:15 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 08:21:15 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 08:21:17 | INFO | root | load pretrained hubert
2023-07-11 08:21:18 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:21:20 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:21:20 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:21:20 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 08:21:21 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 08:21:21 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 08:21:21 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 08:21:21 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 08:21:21 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 08:21:21 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 08:21:21 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:21:21 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:21:21 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:21:21 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:21:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 08:21:30 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 08:21:30 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 08:21:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 08:21:31 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:21:31 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:21:31 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:21:31 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:21:31 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:21:31 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:21:31 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:21:31 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:21:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 08:21:31 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 08:21:31 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 08:21:31 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:21:31 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:21:31 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 08:21:31 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:21:31 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:21:31 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:21:33 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:21:34 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:21:36 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:22:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 08:22:41 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 08:22:41 | INFO | fairseq_cli.train | Start iterating over samples
> 1.  torch.Size([24, 59, 512])
> 1.  torch.Size([16, 94, 512])
> 1.  torch.Size([8, 200, 512])
> 1.  torch.Size([8, 121, 512])
> 1.  torch.Size([8, 158, 512])
> 1.  torch.Size([16, 81, 512])
> 1.  torch.Size([6, 280, 512])
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 6 terminated with the following error:
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 863, in train_step
    raise e
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 513, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 408, in _per_task_train_loss
    loss, sample_size, logging_output = criterion(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT.py", line 263, in forward
    st_prob = model.task_net(st_encoder_output, st_encoder_padding_mask if self.at_nopad else None)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/models/speech_to_text/s2t_joint.py", line 1016, in forward
    x = torch.rand(2, 4, 5)
RuntimeError: The expanded size of the tensor (5) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [2, 4, 5].  Tensor sizes: [2, 4]

/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-07-11 08:24:14 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 08:24:14 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:24:14 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:24:14 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 08:24:14 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 08:24:14 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:24:19 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 08:24:19 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 08:24:19 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 08:24:20 | INFO | root | load pretrained hubert
2023-07-11 08:24:20 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:24:21 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:24:21 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:24:21 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 08:24:21 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 08:24:21 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 08:24:21 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 08:24:21 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 08:24:21 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 08:24:21 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 08:24:21 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:24:21 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:24:21 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:24:21 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 08:24:22 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 08:24:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-07-11 08:24:22 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:24:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-07-11 08:24:22 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2023-07-11 08:24:22 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 08:24:22 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:24:22 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:24:22 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 08:24:22 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:24:22 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:24:22 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:24:24 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:24:25 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:24:27 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:25:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 11788
2023-07-11 08:25:31 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 08:25:31 | INFO | fairseq_cli.train | Start iterating over samples
> 1.  torch.Size([8, 158, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[[False, False, False, False, False],
         [False, False, False, False, False],
         [False, False, False, False, False],
         [ True,  True,  True,  True,  True]],

        [[False, False, False, False, False],
         [False, False, False, False, False],
         [ True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True]]])
> 3.  0.0 tensor([[3, 3, 3, 3, 3],
        [2, 2, 2, 2, 2]])
2023-07-11 08:25:32 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/crash.pt
2023-07-11 08:25:34 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/crash.pt
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 513, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 408, in _per_task_train_loss
    loss, sample_size, logging_output = criterion(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT.py", line 263, in forward
    st_prob = model.task_net(st_encoder_output, st_encoder_padding_mask if self.at_nopad else None)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/models/speech_to_text/s2t_joint.py", line 1022, in forward
    x = clean.sum(dim=1)
AttributeError: 'float' object has no attribute 'sum'
2023-07-11 08:27:30 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 08:27:30 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:27:30 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:27:30 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 08:27:30 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 08:27:30 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:27:35 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 08:27:35 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 08:27:35 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 08:27:36 | INFO | root | load pretrained hubert
2023-07-11 08:27:37 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:27:37 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:27:37 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:27:37 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 08:27:38 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 08:27:38 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 08:27:38 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 08:27:38 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 08:27:38 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 08:27:38 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 08:27:38 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:27:38 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:27:38 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:27:38 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 08:27:38 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 08:27:38 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-07-11 08:27:38 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:27:38 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-07-11 08:27:38 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2023-07-11 08:27:38 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 08:27:38 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:27:38 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:27:38 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 08:27:38 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:27:38 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:27:38 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:27:40 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:27:42 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:27:43 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:28:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 11788
2023-07-11 08:28:47 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 08:28:47 | INFO | fairseq_cli.train | Start iterating over samples
> 1.  torch.Size([8, 158, 512])
> 2.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.4550, 0.5725, 0.4980, 0.9371, 0.6556]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.6397, 0.9743, 0.8300, 0.0444, 0.0246],
         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676]]]) tensor([[False, False, False,  True],
        [False, False,  True,  True]])
> 3.  tensor([[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
         [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
         [0.6387, 0.5247, 0.6826, 0.3051, 0.4635],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],

        [[0.3138, 0.1980, 0.4162, 0.2843, 0.3398],
         [0.5239, 0.7981, 0.7718, 0.0112, 0.8100],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]) tensor([3, 2])
> 4.  tensor([[2.1962, 1.2011, 1.8401, 1.6093, 0.9316],
        [0.8377, 0.9961, 1.1880, 0.2956, 1.1497]]) torch.Size([2, 5])
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 863, in train_step
    raise e
  File "/workspace/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 513, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/workspace/fairseq-AT/fairseq/tasks/joint_triple_pretraining.py", line 408, in _per_task_train_loss
    loss, sample_size, logging_output = criterion(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT.py", line 263, in forward
    st_prob = model.task_net(st_encoder_output, st_encoder_padding_mask if self.at_nopad else None)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/fairseq-AT/fairseq/models/speech_to_text/s2t_joint.py", line 1024, in forward
    x /= lengths
RuntimeError: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 1
python3: can't open file '/home/koukaiqi/fairseq-AT/fairseq_cli/train.py': [Errno 2] No such file or directory
2023-07-11 08:42:41 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:16483
2023-07-11 08:42:41 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:16483
2023-07-11 08:42:41 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:16483
2023-07-11 08:42:41 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:16483
2023-07-11 08:42:41 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:16483
2023-07-11 08:42:41 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:16483
2023-07-11 08:42:41 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:16483
2023-07-11 08:42:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 08:42:41 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:16483
2023-07-11 08:42:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 08:42:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 08:42:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 08:42:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 08:42:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 08:42:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 08:42:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 08:42:42 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:42:42 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 08:42:42 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:42:42 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 08:42:42 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:42:42 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 08:42:42 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:42:42 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:42:42 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:42:42 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:42:42 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 08:42:42 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 08:42:42 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 08:42:42 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 08:42:42 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:42:42 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
Traceback (most recent call last):
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/workspace/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 322, in distributed_main
    cfg.distributed_training.distributed_rank = distributed_init(cfg)
  File "/workspace/fairseq-AT/fairseq/distributed/utils.py", line 272, in distributed_init
    dist.all_reduce(torch.zeros(1).cuda())
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2023-07-11 08:43:34 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:10070
2023-07-11 08:43:34 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:10070
2023-07-11 08:43:34 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:10070
2023-07-11 08:43:34 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:10070
2023-07-11 08:43:34 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:10070
2023-07-11 08:43:34 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:10070
2023-07-11 08:43:34 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:10070
2023-07-11 08:43:34 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:10070
2023-07-11 08:43:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 08:43:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 08:43:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 08:43:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 08:43:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 08:43:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 08:43:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 08:43:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 08:43:35 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:43:35 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:43:35 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:43:35 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:43:35 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:43:35 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:43:35 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 08:43:35 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:43:35 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 08:43:35 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 08:43:35 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 08:43:35 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 08:43:35 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 08:43:35 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 08:43:35 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:43:35 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 08:43:37 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10070', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=0, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 08:43:37 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:43:37 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:43:37 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 08:43:37 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 08:43:37 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:43:41 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 08:43:41 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 08:43:41 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 08:43:43 | INFO | root | load pretrained hubert
2023-07-11 08:43:47 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:43:47 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:43:50 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:43:50 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 08:43:50 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 08:43:50 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 08:43:50 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 08:43:50 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 08:43:50 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 08:43:50 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 08:43:50 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:43:50 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:43:50 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:43:50 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:43:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 08:44:00 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 08:44:00 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 08:44:00 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 08:44:00 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:44:00 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:44:00 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:44:00 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:44:00 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:44:00 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:44:00 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:44:00 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:44:00 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 08:44:00 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 08:44:00 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 08:44:00 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:44:00 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:44:00 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 08:44:00 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:44:00 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:44:00 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:44:02 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:44:03 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:44:05 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:45:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 08:45:10 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 08:45:10 | INFO | fairseq_cli.train | Start iterating over samples
> 5.  tensor([[-0.0459, -0.5674,  0.5547,  ...,  0.3115,  0.0989, -0.9668],
        [ 0.0309, -0.6167,  0.3748,  ...,  0.3889,  0.1923, -0.8647],
        [-0.0500, -0.5132,  0.3960,  ...,  0.1976,  0.0710, -0.7764],
        ...,
        [-0.0290, -0.6851,  0.3784,  ...,  0.4548,  0.1715, -0.8198],
        [-0.1874, -0.5415,  0.5171,  ...,  0.1516,  0.2510, -0.7661],
        [-0.0949, -0.7344,  0.3774,  ...,  0.2661,  0.1367, -0.7866]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
2023-07-11 08:45:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
> 5.  tensor([[ 0.0720, -0.6816,  0.4214,  ...,  0.1910,  0.1191, -0.9473],
        [-0.1022, -0.5215,  0.2942,  ...,  0.3188,  0.0804, -0.8208],
        [ 0.0498, -0.4397,  0.3252,  ...,  0.5083, -0.0204, -0.7163],
        ...,
        [ 0.0148, -0.5850,  0.1288,  ...,  0.2007,  0.1304, -0.9951],
        [ 0.1509, -0.6118,  0.4099,  ...,  0.4038,  0.1069, -1.0967],
        [-0.0606, -0.5557,  0.5459,  ...,  0.2264,  0.0756, -0.7661]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 1.7834e-01, -4.9292e-01,  7.1973e-01,  ...,  5.1709e-01,
          5.8496e-01, -6.5234e-01],
        [ 5.1193e-03, -4.2993e-01,  7.9199e-01,  ...,  4.3750e-01,
          5.1074e-01, -5.1709e-01],
        [-1.3110e-01, -3.7720e-01,  6.6992e-01,  ...,  5.1221e-01,
          7.8223e-01, -8.1104e-01],
        ...,
        [ 1.2939e-01, -4.4800e-01,  8.1396e-01,  ...,  4.5483e-01,
          7.1094e-01, -8.1787e-01],
        [ 5.5647e-04, -6.1084e-01,  8.6035e-01,  ...,  2.8369e-01,
          2.9517e-01, -6.1328e-01],
        [-1.1700e-01, -5.2148e-01,  8.7061e-01,  ...,  6.1621e-01,
          3.8916e-01, -7.2363e-01]], device='cuda:2', dtype=torch.float16,
       grad_fn=<DivBackward0>) torch.Size([48, 512])
> 5.  tensor([[-0.0913, -0.5122,  0.2825,  ...,  0.3752,  0.2305, -0.8604],
        [ 0.0362, -0.4675,  0.5259,  ...,  0.2627,  0.2335, -0.8564],
        [-0.0032, -0.5732,  0.5571,  ...,  0.4026,  0.1111, -0.7021],
        ...,
        [-0.1952, -0.4915,  0.5156,  ...,  0.3057,  0.1521, -0.7026],
        [ 0.1385, -0.5444,  0.4192,  ...,  0.2708,  0.1904, -0.6719],
        [ 0.1455, -0.6694,  0.5991,  ...,  0.5586,  0.1787, -0.8403]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0505, -0.4912,  0.2316,  ...,  0.2250,  0.0950, -0.8867],
        [-0.0484, -0.4595,  0.3704,  ...,  0.3669,  0.1043, -0.7129],
        [ 0.0379, -0.6929,  0.3557,  ...,  0.3323, -0.0021, -0.8862],
        ...,
        [ 0.0137, -0.6221,  0.3960,  ...,  0.4517,  0.0273, -0.8115],
        [ 0.0134, -0.3909,  0.3008,  ...,  0.2815,  0.0094, -0.8750],
        [-0.0329, -0.5781,  0.3901,  ...,  0.1749,  0.0514, -0.8149]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0873, -0.7061,  0.3958,  ...,  0.2153, -0.0662, -0.7075],
        [-0.1475, -0.5015,  0.2544,  ...,  0.4014, -0.0277, -0.9238],
        [-0.0505, -0.4407,  0.2150,  ...,  0.5210, -0.0261, -0.8687],
        ...,
        [-0.0740, -0.3774,  0.3022,  ...,  0.3755,  0.0192, -0.8574],
        [-0.2603, -0.5200,  0.3203,  ...,  0.2239, -0.0933, -0.8125],
        [-0.1741, -0.5303,  0.3777,  ...,  0.3904, -0.0278, -0.8521]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[ 0.0512, -0.5830,  0.2871,  ...,  0.0704, -0.0654, -0.9248],
        [-0.0070, -0.5762,  0.3992,  ...,  0.0881,  0.0170, -0.7773],
        [ 0.0212, -0.6016,  0.2372,  ...,  0.3472,  0.0553, -0.7158],
        ...,
        [-0.0306, -0.3835,  0.2964,  ...,  0.4746, -0.1826, -0.7881],
        [-0.1592, -0.5405,  0.3562,  ...,  0.2047, -0.0677, -0.7773],
        [ 0.1160, -0.5396,  0.2174,  ...,  0.4312,  0.0266, -0.9453]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1432, -0.5962,  0.7153,  ...,  0.5952,  0.8447, -0.7524],
        [ 0.0860, -0.4194,  0.7627,  ...,  0.6982,  0.6191, -0.7959],
        [-0.1791, -0.4675,  0.4988,  ...,  0.6562,  0.7808, -0.6118],
        ...,
        [-0.1702, -0.3755,  0.4099,  ...,  0.6387,  0.6201, -0.6860],
        [-0.2590, -0.5103,  0.8667,  ...,  0.5449,  0.4924, -0.4976],
        [-0.4541, -0.4490,  0.8511,  ...,  0.5347,  0.5874, -0.9438]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([56, 512])
> 5.  tensor([[-0.1766, -0.5200,  0.7383,  ...,  0.1183,  0.4268, -0.8940],
        [ 0.0255, -0.4993,  0.4663,  ...,  0.3418,  0.5825, -0.7002],
        [-0.1714, -0.4666,  0.6118,  ...,  0.3459,  0.4771, -0.9175],
        ...,
        [-0.1425, -0.4875,  0.4758,  ...,  0.5859,  0.3188, -0.7812],
        [ 0.0822, -0.6680,  0.6177,  ...,  0.6821,  0.5508, -0.7207],
        [ 0.1405, -0.4963,  0.7598,  ...,  0.4419,  0.6187, -0.6724]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[-0.3582, -0.9678,  1.0205,  ...,  0.1945, -0.0019, -0.9185],
        [-0.4077, -0.8340,  0.9097,  ...,  0.2494,  0.0715, -1.0576],
        [-0.3872, -0.6533,  0.8750,  ...,  0.1500,  0.2251, -0.9463],
        [-0.3225, -0.7505,  0.8833,  ..., -0.0480,  0.0442, -0.8877],
        [-0.3364, -0.6450,  1.1172,  ...,  0.4121,  0.0586, -0.7974],
        [-0.2908, -0.8687,  0.9458,  ...,  0.3530,  0.4695, -0.5820]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([6, 512])
> 5.  tensor([[-0.0578, -0.4744,  0.4783,  ...,  0.3909,  0.0444, -0.8267],
        [-0.1390, -0.4834,  0.3064,  ...,  0.4670,  0.1210, -0.7598],
        [-0.1415, -0.4932,  0.3567,  ...,  0.5493,  0.1142, -0.8369],
        ...,
        [-0.0199, -0.4868,  0.3508,  ...,  0.4224,  0.1292, -0.9536],
        [ 0.0662, -0.5825,  0.3745,  ...,  0.2715,  0.0019, -0.8188],
        [ 0.1547, -0.7100,  0.4226,  ...,  0.2876,  0.2115, -0.9092]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0133, -0.6460,  0.4636,  ...,  0.1440, -0.1627, -0.8838],
        [ 0.0199, -0.5366,  0.2213,  ...,  0.5264,  0.1187, -0.8594],
        [-0.0665, -0.5151,  0.4023,  ...,  0.3469,  0.1333, -0.7554],
        ...,
        [-0.1276, -0.6543,  0.3511,  ...,  0.3845,  0.0227, -0.8862],
        [-0.0329, -0.5781,  0.2671,  ...,  0.2734,  0.0945, -0.8652],
        [-0.0683, -0.5337,  0.3428,  ...,  0.3896,  0.0146, -0.8853]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1302, -0.1874,  0.4797,  ...,  0.4180,  0.1967, -0.6265],
        [-0.2788, -0.3550,  0.6660,  ...,  0.3218,  0.4734, -0.4487],
        [-0.4595,  0.1110,  0.5806,  ...,  0.2480,  0.3665, -0.2874],
        ...,
        [-0.1289, -0.3389,  0.4839,  ...,  0.4082,  0.1930, -0.5679],
        [-0.5762, -0.4167,  0.0083,  ...,  0.2283,  0.3599, -0.8882],
        [-0.2522, -0.2107,  0.2455,  ...,  0.3801,  0.5693, -0.3174]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([88, 512])
> 5.  tensor([[-0.0793, -0.8979,  0.4412,  ...,  0.4595,  0.2642, -0.7837],
        [-0.0611, -0.7061,  0.3130,  ...,  0.2234,  0.2277, -0.8291],
        [-0.3835, -0.9033,  0.5781,  ...,  0.2834,  0.2086, -0.7529],
        ...,
        [-0.1908, -0.7793,  0.4331,  ...,  0.2759,  0.3486, -0.7251],
        [-0.1180, -0.8931,  0.4502,  ...,  0.4478,  0.3601, -0.7925],
        [-0.2112, -0.7412,  0.5996,  ...,  0.3188,  0.2405, -0.6641]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.2009, -0.6060,  0.3342,  ...,  0.4783, -0.0210, -0.6812],
        [-0.1224, -0.6714,  0.4070,  ...,  0.4207, -0.0985, -0.7876],
        [-0.0260, -0.4229,  0.1798,  ...,  0.6348, -0.0835, -0.6006],
        ...,
        [-0.2708, -0.5459,  0.3721,  ...,  0.1254, -0.1410, -0.7690],
        [-0.1125, -0.6016,  0.3245,  ...,  0.5366,  0.0191, -0.6743],
        [-0.2805, -0.5181,  0.3784,  ...,  0.2646,  0.0277, -0.8892]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1407, -0.4080,  0.6533,  ...,  0.3328,  0.2181, -0.6777],
        [-0.2004, -0.4756,  0.6758,  ...,  0.4175,  0.5439, -1.0410],
        [-0.0384, -0.4856,  0.7827,  ...,  0.7139,  0.6460, -0.4749],
        ...,
        [ 0.0448, -0.4197,  0.8345,  ...,  0.7388,  0.6401, -0.7856],
        [-0.0059, -0.3201,  0.5649,  ...,  0.6626,  0.4788, -0.4722],
        [ 0.0594, -0.6079,  0.4697,  ...,  0.3135,  0.3933, -0.7178]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([48, 512])
> 5.  > 5.  tensor([[-0.0756, -0.5840,  0.5552,  ...,  0.5332,  0.2007, -0.6909],
        [-0.0967, -0.5498,  0.6675,  ...,  0.7446,  0.1866, -0.7446],
        [-0.0791, -0.5327,  0.5791,  ...,  0.4248,  0.1174, -0.6753],
        ...,
        [-0.2466, -0.6724,  0.6050,  ...,  0.4102,  0.3857, -0.8545],
        [-0.2494, -0.7275,  0.7012,  ...,  0.2299,  0.1378, -0.8003],
        [-0.1172, -0.6514,  0.6182,  ...,  0.4282,  0.3972, -0.6240]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.0058, -0.1593,  0.6899,  ...,  0.9077,  0.9409, -0.7471],
        [-0.1893, -0.3789,  0.6270,  ...,  0.6465,  0.4985, -0.8574],
        [-0.2771, -0.3352,  0.4314,  ...,  0.6406,  0.5190, -0.6792],
        ...,
        [-0.5229, -0.3633,  0.5278,  ...,  0.2610,  0.8076, -0.8560],
        [-0.4102, -0.2036,  0.4539,  ...,  0.7266,  0.6099, -0.4031],
        [-0.3955, -0.2423,  0.6733,  ...,  0.4509,  0.2971, -0.6963]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([80, 512])
> 5.  tensor([[-0.2365, -0.3965,  0.6792,  ...,  0.3274,  0.1030, -0.7549],
        [-0.0968, -0.6621,  0.6167,  ...,  0.2896,  0.3740, -0.8833],
        [-0.1045, -0.5107,  0.6436,  ...,  0.2229,  0.0742, -0.7192],
        ...,
        [-0.0555, -0.9590,  0.5073,  ...,  0.2686,  0.2188, -0.9556],
        [-0.1410, -0.6577,  0.7222,  ..., -0.1440,  0.2405, -0.7632],
        [-0.0589, -0.6323,  0.5474,  ...,  0.4475,  0.4275, -0.7656]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1499, -0.5874,  0.4863,  ...,  0.1581, -0.0392, -0.7329],
        [-0.1151, -0.5298,  0.3145,  ...,  0.4480, -0.0688, -0.8940],
        [-0.1512, -0.6108,  0.2971,  ..., -0.0301,  0.3662, -1.0410],
        ...,
        [-0.0731, -0.4973,  0.1953,  ...,  0.4705,  0.0916, -0.9790],
        [-0.3127, -0.7427,  0.3459,  ...,  0.5073,  0.0280, -0.9116],
        [-0.2281, -0.6138,  0.4548,  ...,  0.2849, -0.0870, -0.9971]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.2159, -0.2964,  0.7568,  ...,  0.4114,  0.4475, -0.6421],
        [-0.0466, -0.4688,  0.6777,  ...,  0.6421,  0.5962, -0.6919],
        [-0.1504, -0.6494,  0.7759,  ...,  0.2761,  0.4773, -0.9062],
        ...,
        [-0.0677, -0.6816,  0.7798,  ...,  0.3337,  0.4604, -0.6763],
        [-0.0455, -0.6260,  0.5117,  ...,  0.6899,  0.7407, -0.6382],
        [-0.2732, -0.5894,  0.8604,  ...,  0.7368,  0.4795, -1.1035]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([40, 512])
> 5.  tensor([[-0.1499, -0.5273,  0.3145,  ...,  0.4651,  0.2179, -0.9946],
        [-0.1216, -0.5112,  0.3723,  ...,  0.4116,  0.0387, -0.8711],
        [-0.0235, -0.5879,  0.2825,  ...,  0.3838,  0.0676, -0.9067],
        ...,
        [-0.1039, -0.4099,  0.3269,  ...,  0.2942, -0.0494, -0.8013],
        [-0.0195, -0.5352,  0.2274,  ...,  0.4365, -0.0645, -0.8120],
        [ 0.0375, -0.4058,  0.0669,  ...,  0.1506,  0.0351, -0.8555]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.2467, -0.7646,  1.1406,  ...,  0.3135,  0.4023, -1.0176],
        [-0.3953, -1.0889,  1.0029,  ...,  0.1082,  0.0580, -1.0039],
        [-0.3987, -0.8247,  1.1777,  ...,  0.4973,  0.2947, -0.8179],
        [-0.4402, -0.8550,  1.0830,  ...,  0.2937,  0.2118, -1.0605],
        [-0.2542, -0.9756,  1.0703,  ...,  0.3967,  0.3113, -1.1562]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([5, 512])
> 5.  tensor([[-0.1348, -0.4487,  0.5093,  ...,  0.2051,  0.3513, -0.9839],
        [-0.1656, -0.3669,  0.5698,  ...,  0.1813,  0.1965, -0.5693],
        [ 0.1148, -0.5029,  0.6147,  ...,  0.0581,  0.0299, -0.8032],
        ...,
        [ 0.0065, -0.5435,  0.4263,  ...,  0.4011,  0.1743, -0.7485],
        [-0.1915, -0.6533,  0.5952,  ...,  0.1770,  0.1849, -0.7773],
        [-0.1558, -0.6587,  0.7100,  ...,  0.4756,  0.2305, -0.7642]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0738, -0.6743,  0.6230,  ...,  0.3201,  0.0732, -0.7148],
        [-0.1332, -0.5342,  0.4587,  ...,  0.4382,  0.1774, -0.8237],
        [-0.1333, -0.6689,  0.5054,  ...,  0.3943,  0.2966, -0.8506],
        ...,
        [-0.0447, -0.5688,  0.2274,  ..., -0.0107,  0.1802, -0.4612],
        [-0.2335, -0.9321,  0.3308,  ..., -0.0803,  0.4006, -1.4043],
        [ 0.0641, -0.6631,  0.3933,  ...,  0.3855,  0.1973, -0.6899]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0558, -0.5273,  0.7910,  ...,  0.8345,  0.7881, -0.5713],
        [-0.1478, -0.6943,  0.5259,  ...,  0.7495,  0.5361, -0.6440],
        [ 0.0369, -0.5254,  0.7705,  ...,  0.4714,  0.4666, -0.6963],
        ...,
        [-0.0334, -0.5522,  0.4929,  ...,  0.8042,  0.7046, -0.7446],
        [-0.0584, -0.2157,  0.6362,  ...,  0.3613,  0.4426, -0.8037],
        [ 0.1128, -0.3811,  0.5205,  ...,  0.5942,  0.6392, -0.9248]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([56, 512])
> 5.  tensor([[-0.1519, -0.4685,  0.6060,  ...,  0.3516,  0.1758, -0.8027],
        [-0.0374, -0.5977,  0.4211,  ...,  0.3623,  0.1490, -0.8423],
        [-0.1044, -0.7578,  0.4048,  ...,  0.2559,  0.1895, -1.0215],
        ...,
        [-0.1199, -0.5986,  0.5488,  ...,  0.3918,  0.1934, -0.8511],
        [-0.0245, -0.5576,  0.4934,  ...,  0.3538,  0.1987, -0.8301],
        [-0.0379, -0.3359,  0.5190,  ...,  0.3674,  0.1460, -0.7583]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1691, -0.5679,  0.6211,  ...,  0.5312,  0.0610, -0.6865],
        [ 0.0279, -0.6533,  0.3875,  ...,  0.5649,  0.3020, -0.7412],
        [-0.1251, -0.7603,  0.6729,  ...,  0.4092,  0.1553, -0.8999],
        ...,
        [-0.0820, -0.7651,  0.7886,  ...,  0.4658,  0.2030, -0.7173],
        [-0.1906, -0.6475,  0.7256,  ...,  0.5200,  0.2178, -0.8623],
        [-0.0928, -0.8223,  0.6221,  ...,  0.3547,  0.0426, -0.7676]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.2020, -0.4253,  0.3057,  ...,  0.4980, -0.1089, -0.7090],
        [-0.1381, -0.6484,  0.3477,  ...,  0.3545, -0.0173, -0.7310],
        [-0.3459, -0.6431,  0.4578,  ...,  0.4631,  0.0352, -0.8467],
        ...,
        [-0.0765, -0.6094,  0.1412,  ...,  0.3037,  0.0604, -0.6162],
        [-0.3462, -0.6807,  0.4729,  ...,  0.0848,  0.0111, -0.7212],
        [-0.2178, -0.4636,  0.2524,  ...,  0.3613,  0.0260, -0.6201]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1007, -0.5723,  0.4333,  ...,  0.1396,  0.1036, -1.0186],
        [-0.0513, -0.4604,  0.4290,  ...,  0.2380,  0.1144, -0.8384],
        [ 0.0859, -0.3408,  0.3840,  ...,  0.5254,  0.0325, -0.7222],
        ...,
        [ 0.0891, -0.4316,  0.3247,  ...,  0.3921,  0.1497, -0.8330],
        [-0.0598, -0.5190,  0.5278,  ...,  0.0239,  0.0128, -0.8613],
        [ 0.0075, -0.7480,  0.3125,  ...,  0.4543,  0.1471, -0.9844]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1227, -0.5176,  0.3208,  ...,  0.3569,  0.1064, -0.9106],
        [-0.0769, -0.4863,  0.3223,  ...,  0.1135,  0.1014, -0.8501],
        [-0.1019, -0.4731,  0.3401,  ...,  0.4937,  0.0965, -0.8408],
        ...,
        [ 0.0272, -0.6523,  0.4399,  ...,  0.4827,  0.1743, -0.8262],
        [-0.0428, -0.4661,  0.3589,  ...,  0.4458,  0.0622, -0.7837],
        [-0.0757, -0.5752,  0.3381,  ...,  0.3240,  0.0039, -0.8198]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  > 5.  tensor([[-0.0991, -0.6270,  0.6475,  ...,  0.3145,  0.3345, -0.8169],
        [-0.1180, -0.6675,  0.5288,  ...,  0.2344,  0.4734, -0.6714],
        [-0.1414, -0.6035,  0.6079,  ...,  0.0803,  0.1213, -0.8545],
        ...,
        [-0.1119, -0.6445,  0.5273,  ...,  0.2339,  0.3345, -0.8970],
        [-0.2156, -0.5605,  0.4294,  ...,  0.2686,  0.4360, -0.9902],
        [-0.0123, -0.4043,  0.6680,  ...,  0.4561,  0.3503, -0.6880]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0058, -0.5444,  0.6860,  ...,  0.5020,  0.1929, -0.7012],
        [ 0.0094, -0.5649,  0.5591,  ...,  0.3208,  0.2308, -0.8164],
        [-0.1860, -0.6230,  0.5908,  ...,  0.3975,  0.2788, -0.8721],
        ...,
        [-0.0681, -0.6152,  0.6147,  ...,  0.2751,  0.2423, -0.7290],
        [-0.0894, -0.5635,  0.5039,  ...,  0.2571,  0.3162, -1.2334],
        [-0.0963, -0.7793,  0.7095,  ...,  0.5557,  0.1663, -0.7700]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0821, -0.7222,  1.0898,  ...,  0.0918,  0.1333, -0.6821],
        [-0.0424, -0.9878,  0.9355,  ...,  0.1505,  0.1871, -0.8115],
        [-0.1711, -0.7124,  0.8628,  ...,  0.5283,  0.3516, -0.8989],
        ...,
        [-0.1119, -0.8271,  0.9326,  ...,  0.2477,  0.2664, -0.7515],
        [-0.2903, -1.0762,  0.9663,  ...,  0.1949,  0.4404, -0.8105],
        [-0.1929, -0.9502,  0.9341,  ...,  0.2096,  0.1925, -0.7017]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([7, 512])
> 5.  tensor([[-0.2020, -0.3179,  0.4971,  ...,  0.1450,  0.6621, -0.6797],
        [-0.2659, -0.5874,  0.7339,  ...,  0.3987,  0.3210, -0.5322],
        [-0.0110, -0.5059,  0.7080,  ...,  0.5894,  0.5679, -0.7720],
        ...,
        [-0.0807, -0.6094,  0.4490,  ...,  0.5884,  0.8032, -0.8501],
        [-0.1226, -0.0762,  0.7002,  ...,  0.2290,  0.6753, -0.6094],
        [-0.0732, -0.2412,  0.7773,  ...,  0.3860,  0.4990, -0.9268]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([56, 512])
> 5.  tensor([[-0.3315, -0.6240,  0.4028,  ...,  0.3328,  0.0217, -0.6934],
        [-0.3418, -0.5591,  0.3730,  ...,  0.3748, -0.0926, -0.6899],
        [-0.3228, -0.4680,  0.2883,  ...,  0.2286, -0.0165, -0.8774],
        ...,
        [-0.2299, -0.5913,  0.3235,  ...,  0.4109,  0.0933, -0.6602],
        [-0.1804, -0.5854,  0.1952,  ...,  0.4724, -0.0895, -0.7720],
        [-0.1001, -0.6963,  0.3245,  ...,  0.5400,  0.1449, -0.8037]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[ 0.2134, -0.7485,  0.6538,  ..., -0.1486,  0.1633, -0.8218],
        [-0.1586, -0.5444,  0.7729,  ...,  0.2788,  0.2576, -0.8711],
        [-0.0587, -0.7827,  0.5029,  ...,  0.0816,  0.1464, -0.8818],
        ...,
        [-0.2382, -0.6172,  0.6895,  ..., -0.0983,  0.3022, -1.1846],
        [-0.2487, -0.5806,  1.0244,  ...,  0.1349,  0.3188, -0.9248],
        [-0.2668, -0.6855,  0.7622,  ...,  0.2544,  0.4158, -0.8857]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([7, 512])
> 5.  tensor([[-0.1500, -0.7456,  0.8481,  ..., -0.1298,  0.2910, -1.0898],
        [-0.3152, -0.9683,  0.6343,  ..., -0.4919,  0.1726, -0.7070],
        [ 0.0402, -0.6509,  0.7246,  ..., -0.6108, -0.1310, -0.3667],
        ...,
        [-0.3325, -0.3137,  0.9976,  ..., -0.2522,  0.1768, -0.3674],
        [-0.2256, -0.9009,  0.6768,  ...,  0.0934,  0.2725, -0.7700],
        [-0.0913, -0.9277,  0.9233,  ..., -0.2294,  0.0441, -0.9331]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([7, 512])
> 5.  tensor([[-0.0763, -0.6230,  0.6309,  ...,  0.3250,  0.1489, -0.6230],
        [-0.0980, -0.7207,  0.5762,  ...,  0.5811,  0.2930, -0.6846],
        [-0.2346, -0.7466,  0.3931,  ...,  0.1924,  0.1373, -1.0537],
        ...,
        [-0.1216, -0.7051,  0.6084,  ...,  0.3350,  0.2316, -0.7305],
        [-0.1196, -0.8159,  0.6279,  ..., -0.0254,  0.1124, -0.7358],
        [ 0.0182, -0.4866,  0.6343,  ...,  0.5820,  0.3257, -0.8057]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.1970, -0.5933,  0.2991,  ...,  0.4382,  0.0787, -0.9727],
        [-0.0424, -0.5903,  0.3064,  ...,  0.4653,  0.0088, -0.9307],
        [-0.2156, -0.5151,  0.4136,  ...,  0.4053, -0.0221, -0.7109],
        ...,
        [-0.1885, -0.7002,  0.4231,  ...,  0.3430,  0.1802, -0.9268],
        [-0.1449, -0.5005,  0.3394,  ...,  0.5518, -0.0059, -0.7388],
        [-0.1252, -0.5630,  0.3352,  ...,  0.5688, -0.0133, -0.9087]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0093, -0.5938,  0.4167,  ...,  0.2666,  0.0921, -0.8374],
        [ 0.1087, -0.5776,  0.5474,  ...,  0.2749,  0.0206, -0.9263],
        [-0.0363, -0.7212,  0.4368,  ...,  0.2734,  0.1614, -0.7065],
        ...,
        [-0.0428, -0.5669,  0.5532,  ...,  0.2332,  0.1659, -0.6641],
        [-0.0880, -0.5488,  0.4800,  ...,  0.3342,  0.1042, -1.0615],
        [ 0.1005, -0.5127,  0.4409,  ...,  0.0034,  0.1470, -0.9780]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0148, -0.6138,  0.3325,  ...,  0.3416,  0.0397, -0.7090],
        [ 0.0153, -0.5962,  0.3149,  ...,  0.2314,  0.0096, -0.8486],
        [ 0.1946, -0.5903,  0.3486,  ...,  0.3975,  0.0727, -0.6831],
        ...,
        [-0.0757, -0.5552,  0.2856,  ...,  0.1832, -0.0401, -0.8237],
        [-0.0797, -0.7109,  0.3887,  ...,  0.2607,  0.0964, -0.9985],
        [-0.1317, -0.6934,  0.2629,  ...,  0.5034,  0.2177, -0.8423]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0291, -0.5791,  0.4062,  ...,  0.3025,  0.1071, -0.6650],
        [ 0.0844, -0.3665,  0.2676,  ...,  0.4465,  0.1616, -0.8896],
        [-0.0479, -0.5195,  0.3975,  ...,  0.3647,  0.1473, -0.7290],
        ...,
        [-0.0082, -0.4019,  0.3855,  ...,  0.4602,  0.1055, -0.8525],
        [-0.1008, -0.4282,  0.2864,  ...,  0.2720,  0.0287, -0.7764],
        [-0.0772, -0.5361,  0.3506,  ...,  0.3311,  0.0310, -0.8052]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1145, -0.4844,  0.2361,  ...,  0.4014,  0.0062, -0.6812],
        [-0.0964, -0.7993,  0.4138,  ...,  0.7671,  0.1671, -0.7754],
        [-0.0642, -0.5776,  0.2529,  ...,  0.6587,  0.0643, -0.8232],
        ...,
        [-0.2690, -0.7056,  0.3989,  ...,  0.3257, -0.0460, -0.5762],
        [-0.1011, -0.6646,  0.3831,  ...,  0.3071,  0.0515, -0.7964],
        [-0.2212, -0.6919,  0.1454,  ...,  0.4785,  0.2145, -0.8442]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1643, -0.6328,  0.6724,  ...,  0.1576,  0.1934, -0.8540],
        [-0.0081, -0.7202,  0.5981,  ...,  0.0977,  0.1953, -0.9019],
        [-0.0929, -0.6201,  0.5972,  ...,  0.2900,  0.2209, -0.8638],
        ...,
        [-0.0074, -0.7793,  0.5571,  ...,  0.3225,  0.1437, -0.7827],
        [-0.2035, -0.7266,  0.3938,  ..., -0.1703,  0.1198, -1.4053],
        [-0.0650, -0.6880,  0.5781,  ...,  0.1804,  0.1180, -0.8394]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1975, -0.3853,  0.8247,  ...,  0.6270,  0.2443, -0.6167],
        [-0.2634, -0.2559,  0.5615,  ...,  0.6353,  0.6704, -0.6997],
        [-0.3362, -0.1429,  0.7373,  ...,  0.4243,  0.5103, -0.6802],
        ...,
        [-0.1628, -0.4111,  0.7900,  ...,  0.8286,  0.4390, -0.7124],
        [-0.2637, -0.3650,  0.5874,  ...,  0.7002,  0.4460, -0.4438],
        [-0.3755, -0.4197,  0.4426,  ...,  0.3618,  0.5586, -0.5288]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([64, 512])
> 5.  > 5.  tensor([[-0.0166, -0.6313,  0.3191,  ...,  0.2340, -0.0412, -0.9507],
        [-0.1345, -0.7490,  0.5469,  ...,  0.3892,  0.1459, -0.7778],
        [-0.0923, -0.8511,  0.4812,  ...,  0.3948,  0.0865, -0.8296],
        ...,
        [-0.2542, -0.5767,  0.3340,  ...,  0.4023,  0.0558, -0.9468],
        [-0.0736, -0.8032,  0.2340,  ...,  0.5117, -0.0108, -0.7158],
        [-0.2091, -0.8628,  0.4714,  ...,  0.5322,  0.3657, -0.7280]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[ 1.3977e-01, -6.2061e-01,  2.8345e-01,  ...,  4.5044e-01,
          4.9133e-02, -9.0381e-01],
        [-2.5955e-02, -5.2246e-01,  2.7173e-01,  ...,  3.4497e-01,
         -8.4656e-02, -9.5947e-01],
        [ 1.0211e-01, -4.5068e-01,  3.8989e-01,  ...,  4.2969e-01,
          3.4576e-02, -6.9189e-01],
        ...,
        [-3.0853e-02, -6.3721e-01,  3.1860e-01,  ...,  4.2676e-01,
         -8.9645e-04, -7.4805e-01],
        [-4.0466e-02, -4.8560e-01,  4.1699e-01,  ...,  2.8564e-01,
         -1.4124e-01, -6.8555e-01],
        [ 9.9792e-03, -5.9424e-01,  4.3530e-01,  ..., -9.2285e-02,
         -2.1399e-01, -7.8809e-01]], device='cuda:4', dtype=torch.float16,
       grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0043, -0.7412,  0.7500,  ..., -0.0014,  0.2317, -0.7798],
        [-0.1161, -0.6587,  0.8457,  ...,  0.2590,  0.4341, -0.8364],
        [-0.0657, -0.6357,  0.7163,  ...,  0.2417,  0.3865, -1.0371],
        ...,
        [-0.1973, -0.8135,  0.9390,  ...,  0.1841,  0.3564, -0.8076],
        [-0.0865, -0.8394,  0.7485,  ...,  0.2517,  0.2771, -0.7153],
        [-0.1707, -0.7178,  0.7319,  ...,  0.0012,  0.0983, -0.8823]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([7, 512])
> 5.  tensor([[-0.3447, -0.7793,  0.8652,  ...,  0.5752,  0.3110, -0.7744],
        [-0.0150, -0.7964,  0.7734,  ...,  0.2942,  0.5054, -0.6880],
        [-0.2754, -0.3989,  1.0010,  ...,  0.1387,  0.3638, -0.8740],
        ...,
        [-0.1406, -0.8491,  0.9692,  ...,  0.2288,  0.3035, -0.8589],
        [-0.2964, -0.6763,  0.8638,  ...,  0.2998,  0.4490, -0.6665],
        [-0.2793, -0.6729,  0.8765,  ...,  0.2839,  0.2905, -0.7300]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([7, 512])
> 5.  tensor([[-0.0964, -0.7534,  0.5508,  ...,  0.2401,  0.2649, -0.7451],
        [-0.0801, -0.7095,  0.2473,  ...,  0.5249,  0.4072, -0.7344],
        [-0.0313, -0.6260,  0.4502,  ...,  0.4148,  0.1556, -0.6582],
        ...,
        [-0.2274, -0.9614,  0.4055,  ...,  0.6001,  0.3962, -0.8301],
        [ 0.0031, -0.8076,  0.2903,  ...,  0.1565,  0.2107, -0.9736],
        [-0.1989, -0.7812,  0.5532,  ...,  0.4172,  0.0496, -0.8027]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.3762, -0.7583,  1.1670,  ...,  0.0801,  0.1481, -0.9985],
        [-0.1354, -0.9414,  1.2432,  ...,  0.5488,  0.0764, -0.8589],
        [-0.5117, -0.6992,  0.8960,  ...,  0.3025,  0.3669, -0.7354],
        [-0.5073, -0.7529,  1.0967,  ...,  0.1735,  0.1301, -0.8340],
        [-0.2898, -0.6855,  0.9771,  ...,  0.3113,  0.3408, -0.8325]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([5, 512])
> 5.  tensor([[ 0.1061, -0.7339,  0.1809,  ...,  0.2764,  0.1608, -0.6699],
        [-0.0554, -0.4307,  0.4236,  ...,  0.2498,  0.2656, -0.8887],
        [ 0.0417, -0.6113,  0.3296,  ...,  0.4854,  0.1687, -0.9438],
        ...,
        [-0.1110, -0.5791,  0.4514,  ...,  0.3022,  0.1722, -0.9443],
        [-0.1615, -0.4507,  0.3005,  ...,  0.2502,  0.2078, -0.9160],
        [-0.0753, -0.4988,  0.3889,  ...,  0.3918,  0.1139, -0.7373]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1168, -0.5483,  0.7534,  ...,  0.5664,  0.2759, -0.7290],
        [ 0.2052, -0.6699,  0.7349,  ...,  0.7202,  0.1521, -0.5854],
        [-0.1343, -0.8018,  0.8550,  ...,  0.6631,  0.1070, -0.8271],
        ...,
        [-0.1252, -0.7334,  0.5488,  ...,  0.5850,  0.2727, -0.5991],
        [-0.1157, -0.6055,  0.5088,  ...,  0.2915,  0.1594, -0.6714],
        [ 0.0590, -0.5171,  0.8198,  ...,  0.4866,  0.2037, -0.6377]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[ 0.0318, -0.8730,  0.4246,  ...,  0.2473,  0.3691, -1.2188],
        [-0.0774, -0.5410,  0.6270,  ...,  0.2671,  0.4390, -0.8174],
        [ 0.0753, -0.8149,  0.4016,  ...,  0.2085,  0.3105, -1.1738],
        ...,
        [-0.1136, -0.7285,  0.7988,  ...,  0.2056,  0.2700, -0.8745],
        [-0.0450, -0.6328,  0.5005,  ...,  0.2067,  0.2920, -0.6890],
        [ 0.0659, -0.5586,  0.4136,  ...,  0.3625,  0.3911, -0.6440]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0843, -0.6816,  0.4153,  ...,  0.3169, -0.0595, -0.7959],
        [-0.0157, -0.4802,  0.3440,  ...,  0.5566,  0.0468, -0.8887],
        [-0.3240, -0.9053,  0.4023,  ...,  0.3330,  0.1340, -0.7944],
        ...,
        [-0.1504, -0.9580,  0.5166,  ...,  0.3926,  0.2773, -0.8691],
        [-0.0608, -0.8726,  0.4475,  ...,  0.4185, -0.0031, -0.6982],
        [-0.1643, -0.7734,  0.4207,  ...,  0.5811,  0.2268, -0.7495]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0410, -0.5190,  0.4482,  ...,  0.1444,  0.2365, -0.7544],
        [ 0.1638, -0.7471,  0.4170,  ...,  0.2725,  0.2566, -0.6758],
        [-0.0740, -0.4426,  0.5552,  ...,  0.1621,  0.2905, -0.7358],
        ...,
        [-0.1086, -0.4941,  0.3938,  ...,  0.3672,  0.3362, -0.9116],
        [-0.0891, -0.3643,  0.5679,  ...,  0.3123,  0.1103, -0.6592],
        [-0.2391, -0.7842,  0.3179,  ..., -0.3469,  0.3845, -1.3564]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0960, -0.7417,  0.6758,  ..., -0.1008,  0.4937, -0.7925],
        [ 0.0354, -0.3784,  0.6187,  ...,  0.3501,  0.3374, -0.8936],
        [-0.1227, -0.6045,  0.4595,  ...,  0.7939,  0.8906, -0.8721],
        ...,
        [-0.2021, -0.6914,  0.6841,  ...,  0.4441,  0.6138, -0.7676],
        [ 0.1498, -0.2717,  0.6714,  ...,  0.4551,  0.3657, -0.6982],
        [-0.3018, -0.6260,  0.6167,  ...,  0.2360,  0.6167, -0.7744]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([40, 512])
> 5.  tensor([[-0.1628, -0.4824,  0.8315,  ...,  0.2900,  0.2712, -0.8433],
        [-0.2207, -0.3477,  0.6899,  ...,  0.2847,  0.4294, -0.7944],
        [-0.0291, -0.5386,  0.5757,  ...,  0.3459,  0.3672, -0.8213],
        ...,
        [-0.0551, -0.7622,  0.6685,  ...,  0.3474,  0.3853, -0.8950],
        [-0.3716, -0.4111,  0.6919,  ...,  0.2659,  0.3481, -0.7983],
        [-0.1589, -0.4004,  0.6685,  ...,  0.3853,  0.3154, -0.7554]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1472, -0.7329,  0.3606,  ...,  0.3015,  0.1428, -0.8481],
        [-0.1573, -0.7070,  0.3884,  ...,  0.3047,  0.1460, -0.8896],
        [-0.0431, -0.6323,  0.1061,  ...,  0.3313,  0.2700, -0.7710],
        ...,
        [-0.0791, -0.7646,  0.4802,  ...,  0.3069,  0.0779, -0.7754],
        [-0.1501, -0.8271,  0.4795,  ...,  0.4260,  0.0742, -0.7622],
        [-0.1528, -0.7119,  0.3928,  ...,  0.3232,  0.1609, -0.7373]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0850, -0.4285,  0.2257,  ...,  0.1637, -0.0795, -0.6768],
        [-0.2607, -0.6416,  0.3582,  ...,  0.3423,  0.1874, -0.9229],
        [-0.1164, -0.6333,  0.3682,  ...,  0.4326,  0.0064, -0.7939],
        ...,
        [-0.3887, -0.6367,  0.3269,  ...,  0.2634, -0.0845, -0.8428],
        [-0.1605, -0.4580,  0.2161,  ...,  0.2898, -0.0264, -0.7231],
        [-0.0532, -0.4783,  0.3303,  ...,  0.4866,  0.0155, -0.8442]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  > 5.  tensor([[-0.0533, -0.6182,  1.0410,  ..., -0.1754, -0.0668, -0.5088],
        [-0.3918, -0.7598,  0.8911,  ...,  0.2722,  0.2878, -0.9644],
        [-0.2491, -0.6670,  0.6934,  ...,  0.2238,  0.5054, -1.1523],
        [-0.1083, -0.8794,  1.0605,  ...,  0.2690,  0.2489, -0.9204],
        [-0.1893, -0.8530,  1.0527,  ...,  0.1704,  0.1082, -0.6553],
        [-0.2163, -0.9170,  0.8452,  ...,  0.0601,  0.2961, -0.8975]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([6, 512])
> 5.  tensor([[-0.0430, -0.3645,  0.6216,  ...,  0.2915,  0.6455, -0.4382],
        [ 0.0719, -0.6636,  0.5791,  ...,  0.4934,  0.3323, -0.6235],
        [-0.1044, -0.3391,  0.7578,  ...,  0.6172,  0.8022, -0.7471],
        ...,
        [-0.4575, -0.3503,  0.5635,  ...,  0.5713,  0.7432, -0.9116],
        [-0.5161, -0.4824,  0.8735,  ...,  0.3579,  0.4504, -0.7202],
        [ 0.1698, -0.1119,  0.4302,  ...,  0.6509,  0.7959, -0.4492]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([64, 512])
> 5.  tensor([[-0.0517, -0.7612,  0.3792,  ...,  0.4937,  0.0970, -0.8135],
        [-0.0945, -0.6958,  0.3032,  ...,  0.2891,  0.2649, -0.6929],
        [-0.1061, -0.7446,  0.3574,  ...,  0.5151,  0.0098, -0.8428],
        ...,
        [-0.1996, -0.7588,  0.3772,  ...,  0.2712,  0.1375, -0.8418],
        [-0.1052, -0.4172,  0.4106,  ...,  0.3948, -0.0297, -0.6748],
        [-0.2629, -0.5093,  0.3459,  ...,  0.2347,  0.1559, -0.7803]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[ 0.1384, -0.2842,  0.5283,  ...,  0.6558,  0.2659, -0.5938],
        [-0.1477, -0.4758,  0.7422,  ...,  0.7495,  0.8525, -0.5400],
        [ 0.0523, -0.3494,  0.4570,  ...,  0.4773,  0.4333, -0.6826],
        ...,
        [ 0.0900, -0.4805,  0.5859,  ...,  0.3291,  0.4866, -0.5815],
        [-0.2183, -0.6782,  0.7231,  ...,  0.4634,  0.5210, -0.6987],
        [ 0.1503, -0.3845,  0.3784,  ...,  0.6069,  0.4714, -0.5513]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([48, 512])
> 5.  tensor([[-0.1075, -0.4258,  0.5356,  ..., -0.1525,  0.1443, -1.0762],
        [-0.0227, -0.5430,  0.5249,  ...,  0.3315,  0.0948, -0.8638],
        [-0.0971, -0.5605,  0.4805,  ...,  0.4275,  0.4028, -0.8472],
        ...,
        [ 0.0098, -0.4583,  0.4636,  ...,  0.2434,  0.1209, -0.6040],
        [-0.1108, -0.4451,  0.4343,  ...,  0.4749,  0.1097, -0.9062],
        [ 0.0695, -0.7227,  0.4282,  ...,  0.1174,  0.2340, -1.1484]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1787, -0.5308,  0.5977,  ...,  0.5049,  0.3452, -0.7764],
        [-0.0747, -0.4390,  0.8311,  ...,  0.6450,  0.5093, -0.6895],
        [-0.1763, -0.8125,  0.7114,  ...,  0.4963,  0.6099, -0.6670],
        ...,
        [-0.0562, -0.6177,  0.6055,  ...,  0.6890,  0.6094, -0.7275],
        [-0.1405, -0.8745,  0.6001,  ...,  0.4636,  0.5205, -0.5156],
        [-0.1142, -0.6338,  0.5454,  ...,  0.5508,  0.6143, -0.5786]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([40, 512])
> 5.  tensor([[-0.0365, -0.4966,  0.3179,  ...,  0.4153,  0.2480, -0.7695],
        [ 0.0025, -0.5552,  0.4651,  ...,  0.4316,  0.2781, -0.8467],
        [-0.1360, -0.5161,  0.4766,  ...,  0.4314,  0.0869, -0.7007],
        ...,
        [-0.0202, -0.6421,  0.5312,  ...,  0.3167,  0.0247, -0.7915],
        [-0.0944, -0.5278,  0.4033,  ..., -0.2529,  0.2061, -0.9087],
        [-0.1272, -0.7061,  0.2703,  ..., -0.4187,  0.0910, -1.1201]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1105, -0.6440,  0.4868,  ...,  0.3169,  0.2311, -0.8560],
        [ 0.0800, -0.5864,  0.4368,  ...,  0.4897,  0.3809, -0.8228],
        [-0.0221, -0.6602,  0.4382,  ...,  0.5244,  0.2239, -0.6738],
        ...,
        [-0.1875, -0.8813,  0.4243,  ...,  0.4465,  0.3701, -0.9062],
        [-0.0461, -0.6812,  0.2761,  ...,  0.4495,  0.2839, -0.8237],
        [-0.1550, -0.9834,  0.6499,  ...,  0.4224,  0.2262, -0.7871]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.0568, -0.4951,  0.4624,  ...,  0.4680,  0.1946, -0.8262],
        [ 0.0689, -0.4888,  0.3374,  ...,  0.3252,  0.2264, -0.9551],
        [-0.0475, -0.5322,  0.3860,  ...,  0.3777,  0.1216, -0.8867],
        ...,
        [-0.0808, -0.5610,  0.3608,  ...,  0.5161,  0.1392, -0.8340],
        [-0.0565, -0.5361,  0.3516,  ...,  0.2549,  0.0925, -0.9976],
        [-0.1729, -0.5239,  0.5391,  ...,  0.3416,  0.1178, -0.8320]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0933, -0.3989,  0.5264,  ...,  0.3804,  0.2278, -0.8657],
        [-0.2103, -0.6221,  0.3792,  ...,  0.3726,  0.1716, -0.8140],
        [-0.1066, -0.5146,  0.4089,  ...,  0.2964,  0.2605, -0.6738],
        ...,
        [ 0.0115, -0.7334,  0.4985,  ...,  0.3052,  0.2379, -0.7769],
        [-0.1598, -0.5947,  0.4851,  ...,  0.2881,  0.1024, -0.7622],
        [-0.1388, -0.3418,  0.3596,  ...,  0.2180, -0.0146, -0.7256]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.2395, -0.8096,  0.9062,  ...,  0.3220,  0.6738, -0.5928],
        [-0.1763, -0.5747,  0.8281,  ...,  0.6050,  0.3584, -0.9302],
        [-0.2020, -0.4368,  0.6855,  ...,  0.5107,  0.5972, -0.6318],
        ...,
        [-0.0845, -0.5396,  0.5952,  ...,  0.5596,  0.3518, -0.6597],
        [-0.0191, -0.7847,  0.8389,  ...,  0.6172,  0.5903, -0.8120],
        [-0.2581, -0.9214,  0.8433,  ...,  0.4248,  0.5796, -0.7373]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([40, 512])
> 5.  tensor([[-0.2568, -0.6533,  1.0459,  ...,  0.1783,  0.3477, -0.7817],
        [-0.4392, -0.5532,  0.7979,  ...,  0.2883,  0.2188, -0.8662],
        [-0.2145, -0.9775,  0.6460,  ...,  0.1359,  0.1628, -0.8838],
        [-0.2076, -0.7222,  0.8433,  ...,  0.1001,  0.2432, -0.9712],
        [-0.0458, -0.9365,  0.8413,  ...,  0.0967,  0.2751, -1.1631],
        [-0.2815, -0.6675,  0.8604,  ...,  0.1588,  0.4705, -0.7227]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([6, 512])
> 5.  tensor([[ 0.0510, -0.5923,  0.2020,  ...,  0.3774,  0.1033, -0.9292],
        [ 0.0213, -0.4558,  0.1796,  ...,  0.4275,  0.0798, -0.6958],
        [-0.1195, -0.6006,  0.4402,  ...,  0.5503,  0.2063, -0.7119],
        ...,
        [ 0.0755, -0.5933,  0.2235,  ...,  0.2710,  0.2524, -0.9648],
        [-0.1050, -0.6665,  0.4060,  ...,  0.3921,  0.1581, -0.7026],
        [-0.0550, -0.6763,  0.4512,  ...,  0.3044,  0.0745, -0.8447]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0038, -0.5039,  0.4299,  ...,  0.3503,  0.2476, -0.8735],
        [-0.0750, -0.5767,  0.5449,  ..., -0.0118,  0.0635, -0.8149],
        [-0.0743, -0.7021,  0.5601,  ...,  0.3938,  0.2466, -0.9102],
        ...,
        [-0.2563, -0.6792,  0.5898,  ..., -0.0051,  0.2250, -1.0225],
        [-0.0828, -0.7554,  0.3726,  ...,  0.1572,  0.3113, -0.9976],
        [-0.1832, -0.4272,  0.5137,  ...,  0.3262,  0.0567, -0.7520]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.1870, -0.5278,  0.3083,  ...,  0.7495,  0.3916, -0.2072],
        [-0.2180, -0.2228,  0.2401,  ...,  0.6328,  0.8154, -0.5024],
        [-0.3049, -0.2460,  0.3425,  ...,  0.9014,  0.8154, -0.6152],
        ...,
        [-0.3208, -0.6230,  0.5122,  ...,  0.4841,  0.8408, -0.7524],
        [-0.2698, -0.2993,  0.7310,  ...,  0.6099,  0.8198, -0.6191],
        [-0.2062, -0.3120,  0.2325,  ...,  0.2219,  0.7129, -0.7310]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([72, 512])
> 5.  > 5.  tensor([[-0.1431, -0.6445,  0.5259,  ...,  0.5498,  0.0473, -0.7627],
        [-0.1580, -0.6348,  0.4280,  ...,  0.4280,  0.0084, -0.9375],
        [-0.3762, -0.5684,  0.3862,  ...,  0.4226,  0.2849, -0.7432],
        ...,
        [-0.1926, -0.7319,  0.3394,  ...,  0.4534,  0.0233, -0.8291],
        [-0.1130, -0.6255,  0.3977,  ...,  0.6758,  0.1076, -0.7480],
        [-0.1390, -0.5269,  0.3962,  ...,  0.4080, -0.0641, -0.8047]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0149, -0.4507,  0.6069,  ...,  0.4729,  0.3342, -0.7559],
        [ 0.0467, -0.5190,  0.5962,  ...,  0.5786,  0.8105, -0.6455],
        [-0.0365, -0.4062,  0.6411,  ...,  0.7026,  0.4268, -0.7720],
        ...,
        [-0.0117, -0.6509,  0.6797,  ...,  0.3596,  0.5571, -0.6582],
        [-0.0715, -0.5542,  0.3821,  ...,  0.4758,  0.4937, -0.7231],
        [-0.2310, -0.6338,  0.4924,  ...,  0.3147,  0.4851, -0.8706]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([40, 512])
> 5.  tensor([[-0.0013, -0.5913,  0.3276,  ...,  0.3550,  0.1242, -0.7251],
        [ 0.0283, -0.4602,  0.3196,  ...,  0.1497,  0.0870, -0.7793],
        [ 0.0689, -0.6401,  0.2234,  ...,  0.4412,  0.0914, -0.9370],
        ...,
        [-0.0339, -0.5811,  0.2615,  ...,  0.1448,  0.0903, -1.0215],
        [-0.1165, -0.5991,  0.0553,  ...,  0.1747,  0.0781, -0.9209],
        [-0.1140, -0.7520,  0.2610,  ..., -0.1512, -0.0547, -1.0098]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0032, -0.7100,  0.2668,  ...,  0.2922, -0.0339, -0.8828],
        [-0.1224, -0.5283,  0.3391,  ...,  0.2256,  0.0594, -0.8081],
        [ 0.0121, -0.6436,  0.4128,  ...,  0.1180,  0.0037, -0.7422],
        ...,
        [-0.0265, -0.5571,  0.2502,  ...,  0.3555, -0.0254, -0.8223],
        [-0.0166, -0.5859,  0.2203,  ...,  0.4324,  0.1237, -0.8647],
        [-0.0034, -0.5127,  0.3757,  ...,  0.3923,  0.0832, -0.8589]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.3679, -0.7280,  0.7134,  ...,  0.6177,  0.4421, -0.6299],
        [-0.0430, -0.5151,  0.6890,  ...,  0.5278,  0.1385, -0.6421],
        [-0.2151, -0.4998,  0.7466,  ...,  0.5151,  0.2408, -0.6689],
        ...,
        [-0.3679, -0.8188,  0.6914,  ...,  0.2869,  0.3196, -0.8677],
        [-0.1990, -0.6318,  0.7437,  ...,  0.3921,  0.2839, -0.8740],
        [-0.0535, -0.4448,  0.6606,  ...,  0.4590,  0.2329, -0.7920]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[-0.0593, -0.6245,  0.3369,  ...,  0.4434,  0.0554, -0.7056],
        [-0.0220, -0.4102,  0.2734,  ...,  0.4004,  0.0769, -0.7837],
        [-0.0924, -0.4722,  0.3142,  ...,  0.2451,  0.0023, -0.7993],
        ...,
        [-0.1660, -0.2983,  0.2583,  ...,  0.3193, -0.0273, -0.6865],
        [-0.0737, -0.4619,  0.2013,  ...,  0.3267, -0.0800, -0.8184],
        [-0.0897, -0.6553,  0.2856,  ...,  0.2170, -0.0659, -0.7725]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0398, -0.4985,  0.3047,  ...,  0.3110,  0.0262, -0.9380],
        [-0.0199, -0.5913,  0.2084,  ...,  0.4221,  0.0079, -0.6758],
        [ 0.0522, -0.5942,  0.2441,  ...,  0.4814,  0.0485, -0.8862],
        ...,
        [-0.1880, -0.5010,  0.3484,  ..., -0.0485,  0.1310, -1.0215],
        [ 0.0257, -0.4021,  0.1761,  ...,  0.5415, -0.0309, -0.7246],
        [ 0.0758, -0.5771,  0.1764,  ...,  0.4163,  0.0861, -0.6475]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0872, -0.4565,  0.4968,  ...,  0.1467,  0.1078, -0.8438],
        [-0.0506, -0.4968,  0.2776,  ...,  0.2050,  0.2290, -0.8462],
        [-0.0701, -0.4995,  0.3232,  ...,  0.4265, -0.0117, -0.8940],
        ...,
        [-0.0412, -0.5371,  0.3521,  ...,  0.2791,  0.1285, -0.9678],
        [ 0.1069, -0.8477,  0.4636,  ...,  0.2764,  0.0746, -1.1133],
        [-0.0119, -0.5747,  0.2661,  ...,  0.1793, -0.0686, -1.0117]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1531, -0.6167,  0.5820,  ...,  0.4048,  0.2715, -0.7285],
        [-0.0401, -0.6816,  0.3950,  ...,  0.4709,  0.2610, -0.8774],
        [-0.0666, -0.3850,  0.5581,  ...,  0.3713,  0.0429, -0.8286],
        ...,
        [-0.0405, -0.6885,  0.7036,  ...,  0.1956,  0.1555, -0.9106],
        [-0.0247, -0.4819,  0.4485,  ...,  0.1059,  0.0801, -0.5718],
        [-0.0386, -0.7964,  0.5156,  ...,  0.2281,  0.3267, -0.7202]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.1342, -0.6587,  0.7466,  ...,  0.3142,  0.0312, -0.7080],
        [ 0.0813, -0.8809,  0.5190,  ...,  0.3816,  0.1499, -0.6001],
        [-0.1288, -0.6050,  0.4246,  ...,  0.4319,  0.4272, -0.7627],
        ...,
        [-0.1300, -0.5947,  0.7319,  ...,  0.2922,  0.2695, -0.7905],
        [ 0.1267, -0.7026,  0.3577,  ...,  0.5020,  0.1940, -0.6475],
        [-0.0995, -0.6802,  0.3438,  ...,  0.5010,  0.3218, -0.6396]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.0529, -0.8291,  0.3677,  ...,  0.4934,  0.3235, -0.8096],
        [-0.1587, -0.7119,  0.5439,  ...,  0.6372,  0.1780, -0.5547],
        [-0.2329, -0.7085,  0.4966,  ...,  0.3987,  0.2764, -0.8999],
        ...,
        [-0.1606, -0.6367,  0.5190,  ...,  0.3008,  0.3447, -1.0312],
        [-0.1522, -0.7666,  0.3489,  ...,  0.7124,  0.0466, -0.6602],
        [ 0.0223, -0.6748,  0.4639,  ...,  0.4822,  0.2344, -0.8657]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[ 0.0417, -0.6636,  0.6660,  ...,  0.3848,  0.5591, -1.0117],
        [-0.2169, -0.5210,  0.7627,  ...,  0.4841,  0.6523, -0.7290],
        [-0.1589, -0.7017,  0.6001,  ...,  0.6030,  0.4333, -0.7573],
        ...,
        [-0.2524, -0.6948,  0.7153,  ...,  0.4583,  0.4099, -0.5820],
        [-0.2271, -0.6353,  0.5093,  ...,  0.4412,  0.5640, -0.6597],
        [-0.0705, -0.6040,  0.4844,  ...,  0.6338,  0.5308, -0.8018]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([40, 512])
> 5.  tensor([[ 0.0342, -0.8125,  0.4458,  ...,  0.4285,  0.1313, -0.8667],
        [-0.2212, -0.6426,  0.4167,  ...,  0.3853,  0.0210, -0.6753],
        [-0.0768, -0.7817,  0.5146,  ...,  0.1877,  0.0264, -0.8999],
        ...,
        [-0.2029, -0.7471,  0.2903,  ...,  0.5747,  0.1322, -0.7432],
        [-0.1937, -0.6792,  0.3625,  ...,  0.4236,  0.1466, -0.8726],
        [-0.0584, -0.5405,  0.3110,  ...,  0.4231,  0.1137, -0.8735]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0169, -0.4988,  0.1874,  ...,  0.4099,  0.0159, -0.8755],
        [ 0.0175, -0.4587,  0.3132,  ...,  0.2996,  0.0560, -0.8335],
        [-0.0689, -0.6709,  0.3855,  ...,  0.3689,  0.0877, -0.7773],
        ...,
        [-0.0320, -0.4607,  0.2659,  ...,  0.2438, -0.0899, -0.7861],
        [ 0.0987, -0.4871,  0.2734,  ...,  0.4094,  0.1517, -0.7534],
        [-0.0713, -0.5889,  0.3274,  ...,  0.3069,  0.1212, -1.0166]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1008, -0.5532,  0.7104,  ..., -0.0487,  0.0366, -0.7827],
        [ 0.0057, -0.5698,  0.6689,  ...,  0.2686,  0.4221, -0.6841],
        [-0.1359, -0.5835,  0.7104,  ...,  0.1484,  0.3342, -0.8833],
        ...,
        [-0.1387, -0.5933,  0.5103,  ...,  0.3440,  0.3245, -0.7319],
        [-0.0909, -0.4990,  0.5396,  ...,  0.4893,  0.2744, -0.7627],
        [-0.1807, -0.6895,  0.5547,  ...,  0.0862,  0.3079, -0.9521]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  > 5.  tensor([[ 4.3823e-02, -5.4541e-01,  3.7476e-01,  ...,  5.3760e-01,
          3.2520e-04, -8.6621e-01],
        [ 2.8748e-02, -5.9277e-01,  2.5732e-01,  ...,  3.0469e-01,
          7.3364e-02, -7.8076e-01],
        [-2.2919e-02, -4.3945e-01,  4.5288e-01,  ...,  1.7932e-01,
          1.8970e-01, -8.3154e-01],
        ...,
        [-5.5122e-03, -6.2158e-01,  3.7134e-01,  ...,  2.8833e-01,
          7.4097e-02, -8.0957e-01],
        [-2.9297e-02, -4.7339e-01,  3.3740e-01,  ...,  3.7476e-01,
         -9.2545e-03, -7.5684e-01],
        [-8.8928e-02, -4.6191e-01,  2.4072e-01,  ...,  5.4346e-01,
          1.5820e-01, -8.7158e-01]], device='cuda:7', dtype=torch.float16,
       grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0019, -0.6309,  0.5469,  ...,  0.2610,  0.2258, -0.6611],
        [-0.0107, -0.7227,  0.4443,  ...,  0.5562,  0.1986, -0.7144],
        [-0.0443, -0.7422,  0.5649,  ...,  0.6538,  0.1497, -0.8369],
        ...,
        [-0.2427, -0.6548,  0.6597,  ...,  0.3848,  0.2498, -0.6812],
        [-0.1656, -0.6909,  0.6758,  ...,  0.4160,  0.3552, -0.7310],
        [-0.1488, -0.7153,  0.4709,  ...,  0.4590,  0.3027, -0.5522]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.0195, -0.5200,  0.4319,  ...,  0.5083,  0.0961, -0.8130],
        [-0.0609, -0.5386,  0.3650,  ...,  0.3567,  0.0185, -0.9131],
        [-0.1965, -0.4700,  0.4021,  ...,  0.1875,  0.1190, -0.7646],
        ...,
        [-0.1823, -0.5083,  0.2549,  ...,  0.4177,  0.0851, -0.9038],
        [-0.1394, -0.5015,  0.4626,  ...,  0.3408, -0.0468, -0.8784],
        [-0.0107, -0.4482,  0.2939,  ...,  0.4229,  0.0424, -0.7012]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.3721, -0.5083,  0.5034,  ...,  0.1276,  0.9434, -0.5483],
        [-0.6343, -0.0782,  0.2019,  ...,  0.4336,  0.7749, -0.6597],
        [-0.5229, -0.5342,  0.6411,  ...,  0.4080,  0.4570, -0.5898],
        ...,
        [-0.3955, -0.4282,  0.3240,  ...,  0.1072,  0.4055, -0.6270],
        [-0.0959, -0.7388,  0.2649,  ...,  0.5405,  0.7417, -0.5601],
        [ 0.1036, -0.1401,  0.6348,  ...,  0.6934,  0.1261, -0.6138]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([88, 512])
> 5.  tensor([[-0.1597, -0.6357,  0.6357,  ..., -0.1135,  0.2052, -0.7847],
        [-0.1759, -0.3691,  0.8394,  ...,  0.3535,  0.2979, -0.7856],
        [-0.1295, -0.9805,  0.4851,  ..., -0.3696, -0.0313, -0.9937],
        ...,
        [-0.0582, -0.7305,  0.8755,  ...,  0.3208,  0.2800, -0.7036],
        [-0.1775, -0.6753,  0.8315,  ...,  0.2527,  0.1813, -0.7744],
        [-0.0427, -0.8672,  0.8433,  ...,  0.3577,  0.4019, -0.7402]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([7, 512])
> 5.  tensor([[ 0.0321, -0.5996,  0.2876,  ...,  0.4255,  0.0964, -0.8081],
        [ 0.0555, -0.7407,  0.3330,  ...,  0.3018,  0.1982, -0.8994],
        [ 0.0486, -0.6450,  0.3037,  ...,  0.2534,  0.1395, -0.8892],
        ...,
        [-0.0983, -0.6768,  0.2174,  ...,  0.0089,  0.1564, -1.0479],
        [-0.0248, -0.5918,  0.3362,  ...,  0.1162, -0.0466, -0.9272],
        [ 0.0211, -0.6431,  0.4126,  ...,  0.4421,  0.1349, -1.0078]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.2751, -0.5317,  0.4473,  ...,  0.2651,  0.0166, -0.7124],
        [-0.2213, -0.5220,  0.4177,  ...,  0.4636, -0.0172, -0.8130],
        [-0.3271, -0.5718,  0.3313,  ...,  0.3367, -0.0090, -0.7935],
        ...,
        [-0.2422, -0.4407,  0.4292,  ...,  0.2491, -0.0009, -0.7910],
        [-0.0777, -0.4624,  0.4199,  ...,  0.4719, -0.0036, -0.5996],
        [-0.1469, -0.3159,  0.3843,  ...,  0.4092, -0.1195, -0.7168]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0993, -0.6743,  0.4429,  ...,  0.4397,  0.2920, -0.8149],
        [-0.0378, -0.5166,  0.3523,  ...,  0.4910,  0.1561, -0.8149],
        [-0.0091, -0.5654,  0.3511,  ...,  0.3296,  0.1877, -0.9985],
        ...,
        [-0.1783, -0.6562,  0.6079,  ...,  0.1066,  0.0575, -0.9541],
        [-0.0266, -0.6016,  0.4844,  ...,  0.5117,  0.1862, -0.8667],
        [-0.0385, -0.6104,  0.4521,  ...,  0.3789,  0.2042, -0.9419]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0558, -0.6426,  0.7104,  ...,  0.3008,  0.2462, -0.7124],
        [-0.0403, -0.7061,  0.6357,  ...,  0.1742,  0.2003, -0.9424],
        [-0.1360, -0.5469,  0.4412,  ...,  0.0389,  0.1326, -0.8486],
        ...,
        [-0.2280, -0.5576,  0.6016,  ...,  0.0318,  0.2186, -0.9219],
        [-0.1421, -0.5059,  0.6562,  ...,  0.1692,  0.3062, -0.7681],
        [-0.1223, -0.6089,  0.6328,  ...,  0.3418,  0.2423, -0.6592]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0202, -0.5576,  0.3054,  ...,  0.1543,  0.0748, -0.8169],
        [-0.0089, -0.5996,  0.3540,  ...,  0.5044,  0.0771, -0.9595],
        [-0.0585, -0.5142,  0.3457,  ...,  0.5762, -0.0507, -0.8477],
        ...,
        [-0.0012, -0.5811,  0.3179,  ...,  0.3574,  0.1209, -0.8774],
        [ 0.0711, -0.5991,  0.4316,  ...,  0.4351,  0.0830, -0.8179],
        [ 0.0666, -0.5898,  0.2881,  ...,  0.3252,  0.0337, -0.8496]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0820, -0.4189,  0.5977,  ...,  0.0163, -0.0775, -0.6533],
        [-0.0587, -0.6733,  0.5952,  ...,  0.2798,  0.1980, -0.7402],
        [ 0.0056, -0.5044,  0.7485,  ...,  0.3564,  0.1813, -0.7319],
        ...,
        [-0.1699, -0.4924,  0.7310,  ...,  0.5005,  0.2480, -0.7788],
        [-0.1053, -0.5103,  0.6675,  ...,  0.1726,  0.1945, -0.6792],
        [-0.0012, -0.5864,  0.5957,  ...,  0.0215,  0.0619, -0.7017]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0503, -0.7212,  0.4980,  ...,  0.4590,  0.2996, -0.7832],
        [-0.0828, -0.5977,  0.4893,  ...,  0.4255,  0.3789, -0.6938],
        [-0.1859, -0.7886,  0.3445,  ...,  0.4124,  0.2979, -0.8315],
        ...,
        [-0.1584, -0.7983,  0.5781,  ...,  0.4807,  0.3809, -0.7661],
        [ 0.1403, -0.7593,  0.4836,  ...,  0.5913,  0.1234, -0.9023],
        [-0.0903, -1.0186,  0.4507,  ...,  0.4202,  0.2544, -0.8384]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[ 0.0753, -0.5825,  0.3513,  ...,  0.5513,  0.0557, -0.8096],
        [ 0.1050, -0.6465,  0.3291,  ...,  0.3501,  0.0183, -0.8643],
        [ 0.1157, -0.4915,  0.3240,  ...,  0.4304, -0.0346, -0.9277],
        ...,
        [-0.1565, -0.5898,  0.3508,  ...,  0.3574,  0.1136, -0.9722],
        [-0.0286, -0.5098,  0.3447,  ...,  0.3364, -0.0508, -0.8750],
        [-0.0231, -0.4673,  0.1981,  ...,  0.2610,  0.2695, -0.9199]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1980, -0.5581,  0.6362,  ...,  0.3479,  0.2559, -0.7681],
        [-0.1942, -0.6450,  0.7393,  ...,  0.3999,  0.3220, -0.7593],
        [-0.2866, -0.5474,  0.6494,  ...,  0.4641,  0.2729, -0.8457],
        ...,
        [-0.1244, -0.8564,  0.8682,  ...,  0.5693,  0.2341, -0.5132],
        [-0.2396, -0.7363,  0.7627,  ...,  0.4668, -0.0298, -0.5601],
        [-0.0216, -0.6333,  0.3896,  ...,  0.5630,  0.2183, -0.7061]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[ 0.0056, -0.4929,  0.2810,  ...,  0.3230,  0.0617, -0.7383],
        [ 0.1240, -0.6626,  0.3301,  ...,  0.2352,  0.1497, -0.9722],
        [ 0.0145, -0.6255,  0.2849,  ...,  0.4436,  0.1666, -0.9033],
        ...,
        [-0.1558, -0.5830,  0.4841,  ...,  0.3647,  0.1469, -0.8760],
        [ 0.1196, -0.5767,  0.4131,  ...,  0.1384,  0.0026, -0.9341],
        [-0.0298, -0.6797,  0.3223,  ...,  0.3789,  0.1223, -0.7651]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  > 5.  tensor([[-0.1698, -0.9146,  0.4885,  ...,  0.3599,  0.0948, -0.6938],
        [-0.1923, -0.8057,  0.4536,  ...,  0.5552,  0.1785, -0.7734],
        [-0.1399, -0.6982,  0.3623,  ...,  0.2629,  0.1763, -0.7397],
        ...,
        [-0.1677, -0.9028,  0.3879,  ...,  0.4775,  0.1716, -0.8315],
        [-0.0809, -0.7729,  0.4739,  ...,  0.5405,  0.2213, -0.7432],
        [-0.0215, -0.8530,  0.3977,  ...,  0.3909,  0.1534, -0.7915]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0085, -0.6123,  0.4871,  ...,  0.5991,  0.1580, -0.6797],
        [ 0.1389, -0.6255,  0.5054,  ...,  0.3242,  0.2357, -0.4583],
        [-0.0401, -0.5977,  0.6406,  ...,  0.4573,  0.2008, -0.4814],
        ...,
        [-0.1393, -0.8667,  0.4785,  ...,  0.5254,  0.1967, -0.8032],
        [ 0.0837, -0.5269,  0.4751,  ...,  0.1941,  0.1646, -0.8071],
        [ 0.1575, -0.8135,  0.3616,  ...,  0.3062,  0.2208, -0.7173]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.1844, -0.3979,  0.2302,  ...,  0.2979, -0.1298, -0.7554],
        [-0.1584, -0.6504,  0.3037,  ...,  0.3389,  0.0058, -0.7803],
        [-0.2220, -0.5898,  0.3093,  ...,  0.3472, -0.0082, -0.7759],
        ...,
        [-0.0797, -0.6177,  0.3713,  ...,  0.4944,  0.1620, -0.7407],
        [-0.2659, -0.6055,  0.3013,  ...,  0.1810,  0.0757, -0.8154],
        [-0.2421, -0.5474,  0.1614,  ...,  0.3193,  0.1846, -0.7666]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1241, -0.7651,  0.7896,  ...,  0.1958,  0.4265, -0.9268],
        [-0.2445, -0.7319,  0.7881,  ...,  0.2615,  0.1635, -0.6802],
        [-0.1055, -0.9136,  0.5635,  ..., -0.4326,  0.1957, -1.0361],
        ...,
        [-0.0661, -0.7085,  0.7358,  ...,  0.4661,  0.2394, -0.9009],
        [-0.1761, -0.7788,  0.6606,  ...,  0.3523,  0.3564, -0.7148],
        [-0.1355, -0.6255,  0.4246,  ...,  0.1652,  0.6318, -0.8203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0215, -0.3979,  0.3137,  ...,  0.2920,  0.0782, -0.5156],
        [-0.0125, -0.3362,  0.2620,  ...,  0.1954, -0.0191, -0.8394],
        [-0.1523, -0.6377,  0.4282,  ...,  0.2561, -0.0417, -0.8286],
        ...,
        [ 0.1249, -0.5957,  0.3323,  ...,  0.2642,  0.0933, -0.8906],
        [-0.0241, -0.4856,  0.3022,  ...,  0.3660,  0.2202, -0.8389],
        [-0.0021, -0.5107,  0.2316,  ...,  0.5312,  0.0311, -0.7891]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0687, -0.7065,  0.6353,  ...,  0.4932,  0.4011, -0.7939],
        [-0.2260, -0.7012,  0.7798,  ...,  0.6187,  0.6543, -0.7163],
        [-0.1411, -0.5234,  0.6758,  ...,  0.5630,  0.3459, -0.8193],
        ...,
        [-0.0934, -0.7065,  0.6836,  ...,  0.6182,  0.5581, -0.7686],
        [-0.2454, -0.4285,  0.6143,  ...,  0.4773,  0.4424, -0.8027],
        [-0.1919, -0.5957,  0.5244,  ...,  0.2883,  0.4292, -0.5562]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[ 0.0128, -0.5273,  0.3289,  ...,  0.4802,  0.0865, -0.8794],
        [-0.0286, -0.3567,  0.2729,  ...,  0.3474, -0.1216, -0.8530],
        [ 0.0036, -0.5757,  0.3193,  ...,  0.2805, -0.0110, -0.7661],
        ...,
        [-0.0124, -0.4395,  0.3279,  ...,  0.5259,  0.1478, -0.8364],
        [-0.1552, -0.4839,  0.3059,  ...,  0.2356, -0.1132, -0.8828],
        [-0.1620, -0.5957,  0.2319,  ...,  0.3567,  0.0973, -0.9600]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.2842, -0.6626,  0.5518,  ...,  0.4753,  0.2474, -0.7422],
        [-0.2939, -0.5518,  0.5249,  ...,  0.5200,  0.0575, -0.6890],
        [-0.3252, -0.5273,  0.2397,  ...,  0.5029,  0.2084, -0.8364],
        ...,
        [-0.0086, -0.7222,  0.7979,  ...,  0.5269,  0.3152, -1.0547],
        [-0.1558, -0.6328,  0.5742,  ...,  0.4990,  0.2015, -0.6738],
        [-0.2404, -0.7632,  0.6934,  ...,  0.4539,  0.2495, -0.9365]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[-0.1254, -0.5977,  0.3406,  ...,  0.3806,  0.0101, -0.7827],
        [-0.0677, -0.6777,  0.3357,  ...,  0.5386,  0.1260, -0.8418],
        [-0.0598, -0.4612,  0.4292,  ...,  0.5439,  0.0099, -0.7397],
        ...,
        [-0.2098, -0.6372,  0.4395,  ...,  0.3145,  0.0897, -0.8364],
        [-0.2588, -0.5474,  0.2062,  ...,  0.4893, -0.0234, -0.8794],
        [-0.2201, -0.5269,  0.2328,  ...,  0.3198, -0.0817, -0.6060]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0591, -0.5928,  0.3579,  ...,  0.4780,  0.3474, -0.7163],
        [-0.0655, -0.7520,  0.4185,  ...,  0.3718,  0.1310, -0.8032],
        [-0.0873, -0.7925,  0.5630,  ...,  0.3557,  0.2020, -0.6157],
        ...,
        [-0.1490, -0.7124,  0.4067,  ...,  0.3850,  0.2751, -0.7769],
        [-0.1006, -0.6538,  0.5044,  ...,  0.3667,  0.2739, -0.8462],
        [-0.1409, -0.8818,  0.5039,  ...,  0.4099,  0.2539, -0.7930]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.4102, -0.8926,  1.3311,  ...,  0.2883,  0.1632, -0.7788],
        [-0.3625, -0.9492,  0.8770,  ...,  0.1619,  0.2703, -1.1973],
        [-0.2585, -0.9258,  0.6357,  ...,  0.4971,  0.3364, -0.8872],
        [-0.4133, -0.7876,  1.0156,  ...,  0.1835,  0.1825, -1.0654],
        [-0.3804, -0.8613,  1.2207,  ...,  0.5547,  0.1179, -0.8926]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([5, 512])
> 5.  tensor([[-0.1719, -0.7900,  0.3716,  ...,  0.2429,  0.0805, -0.8770],
        [ 0.0224, -0.7354,  0.3862,  ...,  0.3423,  0.0225, -0.8525],
        [ 0.0228, -0.4858,  0.4670,  ...,  0.1270,  0.0695, -0.8081],
        ...,
        [-0.0628, -0.6055,  0.3096,  ...,  0.3257,  0.0054, -0.9102],
        [ 0.0457, -0.6543,  0.3127,  ...,  0.2262,  0.1072, -0.7739],
        [ 0.1151, -0.3481,  0.1979,  ...,  0.6543,  0.1191, -0.6826]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1217, -0.5903,  0.5117,  ...,  0.4563,  0.6855, -0.4602],
        [ 0.0880, -0.5049,  0.6758,  ...,  0.6714,  0.5073, -0.6387],
        [-0.0367, -0.6372,  0.6440,  ...,  0.5894,  0.7354, -0.8335],
        ...,
        [-0.1013, -0.5332,  0.7471,  ...,  0.6567,  0.3862, -0.8335],
        [-0.1693, -0.7227,  0.8008,  ...,  0.4436,  0.3191, -0.7217],
        [-0.1462, -0.5938,  0.5791,  ...,  0.4104,  0.5718, -0.7358]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([40, 512])
> 5.  tensor([[-0.1727, -0.7720,  0.7266,  ...,  0.3706,  0.2671, -0.7788],
        [-0.1129, -0.4819,  0.7969,  ...,  0.0712,  0.1890, -0.7954],
        [-0.1978, -0.4934,  0.7183,  ...,  0.3958,  0.2727, -0.7178],
        ...,
        [ 0.0342, -0.7578,  0.7173,  ...,  0.4097,  0.3655, -0.9443],
        [-0.2493, -0.5874,  0.6631,  ...,  0.3110,  0.2959, -0.7295],
        [-0.2075, -0.3530,  0.5400,  ...,  0.1477,  0.2142, -0.8682]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1591, -0.5996,  0.7158,  ...,  0.3118,  0.1183, -0.7837],
        [-0.1139, -0.5781,  0.5684,  ...,  0.2954,  0.1492, -0.8301],
        [-0.0351, -0.5176,  0.5728,  ...,  0.2188,  0.1270, -0.8140],
        ...,
        [-0.1481, -0.5869,  0.6860,  ...,  0.2074,  0.2096, -0.8145],
        [ 0.0430, -0.6699,  0.5967,  ...,  0.3506,  0.3447, -0.8613],
        [-0.0846, -0.5122,  0.6206,  ...,  0.0420,  0.2200, -0.7393]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0620, -0.6514,  0.4485,  ...,  0.4048,  0.1137, -0.8101],
        [ 0.1028, -0.6934,  0.2307,  ...,  0.3740,  0.1265, -0.9541],
        [-0.0124, -0.4529,  0.3655,  ...,  0.4819,  0.0428, -0.7686],
        ...,
        [-0.0151, -0.4070,  0.3374,  ...,  0.3022,  0.0464, -0.8730],
        [-0.0958, -0.5396,  0.3298,  ...,  0.3657,  0.0429, -0.7056],
        [ 0.0795, -0.5439,  0.3584,  ...,  0.3552, -0.0227, -0.7549]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0346, -0.5933,  0.4805,  ...,  0.4868,  0.3716, -0.8110],
        [-0.1562, -0.6841,  0.6064,  ...,  0.5908,  0.3484, -0.7788],
        [-0.2598, -0.5728,  0.4771,  ...,  0.4292,  0.4351, -0.8701],
        ...,
        [-0.3533, -0.6104,  0.5356,  ...,  0.4546,  0.2382, -0.8687],
        [-0.1473, -0.6196,  0.7109,  ...,  0.6362,  0.4592, -0.8242],
        [-0.1986, -0.5146,  0.8354,  ...,  0.5732,  0.3611, -0.6372]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[-0.4868, -0.6206,  0.2913,  ..., -0.2700,  0.0101, -0.0442],
        [-0.4309,  0.1534,  0.3076,  ...,  0.6592,  0.6318, -0.3464],
        [-0.3467, -0.7051,  0.0389,  ...,  0.1737,  0.9346, -0.5996],
        ...,
        [-0.4607, -0.8154,  0.4351,  ...,  0.5044,  0.4705, -0.2446],
        [-0.4475,  0.0078, -0.1172,  ...,  0.0289,  0.8203, -0.4189],
        [-0.3298, -0.5996,  0.2017,  ...,  0.4526,  1.3174, -0.8525]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([152, 512])
2023-07-11 08:45:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
tensor([[-0.1018, -0.6416,  0.2566,  ...,  0.4128,  0.0315, -0.6211],
        [-0.2534, -0.4893,  0.2632,  ...,  0.1517, -0.0964, -0.7422],
        [-0.0886, -0.6997,  0.2217,  ...,  0.2822,  0.0767, -0.8521],
        ...,
        [-0.2710, -0.7168,  0.4204,  ...,  0.3716, -0.0176, -0.7524],
        [-0.1107, -0.4456,  0.3743,  ...,  0.1672,  0.0412, -0.8677],
        [-0.1449, -0.3977,  0.1246,  ...,  0.3191, -0.0262, -0.8311]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[ 0.0204, -0.4558,  0.5684,  ...,  0.3005,  0.1666, -0.7480],
        [-0.2646, -0.6157,  0.2455,  ..., -0.1685,  0.0332, -0.7842],
        [-0.0743, -0.3433,  0.5708,  ...,  0.2959,  0.2484, -0.6201],
        ...,
        [-0.0393, -0.5703,  0.5176,  ...,  0.5200,  0.3545, -0.8599],
        [-0.0823, -0.4963,  0.4426,  ...,  0.3127,  0.3228, -0.8994],
        [-0.1251, -0.5356,  0.4502,  ...,  0.2578,  0.2759, -0.7812]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1935, -0.6353,  0.8218,  ...,  0.4658,  0.4365, -0.7349],
        [-0.3433, -0.5254,  0.7817,  ...,  0.3032,  0.2090, -0.8062],
        [-0.1169, -0.8047,  0.6440,  ...,  0.5679,  0.2732, -0.8271],
        ...,
        [-0.1812, -0.8848,  0.5933,  ...,  0.4834,  0.3176, -0.7515],
        [-0.1720, -0.8599,  0.5894,  ...,  0.4673,  0.1760, -0.8823],
        [-0.2551, -0.4800,  0.8535,  ...,  0.6870,  0.4043, -0.8291]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[-0.0176, -0.9048,  0.4165,  ...,  0.2505,  0.1504, -0.8540],
        [ 0.1016, -0.4751,  0.3069,  ...,  0.3875,  0.1323, -0.7319],
        [-0.1843, -0.4509,  0.3679,  ...,  0.4766,  0.2544, -0.9268],
        ...,
        [-0.1644, -0.5703,  0.3804,  ...,  0.0027,  0.0669, -0.9976],
        [-0.0159, -0.4631,  0.5503,  ...,  0.4128,  0.1218, -0.8369],
        [-0.1725, -0.6079,  0.3853,  ...,  0.4685,  0.2559, -0.9937]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0445, -0.6084,  0.7622,  ...,  0.4863,  0.6387, -0.8838],
        [ 0.1272, -0.6030,  0.4023,  ...,  0.4036,  0.5132, -0.4128],
        [-0.2340, -0.3394,  0.4526,  ...,  0.4480,  0.4663, -0.6909],
        ...,
        [-0.1956, -0.6431,  0.5967,  ...,  0.5176,  0.4263, -0.5171],
        [-0.1221, -0.4966,  0.7437,  ...,  0.7207,  0.5308, -0.6836],
        [-0.0411, -0.3501,  0.5220,  ...,  0.4387,  0.6680, -0.5464]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([40, 512])
> 5.  tensor([[-0.2871, -0.9966,  0.9512,  ...,  0.1748,  0.4292, -0.9126],
        [-0.0786, -0.6162,  0.8862,  ...,  0.1382,  0.1674, -0.7964],
        [-0.5063, -0.7412,  1.0176,  ..., -0.0866,  0.3010, -0.9731],
        [-0.2744, -1.0537,  0.9233,  ...,  0.3118,  0.2847, -0.7095],
        [-0.2869, -0.9165,  0.8027,  ...,  0.2668,  0.2722, -0.8560],
        [ 0.0046, -0.9287,  0.8896,  ...,  0.3552,  0.3472, -0.8794]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([6, 512])
> 5.  tensor([[-0.1554, -0.5552,  0.5044,  ...,  0.2052,  0.1542, -0.8188],
        [-0.1086, -0.4727,  0.4922,  ...,  0.3140,  0.2402, -0.8296],
        [-0.0428, -0.6260,  0.4790,  ..., -0.0299,  0.2134, -0.8999],
        ...,
        [-0.1345, -0.4814,  0.6045,  ...,  0.4172,  0.2157, -0.8667],
        [-0.2262, -0.4287,  0.4978,  ...,  0.0576,  0.1589, -0.7515],
        [-0.1388, -0.3120,  0.5591,  ...,  0.1359,  0.2460, -0.8081]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0714, -0.6182,  0.2191,  ...,  0.5894,  0.0251, -0.6738],
        [ 0.0109, -0.6040,  0.2935,  ...,  0.3840,  0.1749, -0.8462],
        [ 0.0667, -0.7412,  0.4304,  ...,  0.2133,  0.0283, -0.8296],
        ...,
        [-0.0037, -0.7119,  0.1777,  ...,  0.4607, -0.1006, -0.7661],
        [-0.0370, -0.4287,  0.2512,  ...,  0.1593,  0.0652, -0.8052],
        [-0.0446, -0.5410,  0.2703,  ...,  0.4233,  0.1210, -0.7124]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.2837, -0.5820,  0.3718,  ...,  0.3840, -0.0170, -0.8125],
        [-0.1564, -0.5908,  0.5034,  ...,  0.3857, -0.0029, -0.6802],
        [-0.1628, -0.5732,  0.2214,  ...,  0.5488,  0.0184, -0.9536],
        ...,
        [-0.2375, -0.7573,  0.3354,  ...,  0.3757,  0.1327, -0.8213],
        [-0.1641, -0.6016,  0.2478,  ...,  0.5361, -0.0797, -0.6416],
        [-0.3152, -0.5137,  0.3850,  ...,  0.4009,  0.0550, -0.7056]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[ 1.7899e-02, -5.0586e-01,  3.6890e-01,  ...,  3.3081e-01,
          3.0212e-02, -8.5205e-01],
        [-7.5928e-02, -5.9131e-01,  4.0820e-01,  ...,  1.9299e-01,
          2.5171e-01, -1.0312e+00],
        [ 1.1589e-02, -3.0713e-01,  4.8950e-01,  ...,  2.9663e-01,
          5.5847e-02, -7.5830e-01],
        ...,
        [ 1.4290e-02, -6.1914e-01,  5.0293e-01,  ...,  3.0737e-01,
          1.1639e-01, -1.0098e+00],
        [ 6.2084e-04, -6.6406e-01,  4.7437e-01,  ...,  3.8013e-01,
          9.0820e-02, -8.5547e-01],
        [-4.1847e-03, -6.3574e-01,  4.4238e-01,  ...,  3.8892e-01,
          1.4221e-01, -8.6523e-01]], device='cuda:4', dtype=torch.float16,
       grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0050, -0.5430,  0.3467,  ...,  0.4233, -0.0997, -0.7554],
        [ 0.0304, -0.6504,  0.3638,  ...,  0.2612,  0.0278, -0.6533],
        [-0.0569, -0.4368,  0.3320,  ...,  0.4807, -0.1221, -0.7266],
        ...,
        [-0.0496, -0.4685,  0.3545,  ...,  0.5239, -0.0439, -0.7520],
        [ 0.0262, -0.5552,  0.2152,  ...,  0.3279,  0.0316, -0.8267],
        [-0.0643, -0.4485,  0.2302,  ...,  0.4417, -0.1515, -0.7944]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.2847, -0.8169,  0.5845,  ...,  0.3193,  0.0227, -0.8354],
        [-0.2512, -0.5229,  0.4033,  ...,  0.2847,  0.0926, -0.7935],
        [-0.1209, -0.5581,  0.5444,  ...,  0.3718, -0.0239, -0.9722],
        ...,
        [-0.1418, -0.6226,  0.4490,  ...,  0.3196,  0.0759, -0.9341],
        [-0.2761, -0.7715,  0.5303,  ...,  0.3760, -0.0275, -0.9780],
        [-0.0174, -0.7515,  0.2673,  ...,  0.5435,  0.0750, -0.7939]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[ 0.0527, -0.6704,  0.3921,  ...,  0.2358,  0.1102, -0.8140],
        [-0.0670, -0.4858,  0.2944,  ...,  0.3423,  0.2423, -0.8574],
        [ 0.0314, -0.6865,  0.3049,  ...,  0.2458,  0.0729, -0.9575],
        ...,
        [ 0.0942, -0.5566,  0.4155,  ...,  0.3638,  0.0330, -0.8320],
        [-0.0320, -0.4612,  0.3374,  ...,  0.3865,  0.0280, -0.7241],
        [-0.0656, -0.5337,  0.2756,  ...,  0.3105, -0.0350, -0.6836]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0867, -0.4587,  0.1925,  ...,  0.3938,  0.0353, -0.7202],
        [ 0.0111, -0.5200,  0.4204,  ...,  0.3728,  0.0558, -0.7656],
        [-0.0121, -0.5298,  0.3347,  ...,  0.4419,  0.0635, -0.6831],
        ...,
        [ 0.0230, -0.6748,  0.4299,  ...,  0.3672,  0.0089, -0.9331],
        [-0.0400, -0.4670,  0.2966,  ...,  0.2883, -0.0745, -0.8647],
        [-0.1248, -0.5459,  0.1929,  ...,  0.3416,  0.0018, -0.8760]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1138, -0.5596,  0.1661,  ...,  0.3767, -0.0542, -0.8369],
        [ 0.0210, -0.6372,  0.2249,  ...,  0.3403,  0.1678, -0.8140],
        [-0.0596, -0.5581,  0.1987,  ...,  0.6060,  0.0221, -0.9067],
        ...,
        [ 0.0613, -0.5933,  0.3958,  ...,  0.2646,  0.0551, -0.7148],
        [-0.0677, -0.5669,  0.3474,  ...,  0.4116,  0.0507, -0.6318],
        [ 0.0282, -0.5796,  0.4167,  ...,  0.4470,  0.0379, -0.6865]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-1.2842e-01, -6.7773e-01,  7.3633e-01,  ..., -1.4636e-01,
          2.0178e-01, -8.1445e-01],
        [-4.1553e-01, -7.7832e-01,  9.0869e-01,  ...,  6.5674e-02,
          1.3013e-01, -1.0781e+00],
        [-2.2131e-01, -6.0986e-01,  9.2969e-01,  ...,  4.8351e-04,
          2.3608e-01, -7.3047e-01],
        ...,
        [-1.1395e-01, -7.0752e-01,  9.9805e-01,  ...,  4.0112e-01,
          1.9299e-01, -8.1152e-01],
        [-2.9907e-01, -9.2969e-01,  8.1641e-01,  ...,  1.1481e-01,
          3.7036e-01, -8.8721e-01],
        [-5.4688e-01, -7.2070e-01,  9.6826e-01,  ...,  2.6685e-01,
          2.3877e-01, -9.8291e-01]], device='cuda:1', dtype=torch.float16,
       grad_fn=<DivBackward0>) torch.Size([7, 512])
> 5.  tensor([[-0.1614, -0.6812,  0.4333,  ...,  0.2944,  0.6758, -0.7759],
        [-0.0142, -0.6191,  0.3826,  ...,  0.3794,  0.6021, -0.6313],
        [-0.0516, -0.6938,  0.6133,  ...,  0.5820,  0.4121, -0.7778],
        ...,
        [-0.1676, -0.7651,  0.7690,  ...,  0.5264,  0.2920, -0.5576],
        [-0.0853, -0.5010,  0.4294,  ...,  0.3357,  0.4731, -0.8022],
        [ 0.0659, -0.4229,  0.6528,  ...,  0.7324,  0.4438, -0.7173]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([40, 512])
> 5.  tensor([[ 0.0325, -0.6694,  0.3198,  ...,  0.3135,  0.0884, -0.9097],
        [ 0.0662, -0.4583,  0.2815,  ...,  0.3066,  0.0019, -0.7886],
        [ 0.0368, -0.5186,  0.3364,  ...,  0.4390,  0.0734, -0.8135],
        ...,
        [-0.1575, -0.6807,  0.3064,  ...,  0.4351, -0.0154, -0.7236],
        [-0.1172, -0.6812,  0.3071,  ...,  0.3835, -0.1002, -0.7773],
        [-0.0519, -0.7319,  0.3557,  ...,  0.0024,  0.0405, -0.9409]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0533, -0.5513,  0.3113,  ...,  0.4348,  0.1203, -0.7861],
        [ 0.1024, -0.6484,  0.2949,  ...,  0.5391, -0.0450, -0.8320],
        [ 0.0471, -0.4458,  0.2383,  ...,  0.3672, -0.0673, -0.6655],
        ...,
        [-0.1508, -0.5562,  0.4082,  ...,  0.2335,  0.0479, -0.8853],
        [ 0.0826, -0.5669,  0.2462,  ...,  0.4517,  0.1359, -0.7290],
        [ 0.0895, -0.4719,  0.2544,  ...,  0.4849,  0.1675, -0.7861]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0739, -0.5820,  0.5747,  ..., -0.1974,  0.2568, -1.0303],
        [-0.2178, -0.6211,  0.4709,  ...,  0.3999,  0.2793, -0.8711],
        [-0.0368, -0.7949,  0.6426,  ...,  0.3164,  0.3379, -0.8247],
        ...,
        [-0.1583, -0.6094,  0.7510,  ...,  0.2399,  0.2476, -0.8408],
        [-0.1569, -0.6182,  0.5415,  ...,  0.0437,  0.2432, -1.0342],
        [-0.2524, -0.5630,  0.7739,  ...,  0.3989,  0.3486, -0.7358]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1290, -0.7275,  0.4929,  ...,  0.1975,  0.0507, -0.8555],
        [-0.2319, -0.6523,  0.2515,  ...,  0.3091,  0.0481, -0.8071],
        [-0.0922, -0.5977,  0.3052,  ...,  0.4521, -0.0398, -0.7783],
        ...,
        [-0.2119, -0.6294,  0.5225,  ...,  0.4277,  0.1260, -0.9468],
        [-0.2242, -0.7227,  0.2812,  ...,  0.3508,  0.0660, -0.9092],
        [-0.1976, -0.6025,  0.2532,  ...,  0.4558,  0.0875, -0.8477]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0637, -0.6851,  0.5762,  ...,  0.5181,  0.1624, -0.7856],
        [-0.2122, -0.7432,  0.6709,  ...,  0.3318,  0.2976, -0.8369],
        [-0.0972, -0.6611,  0.6426,  ...,  0.4004,  0.2041, -0.5557],
        ...,
        [-0.2258, -0.5288,  0.8613,  ...,  0.5000,  0.1331, -0.7124],
        [-0.2603, -0.7905,  0.7183,  ...,  0.3303,  0.2335, -1.1592],
        [-0.2888, -0.7344,  0.6997,  ...,  0.2482,  0.1039, -0.8335]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.2983, -1.0771,  1.0928,  ...,  0.0649,  0.2502, -1.1523],
        [-0.3950, -0.9873,  0.9058,  ...,  0.6597,  0.4595, -0.9336],
        [-0.5371, -0.8755,  1.0469,  ...,  0.3552,  0.5244, -0.8413],
        [-0.4194, -0.9453,  1.2510,  ..., -0.1448, -0.0757, -1.2256],
        [-0.4592, -0.8438,  1.1143,  ...,  0.3892,  0.2095, -1.0771]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([5, 512])
> 5.  tensor([[-0.1962, -0.7969,  0.4824,  ...,  0.4897,  0.1904, -0.7729],
        [-0.2037, -0.7446,  0.2274,  ...,  0.2905,  0.0111, -0.8140],
        [-0.1063, -0.6436,  0.2424,  ...,  0.5352,  0.0265, -0.5938],
        ...,
        [-0.2512, -0.6406,  0.4795,  ...,  0.2793,  0.1765, -0.6519],
        [-0.2003, -0.6855,  0.4739,  ...,  0.4011,  0.0344, -0.9561],
        [-0.2268, -0.7114,  0.3997,  ...,  0.4731,  0.2136, -0.7598]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0449, -0.8589,  0.5181,  ...,  0.4658,  0.0292, -0.9619],
        [-0.1338, -0.8003,  0.2795,  ...,  0.4722,  0.0672, -0.8672],
        [-0.1772, -0.7778,  0.4924,  ...,  0.4597,  0.2374, -0.7109],
        ...,
        [-0.2496, -0.7451,  0.3535,  ...,  0.4775,  0.1411, -0.8027],
        [-0.0813, -0.4958,  0.3145,  ...,  0.3677,  0.1227, -0.7007],
        [-0.1331, -0.8994,  0.3418,  ...,  0.3945,  0.2188, -1.0020]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1926, -0.6021,  0.4397,  ...,  0.3254,  0.0358, -0.8457],
        [-0.1536, -0.6416,  0.5454,  ...,  0.3550,  0.0840, -0.8838],
        [-0.3206, -0.5073,  0.2698,  ...,  0.4399,  0.0152, -0.8584],
        ...,
        [-0.2844, -0.5918,  0.3308,  ...,  0.1837, -0.0420, -0.9067],
        [-0.2029, -0.5039,  0.2849,  ...,  0.5771,  0.1176, -0.7803],
        [-0.1359, -0.6206,  0.3069,  ...,  0.2272, -0.1423, -0.7344]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.2086, -0.3647,  0.5474,  ...,  0.3250,  0.2698, -0.8315],
        [ 0.1233, -0.5317,  0.5928,  ...,  0.1902,  0.2113, -0.7168],
        [-0.0779, -0.4663,  0.5005,  ...,  0.3193,  0.1348, -0.8633],
        ...,
        [-0.0368, -0.5474,  0.6138,  ...,  0.2366,  0.3284, -0.8843],
        [-0.2046, -0.5645,  0.4880,  ...,  0.2554,  0.2488, -0.9287],
        [-0.1008, -0.4392,  0.5054,  ...,  0.1315,  0.2363, -0.7603]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1609, -0.6440,  0.3789,  ...,  0.4783, -0.0649, -0.7842],
        [-0.0572, -0.4548,  0.1799,  ...,  0.4556, -0.0586, -0.8999],
        [-0.1543, -0.4722,  0.2871,  ...,  0.3550,  0.0522, -0.9297],
        ...,
        [-0.0660, -0.5186,  0.2854,  ...,  0.5122, -0.0051, -0.9165],
        [-0.0878, -0.6089,  0.4417,  ...,  0.3132,  0.0440, -0.8740],
        [-0.1036, -0.6289,  0.2766,  ...,  0.2211, -0.1760, -0.8696]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0582, -0.4497,  0.3015,  ...,  0.2399, -0.2488, -0.8057],
        [-0.1285, -0.5859,  0.3467,  ...,  0.5615,  0.0197, -0.8799],
        [-0.1871, -0.4211,  0.1060,  ...,  0.2230, -0.0901, -0.8960],
        ...,
        [-0.0036, -0.5127,  0.3311,  ...,  0.1725, -0.1289, -0.7734],
        [-0.1890, -0.4849,  0.3157,  ...,  0.3540, -0.0751, -0.6948],
        [-0.1138, -0.4995,  0.3450,  ...,  0.2369, -0.1125, -0.8501]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0609, -0.6250,  0.4309,  ...,  0.3931,  0.0453, -0.7930],
        [ 0.0079, -0.5176,  0.1998,  ...,  0.5356,  0.1011, -0.8267],
        [-0.0159, -0.4883,  0.2737,  ...,  0.4524, -0.0499, -0.7856],
        ...,
        [-0.1395, -0.5386,  0.3142,  ...,  0.2443, -0.0305, -0.9058],
        [ 0.0401, -0.6499,  0.2957,  ...,  0.3794, -0.0242, -0.8677],
        [-0.0623, -0.7485,  0.3193,  ...,  0.3596,  0.0653, -1.0225]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0077, -0.4702,  0.3159,  ...,  0.4675,  0.0560, -0.9551],
        [-0.0444, -0.6416,  0.3372,  ...,  0.3062,  0.0350, -0.7974],
        [-0.0812, -0.4360,  0.2357,  ...,  0.4353,  0.0058, -0.8662],
        ...,
        [-0.0532, -0.4148,  0.2698,  ...,  0.4644, -0.0622, -0.8081],
        [-0.1240, -0.5820,  0.4358,  ...,  0.1525,  0.0644, -1.1807],
        [-0.0429, -0.5957,  0.1854,  ...,  0.3577, -0.0549, -0.7588]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1213, -0.7422,  0.7168,  ...,  0.2006,  0.3147, -0.9995],
        [-0.1587, -0.6450,  0.6982,  ...,  0.2178,  0.2474, -0.9424],
        [-0.1077, -0.8896,  0.4575,  ...,  0.2324,  0.4351, -0.9390],
        ...,
        [-0.0332, -0.7295,  0.6006,  ..., -0.3489,  0.0950, -0.9507],
        [-0.2766, -0.8145,  0.7256,  ...,  0.3088,  0.2446, -0.8193],
        [-0.0419, -1.0098,  0.4741,  ...,  0.0818,  0.3430, -1.2480]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0779, -0.7959,  0.8545,  ..., -0.1958,  0.1602, -0.9375],
        [-0.1362, -0.9487,  0.9365,  ...,  0.3613,  0.3047, -0.7983],
        [-0.1681, -0.7002,  0.8618,  ...,  0.2644,  0.3291, -0.8374],
        ...,
        [-0.1036, -0.6909,  0.8037,  ...,  0.4414,  0.4805, -0.8320],
        [-0.1632, -0.7344,  0.8628,  ..., -0.0240,  0.3557, -1.0127],
        [-0.2834, -0.8086,  0.8003,  ...,  0.0671,  0.2103, -0.9868]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([7, 512])
> 5.  tensor([[-0.1027, -0.5645,  0.2466,  ...,  0.3848,  0.0083, -0.7573],
        [-0.1848, -0.4448,  0.4150,  ...,  0.4395,  0.1053, -0.9297],
        [-0.3252, -0.7119,  0.4004,  ...,  0.3169,  0.2214, -0.8325],
        ...,
        [-0.3284, -0.5864,  0.2317,  ...,  0.4082,  0.1289, -0.7524],
        [-0.0807, -0.7212,  0.3062,  ...,  0.4475, -0.1793, -0.7622],
        [-0.1515, -0.6606,  0.2307,  ...,  0.4634,  0.0523, -0.7808]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[ 0.0176, -0.5356,  0.2991,  ...,  0.3984,  0.2083, -0.7715],
        [ 0.1382, -0.5723,  0.3420,  ...,  0.3042, -0.0213, -0.9062],
        [-0.0255, -0.5088,  0.2798,  ...,  0.2321,  0.2307, -0.9028],
        ...,
        [-0.0344, -0.4536,  0.4089,  ...,  0.1917,  0.0151, -0.8203],
        [ 0.0261, -0.4172,  0.2566,  ...,  0.4919,  0.1533, -0.8081],
        [-0.0109, -0.5532,  0.3569,  ...,  0.3313,  0.0815, -0.8096]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1054, -0.7427,  0.6611,  ...,  0.4468,  0.4590, -0.7700],
        [-0.2783, -0.7046,  0.4878,  ...,  0.4929,  0.4006, -0.9858],
        [-0.2238, -0.4458,  0.7266,  ...,  0.5337,  0.3420, -0.7134],
        ...,
        [-0.2754, -0.4207,  0.7236,  ...,  0.8218,  0.4395, -0.7192],
        [-0.2037, -0.6011,  0.5522,  ...,  0.6704,  0.3647, -0.8564],
        [-0.1648, -0.6475,  0.4998,  ...,  0.5688,  0.3459, -0.6797]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[-0.3516, -0.6914,  0.5122,  ...,  0.3904, -0.0743, -0.6528],
        [-0.2783, -0.6494,  0.3049,  ...,  0.4568,  0.1159, -0.8076],
        [-0.1653, -0.6362,  0.3042,  ...,  0.3477,  0.0470, -0.8062],
        ...,
        [-0.2676, -0.4246,  0.2810,  ...,  0.3955, -0.0027, -0.7866],
        [-0.1643, -0.4570,  0.3662,  ...,  0.3984,  0.0297, -0.7656],
        [-0.2988, -0.6201,  0.4199,  ...,  0.4924, -0.0315, -0.8291]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1626, -0.6001,  0.3054,  ...,  0.4563,  0.2196, -0.7031],
        [ 0.1298, -0.4102,  0.2834,  ...,  0.2313,  0.1327, -0.6216],
        [-0.0431, -0.4517,  0.4094,  ...,  0.2844,  0.1177, -0.7915],
        ...,
        [-0.1493, -0.8018,  0.1962,  ...,  0.4487,  0.3159, -0.9766],
        [-0.0047, -0.5952,  0.3992,  ...,  0.2407, -0.0154, -0.8525],
        [-0.2563, -0.6372,  0.4304,  ...,  0.3228,  0.1231, -0.8252]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.1196, -0.6157,  0.3203,  ...,  0.4519,  0.1631, -0.7485],
        [-0.0312, -0.6777,  0.2213,  ...,  0.3333,  0.2223, -0.8203],
        [-0.0637, -0.4480,  0.2324,  ...,  0.1665,  0.0769, -0.8867],
        ...,
        [ 0.1404, -0.7041,  0.2908,  ...,  0.4299,  0.0881, -0.8662],
        [-0.0868, -0.6401,  0.3860,  ...,  0.3027,  0.0902, -0.7588],
        [ 0.0400, -0.5410,  0.3093,  ...,  0.4033,  0.0673, -0.7905]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1588, -0.7695,  0.5718,  ...,  0.5625,  0.3416, -0.7334],
        [-0.1338, -0.8135,  0.2886,  ...,  0.5020,  0.0929, -0.6914],
        [-0.1592, -0.4644,  0.3262,  ...,  0.3320,  0.1514, -0.7446],
        ...,
        [-0.1366, -0.8682,  0.5337,  ...,  0.4380,  0.0637, -0.7446],
        [-0.2346, -0.6987,  0.3684,  ...,  0.4509,  0.1910, -0.7163],
        [-0.1085, -0.7295,  0.4651,  ...,  0.2937,  0.0830, -0.7070]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.4207, -0.6592,  0.5083,  ...,  0.2314,  0.0296, -0.9844],
        [-0.2323, -0.6284,  0.3118,  ...,  0.4731,  0.1125, -0.9062],
        [-0.1598, -0.8311,  0.3296,  ...,  0.2852,  0.1010, -0.7344],
        ...,
        [-0.0726, -0.4563,  0.3071,  ...,  0.4924,  0.1735, -0.8950],
        [-0.1835, -0.5493,  0.3794,  ...,  0.2715, -0.0692, -0.8818],
        [-0.2683, -0.6235,  0.3618,  ...,  0.3713, -0.1166, -0.6626]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.2062, -0.6704,  0.5542,  ...,  0.5562,  0.5068, -0.6104],
        [-0.3186, -0.7168,  0.4680,  ...,  0.6255,  0.3101, -0.5742],
        [-0.0697, -0.6216,  0.5122,  ...,  0.5493,  0.5425, -0.6187],
        ...,
        [-0.0184, -0.4265,  0.7930,  ...,  0.4021,  0.2908, -0.7183],
        [-0.2776, -0.7222,  0.5850,  ...,  0.5200,  0.2632, -0.8623],
        [-0.2712, -0.5151,  0.4460,  ...,  0.4924,  0.4133, -0.6055]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[-0.0921, -0.5312,  0.2145,  ...,  0.3381,  0.1248, -0.9717],
        [-0.0448, -0.5093,  0.3955,  ...,  0.3633,  0.1190, -0.9268],
        [-0.0513, -0.5640,  0.3821,  ...,  0.3813,  0.1804, -0.9385],
        ...,
        [-0.0124, -0.3213,  0.2769,  ...,  0.3174,  0.0325, -0.7524],
        [ 0.0886, -0.4604,  0.2966,  ...,  0.4600,  0.1006, -1.1211],
        [-0.0384, -0.5566,  0.2522,  ...,  0.5498,  0.0922, -0.8174]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.2036, -0.7676,  0.3069,  ...,  0.3750,  0.1132, -0.8149],
        [-0.1580, -0.5503,  0.4570,  ...,  0.4600,  0.0032, -0.8379],
        [-0.0922, -0.4285,  0.2957,  ...,  0.3413, -0.0198, -0.7783],
        ...,
        [-0.2607, -0.6924,  0.3606,  ...,  0.5122,  0.1019, -0.6553],
        [-0.0942, -0.4871,  0.4561,  ...,  0.4951,  0.1165, -0.8022],
        [-0.1875, -0.3911,  0.2576,  ...,  0.3960,  0.0053, -0.6909]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0787, -0.4553,  0.3340,  ...,  0.1768, -0.1414, -0.7139],
        [-0.1798, -0.4929,  0.2316,  ...,  0.1434,  0.1161, -0.8755],
        [-0.1775, -0.5498,  0.2544,  ...,  0.2289, -0.1387, -0.7734],
        ...,
        [-0.2173, -0.5063,  0.1647,  ...,  0.4028, -0.1010, -0.7603],
        [-0.1210, -0.5400,  0.3020,  ...,  0.2369, -0.0449, -0.7993],
        [-0.1874, -0.6543,  0.2883,  ...,  0.3145, -0.0734, -0.7832]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.2377, -0.4536,  0.4421,  ...,  0.7988,  0.5449, -0.7583],
        [-0.2252, -0.4878,  0.7285,  ...,  0.5024,  0.3743, -0.7974],
        [-0.0332, -0.5532,  0.5947,  ...,  0.7993,  0.5649, -0.6064],
        ...,
        [-0.3169, -0.6343,  0.8149,  ...,  0.7705,  0.5176, -0.7041],
        [-0.1324, -0.3708,  0.6602,  ...,  0.4087,  0.5811, -0.8506],
        [-0.1649, -0.4111,  0.7773,  ...,  0.4304,  0.4165, -0.6265]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([48, 512])
> 5.  tensor([[ 0.0779, -0.5894,  0.7202,  ...,  0.3181,  0.1929, -0.7002],
        [-0.0677, -0.8169,  0.7354,  ...,  0.3086,  0.3223, -0.6597],
        [-0.0793, -0.8550,  0.5835,  ...,  0.5615,  0.5776, -0.7236],
        ...,
        [-0.0679, -0.8091,  0.8135,  ...,  0.4119,  0.3689, -0.7842],
        [-0.0242, -0.5703,  0.3474,  ...,  0.4629,  0.2686, -0.7017],
        [ 0.0327, -0.5767,  0.6782,  ...,  0.4468,  0.2993, -0.8623]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.0455, -0.4631,  0.2747,  ...,  0.2034, -0.0119, -0.7202],
        [-0.0077, -0.5303,  0.3340,  ...,  0.3079, -0.0687, -0.7549],
        [-0.0108, -0.6558,  0.3618,  ...,  0.3052, -0.0465, -0.7241],
        ...,
        [-0.0117, -0.3596,  0.3013,  ...,  0.2949, -0.0161, -0.8018],
        [-0.1011, -0.5688,  0.3569,  ...,  0.4827,  0.0991, -0.9648],
        [-0.1392, -0.7183,  0.3640,  ...,  0.2834, -0.0220, -0.8960]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1694, -0.5469,  0.3992,  ...,  0.5210, -0.0419, -0.7363],
        [-0.3337, -0.5972,  0.4270,  ...,  0.3489,  0.0923, -0.8257],
        [-0.2065, -0.7046,  0.3953,  ...,  0.4006,  0.1277, -0.8042],
        ...,
        [-0.1969, -0.5435,  0.2159,  ...,  0.4636,  0.1322, -0.8540],
        [-0.2971, -0.5894,  0.2703,  ...,  0.4241,  0.0475, -0.8892],
        [-0.1354, -0.2856,  0.2512,  ...,  0.2324,  0.1295, -0.7378]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1320, -0.9648,  0.7490,  ...,  0.3318,  0.6294, -0.7051],
        [-0.1118, -0.5312,  0.6826,  ...,  0.1691,  0.4446, -0.9155],
        [-0.1302, -0.7754,  0.6113,  ...,  0.4878,  0.3245, -0.7192],
        ...,
        [-0.0961, -0.9580,  0.6685,  ...,  0.3057,  0.2632, -1.1064],
        [-0.3459, -0.7061,  0.7192,  ..., -0.2915,  0.1545, -0.8066],
        [-0.2629, -0.7632,  0.5264,  ..., -0.2937,  0.2014, -1.2295]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1002, -0.4143,  0.1356,  ...,  0.4358,  0.0479, -0.8472],
        [-0.1538, -0.6426,  0.5005,  ..., -0.3066, -0.1696, -0.5527],
        [-0.1469, -0.5669,  0.3806,  ...,  0.3677,  0.0823, -0.7407],
        ...,
        [-0.1698, -0.5674,  0.4216,  ...,  0.3804, -0.0186, -0.7739],
        [-0.1187, -0.4712,  0.1123,  ...,  0.3870,  0.0392, -0.7974],
        [-0.1332, -0.5122,  0.3123,  ...,  0.2654,  0.0243, -0.8027]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.2491, -0.7739,  0.6558,  ...,  0.6504,  0.6138, -0.6030],
        [-0.2620, -0.7070,  0.6538,  ...,  0.5015,  0.3699, -0.5259],
        [-0.2064, -0.8569,  0.3923,  ...,  0.9399,  0.5713, -0.6245],
        ...,
        [-0.3215, -0.6694,  0.7593,  ...,  0.0903,  0.4568, -0.9141],
        [ 0.0096, -0.5532,  0.4004,  ...,  0.6562,  0.5767, -0.6406],
        [-0.2698, -0.7441,  0.6470,  ...,  0.4788,  0.5786, -0.5679]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[ 0.0664, -0.5894,  0.2291,  ...,  0.4128,  0.0951, -0.7500],
        [-0.0312, -0.4170,  0.2261,  ...,  0.5420, -0.0468, -0.7593],
        [-0.0027, -0.6665,  0.3518,  ...,  0.4031, -0.0912, -0.7163],
        ...,
        [-0.0736, -0.5815,  0.3242,  ...,  0.4185,  0.1835, -0.8755],
        [-0.0041, -0.7612,  0.3044,  ...,  0.2491, -0.1088, -0.6768],
        [ 0.0009, -0.5010,  0.2957,  ...,  0.2917,  0.0161, -0.9155]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.7690, -0.6450,  1.1230,  ...,  0.1066,  0.3550, -1.1729],
        [-0.0752, -0.2798,  0.3940,  ...,  0.6885,  0.8853, -0.4443],
        [-0.3645, -0.1648,  0.4236,  ...,  0.2328,  0.8398, -0.8799],
        ...,
        [-0.2074, -0.2499,  0.4060,  ...,  0.6421,  0.3274, -0.5708],
        [-0.2891, -0.5581,  0.1198,  ...,  0.4377,  0.5010, -0.6143],
        [-0.3943, -0.8242,  0.6631,  ...,  0.3584,  0.9360, -0.6558]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([80, 512])
> 5.  tensor([[ 0.0947, -0.7192,  0.5854,  ...,  0.5894,  0.4348, -0.7744],
        [ 0.1091, -0.6250,  0.5361,  ...,  0.4351,  0.0152, -0.7954],
        [ 0.0809, -0.5850,  0.5254,  ...,  0.5234,  0.1636, -0.6987],
        ...,
        [ 0.0561, -0.5293,  0.4009,  ...,  0.4504,  0.0671, -0.7031],
        [-0.0791, -0.6748,  0.6396,  ...,  0.6226,  0.2184, -0.7695],
        [-0.0740, -0.6255,  0.5977,  ...,  0.6714,  0.4087, -0.6470]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.0245, -0.6626,  0.4172,  ...,  0.3062,  0.0483, -0.6626],
        [-0.2605, -0.6323,  0.3877,  ..., -0.1692, -0.0034, -0.9814],
        [ 0.0199, -0.4810,  0.2798,  ...,  0.1442, -0.0747, -1.0029],
        ...,
        [-0.1481, -0.5132,  0.2578,  ...,  0.0797,  0.0021, -0.9575],
        [ 0.0252, -0.5752,  0.3989,  ...,  0.1042,  0.0670, -0.5972],
        [ 0.0423, -0.5259,  0.4548,  ...,  0.2126, -0.0905, -0.9204]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0373, -0.8003,  0.4236,  ...,  0.6069,  0.2316, -0.6421],
        [-0.1505, -0.7368,  0.3235,  ...,  0.4485,  0.1819, -0.7441],
        [-0.0117, -0.7905,  0.4514,  ...,  0.3577,  0.3245, -0.8516],
        ...,
        [-0.0786, -0.7803,  0.4695,  ...,  0.4597,  0.3157, -0.6914],
        [-0.0125, -0.7925,  0.5288,  ...,  0.3909,  0.2374, -0.7275],
        [-0.1164, -0.7500,  0.4270,  ...,  0.3625,  0.3145, -0.7314]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.1229, -0.6953,  0.7598,  ...,  0.3721,  0.3989, -0.7305],
        [-0.1180, -0.7832,  0.7114,  ..., -0.2423,  0.2460, -0.9604],
        [-0.3835, -0.7495,  0.5947,  ..., -0.4280,  0.1880, -1.4629],
        ...,
        [-0.2495, -0.5532,  0.8525,  ...,  0.3325,  0.4836, -0.7998],
        [-0.1198, -0.5166,  0.7158,  ...,  0.2961,  0.3228, -0.8511],
        [-0.1757, -0.7329,  0.8184,  ...,  0.1252,  0.3848, -1.0859]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([7, 512])
> 5.  tensor([[-0.0764, -0.5078,  0.3777,  ...,  0.3853,  0.0458, -0.7476],
        [ 0.0916, -0.6558,  0.2759,  ...,  0.4331, -0.1020, -0.6523],
        [-0.0483, -0.7476,  0.3762,  ...,  0.4153,  0.0088, -0.7148],
        ...,
        [-0.1693, -0.5464,  0.2571,  ...,  0.4075,  0.1193, -0.8516],
        [ 0.0388, -0.7217,  0.4338,  ...,  0.3960,  0.1207, -0.8442],
        [-0.0412, -0.5078,  0.3123,  ...,  0.3906,  0.1042, -0.8232]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1104, -0.7217,  0.3889,  ...,  0.4185,  0.1202, -0.9111],
        [-0.1453, -0.6226,  0.4011,  ...,  0.4434,  0.0603, -0.8130],
        [-0.2072, -0.5234,  0.4558,  ...,  0.1353, -0.0812, -0.9229],
        ...,
        [-0.0406, -0.5493,  0.3535,  ...,  0.3838,  0.1090, -0.9248],
        [-0.1407, -0.7783,  0.4775,  ...,  0.4153, -0.0203, -0.7856],
        [-0.1792, -0.3501,  0.4788,  ...,  0.2556, -0.0469, -0.7988]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0686, -0.5005,  0.1919,  ...,  0.4434,  0.0803, -0.7490],
        [-0.0830, -0.6699,  0.2313,  ...,  0.4668, -0.0558, -0.9058],
        [-0.1620, -0.5605,  0.2297,  ...,  0.5767, -0.0151, -0.8516],
        ...,
        [-0.2106, -0.5308,  0.1621,  ...,  0.3291,  0.0512, -0.9375],
        [-0.0383, -0.4897,  0.2817,  ...,  0.2959, -0.0901, -0.7363],
        [-0.0804, -0.4683,  0.2350,  ...,  0.1880, -0.0204, -0.8755]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.3931, -1.1035,  1.0713,  ...,  0.4355,  0.3137, -0.7065],
        [-0.3772, -0.5786,  0.5513,  ..., -0.2003,  0.4441, -0.8120],
        [-0.3401, -1.1055,  1.0459,  ...,  0.3816,  0.3430, -0.8784],
        [-0.2939, -0.8818,  0.9565,  ...,  0.2676,  0.3191, -0.8042],
        [-0.3550, -0.5967,  0.8809,  ...,  0.2098,  0.2876, -0.9844],
        [-0.1466, -0.6934,  0.9316,  ...,  0.2793,  0.2413, -0.8125]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([6, 512])
> 5.  tensor([[-0.3521, -0.6489,  0.6772,  ...,  0.3589,  0.8584, -0.6558],
        [-0.0155, -0.5737,  0.8315,  ...,  0.4688,  0.5752, -0.9609],
        [-0.1971, -0.5435,  0.5317,  ...,  0.4875,  0.8237, -0.7065],
        ...,
        [-0.0028, -0.3452,  0.4519,  ...,  0.4180,  0.4431, -0.8550],
        [-0.0407, -0.4558,  0.3811,  ...,  0.4199,  0.4094, -0.8647],
        [-0.0293, -0.4451,  0.7056,  ...,  0.5024,  0.6953, -0.6147]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([48, 512])
> 5.  tensor([[-0.1547, -0.6533,  0.4570,  ...,  0.4026,  0.4153, -0.5249],
        [ 0.0398, -0.5938,  0.5747,  ...,  0.6294,  0.3892, -0.8232],
        [-0.2050, -0.5850,  0.7666,  ...,  0.4294,  0.2502, -0.6616],
        ...,
        [-0.3020, -0.4375,  0.8086,  ...,  0.5405,  0.2098, -0.7490],
        [-0.2888, -0.5845,  0.5991,  ...,  0.3164,  0.4663, -0.9634],
        [-0.2324, -0.5249,  0.7529,  ...,  0.5024,  0.2424, -0.6226]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[-0.3159, -0.5854,  0.4709,  ...,  0.4434, -0.0583, -0.8467],
        [-0.0350, -0.4736,  0.4119,  ...,  0.1056,  0.0705, -0.9282],
        [-0.0928, -0.6865,  0.4753,  ...,  0.4390,  0.0303, -0.8506],
        ...,
        [-0.3621, -0.6426,  0.5894,  ...,  0.2971,  0.1205, -1.0410],
        [-0.0975, -0.5752,  0.4346,  ...,  0.2043,  0.0293, -0.8618],
        [-0.1002, -0.7646,  0.5977,  ...,  0.3071,  0.2179, -0.9033]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0457, -0.5684,  0.5366,  ...,  0.3613,  0.2002, -0.7090],
        [-0.0831, -0.4543,  0.5669,  ...,  0.1537,  0.1696, -0.8213],
        [ 0.0286, -0.5117,  0.4348,  ...,  0.2096,  0.1942, -0.8086],
        ...,
        [-0.0282, -0.5327,  0.5176,  ...,  0.4099,  0.2224, -0.8584],
        [-0.2247, -0.6968,  0.6279,  ...,  0.3640,  0.0905, -0.7319],
        [-0.1147, -0.5938,  0.4480,  ...,  0.4397,  0.3145, -0.8384]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0104, -0.4866,  0.3176,  ...,  0.3188,  0.1108, -0.9976],
        [-0.0512, -0.6357,  0.2223,  ...,  0.6704,  0.0263, -0.8325],
        [-0.0594, -0.4863,  0.3896,  ...,  0.3450,  0.0509, -0.8462],
        ...,
        [-0.0019, -0.5957,  0.4053,  ...,  0.4155,  0.0285, -0.8965],
        [-0.0847, -0.6875,  0.4189,  ...,  0.2783,  0.0144, -0.7534],
        [-0.1476, -0.4482,  0.2761,  ...,  0.4480, -0.0623, -0.8101]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0970, -0.6348,  0.3816,  ...,  0.5337,  0.0637, -0.7583],
        [-0.1361, -0.7349,  0.5430,  ...,  0.3901,  0.0091, -0.8530],
        [-0.0867, -0.8208,  0.3823,  ...,  0.5244,  0.2708, -0.9248],
        ...,
        [-0.3193, -0.7051,  0.3511,  ...,  0.5068, -0.0036, -0.8984],
        [-0.1370, -0.7002,  0.4592,  ...,  0.4736,  0.0587, -0.8213],
        [-0.1152, -0.7144,  0.2693,  ...,  0.4951,  0.2988, -0.7085]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.3157, -0.6465,  0.7979,  ...,  0.2007,  0.4827, -0.7271],
        [-0.1597, -0.6377,  0.8335,  ...,  0.2169,  0.3215, -0.7246],
        [-0.2487, -0.4988,  0.7588,  ...,  0.4294,  0.3533, -0.8491],
        ...,
        [-0.3174, -0.7793,  1.0605,  ...,  0.0809,  0.2159, -0.6465],
        [-0.1769, -0.7656,  0.8779,  ...,  0.2915,  0.3604, -0.8730],
        [-0.1183, -0.4944,  0.5894,  ...,  0.3967,  0.3650, -0.7964]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([7, 512])
> 5.  tensor([[ 0.0286, -0.4443,  0.3840,  ...,  0.4919,  0.0436, -0.8213],
        [ 0.0321, -1.0918,  0.3242,  ...,  0.1033,  0.1036, -1.2676],
        [-0.1334, -0.7036,  0.2825,  ...,  0.2382,  0.0348, -0.9478],
        ...,
        [-0.0338, -0.4819,  0.3992,  ...,  0.5654,  0.2791, -0.8174],
        [-0.0834, -0.5483,  0.4741,  ...,  0.1855,  0.0597, -0.7681],
        [-0.3140, -0.5117,  0.4062,  ...,  0.3508,  0.1237, -0.8286]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1426, -0.4355,  0.1495,  ...,  0.3120,  0.0113, -0.7866],
        [-0.2369, -0.8325,  0.3416,  ...,  0.3159,  0.0763, -0.8423],
        [-0.1015, -0.6216,  0.3601,  ...,  0.4985,  0.0642, -0.8252],
        ...,
        [-0.1669, -0.5122,  0.2737,  ...,  0.3975,  0.0175, -0.9009],
        [-0.2306, -0.4490,  0.2742,  ...,  0.4680,  0.0857, -0.7930],
        [-0.2180, -0.7520,  0.3501,  ...,  0.4136,  0.1021, -0.9058]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[ 0.1841, -0.4573,  0.4968,  ...,  0.5991,  0.6177, -0.7515],
        [-0.1083, -0.6187,  0.8213,  ...,  0.5137,  0.3889, -0.8999],
        [-0.0510, -0.3948,  0.4983,  ...,  0.4712,  0.4648, -0.6011],
        ...,
        [-0.1628, -0.7466,  0.6201,  ...,  0.5820,  0.4854, -0.8599],
        [-0.0591, -0.6030,  0.8408,  ...,  0.4719,  0.5430, -0.7549],
        [-0.4570, -0.7144,  0.7095,  ...,  0.0728,  0.6724, -0.6055]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([40, 512])
> 5.  tensor([[-0.1075, -0.4578,  0.4895,  ...,  0.3645,  0.3438, -0.8193],
        [-0.2639, -0.5303,  0.5220,  ...,  0.3088,  0.3335, -0.8486],
        [-0.1656, -0.4294,  0.3923,  ...,  0.3848,  0.2839, -0.8320],
        ...,
        [-0.1315, -0.7505,  0.6768,  ...,  0.1309,  0.3320, -1.1758],
        [-0.1561, -0.5537,  0.5522,  ...,  0.3098,  0.3440, -0.8154],
        [ 0.0849, -0.4939,  0.4858,  ..., -0.0725,  0.0112, -0.1403]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1235, -0.5439,  0.3567,  ...,  0.3450,  0.1024, -0.7139],
        [-0.1794, -0.6138,  0.4111,  ...,  0.2954,  0.0599, -0.7886],
        [-0.1816, -0.5327,  0.3203,  ...,  0.1627, -0.1906, -0.6421],
        ...,
        [-0.0876, -0.4854,  0.3306,  ...,  0.5215,  0.1353, -0.7178],
        [-0.2993, -0.4387,  0.3784,  ...,  0.1520, -0.1025, -0.8696],
        [-0.2081, -0.5161,  0.3684,  ...,  0.3521,  0.0595, -0.6646]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1233, -0.9204,  0.6543,  ...,  0.4512,  0.2856, -0.8350],
        [-0.1403, -0.5938,  0.6577,  ...,  0.5571,  0.2076, -0.7451],
        [-0.2167, -0.6357,  0.6562,  ...,  0.6221,  0.2067, -0.7095],
        ...,
        [-0.3098, -0.8037,  0.7593,  ...,  0.4685,  0.1718, -0.5376],
        [-0.4504, -0.7993,  0.7246,  ...,  0.2991,  0.1078, -0.7666],
        [-0.2847, -0.6401,  0.5693,  ...,  0.2284,  0.2401, -0.7393]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.1393, -0.6455,  0.3796,  ...,  0.2433,  0.0526, -0.8154],
        [-0.0023, -0.3455,  0.4814,  ...,  0.2944,  0.1262, -0.8257],
        [ 0.0200, -0.6968,  0.5112,  ...,  0.3931,  0.0779, -0.7412],
        ...,
        [ 0.0137, -0.5898,  0.5171,  ...,  0.4312,  0.1539, -0.8779],
        [-0.1718, -0.6250,  0.4846,  ...,  0.4355,  0.2445, -0.7627],
        [-0.1300, -0.5015,  0.4619,  ...,  0.4390,  0.1978, -0.7998]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1638, -0.3345,  0.3359,  ...,  0.3323, -0.0828, -0.8262],
        [-0.1499, -0.4182,  0.4185,  ...,  0.4360,  0.1107, -0.7798],
        [-0.1455, -0.5962,  0.3059,  ...,  0.3267,  0.0532, -0.8657],
        ...,
        [-0.1211, -0.5889,  0.3162,  ...,  0.4294,  0.0648, -0.9165],
        [-0.2128, -0.4958,  0.2629,  ...,  0.2330,  0.0314, -0.8267],
        [-0.2180, -0.5781,  0.4937,  ...,  0.4460,  0.1553, -1.0400]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.4724, -0.8501,  1.0469,  ...,  0.2727,  0.2003, -0.9814],
        [-0.3596, -0.9038,  1.0596,  ..., -0.0143,  0.0284, -0.9263],
        [-0.4307, -0.6724,  1.1504,  ...,  0.5562,  0.1478, -0.8501],
        [-0.1740, -0.9272,  1.0898,  ...,  0.4292,  0.1987, -0.7188],
        [-0.4524, -0.9971,  0.8306,  ...,  0.1207,  0.1610, -1.0625]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([5, 512])
> 5.  tensor([[-0.2042, -0.4048,  0.8008,  ...,  0.3635,  0.2068, -0.5996],
        [-0.2186, -0.7104,  0.6519,  ...,  0.4700,  0.5679, -0.7529],
        [-0.0870, -0.4915,  0.8979,  ...,  0.5332,  0.3262, -0.7505],
        ...,
        [-0.2266, -0.5767,  0.5649,  ...,  0.3279,  0.4580, -0.9443],
        [-0.2372, -0.5103,  0.7319,  ...,  0.2961,  0.1849, -0.8032],
        [-0.3462, -0.5757,  0.6182,  ...,  0.4351,  0.4717, -0.5747]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[-2.8442e-01, -6.3574e-01,  1.0049e+00,  ..., -5.3072e-04,
         -9.1934e-03, -6.2061e-01],
        [-2.8271e-01, -1.0420e+00,  8.0615e-01,  ..., -3.3154e-01,
          8.0322e-02, -1.2559e+00],
        [-2.2314e-01, -9.5996e-01,  1.1602e+00,  ...,  3.3594e-01,
          3.0200e-01, -7.8662e-01],
        [-2.3535e-01, -7.7100e-01,  7.3730e-01,  ...,  4.2017e-01,
          4.3091e-01, -9.6875e-01],
        [-2.3071e-01, -6.3818e-01,  1.2803e+00,  ...,  9.4116e-02,
          1.1865e-01, -7.1143e-01],
        [-1.3184e-01, -9.7803e-01,  9.2627e-01,  ...,  1.9666e-01,
          5.2686e-01, -8.6377e-01]], device='cuda:2', dtype=torch.float16,
       grad_fn=<DivBackward0>) torch.Size([6, 512])
> 5.  tensor([[-0.1801, -0.4233,  0.5396,  ...,  0.2273,  0.1851, -0.8735],
        [-0.1010, -0.5840,  0.5347,  ..., -0.2040,  0.3340, -1.0615],
        [-0.0681, -0.4800,  0.5391,  ...,  0.1987,  0.3005, -0.7959],
        ...,
        [-0.1237, -0.4170,  0.6147,  ...,  0.1705,  0.0821, -0.8335],
        [-0.0363, -0.7061,  0.5991,  ..., -0.0695,  0.3022, -0.9937],
        [-0.0473, -0.6396,  0.5459,  ...,  0.3281,  0.4475, -0.8794]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.2529, -0.6304,  0.9834,  ...,  0.0845,  0.3730, -0.8203],
        [-0.3105, -0.6309,  1.0703,  ...,  0.4563,  0.3464, -0.6987],
        [-0.2832, -0.7144,  0.8291,  ..., -0.1190,  0.2002, -0.8975],
        [-0.2314, -1.0469,  1.0078,  ...,  0.2861,  0.1747, -0.8823],
        [-0.2605, -0.6616,  0.6836,  ...,  0.3105,  0.4890, -0.8584],
        [-0.3306, -0.9814,  0.9741,  ..., -0.0704,  0.2357, -0.8306]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([6, 512])
> 5.  tensor([[-0.1968, -0.5044,  0.6914,  ...,  0.3901,  0.2871, -0.7461],
        [-0.1635, -0.5566,  0.6250,  ...,  0.1537,  0.1735, -0.9111],
        [-0.1043, -0.5264,  0.7261,  ...,  0.2888,  0.2336, -0.8691],
        ...,
        [-0.0201, -0.6353,  0.6016,  ...,  0.0467,  0.3650, -1.0420],
        [-0.0715, -0.5166,  0.4905,  ...,  0.2878,  0.2462, -0.7520],
        [-0.0070, -0.7349,  0.6025,  ..., -0.3152,  0.0426, -0.8491]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1643, -0.3059,  0.3218,  ...,  0.6255,  0.7041, -1.0127],
        [-0.5200, -0.2698,  0.2798,  ...,  0.5474,  0.4812, -0.4402],
        [-0.5005, -0.2803,  0.5713,  ...,  0.2260,  0.6738, -0.4238],
        ...,
        [-0.7822, -0.5811,  0.5239,  ...,  0.3882,  0.8062, -0.3755],
        [-0.3181,  0.0361,  0.5112,  ...,  0.4207,  0.4165, -0.6787],
        [-0.3435, -0.0673,  0.5894,  ...,  0.5869,  0.6855, -0.4656]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([96, 512])
> 5.  tensor([[ 0.0252, -0.4705,  0.4883,  ...,  0.1984,  0.0662, -0.6602],
        [ 0.0163, -0.4526,  0.3586,  ...,  0.2426, -0.0032, -0.7832],
        [-0.0964, -0.4856,  0.6167,  ...,  0.0033,  0.1942, -0.8643],
        ...,
        [-0.1663, -0.4565,  0.4131,  ...,  0.4399,  0.2007, -0.8330],
        [-0.1148, -0.4077,  0.3870,  ...,  0.1077,  0.2052, -0.8447],
        [-0.0384, -0.6240,  0.4294,  ..., -0.3218, -0.0009, -0.4856]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.4441, -0.6855,  0.3069,  ...,  0.4688,  0.9399, -0.4968],
        [-0.3010, -0.2532,  0.5713,  ...,  0.4521,  0.4736, -0.6719],
        [-0.2081, -0.3777,  0.4678,  ...,  0.3208,  0.3948, -0.9214],
        ...,
        [-0.3606, -0.5337,  0.8403,  ...,  0.7046,  0.3711, -0.6714],
        [ 0.0121, -0.3162,  0.1581,  ...,  0.4597,  0.4832, -0.5034],
        [-0.3215, -0.6030,  0.7090,  ...,  0.3430,  0.5425, -0.2979]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([72, 512])
> 5.  tensor([[-0.0292, -0.6030,  0.4148,  ...,  0.4653,  0.1334, -0.8667],
        [-0.0258, -0.6631,  0.2891,  ...,  0.5791,  0.2455, -0.7349],
        [-0.1661, -0.6143,  0.2551,  ...,  0.5645,  0.2764, -0.7241],
        ...,
        [-0.1733, -0.4995,  0.4314,  ...,  0.5239,  0.2690, -0.7334],
        [-0.1136, -0.7173,  0.5762,  ...,  0.2048,  0.1412, -0.7427],
        [-0.2089, -0.8047,  0.6304,  ...,  0.4614,  0.3235, -0.6797]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.5181, -0.9287,  1.3330,  ...,  0.6250,  0.2793, -0.9985],
        [-0.1641, -1.1689,  0.8306,  ...,  0.3264,  0.3618, -0.9834],
        [-0.4834, -1.2568,  1.2803,  ...,  0.3154,  0.1742, -0.7861],
        [-0.1547, -1.0908,  1.1865,  ...,  0.8281,  0.2712, -0.8052],
        [-0.4741, -1.0967,  1.1445,  ...,  0.1509, -0.0685, -0.8647]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([5, 512])
> 5.  tensor([[-0.0081, -0.6411,  0.5693,  ...,  0.1971,  0.0510, -0.8906],
        [ 0.0262, -0.6538,  0.4275,  ...,  0.4138,  0.1840, -0.8374],
        [-0.1111, -0.5688,  0.4155,  ...,  0.3127,  0.0209, -0.8525],
        ...,
        [-0.0017, -0.4460,  0.2930,  ...,  0.3833,  0.1101, -0.6318],
        [-0.0628, -0.7188,  0.3516,  ...,  0.2600,  0.1497, -0.7344],
        [-0.0797, -0.6025,  0.2581,  ...,  0.3210, -0.0214, -0.7764]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0346, -0.4390,  0.3640,  ...,  0.4172,  0.0872, -0.8545],
        [-0.0395, -0.5210,  0.2681,  ...,  0.3252, -0.1416, -0.8184],
        [-0.0295, -0.7266,  0.4080,  ...,  0.4041,  0.0670, -0.8672],
        ...,
        [-0.1345, -0.5591,  0.3210,  ...,  0.3923,  0.0177, -0.7803],
        [-0.2019, -0.7021,  0.3816,  ...,  0.3142,  0.1488, -0.9048],
        [-0.1528, -0.7505,  0.3137,  ...,  0.3369,  0.0220, -0.9971]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1691, -0.6133,  0.5444,  ...,  0.5186,  0.6958, -0.8730],
        [-0.1927, -1.0566,  0.5664,  ...,  0.0174,  0.4998, -0.7637],
        [ 0.0338, -0.6763,  0.7095,  ...,  0.5552,  0.5742, -0.7686],
        ...,
        [-0.1687, -0.5874,  0.4280,  ...,  0.5249,  0.6367, -0.5854],
        [-0.1103, -0.7241,  0.4980,  ...,  0.5200,  0.6860, -0.5308],
        [-0.1445, -0.7324,  0.6616,  ...,  0.6001,  0.6133, -0.8560]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([40, 512])
> 5.  tensor([[-0.0834, -0.3933,  0.8164,  ...,  0.4316,  0.2043, -0.6348],
        [-0.1880, -0.4790,  0.6792,  ...,  0.5205,  0.0480, -0.7441],
        [-0.2446, -0.4287,  0.7114,  ...,  0.6763,  0.1823, -0.9424],
        ...,
        [-0.1700, -0.4062,  0.5620,  ...,  0.6348,  0.1588, -0.6416],
        [-0.2986, -0.4773,  0.6060,  ...,  0.6597,  0.1227, -0.6938],
        [-0.0624, -0.6025,  0.5952,  ...,  0.5625,  0.0365, -0.8330]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[-0.6230, -0.1938,  0.3167,  ...,  0.1862,  0.6880, -1.0254],
        [-0.4468, -0.2467,  0.5361,  ...,  0.3533,  0.3833, -0.7944],
        [-0.0607, -0.3032,  0.7261,  ...,  0.6699,  0.6138, -0.7446],
        ...,
        [-0.0623, -0.4077,  0.2291,  ...,  0.2791,  0.3870, -0.6567],
        [-0.3850, -0.3276,  0.5728,  ...,  0.4551,  0.5400, -0.6714],
        [-0.3728, -0.2383,  0.3979,  ...,  0.4807,  0.5586, -0.6494]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([88, 512])
> 5.  tensor([[-0.0638, -0.7383,  0.3101,  ...,  0.3337,  0.3010, -1.0078],
        [ 0.0133, -0.5479,  0.4026,  ...,  0.2235,  0.0161, -0.7012],
        [-0.0285, -0.4985,  0.3044,  ...,  0.1566,  0.0349, -0.8750],
        ...,
        [-0.0490, -0.3508,  0.2622,  ...,  0.3777,  0.1409, -0.8281],
        [-0.0075, -0.7793,  0.4526,  ...,  0.3047,  0.0721, -0.7822],
        [ 0.0929, -0.6021,  0.2305,  ...,  0.2213,  0.0726, -0.9204]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0693, -0.5820,  0.1736,  ...,  0.4021,  0.2537, -0.7905],
        [-0.0025, -0.5278,  0.3669,  ...,  0.3542,  0.0469, -0.7817],
        [-0.1011, -0.6035,  0.3394,  ...,  0.3540,  0.1173, -0.8125],
        ...,
        [-0.0268, -0.7085,  0.3208,  ...,  0.3767,  0.1100, -0.8452],
        [ 0.0631, -0.6665,  0.2588,  ...,  0.1322, -0.0126, -0.9033],
        [ 0.0529, -0.6235,  0.3401,  ...,  0.1798,  0.0275, -0.9136]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1531, -0.4119,  0.2725,  ...,  0.0125,  0.3508, -0.6455],
        [-0.1279, -0.7432,  0.7144,  ...,  0.2043,  0.1388, -0.9839],
        [-0.1671, -0.6196,  0.6489,  ...,  0.4148,  0.4124, -0.8462],
        ...,
        [-0.1469, -0.6187,  0.5938,  ...,  0.0557,  0.1702, -0.8525],
        [-0.2629, -0.3801,  0.5117,  ...,  0.3113,  0.5859, -0.8701],
        [-0.1797, -0.4558,  0.5420,  ...,  0.1799,  0.3289, -0.8394]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0683, -0.7368,  0.2844,  ...,  0.2852,  0.1183, -1.0527],
        [-0.1115, -0.3069,  0.2766,  ...,  0.1793,  0.0445, -0.9316],
        [-0.1340, -0.5542,  0.4038,  ...,  0.4873,  0.0731, -0.8408],
        ...,
        [-0.0266, -0.7378,  0.2844,  ...,  0.2434,  0.0292, -0.8696],
        [ 0.0528, -0.5142,  0.1606,  ...,  0.2981, -0.0912, -0.7358],
        [-0.2169, -0.7212,  0.1022,  ..., -0.0792,  0.1671, -1.2949]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0703, -0.8145,  0.4458,  ...,  0.5923,  0.2307, -0.7646],
        [-0.0298, -0.8979,  0.5737,  ...,  0.4922,  0.2676, -0.7671],
        [-0.0121, -0.9473,  0.3894,  ...,  0.7900,  0.2542, -0.5996],
        ...,
        [-0.0854, -0.6670,  0.3757,  ...,  0.4749,  0.2200, -0.7788],
        [-0.1115, -0.7485,  0.4409,  ...,  0.5522,  0.2642, -0.6030],
        [-0.1923, -0.7642,  0.3875,  ...,  0.4561,  0.2202, -0.7549]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.0894, -0.4822,  0.1990,  ...,  0.5010,  0.1614, -0.7993],
        [-0.0432, -0.7690,  0.2426,  ...,  0.3782,  0.1053, -0.9365],
        [-0.0991, -0.4941,  0.3120,  ...,  0.4443,  0.0334, -0.8125],
        ...,
        [ 0.0715, -0.4619,  0.2357,  ...,  0.3875,  0.0323, -0.7432],
        [-0.0729, -0.6191,  0.4170,  ...,  0.3516,  0.0993, -0.7212],
        [ 0.0517, -0.5815,  0.3237,  ...,  0.5054,  0.0629, -0.8096]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0658, -0.6104,  0.7168,  ..., -0.0548,  0.1741, -0.8516],
        [-0.0643, -0.5645,  0.7700,  ...,  0.1846,  0.1422, -0.7925],
        [-0.2056, -0.3008,  0.5347,  ...,  0.3989,  0.3706, -0.6494],
        ...,
        [-0.0133, -0.8120,  0.4399,  ...,  0.1438,  0.4570, -1.0801],
        [-0.0323, -0.4551,  0.4915,  ...,  0.1880,  0.2988, -0.9219],
        [ 0.0373, -0.7129,  0.6841,  ...,  0.1940,  0.2705, -0.8872]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0127, -0.3494,  0.2715,  ...,  0.2961, -0.0551, -0.6104],
        [-0.1276, -0.6187,  0.4004,  ...,  0.4526, -0.0417, -0.7529],
        [-0.1909, -0.5918,  0.4072,  ...,  0.4272, -0.1989, -0.8511],
        ...,
        [-0.0404, -0.4924,  0.3135,  ...,  0.1086, -0.1364, -0.7988],
        [-0.0627, -0.6172,  0.2211,  ...,  0.5942,  0.0607, -0.7563],
        [ 0.0370, -0.5991,  0.1858,  ...,  0.3374, -0.1560, -0.7354]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0698, -0.4099,  0.3530,  ...,  0.1428, -0.0064, -0.9741],
        [-0.2391, -0.4136,  0.2343,  ...,  0.4124,  0.0472, -0.6963],
        [-0.2313, -0.4771,  0.2332,  ...,  0.4209,  0.1769, -0.8359],
        ...,
        [-0.1691, -0.4365,  0.5098,  ...,  0.2964, -0.0182, -0.8359],
        [-0.0270, -0.3477,  0.3611,  ...,  0.2849,  0.0095, -0.7739],
        [-0.1886, -0.6123,  0.3450,  ...,  0.3035, -0.0617, -0.6582]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0608, -0.5474,  0.3865,  ...,  0.2964,  0.0264, -0.7534],
        [ 0.1049, -0.5347,  0.3652,  ...,  0.4509,  0.1539, -0.8237],
        [ 0.0261, -0.5474,  0.4541,  ...,  0.2698,  0.1379, -0.7129],
        ...,
        [-0.1098, -0.7246,  0.4810,  ..., -0.1710,  0.0414, -1.0615],
        [-0.0047, -0.4622,  0.4102,  ...,  0.4060,  0.0544, -0.8750],
        [-0.0820, -0.5200,  0.3672,  ...,  0.2810,  0.0815, -0.6655]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1442, -0.4863,  0.5244,  ...,  0.4727, -0.0655, -0.7607],
        [-0.1880, -0.4333,  0.3547,  ...,  0.4722,  0.0435, -0.8188],
        [-0.0984, -0.4023,  0.4312,  ...,  0.3467,  0.1301, -0.8496],
        ...,
        [-0.2605, -0.6870,  0.4270,  ...,  0.2188, -0.0571, -1.1113],
        [-0.1908, -0.5229,  0.5229,  ...,  0.1600, -0.0299, -0.9331],
        [-0.1611, -0.6758,  0.5908,  ...,  0.4646,  0.0413, -0.8530]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0969, -0.3433,  0.3828,  ...,  0.4272,  0.0800, -0.7974],
        [-0.0888, -0.4541,  0.4028,  ...,  0.6191,  0.2327, -0.8003],
        [ 0.0507, -0.6455,  0.4697,  ...,  0.1627,  0.0529, -0.8218],
        ...,
        [-0.1244, -0.4854,  0.3647,  ...,  0.3843,  0.0897, -0.6909],
        [-0.2046, -0.5000,  0.4988,  ...,  0.4023,  0.1665, -0.8535],
        [-0.1927, -0.4604,  0.4456,  ...,  0.4414,  0.0367, -0.7695]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  > 5.  tensor([[-0.1761, -0.6841,  0.3582,  ...,  0.5781,  0.1975, -0.8042],
        [-0.0492, -0.5981,  0.3179,  ...,  0.3989,  0.3262, -0.7329],
        [-0.0164, -0.7109,  0.3596,  ...,  0.5928,  0.2491, -0.7212],
        ...,
        [-0.1421, -0.8906,  0.3901,  ...,  0.6577,  0.3176, -0.6865],
        [-0.1503, -0.7739,  0.5200,  ...,  0.6006,  0.4609, -0.7593],
        [ 0.0186, -0.6538,  0.2776,  ...,  0.6665,  0.2810, -1.0107]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.2218, -0.7017,  0.4553,  ...,  0.4314,  0.0833, -0.9121],
        [-0.2064, -0.4412,  0.4226,  ...,  0.3313,  0.0883, -0.8242],
        [-0.2925, -0.5010,  0.3376,  ...,  0.5859,  0.2115, -0.7876],
        ...,
        [-0.2173, -0.4314,  0.3149,  ...,  0.3586,  0.0609, -0.8359],
        [-0.1123, -0.8081,  0.3225,  ...,  0.5815,  0.0731, -0.8110],
        [-0.2225, -0.6816,  0.4355,  ...,  0.4365,  0.1138, -0.8599]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.3235, -1.2480,  0.1020,  ..., -0.0812,  0.4792, -1.4521],
        [-0.1002, -0.7788,  0.8633,  ..., -0.6914,  0.2098, -0.7310],
        [-0.2061, -1.1270,  1.1387,  ..., -0.7793, -0.0677, -0.4421],
        ...,
        [ 0.0566, -0.8442,  0.5068,  ..., -0.0912,  0.4890, -0.8364],
        [-0.2690, -0.4768,  0.6416,  ...,  0.2590,  0.4705, -0.5640],
        [-0.0906, -0.5693,  0.7461,  ...,  0.2664,  0.3643, -0.7441]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0844, -0.6709,  0.2311,  ...,  0.2428, -0.0915, -0.8022],
        [-0.0654, -0.7319,  0.3049,  ...,  0.3169,  0.1174, -0.8911],
        [-0.1257, -0.6719,  0.3677,  ...,  0.4773,  0.0317, -0.7773],
        ...,
        [-0.1311, -0.6494,  0.2063,  ...,  0.5918,  0.1776, -0.7603],
        [ 0.1620, -0.7876,  0.3457,  ...,  0.2317,  0.0659, -0.9277],
        [-0.1191, -0.5996,  0.3291,  ...,  0.2678,  0.1324, -0.7769]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0177, -0.5269,  0.2808,  ...,  0.3325,  0.1130, -0.8066],
        [-0.0644, -0.5864,  0.4238,  ...,  0.3074,  0.1055, -0.9214],
        [-0.1182, -0.4966,  0.4597,  ...,  0.2588,  0.0080, -0.7080],
        ...,
        [ 0.0553, -0.5991,  0.3379,  ...,  0.3184,  0.0376, -0.9092],
        [-0.0182, -0.5254,  0.4456,  ...,  0.3960,  0.1307, -0.7725],
        [ 0.0319, -0.4233,  0.1670,  ...,  0.3809,  0.1603, -0.8159]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0112, -0.6372,  0.3501,  ...,  0.4482,  0.0297, -0.7563],
        [ 0.1353, -0.5620,  0.1868,  ...,  0.5386,  0.1267, -0.8052],
        [ 0.0196, -0.7100,  0.4207,  ...,  0.2739, -0.0784, -0.7974],
        ...,
        [-0.1443, -0.5405,  0.3694,  ...,  0.3186,  0.0363, -0.8477],
        [-0.0181, -0.6001,  0.2228,  ...,  0.3337,  0.0522, -0.7686],
        [ 0.0101, -0.4355,  0.3647,  ...,  0.3569, -0.0196, -0.9014]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0343, -0.5122,  0.3013,  ...,  0.2465, -0.1263, -0.7646],
        [-0.1131, -0.6309,  0.2207,  ...,  0.4131,  0.1542, -0.9561],
        [-0.0466, -0.5161,  0.1494,  ...,  0.4724,  0.0652, -0.7852],
        ...,
        [-0.1004, -0.4390,  0.2710,  ...,  0.5190, -0.1004, -0.9375],
        [-0.0870, -0.5278,  0.2646,  ...,  0.4038,  0.0040, -0.7988],
        [-0.0397, -0.7852,  0.1890,  ...,  0.2466, -0.0804, -0.7515]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[ 0.0071, -0.7773,  0.2976,  ...,  0.1682,  0.1183, -1.1074],
        [-0.0244, -0.4546,  0.2700,  ...,  0.3440,  0.0211, -0.7354],
        [-0.0591, -0.7207,  0.3940,  ...,  0.0327,  0.1582, -0.8306],
        ...,
        [ 0.0962, -0.5566,  0.2935,  ...,  0.3083, -0.0317, -0.7104],
        [ 0.0326, -0.4521,  0.2615,  ...,  0.3889, -0.0060, -0.7583],
        [ 0.0198, -0.6733,  0.2291,  ...,  0.3669,  0.1329, -0.8008]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0340, -0.8359,  0.4707,  ...,  0.5747,  0.2908, -0.7393],
        [-0.0748, -0.7954,  0.4900,  ...,  0.4705,  0.2023, -0.6909],
        [-0.0920, -0.6650,  0.3044,  ...,  0.3989,  0.1185, -0.7407],
        ...,
        [-0.0029, -0.6143,  0.3979,  ...,  0.6758,  0.2656, -0.5728],
        [-0.1648, -0.6753,  0.3711,  ...,  0.4231,  0.0679, -0.7993],
        [-0.0814, -0.9150,  0.3420,  ...,  0.3860,  0.1802, -0.7466]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.0451, -0.6240,  0.3503,  ...,  0.0234,  0.2106, -0.7559],
        [-0.0079, -0.5757,  0.3855,  ...,  0.3413, -0.0289, -0.7900],
        [ 0.1238, -0.5591,  0.3818,  ...,  0.3145, -0.1249, -0.8027],
        ...,
        [-0.1393, -0.4827,  0.1919,  ...,  0.3987, -0.0099, -0.9321],
        [-0.0432, -0.6089,  0.3887,  ...,  0.3608,  0.0343, -0.8408],
        [-0.0763, -0.4871,  0.2209,  ...,  0.4077,  0.0821, -0.7925]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[ 0.0734, -0.7119,  0.6372,  ...,  0.4290,  0.0531, -0.6924],
        [-0.0533, -0.6416,  0.6729,  ...,  0.4106,  0.2061, -0.6758],
        [ 0.0012, -0.7622,  0.4487,  ...,  0.5479,  0.3374, -0.8320],
        ...,
        [-0.0638, -0.6680,  0.5029,  ...,  0.3162,  0.1608, -0.7261],
        [-0.0492, -0.5908,  0.5024,  ...,  0.5146,  0.4163, -0.7651],
        [-0.0532, -0.5166,  0.6025,  ...,  0.4492, -0.0062, -0.6372]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.2778, -0.8423,  0.3342,  ...,  0.5156,  0.3955, -0.6294],
        [-0.0338, -0.7085,  0.4827,  ...,  0.8564,  0.4722, -0.7207],
        [-0.0251, -0.6880,  0.7480,  ...,  0.6519,  0.6333, -0.6597],
        ...,
        [-0.1619, -0.6567,  0.8887,  ...,  0.5645,  0.6724, -0.9287],
        [-0.1162, -0.6870,  0.5889,  ...,  0.3718,  0.1965, -0.7349],
        [-0.2456, -0.7207,  0.5996,  ...,  0.4719,  0.3916, -0.5908]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([40, 512])
> 5.  tensor([[-0.0884, -0.6436,  0.3928,  ...,  0.4155,  0.0771, -0.7329],
        [-0.0923, -0.3853,  0.4041,  ...,  0.4602,  0.0162, -0.9004],
        [-0.0958, -0.6548,  0.4348,  ...,  0.3938,  0.0558, -0.8896],
        ...,
        [-0.0341, -0.5059,  0.3953,  ...,  0.4539, -0.0510, -0.8550],
        [-0.0232, -0.5947,  0.3000,  ...,  0.4543, -0.0335, -0.7842],
        [ 0.0412, -0.7021,  0.3325,  ...,  0.1691,  0.0523, -0.7090]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0900, -0.6641,  0.4343,  ...,  0.4287,  0.1422, -0.9609],
        [-0.1074, -0.5259,  0.2795,  ...,  0.4727,  0.0882, -0.6836],
        [-0.0904, -0.6587,  0.3425,  ...,  0.3086,  0.1317, -0.7583],
        ...,
        [-0.0645, -0.6558,  0.4077,  ...,  0.2656,  0.1251, -0.7573],
        [-0.1797, -0.5688,  0.4392,  ...,  0.0762,  0.1061, -0.7549],
        [-0.0350, -0.6274,  0.2502,  ...,  0.5562,  0.1744, -0.9194]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1755, -0.8013,  0.6265,  ...,  0.7266,  0.1285, -0.9014],
        [-0.3809, -0.5903,  0.5645,  ...,  0.3411,  0.4058, -0.6650],
        [-0.3862, -0.8179,  0.5386,  ...,  0.4102,  0.0591, -0.6797],
        ...,
        [-0.2805, -0.5576,  0.6167,  ...,  0.5239,  0.3511, -0.6748],
        [-0.1420, -0.7622,  0.5669,  ...,  0.7461,  0.3298, -0.6943],
        [-0.2798, -0.3472,  0.6245,  ...,  0.4204,  0.1783, -0.8335]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[-0.3445, -0.7329,  0.3843,  ...,  0.3315,  0.1395, -0.8501],
        [-0.1368, -0.8452,  0.5264,  ...,  0.3423,  0.2896, -0.6855],
        [-0.0875, -0.7412,  0.6392,  ...,  0.4570,  0.3784, -0.8335],
        ...,
        [-0.2373, -0.7061,  0.4507,  ...,  0.4927,  0.1068, -0.8921],
        [-0.1910, -0.7095,  0.5474,  ...,  0.3188,  0.1335, -0.6934],
        [-0.3083, -0.7881,  0.5942,  ...,  0.3613,  0.2793, -0.8242]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0707, -0.6372,  0.4939,  ...,  0.2937,  0.0449, -0.7227],
        [-0.1746, -0.4216,  0.4573,  ...,  0.2383,  0.0175, -0.7505],
        [-0.0317, -0.5298,  0.3420,  ...,  0.3611,  0.0025, -0.9189],
        ...,
        [-0.1299, -0.5908,  0.2428,  ...,  0.1165,  0.1075, -1.0020],
        [-0.0872, -0.4968,  0.3003,  ...,  0.5127,  0.0316, -0.7427],
        [ 0.0616, -0.5322,  0.3455,  ...,  0.3699,  0.1382, -0.7700]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.1508, -0.5791,  0.4988,  ...,  0.3726,  0.1855, -0.8257],
        [ 0.1610, -0.5742,  0.3718,  ...,  0.5210,  0.2590, -0.5801],
        [-0.0767, -0.5859,  0.4233,  ...,  0.4148,  0.2350, -0.8306],
        ...,
        [-0.2344, -0.7451,  0.5566,  ...,  0.3220,  0.2991, -0.6499],
        [ 0.0387, -0.6250,  0.5415,  ...,  0.4988,  0.2419, -0.6055],
        [-0.1364, -0.5874,  0.4399,  ...,  0.3684,  0.2408, -0.8745]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[ 0.0651, -0.4856,  0.3340,  ...,  0.5010,  0.0701, -0.6006],
        [ 0.0963, -0.3591,  0.4053,  ...,  0.2764,  0.0696, -0.7500],
        [-0.1616, -0.6738,  0.4448,  ...,  0.2666, -0.0516, -0.6802],
        ...,
        [ 0.1000, -0.4934,  0.3516,  ...,  0.2874,  0.1075, -0.8291],
        [-0.0173, -0.4070,  0.2605,  ...,  0.1224,  0.0236, -0.9199],
        [-0.1541, -0.6660,  0.4221,  ...,  0.2498, -0.0949, -0.8950]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.3052, -0.8301,  1.2559,  ..., -0.1099,  0.1284, -0.4714],
        [-0.3931, -1.1992,  1.2842,  ...,  0.3132,  0.2371, -0.9316],
        [-0.2374, -1.0820,  0.9531,  ...,  0.3025,  0.4390, -0.7832],
        [-0.3916, -0.8481,  1.1621,  ...,  0.1722,  0.2144, -0.8096],
        [-0.1815, -1.1572,  1.0527,  ...,  0.2013,  0.1407, -0.9214],
        [-0.2062, -0.8828,  1.0342,  ...,  0.0950,  0.1471, -0.8818]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([6, 512])
> 5.  tensor([[-0.1824, -0.5117,  0.5933,  ...,  0.0952,  0.3267, -0.8276],
        [-0.2002, -0.6670,  0.8667,  ...,  0.0198,  0.2720, -0.7490],
        [-0.1107, -0.6460,  0.5142,  ...,  0.2369,  0.4001, -0.8833],
        ...,
        [-0.2122, -0.5947,  0.3179,  ...,  0.3523,  0.3989, -0.8428],
        [ 0.0266, -0.6172,  0.8418,  ...,  0.0733,  0.1047, -0.5825],
        [-0.0851, -0.5273,  0.6255,  ...,  0.1975,  0.1990, -0.6841]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.4604, -0.5972,  0.4570,  ...,  0.3218,  0.4465, -0.8359],
        [-0.2313, -0.7588,  0.9619,  ...,  0.4211,  0.4041, -0.7856],
        [-0.1295, -0.5210,  0.6016,  ...,  0.3066,  0.2617, -0.6895],
        ...,
        [-0.2158, -0.7651,  0.5205,  ...,  0.2637,  0.4602, -0.5806],
        [-0.0925, -0.5801,  0.5464,  ...,  0.4187,  0.2600, -0.8975],
        [-0.1035, -0.5854,  0.5645,  ...,  0.2373,  0.5435, -0.5674]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[-0.0515, -0.6558,  0.4031,  ...,  0.3818,  0.0729, -0.8364],
        [-0.1952, -0.6655,  0.4048,  ...,  0.2849,  0.1475, -0.7065],
        [-0.0881, -0.7959,  0.5591,  ...,  0.4507,  0.1550, -0.8662],
        ...,
        [-0.1465, -0.4229,  0.4233,  ...,  0.4834,  0.1550, -0.7632],
        [-0.3054, -0.6558,  0.5415,  ...,  0.3577,  0.1880, -0.9404],
        [-0.1252, -0.6411,  0.3899,  ...,  0.4062, -0.1696, -0.9893]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1796, -0.7568,  0.3555,  ...,  0.5210,  0.2041, -0.9458],
        [-0.0193, -0.7114,  0.3206,  ...,  0.5269,  0.1356, -0.6860],
        [-0.2318, -0.5796,  0.2415,  ...,  0.4521,  0.2554, -0.8228],
        ...,
        [-0.0728, -0.7617,  0.3909,  ...,  0.5361,  0.2378, -0.7515],
        [-0.0515, -0.6948,  0.2749,  ...,  0.5146,  0.0495, -0.5786],
        [-0.1509, -0.8364,  0.4619,  ...,  0.4822,  0.0191, -0.7930]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.2070, -0.7344,  0.3330,  ...,  0.5786,  0.3167, -0.6670],
        [-0.0820, -0.9746,  0.6309,  ...,  0.3552,  0.1937, -0.6704],
        [-0.0880, -0.7837,  0.4456,  ...,  0.4382,  0.1997, -0.9448],
        ...,
        [-0.1541, -0.9629,  0.4272,  ...,  0.5156,  0.1447, -0.7476],
        [-0.1788, -0.7202,  0.3398,  ...,  0.5278,  0.0932, -0.6919],
        [-0.0912, -0.7075,  0.4246,  ...,  0.4197,  0.1415, -0.7998]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.0936, -0.4888,  0.3110,  ...,  0.3857, -0.0683, -0.8696],
        [-0.0489, -0.5259,  0.4338,  ...,  0.3557, -0.0459, -0.8472],
        [-0.2039, -0.5693,  0.4587,  ...,  0.3252, -0.0881, -0.8418],
        ...,
        [-0.0352, -0.5928,  0.3235,  ...,  0.4185,  0.0707, -0.7271],
        [ 0.0656, -0.5176,  0.2111,  ...,  0.3557,  0.0687, -0.7988],
        [-0.0822, -0.5264,  0.2830,  ...,  0.4854,  0.0723, -0.7339]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.4629, -0.3579,  0.4644,  ...,  0.3376,  0.4897, -0.4915],
        [-0.1214,  0.1191,  0.6807,  ...,  0.6924,  0.3267, -0.6436],
        [-0.2422, -0.1340,  0.8491,  ...,  0.3691,  0.8579, -0.5190],
        ...,
        [-0.0728, -0.0178,  0.5835,  ...,  0.4194,  0.0945, -0.4038],
        [-0.5186, -0.2988,  0.2029,  ..., -0.0030,  0.8345, -0.5366],
        [-0.3088, -0.2935,  0.6753,  ...,  0.3774,  0.4082, -0.5278]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([96, 512])
> 5.  tensor([[ 0.0311, -0.5400,  0.2347,  ...,  0.4922,  0.2483, -0.7900],
        [-0.0209, -0.4478,  0.2805,  ...,  0.4082,  0.0243, -0.9561],
        [-0.1256, -0.4961,  0.2563,  ...,  0.3481,  0.0185, -0.8179],
        ...,
        [ 0.0177, -0.6226,  0.3772,  ...,  0.4058,  0.0880, -0.8389],
        [-0.1786, -0.4939,  0.1974,  ...,  0.5278, -0.0742, -0.9111],
        [-0.1328, -0.5132,  0.4426,  ...,  0.4900, -0.0426, -0.6294]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1357, -0.7417,  1.0889,  ...,  0.2148,  0.3682, -0.7310],
        [-0.4900, -0.8237,  0.8569,  ..., -0.1984, -0.0049, -1.2686],
        [-0.1074, -1.0625,  1.2656,  ..., -0.0669,  0.1475, -1.3848],
        [-0.3564, -0.5444,  1.1963,  ...,  0.3560,  0.1903, -0.6558],
        [-0.2106, -0.9893,  0.9092,  ...,  0.0406,  0.2292, -1.1875]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([5, 512])
> 5.  tensor([[-0.0627, -0.6240,  0.3982,  ...,  0.3586,  0.1515, -0.6636],
        [-0.0659, -0.6992,  0.3789,  ...,  0.1781,  0.1173, -0.8799],
        [ 0.0057, -0.3743,  0.3369,  ...,  0.3845,  0.0740, -0.7251],
        ...,
        [-0.0280, -0.4502,  0.3162,  ...,  0.1196, -0.0062, -0.8101],
        [-0.2306, -0.5308,  0.3486,  ...,  0.2128,  0.0585, -0.7178],
        [-0.1538, -0.5542,  0.4333,  ...,  0.3926,  0.1851, -0.9126]],
       device='cuda:3', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1403, -0.6147,  0.4392,  ...,  0.1975,  0.1871, -1.0244],
        [-0.0750, -0.5312,  0.3525,  ...,  0.4775, -0.0442, -0.8872],
        [-0.0553, -0.4614,  0.2805,  ...,  0.3777, -0.0978, -0.7021],
        ...,
        [-0.0936, -0.5137,  0.0566,  ...,  0.2354,  0.0737, -0.9282],
        [-0.0870, -0.4287,  0.3010,  ...,  0.0857,  0.0240, -0.8916],
        [ 0.0680, -0.6724,  0.3193,  ...,  0.3892,  0.1949, -0.8901]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1449, -0.6143,  0.4670,  ...,  0.3533,  0.1172, -0.8149],
        [ 0.0524, -0.4617,  0.3628,  ...,  0.2920,  0.0873, -0.6978],
        [-0.1644, -0.5312,  0.2974,  ...,  0.3669,  0.0706, -0.8486],
        ...,
        [-0.0689, -0.4946,  0.4341,  ...,  0.3225,  0.0768, -0.8511],
        [-0.0603, -0.6284,  0.4167,  ...,  0.1323,  0.1541, -0.9146],
        [-0.0012, -0.5098,  0.4543,  ...,  0.1777,  0.0722, -0.9409]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0048, -0.5215,  0.5723,  ...,  0.5356,  0.1382, -0.6265],
        [-0.0221, -0.7261,  0.5645,  ...,  0.4202,  0.3345, -0.6914],
        [ 0.0140, -0.6470,  0.4636,  ...,  0.3848,  0.3062, -0.6572],
        ...,
        [-0.1160, -0.4353,  0.6992,  ...,  0.3547,  0.3555, -0.6514],
        [-0.2610, -0.4429,  0.5234,  ...,  0.3501,  0.2615, -0.7114],
        [ 0.0440, -0.5776,  0.4373,  ...,  0.3706,  0.2087, -0.8174]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.3140, -0.7690,  0.3511,  ..., -0.0043, -0.1092, -0.6099],
        [-0.2007, -0.5669,  0.3179,  ...,  0.4441, -0.0285, -0.8203],
        [-0.2402, -0.6284,  0.4946,  ...,  0.3325,  0.1683, -1.0303],
        ...,
        [-0.1550, -0.4900,  0.4919,  ...,  0.4192,  0.0178, -0.8071],
        [-0.2046, -0.4070,  0.4209,  ...,  0.4019,  0.0241, -0.9688],
        [-0.0743, -0.5410,  0.4326,  ...,  0.2191, -0.1187, -0.7632]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0819, -0.4360,  0.2729,  ...,  0.4971, -0.0287, -0.8877],
        [-0.2030, -0.5444,  0.4744,  ...,  0.2195, -0.0620, -0.8027],
        [ 0.0020, -0.5913,  0.3501,  ...,  0.3689, -0.0123, -0.9307],
        ...,
        [-0.2888, -0.4421,  0.2766,  ...,  0.2213, -0.0506, -0.8906],
        [-0.1630, -0.6406,  0.2515,  ...,  0.2859, -0.0686, -0.7456],
        [-0.2056, -0.5386,  0.2427,  ...,  0.4763,  0.1989, -0.9170]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1362, -0.6812,  0.5786,  ...,  0.6475,  0.6240, -0.4180],
        [ 0.0749, -0.4236,  0.8159,  ...,  0.4207,  0.4224, -0.3860],
        [-0.3472, -0.6025,  0.7344,  ...,  0.3972,  0.6978, -0.7485],
        ...,
        [-0.2559, -0.7437,  0.8071,  ...,  0.6416,  0.8027, -0.7031],
        [ 0.0254, -0.7051,  0.5225,  ...,  0.1707,  0.4792, -0.6436],
        [-0.1691, -0.7798,  0.9326,  ...,  0.4629,  0.6914, -0.7437]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([56, 512])
> 5.  tensor([[-0.2996, -0.7061,  0.7139,  ...,  0.3826,  0.6221, -0.7739],
        [-0.1288, -0.3635,  0.8364,  ...,  0.8735,  0.7407, -0.5151],
        [-0.6118, -0.7241,  0.9092,  ...,  0.2311,  0.4656, -0.7354],
        ...,
        [-0.2742, -0.1216,  0.5630,  ...,  0.5737,  0.5698, -0.7700],
        [-0.4250, -0.5518,  0.4363,  ...,  0.2330,  0.7114, -0.4934],
        [-0.0147, -0.4724,  0.6621,  ...,  0.8662,  0.6587, -0.8687]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([56, 512])
> 5.  tensor([[-0.2078, -0.6211,  0.2388,  ...,  0.4473,  0.2487, -0.8022],
        [-0.1009, -0.8672,  0.4814,  ...,  0.3923,  0.1853, -0.6758],
        [-0.1886, -0.8486,  0.4360,  ...,  0.1422,  0.1709, -0.8135],
        ...,
        [-0.1836, -0.8057,  0.5044,  ...,  0.3936,  0.0858, -0.8579],
        [-0.1053, -0.4978,  0.3589,  ...,  0.4067,  0.1841, -0.7402],
        [-0.1357, -0.8647,  0.4880,  ...,  0.4602,  0.2827, -0.8110]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1238, -0.7583,  0.3389,  ...,  0.2377,  0.3152, -0.6177],
        [-0.1370, -0.8125,  0.5034,  ...,  0.4395,  0.2473, -0.5947],
        [-0.2759, -0.7920,  0.5767,  ...,  0.3540,  0.1602, -0.6289],
        ...,
        [-0.2449, -0.9141,  0.5547,  ...,  0.5562,  0.1152, -0.7100],
        [-0.1078, -0.9053,  0.4902,  ...,  0.2311,  0.1267, -0.7441],
        [-0.1058, -0.6870,  0.4504,  ...,  0.5762,  0.3572, -0.7329]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.3152, -0.1978,  0.5786,  ...,  0.4341,  0.6694, -0.7383],
        [ 0.0773, -0.4810,  0.5557,  ...,  0.4966,  0.3716, -0.1559],
        [-0.8779, -0.6216,  0.9258,  ...,  0.4692,  0.5415, -0.7930],
        ...,
        [-0.6064, -0.5171,  0.3450,  ...,  0.1528,  0.5337, -0.8438],
        [-0.6079, -0.3513,  0.3184,  ...,  0.3040,  0.2930, -0.7773],
        [-0.2346, -0.6284,  0.8823,  ...,  0.4268,  0.3359, -0.4963]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([88, 512])
> 5.  tensor([[ 0.0335, -0.5537,  0.3130,  ...,  0.3040,  0.0274, -0.8657],
        [-0.0582, -0.5371,  0.4126,  ...,  0.3547,  0.0819, -0.8564],
        [ 0.1024, -0.4492,  0.4441,  ...,  0.2449,  0.1163, -0.8301],
        ...,
        [-0.0424, -0.4216,  0.2659,  ...,  0.4353,  0.2140, -0.8521],
        [-0.0058, -0.4358,  0.3423,  ...,  0.2668,  0.1113, -0.7471],
        [ 0.0147, -0.5542,  0.2769,  ...,  0.4429,  0.0501, -0.7583]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0470, -0.6318,  0.4558,  ...,  0.2434,  0.2247, -0.8511],
        [-0.1075, -0.4875,  0.5513,  ...,  0.2045,  0.0434, -0.7559],
        [-0.0238, -0.4714,  0.4307,  ...,  0.3130,  0.1378, -0.8213],
        ...,
        [-0.2362, -0.4912,  0.6821,  ...,  0.1615,  0.2372, -0.8271],
        [-0.0230, -0.5503,  0.5381,  ...,  0.4001,  0.0426, -0.7822],
        [-0.1327, -0.4087,  0.4412,  ...,  0.4399,  0.3030, -0.6841]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0674, -0.5396,  0.2725,  ...,  0.5469,  0.1777, -0.8882],
        [-0.1578, -0.6738,  0.2734,  ...,  0.3857,  0.2494, -0.9146],
        [-0.1644, -0.5791,  0.4722,  ...,  0.3711,  0.1823, -0.7710],
        ...,
        [-0.1721, -0.5947,  0.3081,  ...,  0.6240,  0.0807, -0.6724],
        [-0.0399, -0.8086,  0.4041,  ...,  0.3340,  0.2479, -0.9629],
        [-0.1859, -0.6055,  0.2367,  ...,  0.4775,  0.5747, -0.6772]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1346, -0.2590,  0.7451,  ...,  0.6909,  0.8457, -0.6743],
        [-0.2186, -0.3330,  0.8535,  ...,  0.3530,  0.7085, -0.3948],
        [-0.2520, -0.2935,  0.8486,  ...,  0.3916,  0.7036, -0.5098],
        ...,
        [-0.5908, -0.0877,  0.6792,  ...,  0.3105,  0.5864, -0.4038],
        [-0.4001, -0.2020,  1.0234,  ...,  0.2913,  0.4619, -0.3962],
        [-0.3335, -0.3892,  0.7637,  ...,  0.8154,  0.7983, -0.5737]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([80, 512])
> 5.  tensor([[-0.0205, -0.6689,  0.3638,  ...,  0.5366,  0.1321, -0.6987],
        [-0.0719, -0.5864,  0.3005,  ...,  0.5342,  0.2433, -0.8823],
        [-0.0671, -0.7417,  0.5532,  ...,  0.4712,  0.2148, -0.7295],
        ...,
        [-0.0372, -0.8535,  0.4609,  ...,  0.5562,  0.1996, -0.8281],
        [-0.1149, -0.7612,  0.4668,  ...,  0.4395,  0.1165, -0.7114],
        [-0.2520, -0.7690,  0.5713,  ...,  0.5239,  0.3745, -1.0977]],
       device='cuda:4', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1155, -0.8730,  0.4824,  ...,  0.5693,  0.1664, -0.8784],
        [-0.0991, -0.5273,  0.4766,  ..., -0.0856,  0.1852, -0.3706],
        [ 0.0678, -0.6489,  0.3477,  ...,  0.4990,  0.1968, -0.5244],
        ...,
        [-0.0758, -0.7295,  0.6113,  ...,  0.6274,  0.2489, -0.6509],
        [-0.0856, -0.7505,  0.6885,  ...,  0.5342,  0.2825, -0.7588],
        [-0.0353, -0.8179,  0.6226,  ...,  0.5742,  0.2107, -0.6646]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.2251, -0.8545,  1.1172,  ...,  0.2463,  0.2494, -0.7544],
        [-0.4766, -0.9868,  1.1250,  ...,  0.0610,  0.2908, -0.7051],
        [-0.3040, -0.8711,  1.1230,  ...,  0.4709,  0.3704, -0.8667],
        [-0.4045, -0.7271,  0.8735,  ...,  0.1700,  0.2583, -0.8745],
        [-0.1821, -0.9800,  1.0791,  ...,  0.3867,  0.4329, -0.9609],
        [-0.4160, -0.8125,  0.9722,  ...,  0.3323,  0.4102, -0.9854]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([6, 512])
> 5.  tensor([[-0.2866, -0.3091,  0.7729,  ...,  0.3418,  0.1488, -0.7637],
        [-0.2125, -0.6709,  0.8198,  ...,  0.3413,  0.1990, -0.6694],
        [-0.1133, -0.7358,  0.5488,  ...,  0.3516,  0.2034, -0.7456],
        ...,
        [ 0.0021, -0.7412,  0.8457,  ...,  0.4902,  0.3110, -0.8306],
        [-0.2239, -0.6987,  0.8657,  ...,  0.4585,  0.2456, -0.8081],
        [-0.1594, -0.6753,  0.8115,  ...,  0.5894,  0.3511, -0.7632]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.1995, -0.5244,  0.3142,  ...,  0.0966,  0.0829, -0.8433],
        [-0.1322, -0.7212,  0.2920,  ...,  0.3564,  0.0628, -0.9355],
        [-0.3289, -0.5068,  0.4519,  ...,  0.4199, -0.0078, -0.7148],
        ...,
        [-0.1796, -0.5337,  0.4102,  ...,  0.4131,  0.0878, -0.8154],
        [-0.2214, -0.6118,  0.5728,  ...,  0.1815,  0.1935, -0.9785],
        [-0.1776, -0.5879,  0.2140,  ...,  0.2222,  0.0237, -1.0166]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0362, -0.4592,  0.5825,  ...,  0.3333,  0.5381, -0.7109],
        [ 0.0309, -0.6489,  0.3569,  ...,  0.4302,  0.6880, -0.7075],
        [-0.0617, -0.4705,  0.6099,  ...,  0.1858,  0.4880, -0.5449],
        ...,
        [-0.0233, -0.6646,  0.3982,  ...,  0.4807,  0.3721, -0.8613],
        [ 0.0527, -0.5195,  0.7651,  ...,  0.4272,  0.3083, -0.7446],
        [-0.3333, -0.9688,  0.7925,  ...,  0.3147,  0.3511, -0.7432]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[-0.1687, -0.4702,  0.3440,  ...,  0.2617, -0.1130, -0.8696],
        [-0.0927, -0.6626,  0.3584,  ...,  0.4258, -0.0518, -0.9985],
        [-0.0931, -0.6567,  0.5947,  ...,  0.3008,  0.2250, -0.9585],
        ...,
        [-0.2617, -0.5698,  0.4192,  ...,  0.5015,  0.0459, -0.7930],
        [-0.1964, -0.4937,  0.4084,  ...,  0.2942,  0.1090, -0.7939],
        [-0.1097, -0.6172,  0.3367,  ...,  0.5557,  0.0936, -0.6860]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.2715, -0.5850,  0.5171,  ...,  0.2416,  0.1737, -0.7695],
        [-0.1846, -0.5488,  0.3245,  ...,  0.4006,  0.1581, -0.9033],
        [-0.3040, -0.5386,  0.3921,  ...,  0.3362, -0.1936, -0.7217],
        ...,
        [-0.1765, -0.6284,  0.3652,  ...,  0.3635, -0.0554, -0.8481],
        [-0.2468, -0.6572,  0.4316,  ...,  0.5083, -0.0263, -0.7666],
        [-0.1913, -0.5005,  0.3420,  ...,  0.3469,  0.0537, -0.9614]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1738, -0.9761,  0.3704,  ...,  0.3191,  0.4036, -0.7021],
        [-0.2281, -0.6724,  0.5381,  ...,  0.2300,  0.3184, -0.7686],
        [-0.1508, -0.7153,  0.4180,  ...,  0.3616,  0.2637, -0.7637],
        ...,
        [-0.2086, -0.7905,  0.4907,  ...,  0.3486,  0.4800, -0.7183],
        [-0.2078, -0.7280,  0.3994,  ...,  0.3936,  0.2634, -0.8013],
        [-0.1873, -0.8364,  0.3718,  ...,  0.4863,  0.3318, -0.7847]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.1648, -0.6714,  0.7314,  ...,  0.2141,  0.4744, -0.7222],
        [-0.1224, -0.5356,  0.6152,  ...,  0.2544,  0.2554, -0.7900],
        [-0.1689, -0.5039,  0.3638,  ...,  0.1838,  0.4922, -0.9453],
        ...,
        [-0.1127, -0.4783,  0.5303,  ...,  0.2189,  0.1512, -0.7178],
        [-0.0448, -0.4812,  0.5874,  ...,  0.3022,  0.1791, -0.7988],
        [-0.2335, -0.6377,  0.7778,  ...,  0.2859,  0.3096, -0.9297]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0234, -0.4514,  0.3127,  ...,  0.1455, -0.0355, -0.7231],
        [-0.0169, -0.5474,  0.4387,  ...,  0.2561, -0.0854, -0.7983],
        [-0.0832, -0.7021,  0.4329,  ...,  0.4089,  0.0404, -0.8945],
        ...,
        [-0.0765, -0.5718,  0.4438,  ...,  0.3909, -0.0809, -0.7637],
        [ 0.0303, -0.4692,  0.1735,  ...,  0.3140,  0.0687, -0.8164],
        [-0.1123, -0.6440,  0.4585,  ...,  0.3555,  0.1105, -0.7817]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1566, -0.4805,  0.6929,  ...,  0.3857,  0.2267, -0.6416],
        [-0.1472, -0.5815,  0.5049,  ...,  0.3713,  0.2098, -0.7876],
        [-0.1729, -0.7749,  0.6924,  ...,  0.1144,  0.3198, -0.9146],
        ...,
        [-0.1649, -0.5405,  0.5996,  ...,  0.3333,  0.2277, -0.6460],
        [-0.2495, -0.6470,  0.6206,  ...,  0.2517,  0.4109, -0.8916],
        [ 0.0860, -0.5566,  0.6655,  ...,  0.2551,  0.1423, -1.0615]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1561, -0.4749,  0.3591,  ...,  0.2358,  0.0640, -0.7939],
        [-0.0812, -0.3950,  0.3103,  ...,  0.2910, -0.1925, -0.7466],
        [-0.0550, -0.4348,  0.4238,  ..., -0.4509, -0.3101, -0.5479],
        ...,
        [-0.0977, -0.6226,  0.3652,  ...,  0.3206, -0.0942, -0.9648],
        [ 0.0332, -0.6665,  0.4304,  ...,  0.1841, -0.0188, -0.8579],
        [-0.1081, -0.4861,  0.2112,  ...,  0.4045,  0.0181, -0.7251]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0214, -0.5024,  0.3015,  ...,  0.4275,  0.1160, -0.9468],
        [ 0.0047, -0.6089,  0.3479,  ...,  0.1802,  0.0869, -0.6177],
        [ 0.0392, -0.3972,  0.3547,  ...,  0.3267, -0.0698, -0.7012],
        ...,
        [ 0.1205, -0.6768,  0.4395,  ...,  0.1910,  0.1453, -0.8462],
        [-0.0901, -0.3691,  0.4399,  ...,  0.3440,  0.0409, -0.7710],
        [-0.0911, -0.4434,  0.2134,  ...,  0.4250, -0.0177, -0.8530]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.2690, -0.9692,  0.9902,  ...,  0.2400,  0.4055, -1.1133],
        [-0.4387, -0.6904,  1.0459,  ..., -0.2443,  0.0918, -0.6372],
        [-0.3787, -0.9888,  1.2695,  ...,  0.2305,  0.0543, -0.7632],
        [-0.2949, -0.8472,  1.2246,  ..., -0.1482,  0.1486, -1.1562],
        [-0.2759, -0.7368,  1.0986,  ..., -0.0159,  0.1401, -1.0322]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([5, 512])
> 5.  tensor([[-0.4661, -0.5405,  0.9473,  ...,  0.6733,  0.2961, -0.4666],
        [-0.3975, -0.1885,  0.3372,  ...,  0.5850,  0.9185, -0.5283],
        [-0.1390,  0.2009,  0.3757,  ...,  0.6738,  0.5332, -0.5293],
        ...,
        [-0.5425, -0.1633,  0.2396,  ...,  0.5303,  0.3420, -0.5229],
        [-0.0668, -0.4053,  0.5435,  ...,  0.5640,  0.4961, -0.1661],
        [-0.6382, -0.7261,  0.5610,  ...,  1.0381,  0.9912, -0.6108]],
       device='cuda:2', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([120, 512])
> 5.  tensor([[-0.1721, -0.4233,  0.5571,  ...,  0.0550,  0.0595, -0.7759],
        [-0.0636, -0.3801,  0.5405,  ...,  0.4639,  0.2075, -0.8525],
        [-0.0525, -0.6187,  0.6021,  ...,  0.3616,  0.1991, -0.8887],
        ...,
        [-0.2238, -0.4509,  0.5234,  ...,  0.4121,  0.3528, -0.9297],
        [-0.0875, -0.7256,  0.6147,  ...,  0.2559,  0.0659, -0.7095],
        [-0.0394, -0.5200,  0.5918,  ...,  0.2052,  0.0202, -0.7798]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0580, -0.3833,  0.2568,  ...,  0.3748,  0.0232, -0.8320],
        [ 0.0240, -0.3789,  0.4014,  ...,  0.3225,  0.1357, -0.7554],
        [-0.0562, -0.6313,  0.3594,  ...,  0.2673,  0.0036, -0.6992],
        ...,
        [ 0.0793, -0.5264,  0.3528,  ...,  0.3093,  0.0648, -0.9688],
        [ 0.0744, -0.5107,  0.2878,  ...,  0.2769,  0.1093, -0.6860],
        [-0.0416, -0.4792,  0.3354,  ...,  0.3591,  0.0427, -0.6548]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1338, -0.4912,  0.5454,  ...,  0.2462,  0.0141, -0.8369],
        [-0.1348, -0.5830,  0.2961,  ...,  0.3848,  0.2822, -0.8071],
        [-0.2108, -0.4893,  0.4629,  ...,  0.2583,  0.1006, -0.9673],
        ...,
        [ 0.0513, -0.3965,  0.4312,  ...,  0.5020,  0.0352, -0.7495],
        [-0.0292, -0.6240,  0.4099,  ...,  0.5151,  0.3528, -0.7803],
        [-0.0487, -0.4470,  0.4160,  ...,  0.2352,  0.0342, -0.8267]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0949, -0.6396,  0.5864,  ...,  0.3848,  0.1466, -0.5977],
        [-0.0663, -0.6323,  0.6226,  ...,  0.4126,  0.2253, -0.8237],
        [ 0.0357, -0.8062,  0.3315,  ...,  0.4355,  0.1891, -0.8110],
        ...,
        [-0.3086, -0.7002,  0.6006,  ...,  0.3037,  0.2251, -0.7549],
        [-0.0176, -0.6875,  0.5269,  ...,  0.5186,  0.2104, -0.6299],
        [-0.0922, -0.6606,  0.4890,  ...,  0.4504,  0.2074, -0.7798]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.1958, -0.8984,  0.5454,  ...,  0.4653,  0.6108, -0.9668],
        [-0.2444, -0.5728,  0.6133,  ...,  0.5205,  0.5762, -0.8726],
        [-0.1626, -0.5151,  0.6108,  ...,  0.4773,  0.4841, -0.8037],
        ...,
        [-0.1670, -0.7007,  0.4016,  ...,  0.5107,  0.3337, -0.8711],
        [-0.2491, -0.6328,  0.6650,  ...,  0.5479,  0.6533, -0.8018],
        [-0.2634, -0.5708,  0.7568,  ...,  0.5786,  0.5103, -0.7183]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[-0.1066, -0.7310,  1.0684,  ...,  0.0929,  0.1633, -0.6484],
        [-0.0486, -0.6650,  0.9331,  ..., -0.0071,  0.1912, -0.7080],
        [-0.1355, -0.3628,  0.9937,  ...,  0.1199,  0.2725, -0.7510],
        ...,
        [-0.2010, -0.6021,  0.7183,  ...,  0.2406,  0.2910, -0.8525],
        [-0.1942, -0.7104,  0.9907,  ...,  0.2754,  0.4133, -0.5791],
        [ 0.0144, -0.8770,  0.4053,  ...,  0.1085,  0.4663, -1.0566]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([7, 512])
> 5.  tensor([[-0.2332, -0.5381,  0.6938,  ...,  0.4966,  0.2803, -0.7969],
        [-0.0912, -0.5098,  0.5781,  ...,  0.2056,  0.1322, -0.7969],
        [ 0.0490, -0.5835,  0.3035,  ...,  0.2539,  0.1345, -0.9692],
        ...,
        [-0.1466, -0.6357,  0.7017,  ...,  0.4646,  0.3728, -0.7261],
        [-0.0585, -0.4690,  0.5688,  ...,  0.3845,  0.2375, -0.7119],
        [-0.1757, -0.6235,  0.5503,  ...,  0.3274,  0.2076, -0.7759]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.3662, -0.5977,  0.8252,  ...,  0.3452,  0.1495, -0.7188],
        [-0.0322, -0.7114,  0.5625,  ...,  0.5171,  0.4097, -0.6816],
        [-0.2705, -0.6123,  0.8130,  ...,  0.4368,  0.0994, -0.6909],
        ...,
        [-0.1765, -0.4851,  0.7222,  ...,  0.4446,  0.2158, -1.0898],
        [-0.4104, -0.7285,  0.9312,  ...,  0.6333,  0.2585, -0.5034],
        [-0.0594, -0.5293,  0.6582,  ...,  0.3472,  0.0406, -1.0205]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([32, 512])
> 5.  tensor([[-0.0331, -0.5928,  0.3528,  ...,  0.1464,  0.0946, -1.0176],
        [ 0.0662, -0.3765,  0.4075,  ...,  0.3674, -0.0200, -0.7896],
        [ 0.0466, -0.5508,  0.2443,  ...,  0.4204, -0.0365, -0.6953],
        ...,
        [-0.0368, -0.4890,  0.3601,  ...,  0.3076,  0.1754, -0.8667],
        [-0.0393, -0.5073,  0.2476,  ...,  0.2291,  0.0464, -0.8813],
        [ 0.1147, -0.4746,  0.3206,  ...,  0.2915,  0.0707, -0.8472]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1577, -0.5815,  0.3574,  ...,  0.5156,  0.2045, -0.9976],
        [-0.2424, -0.5947,  0.5127,  ...,  0.2452,  0.0083, -0.6509],
        [-0.1787, -0.7480,  0.2939,  ...,  0.2998,  0.1349, -0.7158],
        ...,
        [-0.2788, -0.7241,  0.5664,  ...,  0.5122, -0.0613, -0.6743],
        [-0.0875, -0.6914,  0.5459,  ...,  0.5190,  0.3455, -0.7656],
        [-0.0616, -0.6138,  0.2058,  ...,  0.4702,  0.2080, -0.8706]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0632, -0.6689,  0.6108,  ...,  0.3787,  0.2756, -0.6865],
        [-0.1949, -0.8423,  0.4570,  ...,  0.3167,  0.4109, -0.9600],
        [-0.0838, -0.7422,  0.4482,  ...,  0.3757,  0.3652, -0.7476],
        ...,
        [-0.0554, -0.8496,  0.5044,  ...,  0.4912,  0.2898, -0.5713],
        [-0.3286, -0.8784,  0.6963,  ...,  0.5181,  0.1321, -0.8857],
        [ 0.0552, -0.7114,  0.4541,  ...,  0.4634,  0.2517, -0.7236]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.1431, -0.6890,  0.3103,  ...,  0.4531,  0.1532, -0.8755],
        [-0.0539, -0.5366,  0.3982,  ...,  0.5605,  0.0777, -0.7891],
        [-0.2448, -0.5386,  0.4099,  ...,  0.3962,  0.0473, -0.8003],
        ...,
        [-0.1772, -0.5576,  0.4004,  ...,  0.5815,  0.1998, -0.7617],
        [-0.2817, -0.6650,  0.4404,  ...,  0.5190,  0.2520, -0.7837],
        [-0.2006, -0.5591,  0.3455,  ...,  0.2957, -0.1187, -0.8359]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.4216, -0.9653,  1.0898,  ...,  0.0505,  0.2593, -0.8403],
        [-0.3179, -0.9902,  1.0420,  ...,  0.2983,  0.4995, -0.7954],
        [-0.6602, -0.8311,  1.2744,  ...,  0.3118,  0.2152, -1.1748],
        [-0.2184, -1.0410,  1.2334,  ...,  0.3059,  0.2271, -0.9453],
        [-0.2549, -1.2432,  0.9756,  ..., -0.2754,  0.2133, -0.5703]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([5, 512])
> 5.  tensor([[-0.1815, -0.4941,  0.3682,  ...,  0.3284,  0.0950, -0.8730],
        [-0.1091, -0.4399,  0.4102,  ...,  0.2469,  0.0914, -0.8169],
        [-0.0740, -0.5557,  0.3818,  ...,  0.2866,  0.0995, -0.7441],
        ...,
        [-0.0618, -0.5527,  0.3655,  ...,  0.2786,  0.0659, -0.6729],
        [ 0.0555, -0.5161,  0.3528,  ...,  0.3826,  0.1340, -0.7578],
        [-0.0945, -0.5303,  0.3535,  ...,  0.3684,  0.1147, -0.6396]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1100, -0.6108,  0.4360,  ...,  0.3567,  0.0323, -0.9419],
        [-0.0996, -0.4104,  0.3474,  ...,  0.0596, -0.0256, -0.7886],
        [ 0.0763, -0.6519,  0.3972,  ...,  0.4290,  0.1042, -0.7827],
        ...,
        [-0.0426, -0.4216,  0.3787,  ...,  0.2969,  0.0514, -0.6562],
        [-0.1555, -0.5635,  0.2651,  ...,  0.4119,  0.0848, -0.7446],
        [-0.0382, -0.5049,  0.3206,  ...,  0.3557,  0.0107, -0.7705]],
       device='cuda:5', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0776, -0.5591,  0.2771,  ...,  0.5610,  0.1158, -0.6782],
        [-0.0370, -0.6006,  0.2715,  ...,  0.2451,  0.0559, -0.7749],
        [-0.0065, -0.5542,  0.3223,  ...,  0.3215, -0.1000, -0.5981],
        ...,
        [ 0.0627, -0.6660,  0.3406,  ...,  0.0997, -0.0018, -0.8613],
        [-0.0490, -0.4912,  0.4006,  ...,  0.2236,  0.0106, -0.8735],
        [-0.1442, -0.4385,  0.3162,  ...,  0.2336,  0.0527, -0.8584]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1730, -0.5957,  0.3079,  ...,  0.3164,  0.0575, -0.6973],
        [-0.1721, -0.5107,  0.3911,  ...,  0.2686,  0.0386, -0.9248],
        [-0.1328, -0.5073,  0.2854,  ...,  0.3071, -0.0348, -0.7065],
        ...,
        [-0.1755, -0.5815,  0.2991,  ...,  0.2228,  0.0132, -0.8511],
        [-0.0640, -0.4294,  0.3804,  ...,  0.3242, -0.0423, -0.8115],
        [-0.1156, -0.6021,  0.2001,  ...,  0.4639, -0.0646, -0.7891]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[ 0.0435, -0.6440,  0.2316,  ...,  0.3308,  0.1261, -0.8931],
        [-0.0246, -0.4087,  0.3198,  ...,  0.3723,  0.1162, -0.8413],
        [-0.0632, -0.4155,  0.3276,  ...,  0.4624,  0.0887, -0.9058],
        ...,
        [-0.0829, -0.5620,  0.3660,  ...,  0.2778,  0.1105, -0.8887],
        [ 0.0996, -0.7188,  0.4336,  ...,  0.3494,  0.1345, -0.7866],
        [ 0.0781, -0.5317,  0.4189,  ...,  0.4207,  0.1193, -0.7651]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0778, -0.5894,  0.3279,  ...,  0.4856,  0.0930, -0.7368],
        [-0.0429, -0.6377,  0.3279,  ...,  0.2949,  0.1200, -0.8330],
        [-0.0108, -0.4805,  0.5195,  ...,  0.1754,  0.0056, -0.6616],
        ...,
        [ 0.0150, -0.6494,  0.4558,  ...,  0.2944,  0.1765, -0.5986],
        [-0.1218, -0.4419,  0.1724,  ...,  0.3071,  0.3345, -0.7808],
        [ 0.1190, -0.4551,  0.2529,  ...,  0.3003,  0.2084, -0.8804]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0504, -0.5068,  0.2188,  ...,  0.4846,  0.1752, -0.6904],
        [ 0.0507, -0.6284,  0.2729,  ...,  0.4382,  0.0577, -0.9150],
        [ 0.0200, -0.5005,  0.2211,  ...,  0.5024,  0.0130, -0.6812],
        ...,
        [-0.0686, -0.4329,  0.1613,  ...,  0.3577,  0.1458, -0.8091],
        [-0.0710, -0.5923,  0.4211,  ...,  0.3467,  0.0840, -0.8091],
        [-0.1831, -0.5601,  0.4231,  ...,  0.2067,  0.1614, -0.8081]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1859, -0.5430,  0.7744,  ...,  0.6689,  0.5034, -0.5420],
        [-0.2595, -0.6250,  0.6201,  ...,  0.6045,  0.4751, -0.9883],
        [-0.1519, -0.5688,  0.8525,  ...,  0.5205,  0.4888, -1.0020],
        ...,
        [-0.2061, -0.4934,  0.9062,  ..., -0.0351,  0.3530, -0.8140],
        [ 0.1851, -0.5879,  0.5903,  ...,  0.8501,  0.6001, -0.7896],
        [-0.3230, -0.7524,  0.8022,  ...,  0.5693,  0.6104, -0.6968]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([40, 512])
> 5.  tensor([[-0.0583, -0.4341,  0.3000,  ...,  0.3152, -0.0656, -0.7637],
        [ 0.0777, -0.4866,  0.2512,  ...,  0.5322,  0.0419, -0.7612],
        [ 0.0098, -0.5659,  0.2445,  ...,  0.3586, -0.0580, -0.7886],
        ...,
        [-0.0966, -0.3750,  0.2559,  ...,  0.3428,  0.1094, -0.7935],
        [-0.1121, -0.8076,  0.3606,  ...,  0.1509,  0.0691, -1.0615],
        [-0.1324, -0.7563,  0.3801,  ...,  0.3364, -0.0102, -0.6948]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0865, -0.6938,  0.5005,  ...,  0.2607, -0.0427, -0.7456],
        [-0.0520, -0.4778,  0.3364,  ...,  0.3950, -0.0544, -0.9424],
        [-0.1300, -0.5635,  0.2098,  ...,  0.5083,  0.0961, -0.5903],
        ...,
        [-0.0958, -0.5571,  0.3594,  ...,  0.2271,  0.1036, -0.6880],
        [-0.0047, -0.4309,  0.2181,  ...,  0.2275, -0.0206, -0.9175],
        [-0.0012, -0.5894,  0.3027,  ...,  0.3525,  0.0089, -0.7231]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1053, -0.6987,  0.3767,  ...,  0.1746,  0.1033, -0.8081],
        [ 0.0126, -0.4771,  0.3149,  ...,  0.2393, -0.0767, -0.7388],
        [-0.1259, -0.5747,  0.2388,  ...,  0.2450,  0.0213, -0.9224],
        ...,
        [-0.0373, -0.5767,  0.3489,  ...,  0.2944,  0.0922, -0.9082],
        [ 0.0842, -0.5635,  0.4590,  ...,  0.1045, -0.0483, -0.7178],
        [-0.0947, -0.5073,  0.4978,  ...,  0.2228, -0.0270, -0.8262]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0612, -0.6694,  0.3950,  ...,  0.4355,  0.0607, -0.6836],
        [ 0.0842, -0.4436,  0.4548,  ...,  0.3848,  0.2627, -0.6562],
        [-0.0566, -0.4297,  0.4436,  ...,  0.3196,  0.0031, -0.7578],
        ...,
        [-0.0876, -0.4858,  0.3779,  ...,  0.1553,  0.1115, -0.7212],
        [-0.1227, -0.6821,  0.4146,  ...,  0.3311,  0.2242, -0.8833],
        [-0.0922, -0.5093,  0.4177,  ...,  0.2773,  0.0891, -0.6631]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1981, -0.6680,  0.5024,  ...,  0.4094,  0.1019, -0.6680],
        [-0.0479, -0.7563,  0.6226,  ...,  0.4688,  0.2115, -0.7798],
        [-0.2000, -0.7725,  0.4915,  ...,  0.3423,  0.5049, -0.5298],
        ...,
        [-0.1477, -0.8296,  0.6069,  ...,  0.5576,  0.2207, -0.6221],
        [-0.0945, -0.5708,  0.4558,  ...,  0.5493,  0.2129, -0.9180],
        [-0.0302, -0.7095,  0.5591,  ...,  0.4934,  0.2676, -0.5918]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.1198, -0.6016,  0.5806,  ...,  0.3450,  0.3665, -0.6973],
        [-0.0731, -0.5039,  0.5127,  ...,  0.4722,  0.2363, -0.6606],
        [-0.1265, -0.7407,  0.8203,  ...,  0.1975,  0.1484, -0.6157],
        ...,
        [-0.2981, -0.5073,  0.4814,  ...,  0.4089,  0.3723, -0.8408],
        [-0.1526, -0.5649,  0.5679,  ...,  0.3254,  0.2452, -0.7603],
        [-0.0377, -0.4417,  0.4949,  ...,  0.3484,  0.2864, -0.7803]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1456, -0.5737,  0.2810,  ...,  0.4846,  0.0501, -1.0244],
        [-0.2003, -0.4351,  0.1937,  ...,  0.5239,  0.0216, -0.8286],
        [-0.1621, -0.5542,  0.3560,  ...,  0.1874,  0.0118, -0.7236],
        ...,
        [-0.2554, -0.7339,  0.5225,  ...,  0.5576,  0.0492, -0.9380],
        [-0.0969, -0.4810,  0.2810,  ...,  0.4448,  0.0207, -0.8208],
        [-0.2600, -0.5645,  0.4944,  ...,  0.3357,  0.0380, -0.7822]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1420, -0.6211,  0.4385,  ...,  0.4080,  0.0844, -0.6948],
        [-0.1483, -0.5610,  0.5029,  ...,  0.1776,  0.0396, -0.8281],
        [-0.2649, -0.5942,  0.4380,  ...,  0.4209,  0.0178, -0.7227],
        ...,
        [-0.1437, -0.6157,  0.2468,  ...,  0.5518,  0.0904, -0.6519],
        [-0.2556, -0.4866,  0.1807,  ...,  0.3960,  0.1586, -0.8042],
        [-0.2537, -0.4778,  0.3484,  ...,  0.3613,  0.0576, -0.7749]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0715, -0.5312,  0.5615,  ...,  0.2793,  0.1279, -0.8413],
        [-0.0522, -0.4888,  0.3672,  ...,  0.2920,  0.3140, -0.9019],
        [-0.2256, -0.5098,  0.4573,  ...,  0.0743,  0.0942, -0.7563],
        ...,
        [-0.1267, -0.4058,  0.6084,  ...,  0.2661,  0.2776, -0.8325],
        [-0.1515, -0.5391,  0.4177,  ...,  0.1901,  0.1506, -0.9365],
        [-0.1110, -0.4290,  0.6250,  ...,  0.3550,  0.2527, -0.6646]],
       device='cuda:1', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0514, -0.6748,  0.3630,  ...,  0.3364,  0.0508, -0.6758],
        [-0.0563, -0.5273,  0.3115,  ...,  0.4236,  0.1860, -0.8564],
        [-0.0649, -0.7520,  0.4575,  ...,  0.3679,  0.0881, -0.7559],
        ...,
        [-0.1016, -0.5278,  0.1718,  ...,  0.3506,  0.0359, -0.7676],
        [-0.0809, -0.6572,  0.3213,  ...,  0.3872,  0.0816, -0.8228],
        [ 0.0025, -0.6587,  0.3652,  ...,  0.3923,  0.0467, -0.8350]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1377, -0.4763,  0.2610,  ...,  0.3105,  0.0730, -0.7773],
        [-0.0722, -0.4661,  0.4265,  ...,  0.2903,  0.0833, -0.8501],
        [-0.0611, -0.5176,  0.3242,  ...,  0.2046, -0.0172, -0.7793],
        ...,
        [-0.0883, -0.4333,  0.2184,  ...,  0.4758,  0.0223, -0.8633],
        [-0.0528, -0.4258,  0.3777,  ...,  0.3926,  0.0524, -0.8901],
        [-0.0981, -0.5674,  0.2505,  ...,  0.2622,  0.0305, -0.9116]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1278, -0.5386,  0.9736,  ...,  0.3879,  0.3149, -0.9097],
        [-0.0755, -0.5229,  0.6904,  ...,  0.2749,  0.4692, -0.7578],
        [-0.1365, -0.6846,  0.9243,  ...,  0.1985,  0.2476, -0.8496],
        ...,
        [-0.0932, -0.6738,  0.8364,  ...,  0.1882,  0.4604, -0.8633],
        [-0.0495, -0.5884,  0.7002,  ...,  0.3811,  0.4509, -0.7183],
        [-0.1071, -0.4705,  0.7148,  ...,  0.2168,  0.1076, -0.8428]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.3562, -0.0067,  0.5332,  ...,  0.1747,  0.4719, -0.2479],
        [-0.4067, -0.4651,  0.2351,  ...,  0.1234,  0.4548, -0.2546],
        [-0.3635, -0.1915,  0.3132,  ...,  0.7178,  0.3394, -0.4700],
        ...,
        [-0.3569, -0.0934,  0.1058,  ...,  0.3259,  0.7930, -0.4968],
        [-0.7046, -0.4448, -0.0621,  ...,  0.0634,  0.9106, -0.3196],
        [-0.1370, -0.0574,  0.5151,  ..., -0.0647,  0.4587, -0.2925]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([136, 512])
> 5.  tensor([[-0.1127, -0.5571,  0.4705,  ...,  0.2620, -0.0071, -0.7378],
        [-0.1860, -0.5215,  0.4365,  ...,  0.3696,  0.0384, -0.8472],
        [-0.2480, -0.5796,  0.3403,  ...,  0.3833,  0.1311, -0.7246],
        ...,
        [-0.2460, -0.6099,  0.3604,  ...,  0.4758,  0.0602, -0.8027],
        [-0.3518, -0.7983,  0.4209,  ...,  0.6538,  0.1122, -0.7920],
        [-0.1448, -0.6606,  0.4116,  ...,  0.3003,  0.0309, -0.8164]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[ 0.0968, -0.6279,  0.4009,  ...,  0.1960,  0.0172, -1.0010],
        [-0.0977, -0.5679,  0.6211,  ...,  0.1902,  0.2024, -0.9453],
        [ 0.0121, -0.3948,  0.5112,  ...,  0.3325,  0.2268, -0.6416],
        ...,
        [-0.1488, -0.6318,  0.4641,  ...,  0.4307,  0.3579, -0.7139],
        [-0.0885, -0.5576, -0.0597,  ...,  0.0760,  0.3423, -0.6079],
        [ 0.0023, -0.3010,  0.5146,  ...,  0.3027,  0.1217, -0.9014]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1133, -0.4375,  0.6411,  ...,  0.2593,  0.2627, -0.7197],
        [-0.2151, -0.4111,  0.3193,  ...,  0.1666,  0.3066, -0.7251],
        [-0.2583, -0.4297,  0.4773,  ...,  0.2629,  0.0732, -0.8164],
        ...,
        [-0.1819, -0.4275,  0.4583,  ...,  0.1051,  0.1768, -0.7852],
        [-0.2000, -0.5474,  0.4263,  ...,  0.2357,  0.1392, -0.5913],
        [-0.1743, -0.4695,  0.5249,  ...,  0.2839,  0.1692, -0.9375]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.2810, -0.3679,  0.4861,  ...,  0.4653,  0.0963, -0.8237],
        [-0.3389, -0.4395,  0.5000,  ...,  0.4392,  0.0412, -0.7764],
        [-0.1440, -0.5347,  0.5015,  ...,  0.5044,  0.0586, -0.6685],
        ...,
        [-0.2118, -0.6001,  0.3894,  ...,  0.4529,  0.1130, -0.8447],
        [-0.1793, -0.5713,  0.4514,  ...,  0.5552,  0.1554, -0.8599],
        [-0.1720, -0.6699,  0.5083,  ...,  0.4358,  0.0739, -1.0410]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1743, -0.6855,  0.6553,  ...,  0.3557,  0.1925, -0.6963],
        [-0.0255, -0.4685,  0.6172,  ..., -0.0022,  0.2076, -0.5903],
        [ 0.0555, -0.6514,  0.7725,  ...,  0.4353,  0.0692, -0.5288],
        ...,
        [-0.2286, -0.5820,  0.6006,  ...,  0.4221,  0.3274, -0.7832],
        [-0.1440, -0.4800,  0.6875,  ...,  0.4287,  0.4229, -0.6602],
        [-0.1292, -0.5405,  0.8979,  ...,  0.3113,  0.0682, -0.6787]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.1016, -0.6992,  0.7910,  ...,  0.3105,  0.3818, -0.6831],
        [-0.1229, -0.6572,  0.6548,  ...,  0.4099,  0.3718, -0.7139],
        [-0.1510, -0.6841,  0.7271,  ...,  0.1543,  0.2314, -0.7417],
        ...,
        [-0.0028, -0.6162,  0.6606,  ...,  0.2057,  0.3486, -0.9463],
        [ 0.0049, -0.7725,  0.5518,  ...,  0.1198,  0.3086, -0.8643],
        [-0.0834, -0.6870,  0.8252,  ...,  0.3176,  0.4929, -0.8145]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1295, -0.5840,  0.5010,  ...,  0.2435,  0.1281, -0.6108],
        [-0.0373, -0.6860,  0.5503,  ...,  0.2522,  0.1964, -1.0469],
        [-0.1251, -0.6074,  0.7056,  ...,  0.3220,  0.2998, -0.5400],
        ...,
        [-0.0314, -0.6187,  0.6162,  ...,  0.3477,  0.1793, -0.5869],
        [-0.0841, -0.6138,  0.4080,  ...,  0.4290,  0.1642, -0.7446],
        [-0.0368, -0.8525,  0.4197,  ...,  0.4431,  0.3059, -0.6279]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.1432, -0.5811,  0.5425,  ...,  0.1825,  0.2462, -0.8403],
        [-0.1300, -0.5601,  0.5635,  ...,  0.3315,  0.2231, -0.8369],
        [-0.0764, -0.3716,  0.6211,  ...,  0.1833,  0.2605, -0.7715],
        ...,
        [-0.0098, -0.7764,  0.5210,  ...,  0.3223,  0.1765, -0.7314],
        [-0.1423, -0.4465,  0.6030,  ..., -0.0134,  0.0938, -1.0742],
        [-0.0654, -0.4558,  0.7578,  ...,  0.1582,  0.1912, -0.9194]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0720, -0.7153,  0.5205,  ...,  0.3813,  0.1906, -0.7515],
        [ 0.0414, -0.8442,  0.5757,  ...,  0.1864,  0.4028, -0.8496],
        [ 0.0074, -0.5547,  0.6758,  ...,  0.3628,  0.1288, -0.9448],
        ...,
        [-0.2101, -0.5767,  0.6177,  ...,  0.2410,  0.2905, -0.7642],
        [-0.0499, -0.6333,  0.6201,  ...,  0.5889,  0.2341, -0.7666],
        [-0.0926, -0.6221,  0.4424,  ...,  0.4277,  0.3689, -0.6372]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  tensor([[-0.4055, -1.2383,  1.0293,  ...,  0.1043,  0.2734, -1.1279],
        [-0.1316, -1.0957,  1.3301,  ..., -0.4502, -0.1946, -0.9360],
        [-0.4580, -0.9858,  1.3320,  ...,  0.2250,  0.1848, -1.0137],
        [-0.1704, -1.0537,  1.0938,  ...,  0.6567,  0.5220, -0.9478],
        [-0.4141, -1.0547,  1.1328,  ...,  0.6440,  0.4243, -0.5449]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([5, 512])
> 5.  tensor([[-0.0617, -0.6885,  0.2673,  ...,  0.3120,  0.0562, -0.9702],
        [-0.1533, -0.6309,  0.6235,  ..., -0.2142, -0.0988, -0.7485],
        [-0.0384, -0.5786,  0.5166,  ...,  0.3538,  0.0551, -0.8970],
        ...,
        [ 0.0674, -0.4038,  0.2529,  ...,  0.2722, -0.0519, -0.8042],
        [-0.0298, -0.4736,  0.3994,  ...,  0.4250,  0.1967, -0.8145],
        [-0.0393, -0.5776,  0.4504,  ...,  0.3843,  0.1766, -0.8413]],
       device='cuda:6', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.2466, -0.5757,  0.7925,  ...,  0.5439,  0.6128, -0.4771],
        [-0.3594, -0.3225,  0.6650,  ...,  0.6704,  0.6113, -0.8726],
        [ 0.1152, -0.7534,  0.3682,  ...,  0.6772,  0.7407, -0.7949],
        ...,
        [-0.2367, -0.6260,  0.5308,  ...,  0.5952,  0.5293, -0.3633],
        [-0.2847, -0.3135,  0.7002,  ...,  0.4304,  0.8667, -0.7402],
        [-0.2808, -0.4978,  0.5679,  ...,  0.4719,  0.8169, -0.5034]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([56, 512])
> 5.  tensor([[-0.2649, -0.4851,  0.3516,  ...,  0.3845, -0.0280, -0.7290],
        [-0.1020, -0.5664,  0.3040,  ...,  0.3516,  0.1245, -0.8262],
        [-0.1814, -0.7803,  0.2783,  ...,  0.0414,  0.2111, -0.9097],
        ...,
        [-0.1971, -0.5576,  0.3606,  ...,  0.4150, -0.0572, -0.6909],
        [-0.1261, -0.6938,  0.3521,  ...,  0.5264,  0.0067, -0.7837],
        [-0.1753, -0.5059,  0.4563,  ...,  0.2434,  0.1021, -0.7476]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.0520, -0.6514,  0.3120,  ...,  0.4155,  0.0962, -0.9224],
        [-0.0249, -0.7256,  0.4053,  ...,  0.4531,  0.1567, -1.0557],
        [-0.0673, -0.5103,  0.1984,  ...,  0.5195,  0.1609, -0.7627],
        ...,
        [-0.0540, -0.4905,  0.4146,  ...,  0.5020,  0.0962, -0.7930],
        [-0.1259, -0.7026,  0.4316,  ...,  0.4426,  0.1650, -0.9214],
        [-0.0925, -0.4746,  0.4434,  ...,  0.1083,  0.0944, -1.0146]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.0192, -0.6865,  0.4348,  ...,  0.1929,  0.1838, -0.8188],
        [ 0.0508, -0.7329,  0.2549,  ...,  0.2181,  0.0947, -1.0322],
        [-0.0529, -0.5439,  0.1431,  ...,  0.4448, -0.0728, -0.9399],
        ...,
        [-0.0137, -0.5474,  0.2520,  ...,  0.2324,  0.0450, -0.7661],
        [-0.0319, -0.8188,  0.4529,  ...,  0.4087,  0.0724, -0.8701],
        [-0.0604, -0.4287,  0.4036,  ...,  0.2119,  0.0364, -0.6694]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1576, -0.5400,  0.5020,  ...,  0.1664,  0.2822, -0.6636],
        [-0.0847, -0.6094,  0.4900,  ...,  0.3198,  0.1271, -0.8506],
        [-0.1775, -0.8013,  0.4631,  ...,  0.1000,  0.1935, -1.0850],
        ...,
        [-0.0751, -0.6846,  0.4631,  ..., -0.2981,  0.1017, -1.3213],
        [-0.1689, -0.6602,  0.2617,  ...,  0.3127,  0.4407, -0.9990],
        [ 0.0166, -0.5303,  0.4209,  ...,  0.3477,  0.2357, -0.7944]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.0457, -0.5698,  0.3401,  ...,  0.3264, -0.0938, -0.7471],
        [-0.0363, -0.5508,  0.2856,  ...,  0.4399, -0.0132, -0.7739],
        [-0.0671, -0.3857,  0.2874,  ...,  0.3840,  0.0741, -0.8560],
        ...,
        [-0.0224, -0.6074,  0.4424,  ...,  0.2683,  0.1005, -0.9204],
        [-0.0522, -0.4556,  0.2299,  ...,  0.3975,  0.0116, -0.8384],
        [-0.1112, -0.6182,  0.3623,  ...,  0.4426, -0.0084, -0.7383]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.3193, -0.8677,  0.9038,  ...,  0.3135,  0.3198, -0.8813],
        [-0.2092, -0.6978,  1.0869,  ...,  0.4280,  0.3000, -0.8662],
        [-0.2778, -0.6118,  0.9800,  ...,  0.3713,  0.3093, -0.6172],
        [-0.3242, -0.6064,  1.1250,  ...,  0.0862,  0.2397, -1.2090],
        [-0.2343, -0.6650,  1.1562,  ..., -0.0993,  0.2062, -0.9619],
        [-0.0833, -0.9907,  1.0762,  ..., -0.5981,  0.0869, -0.6475]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([6, 512])
> 5.  tensor([[-0.0457, -0.5278,  0.3640,  ...,  0.7861,  0.4861, -0.6328],
        [-0.2673, -0.4954,  0.8853,  ...,  0.6304,  0.5469, -0.6865],
        [-0.2649, -0.6284,  0.5659,  ...,  0.7197,  0.7607, -0.8271],
        ...,
        [-0.2881, -0.5010,  0.8696,  ...,  0.5303,  0.3904, -0.5894],
        [-0.2686, -0.2308,  0.5923,  ...,  0.7573,  0.4700, -0.7749],
        [-0.3086, -0.2998,  0.6118,  ...,  0.5156,  0.5166, -0.5791]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([56, 512])
> 5.  tensor([[-1.8530e-01, -6.7432e-01,  3.1470e-01,  ...,  2.5024e-01,
         -6.3171e-03, -7.7539e-01],
        [-1.1139e-01, -5.0977e-01,  4.2261e-01,  ...,  3.7769e-01,
          1.1060e-01, -7.3633e-01],
        [-1.6699e-01, -5.5078e-01,  3.3569e-01,  ...,  2.7686e-01,
         -7.1533e-02, -8.1982e-01],
        ...,
        [-1.5198e-01, -5.0293e-01,  3.0298e-01,  ...,  4.5380e-02,
         -7.5293e-04, -9.8242e-01],
        [-1.1517e-01, -6.8213e-01,  4.4507e-01,  ...,  4.1724e-01,
         -5.7404e-02, -6.8457e-01],
        [-1.8396e-01, -5.4248e-01,  2.2864e-01,  ...,  4.3115e-01,
          1.6907e-02, -8.3545e-01]], device='cuda:7', dtype=torch.float16,
       grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.3877, -0.7646,  0.7573,  ...,  0.0576,  0.1489, -0.9907],
        [-0.4180, -0.8789,  1.1689,  ...,  0.3030,  0.2715, -0.8364],
        [-0.2302, -0.8892,  1.0469,  ..., -0.3433, -0.1545, -1.1006],
        [-0.3882, -1.0000,  1.1826,  ...,  0.1571,  0.3242, -0.9614],
        [-0.4631, -0.7275,  1.1992,  ...,  0.4214,  0.2983, -0.8682]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([5, 512])
> 5.  tensor([[ 0.0303, -0.3853,  0.3738,  ...,  0.2756,  0.2196, -0.8550],
        [-0.0367, -0.4153,  0.4202,  ...,  0.2559,  0.0527, -0.8994],
        [-0.0861, -0.4590,  0.4805,  ...,  0.2974,  0.0693, -0.8242],
        ...,
        [-0.0410, -0.7178,  0.6079,  ..., -0.2800, -0.1621, -0.6973],
        [-0.0237, -0.5571,  0.3289,  ...,  0.0464,  0.1713, -0.8247],
        [-0.0044, -0.6411,  0.3003,  ...,  0.3708,  0.1846, -0.7598]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.2233, -0.4932,  0.3333,  ...,  0.4573, -0.0424, -0.7412],
        [-0.2142, -0.8359,  0.5352,  ...,  0.1896,  0.0370, -1.0410],
        [-0.2507, -0.5708,  0.3977,  ...,  0.3064,  0.0231, -0.9136],
        ...,
        [-0.1591, -0.5591,  0.4150,  ...,  0.5791,  0.0269, -0.6250],
        [-0.3286, -0.6343,  0.3574,  ...,  0.1223, -0.0358, -0.8550],
        [-0.1677, -0.5508,  0.3596,  ...,  0.3828,  0.0400, -0.9092]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([16, 512])
> 5.  tensor([[-0.1606, -0.4822,  0.4490,  ...,  0.4182,  0.1963, -0.8062],
        [ 0.0120, -0.5493,  0.3586,  ...,  0.3799,  0.0440, -0.7231],
        [-0.0381, -0.6079,  0.4358,  ...,  0.3601,  0.2012, -0.8354],
        ...,
        [-0.0478, -0.5396,  0.3306,  ...,  0.5293,  0.1592, -0.7656],
        [ 0.0084, -0.4551,  0.2583,  ...,  0.2494,  0.0970, -0.8564],
        [-0.1838, -0.4800,  0.3252,  ...,  0.3115,  0.1396, -0.9292]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[ 0.1241, -0.6318,  0.4602,  ...,  0.3564,  0.0516, -0.7231],
        [ 0.0414, -0.6470,  0.4170,  ...,  0.4583,  0.1193, -0.7915],
        [-0.0722, -0.3655,  0.4536,  ...,  0.4421,  0.2217, -0.9521],
        ...,
        [-0.0190, -0.7202,  0.5005,  ...,  0.5127,  0.2676, -0.8467],
        [-0.1135, -0.4880,  0.4475,  ...,  0.3367,  0.1460, -0.7676],
        [-0.1150, -0.4993,  0.4297,  ...,  0.1842,  0.0904, -0.8901]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([8, 512])
> 5.  tensor([[-0.1399, -0.6777,  0.6548,  ...,  0.5005,  0.3677, -0.7383],
        [-0.0381, -0.6533,  0.6318,  ...,  0.3359,  0.0770, -0.8208],
        [-0.1173, -0.7568,  0.6479,  ...,  0.4731,  0.2808, -0.8198],
        ...,
        [-0.0079, -0.7139,  0.6772,  ...,  0.6714,  0.3120, -0.6714],
        [-0.0131, -0.9214,  0.6567,  ...,  0.3188,  0.2208, -0.6763],
        [-0.0801, -0.6997,  0.8594,  ...,  0.4436,  0.3818, -0.6655]],
       device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>) torch.Size([24, 512])
> 5.  2023-07-11 08:46:31 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:19864
2023-07-11 08:46:31 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:19864
2023-07-11 08:46:31 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:19864
2023-07-11 08:46:31 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:19864
2023-07-11 08:46:31 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:19864
2023-07-11 08:46:31 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:19864
2023-07-11 08:46:31 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:19864
2023-07-11 08:46:31 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:19864
2023-07-11 08:46:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 08:46:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 08:46:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 08:46:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 08:46:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 08:46:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 08:46:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 08:46:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 08:46:31 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:46:31 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 08:46:31 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:46:31 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 08:46:31 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:46:31 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:46:31 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 08:46:31 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 08:46:31 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:46:31 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 08:46:31 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:46:31 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 08:46:31 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:46:31 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 08:46:31 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 08:46:31 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 08:46:34 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19864', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 08:46:34 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:46:34 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 08:46:34 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 08:46:34 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 08:46:34 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:46:38 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 08:46:38 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 08:46:38 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 08:46:40 | INFO | root | load pretrained hubert
2023-07-11 08:46:44 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 08:46:44 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:46:46 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 08:46:47 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 08:46:47 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 08:46:47 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 08:46:47 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 08:46:47 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 08:46:47 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 08:46:47 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 08:46:47 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:46:47 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:46:47 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:46:47 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:46:53 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 08:46:55 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 08:46:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 08:46:56 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 08:46:56 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:46:56 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:46:56 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:46:56 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:46:56 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:46:56 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:46:56 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:46:56 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 08:46:56 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 08:46:56 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 08:46:56 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 08:46:56 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:46:56 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 08:46:56 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-11 08:46:56 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 08:46:56 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:46:56 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 08:46:57 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:46:59 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:47:01 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 08:48:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 08:48:05 | INFO | fairseq.trainer | begin training epoch 1
2023-07-11 08:48:05 | INFO | fairseq_cli.train | Start iterating over samples
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 353, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 134, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([152, 13, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([152, 15, 512])
> 2.  torch.Size([152, 512])
2023-07-11 08:48:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
> 1.  torch.Size([6, 280, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 32, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 276, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 86, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 111, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 20, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 16, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 303, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 112, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 18, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 322, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 106, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 277, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 106, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 325, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 95, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 294, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 17, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 350, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 120, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 108, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 272, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 16, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 20, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 362, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 121, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 289, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 110, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 339, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 106, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 34, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 96, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  > 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 315, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 105, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 277, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 30, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 15, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 16, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 285, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 349, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 112, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 15, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 29, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  > 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 267, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 103, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 289, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 32, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 330, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 316, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 103, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([7, 260, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 104, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  > 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 22, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 15, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 71, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 17, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 25, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 297, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 332, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 105, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 280, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 333, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 340, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 297, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 97, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 288, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 104, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  > 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 52, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 355, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 132, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 62, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 29, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 35, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  > 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 358, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 115, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([136, 15, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 372, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 138, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 76, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 86, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 15, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 336, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 98, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  > 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([88, 22, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 292, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 14, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 259, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 111, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 90, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
2023-07-11 08:49:18 | INFO | train_inner | epoch 001:    101 / 1474 loss=20.769, trans_loss=5.64, nll_loss=4.216, w2v_ctc_loss=22.495, task_loss=0.704, contrastive_loss=3.311, total=4200.41, n_correct=212.17, ppl=18.59, accuracy=5.051, wps=20188, ups=1.61, wpb=12542.9, bsz=467.5, num_updates=100, lr=4.098e-06, gnorm=0.933, clip=0, loss_scale=64, train_wall=65, gb_free=19, wall=142
torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 26, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 328, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 114, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 294, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 304, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 107, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 103, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 342, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 105, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 324, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 112, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 322, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 103, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 288, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 84, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 335, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 120, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 369, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 137, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 18, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 17, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 258, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 357, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 134, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 310, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 113, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 375, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 188, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 58, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 308, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 293, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 318, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 108, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 35, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 64, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 372, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 130, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 18, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([216, 9, 512])
> 2.  torch.Size([216, 512])
> 1.  torch.Size([216, 17, 512])
> 2.  torch.Size([216, 512])
> 1.  torch.Size([5, 346, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 134, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([6, 276, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 18, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 16, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 282, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 21, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 326, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 124, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 33, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 278, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 50, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 374, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 147, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 13, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 49, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([160, 12, 512])
> 2.  torch.Size([160, 512])
> 1.  torch.Size([160, 13, 512])
> 2.  torch.Size([160, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 88, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 87, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 118, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 47, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 37, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 292, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 98, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 30, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([6, 275, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  > 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 21, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 320, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 97, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 374, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 141, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 34, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 17, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 93, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 292, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 97, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 367, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 147, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 263, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 313, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 22, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 17, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
2023-07-11 08:50:19 | INFO | train_inner | epoch 001:    201 / 1474 loss=18.557, trans_loss=5.459, nll_loss=4.049, w2v_ctc_loss=19.352, task_loss=0.692, contrastive_loss=3.286, total=4127.38, n_correct=245.11, ppl=16.55, accuracy=5.939, wps=20296.5, ups=1.65, wpb=12327, bsz=463.1, num_updates=200, lr=8.096e-06, gnorm=3.622, clip=0, loss_scale=64, train_wall=60, gb_free=19.2, wall=203
torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([6, 307, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 286, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 299, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 25, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 31, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 333, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 106, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 327, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 33, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 16, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 47, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 325, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 104, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 17, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 15, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([6, 309, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 301, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([184, 10, 512])
> 2.  torch.Size([184, 512])
> 1.  torch.Size([184, 14, 512])
> 2.  torch.Size([184, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 79, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 29, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 20, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 329, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 106, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 18, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 17, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 277, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([136, 13, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 17, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 359, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 125, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 17, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 16, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 21, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([5, 370, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 129, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 274, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 269, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 334, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 121, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 20, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  > 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 44, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 263, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 104, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 39, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([168, 12, 512])
> 2.  torch.Size([168, 512])
> 1.  torch.Size([168, 14, 512])
> 2.  torch.Size([168, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 16, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 15, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 324, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 108, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  > 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 39, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 280, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 317, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 105, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 31, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 235, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 267, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 96, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 113, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 340, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 105, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 25, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 97, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  > 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 45, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 310, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 20, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 310, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 272, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 87, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 97, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 317, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 101, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 259, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 303, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 116, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([6, 270, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 293, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 109, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([6, 298, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 107, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 283, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 316, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 103, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 366, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 140, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 269, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
torch.Size([8, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 16, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 76, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 25, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 47, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 30, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 24, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
torch.Size([24, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([112, 16, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 22, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 330, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 109, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 280, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 87, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 106, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 38, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 92, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 278, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 356, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 120, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 270, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 87, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([352, 3, 512])
> 2.  torch.Size([352, 512])
> 1.  torch.Size([352, 7, 512])
> 2.  torch.Size([352, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 96, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([5, 337, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 104, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 41, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 332, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 108, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 118, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 92, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 281, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 303, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 310, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 107, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 235, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 347, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 112, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 30, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 25, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 329, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 101, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 306, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 108, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([128, 14, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 22, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 267, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 112, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
2023-07-11 08:51:20 | INFO | train_inner | epoch 001:    301 / 1474 loss=11.623, trans_loss=5.414, nll_loss=4.051, w2v_ctc_loss=8.803, task_loss=0.716, contrastive_loss=3.201, total=4079.62, n_correct=244.86, ppl=16.58, accuracy=6.002, wps=20036.9, ups=1.64, wpb=12195.4, bsz=438.2, num_updates=300, lr=1.2094e-05, gnorm=4.659, clip=0, loss_scale=64, train_wall=60, gb_free=19.9, wall=264
torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 318, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 92, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([6, 275, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 83, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 41, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 19, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 293, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 110, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 342, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 16, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 30, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  > 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 88, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 37, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 315, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 103, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 361, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 127, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([6, 299, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 104, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 281, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 294, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 329, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 96, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  > 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 276, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 269, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 84, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 266, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 36, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 73, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 277, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 39, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([152, 12, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([152, 15, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 266, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  > 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([208, 9, 512])
> 2.  torch.Size([208, 512])
> 1.  torch.Size([208, 16, 512])
> 2.  torch.Size([208, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 100, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 367, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 139, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 96, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 286, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 85, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 263, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 29, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 16, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([240, 8, 512])
> 2.  torch.Size([240, 512])
> 1.  torch.Size([240, 14, 512])
> 2.  torch.Size([240, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 77, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 99, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 266, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 369, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 127, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 28, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 16, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 289, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 111, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 31, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 77, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 16, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 88, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 270, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 17, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 289, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 93, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 341, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 112, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  > 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 34, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 92, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 31, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 47, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 369, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 147, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 58, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 96, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 275, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([5, 356, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 136, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 297, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
2023-07-11 08:52:20 | INFO | train_inner | epoch 001:    401 / 1474 loss=10.386, trans_loss=5.46, nll_loss=4.128, w2v_ctc_loss=6.828, task_loss=0.624, contrastive_loss=3.234, total=4174.14, n_correct=227.52, ppl=17.48, accuracy=5.451, wps=20547.8, ups=1.65, wpb=12461.1, bsz=460.4, num_updates=400, lr=1.6092e-05, gnorm=3.012, clip=0, loss_scale=64, train_wall=60, gb_free=18.9, wall=325
torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 109, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 94, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 22, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 15, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([5, 373, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 138, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 17, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 263, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 107, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 26, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 107, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 19, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 21, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([184, 3, 512])
> 2.  torch.Size([184, 512])
> 1.  torch.Size([184, 19, 512])
> 2.  torch.Size([184, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([176, 11, 512])
> 2.  torch.Size([176, 512])
> 1.  torch.Size([176, 15, 512])
> 2.  torch.Size([176, 512])
> 1.  torch.Size([7, 266, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 101, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([112, 16, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 15, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 258, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 313, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 104, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 59, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 300, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 352, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 134, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 39, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 266, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 20, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 20, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 338, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 113, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 258, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 361, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 130, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 18, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 258, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 29, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 31, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 20, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 28, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 335, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 288, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 316, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 99, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([6, 290, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 342, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 108, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 334, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 102, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 375, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 143, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 263, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 15, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([5, 316, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 115, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 330, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 107, 512])
> 2.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 41, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 269, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 17, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 278, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 81, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 26, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 76, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 333, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 109, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  > 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 327, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 107, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 287, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 274, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 339, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 111, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 94, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 311, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 31, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 99, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([6, 283, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 23, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 17, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([5, 328, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 115, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1. torch.Size([5, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 31, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 22, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 17, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 17, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 18, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 16, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 298, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 118, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 14, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 512])
> 1.  torch.Size([7, 74, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 17, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 20, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 363, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 133, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([6, 280, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 21, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 75, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([5, 323, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 106, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 313, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 115, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 343, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 117, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 22, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([136, 13, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 118, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 280, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 97, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 44, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 91, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 22, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 315, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 123, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 118, 512])
> 2.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 94, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 99, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 363, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 125, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 101, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 15, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 24, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 20, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 17, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 324, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 109, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([5, 360, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 123, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 368, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 144, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 300, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 30, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 268, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 119, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 332, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 360, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 122, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 290, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 110, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 270, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 85, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([112, 16, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 13, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 15, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 286, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 79, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 45, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 33, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 260, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 30, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([136, 15, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  > 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 352, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 126, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 47, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([6, 280, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
2023-07-11 08:53:22 | INFO | train_inner | epoch 001:    501 / 1474 loss=9.942, trans_loss=5.459, nll_loss=4.134, w2v_ctc_loss=6.165, task_loss=0.545, contrastive_loss=3.228, total=4176.18, n_correct=214.65, ppl=17.56, accuracy=5.14, wps=20345.6, ups=1.63, wpb=12513, bsz=477.4, num_updates=500, lr=2.009e-05, gnorm=1.439, clip=0, loss_scale=64, train_wall=61, gb_free=19.2, wall=386
torch.Size([16, 512])
> 1.  torch.Size([16, 61, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 272, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 111, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 87, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([7, 256, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 283, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 36, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 102, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 74, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 288, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([176, 11, 512])
> 2.  torch.Size([176, 512])
> 1.  torch.Size([176, 29, 512])
> 2.  torch.Size([176, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 259, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 95, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 347, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 110, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 274, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 84, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([5, 337, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 111, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 20, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 26, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 272, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 21, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 285, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 106, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 29, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1. torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([5, 336, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 105, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 346, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 121, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([7, 256, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 25, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 360, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 132, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 85, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 260, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 311, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 283, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.   torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 351, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 117, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 28, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 91, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 16, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 51, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([6, 272, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 260, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 94, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 347, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 130, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 25, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 15, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 31, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 274, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 87, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 367, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 133, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 21, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 268, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 103, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 37, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  > 1.  torch.Size([440, 5, 512])
> 2.  torch.Size([440, 512])
> 1.  torch.Size([440, 12, 512])
> 2.  torch.Size([440, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([168, 11, 512])
> 2.  torch.Size([168, 512])
> 1.  torch.Size([168, 16, 512])
> 2.  torch.Size([168, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 285, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 99, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 318, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 103, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 294, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 112, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 278, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
2023-07-11 08:54:22 | INFO | train_inner | epoch 001:    601 / 1474 loss=9.697, trans_loss=5.484, nll_loss=4.172, w2v_ctc_loss=5.811, task_loss=0.498, contrastive_loss=3.281, total=4147.79, n_correct=213.42, ppl=18.03, accuracy=5.145, wps=20392.7, ups=1.65, wpb=12368.1, bsz=484.2, num_updates=600, lr=2.4088e-05, gnorm=0.736, clip=0, loss_scale=64, train_wall=60, gb_free=18.9, wall=447
torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 317, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 115, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 44, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 276, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 30, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 58, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([184, 10, 512])
> 2.  torch.Size([184, 512])
> 1.  torch.Size([184, 15, 512])
> 2.  torch.Size([184, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 17, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 309, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 275, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 286, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 266, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 70, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 302, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 107, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 47, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 37, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 97, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 306, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.   torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 276, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 114, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 14, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 308, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 358, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 135, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 17, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 20, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 20, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 98, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([5, 315, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 117, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 59, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 89, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 285, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 85, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 92, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 267, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([152, 13, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([152, 13, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 311, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 110, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 282, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 301, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([312, 6, 512])
> 2.  torch.Size([312, 512])
> 1.  torch.Size([312, 14, 512])
> 2.  torch.Size([312, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 27, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 46, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([152, 12, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([152, 16, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([136, 13, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 14, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 17, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 327, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 96, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 23, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 22, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 272, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 114, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 16, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 285, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  > 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 272, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([112, 16, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 18, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 258, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 16, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 20, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 30, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 74, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 276, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 26, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 89, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 15, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 299, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 111, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([88, 22, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 19, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 321, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 109, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 290, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 87, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 17, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 340, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 112, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 375, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 176, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([6, 297, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 112, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 320, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 107, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 23, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 41, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 313, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 100, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 296, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 17, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 300, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 73, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 288, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 20, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  > 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([6, 287, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 107, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 285, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 98, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 334, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 60, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 360, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 122, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 324, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 105, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 364, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 135, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 35, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 17, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 16, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 276, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 20, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 93, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 38, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 17, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 29, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 15, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
2023-07-11 08:55:22 | INFO | train_inner | epoch 001:    701 / 1474 loss=9.542, trans_loss=5.477, nll_loss=4.171, w2v_ctc_loss=5.701, task_loss=0.516, contrastive_loss=3.03, total=4152.1, n_correct=226.34, ppl=18.01, accuracy=5.451, wps=20602.5, ups=1.67, wpb=12373.8, bsz=456.2, num_updates=700, lr=2.8086e-05, gnorm=0.603, clip=0, loss_scale=64, train_wall=60, gb_free=19.5, wall=507
torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 89, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 88, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([5, 372, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 158, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 17, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 314, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 110, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([5, 350, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 313, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 98, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 326, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 33, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([200, 10, 512])
> 2.  torch.Size([200, 512])
> 1.  torch.Size([200, 16, 512])
> 2.  torch.Size([200, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 16, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 25, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([128, 14, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 14, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 277, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([5, 345, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 111, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 308, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 128, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 92, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 323, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 124, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 268, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 355, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 124, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 19, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 302, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 71, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 38, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 359, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 120, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 292, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 267, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 44, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 371, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 143, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 337, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 117, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 58, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([112, 16, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 94, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 22, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 354, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 126, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 268, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 103, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 47, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 235, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 61, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 325, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 109, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 326, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 292, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 106, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([112, 16, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 24, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([5, 315, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 108, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 35, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 311, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 110, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  > 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 301, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 297, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 319, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 115, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 20, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 353, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 119, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 79, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 329, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 144, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 61, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
2023-07-11 08:56:23 | INFO | train_inner | epoch 001:    801 / 1474 loss=9.267, trans_loss=5.433, nll_loss=4.119, w2v_ctc_loss=5.469, task_loss=0.5, contrastive_loss=2.928, total=4123.83, n_correct=250.98, ppl=17.38, accuracy=6.086, wps=20321.6, ups=1.65, wpb=12311.9, bsz=464.1, num_updates=800, lr=3.2084e-05, gnorm=0.835, clip=0, loss_scale=64, train_wall=60, gb_free=19.1, wall=567
torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 21, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 18, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 319, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 107, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 21, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 44, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 70, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 97, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 269, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 17, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 23, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 270, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 22, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 87, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 350, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 111, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 29, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 33, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 374, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 139, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 297, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 111, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 39, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 296, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 97, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 15, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 93, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 314, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 106, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([176, 11, 512])
> 2.  torch.Size([176, 512])
> 1.  torch.Size([176, 15, 512])
> 2.  torch.Size([176, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 512])
> 1.  torch.Size([40, 31, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 304, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 17, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 75, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 290, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 20, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 22, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 17, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 25, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 300, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 88, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 49, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 16, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 18, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([256, 1, 512])
> 2.  torch.Size([256, 512])
> 1.  torch.Size([256, 8, 512])
> 2.  torch.Size([256, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 256, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 30, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 94, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 97, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 300, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 77, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 58, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 235, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 95, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 75, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 16, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 27, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 20, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 16, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  > 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 20, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 293, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 116, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 17, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 102, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 309, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 111, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 263, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 260, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 305, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 16, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([5, 357, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 132, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 335, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 113, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 16, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 352, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 142, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([56, 32, 512])
> 2. torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 62, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 32, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([6, 272, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 16, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 36, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 94, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 256, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([6, 281, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 93, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 301, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([4, 375, 512])
> 2.  torch.Size([4, 512])
> 1.  torch.Size([4, 164, 512])
> 2.  torch.Size([4, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 298, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 289, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 132, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 15, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 97, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 39, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 302, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 115, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 91, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 14, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 300, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 109, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 37, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 25, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 15, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 323, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 110, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 29, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 94, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 263, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 94, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 335, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 111, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 17, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 325, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 106, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 338, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 106, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 288, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 110, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 17, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 339, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 15, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 22, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 281, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 21, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 20, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 343, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 112, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 256, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 101, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 324, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 102, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 282, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 272, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 107, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 278, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 96, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 16, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 268, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 349, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 20, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 14, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 256, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 24, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
2023-07-11 08:57:23 | INFO | train_inner | epoch 001:    901 / 1474 loss=9.014, trans_loss=5.411, nll_loss=4.108, w2v_ctc_loss=5.3, task_loss=0.509, contrastive_loss=2.674, total=4163.61, n_correct=276.36, ppl=17.24, accuracy=6.638, wps=20532.4, ups=1.65, wpb=12409.7, bsz=457.1, num_updates=900, lr=3.6082e-05, gnorm=1.207, clip=0, loss_scale=64, train_wall=60, gb_free=18.8, wall=628
 torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 283, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 289, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 37, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 282, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 97, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([64, 30, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 134, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([5, 365, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 140, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([6, 296, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 268, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 98, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 89, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 115, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 259, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 17, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 318, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 95, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 96, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 314, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 98, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 256, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 39, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 258, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 100, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 512])
> 1.  torch.Size([7, 76, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 334, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 122, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 114, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 27, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 28, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 277, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 344, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 121, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 96, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 303, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 110, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 274, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 290, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 118, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 61, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 281, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 256, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 333, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 112, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([168, 11, 512])
> 2.  torch.Size([168, 512])
> 1.  torch.Size([168, 17, 512])
> 2.  torch.Size([168, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 286, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 302, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 329, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 126, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 306, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 119, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 235, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 319, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 29, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([6, 269, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 17, 512])
> 2.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 349, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 120, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 317, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 113, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 58, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 310, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 30, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 28, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 312, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 108, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 20, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 30, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 79, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([272, 7, 512])
> 2.  torch.Size([272, 512])
> 1.  torch.Size([272, 15, 512])
> 2.  torch.Size([272, 512])
> 1.  torch.Size([7, 266, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 97, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 31, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 339, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 119, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 61, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 314, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 97, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 36, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 77, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 316, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 104, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 24, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 282, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 30, 512])
> 2.  torch.Size([40, 512])
2023-07-11 08:58:25 | INFO | train_inner | epoch 001:   1001 / 1474 loss=8.753, trans_loss=5.392, nll_loss=4.088, w2v_ctc_loss=5.07, task_loss=0.509, contrastive_loss=2.535, total=4135.34, n_correct=295.96, ppl=17, accuracy=7.157, wps=20112.8, ups=1.63, wpb=12363.9, bsz=456.8, num_updates=1000, lr=4.008e-05, gnorm=1.493, clip=0, loss_scale=64, train_wall=61, gb_free=19, wall=689

> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 18, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 301, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 108, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 25, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 287, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 116, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 352, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 120, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 268, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 83, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 314, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 356, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 123, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([5, 342, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 126, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 32, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 308, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 41, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 93, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 79, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 277, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 85, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
torch.Size([56, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 15, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 350, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 117, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 304, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([312, 6, 512])
> 2.  torch.Size([312, 512])
> 1.  torch.Size([312, 12, 512])
> 2.  torch.Size([312, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 58, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 92, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 87, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 289, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 104, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 293, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([64, 30, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([5, 340, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 361, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 139, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  > 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 106, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 368, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 136, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  > 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 76, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 14, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 344, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 113, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 296, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 106, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 370, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 137, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 299, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([64, 30, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 269, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 309, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 14, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 26, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 321, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 117, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 322, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 110, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([6, 287, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 46, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  > 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 351, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 348, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 122, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 33, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 333, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 105, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 18, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 320, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 125, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 36, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([128, 14, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 17, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 32, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([5, 321, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 110, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 58, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
2023-07-11 08:59:25 | INFO | train_inner | epoch 001:   1101 / 1474 loss=8.497, trans_loss=5.382, nll_loss=4.08, w2v_ctc_loss=4.88, task_loss=0.516, contrastive_loss=2.316, total=4147.38, n_correct=313.78, ppl=16.91, accuracy=7.566, wps=20441.7, ups=1.65, wpb=12356.9, bsz=454.9, num_updates=1100, lr=4.4078e-05, gnorm=1.708, clip=0, loss_scale=64, train_wall=60, gb_free=18.8, wall=750
torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 40, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([136, 16, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 20, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 58, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 341, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 115, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 359, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 127, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 69, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 27, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 259, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 339, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 111, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 14, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 39, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 17, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 102, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 277, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 23, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 358, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 132, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 25, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([6, 274, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 97, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 258, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 93, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 30, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 235, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
torch.Size([32, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([112, 16, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 15, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 340, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 114, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 352, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 14, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 299, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 107, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 16, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 321, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 113, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 330, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 62, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([160, 12, 512])
> 2.  torch.Size([160, 512])
> 1.  torch.Size([160, 15, 512])
> 2.  torch.Size([160, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 16, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([6, 282, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 86, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 312, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 25, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 288, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 259, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 312, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 109, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([5, 336, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 129, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 332, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 117, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([5, 346, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 121, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 307, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 74, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 48, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 45, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 324, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 125, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 346, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 122, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 47, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 38, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 305, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 25, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 283, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 104, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 30, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 287, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 285, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 283, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 305, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 108, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 20, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 316, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 117, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 282, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 14, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 58, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 26, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 87, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 260, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 16, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 87, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 302, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 268, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 98, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 91, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 108, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 89, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 314, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 103, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 298, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 26, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 16, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  > 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 88, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 30, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 23, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([64, 30, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 24, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 286, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 44, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 36, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 36, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 18, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  > 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 312, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 93, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 283, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 123, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 30, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([6, 268, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 84, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 287, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 298, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 320, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 105, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 98, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 310, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 104, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  > 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 16, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([224, 9, 512])
> 2.  torch.Size([224, 512])
> 1.  torch.Size([224, 17, 512])
> 2.  torch.Size([224, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 334, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 151, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 76, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 85, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  > 1.  torch.Size([24, 45, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 267, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 328, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 119, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 76, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 318, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 114, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 355, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 130, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 32, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 34, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 269, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  > 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 302, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 277, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 63, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 303, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 96, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 343, 512])
> 2.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 30, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 290, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 306, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 107, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 15, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 26, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 61, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 318, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 114, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([192, 10, 512])
> 2.  torch.Size([192, 512])
> 1.  torch.Size([192, 13, 512])
> 2.  torch.Size([192, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
2023-07-11 09:00:26 | INFO | train_inner | epoch 001:   1201 / 1474 loss=8.259, trans_loss=5.361, nll_loss=4.058, w2v_ctc_loss=4.696, task_loss=0.537, contrastive_loss=2.102, total=4139.9, n_correct=326.02, ppl=16.66, accuracy=7.875, wps=20379.5, ups=1.65, wpb=12386.7, bsz=440.1, num_updates=1200, lr=4.8076e-05, gnorm=1.674, clip=0, loss_scale=64, train_wall=60, gb_free=18.9, wall=810
torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 307, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 259, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 100, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 101, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([5, 347, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 79, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 23, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([6, 305, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 94, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 16, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 350, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 124, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 19, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 286, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 269, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 83, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 97, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 79, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 313, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 117, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 20, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 348, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 36, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 321, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 99, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 33, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 15, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 20, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 50, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 19, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 26, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 341, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 110, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 278, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 23, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 30, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 106, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 77, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 20, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([5, 326, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 108, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 94, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 307, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 107, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 260, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 102, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 287, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 344, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 126, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 331, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 113, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 29, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 45, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 21, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 75, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  > 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 332, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 110, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 275, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 75, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 28, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 28, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 259, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 97, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 25, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 22, 512])
> 2.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 345, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 111, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([136, 19, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 270, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  > 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 16, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 363, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 144, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 16, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 321, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 266, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 375, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 161, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 281, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([7, 260, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 344, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 115, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 37, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 338, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 128, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 319, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 99, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 44, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
2023-07-11 09:01:26 | INFO | train_inner | epoch 001:   1301 / 1474 loss=8.047, trans_loss=5.362, nll_loss=4.062, w2v_ctc_loss=4.507, task_loss=0.518, contrastive_loss=1.924, total=4046.58, n_correct=320.91, ppl=16.7, accuracy=7.93, wps=20047.6, ups=1.66, wpb=12092.5, bsz=439.3, num_updates=1300, lr=5.2074e-05, gnorm=1.744, clip=0, loss_scale=64, train_wall=60, gb_free=19.7, wall=871
torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 296, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 256, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 16, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 17, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 16, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 87, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 290, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 85, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 21, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 304, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 104, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 30, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 30, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 356, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 120, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 66, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 45, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 354, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 127, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 104, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([176, 11, 512])
> 2.  torch.Size([176, 512])
> 1.  torch.Size([176, 14, 512])
> 2.  torch.Size([176, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 87, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 327, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 129, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 21, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 256, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 29, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 21, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 29, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 266, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 28, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 17, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 349, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 125, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 330, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 106, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 24, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 17, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([5, 331, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 117, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 306, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 83, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 267, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 30, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([200, 10, 512])
> 2.  > 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 101, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 275, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 90, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 39, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 19, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 108, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 17, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 19, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 311, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2. torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 281, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 108, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 74, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 21, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 296, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 322, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 128, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 367, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 128, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 328, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 119, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 25, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
torch.Size([200, 512])
> 1.  torch.Size([200, 17, 512])
> 2.  torch.Size([200, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 307, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 111, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 16, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 90, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 47, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 336, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 119, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 29, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 320, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 103, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 364, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 142, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 313, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 111, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 62, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 280, 512])
> 2.  > 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 275, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 326, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 89, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 18, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 30, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 319, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 104, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 75, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 327, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 97, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 29, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([6, 275, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 85, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 293, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([568, 2, 512])
> 2.  torch.Size([568, 512])
> 1.  torch.Size([568, 14, 512])
> 2.  torch.Size([568, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 331, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 121, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 79, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([5, 331, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 113, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([64, 30, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 31, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
2023-07-11 09:02:27 | INFO | train_inner | epoch 001:   1401 / 1474 loss=7.873, trans_loss=5.359, nll_loss=4.07, w2v_ctc_loss=4.33, task_loss=0.513, contrastive_loss=2.009, total=4133.18, n_correct=329.42, ppl=16.8, accuracy=7.97, wps=20345.3, ups=1.65, wpb=12300.7, bsz=455, num_updates=1400, lr=5.6072e-05, gnorm=1.897, clip=0, loss_scale=64, train_wall=60, gb_free=19.9, wall=931
torch.Size([120, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 92, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([128, 14, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 14, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 25, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 28, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 94, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 274, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 38, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 281, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.   torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 292, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 60, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 312, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 365, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 132, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 307, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 275, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([160, 12, 512])
> 2.  torch.Size([160, 512])
> 1.  torch.Size([160, 13, 512])
> 2.  torch.Size([160, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 366, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 136, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 39, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 15, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 353, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 113, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 258, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 258, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 266, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 96, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 17, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 16, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 13, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 83, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  > 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 308, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 116, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 32, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 47, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 326, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 110, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 268, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 18, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 270, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 87, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 19, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 28, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 133, 512])
torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 305, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 15, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 289, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 86, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 288, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 270, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 305, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 31, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 280, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([152, 12, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([152, 16, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 74, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 25, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 87, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 83, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([5, 345, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 105, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 16, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  > 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 90, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([160, 12, 512])
> 2.  torch.Size([160, 512])
> 1.  torch.Size([160, 18, 512])
> 2.  torch.Size([160, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 98, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 23, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 306, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 14, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
2023-07-11 09:03:11 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 61, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 309, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([256, 1, 512])
> 2.  torch.Size([256, 512])
> 1.  torch.Size([256, 8, 512])
> 2.  torch.Size([256, 512])
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 299, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 97, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 263, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([152, 12, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([152, 14, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([256, 1, 512])
> 2.  torch.Size([256, 512])
> 1.  torch.Size([256, 8, 512])
> 2.  torch.Size([256, 512])
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 276, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 314, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 99, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([136, 13, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([6, 278, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 87, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 29, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 323, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 113, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 332, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 114, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 91, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([256, 1, 512])
> 2.  torch.Size([256, 512])
> 1.  torch.Size([256, 8, 512])
> 2.  torch.Size([256, 512])
torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([256, 1, 512])
> 2.  torch.Size([256, 512])
> 1.  torch.Size([256, 8, 512])
> 2.  torch.Size([256, 512])
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 282, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 23, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 25, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 278, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 81, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 23, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 371, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 143, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([6, 300, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 258, 512])
> 2.  torch.Size([7, 512])
2023-07-11 09:03:45 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 9.589 | trans_loss 10.965 | nll_loss 9.955 | w2v_ctc_loss 5.6 | task_loss 2.365 | contrastive_loss 2.334 | total 4003.4 | n_correct 372.3 | ppl 992.45 | accuracy 9.3 | uer 71.401 | wer 69.401 | raw_wer 69.401 | bleu 0.02 | wps 1443.3 | wpb 4003.4 | bsz 141.8 | num_updates 1473
2023-07-11 09:03:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1473 updates
2023-07-11 09:03:45 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_best.pt
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 319, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([3, 526, 512])
> 2.  torch.Size([3, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 359, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([4, 413, 512])
> 2.  torch.Size([4, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 274, 512])
> 2.  torch.Size([6, 512])
2023-07-11 09:03:48 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_best.pt
2023-07-11 09:03:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_best.pt (epoch 1 @ 1473 updates, score 0.02) (writing took 4.72685337002622 seconds)
2023-07-11 09:03:50 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-07-11 09:03:50 | INFO | train | epoch 001 | loss 10.592 | trans_loss 5.432 | nll_loss 4.105 | w2v_ctc_loss 7.65 | task_loss 0.56 | contrastive_loss 2.749 | total 4138.32 | n_correct 267.847 | ppl 17.21 | accuracy 6.472 | wps 19480.7 | ups 1.58 | wpb 12354.6 | bsz 458.3 | num_updates 1473 | lr 5.89905e-05 | gnorm 1.813 | clip 0 | loss_scale 64 | train_wall 893 | gb_free 19.2 | wall 1014
2023-07-11 09:03:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 09:03:50 | INFO | fairseq.trainer | begin training epoch 2
2023-07-11 09:03:50 | INFO | fairseq_cli.train | Start iterating over samples
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
2023-07-11 09:04:15 | INFO | train_inner | epoch 002:     27 / 1474 loss=7.669, trans_loss=5.353, nll_loss=4.052, w2v_ctc_loss=4.124, task_loss=0.488, contrastive_loss=1.842, total=4162.95, n_correct=340.26, ppl=16.58, accuracy=8.174, wps=11535.3, ups=0.93, wpb=12414, bsz=470.8, num_updates=1500, lr=6.007e-05, gnorm=1.611, clip=0, loss_scale=64, train_wall=60, gb_free=19.6, wall=1039
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 290, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 115, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 330, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 58, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 24, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 94, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 88, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 336, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 129, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  > 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 305, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 47, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 37, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 289, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 104, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 92, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 326, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 108, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 365, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 132, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  > 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 263, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 259, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 36, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 260, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 98, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 20, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 86, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 280, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 351, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 117, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 29, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([112, 16, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 298, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 60, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 282, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 235, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 35, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 41, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 97, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 62, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 322, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 103, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([136, 13, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 352, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 134, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 372, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 158, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 260, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 74, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 369, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 127, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 330, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 106, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 278, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  > 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([256, 1, 512])
> 2.  torch.Size([256, 512])
> 1.  torch.Size([256, 8, 512])
> 2.  torch.Size([256, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 17, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 268, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 83, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([5, 375, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 188, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 345, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 111, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([7, 256, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 58, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 306, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 108, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 58, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 102, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 94, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 21, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 27, 512])
> 2.  torch.Size([56, 512])
> 1.  > 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 302, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 15, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 326, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 89, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 92, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 94, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([160, 12, 512])
> 2.  torch.Size([160, 512])
> 1.  torch.Size([160, 13, 512])
> 2.  torch.Size([160, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
2023-07-11 09:05:16 | INFO | train_inner | epoch 002:    127 / 1474 loss=7.512, trans_loss=5.353, nll_loss=4.053, w2v_ctc_loss=4.006, task_loss=0.521, contrastive_loss=1.641, total=4155.98, n_correct=339.36, ppl=16.6, accuracy=8.166, wps=20268, ups=1.64, wpb=12390, bsz=451.6, num_updates=1600, lr=6.4068e-05, gnorm=1.615, clip=0, loss_scale=64, train_wall=61, gb_free=18.9, wall=1100
torch.Size([16, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 44, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 26, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 30, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 77, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 98, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([168, 11, 512])
> 2.  torch.Size([168, 512])
> 1.  torch.Size([168, 16, 512])
> 2.  torch.Size([168, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  > 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 29, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([224, 9, 512])
> 2.  torch.Size([224, 512])
> 1.  torch.Size([224, 17, 512])
> 2.  torch.Size([224, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 346, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 134, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 94, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 329, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 106, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 312, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 109, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 17, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 303, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 110, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 101, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([6, 275, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 260, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 29, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 371, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 143, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 17, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 15, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 20, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 311, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 366, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 140, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 15, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([5, 325, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 95, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 28, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  > 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 306, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 266, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 304, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 104, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 325, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 106, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  > 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 16, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 282, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 321, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 99, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 96, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 17, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 235, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 24, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 20, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 314, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 99, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 293, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 110, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  > 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 364, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 142, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([568, 2, 512])
> 2.  torch.Size([568, 512])
> 1.  torch.Size([568, 14, 512])
> 2.  torch.Size([568, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 277, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 85, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 29, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([128, 14, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 14, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 97, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 30, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 370, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 137, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 345, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 105, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  > 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 301, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 344, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 113, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 76, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 17, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 26, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 268, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 84, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 18, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 280, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 46, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 16, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([160, 12, 512])
> 2.  torch.Size([160, 512])
> 1.  torch.Size([160, 18, 512])
> 2.  torch.Size([160, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 20, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 337, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 24, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 290, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 283, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 104, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 104, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 47, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 91, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 17, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 21, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 280, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 87, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 22, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([5, 360, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 122, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([176, 11, 512])
> 2.  torch.Size([176, 512])
> 1.  torch.Size([176, 15, 512])
> 2.  torch.Size([176, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 17, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 38, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 49, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 118, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 319, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 33, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 317, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 105, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 93, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 320, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 107, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 256, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 313, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 115, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 30, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 259, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 96, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 287, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 118, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 61, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 26, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 22, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 15, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 17, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 96, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
2023-07-11 09:06:17 | INFO | train_inner | epoch 002:    227 / 1474 loss=7.335, trans_loss=5.324, nll_loss=4.021, w2v_ctc_loss=3.805, task_loss=0.452, contrastive_loss=1.672, total=4179.21, n_correct=348.94, ppl=16.24, accuracy=8.349, wps=20479.5, ups=1.64, wpb=12488.1, bsz=488.9, num_updates=1700, lr=6.8066e-05, gnorm=1.512, clip=0, loss_scale=64, train_wall=61, gb_free=19, wall=1161
torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 353, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 113, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 352, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 126, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 47, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 17, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 285, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 106, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 16, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 20, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 312, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 108, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 318, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 103, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 268, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 103, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 260, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 278, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 373, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 138, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 299, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 47, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 20, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 37, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 235, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 95, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 344, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 126, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 263, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 94, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 79, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 269, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 274, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([7, 266, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 334, 512])
> 2.  torch.Size([5, 512])
torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 79, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 20, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 21, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 287, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 116, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 28, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([6, 309, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 111, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 259, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 281, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 44, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2. torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 30, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 24, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 63, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 33, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 58, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 285, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 85, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([5, 375, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 143, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 286, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 58, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 79, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 15, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 263, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 280, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 96, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 30, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  > 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 289, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 111, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 15, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 69, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([5, 359, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 127, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 38, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([176, 11, 512])
> 2.  torch.Size([176, 512])
> 1.  torch.Size([176, 14, 512])
> 2.  torch.Size([176, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 276, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 315, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 103, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 27, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 61, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([112, 16, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 13, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
2023-07-11 09:07:18 | INFO | train_inner | epoch 002:    327 / 1474 loss=7.18, trans_loss=5.327, nll_loss=4.02, w2v_ctc_loss=3.71, task_loss=0.519, contrastive_loss=1.383, total=4146.1, n_correct=351.04, ppl=16.22, accuracy=8.467, wps=20204.9, ups=1.63, wpb=12381, bsz=447.8, num_updates=1800, lr=7.2064e-05, gnorm=1.42, clip=0, loss_scale=64, train_wall=61, gb_free=18.8, wall=1222
 torch.Size([24, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 310, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 104, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 75, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 79, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 339, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 111, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 19, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 15, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 93, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  > 1.  torch.Size([5, 121, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([6, 277, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 347, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 112, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 309, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 283, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 297, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 266, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 96, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 331, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 113, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 30, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([200, 10, 512])
> 2.  torch.Size([200, 512])
> 1.  torch.Size([200, 16, 512])
> 2.  torch.Size([200, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([136, 15, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 267, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 96, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 358, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 115, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 268, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 103, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 17, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 30, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 260, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 102, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 272, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 36, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 327, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 129, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 70, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 260, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 75, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 367, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 128, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  > 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 301, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 301, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 22, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 97, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 275, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 94, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 266, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 302, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 115, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 30, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 37, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 87, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 344, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 115, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 321, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 117, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 296, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 357, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 132, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 277, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([152, 12, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([152, 16, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 77, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 301, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 108, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([5, 324, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 109, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 17, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 21, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 17, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 315, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 117, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 98, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 374, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 147, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 301, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 335, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 355, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 130, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 83, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 235, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 282, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 338, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 106, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 305, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 283, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 323, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 110, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 39, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 366, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 136, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 286, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 292, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 89, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 294, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 235, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 107, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 39, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 20, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 297, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([5, 332, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 114, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([6, 299, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 111, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 44, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 94, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([136, 19, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([6, 299, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 107, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 287, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 41, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 87, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 99, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 286, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 87, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 29, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 280, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 89, 512])
> 2. > 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 39, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 48, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 367, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 139, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([5, 356, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 123, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 39, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 314, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 106, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 13, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 35, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 319, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 99, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
2023-07-11 09:08:18 | INFO | train_inner | epoch 002:    427 / 1474 loss=7.044, trans_loss=5.315, nll_loss=4.01, w2v_ctc_loss=3.615, task_loss=0.567, contrastive_loss=1.205, total=4037.99, n_correct=345.04, ppl=16.11, accuracy=8.545, wps=19992.2, ups=1.66, wpb=12079.4, bsz=415.4, num_updates=1900, lr=7.6062e-05, gnorm=1.431, clip=0, loss_scale=64, train_wall=60, gb_free=18.9, wall=1283
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 347, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 110, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 297, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 112, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 258, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 19, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 269, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 369, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 137, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 14, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
torch.Size([7, 101, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 75, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 17, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 32, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 39, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 49, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 22, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 29, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 25, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 259, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 111, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 26, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 343, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 117, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 258, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 93, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 18, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 313, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 297, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 111, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 17, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 36, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 350, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 111, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 14, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 290, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 87, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 275, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 85, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 15, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 25, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([5, 324, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 112, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 106, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 256, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 61, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 15, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 114, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 94, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 269, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.   torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 20, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 26, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 363, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 125, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 16, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 31, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 28, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 17, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 21, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 29, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 283, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 74, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([5, 331, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 121, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 341, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 112, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 74, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 311, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 110, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 322, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 110, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 330, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 107, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 23, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([5, 350, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 117, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 17, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 256, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 101, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 26, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 314, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 97, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 22, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 17, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 361, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 139, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 276, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 270, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 87, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 328, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 114, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 25, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
2023-07-11 09:09:19 | INFO | train_inner | epoch 002:    527 / 1474 loss=6.945, trans_loss=5.313, nll_loss=4.002, w2v_ctc_loss=3.456, task_loss=0.495, contrastive_loss=1.31, total=4176.97, n_correct=362.17, ppl=16.02, accuracy=8.671, wps=20651, ups=1.66, wpb=12475.1, bsz=468.6, num_updates=2000, lr=8.006e-05, gnorm=1.382, clip=0, loss_scale=64, train_wall=60, gb_free=19.6, wall=1343
2023-07-11 09:09:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 17, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 16, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 34, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 104, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 308, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 361, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 130, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 50, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 15, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 15, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 58, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 88, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 367, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 147, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
torch.Size([112, 14, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 15, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([136, 15, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([152, 12, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([152, 16, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 17, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 36, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 340, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 98, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 18, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 340, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 105, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 97, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 13, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 30, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 90, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 25, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 263, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 107, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 300, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 285, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 296, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 30, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 307, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 320, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 103, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 340, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 305, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 258, 512])
> 2.  torch.Size([7, 512])
2023-07-11 09:09:53 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 9.003 | trans_loss 10.822 | nll_loss 9.771 | w2v_ctc_loss 4.522 | task_loss 2.365 | contrastive_loss 1.656 | total 4003.4 | n_correct 394.3 | ppl 873.41 | accuracy 9.849 | uer 61.814 | wer 59.409 | raw_wer 59.409 | bleu 0.03 | wps 1431.9 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_bleu 0.03
2023-07-11 09:09:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-07-11 09:09:53 | INFO | fairseq.trainer | Saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_2_2000.pt
2023-07-11 09:09:55 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_2_2000.pt
2023-07-11 09:10:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.03) (writing took 9.073796289041638 seconds)
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([4, 413, 512])
> 2.  torch.Size([4, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([5, 313, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 111, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 28, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 355, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 132, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 60, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 335, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 113, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([136, 16, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  > 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 61, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 340, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 114, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 17, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([3, 526, 512])
> 2.  torch.Size([3, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 29, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 76, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 283, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 292, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 28, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 293, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 79, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 16, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  > 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 274, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 355, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 124, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 45, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 269, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 16, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  > 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 319, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 19, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 93, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 350, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 124, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 70, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 374, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 141, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 359, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([5, 339, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 106, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 106, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 102, 512])> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 321, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 20, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 30, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 276, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 256, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 97, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 289, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 296, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 106, 512])
> 2.  torch.Size([6, 512])
> 1.  > 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 292, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 97, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 20, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 303, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 112, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 16, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 20, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 77, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 17, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
2023-07-11 09:11:03 | INFO | train_inner | epoch 002:    627 / 1474 loss=6.813, trans_loss=5.307, nll_loss=3.999, w2v_ctc_loss=3.357, task_loss=0.511, contrastive_loss=1.097, total=4126.49, n_correct=360.44, ppl=15.99, accuracy=8.735, wps=11781.8, ups=0.96, wpb=12299.1, bsz=445.5, num_updates=2100, lr=8.4058e-05, gnorm=1.153, clip=0, loss_scale=128, train_wall=61, gb_free=19.2, wall=1448
torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 358, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 132, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 316, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 104, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 318, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 92, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 73, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 288, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 288, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  > 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 356, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 120, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 300, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 302, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([5, 341, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 110, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 21, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([136, 13, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([112, 16, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 15, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([5, 324, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 108, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 16, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 34, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([5, 344, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 121, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 97, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 307, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 111, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 39, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 372, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 130, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 21, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 22, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 89, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 16, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 20, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 281, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])

> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 90, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 31, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 43, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 25, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 281, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([160, 12, 512])
> 2.  torch.Size([160, 512])
> 1.  torch.Size([160, 15, 512])
> 2.  torch.Size([160, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  > 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 282, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 333, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 292, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 106, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([168, 12, 512])
> 2.  torch.Size([168, 512])
> 1.  torch.Size([168, 14, 512])
> 2.  torch.Size([168, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 285, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1. torch.Size([64, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 111, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 76, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 288, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 329, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 96, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 20, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 325, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 104, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 343, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 112, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 16, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 297, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 61, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 33, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 93, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 327, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 96, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 259, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 97, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 38, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 89, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 87, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 18, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
2023-07-11 09:12:05 | INFO | train_inner | epoch 002:    727 / 1474 loss=6.736, trans_loss=5.289, nll_loss=3.98, w2v_ctc_loss=3.266, task_loss=0.501, contrastive_loss=1.204, total=4149.06, n_correct=369.94, ppl=15.78, accuracy=8.916, wps=20125.6, ups=1.63, wpb=12372.9, bsz=465.4, num_updates=2200, lr=8.8056e-05, gnorm=1.168, clip=0, loss_scale=128, train_wall=61, gb_free=19.2, wall=1509
torch.Size([136, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 88, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 321, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 109, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 62, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 321, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 113, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 268, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 119, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 29, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 310, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 268, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 98, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  > 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 276, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 30, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 267, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 112, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 260, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 94, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 14, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 25, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 16, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 29, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  > 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 16, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 29, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 102, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 348, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 122, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 270, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 16, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 239, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 74, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 283, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 28, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 268, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 79, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 282, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 36, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.   torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 325, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 109, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 272, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 111, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 15, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 32, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 290, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 45, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 99, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 319, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 104, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 267, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 103, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 290, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 85, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  > 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 318, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 95, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 37, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 324, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 105, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 16, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 31, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([64, 30, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 31, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 35, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 61, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 323, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 124, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  > 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 308, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([216, 9, 512])
> 2.  torch.Size([216, 512])
> 1.  torch.Size([216, 17, 512])
> 2.  torch.Size([216, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 274, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 289, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 86, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 300, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 334, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 122, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 25, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 309, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 24, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 51, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 346, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 121, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 339, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([5, 360, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 123, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 278, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 87, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 294, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 84, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 326, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 110, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([32, 47, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 270, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 85, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 278, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 81, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 21, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([5, 336, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 98, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 17, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 35, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 16, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 73, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 349, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 125, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])torch.Size([8, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 277, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 106, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 254, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 30, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 195, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 16, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 21, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 18, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 79, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 276, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 17, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 313, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 100, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 256, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 107, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 85, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 331, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 113, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 352, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([168, 11, 512])
> 2.  torch.Size([168, 512])
> 1.  torch.Size([168, 17, 512])
> 2.  torch.Size([168, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 18, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 30, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 269, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 282, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([120, 16, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 14, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 71, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([5, 332, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 117, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 22, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 368, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 144, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([152, 12, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([152, 14, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([7, 259, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 16, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 277, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 20, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 79, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 45, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 320, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 125, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 35, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 92, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 342, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 306, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 83, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 266, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([6, 278, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 81, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 225, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 100, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 285, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([5, 315, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 108, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([152, 13, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([152, 13, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 25, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([6, 270, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 92, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 306, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 77, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 312, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
torch.Size([32, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 16, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 339, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 111, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 23, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 369, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 147, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 235, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 354, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 127, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
2023-07-11 09:13:05 | INFO | train_inner | epoch 002:    827 / 1474 loss=6.65, trans_loss=5.272, nll_loss=3.958, w2v_ctc_loss=3.197, task_loss=0.513, contrastive_loss=1.149, total=4175.4, n_correct=380.45, ppl=15.54, accuracy=9.112, wps=20520.3, ups=1.64, wpb=12477.6, bsz=460.9, num_updates=2300, lr=9.2054e-05, gnorm=1.018, clip=0, loss_scale=128, train_wall=60, gb_free=19.8, wall=1570
torch.Size([64, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 19, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 25, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([184, 10, 512])
> 2.  torch.Size([184, 512])
> 1.  torch.Size([184, 15, 512])
> 2.  torch.Size([184, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 281, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 30, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 22, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 19, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([6, 293, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([240, 8, 512])
> 2.  torch.Size([240, 512])
> 1.  torch.Size([240, 14, 512])
> 2.  torch.Size([240, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([152, 12, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([152, 15, 512])
> 2.  torch.Size([152, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 334, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 102, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 87, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 23, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 22, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 39, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 39, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 286, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 16, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 92, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 18, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 15, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 324, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 125, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 287, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 107, 512])
> 2.  torch.Size([6, 512])
torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 22, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 292, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 98, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 321, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 110, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 92, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 230, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 33, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 95, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 258, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  > 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 314, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 103, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 87, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 30, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 258, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 258, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 353, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 119, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 23, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 18, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 289, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 132, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 54, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 350, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 120, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 31, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 20, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 241, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 97, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 41, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 32, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  > 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 17, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 26, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 337, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 111, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 91, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 272, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 87, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 288, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 53, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 245, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 334, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 311, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 110, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([7, 259, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 100, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 288, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 84, 512])
> 2.  torch.Size([6, 512])
2023-07-11 09:14:06 | INFO | train_inner | epoch 002:    927 / 1474 loss=6.553, trans_loss=5.26, nll_loss=3.944, w2v_ctc_loss=3.1, task_loss=0.525, contrastive_loss=1.131, total=4104.2, n_correct=377.31, ppl=15.39, accuracy=9.193, wps=20290.8, ups=1.66, wpb=12243.8, bsz=445.9, num_updates=2400, lr=9.6052e-05, gnorm=1.012, clip=0, loss_scale=128, train_wall=60, gb_free=19, wall=1630
torch.Size([7, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 310, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 71, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 45, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 16, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 183, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 319, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 107, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 248, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 98, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 18, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 113, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 308, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 116, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 118, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([136, 14, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([136, 13, 512])
> 2.  torch.Size([136, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  > 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 358, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 135, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([88, 22, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 17, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 315, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 105, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([6, 309, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 18, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 289, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 272, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 114, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([5, 340, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 112, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 229, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 44, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([5, 354, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 126, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 25, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 294, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 93, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([5, 318, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 114, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 286, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 327, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 116, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 343, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 226, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 235, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 78, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 295, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 16, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 362, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 121, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 268, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  > 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 19, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 307, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 21, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 300, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([312, 6, 512])
> 2.  torch.Size([312, 512])
> 1.  torch.Size([312, 14, 512])
> 2.  torch.Size([312, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([5, 316, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 103, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 316, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 99, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 25, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 349, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 112, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([6, 269, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 95, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 328, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 119, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 57, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 29, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 289, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 94, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  > 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 44, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 45, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 30, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 269, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([7, 263, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 76, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 34, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 217, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 30, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 247, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([112, 17, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 14, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 171, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 28, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 211, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  > 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 19, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 71, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 36, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([5, 336, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 119, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 356, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 136, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 57, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 253, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([104, 17, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 15, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([5, 316, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 117, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([6, 277, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 98, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 91, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 278, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 15, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 84, 512])
> 2.  > 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 23, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 94, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 24, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 19, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 16, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 91, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 263, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 104, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 75, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 338, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 113, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 216, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 348, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 55, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 306, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 119, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 249, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 99, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 329, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 144, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 45, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 20, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 23, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([7, 267, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 115, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 223, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([6, 284, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 108, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 38, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 243, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 178, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 20, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 289, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 110, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 56, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 272, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([5, 375, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 161, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 24, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 341, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 115, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 201, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 359, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 125, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 34, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([112, 16, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([112, 22, 512])
> 2.  torch.Size([112, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 37, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([6, 281, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 88, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 22, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 252, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 20, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 19, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 232, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 17, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 48, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 21, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 265, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 275, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 83, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 261, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 90, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 85, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 27, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 152, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 44, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 148, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 68, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 28, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 34, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 214, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 160, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 92, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 66, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 271, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 15, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 199, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 300, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 109, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([72, 25, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 15, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 312, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([5, 316, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 115, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 208, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 79, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 237, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([176, 11, 512])
> 2.  torch.Size([176, 512])
> 1.  torch.Size([176, 29, 512])
> 2.  torch.Size([176, 512])
> 1.  torch.Size([6, 307, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 102, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 264, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 98, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 91, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 193, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 74, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 83, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 194, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 222, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 29, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([6, 307, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 107, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 220, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 89, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 250, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 75, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 298, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 29, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 131, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 370, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 129, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 302, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 101, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 23, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([6, 273, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 83, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 246, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 85, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 16, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
2023-07-11 09:15:06 | INFO | train_inner | epoch 002:   1027 / 1474 loss=6.47, trans_loss=5.256, nll_loss=3.937, w2v_ctc_loss=3.022, task_loss=0.511, contrastive_loss=0.987, total=4102.5, n_correct=379.3, ppl=15.32, accuracy=9.246, wps=20353.5, ups=1.66, wpb=12270.3, bsz=456.3, num_updates=2500, lr=0.00010005, gnorm=0.92, clip=0, loss_scale=128, train_wall=60, gb_free=19.2, wall=1690
torch.Size([24, 512])
> 1.  torch.Size([8, 128, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 101, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 236, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 102, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 308, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 128, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 139, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 18, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([176, 11, 512])
> 2.  torch.Size([176, 512])
> 1.  torch.Size([176, 15, 512])
> 2.  torch.Size([176, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 133, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 233, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 83, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 23, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 109, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 299, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 15, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 353, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 134, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 18, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 52, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 40, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 175, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 263, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 87, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([5, 367, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 133, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 74, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 100, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 98, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([104, 18, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([104, 16, 512])
> 2.  torch.Size([104, 512])
> 1.  torch.Size([144, 13, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([144, 14, 512])
> 2.  torch.Size([144, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 363, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 133, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 112, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 48, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([120, 15, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([120, 20, 512])
> 2.  torch.Size([120, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 37, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 203, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([440, 5, 512])
> 2.  torch.Size([440, 512])
> 1.  torch.Size([440, 12, 512])
> 2.  torch.Size([440, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 110, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 312, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 100, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 215, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([56, 34, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 272, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([352, 3, 512])
> 2.  torch.Size([352, 512])
> 1.  torch.Size([352, 7, 512])
> 2.  torch.Size([352, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 62, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 28, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 72, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 328, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 115, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([72, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 204, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 19, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 280, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 224, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 279, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 103, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 191, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 111, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 206, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 262, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 28, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 91, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 228, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 109, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 35, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 19, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 18, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 19, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 159, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 92, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 311, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 54, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 156, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 76, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 40, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 299, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 96, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 61, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 28, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 71, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 140, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 182, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 141, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 187, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 188, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 305, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 99, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([64, 29, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 41, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 244, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 81, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 238, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 76, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 227, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 56, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 161, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 198, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 129, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 62, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 146, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 320, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 105, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([6, 313, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 117, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 31, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 17, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 74, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 33, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 24, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([6, 287, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 105, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 31, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 27, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 155, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 21, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 15, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 190, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 210, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 22, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 99, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 29, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 17, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([40, 44, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 17, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 64, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 72, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 33, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 293, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 89, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([80, 23, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 18, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 169, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 116, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 158, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 76, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 150, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 51, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 43, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 166, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 184, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 77, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 25, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 33, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 51, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 234, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 86, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 26, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([48, 38, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 73, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 30, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 144, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 212, 512])
> 2.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 26, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([16, 108, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 45, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 45, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 23, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 124, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 40, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 87, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 328, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 119, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 37, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 105, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 50, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 59, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 173, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 255, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 89, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 205, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 40, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 33, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 21, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([6, 283, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 123, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 18, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([8, 185, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 59, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 36, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 157, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 218, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 221, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 137, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 23, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 24, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([6, 291, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 90, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 145, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 209, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 75, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 46, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 142, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 65, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 38, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 118, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 94, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 42, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 177, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 70, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 31, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 251, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 80, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 154, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 41, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 31, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 21, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([6, 512])
> 1.  torch.Size([6, 118, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([8, 149, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 42, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 82, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([72, 26, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([72, 24, 512])
> 2.  torch.Size([72, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 73, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 75, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 32, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 32, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 119, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 39, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 27, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 78, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 35, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([32, 53, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 26, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([24, 70, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 29, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 43, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([7, 257, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 88, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 29, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 97, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 50, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 25, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 189, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 67, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 67, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 31, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 120, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 163, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 121, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 44, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 61, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 26, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([6, 274, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 87, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 96, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 180, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 151, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([32, 49, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 135, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 172, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 83, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 130, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 59, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 352, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 120, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([80, 22, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([80, 15, 512])
> 2.  torch.Size([80, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 58, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 45, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 127, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 50, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 126, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 168, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 56, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 53, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 77, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 41, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([7, 242, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 86, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 202, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 80, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 89, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 22, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([6, 296, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([6, 97, 512])
> 2.  torch.Size([6, 512])
> 1.  torch.Size([16, 118, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 196, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([40, 46, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 25, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([16, 81, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 41, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 62, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 52, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([5, 330, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 118, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([128, 15, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([128, 14, 512])
> 2.  torch.Size([128, 512])
> 1.  torch.Size([16, 113, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 200, 512])
> 2.  > 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 82, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 165, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 65, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 46, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 219, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 82, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 85, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 32, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 55, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 30, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([64, 27, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 20, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 107, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 43, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 92, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 40, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 122, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 47, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 93, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([5, 342, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 126, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([8, 179, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 69, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 34, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([96, 19, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([96, 20, 512])
> 2.  torch.Size([96, 512])
> 1.  torch.Size([8, 186, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 136, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 176, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([56, 32, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 17, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([48, 37, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([8, 132, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 48, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 174, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 167, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([40, 47, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 26, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([5, 317, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([5, 115, 512])
> 2.  torch.Size([5, 512])
> 1.  torch.Size([16, 90, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 34, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 125, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 57, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([8, 162, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 49, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([48, 36, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([48, 22, 512])
> 2.  torch.Size([48, 512])
> 1.  torch.Size([24, 63, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 27, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 88, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 36, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 134, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 207, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 68, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 80, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 38, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 51, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 31, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([8, 138, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 64, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 213, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 66, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 103, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 143, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 54, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([64, 28, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([64, 21, 512])
> 2.  torch.Size([64, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 39, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 192, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 69, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 231, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 78, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 102, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([24, 60, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([24, 39, 512])
> 2.  torch.Size([24, 512])
> 1.  torch.Size([16, 117, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 147, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 55, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 123, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 52, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 114, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 42, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([32, 58, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([32, 27, 512])
> 2.  torch.Size([32, 512])
> 1.  torch.Size([40, 42, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([40, 21, 512])
> 2.  torch.Size([40, 512])
> 1.  torch.Size([56, 30, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([56, 20, 512])
> 2.  torch.Size([56, 512])
> 1.  torch.Size([16, 84, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 35, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([8, 164, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 181, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([16, 95, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 49, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 86, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([7, 240, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([7, 96, 512])
> 2.  torch.Size([7, 512])
> 1.  torch.Size([8, 153, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 60, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 170, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 63, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 197, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([8, 81, 512])
> 2.  torch.Size([8, 512])
> 1.  torch.Size([88, 20, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([88, 16, 512])
> 2.  torch.Size([88, 512])
> 1.  torch.Size([16, 79, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 37, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 106, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 44, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 104, 512])
> 2.  torch.Size([16, 512])
> 1.  torch.Size([16, 47, 512])
> 2.  2023-07-11 09:43:16 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:11299
2023-07-11 09:43:16 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:11299
2023-07-11 09:43:16 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:11299
2023-07-11 09:43:16 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:11299
2023-07-11 09:43:16 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:11299
2023-07-11 09:43:16 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:11299
2023-07-11 09:43:16 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:11299
2023-07-11 09:43:16 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:11299
2023-07-11 09:43:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-11 09:43:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-11 09:43:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-11 09:43:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-11 09:43:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-11 09:43:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-11 09:43:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-11 09:43:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-11 09:43:17 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 09:43:17 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 09:43:17 | INFO | fairseq.distributed.utils | initialized host capios as rank 3
2023-07-11 09:43:17 | INFO | fairseq.distributed.utils | initialized host capios as rank 5
2023-07-11 09:43:17 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 09:43:17 | INFO | fairseq.distributed.utils | initialized host capios as rank 4
2023-07-11 09:43:17 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 09:43:17 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 09:43:17 | INFO | fairseq.distributed.utils | initialized host capios as rank 1
2023-07-11 09:43:17 | INFO | fairseq.distributed.utils | initialized host capios as rank 7
2023-07-11 09:43:17 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 09:43:17 | INFO | fairseq.distributed.utils | initialized host capios as rank 6
2023-07-11 09:43:17 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 09:43:17 | INFO | fairseq.distributed.utils | initialized host capios as rank 2
2023-07-11 09:43:17 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-11 09:43:17 | INFO | fairseq.distributed.utils | initialized host capios as rank 0
2023-07-11 09:43:19 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11299', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_AT_tokenl_check', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_level='sentence', at_nopad=True, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=2, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_AT_tokenl_check', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/workspace/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/workspace/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-07-11 09:43:19 | INFO | fairseq.tasks.joint_triple_pretraining | dictionary size (dict.wrd.txt): 10,000
2023-07-11 09:43:19 | INFO | fairseq.tasks.joint_triple_pretraining | asr dictionary size (dict.wrd.txt): 10,000
2023-07-11 09:43:19 | INFO | fairseq.tasks.joint_triple_pretraining | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-11 09:43:19 | INFO | fairseq.tasks.joint_triple_pretraining | Initial task weight: asr 1.0: mt 1.0
2023-07-11 09:43:19 | INFO | root | load pretrained embeddings: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 09:43:24 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/fairseq-AT/egs/pretrain-all
2023-07-11 09:43:24 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-11 09:43:24 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-11 09:43:26 | INFO | root | load pretrained hubert
2023-07-11 09:43:30 | INFO | root | load pretrained embedding as ctc proj: /workspace/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-11 09:43:32 | INFO | root | load pretrained encoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 09:43:34 | INFO | root | load pretrained decoder: /workspace/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-11 09:43:35 | INFO | root | share the sematic adapter and textual encoder
2023-07-11 09:43:35 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-11 09:43:35 | INFO | fairseq_cli.train | task: JointTriplePretrainingTask
2023-07-11 09:43:35 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-11 09:43:35 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointAT
2023-07-11 09:43:35 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-11 09:43:35 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-11 09:43:35 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 09:43:35 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 09:43:35 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 09:43:35 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 09:43:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-11 09:43:43 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-11 09:43:43 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-11 09:43:44 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 09:43:44 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 09:43:44 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 09:43:44 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 09:43:44 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 09:43:44 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 09:43:44 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 09:43:44 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 09:43:44 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-11 09:43:44 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-11 09:43:44 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-11 09:43:44 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-11 09:43:44 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt
2023-07-11 09:43:45 | INFO | fairseq.trainer | load the task parameters
2023-07-11 09:43:46 | INFO | fairseq.trainer | Loaded checkpoint ./checkpoints/mustc/st/ende_shrink_AT_tokenl_check/checkpoint_last.pt (epoch 2 @ 2000 updates)
2023-07-11 09:43:46 | INFO | fairseq.trainer | loading train data for epoch 2
2023-07-11 09:43:46 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-11 09:43:46 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 09:43:46 | INFO | fairseq.tasks.joint_triple_pretraining | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/workspace/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-11 09:43:48 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 09:43:52 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 09:43:54 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-11 09:44:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-11 09:44:58 | INFO | fairseq.trainer | begin training epoch 2
2023-07-11 09:44:58 | INFO | fairseq_cli.train | Start iterating over samples
>  torch.Size([48, 1]) torch.Size([48, 1])
>  torch.Size([12, 1]) torch.Size([12, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([48, 1]) torch.Size([48, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([112, 1]) torch.Size([112, 1])
>  torch.Size([160, 1]) torch.Size([160, 1])
>  torch.Size([64, 1]) torch.Size([64, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([64, 1]) torch.Size([64, 1])
>  torch.Size([12, 1]) torch.Size([12, 1])
>  torch.Size([96, 1]) torch.Size([96, 1])
>  torch.Size([80, 1]) torch.Size([80, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([128, 1]) torch.Size([128, 1])
>  torch.Size([48, 1]) torch.Size([48, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([48, 1]) torch.Size([48, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([192, 1]) torch.Size([192, 1])
>  torch.Size([144, 1]) torch.Size([144, 1])
>  torch.Size([14, 1]) torch.Size([14, 1])
>  torch.Size([160, 1]) torch.Size([160, 1])
>  torch.Size([14, 1]) torch.Size([14, 1])
>  torch.Size([80, 1]) torch.Size([80, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([80, 1]) torch.Size([80, 1])
>  torch.Size([112, 1]) torch.Size([112, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([48, 1]) torch.Size([48, 1])
>  torch.Size([64, 1]) torch.Size([64, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([80, 1]) torch.Size([80, 1])
>  torch.Size([112, 1]) torch.Size([112, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([80, 1]) torch.Size([80, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([48, 1]) torch.Size([48, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([14, 1]) torch.Size([14, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([48, 1]) torch.Size([48, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([48, 1]) torch.Size([48, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([48, 1]) torch.Size([48, 1])
>  torch.Size([48, 1]) torch.Size([48, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([48, 1]) torch.Size([48, 1])
>  torch.Size([112, 1]) torch.Size([112, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([48, 1]) torch.Size([48, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([112, 1]) torch.Size([112, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([48, 1]) torch.Size([48, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([48, 1]) torch.Size([48, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
>  torch.Size([16, 1]) torch.Size([16, 1])
>  torch.Size([32, 1]) torch.Size([32, 1])
2023-07-11 09:46:16 | INFO | train_inner | epoch 002:    627 / 1474 loss=6.815, trans_loss=5.303, nll_loss=3.993, w2v_ctc_loss=3.36, task_loss=1.292, contrastive_loss=1.102, total=4126.49, n_correct=364.54, ppl=15.93, accuracy=8.834, wps=6670.8, ups=0.54, wpb=12299.1, bsz=445.5, num_updates=2100, lr=8.4058e-05, gnorm=1.15, clip=0, loss_scale=64, train_wall=70, gb_free=19.2, wall=0
