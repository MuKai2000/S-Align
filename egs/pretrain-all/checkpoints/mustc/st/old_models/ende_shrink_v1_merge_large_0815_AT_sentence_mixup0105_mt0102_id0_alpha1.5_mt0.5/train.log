2023-08-15 15:09:15 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:10990
2023-08-15 15:09:16 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:10990
2023-08-15 15:09:16 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:10990
2023-08-15 15:09:16 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:10990
2023-08-15 15:09:16 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:10990
2023-08-15 15:09:16 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:10990
2023-08-15 15:09:16 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:10990
2023-08-15 15:09:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-15 15:09:16 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:10990
2023-08-15 15:09:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-15 15:09:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-15 15:09:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-15 15:09:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-15 15:09:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-15 15:09:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-15 15:09:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-15 15:09:17 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-15 15:09:17 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-15 15:09:17 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-15 15:09:17 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-15 15:09:17 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-15 15:09:17 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-15 15:09:17 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-15 15:09:17 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-15 15:09:17 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-15 15:09:17 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-15 15:09:17 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-15 15:09:17 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-15 15:09:17 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-15 15:09:17 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-15 15:09:17 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-15 15:09:17 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-15 15:09:21 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10990', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-15 15:09:21 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-08-15 15:09:21 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-08-15 15:09:21 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-15 15:09:21 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-08-15 15:09:21 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-15 15:09:26 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-15 15:09:26 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-15 15:09:26 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-15 15:09:28 | INFO | root | load pretrained hubert
2023-08-15 15:09:30 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-15 15:09:32 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-15 15:09:35 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-15 15:09:35 | INFO | root | share the sematic adapter and textual encoder
2023-08-15 15:09:35 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-15 15:09:35 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-08-15 15:09:35 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-15 15:09:35 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-08-15 15:09:35 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-08-15 15:09:35 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-15 15:09:35 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-15 15:09:35 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-15 15:09:35 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-15 15:09:35 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-15 15:09:40 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-15 15:09:40 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-15 15:09:40 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-15 15:09:41 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-15 15:09:41 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-15 15:09:41 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-15 15:09:41 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-15 15:09:41 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-15 15:09:41 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-15 15:09:41 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-15 15:09:41 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-15 15:09:41 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-15 15:09:41 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-15 15:09:41 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-15 15:09:41 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-15 15:09:41 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-15 15:09:41 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-15 15:09:41 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-15 15:09:41 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-15 15:09:41 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-15 15:09:41 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-15 15:09:42 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-15 15:09:44 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-15 15:10:34 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-15 15:10:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 15:10:34 | INFO | fairseq.trainer | begin training epoch 1
2023-08-15 15:10:34 | INFO | fairseq_cli.train | Start iterating over samples
tensor([[   7,  422,  874,    0,    0,    9, 6788,  369,   12, 3648,    0,   26,
            7,  874,    9, 2761,   12,   13,  572,    0,    7,  572,   26,    0,
            0,    9, 2761,    0,   12,   13,  874,    0,    8,  722,   26,    7,
         7465,  131,  166,  251,    0,  248,   63, 3249,   62,    9,    7,  772,
          207,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [1179,   25,  135,   21,  109,   86,    0, 1335, 2348,   12, 1613,   71,
          747, 2693,   11,    6,    8, 1335, 2348,   12,   13,    0,  265,    0,
         6466,    6,  148,  205,   10,    7,  281,  909,  119,  236, 1563, 8562,
            9,  108,  753, 1772,    9,  909,   35, 3080,  109,  909,   35, 9376,
           54,    8, 4429,    0,    8,   53,  467,  132, 2762,  159, 2767,    6,
            0,    2],
        [  67,   21,  465,   11,   18,  468,    7, 2932, 1880,  567,    0,    8,
          113,    7, 2932, 3612, 1381,  832,   18,  233, 1181,   33, 3612, 1181,
         3860,   59, 1710,   10, 5111,   15,   21,    6,  768,   10, 5574,    7,
         1749, 1381,    8,    7,  341,  409, 1955,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  29,   56, 1285,    0,  347,  192,   11,   18,   25,  450,  117, 6010,
           70,   21,   11,    6,  100,  378, 7000,   71,   33, 4716,  434,  427,
          821,  407,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   9,  409,    0, 2753,  427, 1105,  810, 3565,   17,   21,  659,   51,
         1953,    6,   12, 1655,   12,  215,  801,    0,  166,  818,   17,   33,
         1167,  659,   66,  245,   13, 2003,   15,  106,    7, 1247,   12,   13,
          258, 1591,   17, 2963, 1585, 1763,    7, 1644,  511,  762, 1054,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,   11,    6,   13,  563,   93, 2484, 9473,   13, 4866,   10, 2266,
          389,  552, 1252,   11,    6,   38, 5001,  411,  293,    3,   24,   73,
          205,   10,   91,   12,    7,  822,  608,   56, 9694, 1075,    0, 3109,
         3902,    0,    8,  150,  113,   13, 1523, 3609,   12, 2718, 7719,    0,
          106,  747,    9, 1665,   10, 1962,    9, 1912,    0,    2,    1,    1,
            1,    1],
        [  33, 2780,   34, 6291,  111, 2770,   55,    7, 2932, 1232,    0,   67,
          115,   21,   11,    6, 2823,   54,  100, 3139,  140, 2704,    6,    9,
            7,  832,    0,    6,    0,    8, 2627,    0,  206, 2406,    6,   63,
         4213,   48,   10, 3067, 3316, 1006,   85, 3646, 1729,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 108, 2476,   26,   10, 8711,   17,   53, 1516,   71,   13,  509,  841,
           12,  138,   10,  229,  214,  254,  120,   53, 6565,    0,    8,    7,
         2027, 6936,  475, 2041,   17,   25,   73, 1531,  214,  126,  131, 1974,
          416,   54,  336,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:0') tensor([51, 62, 45, 28, 50, 58, 47, 41], device='cuda:0') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
2023-08-15 15:10:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
tensor([[  55,  663,    0,    9,  264,  109,  232,  221,    6,   24,   66,  682,
         3196,   22, 2932,  896,    0,  206,   53, 3860,   75,  241,  221,   77,
          236,  866,    8, 6387,    8,   29,  234,  452, 6029, 7401,    0,    2,
            1,    1,    1],
        [ 115,   24,   11,  121,  492, 1110,   13, 7123,  100,   33,   12,   13,
         1470, 3189,    0,   67, 1470, 3189,    6,   73,   51, 1346,    0,  276,
          103,   53,   11,   57,   86, 1110,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [ 168,   26,  138,  488,   12,   13,  775,  411,   37,   34,  859,    0,
           21,    0,   34, 1523,    0,    0,   10,    0,    0,  508,   25,   13,
          841,   12,    7,    0, 1823,    0,    0,    0,   84,   25,  205,    0,
            2,    1,    1],
        [ 115,    7, 1885, 5055,    0,   12,  538,    0,   26,  206,   13,  325,
           12,    7, 2394, 4793,    8, 2791,  513,  199,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  29,   21,  818,   17,   51,   20, 7931,  373,   63,  133,  324,   85,
          203, 4030,  140,   54, 2429,   35, 2611,    6,    0,    8,   29,   53,
           11,  121,  226,  529,   10, 3258,  251, 4513,   96,    0,    2,    1,
            1,    1,    1],
        [  24,   66,    7, 2216,  768,   10,  205,    0,   38, 5276,   26,   70,
           19,  213,   10,   87,    3,    8,    9,   13,  613,  982, 1479,    0,
           17,    0,   10,  110,    0,   26,   13, 1968,  279,    0,    2,    1,
            1,    1,    1],
        [  25,   73, 6265,  281,   12,   17,   13, 1674,    6,    0,  125,  103,
           25,   11,   57, 4298,  336, 3594,    0,    8,    7,  595, 3336, 1233,
         2233, 4457,  111,    0,   25,   11,   57,  492,  142,   10, 1713,   17,
          595,    0,    2],
        [  29,    0,   85,   33,  613,    0,   19,   87,  213,   10,  450,   25,
           17,  185,   12,    7, 2767, 4131,   24,   66,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 101, 1119,    0,   38,   18, 2704,  518,  155,  227,  715,   96,    3,
          125,   33,   26, 1320,  111, 2059,    0,   77, 2808,   10,    7, 9510,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,  131,   13,  316,  287,    0, 7492,  136,  760,  322,   93,
            8,  697, 1262, 2178,  713,    0,    8,   53,   11,   57, 2091,    9,
         2567, 1507,    0,  116, 5000,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   0,    9,  264, 1884,  166,    0,   63,  142,   10,   51, 1405,    0,
           33,  169,   86,   51, 3118, 1623,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  67,   21,  465,   11,   18,  229,    0,   13, 1878,   10,  565,    0,
          103,    0,  284,  708,  162,  142,   10,   51,    0,   13, 2988,  109,
            0,   13, 2088,    0,   53,  162,    0,  142,   10,  205,   10,  670,
            0,    2,    1],
        [  67,   24,  499,  213,   10,   51, 1123,   17,   33,   26,    7,  281,
         4089,    8, 2426, 3180,   17,   24,   73,   51, 2985,  199, 2406,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,  768,   62,  131, 1962,   35,  699,  119, 3050,    6,  411,
          639,    0, 7176,   20,  111,  752,   10, 1914, 4907,   10,  875,  134,
          199,    7, 1232,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,    7, 5714,   17, 7076,    9,    7, 3227,   12,  423,  759,
         2270, 1098,   18,  500,    6,  148,  618,   69, 1102,  926,    6,   12,
            7, 8783,    8, 6907, 4895,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,  213,   25,   10,  150,  138,   33, 7223,   26, 5802,   62,    0,
            8,   21,   11,    6,  142,   10, 2546,  436,   29,   25,  150,   17,
           21,   11,    6, 1117,  126,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1]], device='cuda:4') tensor([36, 32, 37, 22, 35, 35, 39, 22, 26, 31, 20, 38, 26, 29, 31, 31],
       device='cuda:4') tensor([1.0000, 1.0000, 0.8418, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8721, 0.8208, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
tensor([[  67,    7,  143,   24, 1260,   85,   21,    0,    7,  143,   24, 1873,
           17, 1057,  453, 3540,   34,    7,  133, 1890,  279,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,    0,   24,  552,  132,   71,   13, 4445,    0,    8,    0,  204,
           70,  956,   34,   91,    0, 2767, 2923,   37,    9,  264, 1736,   46,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 211,    0,    0,   19,  641,    0,    0,   19,  154,   46,    0,  682,
            0,   67,   26,   17,  155, 4234,    0,  846,  174,    0,    0,  211,
            0,    0,  682,    0,   70,   34,   17,   91,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  33,   26,   89, 9264,    0,   86,  860,   13,  488,   91,    0,   19,
          213,   10,  289,  116,   13, 1455,   80, 2263,  556, 5610,   54,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19,   11,   45,  142,   10, 4431,   35,  140, 4076,   69,  832, 1486,
          335,    0,    8,   24,   73,  150,  832, 1486,  335, 2546,  829,   69,
           21,    6,  926, 1585,   71,   21,    6, 4241,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [   9, 1296,   10, 2572,   25,   33, 1678,    0,   19,   11,   45,  142,
           10,  296,  185, 5825,    6,   10,  367,  132,   69,    6, 5072,  172,
         2117,    0,    8,   24,   11,   57,  142,   10,   87,   13,  277, 1860,
           35,  399,  102,  287,    0,    2],
        [  24,  465,   11,   18,   87,   33,  131,  204, 2905, 3141,    0,  100,
           25,   87,    9,   13, 1192,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  17, 1073,   13,    0, 1878,  629, 1891,    7,  595,    0,    9, 1416,
           12,   25,    8,    0, 4958,   54,    7, 4292,    0,  109,  893,  199,
           39, 4292,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 115,   26,   33,  250,   17,   24,  451,   51,  101,   62, 1256,   12,
            0,   12,  538,    0,   21,   11,    6,  528,   17,   24,  135,   33,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  77,   12,   33,   10,  289,   17,    7, 4719,  629,   70,   24,   73,
         7039,    8,   70,   24,   73, 2032,   26,  288,  142,   10, 4038,   22,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  33,   26,  106,   38,  469,  291,   18,  187,   62, 1224,   12, 1491,
            3,    9,  564,  868, 1047,    0,   84,  162, 1107,  820, 5149,  690,
         7653,   20,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  60,  116,  188,   13,  264, 3742, 6211,   17,  368,    6, 1019,  908,
          768,  254,  108, 6211,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   7, 5346,   12, 1009,   17, 1241,   13, 1127,  567,   26,    0,  554,
            0, 2816,  419, 1661,   57,  969,    6,  369,  109,  419, 1620,   54,
         5974,  440,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 214,   63,  341,  125,  115,   19,  135,   17, 4751,   26,    7, 1037,
         2829,   17, 1459,  188,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 103,   25, 1260,   85, 2264,  241,  140, 1782, 5152, 1160,  158,   25,
          162, 2639,    0,   25, 2220, 1631,  830,  150,  347,   53, 5953,   48,
         6387,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  24,    0,  692,   10,   66,   13,  740,  672,   71,    0,    7,  230,
            0,  688,    0,   71,    7,  230, 4157,    0,   71,    7,  230, 1404,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:0') tensor([23, 25, 34, 25, 35, 42, 19, 28, 26, 26, 28, 19, 28, 18, 27, 26],
       device='cuda:0') tensor([1.0000, 0.8755, 0.8188, 1.0000, 1.0000, 1.0000, 1.0000, 0.8848, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8677],
       device='cuda:0', dtype=torch.float16)
2023-08-15 15:10:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
tensor([[  67,  108, 8830,   48,  589,    0, 7557,    6,    0,  166,   63,    7,
         7557,    6,   17,  601,  170,   10, 6108,    8, 2544,    8,   51,    0,
            0,   13,  562,   18,    0,    8, 9063,   62,    8,   29, 4404,    8,
           29,   69,    0,   53,    0,   63,  261,  143, 7678,   62,  131, 1912,
          688,    0,    2],
        [  24,  116,  144, 6985, 2421,    6, 4502,   80,  854,   39, 3350,  581,
           77, 1004,    7, 6869, 9271,    0, 3381,   70,   11,    6,  956,    0,
            8,   24,   11,  121, 2138,   13,  325,   12,  183,    9,    7, 1726,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 185, 2193, 6248,   22,    6,    9, 1777,  150,  156,   79,   93,  675,
           45,    9, 5222, 1290, 1373,    6,    6,  693,    0,   67,  294,   73,
           51, 5853,  131,    7, 2932, 3288,    0,    8,  203, 1203, 1015,  922,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 106, 1244,   35, 1966,  673,   54, 2409,   10, 2465, 3602,    6,   10,
           33, 2178, 2445,  266, 1192,   69,   89,  906,    0, 3927,   63, 4015,
         5869,   12, 2434,  108,  179,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,  154,   21,   26, 2497, 1226,   55, 1221,   10, 1772,    9,    7,
          245,  561,  929,   13, 2226, 1757,   55,   70,  169, 1085,   10,    7,
          461,  266,  763,  480,    6, 1042,    7, 3677,  188, 3699,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   7, 4100, 1649, 1085,  143, 1149,    0,   67,  914,   69,    7, 1296,
           12,   13, 4400,   15,   22,  996,    0,  333,  555, 7691,  109,  333,
          555, 3619,  215,    0,   67,   21,   11,    6,  499,  250,   10,   51,
         4837,   12,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   8,    7, 7745,  659,   51,    0,   67,   21,  689,   11,   18, 4432,
           10,  452, 1539,  128,    6,    0,   17,   24,   66, 5020, 6083, 5475,
            6,   46,    7, 1320,   59,  816,  300, 1079, 1643, 1794,  230,   85,
            7, 1219,  859,   35, 2514, 4209,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [   9,  409,    0,    7,   29,   70,   26,    0,    9,    7,  572,    0,
         2317,    9, 7418, 3782,    0,    7,  214,   17,  229,  155,  572,  283,
            0,    7,  214,   17,  229,  155, 3236,  283,    9,  155,  572,    0,
           63, 8220,  111,  415,   59, 8186,   71, 2317,    9, 2011, 3043,    0,
            2,    1,    1]], device='cuda:7') tensor([51, 38, 38, 31, 36, 40, 44, 49], device='cuda:7') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
tensor([[ 131, 1183,  148,   26, 1948,  664,   62,  132,   71,    7, 4617,  292,
         2109, 1775, 4040,    6,   12,   33, 6536,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   19,   11,   48,  100,   10,  227, 2174,  232,  445,  159,  283,
           71,   13,  277, 7074, 4112,    0,    8,    7, 4112,   26,   33,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 1425,   19,  144,   10, 1336,  235,    0,   19,  442,   17, 3486,
            8,   79, 2392,   79,   19,  442,   21,    0,   39, 2285, 1790,   12,
         2922,  552,  244,  110,    0,    2,    1,    1,    1,    1],
        [ 877,  290, 2689, 2345, 1212,  188, 2277,   48,   10, 2699,  143,    8,
          143, 5593,   10,   33,    0, 5413, 3301, 5690,    6,  199,  459,  510,
           96,   17,  719, 2070,    0,    2,    1,    1,    1,    1],
        [  29,   24, 6103,  154,   12, 4781,   79,  378, 3816,  214,   17,   24,
          991,   55,  133, 3926, 3965,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  12,  538,    0,  108, 4734,  192,   11,   18,  116,  468, 1004,  183,
            0,   53,  113,  468,  106,  561,   10,  561,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53,   66, 4595, 1122,   59,  699,   35,    6,  819,  424, 1942,   96,
            8, 4515,    6,   17,   63,   86, 3412,   62,  131,  883,   48,  293,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  103, 2263,   96,  498,    0,   10, 3424,    0,  180,    7,  723,
           73, 5415,    0,    0,    7,  524,    8, 3536,    7, 2517,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 427,  412,  430,  158,   13,  188,   13, 1905,  321,   12, 6873, 1117,
           17,    0,  120, 8491,   48,    0,    7,  427,  412,  430,  158, 3336,
           10, 1051, 1672,  153,  336,    0,  116, 7743,    0,    2],
        [  29,  417,  140, 2362,    6,   93,   26, 3423,   13, 1396, 2233,  199,
           39, 3993, 1874,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33,  188,  226,    7, 3841,    6,   20,   12,  333, 1670, 2127, 9746,
            6,    0, 3448, 7414,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  225, 4576,    6,   10,  110,    7, 1790,  225, 1831,   79,   13,
          785,   35, 1933,   35, 1178, 3642,    9,   17, 6460, 4057,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  116,  323,   13, 4454, 2761,   69, 5335,   80,    7, 5169,    8,
         1095,  248, 8099,    6,  558,   10,  545,  218,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 4639,   17,   24,  220,   51, 1155,  143,  254, 3857,    8, 1064,
            0,   67,  138,   10, 3918,    6,   20,  126,    7,   91,  106,    7,
          218,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  29,    7, 2551,   12,  324,    8, 6790,  214,   26, 2720,   48,   10,
         7572,    8,  211,  234, 1410,  837, 5002,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  101,   11,    6,    0,   13, 4961, 1531,    0,    0,   79, 3708,
           18, 3304,    0,  169,   66,  246,    0,   79,  238,   79,   13, 1880,
         1531,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   8,   10, 4637,  373,    0, 2192,   26,  116,  486, 6101,  567, 4111,
           10,   51, 4637,   62,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 8900,   62,  250,   19,  172,  144,   13, 2184,   22,  311,   69,
            0,   26,   17,    7, 1956,  204,  213,    6,   10,  283,   55,  159,
          110,  128,    0,    2,    1,    1,    1,    1,    1,    1],
        [   8,   17,   11,    6,  593, 1706,   12,   13,  183, 2760,   55,   77,
            7,  341, 1565,  409, 1215,   17,   24,  135,   10,  468,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1777,  188, 1220, 6380,   48,   17, 2439,  558,  464,    0,   53,   11,
           57, 5332,   54,   13, 4369,  200, 1515, 2902,    8, 3585,  567,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  84,   63,  423,   12,  117, 3357,    0,    8,   53,   63, 4673,    9,
          159, 4268,    8,  159, 4972,    0,    8,   53,   63, 2762,   94,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   45,  976, 8372,   17,   24,   66,  535, 1313, 5260,    7,
          613,  206, 4387,    6, 3480,  108,   24, 1591, 2637,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  13,  717,   35, 1315,  322,  801, 5725,    6,  221, 1339, 5061,  292,
            6,    6, 3222, 1026,   14,   48,   71,   33,    9,   21,    6, 9442,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  69,    7,    0,  939, 2836,  834,   26,    0,    7,   56,  111,    6,
         3620,  266,  207,   17,  368,    6, 2829, 7337, 7393,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:7') tensor([20, 25, 30, 30, 20, 22, 26, 24, 34, 17, 18, 24, 22, 27, 21, 27, 18, 28,
        24, 25, 25, 23, 26, 23], device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8550, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8848, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8975], device='cuda:7',
       dtype=torch.float16)
tensor([[  67,   24,  150,   17,  545,   91,   12,  117,   26,  204, 4637,   54,
            0,    8,  188,   13,  341,  207,   12, 3101,  126,  138,   10,   87,
          282,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   89, 1970,   34,   13, 3090,  756,    0,    8,   89,  291, 3769,
            0,    0,   34,   13, 1390,   37,  684,    8,    0,  113,   13, 1116,
         1040,   37,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  164,  450,   25,    0,   13,  546,   12,  248,  341,    0, 2352,
            0,    0,  248, 1284, 2352,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  391, 1551, 1065,    0,   17, 2182,  144, 3814,   48,   89,  637,
            8,  333,  473,  279,    9,   21, 3706,   55,  110,   10,   13, 1098,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  24, 3195, 6482,   35, 6937,  227, 3134,  221,    0,   19,    0,   20,
            0,  108, 2805,    0,  132,   10,  108, 7009,    6,   11, 1492, 1485,
           12, 3402,   93,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  53,  246,   53,  169,  780,   29,  835,   10, 1008,    7, 8365,    6,
           12,    7, 3311,    0, 7443,   17, 1573,  162, 3754,    8,  401,    7,
          370,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   24,  289, 1228, 3374,    0,  281,   12,   25,  154,   80,  708,
            0,   21,   11,    6, 1301,    0, 2703,  901,  439,   12,  708,   87,
          722, 1228, 3374,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 291, 1105,   18, 3396, 1095,    0,    7, 4480,   79, 1019, 2071,    0,
         1095,    7,    0,    0,    0, 5640, 1375,   79, 1019, 2071,  755,    0,
          254,   94,  148,  162,    9,  509, 1862,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57,  116, 2213,   54,    8, 9171,    0,   38, 3127,   89,
         3397,   26,    3,   70,   11,    6, 1226,   71,  682,  616,  249,  407,
            6,    0,   38,  187,   11,  121,  144,  185,  453,  110,  128,    6,
           85,  682,  616,  249,  407,    6,    0,    2],
        [ 101, 5523,   17, 1381,  220,   51, 1405,  131,    7,   94,    0,    8,
           24,  583,  251,   94, 9475, 4637,  373,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  53,  162, 1052, 1563,   94,    0,   67,   53,  162,  113,  172, 7176,
           20, 3979, 2120,    0,    8,   53,  162, 9231,   62,   12, 4391, 3834,
           12, 3716,   35, 7305, 2527,   15,  609,   96,    0, 5653,   10,    7,
          218,  415, 2633, 2120,    0,    2,    1,    1],
        [  85, 4615,    0, 9785,  369,   46,   17,  101,   11,   48,  226,  987,
         1181,  131,   39, 6651, 2266,  160,   22,    0,   13, 5289,  126,   10,
         2585,   20,  366,  565,    0,  276,   10, 9304,    7,  282,  126,   12,
          565,    0,    2,    1,    1,    1,    1,    1],
        [   0,    8,   53,  276,    0, 2982,   80, 1790, 8661,    0,    0,  276,
            0, 1445,   53,  162,    9, 3454,    0,  276, 1445,   53,  162,  830,
         1785,  810,    7, 5167,  608, 2020,   12,  159,  931,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  117, 4312,   22,    6,    0, 1025, 3043,   54,    0,   53,  415,
          158, 1515,   71,    7, 7224,    0,    8,    9,  117,  415,  158,  340,
         1955,   53, 3121,   13, 1905, 1762,   12,  941,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,   34,   85, 8195,   12, 2931, 2735, 1221,   46, 1016, 4618,
          866,  693,   46,  120,  248, 4618,  866,  693,  162,   86,  952,   10,
          545,  218,    0,   19,  169,  939, 8966,  834,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 131,    0, 5114,  235, 2399, 1006,    0,  889,   63,    0,    0,    9,
           13,    0,  841,    0,    0, 1449, 1778,  159, 5186,   10,    7, 1722,
         3609,   12,  422, 6222,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([27, 28, 19, 26, 29, 27, 29, 33, 44, 21, 42, 39, 35, 34, 34, 30],
       device='cuda:1') tensor([1.0000, 0.8887, 0.8271, 1.0000, 1.0000, 1.0000, 1.0000, 0.8584, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8721, 1.0000, 1.0000, 0.8208],
       device='cuda:1', dtype=torch.float16)
tensor([[  53, 2828,  987,  ...,    1,    1,    1],
        [6938,   25,   66,  ...,    1,    1,    1],
        [   8,   12,  538,  ...,    1,    1,    1],
        ...,
        [  29,   24,   66,  ...,    1,    1,    1],
        [  67,  985,  687,  ...,    1,    1,    1],
        [   0,    8,  432,  ...,    1,    1,    1]], device='cuda:1') tensor([14, 13, 27, 18, 21, 17, 19, 15, 18, 17, 24, 12, 15, 15, 33, 26, 20, 11,
        22, 14, 15, 22, 24, 13, 20, 16, 15, 17, 15,  9, 16, 21, 15, 13, 16, 26,
        19, 13, 24, 13], device='cuda:1') tensor([1.0000, 1.0000, 0.8550, 0.8975, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8838, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8848, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8677], device='cuda:1', dtype=torch.float16)
tensor([[   9,   13, 1880, 4745,    0,   25,  498,   71,  211, 2837,    8,  211,
         4774,   10, 1393,   17,  199, 2332,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   29,  131, 3696,    7, 4221,    0,   24,   73,  446,  126,  261,
          143,   80,  155, 1037,  254,   25,  914,  135,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 101,   11,    6,    7, 6099,   12,  958,    9,    7, 6869, 8521, 2353,
           12, 5205,    0,    9,  421,  234,   48,  365,  221,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  24,  162,   86, 2841,    9, 1979,    0,  290,    0,   69,    7, 5225,
           38, 2862,  781,  918,   93,  197, 1017,    0, 2503,   18,  650,  568,
           86, 2115,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [3398, 7936,    8,  284, 1543, 1948, 1985,   87,   21, 2373,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,   84,   11,    6,  288,   91,  227,   15,    6, 2158,    0,    0,
          207,   10,    0,  205,  106,   84,    0,    0,   25,  175,  717,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   0,   10,  101,  128,    0,    0,   10, 1008,    0,   10,  289,    0,
           38,  533, 2056,   13, 1738, 1234,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 206,   11,    6,    7, 2808,    0,   38,  533, 1412, 2215,  159, 2360,
           10,    7, 2182,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 473,  111,    0,   33,   26,   13, 2322,   35, 5444, 2595,  856,  293,
            0,  125,   19,   66,   29,  294, 1516,    6,    9,   89, 1995,  918,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [7494,  185, 1118,  307,   34,   39, 1830,   35, 1933,   35, 1178, 2988,
          120,  101,   34, 1920,   22, 1371,   62,  106, 3022, 1146,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   33,   26,  116,  133,  946,   46,  183,   26,   69,    7, 3089,
           35, 7988,  340,    8,    7, 2509,   26,   69,    7, 1995,   35, 7988,
          340,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,    0,  169,  154, 1827, 1794,    0,  162, 4491,    0,  166,  818,
           53,  451,  601,   25,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   0,    8,   10,   87,   17,    0,   25, 1275,    0,   66,    0,   10,
         4988,   13, 1472,   12, 2066,  416, 1374,  572,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    0, 1220,   25,  659,   51, 1211,   12, 1835,    0,   38,  187,
           11,   45,   86,    9,   18, 2687,  241,   54,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  70,   11,    6,    7,  613,   12,  117, 3742, 3228,    6,   17,   25,
           11,  121,  591,    0,   38,  511,  101,  246,    3,  238,    0,   19,
           11,  121,  211,  479,    0,    2,    1,    1,    1,    1,    1,    1,
            1],
        [  33,  188, 8166,   19,  362, 1838, 1428,   55,   70,    7, 1479,   26,
          142,   10,   87,    9,    7,  858,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  17,   11,    6, 5653,   10, 1719,   59, 5292, 3147,  111, 1830,  439,
           55, 2677,    8,  248,   10,  391,  439,   55, 5031,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,  180, 2568,  347,   19,  305,    0,    0,   29,  261, 7003,  106,
         4990,  589,  596,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   7, 2403,   26,   17,   84,   11,    6,   13,  785,   35, 9058, 3900,
            0,   86,    7, 1953,  322,   35, 3149,   35,  290,   35, 8856, 3900,
          100,   10, 4482, 2147,    0,   67,   13,  785,   35, 9058, 3900,    0,
            2],
        [   8,    7, 1714,  608,  423,  439,    0,   53,  305,   80,  248,  439,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 244, 2869, 4021,    0, 2008, 6248, 1405,   91,   12,    7,  281, 3861,
          338, 3232,  128, 4621,   24,  135,   80,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,   11,   45,  142,   10,  575,   25,  138, 7218,  204, 2895,    6,
           71,  108, 5967,  282,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 110,  153,  240, 1547,    6, 3067,   62, 6873,    0,    8, 1645, 9745,
            6,    0,    0,    0,  593,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  67,  103,   25,  192,   11,   18,   66, 6759,   35,    6, 2174,  297,
           62, 3867,    0,  180,    7,  630,   12, 1703, 6230,   26,   13,  341,
           91, 5020,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:6') tensor([20, 22, 23, 28, 12, 25, 20, 17, 26, 24, 27, 18, 22, 22, 30, 20, 23, 17,
        37, 14, 21, 18, 19, 28], device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 0.8721, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8916, 0.8208, 1.0000, 1.0000, 1.0000, 1.0000, 0.8887,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8706, 1.0000], device='cuda:6',
       dtype=torch.float16)
tensor([[1250,    0,   53,  ...,    1,    1,    1],
        [ 832, 1337,   22,  ...,    1,    1,    1],
        [  67,   25,  113,  ...,    1,    1,    1],
        ...,
        [  24,  278,    0,  ...,    1,    1,    1],
        [  24,  321,   12,  ...,    1,    1,    1],
        [  19,   11,   45,  ...,    1,    1,    1]], device='cuda:6') tensor([11, 11,  9, 13, 12, 12, 12, 12, 16,  9, 10, 11, 10, 11, 11, 12, 13, 11,
        11, 10,  9, 13, 17, 15,  7,  9, 11, 12, 17,  6,  7,  9, 10,  8, 10,  8,
        10, 11, 14, 13, 11,  9,  8, 19, 12,  9,  9,  6,  8, 16,  8, 12, 11, 13,
        10, 12, 10, 13, 10, 15,  7, 12, 11, 12, 13,  9, 12,  8, 13, 20,  8,  8,
         8,  7,  9, 11, 12, 11, 11, 12], device='cuda:6') tensor([0.8091, 1.0000, 1.0000, 0.8789, 1.0000, 0.8735, 1.0000, 1.0000, 1.0000,
        0.8555, 1.0000, 0.8799, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8218, 0.8652, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8691, 1.0000, 1.0000, 0.8521, 1.0000, 1.0000, 0.8984, 1.0000,
        0.8286, 1.0000, 1.0000, 1.0000, 1.0000, 0.8105, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8838, 0.8677, 1.0000, 1.0000, 1.0000, 0.8369,
        0.8652, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:6', dtype=torch.float16)
tensor([[  67,  490,   24,   87,   17,    0,   19,   11,   48,  100,   25,   10,
          154,   17,   85,  419, 1800,  183,  120,   25,   11,   57, 2202,   84,
            0,    9,  155, 1992, 5896, 1247,    0,    7, 2583, 8748, 9169, 3413,
           54,   10,  155, 9879,    0,    7, 9879,   26, 3413,   54,   10,  155,
         2583, 8748, 9169,    8,   33,   26,  138,  155, 4941,   26, 3803,    8,
         8965,   48,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [5609,   10,    7, 1793,  183,  368, 5673,    0, 1276,    6,  499,   87,
         4986,   79,  261, 1269, 2536,   20,   79, 1970,    6,    0,  166,   26,
          509,  254,   21,   34,    9,   95,  424, 3400, 2197,   11,    6,  383,
            0,   67,   19,  499,  154,   17,  250,  225, 2396,   26, 4803, 5040,
            0,   38,  187,   66,   86,  226, 2373,    9,    7, 9091, 1313, 9094,
            3,   68,  194,   65,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  56,  525,   11,    6, 4197,  246,   10,  267, 1848,    0,   38,  160,
            0, 1491,    0,    0,   33, 4925, 2823,    6,   55,  815, 6091, 1086,
            3,  267,    0, 1848,    0,  148,   63, 1102, 4907,    0, 1260,   69,
            0, 4340,  982,    0,    0,   38,  511,   17,   11,    6,   86,   77,
           46,  415, 1040,    0,   26, 1028,  126,    0,   71,   13,  264,    0,
         1375,    0, 2199, 1132,  820,    3,  225,    0,    0,  246,    0,    0,
           38, 2207, 4925,  164,    0, 2823,   55, 1581,    3,    3,  225,    0,
         8640,   48,    8,  246,    0,    0,   38,  187,  192,   11,   18,  135,
            0,  103,   17,   11,    6,    0, 1581,    3, 1995,  241,  221,  109,
            0, 1581,    3, 1793,    0, 1086,    0,    0,   67,    0, 3296,    0,
           21,   11,    6, 1581,    3,    3,   68,  194,   65,    2],
        [   8,    9,    7,  370,  207,  294,   12,  170,   63,  115, 5954, 4273,
            6,   17, 3412,  108, 1259, 1882,    0,  108,  203,    6,  777, 1991,
            0,  108, 3857,    0,   69,    7, 1080,    6,   17,   33,  659,  601,
          170, 3560, 4390,    0,   24,   73,  884, 1179, 5625,   54,    8,   39,
          128,   93,  195,   54,    7, 1289,   24, 2056,    0,   24, 8099,    0,
           24, 3836,    0,   24, 2153,    0,   73,  450,  170, 4324,   12,  183,
         1179,  250,  659,  205, 1226,   71,  108, 3227,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   71,   33,  822, 5108,    9,   13, 3708, 4774,    0,   24,   66,
         6960,   48,    9,  824, 2723,   45,  726, 1090,   17,   24,   73, 3814,
            7, 1729,   35, 1727,  430,  690, 3275,  110,  249,  132,   10, 1421,
          439,   12,   70,   26, 5914, 1417,   46,  248, 1086,  690, 3275,  110,
          249,    0,   24,   87,   21,   85, 7738,    0,  820,  828,    0,    8,
           24,   73,  499,  875,   17, 3151,  346,    0,  630,   12, 1823,    0,
          103,   25,   73,  875,    9,  143, 3708,    6,    0,   24,   73,  204,
          229,    7,  126, 2446,  276, 3028,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 875,   54,   94,  970,   10,  970,   71,  108, 3648,   26,   13,  207,
           12,  875,   54,  134,  970,   10,  970,   71,   94, 1004,  183,    0,
         1004,  672,    0, 4355,  931,  659,   66,  226,  133,  341,   10,  108,
          447,    0,   67,  148,    0,  100,  170,    0,  144, 1080,    6,    8,
         4989,    0, 2084,  335,   18, 1991,    6,    8, 3128, 2261,    9,  159,
          931,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([ 64,  65, 130,  82,  92,  63], device='cuda:3') tensor([1.0000, 1.0000, 0.8271, 1.0000, 1.0000, 1.0000], device='cuda:3',
       dtype=torch.float16)
tensor([[  67,   25,  296,  ...,    1,    1,    1],
        [  84,   11,    6,  ...,    1,    1,    1],
        [  29,   70,  321,  ...,    1,    1,    1],
        ...,
        [  29,   24,   73,  ...,    1,    1,    1],
        [ 229, 1118,   63,  ...,    1,    1,    1],
        [ 103,   25,   11,  ...,    1,    1,    1]], device='cuda:3') tensor([ 9,  9, 13, 11, 14, 10, 14, 16, 15, 10, 13, 13,  9, 18, 11, 17, 20, 14,
        17, 14, 13, 11,  7, 14,  7, 14, 13, 13, 16, 10, 12, 14, 13,  9, 11, 13,
        12, 14, 11, 12, 15, 10, 11, 11, 14,  9, 10, 11, 15, 13,  7, 15, 15, 13,
         8, 17, 13, 20,  7, 16, 14, 13,  7, 13], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 0.8105, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8794, 1.0000, 1.0000, 1.0000, 0.8555, 1.0000,
        1.0000, 0.8579, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8320, 1.0000,
        1.0000, 0.8984, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8652, 0.8369,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8882,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8677, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000], device='cuda:3', dtype=torch.float16)
tensor([[3929,  155,  874,  103,   25,  154,   25,  135,    0,   84,   63, 1230,
            0,   91,   26,  850, 6566,  131,    7, 2932,    8,    7,  218, 1498,
         5804,   10,  170,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,    7,  467,    0,    7,  453, 6578,  148,   34,    7, 1951,   12,
           13, 1318,    8, 5555,   55,   24, 6868,    7, 1406,    0, 2082,    8,
          858, 4007,    6,   10,  367, 4404,    8,   51,   13,  461,   12,    7,
          546,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,   21,   11,    6, 3849,    0,   21, 3560,    6,    0, 1259, 1599,
            0,    0,   21, 2293,    6, 1890,   13,  525,  287, 7195,    6,    8,
         1890, 3577,  901, 7195,    6,    0,  100,    0,   56, 2175, 1488,   35,
          868,    6,    0,  166,   26,  133,  341,  106,    0, 1120,  419,  218,
         2475,   12, 6293,    0,    2,    1,    1,    1,    1],
        [  21,  204, 1847,  214,   24,  169, 2807, 3301,   46,  214,  100,  150,
           48, 2184,    6,  156,    6,  109, 4377,   93, 2058,  424,    6,    6,
           46,    8,   73, 2575,  134,  199,   13, 4458,   18,  160, 1032, 1116,
          111,   45,   37,    0,  166,   25,   73,  774,  199, 1120,  419, 1862,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7, 1384, 1224,  368,    6,   80,  423,  759, 9162,    6,   13,
          383,    0,  166,   26,   80, 1450,  439,   12,   77,    7, 1947,  619,
         5948,    9,    7,  179,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1291,   22,  140,  416,   22,  110,  556, 1949,   11,    6,   13,  391,
           35,  511,   35,  290,   35, 6979,   35,  715,  234, 1927, 3156,   12,
         1954, 6935,    9,    7,   43,   59,  371, 1644,  827, 1151, 3144,    6,
            0,   85,   80,    3, 5494, 1360, 5997,    0,    8,   19,   11,  121,
          226, 1797,   54,   84,   55,  294,  215,    0,    2],
        [  94,  506,  204,  498, 1557,   54,  126, 8282,    6,    0,  934,    0,
            0,  109,    0,   53,  506,   86,   66,    0,   13, 4561,  583,   71,
         1230,   94,   10,    0,  456,   80,   13, 6711,  120,   53,  220,  116,
           87,   13,    0, 4454, 3836,    8,    0,  175,   21,  691,    0,   71,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   7,  853,   22,  335,   26,   17,   25,   73,  446, 2533, 5203,  336,
           21,    0,   68,  194,   65,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   19,   11,  121,  116, 3498,   25,   63, 3302,   12,    7,  133,
          946,    8, 9743, 1081,   17, 5577,    6,    8,  896,  227, 2174,  232,
          445, 6442, 1052,   62,   48,  562,    6,    8, 1122, 1079,  292, 3751,
            6,   73, 9478, 2808,   55,  159,  447, 3965,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  274, 6269,    8, 6269,  199,    7, 1710,   12, 5554,    0,    8,
            9,   17,  207,   24,  175,  914, 4616,    8, 4616,   10,   33, 2570,
         2240,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  218, 2604,    6,   12,  813,    0,   79,   25,   11,   57,  211,
         6449,    0,    0,    0, 4837,   63, 2875,   80,  860, 4825,    6,    0,
          106, 3836, 4506,    6,    0,    0, 1817, 4506,    6,    0, 1817,  837,
         5002,    8,   29, 4404,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,   77,   87,   33,   13,  277, 4838,    0,  166,    0,   26,
          347,   24,    0,   73,    0,   77,  274,  132,   85,    7,  370, 1303,
            0, 3253,    8,  150,    0,   33,    8,  113,   33,    8,  113,   33,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 225, 1639,   85,    7, 1749,  747,  670,   79,   13, 2829, 1020,    9,
            7,  384,  221,   11,    6, 2100,    0,   29,  225,  278,   10,  150,
           77,    7,  807,   17,  278, 3172,   22,  126,   12, 2092,    0,   55,
         1976, 1043,    0,  148,  162, 4111,   10,   51, 7806,   48,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 116,  336,   17,  183,  206,   19,   34,  961,   17, 7374,    0,   19,
         1260, 1004,    7, 4895,   12, 5205,    8, 4776,   84,   34,  486,  546,
           24,  162, 5054,    0,    7,   82,    9, 7723,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   34, 3642,   84,    0,  116,   13,  110,   57, 1802,   35, 1933,
           35, 1178,    0,    8,   19,  934,   48,   13,  422,  378,    9,   13,
         2414, 2196,    0,  125,   33, 5353,  220,   86,  601, 1584,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   45, 2464, 1778,    0,   68,  194,   65,  211,    0,   53,
          162,  172,  488,  184, 2148,    6,    0,    8,   53,  465,   11,   18,
          135,  261,   80,  998,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:4') tensor([29, 39, 53, 50, 30, 57, 50, 18, 47, 27, 42, 38, 48, 34, 36, 30],
       device='cuda:4') tensor([1.0000, 1.0000, 0.8652, 1.0000, 1.0000, 1.0000, 0.8394, 1.0000, 1.0000,
        1.0000, 0.8887, 0.8433, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
tensor([[3706,   55,    7,    0, 3210,   12, 2058,   35,  249,   59,  240, 1144,
            0,   53,   11,   57,  133,  946, 2787,    9, 2047,   12,    0,    0,
           73,    0,   24,    0,  694,  214,   10, 2231, 2736,  941,    0, 1645,
         2078, 2399,   70, 2892, 1075,    0,   73,   87,    8, 2293,  359, 4531,
          946, 6710,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  13,  143, 4089,  207,   10,  154,   80,   89, 6674,    6,  506,   51,
            9, 2047,   12, 1387,  687,    8, 4480,    0,  206,   85,  419, 1800,
          613,    9,  183,    0,   71,  419, 1127,  723,    0,   19,  217, 1102,
         1387,    8, 8219,  106,   17, 2216,    0,   77,   79,   13, 2443,   12,
           70,   19,  296,   10,   87,  230,  115,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   7, 7374,   17, 5290, 1247,    0,   79, 4546, 1247,    0,  220,  508,
         3667,   10,   13,  733, 6608,   12,  341, 1603, 4345,   46, 1259, 1247,
            0,  618,   59, 1247,    0, 3109,  140,   57,  411,  266,   26, 2856,
         1247,   46, 4574,   48,    7, 2303,   12,    7, 1827,    8,    7, 4715,
           12,    7, 1136,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [4157,    0, 7957, 6266, 1119, 1017,   21, 1962,    0,   85,  109, 3019,
          964, 4157,    0,    8,  180, 4587,   13, 1206,  567,   10, 1005,   21,
           84,    0, 4958,    7,   59,  627, 1202,  290, 1408,    0, 6712, 5933,
         6962,   26, 2770,   10, 5989,   85, 1362,   20,  586,  922, 4157,   71,
         3955,   45,  502, 8366,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,   34,   39,    9, 1324,    9, 1954, 6935,    0,   19, 1346,
           80,   13, 1361,   12, 8188, 4039,  148,  144, 1828,  244, 1339, 1027,
          587,  195, 3485,    0,    8,   13, 8188, 1793,  148,  692,   10,  508,
         4460,   69,   17, 3485,    0,    8,  211,  218, 3280,  692,   10,  205,
            8,  601,  267,  508, 4460,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 103,    0,   84,   11,    6,    0,    0,    0,    0,   91,  759, 4039,
           46,    8,   19,  172,  135,   33,   46,  148,   63,    0, 3163,   10,
          229,    0, 1583,  583,    6,    0,  148,   63,    0,    0, 3163,   10,
           51,   69, 3836,    0,   19,  217, 2497, 1123,   17,   24,   73,    0,
          204,  468,    7,  538,   12, 1261,    0, 2318,    0,   55,    7, 4959,
           12, 1146,    0,    2,    1,    1,    1],
        [   8,   29,   19,  246,    0,   38,    6, 1049,    3,    0,   29,   19,
            0,  175,  518,    0,    8,  225, 1559,   69,    0,    8,   71,   13,
          277,  523,   12,    7,  170, 1522,    0,    0,    0,   13,  176,    0,
            0,  180,  225, 1518,  336,    0,    8,  225, 1143,    0,   80,  423,
         2360,    0,    8,    0,    0,  225, 1518,  270,  336,    0,    8,  225,
           11,    6,   77, 6087,    6,    0,    2],
        [  85,    7,  183,   19,   34, 1931,   54,    9,  117,  341, 1427,    0,
           19,  474,   19,   34, 3258,   54, 4029, 1352,    0,   67,   69, 1858,
         2007, 1498, 1261, 9096,  237, 1181,    0,    8,   19, 4600,   19,   11,
           48,  204,  226, 3258,   54,   13, 1127,  546,   55,  143,  254,  423,
          215,    0,    8,    7, 2838,   69,  264, 1736,   34,   21,    6, 2402,
          119, 8197,  271,    0,    2,    1,    1]], device='cuda:5') tensor([52, 57, 53, 54, 55, 64, 67, 65], device='cuda:5') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 0.8584, 1.0000],
       device='cuda:5', dtype=torch.float16)
tensor([[  21,   11,    6,    0,   86,   79,  103,   19,  144,    0,  419,    0,
          479,   70,   21,  169,  274,  100,    0,    0,   67,   84,   34,   13,
          841,   17,   24,  169,   66,   10,   87,  250,   71,  264, 9520,    0,
           12, 5149,    8,  250, 1057,   10,   87,   71,    0, 1588,  468,    8,
          250, 1057,   10,   87,   71, 1136, 1370,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  21,   11,    6,  116,  226, 4174,    0,  166,  818,  251,  148,  162,
         7220,  144,   10,   66,   13,  207,   12, 8254,  839,   10, 1703,  251,
          248,    8,   13,  854,  759, 1086,    0,    8, 1065,    0,  143,   55,
            7, 6580, 8068,    0,    8,    7, 3742, 7856,  249,    0,    8,    7,
         4528,    0,    8, 3097,    7, 2248,  181, 3414,   20,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [  67,    7,  630,   13, 2003,   96,    0, 1189,    0,   29,  138,  261,
           87,   56,  234, 1265,  241, 2142,    0, 2994,  122,  241, 2142,    0,
           13,   59,  380,   18, 3110,    8, 3098,    7, 1503,   12,  886,  412,
         3276,  287,    9, 4928, 1703,   55,   17, 7752,    9, 2446,   10,   17,
         2340,   10,    7, 1503,   12, 7121,  484,    0,  166, 2231,    6,   17,
         3833, 1548,    0,    2,    1,    1],
        [  21,   11,    6,  434, 3919, 8570,    0,    8,   21, 6430,    6,    0,
         2584,  218,  214,    0, 7753, 2736,  296, 1212,    8,  227,   93, 1069,
           96,    0, 2797,   54, 1868,  187, 3635, 1319, 3562, 7289,    8,  218,
         2808,   35, 5444, 2840,    6,   10, 3814, 3001, 3188, 1068,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19,  258,  636,  132,    9, 5950,   20, 1006,    0,    8,    7, 3564,
          162,  172, 3633,   17,    7, 5317,  144,  226,   13, 2786,    0,  125,
           85,   17, 2110,    0,   19,  144,   13,  277,  523,   12, 2080,    9,
           91,   12,   89,  488,   10,   96,    0,    8,   19,  474,    0,   38,
          122,   57,  411,    0,  125,   19,   11,   45,  142,   10,    7, 8022,
            6,    3,   68,  194,   65,    2],
        [   8,  115,  339,  110,  873,  908, 2955,  187,  445,  128,    0,    8,
          289,   17, 4586,  931,   46,  166,  185,   12,    7, 4793,   24,  175,
          568,   69,   33, 4959,   46,  120,   25, 2554,    7,  282,   12, 2307,
            0,   13, 2980,   37,    0,   13, 3700,    0,   13, 1276,    0,   53,
           63,  521, 1015, 3127,   54, 7553,  111,  199,    7, 2340,    0,    2,
            1,    1,    1,    1,    1,    1],
        [   0,   19,  113,  213,   10, 2678,   33,   91,  132,   13,    0,    0,
          523,    0,  138,   73,    0,   53, 4477,    0,   19,   11,   45,    0,
          142,   10,  388,  134,    0,   69,   13,  824,  586,  457, 5445,    0,
            0,  166,    0,   26,    7,   38,  122,  241, 1515,   10, 4853, 1435,
           85, 8251, 1243,    3, 1189,    0,   29,  168,   21,   26,    0,    2,
            1,    1,    1,    1,    1,    1],
        [1220,    9, 3023,    0, 2058,  181,  541, 1775,  271, 4814,    6,   66,
          226,  619,   10, 1754, 7371,  955, 2028,    0,  100, 4896,    6,    0,
         2678,  777,  812,    6,    0, 3183,    0, 2011, 6504,    6,    8, 4222,
            0,   17,   66,  226, 3150,  111, 7225,   62,  199, 2116,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:5') tensor([57, 59, 64, 48, 66, 60, 60, 48], device='cuda:5') tensor([0.8740, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8369, 1.0000],
       device='cuda:5', dtype=torch.float16)
tensor([[ 347,  568,   21,  988,    0,  148, 1006,    6,    0,   26,   33,  116,
           13, 1678,   17, 9004,    6,    8, 6904,    6,  722,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  13, 4746,   35,  803,   18,  249,   26,   13,  382,  783, 1522,  723,
            0,    8,   24, 5874,    8,  749,    6,  174,  589, 6010,  722,   55,
            7,  218, 1543,    0,    2,    1,    1,    1,    1,    1,    1],
        [  70,   33, 3565,  170,   26,    0,   33,  164,   15, 4482,  181, 2958,
            0,   21,   11,    6,  113, 2092,  365,    8,  883,  140,  365,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57, 1211,   17,   17,   11,    6,  204, 1031,  156,   62,
          199,  281,   94,   11,    6, 2702,   32,   54,   85,  185, 1648,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  103,   17,   11,    6,   86,  890,    0,   53,   11,  121, 2107,
           69,   10,  289,    0,  281,  490,   53, 4853,  747,  670,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1325,   53, 5415,   33, 1165,    0,   24,   11,  158,  492, 6290,  565,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  10, 1141,   17,  630,    0,   19,   11,   45,  142,   10,   66,   10,
          884,  486,   91,    0,  166,   26,    0,   70, 1148,  120, 6249, 8221,
            6,  415,  158, 1515,    0,    2,    1,    1,    1,    1,    1],
        [ 115,    0,  138,  323,   19,  367,   10,   33, 1760, 3140,   12,   13,
           48, 1184,  140, 3085,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7, 4268,   12,    7,  744, 6213,   55,   33, 1599,   34,   17,
           21,  220,   51,  619,   69, 8786,    6,    8,   91,   35, 1933,   35,
         1178,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [1008,   46,   25,   11,   57,    7, 3698,    0,    0,    0,  125,   25,
          618,  155, 1037,  282,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,   13, 2198,  614, 1810,   34,    7, 4023, 2922, 2371,   56, 4654,
         2041,    9,    7, 1261,   12,    7, 1384, 1224,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  26,   21,  148, 1073,    7,  281,  839,  109,  148,   26,  281, 8064,
           10,  267, 3557,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   73,  876,  121,   10, 2697, 2360,    0,   10,  206, 7616,  568,
           86, 2705,  307, 1906,    0,    8,   26,  417, 1841, 1059,   62,   71,
         1894,  688,    6,    0,    2,    1,    1,    1,    1,    1,    1],
        [2018,  527,   93,   35, 2287,  234,  439,   12,    7,  179,   11,    6,
         1682,  931,    9,    0,  108,    0, 1884,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29, 4479, 1339,  783,  511,   37, 1405,   13,    0,  682,  247,   17,
          101,  434,    0,   38, 1879,    0, 2102,    3,  166,   26, 1223,    0,
         8044,   55, 3708,    6,    0,    2,    1,    1,    1,    1,    1],
        [  19,  321,   12,  100, 1060,  117, 1532,    8, 2346,   18, 1011,   71,
          565,  359,   13, 2555,  237,  866,    9,  321,   12,   13,  854, 4656,
            6,  737,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  24,   66,  255,  200,   22, 6339,   54, 2200, 2362,    0,    8,   24,
           66,  244,  168,  264, 8973,    6,   69,   13, 4481, 3489,   10, 1764,
          387,  287,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [2805, 2219,   69,   70,   91, 1985,  150,    0,    7, 2805,   12,   17,
          107,   20,   15,    8,  291,  156, 3837,  457,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  103,   17,   11,    6,   86, 4949,  890,    0,   25,   73, 5652,
           91,    8,   25,  175,   13, 2685, 8831,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 106,   17,  552,   13,    0,  325,   12, 5828,    6,    0,  109,  126,
            6, 2893,    6,    0,    0,   79,   19,  434,  134,    0,    0,  214,
           17,   19, 1385, 1313,    0,    0, 6263,   89, 6878,    0,    2],
        [  19,  154,  245,   21,   11,    6,  528,   55,  170,   10, 2613,   70,
           21,   26,   10,   51, 2639,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [3695,   12,  134,   63, 1387,   10,  206,    7, 4458,  362,    6,   63,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  744,   26,    9,    6,  234,   54,   17,   84,   26, 8902, 1666,
         2214,   10,  415,  500, 1783,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0, 6482,  215,  581,    0,  903,    0,  846,  315,   22,   18,
          380, 3089,   35, 3304,   62,  284, 3780,   11,    6,  874,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([23, 29, 25, 25, 24, 14, 30, 18, 28, 18, 22, 17, 29, 21, 30, 28, 28, 22,
        21, 35, 19, 14, 19, 24], device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8584, 1.0000, 1.0000, 1.0000, 0.8706, 0.8208, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8242, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0',
       dtype=torch.float16)
2023-08-15 15:10:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
tensor([[  24,   66,   13, 7524, 4040,    9,  166,   24, 1393,   10,   91,  723,
           10, 8714,   39,  467,  982, 2884,   12, 1833,    0,   10,   51,   89,
         4023,  570,   59,    0,   89,  772, 1751,    0,    7,  772, 5370,    0,
           89, 2444,   62,  521, 1105,   48,  480,    0,   89, 2294, 1661,  221,
          369,    0,   89, 5930, 4888,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,   70, 1320,  335,  684, 4600,    0,    8,   25, 1510,   21,    9,
            7,  227,   93,  362,  715, 2378,    6,   12,  632,  237,    6,   15,
          593,    0,   34,   17,    7,  535,    0, 3139,    0,  227,  233, 2341,
         4267,    6,   12,  574, 2256,   12,    7,  564,  322, 1910,  162, 1028,
           10,   13, 1387,    0,    8,   17,   24,  162,   80,   10,  954,  199,
           91,   12,  251, 9200, 2760,    6,   12, 1261,  120,  768, 2317,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  19, 7750,   25,   10, 1064,    0,   55,    7,  245,  183, 3983,    9,
            7,    0,  179,    0,  168,   69,    7, 1366, 2110,    0,   13,  475,
           35, 5203,    0, 2178, 2445,  266,    0, 6580, 1411,  271,    0,  629,
          110,    0,    8,   89, 1751,    0,  903,    0,    0,  728,  649,   57,
           93, 2734, 2003,    0,    0,  106,    0, 7002,   11,    6,  728,   18,
         3411,  300,    6,  369, 7312,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  17,   26,  120,   13,  150,   48,   26, 6788,   62,    9,   13, 7312,
           10,   87,  250,   86,    9,  191,   48,   62,  131, 1377,   46,  100,
         1520,    7, 2192,   12,   13, 1390,    8, 2985,   21,  199,    7, 2192,
           12,   13,   10,  424,  412,    0, 1995, 2083,    0,  192,   11,   18,
          175,  110, 1226,    0,   19,  100, 1390,    8,   10,  424,  412,   96,
            0,   67,   33,   26,  116,  775,   20, 1718,   93,    0,   68,  194,
           65,    7,  150,   48,    6,   63,  180, 2685,   62,    0,  180, 5847,
            0,    2],
        [   7, 2193, 1793, 2493, 7012, 1198,    6, 1361,    0,   13, 1361,   12,
           94,  148,    0,   69,   13, 1284,  383, 1663,    0, 3284,  199,   13,
         3508,  982,  964,    0,   85,  159, 3506,  160,  128,    6, 3696, 3836,
            8, 2187,    6,    9, 2828,   62, 7689, 1974,   22,   18,    0,  100,
           33,    0,    8,   53,  456,   80, 5002,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,   29,  115,    7, 2932, 1381,    0,  148,   55,   13,   87,  610,
          215,    0,  188,  923, 3150,  111, 6226,   62,    7, 2688,    0,   26,
          115,    9,    7, 3140,   12, 1057,   10, 4007, 1179,   10, 2156,  109,
         6406,  346, 1711, 4166,    0,  125,    7, 6105,   10,   13, 3147,  234,
         1827,   26,   29, 4126,   17,   53,   73,   11,   18, 1799,   71,   21,
          419,  218,  207,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   7, 1232, 5917,    6,  427,  362, 1011,  131,    7, 3022,   46,    8,
           19,   11,  121, 2138,  185,  798,  215, 2115,   54,  336,    7, 4959,
          401, 1221,   69, 8633, 1232,    6,    0,    8,   66, 3395,   62, 3585,
         1118,    9,  392,   10,  798, 1075,    9,   33, 4959,    0, 2386,   12,
         3585, 1118,   46,  752,   10,  661,   70,  513, 1226,   71,  108, 1232,
         5917,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:4') tensor([55, 73, 67, 86, 57, 65, 63], device='cuda:4') tensor([1.0000, 1.0000, 0.8735, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
tensor([[ 800,   91,    0,  103,  886,  424, 3848,   11,    6,  853,  200,  237,
           26,  593, 3401,  120, 3213,  187, 2044,    6, 1570,    6,  199,    7,
          964,    0,  568,   17,  641,   21,   11,    6,  735,  226,  593, 3401,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  24, 8396, 1469,  106,    0,  108, 2914,   37,   93,    0,  131, 2532,
          336, 3494,    0,    0,   67, 1223,   24,   77,   66,   33, 1738,    0,
         2914,   37,   93,   17,   24,   63,  746,    0,   12,    0, 5472, 1117,
          108, 3183,    6,    8,    0, 1459,  994,   26,  126,   84,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [2937,  982,  218,  943,    6,    9,  108, 6249,  451,   66,  774,   62,
         3928,    0,    8, 1800,  282,   13, 2333,   10,  175,  850, 1408, 1072,
            6,    0,  109, 3119,  294, 2518,   12,  215, 3928,  254,  956,   69,
          984,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,  125,   29,  294,   12,    7, 2787,    9, 1370, 2924,   11,   18,
          116,    9,  670,    0,   53,   11,   57,    9, 1037,    8, 1237,    0,
           70,   25,  113,  296,    0, 6957,    0,   26,  143,   69,    7,  230,
          874,  926,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 115,    0,  103,   25, 1816,   66,   13, 5590,   20,  521,   20,  874,
           93,    0,  115,   11,    6,   13,  324,  183,   10,  388,   21,    9,
          155,  874,    8,  598,   21,    0, 2603,   69,    7,  467,    0,   17,
           11,    6,  916,    0,   25,   11,  158,  446,  126,  347,   21,   11,
            6,  434,   13,  862,  365,  232, 1218,   20, 5590,   20,    0,    2,
            1,    1],
        [ 248,  215,  581,   85, 1366,    0,   19,  154,   46,   19,   11,  121,
          367,   10,   33, 7034,   46,   19,  154,   19,  659,   66,  226, 5187,
          106,   13, 3643,  384, 7710,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,   11,    6,    0,    0,    0,    0,    9,    0,   89, 6525,  230,
          115,    0,   38,  187,  144,   33, 1850, 2536,   48,    0, 1850,   62,
           69,    0,    7, 1850,    6,   20, 2286,  307,    0,    0, 4813,  248,
          215,  581,   69, 2154, 1374,   18,  881,   11,    6,  383,    0,   21,
           34,    7,  133, 2354,    0,    7,  473, 2829,    9,    7,  535, 8311,
            0,    2],
        [  29,  115,    0,  103,   24,  274,  270,   85,    7, 3253,    0,   85,
          108, 8196,    6,   17,  162, 5690,   54, 1004,    7, 3253,    0,  117,
         8196,    6,   63,  100,    7,  384,  174, 2003,   12,  108, 2626,  567,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:2') tensor([38, 48, 39, 40, 60, 31, 62, 38], device='cuda:2') tensor([1.0000, 0.8271, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 1.0000],
       device='cuda:2', dtype=torch.float16)
tensor([[ 225,    0,  509,  ...,    1,    1,    1],
        [ 101,  876,  140,  ...,    1,    1,    1],
        [1979,  265,    0,  ...,    1,    1,    1],
        ...,
        [ 109,   63,   24,  ...,    1,    1,    1],
        [ 117,   63, 4700,  ...,    1,    1,    1],
        [   8,   33,  321,  ...,    1,    1,    1]], device='cuda:2') tensor([14, 12, 21, 10, 15, 16, 11, 12, 18, 11, 13, 17, 11, 18,  9, 13, 12, 24,
        19, 14, 20, 21, 20, 19, 17, 18, 19, 24, 10, 15, 10, 13, 18, 14, 10, 15,
        17, 17, 19, 22, 21, 17, 19, 18, 20, 14, 12, 12], device='cuda:2') tensor([1.0000, 1.0000, 0.8433, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8740, 0.8652, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8677, 1.0000, 1.0000, 1.0000, 0.8979, 1.0000, 0.8882, 1.0000, 0.8799,
        1.0000, 1.0000, 1.0000], device='cuda:2', dtype=torch.float16)
tensor([[   7,  245,   91,   26,    0,   25,  305,   17,  524, 4474,  199,   21,
            6,  523,    6,    8, 3675,   29,   17,   25,   73, 5382, 7394,  251,
          523,    6,    8, 3675,    0,    8,  180,   12,  538,   25,   87,    7,
          744,  461,    0,   25,  388,   77,   12,  117,  523,    6,    8, 3675,
          270,  540,  554,   10,  367,   10,  155, 7034,    0,    2,    1,    1],
        [  24,  169,  492,   87,  860,   13, 3310,  279,    0,    0,   67,   46,
            9,  419,    0, 1165,    0,   24,    0,  591,  103,   24,  116, 1223,
         2396,    0,  134,    0,   77,  346,    8, 1296,   62,    0,  134,    0,
            0,   17,  281,    0,   94,  169,  204, 2990,   70,    7, 1296,   54,
          451,   51,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7, 1141,   26, 1111, 6504,    6,    0,    8, 1008,    0,  423,
           12,  117, 3049, 4515,    6,  162, 1028, 1004,    7,   13, 1966,  407,
          816,  333, 1127, 1303,    0,   29,   70,  162,  117, 1816,  401,   71,
           77,    7,  839,   53,  162,  961,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [7616,    8,  415,  584,   26,   91, 4401,    0,    0,    0,   68,  386,
           65,   67,    9,  108, 5605,  336,    7,  179,    0,   24,   66,   77,
         1587,   12,  218, 4401,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,  278,   13, 1335, 1793, 7470,   97, 1411,   37, 2091,  890,
           10, 1557,   13, 1543,   12, 9829,    6,   10, 3022, 1146,   71,  170,
           10, 3522,    7, 4749, 5242,   11,    6, 1051, 2256,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   21,   11,    6,  321,   12,  100,    0,   25, 1057,   13, 1329,
            9, 7419,    0,    8,  180,   25,  175, 6899, 1314,    0,   10,    0,
          574,    0, 9784,    0,    0,    0,    8,  155,  886,  153,   26,   86,
         1767,   80,   33,  125,   25,   11,  121,  278,   10, 1703, 1404, 1610,
           57,    0,    0,  125,   25,   11,   57,    0, 2107, 1978,    0,    2],
        [  29, 8093, 8187,    6, 1452,  736,  439,   12,  159, 1422,    8,  736,
          439,   12,  159, 3857,    0, 6872, 1752,   35,  494,   15,   18, 1568,
         8187,    6, 1452,  736,  439,   12,  159, 1422,    0,   67,  116,  100,
          419, 3460,    8, 4197,    0, 1452,  288,  655,  439,   12,  159, 3857,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53, 1918,   10, 2554,  392,  759, 1086,    0,   29,    0,  100,   29,
          294, 1881,  440,    0,    7, 3516,  278,  540,    8, 6732,   62, 5725,
         2502,    6,    0,    8,  853,  174, 8227,   48,    0,   25,  150,    0,
          853,  174,  689,   11,   18,  703,    9,  906, 2937,    6,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:2') tensor([58, 52, 44, 31, 35, 60, 50, 48], device='cuda:2') tensor([1.0000, 0.8086, 1.0000, 1.0000, 1.0000, 0.8706, 1.0000, 1.0000],
       device='cuda:2', dtype=torch.float16)
tensor([[1189,    0,    0,   29,  339,   11,    6,    0,  289,   21,   11,    6,
         1491,    0,    0,    8,  339,   11,    6,  289,   21,   11,    6,  384,
          372, 1761,   20,    0,    8,  115,    0,   25,   73,    0, 1948,    0,
          205, 1817,    8,  446, 1835,   13, 1484, 3762, 2293,   59,    0,    2,
            1],
        [   0,   67,   25,  135,    0,   19,  154,   25,   63, 4130,   71, 2084,
         1625,  128,    6,    0,   46,    7,   81, 2510, 3496,   18, 9875,    6,
            0,    8,   77,  117,  214,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  91,  383,    0,   19, 7262,   62,  126,   12,    7, 2657,    9, 3918,
         1118,  432,   13, 3040,   71,   13, 3532,   91,    0,    8,   19, 5429,
           62,  199,   13,  682,  181, 3435,    0,  206,   19, 1060,    7, 2859,
           57,    6,    6,   55,   13, 2705,    0,    2,    1,    1,    1,    1,
            1],
        [   8,  120,   19,  289,  923, 1738,    0,   21,  220,  499,   51,   29,
         4733,   17,  211,   91, 3485,   12,  282,  700, 6317,    6,  486,    0,
          166,   26,   13, 3462,  474,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,  113,    7,  473,  203, 2470,  247,   55,    7, 3299,  267,   48,
           12, 2533,  382,  399,    0,    8,  113,    0,   12,  538,    0, 3547,
         6952,   55,  486,  733, 3609,   12,  218, 1369,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  94,   63,   29, 4493,   12, 6608,   17,   53,  780,    8, 2509, 1459,
            0,  276,   94,  148,  192,   11,   18,  213,   10,  109,   73,   11,
           18,    0,   10,  873, 1417,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 432,   77,    0,  305,   13,  274,   85,  117, 2792,    6,    0,   97,
          737, 1010, 1144,    8,  415, 2633, 2041,    0,  179,   82,    6,    0,
         6229, 1106,    0, 7050,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   7,  240, 2120,   66, 2138, 4021,    0,  752,   10,    0,  661,  347,
           21,  188,    0,    0,   33,  133, 2035, 1236,  297,  293,  111, 3495,
           35,   18,  500,   62,  800,    0,    8,    0,   53,   11,  121,  367,
          132,   71,    0,    0,   13,  800,   12, 1254,    0, 7052,    6,    0,
            2],
        [ 125, 2392,   37,  109, 1065,    0,   24,   11,  158,   51, 7258,   62,
           71, 4012,   80,   33,    0,    8,   21,   11,    6,  509,  103,   24,
          154,  835,   80,   21,    0,  276,  103,   24,  213,   10,  154,  835,
           80, 2826,  347,   24,  451,  492,   87,   21,    0,    2,    1,    1,
            1],
        [   8,  168,   21,   26,   55,    7,  245,  183,  291,  121,  233,   62,
           85, 1366,   46,  108,  245, 4878, 1806,  469,  816,   24,  293,  457,
            0, 6712, 5831,    6, 1103, 3007,   54,   71,  282, 1117,   13,   24,
          293,  457, 7757,   54,    0,    2,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   29,   70,   24,  487,   71,   34,  172,   13,  264,  207,   12,
          826,   80, 1009,   39,    9, 3562,    0,  359,   13, 1039,  434,  229,
         1261,    0,  166,   24, 6532,    9, 5855,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 101,   34, 1061,    0,  225,   34,  798,    0,  100,   13,  325,   12,
            7, 1323,    6,   24, 1618,    0,   53, 3994,   11,   18, 2603,   56,
         7038, 1568,   80,  159, 1666, 4388,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 391,  215,  581,   19,   34, 1074,    9,   13, 5200,    9,   13,   56,
         2561,   35,   45,  973, 2102,   54,  325,    0,    8,  440,   19,   11,
           45, 4379,   85, 1366,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,  225, 3673,   62, 1406,  290,  125,  225,  692,   10,  229, 1123,
          225, 3673,   62,  250,  225, 1425,  267,  708,  169, 2034,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   19, 1425,   17,   85,   17,  765,   17,   70,    0,   19,  451,
          508,    0,    0,   10,   89,   94,   26, 1370,    8,  958,    0,    0,
            8,   17,   11,    6,    0,   70,   19,  513,  432,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,   70,   19,   11,   45, 2659,   55,   26,    7, 5708,   12,   13,
          264, 1329, 6813,   46,   19,   11,  158,  367,   10,   33,   13,  277,
         1065,   46,    8, 1645,    7, 4936,   12,   13,  264, 1455,  199,    7,
         2781, 1234,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:0') tensor([48, 31, 44, 31, 34, 31, 30, 49, 46, 42, 33, 32, 30, 24, 35, 40],
       device='cuda:0') tensor([0.8550, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8433, 1.0000],
       device='cuda:0', dtype=torch.float16)
2023-08-15 15:11:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
tensor([[  25,  135,    0,   25,   11,   57, 2465,    0,   25,   66,   77,  117,
          214,  142,   55,   25,    0,   38,  511,   19,   11,   45,  100,    3,
          125,   19,   11,   45,  168,    8,   21,   11,    6, 3939,    0,    8,
           25,  135,    0,   19,  321,   12,  100, 2180,   18,   18,    6, 3884,
          176,    0,   38, 3794,    0,   17,   11,    6,    7, 9132,  608, 1043,
           19,   11,  121,  700, 1346,   55, 4432,   54,   10, 2240,  670,    0,
            2],
        [  29,  339,   11,    6,  498,  518,  131,  384, 2172,   54,  185, 2047,
            0,   70,   26, 7898,    0, 7898,   26,    7, 6408,   24, 1064,  120,
           24,  154,   17,  108, 2082, 2196,  220,   51,  509,  109, 8661,  103,
           24,  144,  691,  250,  341,    9,    7, 1406,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 103,   53,  169,    0, 1744,  116,   13,  277,  908,  183,    0,    0,
           69,  752,   10, 2509,  159,  207, 3298, 1573,    0,    8,   13,  277,
          523,  143,    0,    0,   69, 1880, 5917,   85,  637,    0,   53,  506,
            0,  508,  159, 3226,   13,  509, 2333,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  24,   66,   10,  229, 1123,   53,   11,   57, 3163,   10,   51,    7,
         3348,   12,   33,  453,  753,   12,  108,    6,    0,   13,  753,   17,
           26,  100,  211,  218,    0,   13,  753,   17,  217, 6167,   96,  110,
          333, 1127,  383,    0,   13,  753,   17,   11,    6, 2084, 1625, 1563,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [8084, 3531,  170,   10, 1103,  297,  362,    6,   20,  199,   13,  858,
           12,   70, 6204, 3071,  506,  274,  100,    9,   13, 5591,   35, 1218,
          375,  140, 1011,  179,  206,   94,   66, 2214,   10, 3901, 2465, 5259,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  70,  220,   51,  143, 9649,  254,   10,   51,   56,   15,  121, 3576,
           62,    9, 6462,    0,   10,   51,    7,  473,   12,  155,   94,   10,
         2056,  155, 1234,    0,   10,   66,  211,  207,   10, 2423,   69,    7,
         6266,   12,    7,   39,  430,  119, 1215,  109, 9237,    7, 4893,   12,
            7,  708,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    7, 5873, 5318,   48,   17,  244,    7, 1830,  215,   53,  162,
         2839,   54, 1723,    6,    0, 1061,  584,    3, 4039,   14,   48,  909,
         2965, 1049,  111,    0,   86,  106, 3611,    0,   67,  106,    7, 4699,
           17, 3611,   26,  999,   55,   25,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    0, 5385,   71,    7, 2184,   59, 1775,  221,   20,    0,   71,
           33,   38, 3794,    3,    7, 4503,  465,   11,   18,   66,    7, 8511,
           12,  129, 2286, 1331,   54, 2184,   59, 1775,  221,   20, 4821,    0,
           38, 3794,  197,    9,   33,  841,   26,   13,  909, 4674,    0,    8,
           13,  909, 4674, 1847,   39, 2987,    0,  166,   26,   13,  211,  500,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:2') tensor([73, 46, 45, 50, 38, 52, 44, 62], device='cuda:2') tensor([1.0000, 1.0000, 0.8320, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:2', dtype=torch.float16)
tensor([[   8,   21,   11,    6,   13,   24,  237, 1355,   54, 1909,    0,   19,
          154,    0,    9, 8041,   12, 4972,    0, 3982,  366,   12, 4972,    0,
            8,   19,  154,   33, 5510,   12, 1102,  639,    8,  998,    9,    7,
         1136, 5055,   26,   39, 1909,  206,    7,  832,    0,    6,    0,   73,
          172,  305,   13, 5247, 2883,    0,    8, 7419,   26,   91,  663,    0,
            2,    1,    1,    1,    1,    1],
        [  29,  168,   25,   66,   13,  567,    0,   25,   66,  250,   46,    8,
           84, 1631,  227, 8263,   12,   17,  993,    9, 3804,   46,  185, 8829,
         2385,   39, 4312,   22,    0,    8,    7, 4312,   22, 3060,   96,   10,
         3793,    0,    8,  288,  120,   13, 6737,  925, 1585,   17,  188,    7,
          230, 1051,  174, 1991,  568,    7, 5250, 1085,    0, 1239,    0,    2,
            1,    1,    1,    1,    1,    1],
        [  84,  188,  226,   39, 2244,   48, 2080,  199, 2627,  106, 1146,    0,
          106,    0, 5036,    0,   67, 5830,    0, 2288,    0,    8,   77,   12,
           13, 5827,   24,  144,   33,    0, 3024, 2244,    9,    7,  245, 1345,
           12,   33,    0,  464,    0,  347,    0,   19,  154,   84,    0,   63,
          391, 2826,    0,  248,  535,   35, 4935,    0,    0,    0, 1649,    8,
            7, 7678,    0,    2,    1,    1],
        [  19,  246,    0,   38,  187,   11,  158,   87,   21,  106,  384,  237,
          803,    3,   53,  246,    0,   38,  469,   57,   11,    6,  211,  207,
           25,   11,   57,  142,   10, 6785,   13,  759, 6052,   35, 3481,  176,
           12, 2811,  839, 2202,    9,  384,  237,  803,    3,   29,    9, 6769,
            0,   19, 5016, 1222,   13, 6478,  244,  699,  411,    8, 3122,   10,
          264, 1027,  119,  232,    0,    2],
        [ 101,  246,    0,   38, 3127,   19,  692,   10,   51,   13, 2182,  684,
            3,    8,  101,  246,    0,   38,  200,  969,   19,  278,   10,    7,
         8459,  464,   12,  670,    0,   89, 3540,  465,   11,   18,  305,   21,
         6064,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19, 1060,  267,   80,  267, 3013,    0,    8,  225,  246,    0,   38,
         1969,  135,    0,  635,   19,   11,  158,  618,    0,  635,   19,   11,
          158,   14,    0,   67,   19,  192,   11,   18,   66,   13, 3013,    3,
          225,   34, 1922,  200,   54,   51, 1795, 2073,   18,    0,  166, 6121,
          267, 7150,  244,    7,  215,   10, 1393,  133, 1665,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [ 476,   25,    0,   68,  386,    0,   65,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 101,  591,   17,   53,  162,   56, 8862,   10,  384, 1050,  140,   54,
            7, 7522,  469,   18, 1568,    0,   10, 9547,  945,   80,   70,  506,
           51,    0,    8,  101,  591, 1730,   17,   53,  465,   11,   18, 1799,
          238,   71, 6765, 1955,  109,  802, 6027,   69,  251, 6765, 1955,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:6') tensor([61, 60, 64, 66, 39, 59,  8, 49], device='cuda:6') tensor([1.0000, 1.0000, 0.8530, 1.0000, 1.0000, 1.0000, 0.8110, 1.0000],
       device='cuda:6', dtype=torch.float16)
tensor([[ 225, 1119,    0,   38, 1461,    0,   17,   11,    6, 1284,    0,   70,
           26,   21,    0,   26,   17, 7148,  650, 3727, 1994,    3,    8, 4069,
            0,   19,   73,   51,   13,  277,  593, 4187,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21,   34,  133, 3315,    8,   19, 1385,  185, 4673,  214,    8,
          442,  185, 1968, 3051,    6,   17,   19,  213,   10, 1452,   71,   25,
          440,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 294, 3940,    0,   68,  386,   65,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,  251,  370,  215,    0,   89, 1970,    8, 3460, 1102,   14,   48,
           12, 1167,    0,    8,   19,   34,  914,  244,  221,  505, 1563,   80,
         6088,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 103,   25,  162,  461,   12,   33, 5673,    0,   70,  169,   25,  289,
            0,   63,   25,  850, 1203,  494,    0,    0,    0,  238,    0, 2859,
           46,  138,   87,   25,    0,  276,  135,    0,  125,   25,   11,   57,
            0,   86, 3531,   10,  456,   80,   21,    0,    2],
        [  29,   33,   26,   86,  116,   13,    0, 6429,    0,   80, 7291,    6,
            0,    0,   21,   11,    6,   13, 6429,   80, 3295,    9, 1146,   79,
          238,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  89, 2574,   26, 5205,    8, 4970,   63,    7,  473,  825,  120,    7,
         3022,  188, 2096,   10,   87,   21, 1574,    0,    8,   24, 2775,   11,
           18, 5768,   62,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 276,   13, 2900, 6815, 5707,   62,  392,  439,  143,    9, 7001,  254,
          225,  323,    9, 8979,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [8310,   54,   17,    0,    7, 2468,   12, 2573,    0, 1067,  525,  436,
            0,   17,   19,   34,  752,   10,  747, 2653,    0,  288,   51, 1217,
            6,    9,  248, 6874,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,    7,  207,    7,  227,   93,   45, 1604, 2334, 2895, 5070,  170,
           46,   24,   11,  121,  115,  278,   13, 1234,   10, 9375,  347,  117,
          227,   93,   45, 1604, 2334,   63, 7071,  341,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   53,  245,  487, 2980,   54, 2263, 1315,    0,   21,  220,  305,
           79,  294,   79, 1111, 6052,   12, 2533, 1390,   10,  229,   13, 1127,
         1116,  569,   12, 2263, 1315,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 1797,    6,    7,   39,   22, 1522, 3667,    8, 2322,   12, 7932,
         2673,   96,    8, 4157,    6,  142,  270,  490,    7,   69,    6,  307,
           12,    7,  473, 2217, 1137,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57,  961,  736,  439, 7692,  629, 2481, 2286,   62,  235,
            8, 5225, 5108,    8, 2288, 5225, 5108,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  388,    9, 3189,    0,    9,    7,    0, 1832, 2930,    0,   84,
            0, 7774,   62,   77, 1587,   12,  993,  106,    7, 1465,    0,   80,
         2192, 6631,  793,    0,    0,  281,   12,  166,    0,   19,  465,   11,
           18,  661,    0,    2,    1,    1,    1,    1,    1],
        [   8,   21,  875,    6,   25,  199,    7, 7771,   12,   70,   11,    6,
          226,  434, 1440,    0, 2243,   59,  827,  760,    0,  883,  424,    0,
         1579,  287,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,    0,  154,  281,   12,  134,  169,   51, 8661,    8,  958,  756,
            0,    0,    0,    8,    0,   79, 5474,    0,  294,   12,  134,  169,
          914,   51, 7078,  706,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:6') tensor([34, 27,  7, 27, 45, 27, 29, 18, 30, 34, 31, 32, 21, 40, 28, 30],
       device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8877, 0.8564, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8320, 1.0000, 0.8223],
       device='cuda:6', dtype=torch.float16)
tensor([[   8,   46, 2307, 1008,   33,  283, 7772,    0,    0,   38,  160,  119,
          480,    0, 6880,   55,  155, 5223,  197,   46, 1363, 1850,   37,    0,
            0,    9,  545,    0, 1227,    0,   33,   34,  138, 7803, 1808,    0,
            0,  192,   11,   18, 3995,    0,    0,   13, 1192,  552,  126,    0,
           21,  144,  211, 3236,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,    7, 4174, 2078,  487,    0,    7, 1762,   12, 2718, 2202, 6611,
         6671,    9,    7,  774,   12, 5428,   34,   79,  488,   79,    7, 1762,
           12, 2718, 2202,  850, 3909,  607, 4357,  407,    9,    7,  774,   12,
         1947,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   85,    7,  467,   12,   21,    0,   24, 8152,   48,   17, 2878,
           12,  708,   73, 1082,   10,  368, 2930,    8,    7, 1465,   69,  159,
          447,    0, 5333, 2515,  366,   12,  148,  109,  206,   53,  162,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   24, 1405,   13,  846, 4399, 7016,   12,   13,   56, 2515,  292,
         2404,    0,    8,   25,   73, 3684,  375,  688,  359,  341, 8829,    6,
           17, 2231,  341, 7125,    6,    0,   29,   17,   73,  601,   25, 5415,
           70,   11,    6,    9,    7,  563,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   11,    6,  916,   26,   53,  323,   21,    9, 2008, 1491,    8,
         6343,    0,  206,   21,   11,    6,   38,  626,   57, 6505,  128,   10,
          508,  561,  804,    6,    3,   53,  144,   10,  508,   21,  113,    9,
            7,  832,    0,    6,    0,   10,  175, 1719,  292, 2460,    0,   29,
           19,  154,   84,  162,  391,  832,    0,    6,    0, 2116,    9,  132,
         6034,  264, 1736,  148,  162,  461,   12,    7, 3677,    0,    2],
        [   0,   33,   26,   13, 1523, 7376,    0,    0,   21,   11,    6,   39,
          985,  687,    0,    0,   21,   11,    6,    0,   13, 4912,   12,   77,
         1587,   12,  813,    0,   86,  116,   80, 5807,    8,    0, 9827,    8,
         6689,    8,   29,   69,    0,   67,   80,  896, 2604,    6,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   21,  169,  492,   66, 7747,   10,  110,   10,  154,   17,  116,
          125,   19,  144, 1157,   13, 5462,    9,  166,   13, 2232,   34,   13,
          227,   37, 1010, 2650,   37,   17,  101,   34, 3524, 2015, 1197,   12,
           77, 4039,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  169, 1017, 8788,   55,    7, 1732,    6,    0,    8,  780,   10,
         2895,   71,  134,    9,   13,  207,   17,   34,   79,    6,  762,  366,
         1094, 3835, 1256,    0, 6510,   54,   13, 2866,  979,   12, 3802,    8,
         7376,    9,  166,   24,  220, 1082,   10,  283,  540,    8, 1766,   91,
          486,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:7') tensor([54, 39, 37, 44, 71, 48, 40, 51], device='cuda:7') tensor([0.8584, 1.0000, 1.0000, 1.0000, 1.0000, 0.8721, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
tensor([[ 24, 498,  71,  ...,   1,   1,   1],
        [ 33,  34, 245,  ...,   1,   1,   1],
        [ 87,  21, 238,  ...,   1,   1,   1],
        ...,
        [886,   0, 476,  ...,   1,   1,   1],
        [ 29,  19, 415,  ...,   1,   1,   1],
        [596,  73,  66,  ...,   1,   1,   1]], device='cuda:7') tensor([ 6, 10, 10,  7, 10,  7, 10, 11, 10, 10,  6, 11, 10, 14, 10, 12, 14, 18,
        14,  8,  8, 14, 13, 11, 11, 15, 12, 11, 10, 14, 16, 10, 13, 20,  8, 17,
         9,  9, 21, 10, 17, 10, 12, 12,  8,  8,  8, 12, 14, 11, 10, 11,  9, 10,
        11, 12,  7,  9,  8,  4, 10, 12, 10, 13,  9, 14, 11, 15, 11, 10, 12, 14,
         8, 11,  7, 12,  9, 13,  9, 14, 13,  9, 13, 12,  8, 10, 10, 13],
       device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8633, 0.8892, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8174, 1.0000, 1.0000, 0.8384, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8755, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8521, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8091, 0.8799, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8105, 1.0000, 1.0000, 1.0000, 1.0000, 0.8579,
        0.8691, 1.0000, 1.0000, 0.8516, 1.0000, 1.0000, 0.8652, 0.8286, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8789, 1.0000, 1.0000, 1.0000, 0.8564, 1.0000,
        0.8223, 1.0000, 1.0000, 0.8276, 1.0000, 1.0000, 0.8394],
       device='cuda:7', dtype=torch.float16)
tensor([[ 976, 1475,    0, 2184,  176,    0,   24,  154,   29,    0,   29,  115,
           17,   24, 2534,  117, 2522,    8,  591,  117, 3422,   17,  339,  170,
           87,  117,  214,    0,   24,  487,   10, 2252,   17,    0, 3423,    0,
          995,   17,   24,   73,   87,   71, 1780,    0,  995,   17,   24,   73,
           87,   71,   13, 1472,   12, 1780,    8,   13, 2705,   24,   73,  115,
           87,   71, 5414,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  499, 3008,  813,   69, 3634,    6,    0,   67,  115,   24,   73,
         3008,   13,  325,  143,  813,    0,  143,  254,  700,  490,    0, 2761,
           54,   21,   26, 4865,    0, 4585,   54,   21, 4865,    0, 4912,   21,
           26, 4865,    0,  979,   54,   21,   26, 4865,    0,    8,   70,   24,
           73,   87,   26,   24,   73,  203, 1792,   33,  813,   55,  368,    6,
           17,   24,  492,  276,  934,   48,  120,   24,  245, 2861,   62,    7,
          660,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21, 1518,  126,   17,   71, 1421, 1601, 2421,    6, 3195,   54,
            9,    7, 5199,    0,  432,  101,  976,  261,  442,  284, 3140, 2226,
           69,   33,   46,  101,  144,  284, 7986,    8,  101,  144,  284,   46,
          101,  258,   57,   33, 1266,   55,  908,  254,  248, 1551,    8,   34,
          529,   10, 7554, 4115,   13, 4058,    8,  278,  923, 2294,  244,    7,
          409,   17,    0, 5098,   46,  284, 5098,   46,   21,   11,    6,    7,
          245,  183,  101,   11,    6, 1831,  100,  101,   11,    6,  144,   39,
         1266,    9,  815, 1132,  215,    0,    2,    1,    1,    1],
        [ 108,  296,   55, 3051,    0,  108,  296,   55, 4029,  687,    0,  109,
          108,  296,   55, 3069,    8, 8464,    0,  109,  108,  296,   55,  540,
          687,    8,   55, 2848, 2050,   93,    0,    8,  103,   25,  154,   80,
            7,  277, 1920,  148, 2456,    6,   69,  155, 1067,  265,    8,  148,
           26,  415,  878,  111, 7307,   62,  168,    8,  133, 7759,    8, 6128,
            0,    8,   85,  185,  613,   77,   12,  170,  296,   10,  205,  126,
          199,    7,  179,   10, 3231,    8,   10, 3522,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29, 2983,   84,   11,    6,   13, 3024, 7927, 4719,    0,    8,   33,
          172,  132,    6,  307,  110,    0,  125,   13, 1027,  631,  407, 3565,
           25,  432,   13, 2767, 6204,    0,   84,   11,    6, 7008,   80,   39,
         1061,   35, 1315,  322,  183, 5848,   10,   46,   24,  321,  290, 4971,
            0,  498,    7, 4971,   93,  979,    0,   67,   70,  281,   94,  192,
           11,   18, 2252,   26,   17,   69, 2313,   21, 1847, 3497,   10, 1421,
         1113,  109,  143,   55,    7,    9, 1610,   45, 1032, 1809,  424, 1420,
          233,  373,   10,  276, 1772,   10,  575,  132,    0,    2],
        [5902,  589,    6,   69,  837, 1827,  162, 6386,    0,   38,  597,  269,
          340,  407,  323,   21,    0,  347,   73,   11,   18,   24,    3,   19,
         1850,   62,   39, 2792,   69, 5063,    8,  434,   21,   38,  290, 2078,
         1525, 6664,    0, 9413,    8, 9696, 2419,    3,   19, 8063,   48,   13,
          630,   10,    7,  640,    3, 6463,   12,    7, 2672,   85,    7,  183,
            0,   38,  412, 2366,   26,    7, 1802,  322,   12, 8338,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  203,  779,  675,  793,   26,  142,   10, 3128, 2129,   12,  248,
         3834,    0, 2129,   24,  164, 8896,  117, 1752,   35,    6,  335,   18,
         1617,  457,  183,   35, 2470,    6,   96,    9, 1764,   20,  484,  480,
         1081,   12,  108,  447, 2573,  131, 1520,  203, 7712,  237, 2332,    0,
          109,  994,  117, 3382,    6,   63,  142,   10,  175, 7154,   48,    9,
          291, 1076,   20,  484,  480, 1081,   86,   12,  108, 2573,   46, 1187,
          111,    0,  131,   82,    0, 1599,  109, 2145,  586,  271,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([66, 75, 91, 82, 94, 72, 84], device='cuda:5') tensor([1., 1., 1., 1., 1., 1., 1.], device='cuda:5', dtype=torch.float16)
tensor([[  29,    0,   25,  ...,    1,    1,    1],
        [   8,  180,    7,  ...,    1,    1,    1],
        [   7, 4821,   10,  ...,    1,    1,    1],
        ...,
        [ 540,   53, 3122,  ...,    1,    1,    1],
        [   8,   19,   11,  ...,    1,    1,    1],
        [ 635,   24,  296,  ...,    1,    1,    1]], device='cuda:5') tensor([13, 13, 20, 13, 14, 18, 18, 10, 16, 11, 16, 15, 17, 11, 19, 12,  6, 12,
         8, 17, 12, 21, 13, 12, 15, 18, 17, 15, 19, 24, 15, 14,  9, 18, 11, 16,
        15, 14, 12, 21, 14, 15, 15, 11, 21, 14, 20, 17, 23,  7, 17, 14, 12, 14,
        15, 13], device='cuda:5') tensor([1.0000, 1.0000, 0.8794, 1.0000, 1.0000, 1.0000, 0.8799, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8735, 1.0000, 1.0000, 0.8789, 1.0000, 1.0000, 0.8286,
        1.0000, 0.8652, 1.0000, 1.0000, 0.8691, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8677, 1.0000, 0.8550, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8882, 1.0000, 1.0000, 1.0000, 0.8105, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8320, 1.0000, 1.0000, 1.0000, 0.8521, 1.0000,
        1.0000, 1.0000], device='cuda:5', dtype=torch.float16)
tensor([[   9,   33, 1760, 1165,    0,    0,   24,  162,  802,   13, 2270,   45,
            0,  109,    0,    0,  339,  110,  116,  150,  103,    0,   19,   73,
          175,   33, 4481,    0,    0,   13, 9238,    0,  217, 1076, 5997,   56,
         8368,  922,    0,    0, 2806,  240, 2175,  249,    0,    2],
        [   7,  225,  158,   12,    7, 1052,  249,  781, 1158,   34,  561,   48,
          199, 1580, 6032,   85,   13, 2290,   17,   24,   11,   57, 2438,   54,
          131,    7,  467,   12,   33, 1910,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    9,  251, 1113,    0,   19, 1608,   11,   18,  283,   69,    7,
         6389, 6035,    0,   19, 1608,   11,   18, 1927,   13, 4446,  109, 2618,
           39, 5651,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  144,    7, 1666,  941,   35, 9748,  688, 9372,    6,   17,  842,
          785, 1601,   10,   82,   45,  132,    8,  180,   25,  162,  859,  665,
           13,  321,   12, 4594,  111, 1650,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,   77,   12,   13, 5827,    0,  168,   19,  217,    0, 6815,
          241,  829,  106,  747,  670,    9,  564,  957, 1132,    0,    8,   33,
         1148,    0,    8,   24, 2252,   17,   38,  290, 1408,  197,  513,  755,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7, 1043,   19,   11,   45,   29, 7105,   80,  467,   54, 3753, 3023,
         1382,   11,   18,  116,  125,   21,   11,    6,  999, 3023,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,  154,   80,   21,   79,   13, 1780,   35,   18, 1397,    0,
         1523, 1909,   12,  294, 2386,   12, 1655,   12, 2602,    0,  166,   26,
         2546,  829,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 1831,   17,  103,   19,  220, 3665,  929, 7897,    0,   19,  220,
         4075,   17,    0,  432,   77,    0,   19, 1359,   11,   18,  172, 2702,
          111,   19,  158,    0,   21,   34,  185, 4416, 4113,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    9,   33, 1165,    0,   21,   11,    6,   39, 1663, 1039,  369,
           55, 2327, 1034,  240,   20,   69,  117, 4601,  825, 3275,   35, 3794,
         2885,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   56, 3168, 2445,   93,   11,    6,   13,  133,  528,  461,   12,
            7, 9655,    8,   24, 5150,  108, 1613,   10,  274,    9, 1820,   55,
         4455,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,   69,    7,  218,    0,  874,    0,   24,   66, 1075,   17,
            0,   63,  244,  737,  891,   54,    0,   69,  837, 2694,    0, 5955,
           10,  159, 6768,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [2072,  988,   26,  250,   17,  689,   11,   18,  100,   10, 2895,  133,
          261,    0, 3706,  359, 7036,    0,    8,   12,  538,   24,  169,  100,
           10, 1082,  143,   80,   21,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,    7,  218, 2475,   12,   81,   18,  340, 8978,   26,    7,
          227,  424,    6,  560, 5824,  424, 1825, 1158,    0,    8,  117, 1816,
          985,  132,  227,  760,  233,    6,   55,   13, 1074,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1980, 1068,    0,   24,  296,   13,  574,  833,   12,  524,   35, 1105,
          505,   54,    0,   86,  116,  524,   35, 5001, 3635,  833,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   33,   34, 2861,   62,  131,   39,  786,  727,   46,  166,   26,
          100,    0, 1730,    9,    7,  467,    0,  101,  164,   86,   66,  995,
            0,  125,   21,  164,  417,  827,  603, 1906,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 138,   87,   25, 1005,    7,  909,  609,    6,  369,   12,    0,    7,
            0, 2018, 2510,  429,   12,    7, 1683,  724, 1025, 6433,   54,    7,
         8094,  429,    0,   12,    7,    0, 1478,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([46, 32, 28, 32, 38, 24, 28, 35, 28, 27, 29, 31, 35, 24, 34, 33],
       device='cuda:3') tensor([0.8242, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8530, 1.0000, 1.0000, 1.0000, 1.0000, 0.8706],
       device='cuda:3', dtype=torch.float16)
tensor([[   7, 3288,   63,  ...,    1,    1,    1],
        [  53,  113,  180,  ...,    1,    1,    1],
        [ 282,   26,   80,  ...,    1,    1,    1],
        ...,
        [4069,    0,   21,  ...,    1,    1,    1],
        [2869, 3540, 4712,  ...,    1,    1,    1],
        [  33,   26,    7,  ...,    1,    1,    1]], device='cuda:3') tensor([15, 15, 16, 13, 21, 10, 18, 12, 12, 14, 16,  9, 14, 19, 15,  8, 15, 16,
        15, 12, 10, 15, 19, 18,  8, 23, 16, 18, 13, 12, 11, 20, 19,  8, 15, 18,
        11, 23, 15, 13, 18, 15, 15, 11, 16, 13,  7, 14], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8711,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8320,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8691, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8892, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8320,
        1.0000, 1.0000, 1.0000], device='cuda:3', dtype=torch.float16)
tensor([[ 125,    9, 2483,   35,  606, 5731,    0, 2932, 6930,    6, 5989,    8,
          447,  281,   12,    7, 2483,    6,   46, 4346,  737,    0,  742,  160,
          140,    0, 3213,   46,    8,   53, 5470,    7, 2791, 2008,    8, 3156,
          199, 2248,  713, 1777,    0,    2],
        [  13, 4495, 1435,    9,    7,  832,    0,    6,    0, 2721,   17,    0,
           12, 5990, 8459, 3665, 1118,    0,  248,   35, 8516,    6,   12,    7,
         5990,  889,  144,  708,    8,  288,   91,   35, 8516,   12,    7, 5990,
          596,  144,  708,    0,    2,    1],
        [ 168,   26,   13,   29,  300,  166,  188,  278,   77,    7, 8681,   12,
         7343,    0, 3280,    0, 1976,   25,  289,    0,   19, 3250,   21,    0,
           39,   19,  362, 1838,  235, 5384,    9,   25,    8,  180,   25, 3745,
           10,   21,    0,    2,    1,    1],
        [ 822, 2317,    9,  984,   11,    6, 6339,   17, 4877,  244, 1953,    6,
           10, 2386,   12, 1655,   12,  215, 4373,    7, 6484,   12, 7616,   69,
          984,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  67,   70,  143,  220,  225,   66,  691,  103,  225, 1631, 1088,  144,
            7, 1885,  818,   12, 3802,    6, 2875,   10,  267,   10,  719,   13,
          841,   17,    7, 9413,   17,   94, 1095,  144,   10,   51, 1529,   62,
         3840, 4173,    0,    2,    1,    1],
        [  19, 1223, 2894,   55, 3101,  839,   69,    7, 2291,    0,   68,  194,
           65,    8,   19, 5429,   80,    7, 5986,    6,    0,    8,   19,   66,
          591,   13,  555,  214,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  66,   25, 1060, 9832,  347,    8,  138,   29,  294, 9696, 2419,    6,
           66, 6028,   48,  244,    7,  473,  640,  215,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [1223,   70,   21,  968,  110,   34,   46,   68,  194,   65,   68,  386,
           65,   46,    7, 1587,   12, 1075,   24, 5559,   63,  324, 1075,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([42, 41, 40, 27, 40, 30, 22, 25], device='cuda:1') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:1', dtype=torch.float16)
tensor([[  21,  914,   26,    7, 1226,  279,   10,   87,   10, 4746,    8,  368,
            7, 3939,    8,  946, 2780,    0,   38,  511,    7, 1381,    0,   53,
         1346,  267,  546,    8,    7,  218,   94,    0,    8,   53,  246,    3,
         1768,    0,   25,   11,   57,  230,    0,   24,  451,  229,   13, 2240,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   7, 2964,  169, 2154,  724, 2468, 7147,   69,    7, 2618,    8,  719,
          117, 1230,   35, 6937,   35, 7596, 1710,    6,  442,   12,   13, 1127,
         2468,    0,  736,  439,  203, 1255, 4711, 1755,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [7823,   94,   63,    0, 4820, 1256,   94,    0,    8, 4820, 1256,   94,
           46,    7,  143,    8,  143, 4820, 1256,   94,   84,   63,    0,    7,
          143,    8,  143,   24,   11,  158,    0,    0,   66,   13, 4820, 1256,
            0,  179,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  84,   34, 9199,    0,  180,  552,  853, 3174, 4796,    0,  440,   24,
           66,  211, 3174, 4796,    0,    7, 1575,   35,  372,   59,  247,  119,
         1819, 2735,    9,    7,  179,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 180,   19, 1095,  108, 2333,  158,  240, 1028,   10,  670,    9,   58,
          389,    6,    8, 2852,   59,   18, 1374, 2197,    6,    0,    8,  474,
            0,   21,   11,    6,   86,  230,   55,  110,   10, 1005,   33,  218,
          939,  668,  232,  834,   29,   19,  339,  134,  116,   46,   53,  144,
           10,   51, 1644,  411,    8, 2736,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  17,  370, 4506,   17,   24,  116,  323,   71,    7,    0, 6144,  779,
          569,  164, 2676,   66,  475,   35, 2371,    0, 1683,  979,   54,    0,
            8,    0,    0,    7, 3180,  164,  289,    0,   38,  786,    0,  346,
            0,  859,    0,    0,  230,    0,   13,  176,    0,   95,  266,    0,
           17,   11,    6,    7,    0, 2636, 4057,   10, 1557,    0,   17, 1683,
          518,   10,  155, 3280,    3,    2],
        [  19,  513,  270,   10,    7, 1224,    8,  487,  665,  336,   10,  150,
          103,   19,  220,  446, 3302,  206,    0, 7490, 1390,   54, 7521,    6,
          144,  226,    0,    0, 2517,   48,    0,    8,   21, 1932,  126,   84,
          162,    0, 3695,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  77,  230,    0,   29,  339,  110,  508,    0,   25,   39,  663,   12,
         8681,    0,   12,   13, 1760,  321,    0,    8,   19,  213,   10, 7032,
           13, 1455,   17,   19,  154,   26,    0,    0,  133, 4501,    0,  166,
           26, 8164,   54,    0,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([50, 34, 40, 31, 56, 66, 41, 42], device='cuda:1') tensor([1.0000, 1.0000, 0.8799, 1.0000, 1.0000, 0.8735, 0.8794, 0.8555],
       device='cuda:1', dtype=torch.float16)
tensor([[ 250,   17,  169,   51,  528,    0,   67,  113,  250,   17,    0,  169,
          305, 5542,   12,    0,   77,   12,  117, 7099,    6,   17,   91,    0,
            0,    0,  144,    0,   38,  511,    9,    7, 1165,   12,    7, 1827,
         2260,    0, 4619,   13,  325,   12,   94,    0, 4619,    0,   94,  148,
          162, 2129, 9829,    6,  109, 4488,   93,    0,    0,    0,    8,  113,
           86, 1057,    0,    0,    0,    9,   89,    0,    0,  447, 1165,    0,
           13, 3557,   10,    0, 4110,   80, 3903,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   19,  278, 1421, 3675,  126,   12,   21,   55,   89,  670,  125,
           19, 1831,   17,   24, 1591, 2637, 3181,    6,    8,  811,   35,  426,
          356, 5149,    8, 1127, 1848, 7178,   48,   10,  367,   10,   13,  670,
          206,   84,   34,  874, 2286, 5575,   62, 8517,   17,  877,   20,  307,
           62,  134,  333,  383,    0,  125,   21, 1017,    6,   13,   10,  375,
            8,   39, 7574,   80,  138,   25,  598,   80,   94,  535,  490,   25,
          508,  134,    7, 4340,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   0,  432,  423,  215,   12, 7989, 1435,    8,   56, 2687,  240, 1032,
         6606, 6480,    0,  131,    0, 1655,   12, 5709, 2245,    0,  333, 2767,
         2931, 3182,    9,    7,  179,  188, 8152,   48,   17,    0,    7, 6929,
            6,    0,    0, 5914,   69,    7, 1232,   63, 2426,   10, 2034,    8,
           17,    7,  979,   12, 2753, 4429,   26,  211,    0,  143, 1565,   93,
          254, 3563, 4401,    6,   12, 2753,  824,   48, 4577,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   79,   25,  150,    7, 1819,  205,    9, 6714,  111,  359,   84,
            0,    8,  180,    0,    0,    0,   68,  386,   65,    8,    7,  281,
         4673, 1809,  411,   26,   17,   89,  874,  204, 1917,   62,  499,  535,
          890,   10,   87,   17,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  33,   34,   89,  765,   10,  749,  221,    9,    0,   10, 2750, 1222,
         2433,   55,   70,   63,  172,  288,   13,  874, 1256,   12,    7,  133,
         1219, 5222, 3770, 3254,    0,    8,   19,  144,  116, 5893,   13,  488,
            0, 1061,   35, 1315,  322, 1039,   55, 2829, 1020, 1747,  160,  741,
            0, 3150,  111,    0,    8,   19, 1425,   19,  220, 6785,   13, 3028,
         1329,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  269,  290, 1543,  106,   10, 1059,  188,  691,    7,  291,   18,
         1397,  156,  457,    0,  391, 1543,    6, 4437, 2361, 1268,    0,  895,
         1101,  269,  484,    0, 1912, 2172,    0, 5087, 2172,    8, 1339, 1373,
          699,   57,   77,   85,    7,  370,  183,   46, 6835,  203,  620,  293,
            6,   62, 4437, 1395, 4781,    9,  166,   24,  205,  126,    0, 2213,
          132, 1546,  121,   22, 1691,  269,  484,    0,  388,    9,    7, 4437,
            6,   17,  204,   66,    7, 4273,    6,    0, 3020,  126,    7,  269,
          290,    8,  180,  339,  134,  205,    0,    2],
        [   7,  218, 8314,   12, 1780, 1119,    0,   38, 1865,   89,  227, 2704,
            7,  179,   34, 1621,    3,  166,   26,   10,  289,   21,   11,    6,
         1301,   17,   19,   73,   11,   18,   87,  778,    0,   67,   19,   73,
         1123,  111,   87,  250,    0,   19,   73,   55,  122,  366,    0,   19,
           73,  570,    0,   19,   73,  575,  132,    0,   19,   73, 6295,    0,
           19,   73,   51,   13,  461,   12,   33, 2469,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:4') tensor([81, 78, 71, 42, 63, 92, 70], device='cuda:4') tensor([0.8003, 1.0000, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
tensor([[ 220,   25,  367,  180,    0,   38,  235, 4639,  321,   12, 5832,   10,
           87,    0,    8,  204,    0,   17,  245, 3321,    0,   19, 1008,  826,
            3,   19,   66,   10,   51,    7,   91,   10,  884,    7,  558,  630,
            0,   38,  242, 1027, 1792,   19, 1425,   19,   34,  142,   10, 2184,
          649,    8, 3997,  649,   56, 1628,   33, 2469,    0,    2,    1,    1,
            1,    1,    1],
        [  84,   26,   39, 1248, 3046,   24,   63,  142,   69,   10,  873,  422,
          554,    0,   10,   51,  461,   12, 1237,  554,    0,   10, 1452,   12,
         1469,    0,   10,   51, 6610,    0,    8,   21,   11,    6,  133, 3315,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 101,  513,   10,  283,  333,  383,    9,   39, 6101, 6292,  100,   33,
           91,    0,  752,   10, 3067,   39,   96,  469,    6,  407,    8, 2572,
         1573,  138,   10,   87,   29,  802,   17,  370, 6629,   17, 1505,   29,
          291,   57,  297,  457,    0,    8, 7714,  107, 2398,   20,    0,    9,
          284, 2406,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 126,   12, 7680,    0,   51,  656, 6383,    6,    0,  148,   24,  144,
         1220,  226,  968,   34, 2429,    0, 3941,   45, 1755,   48,  199,    7,
         1953,   18,    0,  116,  100,   13, 1720,   45, 3328,    0,  101, 4311,
          199,    7, 1953,   18,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  84,   63,  453, 3254,    8,  453, 3557,    6,    0,    0,    8,  180,
           84,   63,    7,    0,  747,   35, 3455, 6758,    0,    0,    0,    0,
          747,   35,  119,   57,    6,    6,    0,    0, 2011,    6, 2083,   54,
            0,   29,  300,   35,   48,  608,  292,   93,   54,    0, 1587,    0,
           12, 3254,    0,    8, 6244,  111, 1155,    9,   35,  242,   18,  533,
           15,    0,    2],
        [1800,   17,   19, 1744,   89, 1113,    8, 1303,    6,   97,  652,  160,
           54,    7, 4615,   17,  220, 1085,    0,   21, 2220,   11,   18,   51,
         4822,  103,   89,  179, 6789,   34, 2264, 2304, 3328,    0,   68,  194,
           65,   21,   11,    6,   86,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  85,    7,    0,    0,  765,    0, 2627, 3188,    6,    0,   69,   97,
         1411,   54,    0, 2518,    0,   12,   10,   22,    6,   12,   29,   93,
          106, 2008, 1491,    0,  206,   21,    6, 4323, 6358,    6,   10,  913,
         6811,    0,   10, 9515,    0,   10, 9343, 4513,    0,    0,   10, 3018,
          931,   18, 1994,  168,    9, 2627,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [  67,   24,   11,   57,  168,   55,    7,  324, 1569,    0,    8,    7,
          324, 1569,   26,   17,   19,  154,   24,   73, 3522,  347,   33,  133,
          475,  422, 3916,    0,   33,  133, 7488, 2042,   12, 2070,    0,   26,
         4877, 1069,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1]], device='cuda:4') tensor([58, 38, 52, 42, 63, 43, 56, 40], device='cuda:4') tensor([[  67,   24,  192,   11,   18,  100, 1752, 7311,    6,   10,  368,  839,
           10,    0,    9, 2109,  673, 1115,   94,   10,    0,    0, 2231,  143,
            9,    0,  837,    0, 3762,    0,    2,    1,    1,    1,    1,    1],
        [ 117,   63,   13,  555,   12,    7, 5742,    6,   19, 1797,   62,   55,
          368,  120, 2346,  195,    8,   19, 2947,   62,   69,    7, 1868,   59,
         1029, 1997,  181,   57,   93, 1810,    0,    2,    1,    1,    1,    1],
        [   9,  409,    0,   91,   12,    7,    0,    0,  281, 8161, 1288,   34,
            0,   38, 6435,    0,    0,  339,   11,    6,   66,   13, 7862, 1109,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  474,   19,  169, 1157,    0, 5726,    6,   19,   66,   17, 7010,
           10,    0,    7,    0, 2860,   12,   25,  322,    8, 1137,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7, 9869,   17, 3142,  134,   77,    0,  108, 6535,  369,   10, 2718,
         2219, 3399,    6,    0,  100, 1593,  901, 5428,    8, 5222, 1947,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  71,  333, 2423,   54,  464,    0,   19, 2252,  138,  261,  143, 1894,
            7,  479,   12, 7652,  172,   26,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,    0,    9,  409,    0,    0,   21, 2947,   62,    9,   13, 1116,
           59,  679, 7778, 1227,    0,  166,   26,    7, 3715,   69,    7,  230,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53,   11,   57,   86, 1449,  816,  214,    0,   53,  468,    0,   53,
         2154,  724,    0,   53, 7831,    0,    8, 4309,    6,   87,    7,  370,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,    0,   33,  546,   12,    7, 2569,   12, 3311, 2413, 3643,
          111, 4130,    0,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 103,   25,  274, 1387,  111,    0,   25,   11,  158,  150,   84,   11,
            6,   13,  277, 1950,    8, 7956, 1390,   17, 4974,    6,  126, 3019,
          134,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,  333, 1237,    9,    7, 1384,    0,    0, 1224,    0,   25,  220,
           66,  155,  447, 1392, 1181, 1636,   12,   17,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 339,   11,    6,  274,   85,    0,  347,   53,  205,    0,   29,  238,
            0,  540,    0,    0,    7, 5462,    8,  728,  128, 1032,   93,    0,
            0,    0,  728,  128, 1032,   93,    8,  427,  335,   18,    0,    2],
        [  21,   11,    6, 5309,    8, 8509,   29,   21,   11,    6, 2426,    8,
           24,  192,   11,   18, 3188,   69, 7007,  203,  181,   59, 1159,  271,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67, 2816,    0,    7,    0, 4766, 8096,  690,  156,    6,    0, 5990,
           94,  229,  143,  839,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  296,  143, 3348,    0,    0, 3649,  188,    0,  388,   94,   69,
            7, 4241,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53,  162,   79,  324,   79,    7, 4059,    9, 1579,  187, 4534,  148,
           11,   48,  226, 3976,   55,  392,    8,   13,  854, 1345,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,  296,   13,  488, 1543,   12,  100,   35,  525,   48,   62,  807,
            0,    8,   29,   24,  774,   62,    7,  131,   20,  131,   20, 3434,
         4925,    6,  775, 2592,    0,    2,    1,    1,    1,    1,    1,    1],
        [   8,   21,   26,    7,  245,  183,   17, 1160,   45,  188, 4691, 2101,
           18,  390,   11,    6,  874,    9, 1646,  215,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   53,  498,  952,    0,    0,    8,    0,   91,   12,  134, 1119,
            0,   38, 6435,    0,    0,  188, 2881, 2096,    0,   10, 2632,   55,
           33,  279,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  79,   13, 1747,   37,  122,   93,  684,    0,   25,   73,  934,  138,
          126,   12,  561,   19,  598,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  120,   19,  513,  270,   10, 4610,  117, 1830, 3676,    6,    0,
            7, 1470, 4272,  244,   89,  906, 4606,   62, 1360,  287,  247, 2071,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 143, 2523,    0, 3101, 4748, 1261,    0,  138,  294, 4748, 6214, 1118,
           66,  144,   13,  999,  383,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  204,    0,    7, 7990, 1726,    6,   24,   11,   57,  802,   63,
         1223,   80,    7,  370,   79,    7,  984,   11,    6, 7990, 1726,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   33,   26,  206,   94,  513,    0,   79,   18, 3304,  554,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([31, 32, 26, 24, 25, 20, 26, 26, 17, 27, 22, 36, 26, 18, 16, 24, 30, 22,
        28, 19, 26, 19, 25, 13], device='cuda:3') tensor([0.8105, 1.0000, 0.8794, 0.8315, 1.0000, 1.0000, 0.8560, 1.0000, 0.8667,
        1.0000, 0.8911, 0.8027, 1.0000, 0.8887, 0.8779, 1.0000, 1.0000, 1.0000,
        0.8398, 1.0000, 1.0000, 1.0000, 1.0000, 0.8779], device='cuda:3',
       dtype=torch.float16)
tensor([[  21,   11,    6,  434, 6144,   35, 3326, 6184,    0,    8,  120,   25,
         4432,   21,   10,  419, 2468,    0,   21, 1518,  199,   13, 1629, 3351,
         1966, 7126,  174,  266, 3684,  356,   48,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 109,  728,   22, 1941,    0,  148,   26,   13, 1127, 1276,   12,  248,
            8,  213,    6,   10, 1320,  375,  267, 4159,   29,   17,  225,   73,
          205,  270,    8, 3768,  267, 4546,   11,    6, 5912,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   29,   70,   24,    0,  323,   34,   24,  842,   13, 3631,   12,
         5825,    6,    8,   24, 8425,    0,  134,   10, 6144,  879,  416,  307,
            0,  688,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   84,   11,    6, 1221, 3243,   17, 1880,  909, 3196,   48, 2658,
            6,   63, 6269,    8, 2183,   37,  254, 3753,  909, 3196,   48, 2658,
            6,    9,    7,  753,  115,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   24,   11,   57,  142,   10,  274,   85,  185, 1636,    6,   12,
            7, 1406,    0,    7, 2082,    8,  185, 1636,    6,   25, 2775,   11,
           18, 1110,    9, 1296,   10,  175,   13,  841,   12,  206,  214,   63,
          142,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,   24,  824,   48, 3267,    7, 1868,  816,    6,   12,    7,
         2421,    9, 1296,   10, 8601,    7, 2610,   11,    6, 2326,   13, 1236,
          429,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 682,    0,   77,  230,    0,  339,   11,    6,  150,   17, 1228,  180,
            0,  125,   33,   26,  204, 1248,  120,   25,  154,   80,   21,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  86,   91,   11,    6, 1848,   11, 2332,    6,   69,   91,   11,    6,
           51, 6979,    0,   67,  120,   91,   11,    6,    0,    0,  447, 2332,
            6, 2141,    0,   10, 5204,    0,    6,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57,    0,   77,  401,   21,    0,  192,   11,   18, 1233,
            0,   79, 2392,   79,   25,  498,    0,  952,    0,    0,   25,  498,
          802,  155, 3200,   54, 1501, 4838,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24, 1808,   71,   13,  822, 3580,   37, 3085, 2092,   10, 1157,    7,
         1097, 1486,    0,  180,   13, 3603, 2092,    0,  180,   39, 2781, 2092,
            0,  180, 1192, 2092,   96,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  85,   13, 1905,  613,    0,   33, 6652,  369, 8314,    0,    9,  564,
          895,  895,   46,  939,  533, 7898,   17,   24,   63,  291,  457,   10,
          368,    7,   56,   15,  140,  849,   62, 2468,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,  225,  188,  391,   29,   22,    6,  148,  225,    0,  388,  359,
         2108,  140,  271,  128,    0,  670,    0,    8,    0,  225,   11,    6,
          113,    7,    0, 5949,    0,   12,  267, 1749, 2346,  424,    0,  109,
         4586,    6, 1361,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   39, 8151,    0,   38,  779,   21,   11,    6,    0,
           55,  110,    0,   13,  453, 2913,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,   11,   57,  142,  359,    7,  564,  868, 1047,    6,    0,
           84,   11,    6, 2154,  338, 2256,    0,   84,   63,  214,  142,   69,
            0,   67,   21,   11,    6,   77,  321,   12,    9,    7, 4579,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   45,  244, 2371,    0,   29,   19,  116,  692,   10,  289,
           25,   73,    0,  103,   25,   11,   57,    9,  264, 1736,    0,   25,
           73, 2981,  126,   89,  283,   85,    7, 4656,   18, 2739, 2582,   15,
           22, 1010,  558, 1822,    0,    8,  113,   85,  523,  891,    6, 1492,
          158,   37,   93,    9, 1922,  237,    6,   20,  290,    0,    2],
        [   9, 4936,   10, 6461, 1234, 3707,   21,    0,    0,  113, 2828,   96,
         3238,    8,  415,  122,   22, 2278, 4340, 2806,   15, 1255,    8, 4340,
         4323,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:1') tensor([33, 35, 28, 31, 39, 27, 25, 33, 32, 31, 34, 41, 20, 37, 59, 27],
       device='cuda:1') tensor([1.0000, 1.0000, 0.8838, 1.0000, 1.0000, 1.0000, 1.0000, 0.8589, 0.8755,
        1.0000, 1.0000, 0.8149, 1.0000, 1.0000, 1.0000, 0.8989],
       device='cuda:1', dtype=torch.float16)
tensor([[   7, 9449,   34,  142,   10,   66,  248, 6734, 2028,   10,   21,    0,
           91,   34,  206,   24,  175, 5744,   10, 3793,  518, 3632,  131,   13,
         3941,   22,   18,  684,  199,   39, 1404, 4925,    0,    8,    7,  744,
           34,    7, 6815,  241,  271,   10,    7, 1692,    0,   24,   11,   48,
         4938,   13,  316,  362,  532, 1722,   12, 2468,    8,   24,   11,   48,
         3793,  199,   21,    0,    2],
        [1979,    6,    0, 1189,    0, 6071,  371,    0,   67,   12,  538,    0,
         4059,  570,  159, 3181,   45,  693,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8,   79, 2392,   79,   53, 1095,  110,   53,  246,    0,   38,  533,
          296,   13, 3592,  979,  240,    8,   13,  509, 5991,    3,   68,  194,
           65,   19,   34,  475, 6011,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  21, 1518,  126,   17, 1482, 1067,  362, 2304, 2871,  168, 2396,   10,
            7,  227,   22,  293, 4037, 1300,   17,  568,    7, 2696,    0,   86,
           71,   13, 3666, 3210,    0,   86,   71,   39, 2797,   12, 1703,  445,
            0,   67,  116,  246,    0,   38, 6435,    0,  169,   25, 1066,    3,
            7,  723,  246,    0,   38,  679,    0,   17,   11,    6, 3495,    3,
            2,    1,    1,    1,    1],
        [   9,   81,    6, 1955,  206, 1459,    9,   89,  775, 2592, 1559,  159,
          447,  964,   67,   84,   11,    6,  211, 8019,    0,    8,    9, 1052,
         2023,  227, 1996,   18,    6,    0, 1459,   69,    7, 3686,    9,   91,
          964,   71,  211, 6879,    6,   67,   71, 8019,    0, 3761,  961,   21,
            7,  509, 4387,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8,   33,   26,   46,   19,  150,   33, 3195,   54,  126,   77,  244,
           46,   33,   26,  108, 2696,    9, 1267,  158,  480, 2142,   18,  727,
          804,    0,  690,  241,    0,  717,  215,  581,    0,  206,   53,  245,
         1095,  159,  245, 2930,    0,  115,   53,   66, 2930,    9,  159, 5003,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  29,   24,   77,  229,   13, 5710,   30,  176,   10,   17, 2262,  620,
         1591,    0,  125,   24,  598,  100,  103,   24, 2924,   11,   18,  752,
          778,    0,   21,   11,    6,   79,  103,   24,   11,   57,  401, 1155,
            8,   24,   11,   57, 9796,   54,   69,  108,  403, 3034, 1428,   10,
          108,  807,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  21,   11,    6, 3118,  528,   46,   33, 1382,   11,   18,   69,    7,
          179, 4038, 2688,    0,   26,   21,    0,   19,   11,   45,   80,   10,
          175, 2182,   48,   46,   21,   11,    6, 3118,  528,   17,   25,  492,
          388,    7, 3316,   12,  155,  282,    9,    7, 1860,   12,   13, 4890,
         8182,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:5') tensor([65, 20, 31, 61, 53, 51, 52, 51], device='cuda:5') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:5', dtype=torch.float16)
tensor([[   8,   29,  103,   19,  162,  142,   10,  508,   25,    7,  467,   54,
           12,   33,  546,    0,   21,  169,  205,  250,  100,   33,    0,    8,
           17,   11,    6,   70, 5017, 2912,  110,   10, 4379,   10,   25,  168,
           85, 1366,   80,  546,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 115,    0,    7, 1043,   55,   33,    0,    7, 1043,   17,    7, 5367,
         2735,  689,   11,   18,   66,  419, 4585, 2836, 6672,   26,  125,    7,
         4910,    6, 1446,  535,  581,   17, 1719,  293,  356,   26,  593,  832,
           18,  233,  235, 4356,   10,   56, 2917,  597,   93,   55, 4585, 2836,
         6672,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   7, 1037, 4343, 6976,   34,   69,    7, 1832,    0,    0,    8,   89,
         3780,  246,    0,    0,   38,  779,    0,  995,  132,   84, 2320,   10,
         4432,    3,    8,    0,  225,  321,   12, 1260,  346,    7, 2884,    0,
            8,    0,    0,    0,  225,  246,    0,   38,  174, 1069,    0,    0,
           94,  540,    3, 3111,    0,   24,  144,   13,  207,  199,    7, 2469,
            0,    2],
        [ 115,   19,  217,  961, 1819,    6,    0,  473,  464,    0,   19, 5893,
           13, 1819,  434,   38, 2347,  588,  929,  889,    3,   38, 2347,  588,
          929,  889,  197, 3447,    6,   10, 1261,    0,   67,  486,  461,   12,
          108, 4990,  589, 1261,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  19,  474,   19,   11,   48,  456,   13,  277,  523,   80,  185,  116,
          488, 1288,   80,   33,    0,    8,  180,  175, 3570,  270,  126,  168,
           29,   24,   73,  456, 2895,  366,  111,   13,  277,  523,  143,    8,
          154,    8,  884, 1532,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  29,   70,    0,   19,  323,   26,    0,   19,  842,   77,   33, 1221,
            8,   19,  442,   21,  199,    0,   13,   46,  238,    0, 1223,   21,
           11,    6,    0,   13, 2913,  682,  455, 1434,    0,   12,   33,   91,
         4700,    0,    0,    8,   21,    0,  595, 2334,   13,  316, 1076,  266,
          436,    0,   12,    0,  284, 4896, 4437,   69,    7,  270,    0,    2,
            1,    1],
        [   9,  117, 2822,    6,    0,   25,  150,   39, 5020, 4373,   62, 4358,
            0,    8, 6013, 1708,  117,  824,  569,    6,   12, 1593,   18,   63,
          117, 3275,    0,  203,  140,   18, 1409, 2130, 1318,    6,  166, 2456,
         1646, 5494, 4653,    7, 2059,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  29,   33, 1119,   17,   21,   11,    6, 3756,    0,  131, 5057,    0,
           10, 2032, 1456,    7, 1503,   46,   19,    0,   20,    0,    0,    7,
         3140,   46,    8,    7,  765,  502,   12,   13, 8683,    0,  125,    7,
         1529,   12, 8634,   21,    0,  131, 5057,    0, 2317,   21,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:5') tensor([42, 51, 62, 42, 42, 60, 43, 48], device='cuda:5') tensor([1.0000, 1.0000, 0.8237, 1.0000, 1.0000, 0.8369, 1.0000, 1.0000],
       device='cuda:5', dtype=torch.float16)
tensor([[   8,  225, 1321,  270,    8,  568,   33,    0,   38,  187,  135,    3,
           68,  194,   65,    8,  225,  246,    0,   38,  533, 6090,  155, 1366,
          456,    9,   89, 1227, 7005,    0,  180,   24, 1157,  155, 1227,    8,
           24,  203, 6360,   48, 1469,   11,    7, 2033, 6665, 1031,  242,    6,
            0,  501,  197,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 204,    0,   19, 1042, 4744,  100, 1421, 1086,   10,  150,   13,  133,
          453,   13, 2553,   10,   87,   13, 3479,   12,   38, 1778,  749,  293,
            3,    8,   19, 1831,  172, 6760, 3785,    0,  125,  131,    7,  183,
            7,   13, 2553,  487,  378, 4940,  749,  293,    0,  101, 4609,  378,
            7,  453,   13, 2553,   17,   19,  144, 4744,  839,   10,  150,    0,
            2,    1,    1,    1,    1,    1,    1,    1],
        [ 103,    7,    0, 2313,    0, 1137,    9,  155,  753,   26, 3497,    0,
           10, 7892,    0,    0,  115,    7, 2313,    0,  723,   26,  665,   85,
         8481,    0, 1666,   35, 1921,  307, 8481,    0, 1259, 4418,    0,  415,
         1252, 1020,  998,   37,   93, 1599,   46,  214,   17,    0,   63,    9,
          560,   15,   18,  111,  143,    0, 1623,   10, 2644,    0,    8,  261,
          143, 4167,   10, 2644,    0,    2,    1,    1],
        [  19, 8547,  111, 1621,   13, 5063, 2672,    8,  434,   21,   38,  533,
           63,   77,  421, 1054,   62,  246,    3,    9,  116,  391, 1113,    0,
            7, 2672,  144,  244,    3,   94,    0, 5996, 5902,  589,    6,  148,
         4240,    7,  370, 6429,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,   34, 6583, 6071,   57,  436,    0,   84,   19,  205,  554,
           46,   19,   73,   11,   18,  601,   21,    0,   21,   11,    6, 1301,
           46,   68,  194,   65,   19, 1621,   13, 6762, 5831,   69,  384,  237,
          455, 1404, 1386,    6,   17,  473,   62,   55,   13, 1323,   12,  215,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 830,   48,    0,  106,    7,  183,   19,   34,  133,  963,    0,   69,
         1820,    0,   19,   34, 1800,   13,  325,   12,  341,  203,  809, 1921,
         1331,  233, 1611,    0,    8,   21,  735, 4639,   10,  110,    0,  120,
           19,   34,  963,    0,   17,  778,   34, 1067,  494,  126,  490,  110,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   9,  409,    0,  120, 1183, 3565,   25,   53,   11,   57,   13,  227,
         1147,  249,  440,    0,   53,  976,  261,  641,   13, 2291,  227, 1147,
          249,    0,  125, 1363,    6,  901,  232,    0,   21,  842,   80,  785,
          215,   55,   21,   10,   14,    0,    8,   85,   17, 2110,    0,   19,
           11,   48,  226,   13,   38,  311, 5296,  369,  197, 8823,   55, 1498,
          215,    0,  166,   46, 2290, 2592,    0,    2],
        [1552,  254, 2456,   71, 8760,   22,    0, 4456,    6,    0, 8393,  589,
            0, 1201,  588,    0,    0, 2008, 2593, 3284, 1727,    6,   54,    0,
         3348,    0,  108, 4341,    0,  446,   21,  143, 7553,    0,   10,  456,
           10,    7,   97,  181,    8,    7,  179, 2535,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:7') tensor([52, 61, 66, 42, 50, 50, 68, 46], device='cuda:7') tensor([1.0000, 1.0000, 0.8789, 1.0000, 1.0000, 1.0000, 1.0000, 0.8506],
       device='cuda:7', dtype=torch.float16)
tensor([[  33,   26,  204,   13, 3869,   17, 4974,    6,   85,    7, 2937, 1408,
         7174,   85, 6579, 2519,  670,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   33, 6735,   12, 1396, 2233,  106,  378,  116,   13,
         1466,   37,  199, 1692,   37,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   9,  116, 1992,  215,    0,   24, 4311, 1469,  132,   10,    7, 4389,
           12, 6204,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,    9,  409,    0,   24,   66, 2134,   55, 7691,   80,    7, 9270,
           15,   18,  768,   12,   29,  237, 5997,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  33,   26,    7,   51, 4999,  741, 3647,  941, 2394,    9, 4517,    0,
          166,   26,   91,   12,    7, 1849,  608, 3632,    9,    7,  179,    0,
            2,    1,    1,    1,    1,    1,    1],
        [  67,  735,   24, 1566,  371, 1993, 2482,   10, 3858,    0,    0,   71,
          267,  245,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,    9,    7, 1165,   12,    7, 1291,  821,    0,   84,  204,   66,
          226,   91,  109,  248,   17,   66,  226, 1110,    9,    7, 2533,    0,
            2,    1,    1,    1,    1,    1,    1],
        [   7,  279,   80,  283,    9, 2018,   22,  713,   26,   33,    0,   53,
          192,   11,   18,  403,    6,   96,    6,   80,  251, 7806,    6,    0,
            2,    1,    1,    1,    1,    1,    1],
        [  29,    7, 2884,   12, 7470,    6,  568,  450,   25,  250,    0,    8,
         1275,   21, 3565,   25,  250,   17,   11,    6, 1226,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 101,    0, 1918,   10,    0,    0,  468,  284,  427,    6, 5294,    6,
           46,   13,  264, 5931,   12,  427,    6, 5294,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   0,    0,    8,   19, 1917,   62, 1117,   12,   17,  563, 7373, 7443,
           89, 4764,    0, 4157,  169,  498,   10, 3424,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,   33,   26,   13,  277, 1819,   19,  442,   80,    7,  961,   12,
            7,  417,  727,   96, 1436,  165,   20, 4879,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  25,   63, 4265,   54,   10,  575,   94,   17,   25,  661,  134,    0,
           17,   25, 2990,   71,  134,    0,   17,   25,   11,   57,  461,   12,
            7,  370, 1361,   79,  134,    0,    2],
        [  19,  213,   10, 3499,    9,   89, 1259, 5258,    7, 1890, 1455,   12,
            7, 8022,    6,   46, 4469,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  25,   73, 1157,  629,    7, 3744,    9,  251, 1532,    0,    8,    7,
         1141,  188, 1155,   10,   87,   71, 4289,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  19,    0, 2138,    7,  245,  785, 1601, 7396, 7833,  111,    0,  752,
           10, 2866,   89, 1259, 1882,  346,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,   29,   24,  487,  826,   80,    0,  238,    0,  138,   87,   24,
          229,  251,  248, 4082,    6,    8, 1393,  134,  199,   91,    0,    2,
            1,    1,    1,    1,    1,    1,    1],
        [  17,   11,    6,  347,   43,  588,    6,   26,  115,    9,    7, 4370,
           53,   63,    9,    8,   17,   11,    6,  347,   56,  525,   26,    9,
            7, 4370,   53,   63,    9,    0,    2],
        [ 903,    0,  853,   48,   22,  293,  552,   10,    7,  452, 1539, 1315,
           93,    8,  225, 1945,   62,   17,  225, 3367,  132,    9,  832,  156,
         3056,   20,    0,    2,    1,    1,    1],
        [  33,  987,  607,  445,   26,  736,  439,  442,  132,   12,    7,  225,
          158,    6,   12, 2481,    6, 5110,  266, 3529,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   9,  409,    0,  108, 6180,   46,  131,   38, 3392,  197,   19,  641,
           24, 6814,    6,   46,   91,   12,  108,  488, 6180,    6,   34,    7,
          561,   22,  455,    0,    2,    1,    1],
        [  19,  220,  116,  450,   84,   34,   13, 3917,  694,  850,   21,   46,
            9,  333, 3489,    0,  333, 5082,    0,  333, 1375,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  21,   26,  914, 3529,    0, 3706,   17,  117,   66,  226, 3572,   62,
            8, 9304,   62,  850, 4213,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [7894, 2071,  627, 5668,    6, 2027,    9,  108, 1402,  162, 2568,    9,
          564,  895,  895,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1]], device='cuda:7') tensor([19, 19, 16, 21, 25, 16, 25, 25, 23, 23, 22, 22, 31, 19, 21, 20, 24, 31,
        28, 22, 29, 23, 19, 17], device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8286, 1.0000, 1.0000, 1.0000,
        0.8311, 0.8413, 1.0000, 1.0000, 1.0000, 1.0000, 0.8843, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:7',
       dtype=torch.float16)
tensor([[ 281, 1810,   45,  373,    9,  251, 1113, 1639,   69, 2248,  181, 3414,
           96,    0, 1009,  214,  100, 5032, 3157, 1794,    0, 1703, 2729, 1794,
            8, 3515,   35, 1203,   93,   54, 1794,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 138,  294,   12,   25,  168,    0,  103, 1482,   46,  185, 2465, 1339,
         2197,   46, 3336, 2432,   25,  138,   13, 1760, 3212, 4112,   26,  691,
            0,   25,  746,   12,  213,   10, 2477,  155, 4896,    6,    8,  289,
            0,   38,  679,    0,  211,    0,   19,  192,   11,   18,  213,   10,
          135,    0,    2,    1,    1,    1,    1],
        [  29,    0,   12,  538,    0, 1320,   45,  500, 1236,  675,    6,   26,
         6957,   86,   13, 8659,    0,   21,   11,    6,   25,    0,   21,   11,
            6,  110,    0,   21,   11,    6,  108, 5867, 1874,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  29,  276, 1445,   19,   34,   13, 1185,  501,  820, 1047,   38,    0,
         3938,   35,  603,  569,  415,  158,   20, 2415,  436, 5865,   37,    0,
            8, 1183,  148,  220,  467, 1049, 1230, 1551,   12, 1580, 1069, 7754,
         3454,   46,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 155, 8019, 2493, 1187,   26,   29, 3764,   17,   19,   34,  529,   10,
         5590, 5412,   21,  802,  660, 2875,  985,  111,   69,    7, 1465,   71,
          211, 4637,   54,  109, 5460,    0, 5460, 4112,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 120,    0,   19,   34,   13, 2988,    0,    9, 1416,    0,   12,    0,
          333, 1318,    9,    7, 4267,  276,   54,    6,    0,    0,   25,  169,
          446,  963,   94,  540, 2327,   54,    7, 3297,    6,   12,    7,  383,
            0,  109,    7,    0,  801, 3297,    6,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   0,   21,   11,    6,  528,   17,   24,  991,   33, 5445,    0,   17,
           24,   66,    7, 3398,    0,  421,  678,  533,  153,    0,    8, 1573,
           12,    0,  108, 3348,  148,    0,    0,   63, 1211,    0,   38, 5034,
            0,   24,    0, 1412,   87,  250,  341,    3,  125,    0,   24,   63,
         7258,   62,   71,   13, 2020,    0,    2],
        [ 115,    0,   70,  103,   89,  964, 3147, 1425,   80,   89, 1751, 2079,
         1326,   19,   93,  511,  290,    0,   13, 1728,  982, 1365,  148, 5056,
            6,   13, 4158,  575,    9, 2891,  794,    0,    8,   26, 4962,   48,
           10,  450,    7, 1352,   17,   24, 5559,   10, 3995,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1]], device='cuda:0') tensor([33, 51, 35, 39, 35, 45, 55, 47], device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8696, 0.8828, 1.0000],
       device='cuda:0', dtype=torch.float16)
tensor([[ 166,   26,  347, 2290,  233,  824,   59,  338,  650,  583,    6, 1017,
          187,    0,   38,  469, 3920,  290,   20, 2992,   12,    7,  858,    3,
           21, 3565,  170,   80,  159, 1406,    0,   67, 3412,  369,   12,   13,
         3413, 3565,  170,   21,   11,    6, 1254,   55,  170,   10,   66,   13,
          535,  858,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [3761,    0,  103,   33,   26,  230,    0,   24, 1412,  203,  500, 1547,
            0,    7, 1880,    8, 1519, 7153,    6,    0,    8,  509,   87,    0,
           21,   71,   13,  102,  794,  378,    0,    9,    0, 1206,    0,    0,
          100,    9, 4748,   85,  969,    6, 3706,  929,    7, 8869,    6,    0,
          109,    7,  811,  140, 7710,   12,  596,    8,  848,  236, 1486,   18,
            6,    0,    2,    1,    1,    1,    1],
        [  67,   19, 1932,   10,   89,   13,  500,   18,  120,   19,  278,   17,
         3493,    8,  246,    0,   38, 1969,  135,    0,   19,  154,   33,  116,
          818,   17,   55,    7,  245,  183,    9,   89,  282,    0,  378, 1714,
          188, 4744,  172,  238,    3,   29,   19,  465,   11,   18,   66, 2214,
           10,   17,  813,  125,    7,  837, 5002,  336,  110,  465,   11,   18,
           66, 2214,   10,   17,  813,    0,    2],
        [ 115,    0, 5873,  296,   10,  229,  333, 3916,   10, 8711,   17,   39,
         7681,   17,  188,  226, 3498,   10,   51,   51,   22, 1987,  266, 1010,
           56, 1628,   13, 5895, 3677,   26, 8509,   10,    7,  461,  266,  763,
          480,    6,   12,    7, 3677, 1042,    7, 3677,  188,  226, 3768,   48,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  29,   55, 4212,    0,    7, 1650, 9787,   26,  923, 1052,  491,   18,
            0, 1120,  690, 2470, 3080,    0,    8,  419, 1289,   17,   19,   79,
          779,  609,  436,   71, 9787, 5953,    7,  370,  207,    0,  860,   79,
           38,    6,  500,    6,  307,  197,   46,   13,  133,  851,  266,   93,
         1455,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,  144,  860,   46,   21,   34,   39, 2576,   35,  781,   15,   54,
         1064,   10, 1510,  134,  456,   80,    7,  179,   17,   26, 1094,   10,
          367,  359,  639,    8,  948,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   48,    0,    0,  116,  570,   10,   51,  388,    9, 1416,
           12,  185,   12,    7,  281, 1248,  954, 1118,    0,    8, 6264, 1118,
            0,    9,    0, 1491,    0,    0,   38,  511,   13, 3087, 1065,    0,
         1366, 1583,   48,  110,  132,    8, 1742,  110,   33,   13, 1820,    0,
           19,   11,   45,    0,  168,    0,   29,    0,   89, 3013,    0,    2,
            1,    1,    1,    1,    1,    1,    1],
        [3379, 5486,  412,   45, 1115, 1449,    6,  340,    0, 3379,   63, 5053,
           62,    9,    7, 2059,    9,   91,  561,   55,  294,  422, 2042,    6,
            0,   67,  103,   24, 3474,  108, 3064,  106,    7, 2591, 2023,   10,
            7,  830,  200,  236,    6,    0, 3379,  873,  133, 4889,  577, 1611,
            0, 1944,    8, 2288,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1]], device='cuda:0') tensor([52, 63, 67, 50, 51, 31, 60, 54], device='cuda:0') tensor([1.0000, 0.8687, 1.0000, 1.0000, 1.0000, 1.0000, 0.8716, 1.0000],
       device='cuda:0', dtype=torch.float16)
tensor([[  24, 1704,   11,   18, 6528,  419, 6669,    6,    0,   24, 1704,   11,
           18, 6528,  419, 3677,    6,    0, 5784,   53,   11,  121,  226, 9342,
           62,  490,   53, 1808,    0,   67,   53,  465,   11,   18, 2215,    7,
         1375,    0,    2,    1],
        [ 115,   21,   11,    6, 2076,  362,  945,   10,  213,   10,  175,    7,
          170,  443,  803,  607,    6,  929,    7, 1436,  237, 1027,   18,    6,
            0,  230,    0,   10,  175,    7, 3795,  993,  929,    7, 3172,  290,
         1408,  993,    0,    2],
        [  29,    0,   21,   11,    6,  100, 3457, 3539,    0,  700,  608,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  25, 1095,    0,    0,   19,  154,    0,    7,  456,    8, 3395,   17,
           56,   62, 1820, 5589,  303,    0, 1742,  168,    0,   13, 1323, 1113,
          581,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  67,    9, 4129,  409,    0,   19, 2775,   11,   18,  442,   33, 1677,
           29,   17,   19,   73, 1290, 5366,  214,   17, 1220, 1620,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   8,   10, 6757,    0,   55,   13, 1720,   18,  480,    0,   84,    0,
           26,   13, 3955,   45,  502,   12,    0,    0, 5506,    0,   13, 3955,
           45,  502,   12,  227, 1411,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [1088,   45,    0,   55,  110,    0,   89, 1276,   34,  909, 1803,   54,
          939,  458,  834,  120,   19,   34,   13, 1269,   10,  873,   13, 3280,
            0,   67,   19,  172,  465,   11,   18,  213,   10,    0,    2,    1,
            1,    1,    1,    1],
        [  13,  488, 2020,   26,    0,  138,  451,    7,   19, 1203,   48,    6,
           51,  619,    0,    8,   70, 1719,    6,  451,   24,  388,   69,    7,
           19, 1203,   48,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   7,   94,  148,  162, 3045,   69,    7,  830,   57,  556, 1326,  158,
          499,  144,   13,  203,    6,  494, 1317, 2958,   12,    7, 3045,    0,
            8,   53,  162,  499, 3066, 8749,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   8,  180,  101,  169, 2153,   80,   21,    0,    8,  101, 1831,   17,
          101,   34,   13, 4083, 1262,    8,   17,  101,   34, 2914,   57, 3449,
            6,   15,  945, 4115,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   9,   77,  117, 3302,    0,   84,   11,    6,   91, 4040,  774,    0,
          125,  117,   63,  474,    9, 2047,   12, 1710,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  53, 1082,  261, 3592,    0,    0,   53,   63,  143, 8192,    0,   69,
            7, 3516,  440,   55, 6263,  203,  430,    6,    6,    0, 6288,    0,
          288,    0, 4478,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   8,   55,  251,   12,   25,  148,   63,   86, 1848,    0, 1487, 1059,
           93, 6977,    6,   63,  251,  277, 3434,  214,   17,  155, 1269, 4853,
            6,   10,  432,  802, 5109,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1],
        [ 854,    7,  800,   12, 2028,   63, 4198,   62,    9, 6243,    9, 9022,
           10,   13, 5787, 8641,  595,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   8,   33,   26,    7, 4340,   17,  101, 1742,  120,  101,   34,   69,
         3677,   46,    8,   21,   11,    6,   13,  453, 4340,    0,   38,  242,
          119,   12,   77,    3,  101, 1119,    0,   38,  340,   10, 1997,    0,
            2,    1,    1,    1],
        [  70, 1878,  323,   21,  229,    9,  159, 2332,    6,    0,  159, 4388,
           10, 1487,  362,  297,  181,   93,    0, 3647,    0,   67,   70, 4877,
            6,   55, 3647, 2302,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1]], device='cuda:6') tensor([39, 40, 13, 27, 24, 31, 35, 30, 32, 30, 22, 29, 32, 19, 37, 30],
       device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 0.8530, 1.0000, 0.8916, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8838, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:6', dtype=torch.float16)
tensor([[   7,  370,  521, 6906,    0,    7,  370,   29,   59, 2405,    0,    7,
          370, 1790,   12,  378,   51,   18, 3304,   62,    8,   86, 5804,   54,
           10, 2307,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  19,  154,    7, 1043,   33, 1429,   29,  238,   26,  125, 1308, 3006,
         1997,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  24,  296,   10,  446, 1081,   12, 9695, 3834,   12, 4619,   17,   63,
          143, 4414,    0,    8,  192,   11,   18,  735, 6430, 2423,   54,  518,
          108, 3916,  199,  108, 5627,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8, 5493,    0,  101,    0,  246,  101,   34,  142,   10,  175, 4532,
           12,    7,    0, 2902,    6, 4775, 1890,   46,   68,  194,   65,   46,
            0,    8,    7,    0, 2073,   45, 4775, 1890,    0,    0,  593,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 115,    0,    7, 3966, 1472,   12,   33,    0,    7,  305,   35,  176,
         2175, 2187,  106,   13,  740, 3238, 4774, 1109, 5412,   26,   33,    0,
           70,   24,  979,    0,   24, 1082,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   0,   67, 1223,    0, 1155,  261,   26,  142,   69, 1325,   25,  498,
            0,   10,  175,    0, 6509,  411,  266, 5408,    0,    0,    8,  131,
           17, 2110,    0,    0,   25,   11,   57,   86,  665,  453,    0,   25,
           11,   57,    0,   86, 1790,  453,    0,   25,   11,   57,   86, 1057,
           17,  261, 2314,    0,    2],
        [   7,  744,  279, 4673,   80,    7, 1579,  371, 1898,  424,  371,   26,
            0, 2027,  199,  801, 1137,   46, 1992,   10, 1536,  215,  801,   46,
          117, 1816, 2924,   11,   18, 2532,  886,  371,  322, 1921,    0,   53,
           11,   57, 2532,  110, 1488,   35,  424,  371,  322, 1921,    0,    2,
            1,    1,    1,    1,    1],
        [   7,  218,  461,   12,    7,  546,   63, 2720, 2261,   17,    7, 2932,
         1381, 1427,   69,   77, 1881, 6101,   69,    7, 2932, 1465,    0, 2134,
           79,   13,  567,   12, 1244,   35,  607,    6,  140,  763, 1386,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:6') tensor([28, 16, 31, 37, 32, 53, 48, 37], device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 0.8457, 1.0000, 0.8691, 1.0000, 1.0000],
       device='cuda:6', dtype=torch.float16)
tensor([[  24, 1621,   86,  288,   39, 2987,    0,    0,   67,   85,    7, 1740,
           12,   33, 2987,   24,    0, 1621,   13, 4601,    0,    0,  672,    0,
            0,    8,   33,  672,   34,   80,    7,    0, 1064,    0,   12,   13,
         4328,    0,    7, 1064,   12, 7376,    8,   12,  540,  687,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  67,    7, 8088,   54,  279,   26,   17, 4623, 2050,  373, 2523,   66,
          226, 5037, 4031,   17,   63, 2414,   10,  108, 1715,    0,    8,   53,
           11,  121,  591,   17,   13,  800,   12,  134,    0,  120,   53,   11,
           57,   80,    7, 1137,   12,  108, 1715,    0, 5368,   15,  131,   13,
          409,  240,   12,   79,  261,   79,  423,    0,  689,   11,   18,  473,
           55,  133,  535,    0,    2],
        [ 545,  188,  822,    0, 2683,  964,    6,    0,  206,    7, 8869,    6,
            0,  596,    0, 1585,   71,  963, 2352,    8, 4478,    0,  185,   79,
          963,   79, 1646,  215,  801,    0,   63, 6324,   10, 3284,   18, 1617,
            7, 7009,    6,    0, 9695,  134,   10, 1713,  143,  896,    8, 8939,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8,    7, 5407,   12,  961,   13, 2688, 3860,   59,   55,   13, 6049,
         3367,  199,   70, 1505, 2134,   79,    7, 1465,   12,  214,    0,  166,
           26, 2318,   39, 2735,  115,   71, 4960,   19,  362, 1838, 1428,   55,
          958, 1006,    0,  941, 7824,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 185, 1075,   46,  305, 6124,    0,   39, 4174,  768, 6503,   71,   13,
         2127,   86,   17,  341,  106, 5200,  699,  241,  352,   11,    6,    0,
          131,    7,  207,   46,   91,  383,  473, 9472,    0,  278, 1759,  820,
          439,   12,   77,   21,    6,  941,  106, 8307, 2791,    0, 2248,  111,
         2626,    8, 2678,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  84,   63,  133,  555,  214,    0,  133,  555,  214,   17,   25,   73,
          172,   87,   17,  164,  468,    7,  207,   17,   25,   73, 2644,  117,
         1587,   12, 4390,    8, 1064,   70,   19,  169,  583, 3849,   13, 1395,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  67,   39,  276,  143,  528,  774,   12, 9063,  271,   55,  565,    0,
           17,   56,  111,   22, 4014, 3570,  650,  492,  220, 3890,    0,   34,
           13,  570,   12,   46, 3524,   46, 7683,    0,    8, 1790,  126,   70,
         2275,  372,   59, 1563, 2028,   12,  282,   73, 2231,   79,   13,  926,
         2653,   10,    7, 3462,  687,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 148,  220,   66,  934,   48,   17,   13, 1127, 1369,   12, 2951,   57,
          200,   35, 3794, 6814, 1074,    9,    7, 6356,    6,   12,    7, 5495,
          179,  169, 7831,  199, 2955,  634,   18, 3772, 5869,   12, 2232, 2399,
            8, 2434,    7,  133, 5495,    6,   53, 1412,   66,  903,   20,  556,
           62,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:1') tensor([48, 65, 50, 43, 53, 38, 55, 51], device='cuda:1') tensor([0.8169, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:1', dtype=torch.float16)
tensor([[ 148,   63,   77,   12,    7,  218,  596,   69,  117,  255,  945, 2696,
            6,    0,   19,  591, 6087,   93,  122,  551,  237,  820, 1132,  895,
          828,    0,  225,  246,  225,   34,   13,   38, 2470,   22, 2088,  148,
           26, 1767,    8,  126,  606,   54,    3,  225, 2884,   62,  267, 1329,
           79,   38,  153, 1040,   37,    3,  225,  246,  225,   26,   38,    6,
          233,  111,    0, 2292,    8, 1751,  111,    3,  225,  100,    6,   10,
          229,   94, 4265,   38,  128,  959,    3,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   85,    7, 1565,   12, 5604,   54, 1222,   13,  277,  523,    0,
          120,   19,  274,   85,   70,   11,    6,  142,   69,   71, 1578,  639,
          117, 1113,    0,   24,   63,   86, 3983, 3019,  359,   71,   33, 3046,
            0,    8,  120,   19,  274,   85,   70,   26, 1592,   10,  108, 8281,
            8,  108, 4621,    0,   89, 1127, 7034,   26,   17,   24,   13,  160,
           11,   18, 1110, 1155, 1094,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  17,   26,  347,   25,    0,    9,  155,  294, 4159,    8, 9034,    0,
           63,   13, 3764, 1986,  365,    0,   13, 1986,  365,   17,   11,    6,
         4690,  111,  341,    9,  155, 2395,  572,    9, 3201,  254,    7,  572,
           12,   39, 2216,  736,  215,  581,    0, 4126,  111,  341,    9,    7,
         3201,    6,  106,    7,  572,   12,    7, 2313, 2216,    3,  215,  581,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  180,   19,  213,   25,   10,  934,   17,   33, 2088, 1603,   26,
         2952,    0,    8,   21,   11,    6, 6487,    0,    8,   21,   11,    6,
         4469, 1584,    0,    8,   21,   11,    6, 9101,    0,    8,   21,   11,
            6,  985,  687,    0,    8,   21,   11,    6,    9,  191,    6,  429,
            0,    8,   21,   11,    6, 9059,    0,    8,   21,   11,    6, 2549,
            0,    8,   21,   26,    9,   18, 2738,  366,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [3524,  109,  218,   46,  635,   91, 9737,   62,  486,    0,   24,   11,
          158,  492,  135, 1456,  347,   46,   67, 1042,   53,  278,   13, 9745,
          336,  134,    0,   53,  162,   77,    9,    7,  370, 9745,    0,  115,
           77,    7, 4488,   35,  140,   57,  922,  876, 5521,   12, 4618,    0,
           77,    7,  453,  687, 1621,  131, 8218,    0, 1917,    6, 4775,   62,
         1117,    7, 9745,    8,   24,   11,  121,  278,   13, 1629, 3438,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33,   26,  824,  174,  779,  527,    0,    0,   91,   12,    7, 1881,
           17,   33, 8223,  140,  338,  525,  128, 6688,    0,  447,   62,    0,
            0,    8,   39,  916,  279,   80,  824,  174,  779,  527,    0,    0,
           26,    7,  655,   35,  737, 2109,  447,   37,   12,   33, 1850,   62,
            0,   13, 1329,   13,   48,  352,   18,    0,    8,   33, 1329,   13,
           48,  352,   18, 4223,   62,   91,   12,    7, 7577,    0, 2348,  106,
            7, 3141, 3928,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   19,   66,    0,   13,  277, 1964,   69,   89,  664, 6729,    0,
            8,    0,    0,   21, 1119,    0,   38,  242, 1027, 1792,   19,   11,
           45,    0,    7, 3806, 1251,    0,   17,   11,    6,  347,    3, 4701,
            0,  211,    0,  143, 7052,    0,    0,   38,  187,   11,   45,    0,
            0,    0,    7, 3806, 1251,    0,    0,   25,   11,   57,   86,    0,
         2456,    3,    0,    0,    0,    8,   84,   11,    6,    0,  825,    0,
           55,  663,    0,    0,    0,  103,   89,   29,   22,   11,    6, 1431,
         1516,    7, 2657,  985,    0,    7, 2918,    6,   66,   10,  135,   25,
          192,   11,    0,   18, 1396, 1004,   33, 1375,    0,    2],
        [  70,   33, 1221, 3556,  170,   46,   86,  116,   89, 2260,    0,   67,
          108, 6142,    0,    8, 6579, 2519,  670,  143, 6902,   46,   26,   17,
           24,   66, 3764, 2791,    9,   13, 1027,  631,  407,   55, 3001, 5605,
            0,   17,  108, 1740,    0,  166,  188, 1108,   62, 1645,  143, 1167,
         5152,    9,   13, 2931,  207,  254,  419,  218,    0,  492,  442,   91,
           12,   21,    6,  447,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:1') tensor([ 80,  67,  62,  70,  73,  77, 106,  66], device='cuda:1') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8936, 0.8052, 1.0000],
       device='cuda:1', dtype=torch.float16)
tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8506, 1.0000, 0.8789, 1.0000],
       device='cuda:4', dtype=torch.float16)
tensor([[   8,   24,   77,  135,   17,  103,   25,   66,   91, 6270,    0,   25,
           73, 2026,   91, 8971,  131, 4619,    7,  218,  248,  214,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  19,  154,   17,   84,   26, 7033,   13,  264,  321,   12, 1478,   12,
         2895, 6053,   17,   11,    6, 2439,   10, 4504, 1817,  230,  115,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 473,  464,    0,   85,   33,  183,    0,   84,  162,  423, 6504,    6,
            0, 2697, 6214, 1118, 4691, 5056,  709,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 294,   94,  154,    0,   38, 1367,    0,   21,   11,    6, 2930,    0,
            8,   21,   11,    6,  691,  131, 3927,    8,  993,  100,   17,    3,
          238,    0,   53, 9736,   21,    0,   67,    7, 3573,    6,   63,   77,
          691,  131,  874,    0,    2],
        [   7,  218,  207,   26,   17,  155,  572,   26,   86,  288, 1520,    9,
         6445,   19,  362,  300,    6,   96,    0,   25,   11,   57,  113, 1557,
           54,  126,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   7, 1511, 3316,    0,  109,    7, 1511,   12,    7, 1511,   26,  138,
           19,  100,   10, 4576, 1160,   45,  174,   57,    0,   10,  375, 1650,
           46,   53, 1985,  450,  117,  214, 9335,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  29,  245,   12,   77,    0,    7,  245, 2110,   17,   24,   11,  121,
         1110,   12,    7, 1465,   34,   17,   21,   34,  142,   10, 3249, 2930,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  84,  162, 7037,    6, 4127,   62,   85, 1102,  467,    6,   12,  545,
          595,    0,   79,  103,   24,  162, 6959,    6,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  67,  101,  492,  469,  982, 6953,  518,   91,   12,    7,  281, 2694,
          366, 6105,    6,  419,  753,  188,  700, 1110,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 101,  278,   13, 8429,    0,    8,  101,   34,  172,    0,  172, 4594,
            0,   29,  101,  172, 1608,   11,   18,  450,  110,   70,   10,   87,
          558,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  67,   19,  154,   21,   11,    6,    9,    6,  221,   20,   10,  703,
           17,   24,  164,  700,  229,   13, 1504,   17,   26, 9519,  111,  110,
          338,   18, 1327,  371,  816,    0,   21,   11,    6,   39, 3756, 2484,
            0,    2,    1,    1,    1],
        [   7,  744, 3743,  106,    7, 1219,   26,   13, 1153,   12, 2060,  636,
            8,  284,  248, 3636,    6,  893,  126,   12,  159,  595,    9,  159,
          447, 1927, 1408,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   9,    7, 5003,    0,   19, 2020,   89, 1613,   10, 3522,    7, 6462,
            6,    9,  159,  447,  931,  359, 6762,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  17,  941,   26, 5823,   62, 1585, 7990, 1726, 3744,  346,  199,    7,
         6447, 2353,    6,    0, 2173,   54,    9, 2749, 1284,   13,  234,  240,
          290,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  19,  552,  132,   71, 3900,  800, 2703,  432,   19,  204,  465,   11,
           18, 1757,   55,   13, 2322,    0,    8,  513,   80, 1016, 2360,    8,
         6356,   62,   13,  846, 1331,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   0,    8,  138,  261, 1747, 2611,  220,   53,  875,   10, 3848,  103,
           53, 5248,   10, 4996,    0,    0,   69,    7, 2517,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:4') tensor([24, 25, 21, 41, 28, 33, 26, 22, 22, 27, 38, 29, 21, 27, 31, 23],
       device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8369],
       device='cuda:4', dtype=torch.float16)
tensor([[ 29,   0,  19,  ...,   1,   1,   1],
        [ 29, 138,  63,  ...,   1,   1,   1],
        [ 91, 279,  19,  ...,   1,   1,   1],
        ...,
        [  8,  53,  77,  ...,   1,   1,   1],
        [ 68, 194,  65,  ...,   1,   1,   1],
        [ 33,  26,  13,  ...,   1,   1,   1]], device='cuda:4') tensor([19,  8, 26, 17, 18, 20, 15, 22, 22, 27, 20, 16, 36, 27, 19, 21, 26, 18,
        19, 23, 21, 15, 32, 20, 21, 22, 16, 28, 21, 22, 18, 20],
       device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8721,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8657, 1.0000], device='cuda:4',
       dtype=torch.float16)
tensor([[5082,    6,   12, 2422, 7713,    0, 2422, 7254,   54,    8, 2422, 8365,
          271,    0,   84, 1859,  961,   17,    9,   18, 1625, 6483,  119,  724,
           12,    7,   73,  781,   93,  291,  457,   10, 2443,    9,    7, 9370,
         1032, 1081,   17,   21,  188,  120,   21,   26,   86,  838,   18,  234,
         3785,  131, 2048,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  91,   17,   24,  939,    6,   20,   20,  834,  168,    0,  886,    6,
         1558,  609,  287,    0,    7, 5694,   12, 1954,  455, 1943,  407, 5462,
          372,    9, 5556,   59, 1068,    0,   29,   24,  116,  246,    0,  238,
          635,    0, 1051,    6, 3287,  188,  691,  250,  100,   17,    9,    7,
         1165,   12,   33,  453,  283,   12,  998,  131,  749,  399,  918,  287,
            0, 1313,  101,   34,   13,  453,   13,   48,   45,  551,   37,   12,
          749,  399,  918,  287,  255, 1051,   22,  609,    0,    2],
        [  29,    0,   24,  278, 6713,    0,   55,    7, 1793, 1956,    0,   77,
          230,    0,    8,  168,   26,   33, 1284,  422, 1603,    0,    8,   25,
           73,  934,   17,  168,  113,    0, 3317, 1143,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,   13,  555, 2168,    0, 1459,    9,    7, 4857, 3539,   34,    9,
          108, 2092,   96,    0,   38,    6, 2704,  760,   26,   13, 3700,   85,
           13,  183,  120,   10, 7077,  596,   26,   13, 3975, 1201,    9, 4970,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 101, 1772,    6,   33, 4340,    0,  166,   34, 6483, 1797,   62,    9,
         9479, 1706,  111,  490,  101,   14,   48,   46,  101, 1772,    6,  131,
         1945,   54,  138,  948, 1429,  359,    7, 6501,   12, 7522, 5294,    6,
           17,   63, 5848,   48,   10,   51, 6610,   10,  838,  927, 3149,    0,
            8,  180,  101, 1143,   69,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   25,   11,   57,   86,  172,  142,   10,    0,    9,    7,  501,
         7015,    6,    0,  467, 7886,  131, 1766,   54,   91,   12,  117, 2878,
            0,   21,   11,    6,   13, 6410,   12, 1752,    6,   15,    6,   20,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 336,    7,  370,  183,    0,   19,   34,  142,  359,   39, 8087,  369,
           46,    7,  245,  700,  985, 8087,  369,  131, 1553, 4528,    9, 1777,
           46,   71,  486, 3619, 2900, 2352,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  509, 6406,  132,    0,   68,  386,   65,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:2') tensor([53, 82, 34, 38, 55, 38, 32,  9], device='cuda:2') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:2', dtype=torch.float16)
tensor([[  24,  175,  700,  ...,    1,    1,    1],
        [ 851,    0, 1598,  ..., 1262,    0,    2],
        [  29,  339,   11,  ...,    1,    1,    1],
        ...,
        [ 125,   21,  188,  ...,    1,    1,    1],
        [  70,  169,   25,  ...,    1,    1,    1],
        [  19,  116,  465,  ...,    1,    1,    1]], device='cuda:2') tensor([ 7, 16, 12,  7,  9,  8, 10,  7, 14, 15,  9,  9,  9, 11,  8,  6,  9,  7,
         7,  8,  9, 13,  7, 10,  9,  8,  7,  9,  7, 11, 11, 11,  8, 15, 10,  9,
        11,  8,  9,  8, 15,  7,  8, 10,  8, 12,  9, 10,  9,  6,  8, 11, 14, 10,
         8, 12, 10,  8, 14, 14, 12,  9, 10,  8,  8, 11,  6, 12, 10, 13,  8,  8,
        10, 10, 10, 12,  8,  9,  7,  8,  8,  9, 11, 11, 12,  9,  9,  7, 10,  7,
        10, 10,  8, 11,  8,  8, 10,  8,  9,  8, 11,  8,  8,  7, 12, 10, 13,  7,
        14,  9, 15,  9,  8,  8, 10, 10, 10,  7,  8, 10], device='cuda:2') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8735, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8130, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8696, 1.0000, 1.0000, 0.8672, 0.8652,
        1.0000, 1.0000, 1.0000, 0.8687, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8955, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8750, 1.0000, 0.8003, 1.0000, 1.0000, 0.8154,
        1.0000, 1.0000, 1.0000, 0.8145, 0.8315, 0.8403, 1.0000, 1.0000, 0.8857,
        0.8267, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8125, 1.0000,
        1.0000, 1.0000, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8677, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8857, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8735, 1.0000, 1.0000, 1.0000, 0.8867, 1.0000, 1.0000, 1.0000,
        0.8481, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000], device='cuda:2', dtype=torch.float16)
tensor([[  17,   11,    6,  ...,    1,    1,    1],
        [  86,   85,   77,  ...,    1,    1,    1],
        [ 120, 8121,  728,  ...,    1,    1,    1],
        ...,
        [  53,   11,   57,  ...,    1,    1,    1],
        [   8,   86,  288,  ...,    1,    1,    1],
        [  24,   63,   86,  ...,    1,    1,    1]], device='cuda:2') tensor([12, 18, 15, 13, 14, 14, 16, 14, 15, 17, 11, 11, 14, 16, 14, 13, 13, 13,
        13, 10, 18, 12, 12, 15, 13, 18, 16, 15,  9, 16,  9, 17, 12, 15, 13, 11,
        13, 15, 11,  9, 17, 20, 14,  9, 11, 15, 11, 15, 13,  9, 13, 12, 11, 10,
        21, 15, 10,  7,  9, 19, 14, 16, 15, 11], device='cuda:2') tensor([0.8955, 1.0000, 0.8086, 1.0000, 0.8545, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8662, 0.8711, 0.8091, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8262, 0.8496, 1.0000, 1.0000,
        1.0000, 0.8403, 1.0000, 1.0000, 1.0000, 1.0000, 0.8154, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8760, 0.8003, 1.0000, 1.0000, 1.0000, 0.8896, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8188, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000], device='cuda:2', dtype=torch.float16)
tensor([[  29,   70,   10,   87,    0,  238,    0,   39, 1141,  659,  367,  106,
         1560,  121, 3165,    8, 2185,  502,    6, 1574,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,    0, 1296,   10,    0,   87,   33,    0,   21,   11,    6,   13,
          325,   12, 8512,    8, 3236,   17,    0, 1202,    6,    9,   13,    0,
          133,  747,  203,  779,  675,  793,    0,    2,    1],
        [  19,  246,    0,   38,  242, 1865,   20,   25,  162, 2188,    0,   21,
           34,  116,  155, 3181,    8, 3806,    0,  109, 3181,    8, 3181, 3328,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [ 440,    0,   21,   26, 1970,    6,    0,   86, 1276,    6,    0,  148,
         2508,    7,  281,  283,   35, 5203, 3382,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   17,   11,    6,   13,  324,  279,    0,  125,    9, 1296,   10,
          229,  995,   13, 1874,    0,   25,   66,   10, 2484,   80,   21,  245,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  17,   11,    6,   13,  341,  561,   10,   51,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7, 4460,   12,   13,  384,  372,   93,   62, 8093, 8187,   26,   86,
          142,   10, 6264, 4032, 5099,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24, 6532,    7, 5408,  338, 1344,   71,   39, 6291, 5108,   12,  655,
            3, 1086,    9, 1565, 3979,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   11,    6,  276,  143, 1248,   26,   21,  164, 3162,  126,   13,
         1850,  709, 6649,   69,    7, 4057,    0,  125,   21,  135,    6,    7,
          207,   12,    7,   21,  510,    0,    2,    1,    1],
        [   8,   79,    7, 9448,  480,    6,   12,  117, 3522, 1118,    0,   24,
           66,  159,  211,  424,   48,  266, 2011,  415,  234,    6,   54,  359,
          108,  447, 2200, 2374,    0,    2,    1,    1,    1],
        [ 238,    0,  274,   85,  185,  801, 1081,    0,  185, 1081,   17,   24,
           11,   48, 1220, 6356,   62,    0, 1696,  140,  292, 4635,    0, 1696,
          140,  292, 4635,  172, 2244,    6, 4232,    0,    2],
        [  24,  144, 3181,    6,  148,  169,  367,    9,    0, 3636,  188,   79,
          322,  424,    0, 6108,    6,  132, 6686,    9,  415,  656,  292, 4117,
            6,  333, 2223,    0,    2,    1,    1,    1,    1],
        [ 115,   17,   19,   11,  121,  691,   17,   46,   19,   11,  158, 1945,
           21,    9,   13,  765,   46,   19, 1412,  450,   25,   17,   19,  217,
           39,   13, 2553,    0,    2,    1,    1,    1,    1],
        [  67,    7, 7552,   17,    7,  942,  188,  442,   10,    7, 1136, 1909,
           26, 4050,    0,    8, 1120,   39, 1670, 1478,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,   67,  218,  825,   53,   11,   57,    0,   86,    0,    8,    0,
           25,   66,  288,  427,  174,  541,  233, 1611,   55,    0,  341, 5204,
            6,    0,    2,    1,    1,    1,    1,    1,    1],
        [5385,   13, 3495,  998,    0,  860,   79, 7286,  109, 3717,    0, 2475,
         6414,    6,   21,    6, 4401,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  703,   17,    7, 1880, 5917,    6,   63,   13, 1412,   55, 1777,
           10, 6433,   21,    6, 2247,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33,  279,   73,  388, 4743,    3, 4997,    6,  690, 3350, 3298,   39,
         5414,    6, 3516,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    0,   53,  246,   10,    7, 5410,    6,    0,   38, 5034,    0,
          108,  415,   59,  338, 4482,    6,    0,   63,   39,    0,    0, 5441,
         1303,  424,   57,    0,    2,    1,    1,    1,    1],
        [   7,  288,    0,  279,   17,   53,   73,   11,   18,    0, 4585,   26,
            7,    0, 4129, 3585, 2908, 6649, 1241,   17, 1472,   12, 1719,  293,
          356,    0,    2,    1,    1,    1,    1,    1,    1],
        [ 120,   24,  162,  142,    9,    7,  595,  270,   10,    7, 8145,    0,
            0,   24,  162,  826,    0,  347,   34,   33,    0,   29, 1894,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   86, 5424,   62,    0,   17,   11,    6,   13, 1931,
            8,   21,   11,    6, 1897,   69,    7,  876, 6106,   22,  128,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   53,  144,   39, 1248, 3473,    0,  166,   34,   53,  465,   11,
           18,   66,   10,  229,  419,  839,  106,   21,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  229, 6730,    6,    8, 1560,  300,  901, 8018,    6,   12, 6027,
            0,   24, 3065, 3285, 6443,    0,   24,  135,   17,   24,   63,  230,
            0,    8,   53,   63, 1226,    0,    2,    1,    1]],
       device='cuda:7') tensor([22, 32, 26, 21, 26, 10, 19, 19, 31, 30, 33, 29, 29, 22, 27, 20, 19, 17,
        29, 27, 25, 25, 22, 31], device='cuda:7') tensor([1.0000, 0.8369, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8042, 1.0000, 1.0000, 1.0000,
        0.8306, 0.8911, 0.8936, 1.0000, 1.0000, 1.0000], device='cuda:7',
       dtype=torch.float16)
tensor([[  10,  388,   21,  133, 1948,    0,    7, 6730,   29, 1019,  188,  226,
           17,  103,   25,  508,   94,  890, 2273, 6053,    0,  103,   25,  508,
          134,  890, 5259,    0, 3226,  164,    9,   20,  879,   18, 3042, 2679,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   8,  391,  214, 3803, 1028,  132,   55,  333,  524,    0, 6664,    0,
         5691, 4140,    8,    7,   94,  148, 1202,  134,   46,    7, 3348,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   8,  120,   25,  175,   10,  227, 6085, 6106,    0,  204,    0,   21,
           73,   51,  250,   12,   39,   39,  816,  297,  424,  505,    0,  125,
           84,   63,    3,   94, 1074,   84,  148,   63,   86, 5607, 1719, 3475,
         1811,   25,   79,   25,   11,   57, 1028,  199, 3594,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [  13,  325,   12,   94,   71,  452,  265,  192,   11,   18, 1570,    0,
           67,   89, 1848,  465,   11,   18,  703,    9,   38, 3760,   11,   18,
            3,   89, 1970,   11,    6,   81,  587,   34,    0,   38, 1969,   73,
           87,   21,    0, 1586,   25,   73,   73,    3,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,   17,   26,  347,   19,  213,   10,  456,   10,   25,   80, 1729,
          290,  846, 2956,    0,  125,   19,  703,   24,   63,   13,  453, 8098,
            9, 8969,   54,   13, 2805,   55, 2394,  929, 4067, 3399,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  24,   11,  158,  229, 1123,   21,   11,    6, 3008,   48, 8289,    0,
           38, 9920,  159, 8460,    6,   66, 2107,  132,  248,   35,    0,  391,
           35,    0,  717,  181, 1178,    8,   53, 1531,   21,  126,    0,  125,
           21,   11,    6,    7,  245, 8370,   48, 2212,   53,   11,  121,  144,
            9,  159,  282,    0,    2,    1],
        [ 103,   25,  229,  143,    0,  254,    0,    0,  655,    3, 6479,    6,
           13,  464,    0,    9,    7,  832,    0,    6,    0,    0,    0,  419,
         2263, 1020, 2244,   25,   11,   57,  142,   10, 1064,  164,   66,  288,
            0,   13, 2768,    0,    0,    0, 2768, 4455,   69,  155,  244, 1021,
          238,   35,  242,   54,    0,    2],
        [9026,  131,   33,   89,  781,  266, 1466,    0,   89, 1751,    8,   19,
          144,   33, 3310,  479,    0,  339,   11,    6, 2033,  108,  743,   85,
           13,  341,  824,    6, 1090,    9,   13,  341, 1503,  545, 1303,   12,
          883,  424, 5089,    8, 1452,  251, 1352,   69,   13, 4813,    0,    2,
            1,    1,    1,    1,    1,    1]], device='cuda:3') tensor([38, 25, 47, 45, 37, 53, 54, 48], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8237, 1.0000],
       device='cuda:3', dtype=torch.float16)
tensor([[ 103,  185,   12,   70, 5070,    6,   94,    9,  108, 4621,   10,  229,
           77,   12,    7, 4388,   24,  229,  162, 3474,   62,   10, 4621,    9,
          166,   94,   66,  593,  555, 4387,    6,    0,   86,  288,  169,  251,
           94,   11,    6,  931,   51, 3480,   48,    0,   67,  108,    6,  169,
           51, 3480,   48,  113,    0,  166,   26,   70, 5767,    6,  583,   13,
           38, 1803,   20,  412,   35,  389,  927,  586,   54,  954,    3, 4232,
         1665,  365,  338, 1337,  793,  164,  229, 1459,  509,  518,   46,   86,
          116, 1714,   94,   46,  125,   12,  138,   77,   33, 9733, 2573, 1764,
          652, 1317,    6,  170,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,   73, 8794,   20,  111,  450,   25,   17, 1267,  411,  458,  128,
            0,  728,  158,   35,  287,    0,  775,   20,  727,   12, 9779,    0,
         1492,  156,    0, 3997, 4999,    0, 1747, 2142,    0, 5183, 1353, 1027,
            0, 1487, 3174,  388,  901,    8,   10,  424,  412, 1406,   20,   87,
           86, 3715,  359,   13, 6069,   17,   11,    6,  415,  233,   62,  132,
          850,  155, 1729,  502,   96,   17,   11,    6, 3463,   10,  367,  126,
           39,  109,  597, 2658,    9,  155, 1922,  119,    8,  851, 3304, 2946,
            7, 1956,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  29,    7,  753,    9,    7,  179,   71,    7, 6100,  837, 2694,    0,
            7,  800,   91,  753,   69,  837, 2694,   26,  264,  742,   20,  128,
          511,    0,   68,  386,   65,  238,  691,    0,  492,  226,    0, 1412,
          205,    0,   68,  194,   65,    7,  753,   71,    7, 1686,  837, 2694,
            0,   19,   11,   45, 4701,   10,  289,    0,   26, 2346,   48,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   7,  841,    7,  841,   12, 1057, 4311,  106, 2027, 1117, 1835,  126,
          199,    7,  203,  121,  237,  271,    0,   10,   66, 1565,   62, 1835,
           55,  250,   17, 4639,   10, 1109, 1102, 1117,   25,    8, 1019, 2816,
           25,    0,    8,   17,  434,   25,  270,    9,    7,  467,   10,    7,
          288, 2397,   25,  220, 2679,    0, 3045,   79,   25,  323,    0,    9,
          155,  883,  122,    6,   12,  570,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  24,  205,    9,    7, 7373,    6,   71,    7,  269,  290,   46,   24,
         2677,  159, 9060, 3183,   46,   21,   11,    6,  976, 1248,    0,   21,
          598,    6, 1968,    0,    8,  180,    0,  509, 1094,    0,   24,   11,
          121,  278,  108,  447, 3479,   12,  269,  290, 4656,    6,  737,  373,
            0,  108,  447, 3222, 2083, 1019, 1367,    0, 1339,  783, 2734,  741,
            0,  148,   73,  305,   13,  488,  269,  290,    8,    9,   91, 5563,
            0,  388,   21,  199,   39,   56,   15,  121, 3576,   20,   12,  563,
            0,   29,   17,   24,   73,  204,  283,   71,    7,  269,  290,    8,
         1082,    7, 4814,    6,   21, 1847,   10,   86,    9, 3196,   57,   33,
         1390,  148,  492,  150,    6,   13,  853,  569, 1020,    9,    7,  985,
         1580,    0,    2]], device='cuda:3') tensor([102,  88,  61,  67, 123], device='cuda:3') tensor([1., 1., 1., 1., 1.], device='cuda:3', dtype=torch.float16)
tensor([[3111,    7, 1732,  465,   11,   18, 2320,  923,   29,   51,   22,  236,
           22, 3903,    0,    8,  120,  225,    9,    6,  365,   62,   17,   19,
          150,  156, 2519, 2303,    0,   19,  316,  111, 1661,  297,   62,    0,
            8,  166, 4075,   48,   10,   51, 4113,  800,  248,    0,    2],
        [   8,   29,  103, 2439,   71,   13,  555, 2960,   12,  170,    9,   33,
          964,    0,   24,   73, 7647,   10,  826,   80, 7053,   54,    9,   33,
          341,  688,    0,  180,   24,   73,   16,  287,   17, 1417, 2041,   19,
           34, 6037, 1069,   10, 3928,    0,    2,    1,    1,    1,    1],
        [  89, 3965,  440,   26,   10, 1914,  126, 1004, 7806,    6,    8, 7750,
          694,  826,  199,   33,  488, 2469,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 101, 1559, 5990, 1323,    6,    0,  889,    8,  596,    0,  199,    7,
         2260,    0,    8,  101, 2700,  134, 3611, 1256, 2469,    6,   10,   66,
         1025,  101,   32,   96,  134,  132,   10,   13, 1116,  111, 8068,   29,
          101,   73,  150,  134, 4015, 3611,   62,    0,    2,    1,    1],
        [  53, 6960,   48,   17,    7,  207,   94, 5973,  185, 1760, 1361,   12,
         1573,    0,   34, 6027, 1490,    9, 1218, 6357,    0,    0,    0,   71,
            7,  207,   53,    9,    6,  365,   62,   69,  378, 5973, 1574,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115, 2283,    0,   79,   21, 2546,  436,    6,  270,    0,  138, 2117,
          155, 5897, 9023,    6,    0, 1239,  115,    0, 2283,   21,   79,   21,
         2546,  436,    6,  270,  554,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  432,   13, 2767, 9028,   12,    0,  155, 3612, 6385,  567,    0,
            7, 2116, 1149, 3320,   71,   13, 7069,  874,  266,    0, 1034,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 101,  591, 2869,  133,  528, 4506,    6,    0,    8,   29,   24, 6631,
         1011,   33,    8,   24, 2721,   17,   33,   26,   13,  207,   17,  172,
         1429,   55, 3101, 3001,   35, 1966, 1265, 4506,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33, 1321,  133,  341,  254,  419, 3839,   69,  984,   17,   19,   11,
          121,  700, 1110,    0,    8,   19,   11,  121, 1110,   13,  325,   12,
         3839,   69,  984,    0,  703,  110,    0,   68,  194,   65,  274,   85,
           33, 3189,    9,    7, 1403,    0,    2,    1,    1,    1,    1],
        [  21,   34,    7,  403,    6, 3211,   71, 3039,    0,   13,  140, 1355,
         3471,  187,   62,  131,    7, 4401,   12, 1519, 3473, 2551,   54,  336,
            7,  179,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   17,   11,    6,    0,  125,    7,  572,   26, 5802,   54, 2015,
         1428,   12,  214,   17,   63,    0,  415,   59, 8186,    9,  277,    0,
          765,    6,   12,    0,  183,    8,   17, 7010,    0,   10,   91,  486,
            9,  277,  765,    6,   12, 2786,  366,  183,    0,    2,    1],
        [  67,  244,    7,  501,  895, 1047,    6,    8,   11,    0, 1536,    6,
            0,    0,    7, 2008, 6248,    0, 1381, 1446,   53,  692,   10, 6896,
         4174, 1115,    0,    0,    8,   29,    0,   70,   53,  323,   34,    0,
           53,  487,   10, 2750,  596,  199,    7, 9824,    0,    2,    1],
        [   0,   13,  555,    0,    0, 2960, 2602,    0,  755,    0,    9,  383,
          741,    0, 1851, 1353,    0,    0,  109, 6893,    8, 1106,  233,  174,
          234, 1106, 2836,    0,   53,  144, 3695,   12,   70,   24,    0, 2807,
           10,   51,    7, 8968,   55, 2786,    0,    2,    1,    1,    1],
        [  19,   34,  133, 6011,  120,   19,  487,   10,   87,   33,   10,  150,
           17,    0,    9,  409,    0,  276,    7,   94,  148, 9235,   48,    7,
          368,   12, 1752, 2856, 1054, 5786,  131,    7, 3973,  144,   22,   11,
           18,  204,  691,   17,    0,    2,    1,    1,    1,    1,    1],
        [  21,   26,  108,  688,    8,   86,  108, 2072,  687,   17,  664, 2836,
           15,    6,  170,    0,   38, 3328,  583,   10, 2332,  440,   26,   17,
           24,  296,   10,  175,  244,  108, 1728,   12,  138, 2583, 9646, 1894,
           24,   73,   51,    9,  545,  218,   11,    6,  931,    0,    2],
        [  91,   12,    7,  214,   17,  956,  432,   17, 1064,   26,   17,   19,
          474,   80,    7,  218,  889,  148,  162,    9, 2536,  140,   37,  922,
         1585, 1763,   12,  110,    0,    8,  138,  261,   19,  692,   10, 1452,
           33,   71,  134,    0,    2,    1,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([47, 43, 20, 45, 37, 31, 25, 35, 43, 28, 46, 46, 44, 42, 47, 41],
       device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8789, 1.0000, 0.8970, 1.0000, 1.0000,
        1.0000, 0.8662, 0.8765, 0.8071, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
tensor([[ 115,    0,  101,   34,   13, 6791,  903, 3902,   18,  365,    0,    0,
            0,  101,   34,   13, 6791,  903, 3902,   18,  365,    8,  101, 5523,
           84,   26,   13,  785,   35, 1625, 1710,    0,  166,  188,   39,  811,
         4674,    0,   13, 8092, 2332,    0,   13, 1747,  389, 7988,    0,   13,
         2322,   54, 2332,    8,   13,   58, 5594,   20,  445,    0,  166,   26,
            7,  291,  371,  121,  810,  109,    7,  203,  779,  675,  793,   12,
            7,  546,    0,    2,    1,    1,    1,    1,    1,    1],
        [  25,  659,   86,  154,   12, 3139,  424,  233,   79,  378,  198, 1558,
          300, 1032,    0,   67,   55,  110,   21,   34,  198, 1558,  300, 1032,
            0,  125,   19, 4776,    0,  116,   79,   19,   34, 1106,   57,  119,
          810,   71,   33,  524,   46,   19,   34,  740,    9, 1319,   35, 7469,
           22, 1146,    0, 4402,    0,   85,    7,  183,   46,   19, 4776,   17,
          333, 1319,   35, 7469,   22, 2593,  958,  283,   37,   17,   19,   34,
          740,   71,  144,   13, 3139,  424,  233, 3078,    0,    2],
        [ 115,   19,   11,   45,  142,   10, 2797, 2117, 2869,  143, 3676,    6,
           17,  164,   51, 4501,    9,   56, 9338,  155, 1370,    8, 3557,    0,
          109,  103,   25,   11,   57, 4313,    0,  138,   25,  506, 8863,  155,
          447, 4313,    8,  415,  500, 1783,   54,   12,  963, 2245,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [2901,  245,    0,  180, 2455,    0,   24, 2859,   62,    0,  452,   48,
           35, 2917,  429, 1511,  244,    7, 3549,    0,  180, 2496,    6,  244,
            7, 1465,    0,  321,   12, 2285,    0,    8,  180,    7, 3901, 1583,
         7747,    0, 2901,    0, 2455,    0,   56, 7263,    0, 1228,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,  172,  916,  120,   25,  780,    0,    8,    0, 1531,
            0,   13,  207,   10, 2872,   13, 2872,  371,  340,   54, 3182, 1405,
          518, 2288, 1412, 4117,    6,    0,   68,  194,   65,    0,  339,  110,
          450,    0,   25,   17,   84,   11,    6,   86,  593,  294,    0,   94,
            0, 2091,    9, 2516,   54,    9,   17,    0,    0,   86,  276,    7,
            0,  427, 6034, 1167, 4445,    0,  148,   24,   11,   48, 5443,   80,
            3,  584,  759, 1086,   55,   85,   17, 2110,    0,    2],
        [  89, 4856,    0,  255,  121,    0,    8,   19, 1570,  244,   10,    7,
         1396,    6,   12,    7, 1930, 3449,  458, 4910,   10, 6682,   17, 3486,
           71,   29,  294,  218,   94,    0,    8,   19, 1608,   11,   18,  601,
           67,  154,  138, 1019,   24,  552,  336, 1979,  122, 1630,  230,    6,
            8, 1094,  138, 1019,   24, 1918,   10,  205,  336, 2787,   12, 6535,
          369,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,  220, 1393,  134,  132,  109, 1393,  134,  346,    0,   94, 4600,
          134,    0,   67,    9,    7, 5970,    6,    0,    7,  941, 3741, 7602,
            0,    8, 4597,  373,  487,  826,   80,  138,   10, 2554,  941,    0,
           29,   70,  956,    0,    7,   59,  626,    6, 3668, 5125, 1446,   10,
         2699,   13,  264, 1396,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,   73,  154,   12, 2932, 5743,    6, 1645,   79, 1291,   22,  241,
          505,    0,  746,   12,   39,  985,   35, 4797,  279,    0,  230,    0,
           68,  194,   65,  206, 1288,  106,   91,  723,   73,   51,  415,  777,
           62,    8, 3411,  652,  922, 1004,    7, 1711,  567,    0,   17,   84,
           73,   51, 1986, 1181, 3479,    6,   12, 2932,  896,    0, 3188,   54,
           69,    7, 2353,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:6') tensor([76, 82, 48, 48, 82, 63, 54, 65], device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8784, 1.0000, 1.0000, 1.0000],
       device='cuda:6', dtype=torch.float16)
tensor([[   0,   19,  144,    7,    0, 2212,   10, 2115,  336,    0,    7, 1291,
           45,  603,  603,    0,  427, 2984,  430,    9, 2008, 1146,   71,    0,
           39, 3698,    9, 1031, 1967,  541, 3379,    0,    2,    1,    1,    1,
            1,    1],
        [   8,   12,  538,    0, 1042,   25,   66,  264, 1893,    0,  276,    7,
          801, 6878,    6,    0,  276,  155,  637,  873,  250,  341,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,    7,    0, 1043,  159,  813,  188,  419, 2070,   26,    0,    0,
          238,    0,  125,  211,   91,  994,  188, 2214,   10,   21,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   0,    0,    8, 3108,  144,  246,    9,   39, 3836,   17,   24,  451,
            0,  172,  875,    0,  250,  264,    0,   10, 1366,    0,  250,   17,
           24, 2775,   11,   18, 3498, 2307,    0,    2,    1,    1,    1,    1,
            1,    1],
        [  69,   33, 1472,   12, 1780,   26,    7, 1261,    7,  207,   19, 1385,
           21,  106,   89, 1067,  971,  455, 1431,    8, 1037,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,  108,  630,  172,   34,    0,   87,  108, 1752,  352, 4041,    6,
         7420,  138,   24,  154,    8,  598,   80, 1469,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 169,  378,   13, 1396, 4988,   48,  106, 7801,   55,   13,  555, 4299,
          131,  378, 4744,  131,   10,  861,  229,   13, 1878,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,  101, 2770,   33,  567,    0,   67,   21,  172,  465,   11,   18,
          283,    0,  125,  284, 7393,  162, 5785, 4777,   54,  545,  218,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  19,   11,  121, 1621,   13, 2695,  434,   38, 2366,   10, 1303,  197,
            8,   19,  703,   21,   11,    6,  142,   10,  468,    7,  207,   25,
          274,   85,    7,  179,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1],
        [2993,   51, 7989,    0,   38,  511,  180,    0,   12,  538,    0,    7,
          558,  279,   19, 1510,   26,    3,   19,   11,  121,  442,   21,   10,
            7, 1219,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,  103,   25,   11,   57,    0,    9,    7,  417, 1236,  556,  240,
          589,    0,    0,    0, 1052, 8374,  371,  626,    0,   25,   11,   57,
          142,   10, 2034,   13, 1335, 1273,  303,   18,  434,   13, 2973,   93,
            0,    2],
        [  29,   12,  538, 1976,   25,  192,   11,   18,  172,   87, 1835,  689,
           11,   18,  172,  175,  691, 8289,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  79,   13, 5425,   59,    0,   19,   34, 8784,   54,    0, 5184,    0,
            8,   19,  692,   10,   51,   39, 2420,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,  276,  115,    0,  432, 1450,  215,    0,   53,   11,   57,   86,
          132,   84,    0,   53,   66,   86,  203,  699, 2633, 1181, 1501,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 103,   25,  204,  274,   85,   13,  453, 1201,    0,   25,   11,  158,
         4712,  735,  150,   77,   12,  117,  391,  214, 1028,  199,  722,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 719, 6382, 8193,    6,   17,  811,  265, 1782,    0,   94,   10,    7,
         4620,    6,    0,   12,  159, 2332,    6,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,    7, 5410,   26,  172,    0,  101,  954,    6,  199,    7, 3140,
           12,  378,   39, 9005, 1198,   12,   77,   12,  117, 6710,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,   34,  391, 1113,    0,    0,  717,    0,  110,  128,    6,    0,
            8,   85, 5826,   69,  227, 2124, 2366,    0,    0,    0,   24, 5623,
            7,  501,  887,  895,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,  665,   85, 1228, 6529,    6, 1850,   62,  131, 5543,    6,    0,
           13,  179,   12, 5407,  985,    6,  132,   55,  565,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  24,  144,  521,  870,   45,    6,    9,   77,    7,  203,  181,   59,
         1159,  866,    6,    9,    7, 6870,    6,    8,    7, 3061,    0,  125,
         8939,   97, 1203,  551,    6, 7955,    0,    2,    1,    1,    1,    1,
            1,    1],
        [  29,  154,   80,    0,   17,  765,    0,  270,    9, 8085,  584,    0,
            0,  554,  206,   13,  664,   37, 1327, 1563,    0, 3848, 1505,   13,
         1366, 1251, 3848,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 115,    0,  281,   12,    7, 1583,    6,   84,   63,   86,  744,   35,
         2514,   85,   77,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   9,  409,    0,  108,  288,  818,   12, 3802,  169,   51,  359,    7,
          801,   35,  181, 3488,  369,   62, 1850, 2100,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  29,    0,    7,  207,  117, 2032, 2261,    0,    0,    0,  162,  691,
            0,   84,   11,    6,  226,   13, 1017,   12, 4795,    6,    0,    8,
           33,    0,   26,  206,   25,  175,   10,  150,    0,    2,    1,    1,
            1,    1]], device='cuda:5') tensor([33, 24, 24, 32, 23, 22, 23, 25, 30, 28, 38, 20, 21, 25, 25, 21, 24, 30,
        23, 32, 29, 17, 22, 34], device='cuda:5') tensor([0.8315, 1.0000, 0.8818, 0.8154, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8682, 1.0000, 1.0000, 1.0000, 1.0000, 0.8813, 1.0000, 0.8257,
        1.0000, 1.0000, 0.8022, 1.0000, 1.0000, 0.8340], device='cuda:5',
       dtype=torch.float16)
tensor([[ 125,   70,   25,  296,   26,    7, 3189,    0,   86,    7, 7619,    0,
           68,  194,   65,   68,  386,   65,   29,  347,  192,   11,   18,   25,
          203,   22,   18,    7, 7619,    0,  109,    0,  276,  509,    0,  203,
           22,   18,  126,  155,  447, 7619,   10,  218,   94,    8,  229,  185,
          839,  106,   21,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,  103,   25,   87,  296,   10,  456,   80,  250,    0,   25,   73,
         1503,   21,    9,   13,  207,   17, 2700,   25,  211, 2693,  340,  181,
         1625,  369,    0,  860,   79,    0,   38,  187,  172,  213,   10, 1202,
           33, 1943,  411,  715,   22,    0,   29,   19,  296,   10, 2365,  785,
          825,   13, 1822,    8, 6015,   89,   79,    6,  103,   19,  192,   11,
           18,    0, 1189,    3,    2,    1,    1,    1,    1,    1,    1],
        [ 432,  392,  215,    9,  749, 3333,  399,    0,  106, 1553,  886,  371,
          322, 1921,  109,  106, 1553, 2792,    6,   10, 4100, 2353,  128, 3753,
            6,    0,   24,   11,  121, 1110,   17,   94,  213,   10, 1202,   55,
           13,  509,  858,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  103,   24,  487,  665,  199,   17, 2753,  813,    0,   24,   11,
           48,  150,   13,  800,   12,  170, 1522, 7777,    6,  126,   84,   46,
           12,  538,    0,   13,  325,   12,  422, 2753,  813,    0,   67,  113,
         3343,  237,    8, 8430,  813,    0, 4402,  106,  214,   17,   63, 1501,
         3919,  982, 1241,  155,  211,    6,   20,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,   77,    7, 2826,   25,  150, 2884,   62,  168,    0,   24,  154,
           84,   11,    6,   13,  453, 2212,   55, 6904, 1740,    6,   10, 6757,
            9,   33, 9648,    0, 2695, 1522,  111, 4112,   93,    8, 3066, 7806,
           12, 7016, 3001, 5605,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   53, 5164,  248,   12,  134,  132,    0,    8,   53,   66,  248,
          205,   59,  247, 1032, 1830,   35, 1859,   35,  820, 1047, 1103,  849,
            6,  693,   12, 7097,  214,   10,  134,    0,    8,   24,  289,    0,
           38,  200,  176, 1328,   91,  169,   25,  100,   10,  508,  132,    3,
           38,  187,   66,   10,  508,   91,  132,    3,   38,   93,   96,    0,
           24,  296,   91,   79, 2808,   12,    7, 2092, 1039,    0,    2],
        [  77,   17,   19,  217, 5290,    6,  106,  120,   19,  278, 3298,   13,
         2365,    9, 1051,   15,  760,    0,  461,   12,    7,  321,  762, 1486,
            6, 1411,   17, 6609, 4712,    3, 8867,  708,  106, 1122,  878, 2627,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,  294,    0, 2076,   15,    6,    0,    0, 2638,   18,  810,    0,
         9344, 2544, 4513,    0,  159,  205,   35,  412, 5492,   10, 1661,   15,
            6,  436,   26,  521, 2391,   54, 1335, 1336,  480, 1611,    0,    0,
           12,  682,  649,  539,   20,    0,    9,    7,  774,   12, 5668,  187,
         2084, 1371,  241,  140,  609,  679,    6,    0,    0,  109,  941, 4058,
            6,    8, 3489,    6,    0,    2,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([53, 65, 41, 57, 42, 71, 38, 66], device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8560],
       device='cuda:0', dtype=torch.float16)
2023-08-15 15:11:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-15 15:12:10 | INFO | train_inner | epoch 001:    105 / 1474 loss=20.052, trans_loss=5.872, nll_loss=4.68, w2v_ctc_loss=22.339, task_loss=1.776, contrastive_loss=3.263, total=4211.8, n_correct=124.71, ppl=25.64, accuracy=2.961, wps=16622.4, ups=1.32, wpb=12567.1, bsz=472.7, num_updates=100, lr=4.098e-06, gnorm=2.857, clip=0, loss_scale=4, train_wall=86, gb_free=19.4, wall=149
2023-08-15 15:13:22 | INFO | train_inner | epoch 001:    205 / 1474 loss=16.604, trans_loss=5.861, nll_loss=4.693, w2v_ctc_loss=17.097, task_loss=1.711, contrastive_loss=3.236, total=4114.86, n_correct=114.75, ppl=25.87, accuracy=2.789, wps=16926.1, ups=1.38, wpb=12286.8, bsz=458.8, num_updates=200, lr=8.096e-06, gnorm=7.239, clip=17, loss_scale=4, train_wall=72, gb_free=19.3, wall=222
2023-08-15 15:14:33 | INFO | train_inner | epoch 001:    305 / 1474 loss=9.937, trans_loss=5.859, nll_loss=4.729, w2v_ctc_loss=6.899, task_loss=1.666, contrastive_loss=3.175, total=4080.91, n_correct=108.34, ppl=26.52, accuracy=2.655, wps=17149.6, ups=1.41, wpb=12190.4, bsz=439.4, num_updates=300, lr=1.2094e-05, gnorm=2.353, clip=0, loss_scale=4, train_wall=71, gb_free=18.5, wall=293
2023-08-15 15:15:42 | INFO | train_inner | epoch 001:    405 / 1474 loss=9.425, trans_loss=5.815, nll_loss=4.709, w2v_ctc_loss=6.116, task_loss=1.43, contrastive_loss=3.207, total=4176.41, n_correct=96.6, ppl=26.15, accuracy=2.313, wps=18056.6, ups=1.45, wpb=12470, bsz=461.3, num_updates=400, lr=1.6092e-05, gnorm=1.297, clip=0, loss_scale=4, train_wall=69, gb_free=19.4, wall=362
2023-08-15 15:16:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-15 15:16:59 | INFO | train_inner | epoch 001:    506 / 1474 loss=9.217, trans_loss=5.75, nll_loss=4.645, w2v_ctc_loss=5.808, task_loss=1.278, contrastive_loss=3.3, total=4180.13, n_correct=94.24, ppl=25.02, accuracy=2.254, wps=16314, ups=1.31, wpb=12491.1, bsz=487, num_updates=500, lr=2.009e-05, gnorm=1.068, clip=0, loss_scale=2, train_wall=76, gb_free=19.1, wall=438
2023-08-15 15:18:08 | INFO | train_inner | epoch 001:    606 / 1474 loss=9.088, trans_loss=5.776, nll_loss=4.678, w2v_ctc_loss=5.639, task_loss=1.241, contrastive_loss=3.254, total=4137.35, n_correct=84.93, ppl=25.6, accuracy=2.053, wps=17819.3, ups=1.44, wpb=12337.2, bsz=474.5, num_updates=600, lr=2.4088e-05, gnorm=0.93, clip=0, loss_scale=2, train_wall=69, gb_free=18.7, wall=507
2023-08-15 15:19:19 | INFO | train_inner | epoch 001:    706 / 1474 loss=9.006, trans_loss=5.802, nll_loss=4.711, w2v_ctc_loss=5.563, task_loss=1.311, contrastive_loss=3.128, total=4145.85, n_correct=77.35, ppl=26.19, accuracy=1.866, wps=17496.7, ups=1.41, wpb=12381.9, bsz=454.4, num_updates=700, lr=2.8086e-05, gnorm=1.215, clip=1, loss_scale=2, train_wall=70, gb_free=19.4, wall=578
2023-08-15 15:20:31 | INFO | train_inner | epoch 001:    806 / 1474 loss=8.911, trans_loss=5.921, nll_loss=4.854, w2v_ctc_loss=5.358, task_loss=1.271, contrastive_loss=3.136, total=4129.2, n_correct=64.38, ppl=28.91, accuracy=1.559, wps=17052.5, ups=1.38, wpb=12318.4, bsz=463.3, num_updates=800, lr=3.2084e-05, gnorm=1.232, clip=0, loss_scale=2, train_wall=72, gb_free=19.2, wall=650
2023-08-15 15:21:41 | INFO | train_inner | epoch 001:    906 / 1474 loss=8.754, trans_loss=5.969, nll_loss=4.907, w2v_ctc_loss=5.136, task_loss=1.291, contrastive_loss=3.042, total=4167.97, n_correct=58.68, ppl=30.01, accuracy=1.408, wps=17907.1, ups=1.44, wpb=12446.3, bsz=458.8, num_updates=900, lr=3.6082e-05, gnorm=1.504, clip=0, loss_scale=2, train_wall=69, gb_free=18.5, wall=720
2023-08-15 15:22:50 | INFO | train_inner | epoch 001:   1006 / 1474 loss=8.635, trans_loss=6.075, nll_loss=5.04, w2v_ctc_loss=4.881, task_loss=1.292, contrastive_loss=3.035, total=4137.5, n_correct=46.4, ppl=32.9, accuracy=1.121, wps=17915.5, ups=1.45, wpb=12361.1, bsz=459.1, num_updates=1000, lr=4.008e-05, gnorm=2.013, clip=0, loss_scale=2, train_wall=68, gb_free=19.3, wall=789
2023-08-15 15:23:58 | INFO | train_inner | epoch 001:   1106 / 1474 loss=8.458, trans_loss=6.127, nll_loss=5.099, w2v_ctc_loss=4.673, task_loss=1.327, contrastive_loss=2.943, total=4151.84, n_correct=46.77, ppl=34.28, accuracy=1.126, wps=18164.9, ups=1.47, wpb=12382.4, bsz=452.7, num_updates=1100, lr=4.4078e-05, gnorm=2.015, clip=0, loss_scale=2, train_wall=68, gb_free=18.7, wall=857
2023-08-15 15:25:06 | INFO | train_inner | epoch 001:   1206 / 1474 loss=8.27, trans_loss=6.092, nll_loss=5.058, w2v_ctc_loss=4.513, task_loss=1.383, contrastive_loss=2.846, total=4123.25, n_correct=61.37, ppl=33.31, accuracy=1.488, wps=18089.2, ups=1.47, wpb=12316.7, bsz=437.7, num_updates=1200, lr=4.8076e-05, gnorm=2.575, clip=0, loss_scale=2, train_wall=68, gb_free=19.6, wall=925
2023-08-15 15:26:16 | INFO | train_inner | epoch 001:   1306 / 1474 loss=8.13, trans_loss=6.104, nll_loss=5.072, w2v_ctc_loss=4.345, task_loss=1.302, contrastive_loss=2.791, total=4066.16, n_correct=60.8, ppl=33.65, accuracy=1.495, wps=17448.2, ups=1.44, wpb=12138.7, bsz=445.8, num_updates=1300, lr=5.2074e-05, gnorm=2.453, clip=0, loss_scale=2, train_wall=69, gb_free=18.7, wall=995
2023-08-15 15:27:24 | INFO | train_inner | epoch 001:   1406 / 1474 loss=7.982, trans_loss=6.09, nll_loss=5.062, w2v_ctc_loss=4.195, task_loss=1.324, contrastive_loss=2.863, total=4119.98, n_correct=62.1, ppl=33.4, accuracy=1.507, wps=17855.9, ups=1.45, wpb=12311.1, bsz=449.5, num_updates=1400, lr=5.6072e-05, gnorm=2.391, clip=0, loss_scale=2, train_wall=68, gb_free=18.5, wall=1064
2023-08-15 15:28:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([[  39, 5410,  148,   11,    6, 5184,  134,   71,   13, 2705,    8,   13,
         1780,  169,  914,  305, 1345,    0,  109,   21,  169,  305,  276,   13,
          464,   10, 3152,   77,    7, 7035,    6,    0,   77,   12,    7, 1362,
           20,  586, 1428,    0,   25,   73,  288,  719,  250,  100,   33,  359,
           39, 4516,    0,    2,    1,    1,    1,    1],
        [ 391, 6730,    6,   63,    0,  442,  131,    7, 7832, 1880,    0, 8789,
           12,  108,  183,    0,  860,   13,  567,   26,    0, 5317, 1490,   56,
         2687,  494,    0,    0,    0, 1880,  111, 6083,    0,    8, 2645,  111,
            0,   19,  237, 1875, 3027, 3147,    0,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [5832,   17,    7, 5347,  119,  572,  492,  934,   48,   21,    6, 3764,
         2070,    9, 1221,    0,  635, 4586, 1183,  994,  106,   70,   21,   26,
           53,   11,   57,   86,  923, 1123,   19,   66,    0, 5352,  249,   54,
            0,   17,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [1953, 3619,  215,  581,    0,    7, 2048,    8,  931,   18, 1994,    8,
         3380,    6,  162,   86,  276,   91, 1953,  322,   12,   91,  439,    8,
         2220,   11,   18,  276,   66,  226, 7548,   69,  860,   13, 5851,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,  875,    7,  392,    0, 1074,    8, 1244,   35,   45, 2147,
           54,   51,   22, 1150,    6,    9,    7, 7996,  964,    0,    0,    0,
         3570,    0,    0,   34,  116,   51, 1763,  110,    0,    8,   34,  100,
            0,   38, 2045,   45,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  86,   10, 6903,    0,   84,   11,    6,   13, 4505, 5656,  142,   69,
            0,   29,  276,  103,   25,   87, 5032,  250,    0,    7, 5168, 6142,
            8,  832,  140,   51,   59,  636, 2876,   63,    9,   33, 2285, 4505,
         5656,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   24,  323,  446,    0,    0,   17, 4764, 1674,   18, 6051, 1101,
            0, 4198,    6, 6478, 5933,    6,    0,    0,    0,    0,   13, 1118,
           15,  266,    8, 3222,  292,   45,  996,   46,   13, 1118,   15,  266,
            0,   85,  747,  890, 4266,   10,   66, 1167,   35,  140,  904,   54,
         4608,    0,    2,    1,    1,    1,    1,    1],
        [  29,    7,  942, 1757,  706,    6,    0,   53,  175,  540,    8,   53,
         1531,   53,   11,   57,  142,   10,  468,    7, 1187, 2008, 3612,   10,
          229,   21, 2015,  250,  994,    0,   29,   53,  468,   21,   10, 2008,
         1633, 7861,    0,  100,   33,   26,  142,   10, 2828,   70,   11,    6,
          172,  142, 1226,    9,    7,  942,    0,    2]], device='cuda:4') tensor([52, 45, 40, 37, 42, 39, 51, 56], device='cuda:4') tensor([1.0000, 0.8242, 1.0000, 1.0000, 0.8584, 1.0000, 0.8413, 1.0000],
       device='cuda:4', dtype=torch.float16)
tensor([[ 339,   11,    6,    0, 3522,    0,    0,    7,  179,   77,  336,  170,
           71,   33,  264, 7260,    0,    8,    0,  661,    7, 1192,  179,  106,
           13, 4149,  264, 3064,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   13,  277,  523,  143,  567,  411, 1181,    0,   89,
         5199,   26, 6224,  660, 1740,    6,    9, 2406,    6,   12,   77,  214,
            0,   29,   19,  135,   13,  277,  523,   80, 2567,   13, 8192, 1422,
            0,    2,    1,    1,    1,    1,    1],
        [   8,  103,   21,  144, 3548,   48,    0,    7,  858,   12, 6895,  169,
          914, 1220,   51,  168,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 225,  246,    0,   38, 3127,   21,   11,    6,   86, 3718,    0,  125,
           84,   26,  288,   91,  282,    0,    8,   84,  451,  288,   51,   91,
         1723,    3,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [1732, 1107,    0,   77,    7,  126,  603,  234,   54,   12, 3429,  369,
            8, 6408,   17,  552,  106,  108,  753,   34,  250,  172,   17,  164,
         5258,    0,  700, 1917,   71,  110,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,  434,   56, 1949, 3287,    9, 8393,    0,  206,   19,
         5747,   21,    0,   21,   11,    6,  434,   81, 1027,  372,  168,   69,
            7, 3156, 4710,    0, 1031,  287,    9, 5712,    0,   29,  606,  994,
         6228,    0,    2,    1,    1,    1,    1],
        [  29,  103,   24,  274,  336,  554,    0,   84,   63,   80,  717,   94,
          499, 3642,    0,    8,  204,   19,   11,   45,   86,  142,   10,  388,
           25,   69,    7, 4057,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [1250,   12,   13, 8690,    0,  116,   33,  225,  265,  560,   48,   11,
            6, 7304,    0,    8,  101, 1119,   46,  101,   11,    6,    9,    6,
          300, 1011,   46,   38,  727,   19,   13, 2918,   17,   25,  169,  367,
           10,  110,   71, 3020,    6,    3,    2],
        [  24,  113, 5332,   62,  131, 3158,  959, 6037,  856,  502,    7, 2100,
           12, 9380, 8428,    6,   55,    7,  942,   12,  264, 1736,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,   12,   77,    7,   94,    6,   17,   19,   11,  121,  700,
          226,   71,    0,    7,  281, 3586,   63,    7, 1097, 2415,   12,    7,
           43,   59,  371, 1644,  827, 1151,  384, 1954,  455, 1943,  455,    9,
         6869,  415,  237, 4599,  407,    0,    2],
        [  19,   34, 4022,  215,  801,   85,    7,  183,    0,    8,   19, 1425,
           17,   17, 3463,   24,  144,   13, 3286, 1565,   12, 1057,   13, 1269,
           71,   13, 4460,  384,  616,  140,   18,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  19,    0,  278,   10,   87,   70,  294,  596,   66,   46,    7,    0,
          728, 3174,   69,    7,   51, 3174,    0,    8,    7,    0,   38,  174,
          195,  195,  195,  195,    3,   89, 3780,    0,  552,   71,  110,    0,
            2,    1,    1,    1,    1,    1,    1],
        [  29,    0,    7,  245, 1017,   12,    0, 2296, 3403,    6,    0,   53,
          467,  132,    0,    9,    7,    0, 3020,   81,   69,  284,    0,   19,
         1218,    0,   67,    7, 1590,   12,    0,  134,   63,  172,   70,   11,
            6,  528,  168,    0,    2,    1,    1],
        [  67,  120,   24, 2982,   10,   94,    0,   21, 2947,   62,   17,   13,
         2079,   22,  356, 2958,  144,  956,    9,  117, 6786,    6,   77, 1004,
            7, 1384, 1224,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [1436,   59,  760,    8, 1546,  607,  322, 1873,   17,  145,   37,  144,
          226,   79, 1226,   80, 7103,  378, 4733,   79,  101,  144,  226,   80,
         1848, 7993,   21,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,    0,    7, 1793,  958, 1006,  567,    0,    0,  188,  143,
            0,  254,   21,    6, 3718, 1452,   12,   56, 1251,    6, 2470,   22,
          140,  793,   46,   10, 4223,   21,    6,    0,  862,  233,  297,  833,
            0,    0,   10,   51, 1123,    0,    2]], device='cuda:7') tensor([30, 38, 18, 27, 32, 39, 30, 43, 24, 43, 33, 37, 41, 29, 29, 43],
       device='cuda:7') tensor([0.8560, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8857, 0.8438, 1.0000, 1.0000, 0.8413],
       device='cuda:7', dtype=torch.float16)
tensor([[   9,   56,  525,   11,    6,    0,  179,    0,    7,  415, 1040,    0,
         4925,    6,  144,   13, 6149, 9417,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [2129,   21,  689,   11,   18, 1620,   85,   77,    0,  109,   21,   11,
            6,  250,  994,    0,   13, 1192, 1810,  109,  185,  255,   45,   22,
         1974,  416,  279,    0,   67,    9,  419, 1165,   21,   11,    6,   86,
          461,   12,  948,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   80,   39, 1759,   35, 6937,   35, 7596, 6069,   12,
         5933,    0,   21,   11,    6,  278,   13, 2960,  109,   29, 3189,    6,
           69, 1219,    0,   69,   17,  926,   26,    7, 6342,    0,    8,  168,
           26,  185, 2260, 2852,  174,   54,    0,    8,   21,   11,    6, 3364,
           10,   33, 7373,   12, 3411,  221,   20,    0,    2],
        [ 554,    0,   85,   80,  423,    3,  215,   79,   13, 5030,   35,  181,
         2482, 2361,  170,    0,    8,   24,  288,  859, 1146,   80, 1992,    3,
          215,  581,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 138,  323,  101,   86,  135,   46,   21,   11,    6,   70,   24,   11,
          121,   77,   55,  606, 2490,   46,   17,   24,   11,   57,   86,    7,
          245, 1649,   10,  991,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 339,  110, 7032,   25,   10,   89, 4149,  264, 1039,    0,   38,  712,
          287, 1090,    9, 3164,    3,  166, 1505,   13, 3213,   15, 3634,  281,
         2523,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 432, 1655,   12,  203,    6,  856,    6,    9,  203,  372,   93,    0,
            7, 1269,   34,  591,    0,    8,   24, 6853,   62,    7,  203,  500,
          369,   12,    7, 1037,  359, 2481,  174, 1434, 1395,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 109,  305, 6908,    0,  440,    0,  149,    6,   96, 1744,   39, 2313,
           12, 2199, 1601,  690, 3474,  665,   55, 2519, 6629,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  71,   33,    9, 1066,    0,   24, 2534,   13, 1501,  264, 6129,  567,
           10, 1766,    7, 3708,    9,  419, 3440,   12,  672,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 103,   25,  274,   85,    7, 1912, 1375,    0,    7, 2384,   71, 1962,
         4266,   12, 5894,    0,   25,   73,  150,    9,  251, 2384,    0,  204,
            0,    7,  800,   12, 4224, 3883,    6,   26, 2585,   57,  484,   54,
           13,  325,   79, 3538,  909, 2460, 1068, 1143,  132,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  180,  185,   94,    9, 7419,  552, 1585,    8,   53,  246,    0,
           38,  160, 2402, 5448,    0,    7,  134,   20,   12,    7, 7419,  422,
         1611, 9797,   26,  913, 6811,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1239,    0,   19,  213,   10,  575,   25,  321,   12,   13, 7306, 6143,
            0,   67,    9,   21,   26,   89, 7143,   71,   70,   11,    6,  142,
           10, 1085,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  79,   25,   73,  934,    0,   19, 2138,   13,  325,   12,  183,   85,
         8145, 3069,    0,   68,  194,   65,   89, 3780, 2220,   11,   18, 2744,
            6,    6,  110,   55,    7,  473,  248, 1345,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 1321,  453,    0,    8,   25,  169,  205,  199,   13,  384, 1838,
          436, 3084,    8,   77,   12,   13, 5827,  150,   17,   69,    7,  225,
         1591,    0,    8,   21,   11,    6, 1968,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 7602,  110,   17,   21,  169,   66,  226, 3643, 3296,    0,   55,
          728,    6,  335,   10,  367,  270,  554,    8,  554, 2219,   69,    7,
          341,  183, 5331,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 276, 1445,   24,   11,  121,  367,   10,   33,  106,  341, 3773,    0,
           24,   77,   66,   10, 1529,   69,  108, 2410,   10,  468,    7,  207,
           17, 1459,  154,    6,   80,  896,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([20, 41, 57, 28, 30, 27, 35, 23, 23, 47, 31, 28, 34, 33, 30, 32],
       device='cuda:5') tensor([0.8584, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:5', dtype=torch.float16)
tensor([[  29,    0,   25,   11,  121,  278,   33, 2196,    0,    9, 7194,    0,
           24,  144,   39, 1980,  852,   22,  820, 4747,   17, 3793,   62,  106,
         4435,  199, 2048,    9,   13,  555, 8347, 3885,   71,   39, 1719,  293,
           15,   18, 7347, 1882,   12, 1992,  439,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,   73,  150,    7, 8639,   12, 1146,    9,    7,  245,  555,
         1598,    6,  168,    0, 2247,   34, 3600,    0,   94,  162,  204,  893,
         1714,   37,  254,  159, 1848,    0,    8, 1275,  276, 1714,   37,  254,
          159, 9703,  144,  226,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,  125,   85,    0,    7,  467,   12,    7,    0,  383,    0,   70,
           24,   66,   10,  575,    0,   26,   17, 2394,   71,    0, 8307,  941,
           26,  324,   55,    7,   94,    0,   55, 1729,  290,   56, 1775,  221,
            6,    0,   17,   63,    0, 3754,  440,    8, 2603,   55,  251,  148,
            0,    0, 2775,   11,   18,  226, 2188,    0,    2,    1,    1],
        [  29,    0,  117,   63, 5168,    6,  620,  307,    6,    0, 1579, 1688,
         3635,    6,    0, 5588,    6,  162,  378,  144,   77,   80,   33,  283,
            0, 1819,    6,  162,  378, 4598, 2871,  490,   94,  144,  204,  144,
            7,  274,   85,    7,  283,    0, 5911,    6,  162,  893, 3006,   46,
           77, 3868,   12,  214,   46,  453,  906, 1386,    6,    0,    2],
        [ 294,    9,   89, 2042,   46,  125,   12,  238,   35,  160,  191,  793,
           62, 5370,   54,    8, 1244,   35,   20,  426,  510, 1370,   46,  162,
          837, 1181,   10,  703,   17,   24,  162, 1986,  277, 5589,  181,  372,
          636,    6,   46,   68,  194,   65,  148,  162,  142,   10,  205,  126,
            8, 2554,    7,  179,    0,    2,    1,    1,    1,    1,    1],
        [ 168,   63,    7, 1219,  785,  214,   17,   94,   71, 1850,   35,  587,
          241, 5188, 2247,  289,    0,   38, 3328, 5114, 1611,   66, 1781,    3,
           38,  187,   11,   45,   86, 4493,   10,   87,   70, 1073,  110, 1767,
            3,   38,  187,  598, 4616,   10,   89, 1431,    8, 1037,    3,   38,
          187,  661, 1222,  509,    0,    2,    1,    1,    1,    1,    1],
        [  21,  442,  110, 2743,  103,   84,  220,   51,   13,  509,  207,   46,
           13,  207,   10,  452,  551, 1236,   45,  121,   22,   18, 1723,    8,
         1094, 3067,    7, 4102,   12,  282,   17,  506, 8291,  111, 2302, 2518,
           12, 2116, 6683,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,    7, 1377,   12,    7, 1009,   26,   17,   21, 1321,   85,
          665,   46,   19,  641,   17,   11,    6,   21,    6, 7202, 8069,    0,
         1102,   21,    6, 1810,    8,   21,    6, 9337,  521,  430,  235,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:1') tensor([45, 42, 57, 59, 54, 54, 41, 37], device='cuda:1') tensor([1.0000, 1.0000, 0.8008, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:1', dtype=torch.float16)
tensor([[  67,    0,   21,  188,   10,    0,   51, 7118,    0,   29,   19,   11,
          158,  456,   80, 4312, 1098, 1994, 7289,    0,   68,  194,   65,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1057,  453, 4997,    6,   26,   86,  890,    0,    8, 1094,   24,   11,
          121,  226, 8160,    9, 3023,   71, 4997,    6,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   26,  324,   10,   51, 1751,  111,    0,    8,   21,   11,    6,
          324,   10, 1082,  120,   86,   10,   51,    0,   67, 3695,   12,   17,
          818,   24,   66,   10,   51, 4493,    0,    2,    1],
        [  21,   11,    6,   13, 8308,    0,    8,   25,  154,    0, 8308,    0,
         1768,    0,   67,   33,   34,   86,  116,  419, 8308,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7,  245, 1289,   17,   19, 4415,  162,    9, 6062,    0, 1088,
            0,  140,    0,    0,   69,    7,  423,  322,   39, 2227, 1305, 1020,
           12,  984,  383,    0,    2,    1,    1,    1,    1],
        [   7,   94,  148, 5523,    9,    7, 1106, 2836, 3460,    6,   11, 2484,
         1639,   71,  134,   71, 2011,    8, 7889,    8, 3918, 1118,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 682,    0,   19,  641,    0,   70,  169,   21,  305,   10,    0,  690,
            6,  241, 2427,  108, 1507,    0,   10,  346, 7378,   21,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 440,   19,  213,   10,  521,  616,    6,    6,  250,   10,   25,    0,
           67,  245,   12,   77,   19,   11,   45,  142,   10,  884,   25,   13,
         1323,   12, 1532,    0,    2,    1,    1,    1,    1],
        [  55, 4212,    0,   89,  277, 3460,    0,  101,   11,    6,  133, 7069,
          111,   13,  387, 2459,    0,  101,   11,    6, 1752,  352, 4041,    0,
          101,   73,   11,   18,  456,   85,   77,    0,    2],
        [   8,  120,   25, 2215,  251, 2636,  277, 4059,    9,  155,  874,    0,
          108, 1329,   26,   86,   10,  289,    0,   38, 5034,   85,  267,    0,
          225,   11,    6, 2636,    0,    2,    1,    1,    1],
        [  21,   11,    6,   86,  116,   17,   25,   11,  158,  492, 3121,  155,
         1832,  307,  109,  155, 1890,    6, 3903,    0,  109,  155, 1269,  120,
           25,   11,   57,   85,  838, 2739,  713,    0,    2],
        [  39,   19,  371, 4676,   34, 3489,    0,    8,  284,  595, 1764, 1949,
           62,  199,   94,   69,    7,  926,   12,    7, 2291,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,  185,  841,    0,   17,   11,    6,   70,  956,    0, 1456,    0,
           10,    7, 1017,   12,   94,   24,  162,  665,   85,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1482, 2700,   25,   13,    0,   51,   18,    0,   25,  164,    0,  274,
           85,   13, 9096, 3158,    0,    8,   25,  164,    0,  150,    7,  858,
            0,    7,  858,  164,   51, 6461,    0,    2,    1],
        [   8,   19,  278,  172,  324,   85, 3044,   54,  138,  294,  955, 4925,
            6,   25,  169,  296,   55,    7,   94,  148,  162,  142,   10,   14,
            9,  117, 3539,    6,    0,    2,    1,    1,    1],
        [ 238,    0,   21,   34, 1968,    0,  853, 8637,    6,   89, 1066,    0,
           33,   26,   70,   24,  278,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,    0, 1005, 2432,   89, 1629,   20,  606,   10,    0,  270,  518,
            0,    8,  339,  110, 3890,   70,   19,  499,   66,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,   91,  854,   12,    7,   94,    0,   19,  388,    0,    9,   13,
          572,   17,   11,    6,    0,  321,   12,    7, 3505,    0,    0, 7518,
           48, 6129,  572,    0,    2,    1,    1,    1,    1],
        [  63,  155,  451,  373, 3163,    0,    8,   53,  162,    0,   53,  162,
         7070,    0,  211, 5060,  859,  291,   18,  234, 2871,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 244, 3455,   62,    0,  244,  335,   62,    0,  244, 4014,   20,    0,
         3572,   62,  126,    0,    7, 3349, 4609,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  13,  409,  552,  126,   12,   60,    0, 1323,   12,  215,  581,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 138,   73,   21,   51,   17,   86, 1891,    7,  179, 6461,  111, 2700,
          170,   13, 6166, 5542,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   34, 2291, 6292,    0,  985,   35,  290,  551, 5588,    6,    0,
          743,   54,    8, 1809,  484,  945,    8,   10,  484,  945,    0, 2994,
         1344,    8,  853,   22, 1105,   57,    6,    0,    2],
        [  67,   33,   26,  113,    0,  103,   19,  305,   13,  488, 1396,  270,
          106,   33,    0,   33,   26,  113,   13,  133, 1392, 3046,   55,  110,
            0,    2,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([24, 22, 32, 23, 29, 24, 24, 29, 33, 30, 33, 23, 23, 32, 30, 19, 23, 29,
        23, 21, 13, 18, 33, 26], device='cuda:3') tensor([0.8950, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8906, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8774, 1.0000, 1.0000, 0.8340, 0.8726,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:3',
       dtype=torch.float16)
tensor([[  29,  250,   55,   25,   10,  154,   80,    0,   79,   24, 5332,  117,
          264, 5130,    6,    8, 3901, 5259,    0,   79,   24,  722,   71,  117,
         3684, 1941,  264,   10,   93,    6,    0,  138,  261,   63,   24, 1420,
         1811,  518,  521,  121, 2227, 1068,   55, 7127,    8, 3069,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   34,   56, 3856, 7489, 1298, 1490,    0,   67,   19,  154,    7,
          143,  528,  279,   26,   21,   34, 3315,  125,   19,  278,   10,  305,
           69,  486, 2883,    0, 2603, 1800,   17,   21, 2947,   62,    9,   13,
         4098,    0,    8,   17,   26,   79, 2326, 2420,    0,   68,  194,   65,
           24,   11,   57,  142,   10, 4938,  132,    7,  733,  279,    0,  192,
           11,   18, 4110,    0,   68,  194,   65,   19,  217, 2541, 1079, 1344,
            0,   68,  194,   65,    2,    1,    1,    1,    1,    1,    1,    1],
        [  24, 2516,  236,  436,    0,   24, 2508,    0,   10,  291,  699,  352,
            7,   94,  172, 5555,   55, 2872,   54, 3382,   46,   55, 7644,   54,
         2518,  106, 4518,  336,    7,  179,    0,  113, 2134,   79, 1503, 1436,
          959,   54,    0,    8,   55, 4777,   54,    7, 1422,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   69,    7,  859,   63,    7, 6848, 3927,    0,  166,  180,    0,
            9,    7, 6412,    0, 5070,  251, 2216, 3675,   10,   51, 7893,  922,
            8, 5493,  109, 8520,  133,    0,  133,  555, 4400,  187, 2404,    6,
            0,   10,   51, 2185,  959, 1011,  540,   69, 2696,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   69, 1219,   12,   77,  518,  117, 3002,    0,   53, 1005, 2762,
            0,   68,  194,   65,   24,  192,   11,   18,  135,   70,   11,    6,
          142,   69,  854,    7,  183,    0,   29,   21,  388,    6,  170,    9,
           13,  133, 1953,  241, 1032, 3140,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  84,   34,   13, 3008,    9,    0,   89, 3594,   17,  144,   13,   38,
         1341,  200,    6,  197, 1729,  502,   20,    9,    0,  159, 3508,    0,
            0,    0,    8,   89, 3181, 1412,   66,  244,  620,  918,  110,  952,
            0,   10, 1183,   80,  138, 7721,   19,  474,   33, 1729,  502,   20,
            0,   34,    0,  125,   13, 1323, 1113,  490, 4468, 1949,   20,   15,
            0,  225, 2036, 2592,   89, 8659,   54, 1066,    0,  131, 2385,  110,
           33,   38, 1341,  200,    6,  197,    0, 1729,  502,   20,    0,    2],
        [   0,  101,  968,  110,   17,    0,  778,   46,    7, 1809, 1305,    0,
            0,    7,   29,   57, 1911,  292,  411,    6,    0,    0,    0,    7,
         1487,   22,  335,    0,    0, 6898,    0,   77,   12,    7,    0, 2673,
           18,  292,  160, 2719,  160,  128,    0, 1540,   59, 4911,    0,    0,
            8, 2788,  407,  140, 6509,    6,   46,  162,  378, 6121,  131,  185,
         8219, 2294, 6146,   17,   19,  220,   86, 1008,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21,   11,    6, 1241,   17, 3297,   17,   24,  164,   77, 1665,
          340,  699,  352,    7, 5407,   12,  378,   70,   24,   63,    0,   13,
         5030, 6033, 1369,    0, 5030, 4837,   12,   56,   15,    6,  234,   54,
           17,   77,   94,    6,    8,   77, 5291,    6,  446,   13,  207,   10,
         9780,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:6') tensor([48, 77, 47, 47, 44, 84, 70, 51], device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8970, 0.8232, 1.0000],
       device='cuda:6', dtype=torch.float16)
tensor([[ 115,  103,   19,  ...,    1,    1,    1],
        [  55,  663,    0,  ...,    1,    1,    1],
        [  55,    7,  473,  ...,    1,    1,    1],
        ...,
        [  33,   26,   13,  ...,    1,    1,    1],
        [  29, 4232,  818,  ...,    1,    1,    1],
        [ 251,   63, 3744,  ...,    1,    1,    1]], device='cuda:2') tensor([29, 22, 29, 26, 20, 21, 33, 26, 32, 27, 36, 30, 40, 35, 23, 34, 24, 45,
        38, 14, 20, 28, 16, 25], device='cuda:2') tensor([1.0000, 0.8721, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8198, 0.8774, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8545,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:2',
       dtype=torch.float16)
2023-08-15 15:28:53 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 12.408 | trans_loss 13.789 | nll_loss 13.618 | w2v_ctc_loss 5.475 | task_loss 7.416 | contrastive_loss 4.057 | total 4003.4 | n_correct 45.1 | ppl 12575.9 | accuracy 1.127 | uer 70.823 | wer 68.476 | raw_wer 68.476 | bleu 0 | wps 1144.7 | wpb 4003.4 | bsz 141.8 | num_updates 1468
2023-08-15 15:28:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1468 updates
2023-08-15 15:28:53 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 15:28:57 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 15:29:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 1 @ 1468 updates, score 0.0) (writing took 10.106278628110886 seconds)
2023-08-15 15:29:03 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-08-15 15:29:03 | INFO | train | epoch 001 | loss 10.081 | trans_loss 5.943 | nll_loss 4.862 | w2v_ctc_loss 7.192 | task_loss 1.393 | contrastive_loss 3.078 | total 4138.59 | n_correct 78.1328 | ppl 29.07 | accuracy 1.888 | wps 16659.3 | ups 1.35 | wpb 12355.8 | bsz 458.6 | num_updates 1468 | lr 5.87906e-05 | gnorm 2.237 | clip 1.2 | loss_scale 2 | train_wall 1041 | gb_free 18.8 | wall 1162
2023-08-15 15:29:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 15:29:03 | INFO | fairseq.trainer | begin training epoch 2
2023-08-15 15:29:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 15:29:37 | INFO | train_inner | epoch 002:     32 / 1474 loss=7.872, trans_loss=6.102, nll_loss=5.073, w2v_ctc_loss=4.043, task_loss=1.253, contrastive_loss=2.829, total=4165.61, n_correct=65.69, ppl=33.67, accuracy=1.577, wps=9356, ups=0.75, wpb=12422.6, bsz=470.4, num_updates=1500, lr=6.007e-05, gnorm=2.577, clip=0, loss_scale=2, train_wall=70, gb_free=18.6, wall=1197
2023-08-15 15:30:45 | INFO | train_inner | epoch 002:    132 / 1474 loss=7.728, trans_loss=6.079, nll_loss=5.047, w2v_ctc_loss=3.957, task_loss=1.31, contrastive_loss=2.683, total=4153.7, n_correct=67.28, ppl=33.05, accuracy=1.62, wps=18163.5, ups=1.47, wpb=12391.2, bsz=453.7, num_updates=1600, lr=6.4068e-05, gnorm=2.442, clip=0, loss_scale=2, train_wall=68, gb_free=19.1, wall=1265
2023-08-15 15:31:56 | INFO | train_inner | epoch 002:    232 / 1474 loss=7.657, trans_loss=6.093, nll_loss=5.069, w2v_ctc_loss=3.801, task_loss=1.144, contrastive_loss=2.754, total=4201.44, n_correct=62.49, ppl=33.57, accuracy=1.487, wps=17872.3, ups=1.42, wpb=12547.2, bsz=493.6, num_updates=1700, lr=6.8066e-05, gnorm=2.68, clip=0, loss_scale=2, train_wall=70, gb_free=18.8, wall=1335
2023-08-15 15:33:04 | INFO | train_inner | epoch 002:    332 / 1474 loss=7.465, trans_loss=6.072, nll_loss=5.043, w2v_ctc_loss=3.753, task_loss=1.337, contrastive_loss=2.513, total=4130.13, n_correct=67.83, ppl=32.98, accuracy=1.642, wps=18074.2, ups=1.47, wpb=12330.1, bsz=445.5, num_updates=1800, lr=7.2064e-05, gnorm=2.477, clip=0, loss_scale=2, train_wall=68, gb_free=18.6, wall=1403
2023-08-15 15:34:12 | INFO | train_inner | epoch 002:    432 / 1474 loss=7.318, trans_loss=6.061, nll_loss=5.032, w2v_ctc_loss=3.691, task_loss=1.468, contrastive_loss=2.329, total=4035.12, n_correct=65.19, ppl=32.72, accuracy=1.616, wps=17724.1, ups=1.47, wpb=12062.7, bsz=413.5, num_updates=1900, lr=7.6062e-05, gnorm=2.44, clip=0, loss_scale=2, train_wall=68, gb_free=19, wall=1471
2023-08-15 15:35:21 | INFO | train_inner | epoch 002:    532 / 1474 loss=7.246, trans_loss=6.046, nll_loss=5.008, w2v_ctc_loss=3.551, task_loss=1.268, contrastive_loss=2.473, total=4183.09, n_correct=73.11, ppl=32.19, accuracy=1.748, wps=18203.6, ups=1.46, wpb=12479, bsz=468.4, num_updates=2000, lr=8.006e-05, gnorm=2.514, clip=0, loss_scale=2, train_wall=68, gb_free=18.4, wall=1540
2023-08-15 15:35:21 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 15:36:08 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 11.601 | trans_loss 13.285 | nll_loss 12.971 | w2v_ctc_loss 4.625 | task_loss 7.415 | contrastive_loss 3.506 | total 4003.4 | n_correct 58.9 | ppl 8029.72 | accuracy 1.471 | uer 61.981 | wer 60.024 | raw_wer 60.024 | bleu 0 | wps 1141.6 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_bleu 0
2023-08-15 15:36:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-08-15 15:36:08 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_2_2000.pt
2023-08-15 15:36:12 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_2_2000.pt
2023-08-15 15:36:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.0) (writing took 39.53839764371514 seconds)
2023-08-15 15:37:56 | INFO | train_inner | epoch 002:    632 / 1474 loss=7.093, trans_loss=6.017, nll_loss=4.97, w2v_ctc_loss=3.463, task_loss=1.301, contrastive_loss=2.293, total=4123.85, n_correct=67.71, ppl=31.34, accuracy=1.642, wps=7891.8, ups=0.64, wpb=12306.4, bsz=448.7, num_updates=2100, lr=8.4058e-05, gnorm=2.455, clip=0, loss_scale=2, train_wall=69, gb_free=18.6, wall=1696
2023-08-15 15:39:05 | INFO | train_inner | epoch 002:    732 / 1474 loss=7.039, trans_loss=6.033, nll_loss=4.99, w2v_ctc_loss=3.398, task_loss=1.288, contrastive_loss=2.361, total=4148.13, n_correct=71.06, ppl=31.79, accuracy=1.713, wps=18156, ups=1.47, wpb=12381, bsz=462.1, num_updates=2200, lr=8.8056e-05, gnorm=2.494, clip=0, loss_scale=2, train_wall=68, gb_free=18.5, wall=1764
2023-08-15 15:40:13 | INFO | train_inner | epoch 002:    832 / 1474 loss=6.934, trans_loss=6.01, nll_loss=4.964, w2v_ctc_loss=3.343, task_loss=1.301, contrastive_loss=2.31, total=4172.27, n_correct=75.52, ppl=31.21, accuracy=1.81, wps=18270.9, ups=1.47, wpb=12465.7, bsz=464.5, num_updates=2300, lr=9.2054e-05, gnorm=2.246, clip=0, loss_scale=2, train_wall=68, gb_free=18.7, wall=1832
2023-08-15 15:41:21 | INFO | train_inner | epoch 002:    932 / 1474 loss=6.796, trans_loss=5.997, nll_loss=4.945, w2v_ctc_loss=3.258, task_loss=1.362, contrastive_loss=2.232, total=4101.67, n_correct=73.96, ppl=30.8, accuracy=1.803, wps=17917.3, ups=1.46, wpb=12242.5, bsz=441.6, num_updates=2400, lr=9.6052e-05, gnorm=2.351, clip=0, loss_scale=2, train_wall=68, gb_free=18.9, wall=1901
2023-08-15 15:42:31 | INFO | train_inner | epoch 002:   1032 / 1474 loss=6.714, trans_loss=6.008, nll_loss=4.959, w2v_ctc_loss=3.196, task_loss=1.324, contrastive_loss=2.118, total=4091.09, n_correct=72.17, ppl=31.09, accuracy=1.764, wps=17614.2, ups=1.44, wpb=12214.8, bsz=451.5, num_updates=2500, lr=0.00010005, gnorm=2.19, clip=0, loss_scale=2, train_wall=69, gb_free=19.1, wall=1970
2023-08-15 15:43:39 | INFO | train_inner | epoch 002:   1132 / 1474 loss=6.704, trans_loss=6.01, nll_loss=4.961, w2v_ctc_loss=3.106, task_loss=1.149, contrastive_loss=2.345, total=4219.19, n_correct=77.54, ppl=31.14, accuracy=1.838, wps=18286.4, ups=1.45, wpb=12595.1, bsz=500.6, num_updates=2600, lr=0.000104048, gnorm=2.169, clip=0, loss_scale=4, train_wall=68, gb_free=19, wall=2039
2023-08-15 15:44:48 | INFO | train_inner | epoch 002:   1232 / 1474 loss=6.599, trans_loss=6.001, nll_loss=4.948, w2v_ctc_loss=3.072, task_loss=1.21, contrastive_loss=2.153, total=4212.91, n_correct=75.29, ppl=30.87, accuracy=1.787, wps=18359.9, ups=1.46, wpb=12571.7, bsz=486.8, num_updates=2700, lr=0.000108046, gnorm=2.083, clip=0, loss_scale=4, train_wall=68, gb_free=19.1, wall=2107
2023-08-15 15:45:56 | INFO | train_inner | epoch 002:   1332 / 1474 loss=6.483, trans_loss=5.99, nll_loss=4.939, w2v_ctc_loss=3.036, task_loss=1.271, contrastive_loss=1.931, total=4142.48, n_correct=74.23, ppl=30.67, accuracy=1.792, wps=18279.3, ups=1.48, wpb=12381.2, bsz=456.3, num_updates=2800, lr=0.000112044, gnorm=2.147, clip=0, loss_scale=4, train_wall=67, gb_free=18.8, wall=2175
2023-08-15 15:47:05 | INFO | train_inner | epoch 002:   1432 / 1474 loss=6.396, trans_loss=5.997, nll_loss=4.945, w2v_ctc_loss=2.987, task_loss=1.386, contrastive_loss=1.985, total=4063.28, n_correct=69.21, ppl=30.8, accuracy=1.703, wps=17500, ups=1.44, wpb=12131.5, bsz=444.3, num_updates=2900, lr=0.000116042, gnorm=1.994, clip=0, loss_scale=4, train_wall=69, gb_free=19.6, wall=2244
2023-08-15 15:47:33 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 15:48:15 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 10.845 | trans_loss 13.032 | nll_loss 12.631 | w2v_ctc_loss 3.788 | task_loss 7.415 | contrastive_loss 2.657 | total 4003.4 | n_correct 82.2 | ppl 6341.3 | accuracy 2.053 | uer 53.452 | wer 52.515 | raw_wer 52.515 | bleu 0 | wps 1079.4 | wpb 4003.4 | bsz 141.8 | num_updates 2942 | best_bleu 0
2023-08-15 15:48:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2942 updates
2023-08-15 15:48:15 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 15:48:38 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 15:48:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 2 @ 2942 updates, score 0.0) (writing took 37.936472434550524 seconds)
2023-08-15 15:48:53 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-08-15 15:48:53 | INFO | train | epoch 002 | loss 7.013 | trans_loss 6.031 | nll_loss 4.988 | w2v_ctc_loss 3.402 | task_loss 1.292 | contrastive_loss 2.32 | total 4138.65 | n_correct 70.7551 | ppl 31.74 | accuracy 1.71 | wps 15303.4 | ups 1.24 | wpb 12355.8 | bsz 458.5 | num_updates 2942 | lr 0.000117721 | gnorm 2.33 | clip 0 | loss_scale 4 | train_wall 1004 | gb_free 18.9 | wall 2352
2023-08-15 15:48:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 15:48:53 | INFO | fairseq.trainer | begin training epoch 3
2023-08-15 15:48:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 15:49:41 | INFO | train_inner | epoch 003:     58 / 1474 loss=6.306, trans_loss=5.986, nll_loss=4.93, w2v_ctc_loss=2.938, task_loss=1.362, contrastive_loss=1.84, total=4048.67, n_correct=72.56, ppl=30.49, accuracy=1.792, wps=7734.2, ups=0.64, wpb=12085.6, bsz=433.9, num_updates=3000, lr=0.00012004, gnorm=1.934, clip=0, loss_scale=4, train_wall=68, gb_free=18.7, wall=2401
2023-08-15 15:50:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-15 15:51:18 | INFO | train_inner | epoch 003:    159 / 1474 loss=5.685, trans_loss=5.345, nll_loss=4.12, w2v_ctc_loss=2.705, task_loss=0.901, contrastive_loss=1.799, total=4158.58, n_correct=249.28, ppl=17.39, accuracy=5.994, wps=12851.4, ups=1.04, wpb=12415.7, bsz=463, num_updates=3100, lr=0.000124038, gnorm=3.946, clip=3, loss_scale=2, train_wall=96, gb_free=16.1, wall=2497
2023-08-15 15:52:56 | INFO | train_inner | epoch 003:    259 / 1474 loss=4.852, trans_loss=4.704, nll_loss=3.273, w2v_ctc_loss=2.417, task_loss=0.925, contrastive_loss=1.595, total=4155.72, n_correct=716.25, ppl=9.66, accuracy=17.235, wps=12660, ups=1.02, wpb=12415.9, bsz=465.5, num_updates=3200, lr=0.000128036, gnorm=2.631, clip=0, loss_scale=2, train_wall=97, gb_free=17.3, wall=2595
2023-08-15 15:54:33 | INFO | train_inner | epoch 003:    359 / 1474 loss=4.393, trans_loss=4.297, nll_loss=2.732, w2v_ctc_loss=2.311, task_loss=0.916, contrastive_loss=1.515, total=4154.07, n_correct=1154.87, ppl=6.64, accuracy=27.801, wps=12767.1, ups=1.03, wpb=12396.6, bsz=464.9, num_updates=3300, lr=0.000132034, gnorm=2.45, clip=0, loss_scale=2, train_wall=96, gb_free=15.2, wall=2692
2023-08-15 15:56:10 | INFO | train_inner | epoch 003:    459 / 1474 loss=4.153, trans_loss=4.207, nll_loss=2.619, w2v_ctc_loss=2.201, task_loss=0.902, contrastive_loss=1.335, total=4212.17, n_correct=1303.93, ppl=6.14, accuracy=30.956, wps=12927.4, ups=1.03, wpb=12572.6, bsz=473.8, num_updates=3400, lr=0.000136032, gnorm=2.203, clip=0, loss_scale=2, train_wall=97, gb_free=15.3, wall=2790
2023-08-15 15:57:47 | INFO | train_inner | epoch 003:    559 / 1474 loss=3.971, trans_loss=4.166, nll_loss=2.57, w2v_ctc_loss=2.115, task_loss=0.984, contrastive_loss=1.209, total=4081.04, n_correct=1317.54, ppl=5.94, accuracy=32.284, wps=12605.3, ups=1.03, wpb=12190.8, bsz=440.1, num_updates=3500, lr=0.00014003, gnorm=2.119, clip=0, loss_scale=2, train_wall=96, gb_free=16.1, wall=2886
2023-08-15 15:59:25 | INFO | train_inner | epoch 003:    659 / 1474 loss=3.895, trans_loss=4.139, nll_loss=2.53, w2v_ctc_loss=2.038, task_loss=0.877, contrastive_loss=1.281, total=4231.09, n_correct=1416.67, ppl=5.77, accuracy=33.482, wps=12879.6, ups=1.02, wpb=12615.6, bsz=484.4, num_updates=3600, lr=0.000144028, gnorm=1.938, clip=0, loss_scale=2, train_wall=97, gb_free=15.7, wall=2984
2023-08-15 16:01:01 | INFO | train_inner | epoch 003:    759 / 1474 loss=3.737, trans_loss=4.103, nll_loss=2.489, w2v_ctc_loss=1.99, task_loss=0.884, contrastive_loss=0.992, total=4160.74, n_correct=1439.42, ppl=5.61, accuracy=34.595, wps=12927.5, ups=1.04, wpb=12428.5, bsz=469.2, num_updates=3700, lr=0.000148026, gnorm=1.753, clip=0, loss_scale=2, train_wall=96, gb_free=16.5, wall=3080
2023-08-15 16:02:37 | INFO | train_inner | epoch 003:    859 / 1474 loss=3.645, trans_loss=4.089, nll_loss=2.469, w2v_ctc_loss=1.954, task_loss=0.937, contrastive_loss=0.916, total=4160.47, n_correct=1469.97, ppl=5.54, accuracy=35.332, wps=12910, ups=1.04, wpb=12423.6, bsz=455.4, num_updates=3800, lr=0.000152024, gnorm=1.846, clip=0, loss_scale=2, train_wall=96, gb_free=15.9, wall=3177
2023-08-15 16:04:14 | INFO | train_inner | epoch 003:    959 / 1474 loss=3.578, trans_loss=4.066, nll_loss=2.438, w2v_ctc_loss=1.921, task_loss=0.899, contrastive_loss=0.909, total=4162.26, n_correct=1518.37, ppl=5.42, accuracy=36.479, wps=12820.8, ups=1.03, wpb=12416.1, bsz=467.9, num_updates=3900, lr=0.000156022, gnorm=1.939, clip=0, loss_scale=2, train_wall=96, gb_free=17.6, wall=3274
2023-08-15 16:05:51 | INFO | train_inner | epoch 003:   1059 / 1474 loss=3.49, trans_loss=4.038, nll_loss=2.404, w2v_ctc_loss=1.909, task_loss=0.98, contrastive_loss=0.813, total=4062.67, n_correct=1515.74, ppl=5.29, accuracy=37.309, wps=12546.2, ups=1.03, wpb=12131.7, bsz=443.2, num_updates=4000, lr=0.00016002, gnorm=1.721, clip=0, loss_scale=2, train_wall=96, gb_free=15.4, wall=3370
2023-08-15 16:05:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 16:06:26 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.733 | trans_loss 6.94 | nll_loss 4.705 | w2v_ctc_loss 2.319 | task_loss 4.218 | contrastive_loss 1.09 | total 4003.4 | n_correct 1629.9 | ppl 26.07 | accuracy 40.713 | uer 32.758 | wer 33.097 | raw_wer 33.097 | bleu 3 | wps 1488.8 | wpb 4003.4 | bsz 141.8 | num_updates 4000 | best_bleu 3
2023-08-15 16:06:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-08-15 16:06:26 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_3_4000.pt
2023-08-15 16:06:30 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_3_4000.pt
2023-08-15 16:07:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 3.0) (writing took 70.53501944057643 seconds)
2023-08-15 16:09:12 | INFO | train_inner | epoch 003:   1159 / 1474 loss=3.394, trans_loss=4.021, nll_loss=2.38, w2v_ctc_loss=1.861, task_loss=0.997, contrastive_loss=0.739, total=4046.76, n_correct=1547.29, ppl=5.2, accuracy=38.235, wps=6008.9, ups=0.5, wpb=12078.6, bsz=433.9, num_updates=4100, lr=0.000164018, gnorm=1.624, clip=0, loss_scale=2, train_wall=95, gb_free=15.7, wall=3571
2023-08-15 16:10:47 | INFO | train_inner | epoch 003:   1259 / 1474 loss=3.318, trans_loss=3.987, nll_loss=2.336, w2v_ctc_loss=1.829, task_loss=0.981, contrastive_loss=0.687, total=4064.26, n_correct=1597.81, ppl=5.05, accuracy=39.314, wps=12807.4, ups=1.06, wpb=12137.6, bsz=434, num_updates=4200, lr=0.000168016, gnorm=1.629, clip=0, loss_scale=2, train_wall=94, gb_free=16.3, wall=3666
2023-08-15 16:12:24 | INFO | train_inner | epoch 003:   1359 / 1474 loss=3.274, trans_loss=3.961, nll_loss=2.302, w2v_ctc_loss=1.786, task_loss=0.933, contrastive_loss=0.77, total=4137.36, n_correct=1676.78, ppl=4.93, accuracy=40.528, wps=12721.7, ups=1.03, wpb=12352.2, bsz=461.5, num_updates=4300, lr=0.000172014, gnorm=1.501, clip=0, loss_scale=2, train_wall=97, gb_free=15.7, wall=3763
2023-08-15 16:14:01 | INFO | train_inner | epoch 003:   1459 / 1474 loss=3.237, trans_loss=3.938, nll_loss=2.273, w2v_ctc_loss=1.785, task_loss=0.885, contrastive_loss=0.727, total=4207.75, n_correct=1747.84, ppl=4.83, accuracy=41.539, wps=12952.5, ups=1.03, wpb=12567.9, bsz=476.7, num_updates=4400, lr=0.000176012, gnorm=1.789, clip=1, loss_scale=2, train_wall=96, gb_free=17.1, wall=3860
2023-08-15 16:14:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 16:14:49 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.315 | trans_loss 6.543 | nll_loss 4.193 | w2v_ctc_loss 2.081 | task_loss 4.151 | contrastive_loss 0.88 | total 4003.4 | n_correct 1844.2 | ppl 18.3 | accuracy 46.066 | uer 31.452 | wer 31.74 | raw_wer 31.74 | bleu 6.69 | wps 1391 | wpb 4003.4 | bsz 141.8 | num_updates 4415 | best_bleu 6.69
2023-08-15 16:14:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4415 updates
2023-08-15 16:14:49 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 16:15:05 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 16:15:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 3 @ 4415 updates, score 6.69) (writing took 30.810543958097696 seconds)
2023-08-15 16:15:20 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-08-15 16:15:20 | INFO | train | epoch 003 | loss 3.99 | trans_loss 4.286 | nll_loss 2.725 | w2v_ctc_loss 2.09 | task_loss 0.944 | contrastive_loss 1.122 | total 4138.87 | n_correct 1287.25 | ppl 6.61 | accuracy 31.101 | wps 11468.9 | ups 0.93 | wpb 12356.3 | bsz 458.6 | num_updates 4415 | lr 0.000176612 | gnorm 2.07 | clip 0.3 | loss_scale 2 | train_wall 1399 | gb_free 16 | wall 3939
2023-08-15 16:15:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 16:15:20 | INFO | fairseq.trainer | begin training epoch 4
2023-08-15 16:15:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 16:16:49 | INFO | train_inner | epoch 004:     85 / 1474 loss=3.11, trans_loss=3.908, nll_loss=2.231, w2v_ctc_loss=1.743, task_loss=0.962, contrastive_loss=0.545, total=4095.18, n_correct=1747.83, ppl=4.7, accuracy=42.68, wps=7269.8, ups=0.59, wpb=12222.9, bsz=437.8, num_updates=4500, lr=0.00018001, gnorm=1.529, clip=0, loss_scale=2, train_wall=95, gb_free=12.2, wall=4028
2023-08-15 16:18:25 | INFO | train_inner | epoch 004:    185 / 1474 loss=3.052, trans_loss=3.872, nll_loss=2.186, w2v_ctc_loss=1.701, task_loss=0.883, contrastive_loss=0.548, total=4178.83, n_correct=1839.6, ppl=4.55, accuracy=44.022, wps=12985.8, ups=1.04, wpb=12476.1, bsz=469.5, num_updates=4600, lr=0.000184008, gnorm=1.362, clip=0, loss_scale=2, train_wall=95, gb_free=14.4, wall=4124
2023-08-15 16:20:03 | INFO | train_inner | epoch 004:    285 / 1474 loss=3.046, trans_loss=3.864, nll_loss=2.177, w2v_ctc_loss=1.694, task_loss=0.94, contrastive_loss=0.653, total=4142.3, n_correct=1838.04, ppl=4.52, accuracy=44.372, wps=12661.6, ups=1.02, wpb=12373.8, bsz=460.7, num_updates=4700, lr=0.000188006, gnorm=1.399, clip=0, loss_scale=2, train_wall=97, gb_free=12.9, wall=4222
2023-08-15 16:21:39 | INFO | train_inner | epoch 004:    385 / 1474 loss=2.973, trans_loss=3.85, nll_loss=2.156, w2v_ctc_loss=1.676, task_loss=0.964, contrastive_loss=0.478, total=4124.92, n_correct=1861.17, ppl=4.46, accuracy=45.12, wps=12842.7, ups=1.04, wpb=12307.7, bsz=444.2, num_updates=4800, lr=0.000192004, gnorm=1.343, clip=0, loss_scale=2, train_wall=95, gb_free=11.9, wall=4318
2023-08-15 16:23:17 | INFO | train_inner | epoch 004:    485 / 1474 loss=3.004, trans_loss=3.824, nll_loss=2.125, w2v_ctc_loss=1.634, task_loss=0.841, contrastive_loss=0.852, total=4216.09, n_correct=1942.02, ppl=4.36, accuracy=46.062, wps=12860.1, ups=1.02, wpb=12585.8, bsz=497.6, num_updates=4900, lr=0.000196002, gnorm=1.295, clip=0, loss_scale=2, train_wall=97, gb_free=16.4, wall=4416
2023-08-15 16:24:53 | INFO | train_inner | epoch 004:    585 / 1474 loss=2.927, trans_loss=3.801, nll_loss=2.094, w2v_ctc_loss=1.65, task_loss=0.863, contrastive_loss=0.539, total=4231.12, n_correct=1991.07, ppl=4.27, accuracy=47.058, wps=13069.6, ups=1.03, wpb=12629.4, bsz=490.1, num_updates=5000, lr=0.0002, gnorm=1.268, clip=0, loss_scale=2, train_wall=96, gb_free=15.7, wall=4512
mt_weight tensor(0.5000)
asr_weight tensor(0.5206, device='cuda:0')
2023-08-15 16:26:31 | INFO | train_inner | epoch 004:    685 / 1474 loss=2.869, trans_loss=3.793, nll_loss=2.08, w2v_ctc_loss=1.611, task_loss=0.959, contrastive_loss=0.555, total=4176.95, n_correct=1991.69, ppl=4.23, accuracy=47.683, wps=12725.3, ups=1.02, wpb=12451, bsz=455.2, num_updates=5100, lr=0.00019803, gnorm=0.863, clip=0, loss_scale=4, train_wall=97, gb_free=14.7, wall=4610
2023-08-15 16:28:07 | INFO | train_inner | epoch 004:    785 / 1474 loss=2.836, trans_loss=3.775, nll_loss=2.062, w2v_ctc_loss=1.63, task_loss=1.023, contrastive_loss=0.416, total=4016.91, n_correct=1932.29, ppl=4.18, accuracy=48.104, wps=12442.6, ups=1.04, wpb=11995.1, bsz=418.7, num_updates=5200, lr=0.000196116, gnorm=0.887, clip=0, loss_scale=4, train_wall=96, gb_free=15.8, wall=4707
2023-08-15 16:29:44 | INFO | train_inner | epoch 004:    885 / 1474 loss=2.866, trans_loss=3.759, nll_loss=2.042, w2v_ctc_loss=1.623, task_loss=0.931, contrastive_loss=0.588, total=4183.4, n_correct=2035.64, ppl=4.12, accuracy=48.66, wps=12983.1, ups=1.04, wpb=12493.2, bsz=465.4, num_updates=5300, lr=0.000194257, gnorm=0.978, clip=1, loss_scale=4, train_wall=96, gb_free=15.1, wall=4803
2023-08-15 16:31:21 | INFO | train_inner | epoch 004:    985 / 1474 loss=2.817, trans_loss=3.747, nll_loss=2.026, w2v_ctc_loss=1.611, task_loss=0.944, contrastive_loss=0.462, total=4128.78, n_correct=2034.51, ppl=4.07, accuracy=49.276, wps=12688.6, ups=1.03, wpb=12332, bsz=456.8, num_updates=5400, lr=0.00019245, gnorm=1.022, clip=0, loss_scale=4, train_wall=96, gb_free=15.6, wall=4900
2023-08-15 16:32:57 | INFO | train_inner | epoch 004:   1085 / 1474 loss=2.77, trans_loss=3.738, nll_loss=2.014, w2v_ctc_loss=1.59, task_loss=0.997, contrastive_loss=0.413, total=4080.2, n_correct=2028.64, ppl=4.04, accuracy=49.719, wps=12670, ups=1.04, wpb=12179.1, bsz=437.7, num_updates=5500, lr=0.000190693, gnorm=0.795, clip=0, loss_scale=4, train_wall=95, gb_free=15.7, wall=4996
2023-08-15 16:34:34 | INFO | train_inner | epoch 004:   1185 / 1474 loss=2.784, trans_loss=3.724, nll_loss=1.998, w2v_ctc_loss=1.579, task_loss=0.87, contrastive_loss=0.528, total=4163.45, n_correct=2097.61, ppl=3.99, accuracy=50.382, wps=12881.1, ups=1.04, wpb=12436, bsz=485.6, num_updates=5600, lr=0.000188982, gnorm=0.81, clip=0, loss_scale=4, train_wall=96, gb_free=14.8, wall=5093
2023-08-15 16:36:09 | INFO | train_inner | epoch 004:   1285 / 1474 loss=2.733, trans_loss=3.703, nll_loss=1.97, w2v_ctc_loss=1.558, task_loss=0.888, contrastive_loss=0.471, total=4152.41, n_correct=2117.59, ppl=3.92, accuracy=50.997, wps=12972.5, ups=1.05, wpb=12401.3, bsz=471.4, num_updates=5700, lr=0.000187317, gnorm=0.74, clip=0, loss_scale=4, train_wall=95, gb_free=12.4, wall=5188
2023-08-15 16:37:44 | INFO | train_inner | epoch 004:   1385 / 1474 loss=2.685, trans_loss=3.694, nll_loss=1.959, w2v_ctc_loss=1.553, task_loss=0.955, contrastive_loss=0.343, total=4103.57, n_correct=2111.2, ppl=3.89, accuracy=51.448, wps=12862.1, ups=1.05, wpb=12255.2, bsz=437.9, num_updates=5800, lr=0.000185695, gnorm=0.764, clip=0, loss_scale=4, train_wall=95, gb_free=16.4, wall=5284
2023-08-15 16:39:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.5206, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.5206, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.5206, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.5206, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.5206, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.5206, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.5206, device='cuda:4')
2023-08-15 16:39:37 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 4.605 | trans_loss 5.777 | nll_loss 3.203 | w2v_ctc_loss 1.803 | task_loss 4.449 | contrastive_loss 0.55 | total 4003.4 | n_correct 2290.6 | ppl 9.21 | accuracy 57.216 | uer 26.305 | wer 27.654 | raw_wer 27.654 | bleu 14.77 | wps 1616.3 | wpb 4003.4 | bsz 141.8 | num_updates 5889 | best_bleu 14.77
2023-08-15 16:39:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5889 updates
2023-08-15 16:39:37 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 16:39:53 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 16:40:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 4 @ 5889 updates, score 14.77) (writing took 31.20475018955767 seconds)
2023-08-15 16:40:08 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-08-15 16:40:08 | INFO | train | epoch 004 | loss 2.875 | trans_loss 3.782 | nll_loss 2.071 | w2v_ctc_loss 1.625 | task_loss 0.929 | contrastive_loss 0.522 | total 4138.65 | n_correct 1979.74 | ppl 4.2 | accuracy 47.835 | wps 12237 | ups 0.99 | wpb 12355.8 | bsz 458.5 | num_updates 5889 | lr 0.000184287 | gnorm 1.051 | clip 0.1 | loss_scale 4 | train_wall 1412 | gb_free 14.5 | wall 5428
2023-08-15 16:40:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 16:40:09 | INFO | fairseq.trainer | begin training epoch 5
2023-08-15 16:40:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 16:40:26 | INFO | train_inner | epoch 005:     11 / 1474 loss=2.661, trans_loss=3.685, nll_loss=1.947, w2v_ctc_loss=1.525, task_loss=0.974, contrastive_loss=0.358, total=4031.51, n_correct=2086.37, ppl=3.86, accuracy=51.752, wps=7432.4, ups=0.62, wpb=12037.5, bsz=436.7, num_updates=5900, lr=0.000184115, gnorm=0.766, clip=0, loss_scale=4, train_wall=95, gb_free=13.9, wall=5446
2023-08-15 16:42:02 | INFO | train_inner | epoch 005:    111 / 1474 loss=2.601, trans_loss=3.646, nll_loss=1.896, w2v_ctc_loss=1.452, task_loss=0.834, contrastive_loss=0.383, total=4256.63, n_correct=2267.1, ppl=3.72, accuracy=53.26, wps=13296.4, ups=1.05, wpb=12710, bsz=500.1, num_updates=6000, lr=0.000182574, gnorm=0.684, clip=0, loss_scale=4, train_wall=95, gb_free=15.8, wall=5541
2023-08-15 16:42:02 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 16:42:30 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.57 | trans_loss 5.764 | nll_loss 3.18 | w2v_ctc_loss 1.746 | task_loss 4.402 | contrastive_loss 0.521 | total 4003.4 | n_correct 2296.9 | ppl 9.06 | accuracy 57.374 | uer 25.974 | wer 26.882 | raw_wer 26.882 | bleu 15.55 | wps 1742 | wpb 4003.4 | bsz 141.8 | num_updates 6000 | best_bleu 15.55
2023-08-15 16:42:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6000 updates
2023-08-15 16:42:30 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_5_6000.pt
2023-08-15 16:42:34 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_5_6000.pt
2023-08-15 16:43:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_5_6000.pt (epoch 5 @ 6000 updates, score 15.55) (writing took 34.732457106933 seconds)
2023-08-15 16:44:40 | INFO | train_inner | epoch 005:    211 / 1474 loss=2.624, trans_loss=3.648, nll_loss=1.896, w2v_ctc_loss=1.455, task_loss=0.867, contrastive_loss=0.568, total=4186.83, n_correct=2230.34, ppl=3.72, accuracy=53.27, wps=7913.1, ups=0.63, wpb=12492.2, bsz=485.3, num_updates=6100, lr=0.000181071, gnorm=0.673, clip=0, loss_scale=4, train_wall=95, gb_free=15.8, wall=5699
2023-08-15 16:46:16 | INFO | train_inner | epoch 005:    311 / 1474 loss=2.601, trans_loss=3.634, nll_loss=1.885, w2v_ctc_loss=1.478, task_loss=0.954, contrastive_loss=0.418, total=4094.07, n_correct=2183.32, ppl=3.69, accuracy=53.329, wps=12710.2, ups=1.04, wpb=12241.2, bsz=446, num_updates=6200, lr=0.000179605, gnorm=0.682, clip=0, loss_scale=4, train_wall=96, gb_free=15.8, wall=5796
2023-08-15 16:47:53 | INFO | train_inner | epoch 005:    411 / 1474 loss=2.579, trans_loss=3.622, nll_loss=1.868, w2v_ctc_loss=1.432, task_loss=0.914, contrastive_loss=0.494, total=4140.39, n_correct=2234.62, ppl=3.65, accuracy=53.971, wps=12813.9, ups=1.04, wpb=12372.8, bsz=467.9, num_updates=6300, lr=0.000178174, gnorm=0.667, clip=0, loss_scale=4, train_wall=96, gb_free=15.7, wall=5892
2023-08-15 16:49:29 | INFO | train_inner | epoch 005:    511 / 1474 loss=2.526, trans_loss=3.624, nll_loss=1.869, w2v_ctc_loss=1.441, task_loss=1.04, contrastive_loss=0.28, total=4026.21, n_correct=2173.82, ppl=3.65, accuracy=53.992, wps=12531.1, ups=1.04, wpb=12028.8, bsz=418.1, num_updates=6400, lr=0.000176777, gnorm=0.659, clip=0, loss_scale=4, train_wall=95, gb_free=16.7, wall=5988
2023-08-15 16:51:05 | INFO | train_inner | epoch 005:    611 / 1474 loss=2.554, trans_loss=3.629, nll_loss=1.872, w2v_ctc_loss=1.429, task_loss=0.963, contrastive_loss=0.45, total=4109.94, n_correct=2222.23, ppl=3.66, accuracy=54.07, wps=12684.9, ups=1.03, wpb=12260.2, bsz=449.8, num_updates=6500, lr=0.000175412, gnorm=0.742, clip=0, loss_scale=4, train_wall=96, gb_free=15, wall=6085
2023-08-15 16:52:42 | INFO | train_inner | epoch 005:    711 / 1474 loss=2.536, trans_loss=3.614, nll_loss=1.855, w2v_ctc_loss=1.413, task_loss=0.875, contrastive_loss=0.426, total=4176.83, n_correct=2283.74, ppl=3.62, accuracy=54.676, wps=12943.5, ups=1.04, wpb=12467.5, bsz=483.6, num_updates=6600, lr=0.000174078, gnorm=0.651, clip=0, loss_scale=4, train_wall=96, gb_free=16.7, wall=6181
2023-08-15 16:54:19 | INFO | train_inner | epoch 005:    811 / 1474 loss=2.508, trans_loss=3.609, nll_loss=1.848, w2v_ctc_loss=1.414, task_loss=0.962, contrastive_loss=0.345, total=4127.9, n_correct=2257.27, ppl=3.6, accuracy=54.683, wps=12718.2, ups=1.03, wpb=12321.2, bsz=447.5, num_updates=6700, lr=0.000172774, gnorm=0.671, clip=0, loss_scale=4, train_wall=96, gb_free=15.3, wall=6278
2023-08-15 16:55:55 | INFO | train_inner | epoch 005:    911 / 1474 loss=2.483, trans_loss=3.599, nll_loss=1.837, w2v_ctc_loss=1.406, task_loss=0.961, contrastive_loss=0.308, total=4101.19, n_correct=2260.42, ppl=3.57, accuracy=55.116, wps=12759.2, ups=1.04, wpb=12245.9, bsz=447.9, num_updates=6800, lr=0.000171499, gnorm=0.691, clip=0, loss_scale=4, train_wall=95, gb_free=16.9, wall=6374
2023-08-15 16:57:31 | INFO | train_inner | epoch 005:   1011 / 1474 loss=2.49, trans_loss=3.6, nll_loss=1.837, w2v_ctc_loss=1.398, task_loss=0.923, contrastive_loss=0.381, total=4164.27, n_correct=2296.67, ppl=3.57, accuracy=55.152, wps=12903.8, ups=1.04, wpb=12430.2, bsz=462, num_updates=6900, lr=0.000170251, gnorm=0.676, clip=0, loss_scale=4, train_wall=96, gb_free=14.6, wall=6470
2023-08-15 16:59:08 | INFO | train_inner | epoch 005:   1111 / 1474 loss=2.501, trans_loss=3.594, nll_loss=1.829, w2v_ctc_loss=1.407, task_loss=0.93, contrastive_loss=0.387, total=4168.94, n_correct=2312.61, ppl=3.55, accuracy=55.472, wps=12769.4, ups=1.03, wpb=12436.1, bsz=464.1, num_updates=7000, lr=0.000169031, gnorm=0.619, clip=0, loss_scale=4, train_wall=97, gb_free=16.3, wall=6568
2023-08-15 17:00:44 | INFO | train_inner | epoch 005:   1211 / 1474 loss=2.452, trans_loss=3.589, nll_loss=1.822, w2v_ctc_loss=1.382, task_loss=0.946, contrastive_loss=0.289, total=4171.16, n_correct=2321.45, ppl=3.54, accuracy=55.655, wps=12950.5, ups=1.04, wpb=12443.2, bsz=456.3, num_updates=7100, lr=0.000167836, gnorm=0.62, clip=0, loss_scale=4, train_wall=95, gb_free=15.5, wall=6664
2023-08-15 17:02:20 | INFO | train_inner | epoch 005:   1311 / 1474 loss=2.417, trans_loss=3.58, nll_loss=1.812, w2v_ctc_loss=1.363, task_loss=0.952, contrastive_loss=0.243, total=4126.97, n_correct=2310.87, ppl=3.51, accuracy=55.994, wps=12822.3, ups=1.04, wpb=12317.9, bsz=443.4, num_updates=7200, lr=0.000166667, gnorm=0.605, clip=0, loss_scale=8, train_wall=96, gb_free=15.1, wall=6760
2023-08-15 17:03:57 | INFO | train_inner | epoch 005:   1411 / 1474 loss=2.438, trans_loss=3.58, nll_loss=1.814, w2v_ctc_loss=1.367, task_loss=0.94, contrastive_loss=0.314, total=4138.54, n_correct=2322.25, ppl=3.52, accuracy=56.113, wps=12841.3, ups=1.04, wpb=12359.3, bsz=459, num_updates=7300, lr=0.000165521, gnorm=0.677, clip=0, loss_scale=8, train_wall=96, gb_free=16.3, wall=6856
2023-08-15 17:04:57 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 17:05:23 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.351 | trans_loss 5.559 | nll_loss 2.936 | w2v_ctc_loss 1.549 | task_loss 4.481 | contrastive_loss 0.465 | total 4003.4 | n_correct 2416.3 | ppl 7.65 | accuracy 60.356 | uer 24.256 | wer 25.875 | raw_wer 25.875 | bleu 17.61 | wps 1892.2 | wpb 4003.4 | bsz 141.8 | num_updates 7363 | best_bleu 17.61
2023-08-15 17:05:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7363 updates
2023-08-15 17:05:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 17:05:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 17:05:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 5 @ 7363 updates, score 17.61) (writing took 30.19413187727332 seconds)
2023-08-15 17:05:54 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-08-15 17:05:54 | INFO | train | epoch 005 | loss 2.52 | trans_loss 3.611 | nll_loss 1.851 | w2v_ctc_loss 1.416 | task_loss 0.932 | contrastive_loss 0.377 | total 4138.65 | n_correct 2262.98 | ppl 3.61 | accuracy 54.679 | wps 11783.7 | ups 0.95 | wpb 12355.8 | bsz 458.5 | num_updates 7363 | lr 0.000164812 | gnorm 0.665 | clip 0 | loss_scale 8 | train_wall 1409 | gb_free 15.8 | wall 6973
2023-08-15 17:05:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 17:05:54 | INFO | fairseq.trainer | begin training epoch 6
2023-08-15 17:05:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 17:06:38 | INFO | train_inner | epoch 006:     37 / 1474 loss=2.413, trans_loss=3.561, nll_loss=1.787, w2v_ctc_loss=1.352, task_loss=0.959, contrastive_loss=0.304, total=4113.87, n_correct=2328.64, ppl=3.45, accuracy=56.605, wps=7607.5, ups=0.62, wpb=12276.7, bsz=447.4, num_updates=7400, lr=0.000164399, gnorm=0.618, clip=0, loss_scale=8, train_wall=96, gb_free=17.5, wall=7017
2023-08-15 17:08:14 | INFO | train_inner | epoch 006:    137 / 1474 loss=2.367, trans_loss=3.533, nll_loss=1.752, w2v_ctc_loss=1.302, task_loss=0.925, contrastive_loss=0.34, total=4161.2, n_correct=2388.47, ppl=3.37, accuracy=57.399, wps=12930.9, ups=1.04, wpb=12428.5, bsz=458.1, num_updates=7500, lr=0.000163299, gnorm=0.599, clip=0, loss_scale=8, train_wall=96, gb_free=16.5, wall=7114
2023-08-15 17:09:51 | INFO | train_inner | epoch 006:    237 / 1474 loss=2.368, trans_loss=3.544, nll_loss=1.768, w2v_ctc_loss=1.333, task_loss=1.002, contrastive_loss=0.247, total=4110.12, n_correct=2338.87, ppl=3.41, accuracy=56.905, wps=12661.9, ups=1.03, wpb=12279.2, bsz=437.3, num_updates=7600, lr=0.000162221, gnorm=0.588, clip=0, loss_scale=8, train_wall=96, gb_free=16.8, wall=7211
2023-08-15 17:11:29 | INFO | train_inner | epoch 006:    337 / 1474 loss=2.403, trans_loss=3.53, nll_loss=1.75, w2v_ctc_loss=1.283, task_loss=0.876, contrastive_loss=0.553, total=4170.52, n_correct=2404.42, ppl=3.36, accuracy=57.653, wps=12708.2, ups=1.02, wpb=12453.1, bsz=488.4, num_updates=7700, lr=0.000161165, gnorm=0.662, clip=0, loss_scale=8, train_wall=97, gb_free=15.3, wall=7309
2023-08-15 17:13:05 | INFO | train_inner | epoch 006:    437 / 1474 loss=2.341, trans_loss=3.531, nll_loss=1.75, w2v_ctc_loss=1.301, task_loss=0.897, contrastive_loss=0.259, total=4154.89, n_correct=2396.2, ppl=3.36, accuracy=57.672, wps=12989.3, ups=1.05, wpb=12405.6, bsz=470.4, num_updates=7800, lr=0.000160128, gnorm=0.616, clip=0, loss_scale=8, train_wall=95, gb_free=16, wall=7404
2023-08-15 17:14:41 | INFO | train_inner | epoch 006:    537 / 1474 loss=2.331, trans_loss=3.529, nll_loss=1.747, w2v_ctc_loss=1.299, task_loss=0.928, contrastive_loss=0.242, total=4174.46, n_correct=2414.37, ppl=3.36, accuracy=57.837, wps=12953.7, ups=1.04, wpb=12460.1, bsz=458.3, num_updates=7900, lr=0.000159111, gnorm=0.589, clip=0, loss_scale=8, train_wall=96, gb_free=16.9, wall=7500
2023-08-15 17:16:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-15 17:16:18 | INFO | train_inner | epoch 006:    638 / 1474 loss=2.316, trans_loss=3.53, nll_loss=1.748, w2v_ctc_loss=1.28, task_loss=0.894, contrastive_loss=0.237, total=4140.84, n_correct=2396.56, ppl=3.36, accuracy=57.876, wps=12718, ups=1.03, wpb=12359.2, bsz=467, num_updates=8000, lr=0.000158114, gnorm=0.591, clip=0, loss_scale=4, train_wall=97, gb_free=12.1, wall=7597
2023-08-15 17:16:18 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 17:16:43 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.267 | trans_loss 5.463 | nll_loss 2.81 | w2v_ctc_loss 1.549 | task_loss 4.511 | contrastive_loss 0.41 | total 4003.4 | n_correct 2459.4 | ppl 7.01 | accuracy 61.433 | uer 23.006 | wer 24.697 | raw_wer 24.697 | bleu 18.5 | wps 2254.8 | wpb 4003.4 | bsz 141.8 | num_updates 8000 | best_bleu 18.5
2023-08-15 17:16:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8000 updates
2023-08-15 17:16:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_6_8000.pt
2023-08-15 17:16:47 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_6_8000.pt
2023-08-15 17:17:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_6_8000.pt (epoch 6 @ 8000 updates, score 18.5) (writing took 62.08267597295344 seconds)
2023-08-15 17:19:22 | INFO | train_inner | epoch 006:    738 / 1474 loss=2.329, trans_loss=3.528, nll_loss=1.746, w2v_ctc_loss=1.3, task_loss=0.956, contrastive_loss=0.246, total=4147.61, n_correct=2404.62, ppl=3.36, accuracy=57.976, wps=6736.8, ups=0.54, wpb=12383.5, bsz=453.2, num_updates=8100, lr=0.000157135, gnorm=0.589, clip=0, loss_scale=4, train_wall=96, gb_free=15.9, wall=7781
2023-08-15 17:20:58 | INFO | train_inner | epoch 006:    838 / 1474 loss=2.31, trans_loss=3.528, nll_loss=1.746, w2v_ctc_loss=1.285, task_loss=0.977, contrastive_loss=0.225, total=4114.7, n_correct=2385.35, ppl=3.35, accuracy=57.971, wps=12781.7, ups=1.04, wpb=12284.9, bsz=442.6, num_updates=8200, lr=0.000156174, gnorm=0.572, clip=0, loss_scale=4, train_wall=95, gb_free=17.5, wall=7877
2023-08-15 17:22:35 | INFO | train_inner | epoch 006:    938 / 1474 loss=2.34, trans_loss=3.53, nll_loss=1.748, w2v_ctc_loss=1.291, task_loss=0.989, contrastive_loss=0.326, total=4082.44, n_correct=2363.94, ppl=3.36, accuracy=57.905, wps=12590.4, ups=1.03, wpb=12184.1, bsz=442.4, num_updates=8300, lr=0.00015523, gnorm=0.591, clip=0, loss_scale=4, train_wall=96, gb_free=15.8, wall=7974
2023-08-15 17:24:12 | INFO | train_inner | epoch 006:   1038 / 1474 loss=2.34, trans_loss=3.517, nll_loss=1.732, w2v_ctc_loss=1.277, task_loss=0.879, contrastive_loss=0.403, total=4168.55, n_correct=2431.79, ppl=3.32, accuracy=58.337, wps=12868.9, ups=1.03, wpb=12442.3, bsz=478.7, num_updates=8400, lr=0.000154303, gnorm=0.63, clip=0, loss_scale=4, train_wall=96, gb_free=15.3, wall=8071
2023-08-15 17:25:48 | INFO | train_inner | epoch 006:   1138 / 1474 loss=2.294, trans_loss=3.514, nll_loss=1.728, w2v_ctc_loss=1.275, task_loss=1.025, contrastive_loss=0.225, total=4075.88, n_correct=2380.03, ppl=3.31, accuracy=58.393, wps=12635.4, ups=1.04, wpb=12168.6, bsz=430.6, num_updates=8500, lr=0.000153393, gnorm=0.581, clip=0, loss_scale=4, train_wall=96, gb_free=13.7, wall=8167
2023-08-15 17:27:25 | INFO | train_inner | epoch 006:   1238 / 1474 loss=2.348, trans_loss=3.505, nll_loss=1.719, w2v_ctc_loss=1.261, task_loss=0.916, contrastive_loss=0.543, total=4136.41, n_correct=2424.82, ppl=3.29, accuracy=58.621, wps=12755, ups=1.03, wpb=12356.2, bsz=470.5, num_updates=8600, lr=0.000152499, gnorm=0.578, clip=0, loss_scale=4, train_wall=96, gb_free=15, wall=8264
2023-08-15 17:29:00 | INFO | train_inner | epoch 006:   1338 / 1474 loss=2.269, trans_loss=3.51, nll_loss=1.721, w2v_ctc_loss=1.258, task_loss=0.924, contrastive_loss=0.207, total=4123.87, n_correct=2427.41, ppl=3.3, accuracy=58.862, wps=12856.2, ups=1.05, wpb=12299.9, bsz=453.6, num_updates=8700, lr=0.00015162, gnorm=0.57, clip=0, loss_scale=4, train_wall=95, gb_free=15.9, wall=8360
2023-08-15 17:30:37 | INFO | train_inner | epoch 006:   1438 / 1474 loss=2.268, trans_loss=3.503, nll_loss=1.715, w2v_ctc_loss=1.257, task_loss=0.929, contrastive_loss=0.212, total=4197.44, n_correct=2471.44, ppl=3.28, accuracy=58.88, wps=12942.4, ups=1.03, wpb=12530.9, bsz=462.9, num_updates=8800, lr=0.000150756, gnorm=0.555, clip=0, loss_scale=4, train_wall=96, gb_free=16, wall=8457
2023-08-15 17:31:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 17:31:38 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.2 | trans_loss 5.405 | nll_loss 2.742 | w2v_ctc_loss 1.484 | task_loss 4.536 | contrastive_loss 0.384 | total 4003.4 | n_correct 2501.3 | ppl 6.69 | accuracy 62.479 | uer 22.154 | wer 23.828 | raw_wer 23.828 | bleu 18.92 | wps 1840.3 | wpb 4003.4 | bsz 141.8 | num_updates 8836 | best_bleu 18.92
2023-08-15 17:31:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8836 updates
2023-08-15 17:31:38 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 17:31:54 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 17:32:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 6 @ 8836 updates, score 18.92) (writing took 30.7675158418715 seconds)
2023-08-15 17:32:09 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-08-15 17:32:09 | INFO | train | epoch 006 | loss 2.329 | trans_loss 3.524 | nll_loss 1.741 | w2v_ctc_loss 1.286 | task_loss 0.934 | contrastive_loss 0.304 | total 4138.46 | n_correct 2401.45 | ppl 3.34 | accuracy 58.028 | wps 11554.4 | ups 0.94 | wpb 12355.3 | bsz 458.3 | num_updates 8836 | lr 0.000150448 | gnorm 0.593 | clip 0 | loss_scale 4 | train_wall 1413 | gb_free 14.7 | wall 8548
2023-08-15 17:32:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 17:32:09 | INFO | fairseq.trainer | begin training epoch 7
2023-08-15 17:32:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 17:33:20 | INFO | train_inner | epoch 007:     64 / 1474 loss=2.24, trans_loss=3.488, nll_loss=1.695, w2v_ctc_loss=1.229, task_loss=0.918, contrastive_loss=0.225, total=4105.94, n_correct=2440.19, ppl=3.24, accuracy=59.431, wps=7552.3, ups=0.62, wpb=12258, bsz=460.8, num_updates=8900, lr=0.000149906, gnorm=0.566, clip=0, loss_scale=4, train_wall=95, gb_free=14.6, wall=8619
2023-08-15 17:34:56 | INFO | train_inner | epoch 007:    164 / 1474 loss=2.236, trans_loss=3.479, nll_loss=1.683, w2v_ctc_loss=1.21, task_loss=0.952, contrastive_loss=0.295, total=4101.13, n_correct=2444.53, ppl=3.21, accuracy=59.606, wps=12757.9, ups=1.04, wpb=12245.3, bsz=452.9, num_updates=9000, lr=0.000149071, gnorm=0.555, clip=0, loss_scale=4, train_wall=95, gb_free=16.6, wall=8715
2023-08-15 17:36:32 | INFO | train_inner | epoch 007:    264 / 1474 loss=2.21, trans_loss=3.473, nll_loss=1.675, w2v_ctc_loss=1.209, task_loss=0.928, contrastive_loss=0.201, total=4143.65, n_correct=2476.38, ppl=3.19, accuracy=59.763, wps=12849.1, ups=1.04, wpb=12365.3, bsz=458, num_updates=9100, lr=0.00014825, gnorm=0.554, clip=0, loss_scale=4, train_wall=96, gb_free=15.6, wall=8811
2023-08-15 17:38:08 | INFO | train_inner | epoch 007:    364 / 1474 loss=2.261, trans_loss=3.478, nll_loss=1.681, w2v_ctc_loss=1.198, task_loss=0.905, contrastive_loss=0.462, total=4190.59, n_correct=2499.64, ppl=3.21, accuracy=59.649, wps=12941.1, ups=1.03, wpb=12506.9, bsz=477.2, num_updates=9200, lr=0.000147442, gnorm=0.561, clip=0, loss_scale=4, train_wall=96, gb_free=16.4, wall=8908
2023-08-15 17:39:45 | INFO | train_inner | epoch 007:    464 / 1474 loss=2.237, trans_loss=3.473, nll_loss=1.677, w2v_ctc_loss=1.192, task_loss=0.927, contrastive_loss=0.378, total=4154.13, n_correct=2486.95, ppl=3.2, accuracy=59.867, wps=12879.1, ups=1.04, wpb=12405.4, bsz=461.6, num_updates=9300, lr=0.000146647, gnorm=0.57, clip=0, loss_scale=4, train_wall=96, gb_free=16, wall=9004
2023-08-15 17:41:21 | INFO | train_inner | epoch 007:    564 / 1474 loss=2.201, trans_loss=3.472, nll_loss=1.672, w2v_ctc_loss=1.198, task_loss=0.911, contrastive_loss=0.206, total=4171.52, n_correct=2505.84, ppl=3.19, accuracy=60.07, wps=12984.8, ups=1.04, wpb=12446, bsz=461, num_updates=9400, lr=0.000145865, gnorm=0.543, clip=0, loss_scale=4, train_wall=95, gb_free=16.6, wall=9100
2023-08-15 17:42:57 | INFO | train_inner | epoch 007:    664 / 1474 loss=2.196, trans_loss=3.469, nll_loss=1.67, w2v_ctc_loss=1.196, task_loss=0.935, contrastive_loss=0.193, total=4151.13, n_correct=2498.11, ppl=3.18, accuracy=60.179, wps=12800.1, ups=1.03, wpb=12385.9, bsz=454, num_updates=9500, lr=0.000145095, gnorm=0.55, clip=0, loss_scale=4, train_wall=96, gb_free=12.3, wall=9197
2023-08-15 17:44:34 | INFO | train_inner | epoch 007:    764 / 1474 loss=2.19, trans_loss=3.461, nll_loss=1.662, w2v_ctc_loss=1.196, task_loss=0.977, contrastive_loss=0.189, total=4124.23, n_correct=2481.36, ppl=3.16, accuracy=60.165, wps=12739.4, ups=1.03, wpb=12314.5, bsz=446.8, num_updates=9600, lr=0.000144338, gnorm=0.549, clip=0, loss_scale=4, train_wall=96, gb_free=13.8, wall=9293
2023-08-15 17:46:11 | INFO | train_inner | epoch 007:    864 / 1474 loss=2.198, trans_loss=3.471, nll_loss=1.673, w2v_ctc_loss=1.197, task_loss=0.937, contrastive_loss=0.208, total=4148.43, n_correct=2492.53, ppl=3.19, accuracy=60.084, wps=12732.3, ups=1.03, wpb=12380.2, bsz=461.8, num_updates=9700, lr=0.000143592, gnorm=0.559, clip=0, loss_scale=4, train_wall=97, gb_free=16.1, wall=9391
2023-08-15 17:47:48 | INFO | train_inner | epoch 007:    964 / 1474 loss=2.208, trans_loss=3.461, nll_loss=1.662, w2v_ctc_loss=1.184, task_loss=0.895, contrastive_loss=0.301, total=4141.1, n_correct=2497.48, ppl=3.16, accuracy=60.31, wps=12826.2, ups=1.04, wpb=12362.4, bsz=473.7, num_updates=9800, lr=0.000142857, gnorm=0.55, clip=0, loss_scale=4, train_wall=96, gb_free=13.4, wall=9487
2023-08-15 17:49:24 | INFO | train_inner | epoch 007:   1064 / 1474 loss=2.179, trans_loss=3.467, nll_loss=1.669, w2v_ctc_loss=1.19, task_loss=0.977, contrastive_loss=0.172, total=4100.93, n_correct=2474.71, ppl=3.18, accuracy=60.345, wps=12667.3, ups=1.03, wpb=12243.4, bsz=437.6, num_updates=9900, lr=0.000142134, gnorm=0.541, clip=0, loss_scale=4, train_wall=96, gb_free=14.4, wall=9584
2023-08-15 17:51:01 | INFO | train_inner | epoch 007:   1164 / 1474 loss=2.237, trans_loss=3.456, nll_loss=1.657, w2v_ctc_loss=1.183, task_loss=0.91, contrastive_loss=0.434, total=4139.88, n_correct=2499.84, ppl=3.15, accuracy=60.384, wps=12803, ups=1.04, wpb=12369.6, bsz=471.4, num_updates=10000, lr=0.000141421, gnorm=0.554, clip=0, loss_scale=4, train_wall=96, gb_free=16.3, wall=9680
2023-08-15 17:51:01 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 17:51:29 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.112 | trans_loss 5.339 | nll_loss 2.661 | w2v_ctc_loss 1.359 | task_loss 4.568 | contrastive_loss 0.37 | total 4003.4 | n_correct 2541.7 | ppl 6.33 | accuracy 63.489 | uer 20.561 | wer 22.255 | raw_wer 22.255 | bleu 19.69 | wps 1870.1 | wpb 4003.4 | bsz 141.8 | num_updates 10000 | best_bleu 19.69
2023-08-15 17:51:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10000 updates
2023-08-15 17:51:29 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_7_10000.pt
2023-08-15 17:51:32 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_7_10000.pt
2023-08-15 17:52:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_7_10000.pt (epoch 7 @ 10000 updates, score 19.69) (writing took 46.24568055383861 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:0')
2023-08-15 17:53:51 | INFO | train_inner | epoch 007:   1264 / 1474 loss=2.17, trans_loss=3.458, nll_loss=1.659, w2v_ctc_loss=1.177, task_loss=0.952, contrastive_loss=0.198, total=4129.16, n_correct=2493.14, ppl=3.16, accuracy=60.379, wps=7252.8, ups=0.59, wpb=12331, bsz=448.9, num_updates=10100, lr=0.00014072, gnorm=0.426, clip=0, loss_scale=8, train_wall=95, gb_free=16.4, wall=9850
2023-08-15 17:55:26 | INFO | train_inner | epoch 007:   1364 / 1474 loss=2.188, trans_loss=3.452, nll_loss=1.651, w2v_ctc_loss=1.183, task_loss=0.874, contrastive_loss=0.238, total=4177.71, n_correct=2532.48, ppl=3.14, accuracy=60.619, wps=13086.4, ups=1.05, wpb=12473.7, bsz=478.6, num_updates=10200, lr=0.000140028, gnorm=0.424, clip=0, loss_scale=8, train_wall=95, gb_free=16.2, wall=9946
2023-08-15 17:57:04 | INFO | train_inner | epoch 007:   1464 / 1474 loss=2.197, trans_loss=3.456, nll_loss=1.657, w2v_ctc_loss=1.184, task_loss=1.01, contrastive_loss=0.295, total=4107.01, n_correct=2484.01, ppl=3.15, accuracy=60.482, wps=12546.3, ups=1.02, wpb=12270.2, bsz=442.8, num_updates=10300, lr=0.000139347, gnorm=0.425, clip=0, loss_scale=8, train_wall=97, gb_free=12.7, wall=10043
2023-08-15 17:57:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:6')
2023-08-15 17:57:38 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.106 | trans_loss 5.323 | nll_loss 2.637 | w2v_ctc_loss 1.384 | task_loss 4.58 | contrastive_loss 0.361 | total 4003.4 | n_correct 2552.3 | ppl 6.22 | accuracy 63.753 | uer 20.715 | wer 22.43 | raw_wer 22.43 | bleu 19.85 | wps 2039.6 | wpb 4003.4 | bsz 141.8 | num_updates 10310 | best_bleu 19.85
2023-08-15 17:57:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10310 updates
2023-08-15 17:57:38 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 17:57:56 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 17:58:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 7 @ 10310 updates, score 19.85) (writing took 33.86151970922947 seconds)
2023-08-15 17:58:11 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-08-15 17:58:12 | INFO | train | epoch 007 | loss 2.208 | trans_loss 3.467 | nll_loss 1.668 | w2v_ctc_loss 1.194 | task_loss 0.935 | contrastive_loss 0.268 | total 4138.65 | n_correct 2488.05 | ppl 3.18 | accuracy 60.118 | wps 11656 | ups 0.94 | wpb 12355.8 | bsz 458.5 | num_updates 10310 | lr 0.000139279 | gnorm 0.527 | clip 0 | loss_scale 8 | train_wall 1412 | gb_free 12.8 | wall 10111
2023-08-15 17:58:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 17:58:12 | INFO | fairseq.trainer | begin training epoch 8
2023-08-15 17:58:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 17:59:46 | INFO | train_inner | epoch 008:     90 / 1474 loss=2.139, trans_loss=3.443, nll_loss=1.635, w2v_ctc_loss=1.145, task_loss=0.995, contrastive_loss=0.191, total=4106.01, n_correct=2509.16, ppl=3.11, accuracy=61.109, wps=7548.4, ups=0.62, wpb=12242.2, bsz=440.9, num_updates=10400, lr=0.000138675, gnorm=0.422, clip=0, loss_scale=8, train_wall=95, gb_free=16.8, wall=10206
2023-08-15 18:01:22 | INFO | train_inner | epoch 008:    190 / 1474 loss=2.138, trans_loss=3.433, nll_loss=1.622, w2v_ctc_loss=1.142, task_loss=1.013, contrastive_loss=0.21, total=4043.12, n_correct=2481.44, ppl=3.08, accuracy=61.374, wps=12618.6, ups=1.05, wpb=12058.4, bsz=430, num_updates=10500, lr=0.000138013, gnorm=0.429, clip=0, loss_scale=8, train_wall=95, gb_free=12.8, wall=10301
2023-08-15 18:02:58 | INFO | train_inner | epoch 008:    290 / 1474 loss=2.131, trans_loss=3.426, nll_loss=1.616, w2v_ctc_loss=1.137, task_loss=0.878, contrastive_loss=0.205, total=4207.9, n_correct=2589.16, ppl=3.07, accuracy=61.531, wps=13040.2, ups=1.04, wpb=12558.3, bsz=488.1, num_updates=10600, lr=0.000137361, gnorm=0.423, clip=0, loss_scale=8, train_wall=96, gb_free=13.5, wall=10397
2023-08-15 18:04:35 | INFO | train_inner | epoch 008:    390 / 1474 loss=2.15, trans_loss=3.434, nll_loss=1.625, w2v_ctc_loss=1.157, task_loss=0.985, contrastive_loss=0.229, total=4134.6, n_correct=2531.2, ppl=3.08, accuracy=61.22, wps=12688.3, ups=1.03, wpb=12337.2, bsz=444.7, num_updates=10700, lr=0.000136717, gnorm=0.424, clip=0, loss_scale=8, train_wall=97, gb_free=17, wall=10495
2023-08-15 18:06:13 | INFO | train_inner | epoch 008:    490 / 1474 loss=2.203, trans_loss=3.431, nll_loss=1.623, w2v_ctc_loss=1.138, task_loss=0.847, contrastive_loss=0.493, total=4196.6, n_correct=2573.47, ppl=3.08, accuracy=61.323, wps=12881.4, ups=1.03, wpb=12528.1, bsz=501.4, num_updates=10800, lr=0.000136083, gnorm=0.459, clip=0, loss_scale=8, train_wall=97, gb_free=12.3, wall=10592
2023-08-15 18:07:49 | INFO | train_inner | epoch 008:    590 / 1474 loss=2.135, trans_loss=3.428, nll_loss=1.623, w2v_ctc_loss=1.159, task_loss=1.018, contrastive_loss=0.163, total=4065.55, n_correct=2490.99, ppl=3.08, accuracy=61.271, wps=12634.9, ups=1.04, wpb=12153.4, bsz=428.3, num_updates=10900, lr=0.000135457, gnorm=0.423, clip=0, loss_scale=8, train_wall=96, gb_free=15.8, wall=10688
2023-08-15 18:09:26 | INFO | train_inner | epoch 008:    690 / 1474 loss=2.125, trans_loss=3.425, nll_loss=1.615, w2v_ctc_loss=1.148, task_loss=0.967, contrastive_loss=0.173, total=4135.41, n_correct=2547.49, ppl=3.06, accuracy=61.602, wps=12743.1, ups=1.03, wpb=12343.8, bsz=447.2, num_updates=11000, lr=0.00013484, gnorm=0.424, clip=0, loss_scale=8, train_wall=96, gb_free=15.6, wall=10785
2023-08-15 18:11:01 | INFO | train_inner | epoch 008:    790 / 1474 loss=2.137, trans_loss=3.42, nll_loss=1.612, w2v_ctc_loss=1.142, task_loss=0.948, contrastive_loss=0.258, total=4128.86, n_correct=2544.36, ppl=3.06, accuracy=61.624, wps=12910, ups=1.05, wpb=12339, bsz=452.1, num_updates=11100, lr=0.000134231, gnorm=0.427, clip=0, loss_scale=8, train_wall=95, gb_free=15.8, wall=10881
2023-08-15 18:12:37 | INFO | train_inner | epoch 008:    890 / 1474 loss=2.135, trans_loss=3.424, nll_loss=1.616, w2v_ctc_loss=1.132, task_loss=0.904, contrastive_loss=0.265, total=4166.92, n_correct=2565.32, ppl=3.07, accuracy=61.564, wps=12944.3, ups=1.04, wpb=12446.8, bsz=471.5, num_updates=11200, lr=0.000133631, gnorm=0.422, clip=0, loss_scale=8, train_wall=96, gb_free=14, wall=10977
2023-08-15 18:14:13 | INFO | train_inner | epoch 008:    990 / 1474 loss=2.101, trans_loss=3.423, nll_loss=1.613, w2v_ctc_loss=1.126, task_loss=0.903, contrastive_loss=0.166, total=4150.39, n_correct=2567.07, ppl=3.06, accuracy=61.851, wps=12925.2, ups=1.04, wpb=12390.2, bsz=462.8, num_updates=11300, lr=0.000133038, gnorm=0.411, clip=0, loss_scale=8, train_wall=95, gb_free=16.9, wall=11073
2023-08-15 18:15:51 | INFO | train_inner | epoch 008:   1090 / 1474 loss=2.162, trans_loss=3.428, nll_loss=1.619, w2v_ctc_loss=1.138, task_loss=0.926, contrastive_loss=0.392, total=4197.39, n_correct=2578.46, ppl=3.07, accuracy=61.43, wps=12832.8, ups=1.02, wpb=12529.5, bsz=466.4, num_updates=11400, lr=0.000132453, gnorm=0.484, clip=0, loss_scale=8, train_wall=97, gb_free=16.5, wall=11170
2023-08-15 18:17:27 | INFO | train_inner | epoch 008:   1190 / 1474 loss=2.112, trans_loss=3.417, nll_loss=1.607, w2v_ctc_loss=1.133, task_loss=0.887, contrastive_loss=0.175, total=4180.55, n_correct=2581.63, ppl=3.05, accuracy=61.753, wps=12992, ups=1.04, wpb=12487.2, bsz=472.7, num_updates=11500, lr=0.000131876, gnorm=0.416, clip=0, loss_scale=8, train_wall=96, gb_free=16.9, wall=11266
2023-08-15 18:19:02 | INFO | train_inner | epoch 008:   1290 / 1474 loss=2.121, trans_loss=3.422, nll_loss=1.614, w2v_ctc_loss=1.141, task_loss=0.979, contrastive_loss=0.198, total=4062.6, n_correct=2498.46, ppl=3.06, accuracy=61.499, wps=12747, ups=1.05, wpb=12135.3, bsz=437.8, num_updates=11600, lr=0.000131306, gnorm=0.426, clip=0, loss_scale=8, train_wall=95, gb_free=12.6, wall=11362
2023-08-15 18:20:38 | INFO | train_inner | epoch 008:   1390 / 1474 loss=2.133, trans_loss=3.424, nll_loss=1.615, w2v_ctc_loss=1.132, task_loss=0.91, contrastive_loss=0.265, total=4159.11, n_correct=2567.43, ppl=3.06, accuracy=61.73, wps=13023.2, ups=1.05, wpb=12419, bsz=468.7, num_updates=11700, lr=0.000130744, gnorm=0.415, clip=0, loss_scale=8, train_wall=95, gb_free=12.9, wall=11457
2023-08-15 18:21:58 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 18:22:23 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 4.071 | trans_loss 5.283 | nll_loss 2.587 | w2v_ctc_loss 1.363 | task_loss 4.614 | contrastive_loss 0.356 | total 4003.4 | n_correct 2578.3 | ppl 6.01 | accuracy 64.403 | uer 20.566 | wer 22.464 | raw_wer 22.464 | bleu 20.33 | wps 1782.3 | wpb 4003.4 | bsz 141.8 | num_updates 11784 | best_bleu 20.33
2023-08-15 18:22:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11784 updates
2023-08-15 18:22:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 18:22:40 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 18:22:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 8 @ 11784 updates, score 20.33) (writing took 32.8543429300189 seconds)
2023-08-15 18:22:57 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-08-15 18:22:57 | INFO | train | epoch 008 | loss 2.137 | trans_loss 3.427 | nll_loss 1.618 | w2v_ctc_loss 1.14 | task_loss 0.936 | contrastive_loss 0.247 | total 4138.65 | n_correct 2545.91 | ppl 3.07 | accuracy 61.515 | wps 12262.2 | ups 0.99 | wpb 12355.8 | bsz 458.5 | num_updates 11784 | lr 0.000130277 | gnorm 0.429 | clip 0 | loss_scale 8 | train_wall 1410 | gb_free 16.5 | wall 11596
2023-08-15 18:22:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 18:22:57 | INFO | fairseq.trainer | begin training epoch 9
2023-08-15 18:22:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 18:23:21 | INFO | train_inner | epoch 009:     16 / 1474 loss=2.132, trans_loss=3.418, nll_loss=1.606, w2v_ctc_loss=1.118, task_loss=0.916, contrastive_loss=0.363, total=4121.25, n_correct=2551.89, ppl=3.04, accuracy=61.92, wps=7538.1, ups=0.61, wpb=12298.5, bsz=466, num_updates=11800, lr=0.000130189, gnorm=0.429, clip=0, loss_scale=8, train_wall=96, gb_free=17.4, wall=11620
2023-08-15 18:24:57 | INFO | train_inner | epoch 009:    116 / 1474 loss=2.071, trans_loss=3.392, nll_loss=1.574, w2v_ctc_loss=1.088, task_loss=0.881, contrastive_loss=0.195, total=4191.82, n_correct=2629.63, ppl=2.98, accuracy=62.732, wps=13070.3, ups=1.04, wpb=12518.2, bsz=479.9, num_updates=11900, lr=0.000129641, gnorm=0.407, clip=0, loss_scale=8, train_wall=95, gb_free=15.6, wall=11716
2023-08-15 18:26:33 | INFO | train_inner | epoch 009:    216 / 1474 loss=2.061, trans_loss=3.396, nll_loss=1.579, w2v_ctc_loss=1.093, task_loss=1.008, contrastive_loss=0.149, total=4061.27, n_correct=2543.27, ppl=2.99, accuracy=62.623, wps=12640.8, ups=1.04, wpb=12126.5, bsz=431.4, num_updates=12000, lr=0.000129099, gnorm=0.414, clip=0, loss_scale=8, train_wall=95, gb_free=17.3, wall=11812
2023-08-15 18:26:33 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 18:26:59 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.083 | trans_loss 5.289 | nll_loss 2.599 | w2v_ctc_loss 1.392 | task_loss 4.546 | contrastive_loss 0.358 | total 4003.4 | n_correct 2570.6 | ppl 6.06 | accuracy 64.21 | uer 21.063 | wer 22.967 | raw_wer 22.967 | bleu 20.38 | wps 2023.2 | wpb 4003.4 | bsz 141.8 | num_updates 12000 | best_bleu 20.38
2023-08-15 18:26:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12000 updates
2023-08-15 18:26:59 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_9_12000.pt
2023-08-15 18:27:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_9_12000.pt
2023-08-15 18:27:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_9_12000.pt (epoch 9 @ 12000 updates, score 20.38) (writing took 30.591106111183763 seconds)
2023-08-15 18:29:08 | INFO | train_inner | epoch 009:    316 / 1474 loss=2.066, trans_loss=3.386, nll_loss=1.568, w2v_ctc_loss=1.085, task_loss=0.882, contrastive_loss=0.202, total=4146.43, n_correct=2602.27, ppl=2.97, accuracy=62.759, wps=7988.9, ups=0.64, wpb=12389.9, bsz=474.6, num_updates=12100, lr=0.000128565, gnorm=0.412, clip=0, loss_scale=16, train_wall=95, gb_free=15.7, wall=11967
2023-08-15 18:30:45 | INFO | train_inner | epoch 009:    416 / 1474 loss=2.067, trans_loss=3.4, nll_loss=1.583, w2v_ctc_loss=1.09, task_loss=0.921, contrastive_loss=0.165, total=4194.84, n_correct=2618.79, ppl=3, accuracy=62.429, wps=12885.2, ups=1.03, wpb=12524.6, bsz=466.7, num_updates=12200, lr=0.000128037, gnorm=0.405, clip=0, loss_scale=16, train_wall=97, gb_free=15.8, wall=12064
2023-08-15 18:32:20 | INFO | train_inner | epoch 009:    516 / 1474 loss=2.092, trans_loss=3.4, nll_loss=1.583, w2v_ctc_loss=1.114, task_loss=0.981, contrastive_loss=0.216, total=4124.3, n_correct=2570.45, ppl=3, accuracy=62.325, wps=12908.8, ups=1.05, wpb=12310.3, bsz=439.5, num_updates=12300, lr=0.000127515, gnorm=0.433, clip=0, loss_scale=16, train_wall=95, gb_free=11, wall=12160
2023-08-15 18:33:56 | INFO | train_inner | epoch 009:    616 / 1474 loss=2.062, trans_loss=3.392, nll_loss=1.576, w2v_ctc_loss=1.091, task_loss=0.957, contrastive_loss=0.177, total=4120.96, n_correct=2576.84, ppl=2.98, accuracy=62.53, wps=12803.6, ups=1.04, wpb=12316.4, bsz=453.4, num_updates=12400, lr=0.000127, gnorm=0.417, clip=0, loss_scale=16, train_wall=96, gb_free=15.8, wall=12256
2023-08-15 18:35:33 | INFO | train_inner | epoch 009:    716 / 1474 loss=2.097, trans_loss=3.399, nll_loss=1.584, w2v_ctc_loss=1.11, task_loss=0.952, contrastive_loss=0.257, total=4088.53, n_correct=2551.77, ppl=3, accuracy=62.413, wps=12694.1, ups=1.04, wpb=12213.7, bsz=451.4, num_updates=12500, lr=0.000126491, gnorm=0.415, clip=0, loss_scale=16, train_wall=96, gb_free=16.6, wall=12352
2023-08-15 18:37:09 | INFO | train_inner | epoch 009:    816 / 1474 loss=2.133, trans_loss=3.393, nll_loss=1.578, w2v_ctc_loss=1.1, task_loss=0.844, contrastive_loss=0.405, total=4220.43, n_correct=2637.42, ppl=2.99, accuracy=62.492, wps=13020.4, ups=1.03, wpb=12611.1, bsz=501.1, num_updates=12600, lr=0.000125988, gnorm=0.427, clip=0, loss_scale=16, train_wall=96, gb_free=14, wall=12449
2023-08-15 18:38:47 | INFO | train_inner | epoch 009:    916 / 1474 loss=2.103, trans_loss=3.396, nll_loss=1.577, w2v_ctc_loss=1.092, task_loss=0.967, contrastive_loss=0.388, total=4146.05, n_correct=2596.36, ppl=2.98, accuracy=62.622, wps=12710.3, ups=1.03, wpb=12371.5, bsz=450.3, num_updates=12700, lr=0.000125491, gnorm=0.418, clip=0, loss_scale=16, train_wall=97, gb_free=17.4, wall=12546
2023-08-15 18:40:24 | INFO | train_inner | epoch 009:   1016 / 1474 loss=2.07, trans_loss=3.403, nll_loss=1.587, w2v_ctc_loss=1.101, task_loss=1.043, contrastive_loss=0.164, total=4101.48, n_correct=2556.23, ppl=3, accuracy=62.325, wps=12637.7, ups=1.03, wpb=12241.7, bsz=424.4, num_updates=12800, lr=0.000125, gnorm=0.416, clip=0, loss_scale=16, train_wall=96, gb_free=15.5, wall=12643
2023-08-15 18:42:00 | INFO | train_inner | epoch 009:   1116 / 1474 loss=2.068, trans_loss=3.399, nll_loss=1.579, w2v_ctc_loss=1.089, task_loss=0.881, contrastive_loss=0.186, total=4179.09, n_correct=2622.43, ppl=2.99, accuracy=62.751, wps=12959.3, ups=1.04, wpb=12457.7, bsz=474.7, num_updates=12900, lr=0.000124515, gnorm=0.412, clip=0, loss_scale=16, train_wall=96, gb_free=14.8, wall=12739
2023-08-15 18:43:37 | INFO | train_inner | epoch 009:   1216 / 1474 loss=2.075, trans_loss=3.399, nll_loss=1.583, w2v_ctc_loss=1.107, task_loss=0.99, contrastive_loss=0.17, total=4140.66, n_correct=2584, ppl=3, accuracy=62.406, wps=12774.4, ups=1.03, wpb=12363.4, bsz=448.1, num_updates=13000, lr=0.000124035, gnorm=0.414, clip=0, loss_scale=16, train_wall=96, gb_free=16.7, wall=12836
2023-08-15 18:45:13 | INFO | train_inner | epoch 009:   1316 / 1474 loss=2.099, trans_loss=3.389, nll_loss=1.569, w2v_ctc_loss=1.083, task_loss=0.852, contrastive_loss=0.356, total=4204.43, n_correct=2644.73, ppl=2.97, accuracy=62.903, wps=13078.2, ups=1.04, wpb=12544.9, bsz=492.5, num_updates=13100, lr=0.00012356, gnorm=0.405, clip=0, loss_scale=16, train_wall=95, gb_free=17.4, wall=12932
2023-08-15 18:46:48 | INFO | train_inner | epoch 009:   1416 / 1474 loss=2.062, trans_loss=3.403, nll_loss=1.587, w2v_ctc_loss=1.098, task_loss=1.013, contrastive_loss=0.144, total=4069.19, n_correct=2545.59, ppl=3.01, accuracy=62.558, wps=12684.7, ups=1.04, wpb=12143.2, bsz=427.7, num_updates=13200, lr=0.000123091, gnorm=0.411, clip=0, loss_scale=16, train_wall=95, gb_free=16.1, wall=13028
2023-08-15 18:47:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 18:48:08 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.033 | trans_loss 5.245 | nll_loss 2.541 | w2v_ctc_loss 1.349 | task_loss 4.624 | contrastive_loss 0.33 | total 4003.4 | n_correct 2597.2 | ppl 5.82 | accuracy 64.875 | uer 19.587 | wer 21.431 | raw_wer 21.431 | bleu 20.67 | wps 2046.8 | wpb 4003.4 | bsz 141.8 | num_updates 13258 | best_bleu 20.67
2023-08-15 18:48:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 13258 updates
2023-08-15 18:48:08 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 18:48:23 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 18:48:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 9 @ 13258 updates, score 20.67) (writing took 36.77672202512622 seconds)
2023-08-15 18:48:45 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-08-15 18:48:45 | INFO | train | epoch 009 | loss 2.081 | trans_loss 3.396 | nll_loss 1.579 | w2v_ctc_loss 1.096 | task_loss 0.936 | contrastive_loss 0.233 | total 4138.65 | n_correct 2589.51 | ppl 2.99 | accuracy 62.569 | wps 11762.1 | ups 0.95 | wpb 12355.8 | bsz 458.5 | num_updates 13258 | lr 0.000122822 | gnorm 0.415 | clip 0 | loss_scale 16 | train_wall 1411 | gb_free 11.1 | wall 13145
2023-08-15 18:48:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 18:48:46 | INFO | fairseq.trainer | begin training epoch 10
2023-08-15 18:48:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 18:49:34 | INFO | train_inner | epoch 010:     42 / 1474 loss=2.062, trans_loss=3.385, nll_loss=1.564, w2v_ctc_loss=1.072, task_loss=0.894, contrastive_loss=0.24, total=4100.8, n_correct=2587.47, ppl=2.96, accuracy=63.097, wps=7373.8, ups=0.6, wpb=12238.2, bsz=469.4, num_updates=13300, lr=0.000122628, gnorm=0.408, clip=0, loss_scale=16, train_wall=96, gb_free=15.9, wall=13194
2023-08-15 18:51:10 | INFO | train_inner | epoch 010:    142 / 1474 loss=2.016, trans_loss=3.368, nll_loss=1.543, w2v_ctc_loss=1.045, task_loss=0.885, contrastive_loss=0.163, total=4247.35, n_correct=2698.97, ppl=2.91, accuracy=63.545, wps=13269.6, ups=1.05, wpb=12684.5, bsz=479.6, num_updates=13400, lr=0.000122169, gnorm=0.398, clip=0, loss_scale=16, train_wall=95, gb_free=11.1, wall=13289
2023-08-15 18:52:46 | INFO | train_inner | epoch 010:    242 / 1474 loss=2.046, trans_loss=3.366, nll_loss=1.539, w2v_ctc_loss=1.055, task_loss=0.924, contrastive_loss=0.284, total=4122.82, n_correct=2618.77, ppl=2.91, accuracy=63.519, wps=12845.2, ups=1.04, wpb=12303.3, bsz=461.4, num_updates=13500, lr=0.000121716, gnorm=0.404, clip=0, loss_scale=16, train_wall=95, gb_free=15.9, wall=13385
2023-08-15 18:54:22 | INFO | train_inner | epoch 010:    342 / 1474 loss=2.022, trans_loss=3.365, nll_loss=1.543, w2v_ctc_loss=1.049, task_loss=0.947, contrastive_loss=0.196, total=4138.27, n_correct=2626.64, ppl=2.91, accuracy=63.472, wps=12769.2, ups=1.03, wpb=12371, bsz=453.8, num_updates=13600, lr=0.000121268, gnorm=0.402, clip=0, loss_scale=16, train_wall=96, gb_free=16, wall=13482
2023-08-15 18:56:00 | INFO | train_inner | epoch 010:    442 / 1474 loss=2.048, trans_loss=3.372, nll_loss=1.549, w2v_ctc_loss=1.035, task_loss=0.899, contrastive_loss=0.375, total=4196.37, n_correct=2658.47, ppl=2.93, accuracy=63.352, wps=12879.1, ups=1.03, wpb=12528, bsz=481.1, num_updates=13700, lr=0.000120824, gnorm=0.407, clip=0, loss_scale=16, train_wall=97, gb_free=15.6, wall=13579
2023-08-15 18:57:36 | INFO | train_inner | epoch 010:    542 / 1474 loss=2.031, trans_loss=3.38, nll_loss=1.555, w2v_ctc_loss=1.069, task_loss=1.004, contrastive_loss=0.15, total=4102.8, n_correct=2595.91, ppl=2.94, accuracy=63.272, wps=12779.1, ups=1.04, wpb=12234.1, bsz=437.8, num_updates=13800, lr=0.000120386, gnorm=0.408, clip=0, loss_scale=16, train_wall=95, gb_free=16.7, wall=13675
2023-08-15 18:59:12 | INFO | train_inner | epoch 010:    642 / 1474 loss=2.054, trans_loss=3.378, nll_loss=1.555, w2v_ctc_loss=1.062, task_loss=0.892, contrastive_loss=0.267, total=4176.56, n_correct=2643.18, ppl=2.94, accuracy=63.286, wps=12958.1, ups=1.04, wpb=12464, bsz=477.2, num_updates=13900, lr=0.000119952, gnorm=0.405, clip=0, loss_scale=16, train_wall=96, gb_free=15.7, wall=13771
2023-08-15 19:00:47 | INFO | train_inner | epoch 010:    742 / 1474 loss=2.035, trans_loss=3.374, nll_loss=1.55, w2v_ctc_loss=1.077, task_loss=0.938, contrastive_loss=0.149, total=4125.87, n_correct=2611.99, ppl=2.93, accuracy=63.308, wps=12928.8, ups=1.05, wpb=12315.3, bsz=454.4, num_updates=14000, lr=0.000119523, gnorm=0.408, clip=0, loss_scale=16, train_wall=95, gb_free=13.9, wall=13866
2023-08-15 19:00:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 19:01:14 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.035 | trans_loss 5.239 | nll_loss 2.53 | w2v_ctc_loss 1.372 | task_loss 4.632 | contrastive_loss 0.333 | total 4003.4 | n_correct 2607.6 | ppl 5.78 | accuracy 65.135 | uer 19.537 | wer 21.267 | raw_wer 21.267 | bleu 21.29 | wps 2175.1 | wpb 4003.4 | bsz 141.8 | num_updates 14000 | best_bleu 21.29
2023-08-15 19:01:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14000 updates
2023-08-15 19:01:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_10_14000.pt
2023-08-15 19:01:18 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_10_14000.pt
2023-08-15 19:02:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_10_14000.pt (epoch 10 @ 14000 updates, score 21.29) (writing took 55.88899783976376 seconds)
2023-08-15 19:03:48 | INFO | train_inner | epoch 010:    842 / 1474 loss=2.011, trans_loss=3.369, nll_loss=1.545, w2v_ctc_loss=1.046, task_loss=0.928, contrastive_loss=0.149, total=4128.44, n_correct=2622.95, ppl=2.92, accuracy=63.534, wps=6816.9, ups=0.55, wpb=12327.8, bsz=456.3, num_updates=14100, lr=0.000119098, gnorm=0.399, clip=0, loss_scale=16, train_wall=96, gb_free=14.4, wall=14047
2023-08-15 19:05:23 | INFO | train_inner | epoch 010:    942 / 1474 loss=2.032, trans_loss=3.372, nll_loss=1.546, w2v_ctc_loss=1.059, task_loss=0.9, contrastive_loss=0.188, total=4160.94, n_correct=2638.71, ppl=2.92, accuracy=63.416, wps=13086.2, ups=1.05, wpb=12411.1, bsz=468.1, num_updates=14200, lr=0.000118678, gnorm=0.406, clip=0, loss_scale=32, train_wall=94, gb_free=15, wall=14142
2023-08-15 19:06:59 | INFO | train_inner | epoch 010:   1042 / 1474 loss=2.023, trans_loss=3.371, nll_loss=1.547, w2v_ctc_loss=1.061, task_loss=1.014, contrastive_loss=0.162, total=4067.53, n_correct=2575.42, ppl=2.92, accuracy=63.317, wps=12655.8, ups=1.04, wpb=12145, bsz=434.3, num_updates=14300, lr=0.000118262, gnorm=0.413, clip=0, loss_scale=32, train_wall=95, gb_free=16.5, wall=14238
2023-08-15 19:08:34 | INFO | train_inner | epoch 010:   1142 / 1474 loss=2.035, trans_loss=3.38, nll_loss=1.559, w2v_ctc_loss=1.079, task_loss=1.043, contrastive_loss=0.145, total=4044.03, n_correct=2550.15, ppl=2.95, accuracy=63.06, wps=12683.3, ups=1.05, wpb=12074.4, bsz=422.3, num_updates=14400, lr=0.000117851, gnorm=0.414, clip=0, loss_scale=32, train_wall=95, gb_free=17, wall=14333
2023-08-15 19:10:09 | INFO | train_inner | epoch 010:   1242 / 1474 loss=2.018, trans_loss=3.365, nll_loss=1.543, w2v_ctc_loss=1.064, task_loss=0.96, contrastive_loss=0.14, total=4110.41, n_correct=2605.06, ppl=2.91, accuracy=63.377, wps=12930.2, ups=1.05, wpb=12291.6, bsz=445.2, num_updates=14500, lr=0.000117444, gnorm=0.409, clip=0, loss_scale=32, train_wall=95, gb_free=16.1, wall=14428
2023-08-15 19:11:44 | INFO | train_inner | epoch 010:   1342 / 1474 loss=2.021, trans_loss=3.372, nll_loss=1.549, w2v_ctc_loss=1.065, task_loss=0.957, contrastive_loss=0.15, total=4121.38, n_correct=2614.37, ppl=2.93, accuracy=63.434, wps=12883.3, ups=1.05, wpb=12308.4, bsz=451, num_updates=14600, lr=0.000117041, gnorm=0.406, clip=0, loss_scale=32, train_wall=95, gb_free=13.5, wall=14524
2023-08-15 19:13:21 | INFO | train_inner | epoch 010:   1442 / 1474 loss=2.08, trans_loss=3.381, nll_loss=1.558, w2v_ctc_loss=1.047, task_loss=0.885, contrastive_loss=0.402, total=4192.39, n_correct=2647.67, ppl=2.95, accuracy=63.154, wps=12968.8, ups=1.04, wpb=12506.1, bsz=482, num_updates=14700, lr=0.000116642, gnorm=0.411, clip=0, loss_scale=32, train_wall=96, gb_free=16.7, wall=14620
2023-08-15 19:13:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 19:14:15 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.022 | trans_loss 5.228 | nll_loss 2.516 | w2v_ctc_loss 1.357 | task_loss 4.599 | contrastive_loss 0.332 | total 4003.4 | n_correct 2617.1 | ppl 5.72 | accuracy 65.372 | uer 19.128 | wer 20.872 | raw_wer 20.872 | bleu 21.35 | wps 2122.2 | wpb 4003.4 | bsz 141.8 | num_updates 14732 | best_bleu 21.35
2023-08-15 19:14:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14732 updates
2023-08-15 19:14:15 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 19:14:31 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 19:14:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 10 @ 14732 updates, score 21.35) (writing took 30.972335040569305 seconds)
2023-08-15 19:14:46 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-08-15 19:14:46 | INFO | train | epoch 010 | loss 2.035 | trans_loss 3.372 | nll_loss 1.548 | w2v_ctc_loss 1.057 | task_loss 0.937 | contrastive_loss 0.219 | total 4138.65 | n_correct 2622.89 | ppl 2.92 | accuracy 63.375 | wps 11666.4 | ups 0.94 | wpb 12355.8 | bsz 458.5 | num_updates 14732 | lr 0.000116516 | gnorm 0.406 | clip 0 | loss_scale 32 | train_wall 1406 | gb_free 16.9 | wall 14706
2023-08-15 19:14:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 19:14:47 | INFO | fairseq.trainer | begin training epoch 11
2023-08-15 19:14:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 19:15:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-15 19:15:59 | INFO | train_inner | epoch 011:     69 / 1474 loss=2.005, trans_loss=3.351, nll_loss=1.521, w2v_ctc_loss=1.028, task_loss=0.873, contrastive_loss=0.228, total=4171.19, n_correct=2671.93, ppl=2.87, accuracy=64.057, wps=7859.5, ups=0.63, wpb=12452.9, bsz=477.2, num_updates=14800, lr=0.000116248, gnorm=0.402, clip=0, loss_scale=16, train_wall=95, gb_free=17.6, wall=14779
2023-08-15 19:17:35 | INFO | train_inner | epoch 011:    169 / 1474 loss=1.989, trans_loss=3.352, nll_loss=1.524, w2v_ctc_loss=1.03, task_loss=0.959, contrastive_loss=0.149, total=4100.74, n_correct=2625.2, ppl=2.88, accuracy=64.018, wps=12838, ups=1.05, wpb=12251.1, bsz=450.6, num_updates=14900, lr=0.000115857, gnorm=0.408, clip=0, loss_scale=16, train_wall=95, gb_free=14, wall=14874
2023-08-15 19:19:10 | INFO | train_inner | epoch 011:    269 / 1474 loss=1.973, trans_loss=3.348, nll_loss=1.518, w2v_ctc_loss=1.018, task_loss=0.969, contrastive_loss=0.134, total=4115.58, n_correct=2640.26, ppl=2.86, accuracy=64.153, wps=12872, ups=1.05, wpb=12290.6, bsz=444.2, num_updates=15000, lr=0.00011547, gnorm=0.401, clip=0, loss_scale=16, train_wall=95, gb_free=15.7, wall=14970
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:0')
2023-08-15 19:20:23 | INFO | train_inner | epoch 011:    369 / 1474 loss=2.058, trans_loss=4.982, nll_loss=2.263, w2v_ctc_loss=0.765, task_loss=1.443, contrastive_loss=0.11, total=4094.16, n_correct=2619.95, ppl=4.8, accuracy=63.992, wps=11331.2, ups=1.38, wpb=8227.1, bsz=296.5, num_updates=15100, lr=0.000115087, gnorm=0.547, clip=0, loss_scale=16, train_wall=72, gb_free=12.3, wall=15042
2023-08-15 19:21:36 | INFO | train_inner | epoch 011:    469 / 1474 loss=2.081, trans_loss=5.017, nll_loss=2.29, w2v_ctc_loss=0.771, task_loss=1.456, contrastive_loss=0.23, total=4112.8, n_correct=2618.36, ppl=4.89, accuracy=63.664, wps=11218.1, ups=1.36, wpb=8225.6, bsz=302.2, num_updates=15200, lr=0.000114708, gnorm=0.575, clip=0, loss_scale=16, train_wall=73, gb_free=16.7, wall=15115
2023-08-15 19:22:49 | INFO | train_inner | epoch 011:    569 / 1474 loss=2.077, trans_loss=5.011, nll_loss=2.282, w2v_ctc_loss=0.775, task_loss=1.514, contrastive_loss=0.224, total=4071.06, n_correct=2597.25, ppl=4.86, accuracy=63.798, wps=11186, ups=1.37, wpb=8142.1, bsz=292.6, num_updates=15300, lr=0.000114332, gnorm=0.56, clip=0, loss_scale=16, train_wall=72, gb_free=15.9, wall=15188
2023-08-15 19:24:02 | INFO | train_inner | epoch 011:    669 / 1474 loss=2.079, trans_loss=5.004, nll_loss=2.273, w2v_ctc_loss=0.772, task_loss=1.374, contrastive_loss=0.286, total=4156.4, n_correct=2653.54, ppl=4.83, accuracy=63.842, wps=11333.3, ups=1.36, wpb=8312.8, bsz=310.2, num_updates=15400, lr=0.000113961, gnorm=0.536, clip=0, loss_scale=16, train_wall=73, gb_free=15.7, wall=15262
2023-08-15 19:25:16 | INFO | train_inner | epoch 011:    769 / 1474 loss=2.068, trans_loss=5.014, nll_loss=2.286, w2v_ctc_loss=0.78, task_loss=1.423, contrastive_loss=0.108, total=4169.17, n_correct=2664.53, ppl=4.88, accuracy=63.91, wps=11391.6, ups=1.37, wpb=8338.3, bsz=304.8, num_updates=15500, lr=0.000113592, gnorm=0.529, clip=0, loss_scale=16, train_wall=73, gb_free=11.7, wall=15335
2023-08-15 19:26:28 | INFO | train_inner | epoch 011:    869 / 1474 loss=2.064, trans_loss=5.012, nll_loss=2.282, w2v_ctc_loss=0.773, task_loss=1.47, contrastive_loss=0.098, total=4120.01, n_correct=2628.83, ppl=4.87, accuracy=63.806, wps=11351.7, ups=1.38, wpb=8240, bsz=293.5, num_updates=15600, lr=0.000113228, gnorm=0.526, clip=0, loss_scale=16, train_wall=72, gb_free=13, wall=15407
2023-08-15 19:27:40 | INFO | train_inner | epoch 011:    969 / 1474 loss=2.06, trans_loss=5.008, nll_loss=2.279, w2v_ctc_loss=0.768, task_loss=1.422, contrastive_loss=0.11, total=4145.45, n_correct=2649.54, ppl=4.85, accuracy=63.914, wps=11464.6, ups=1.38, wpb=8290.9, bsz=303.7, num_updates=15700, lr=0.000112867, gnorm=0.528, clip=0, loss_scale=16, train_wall=72, gb_free=16.7, wall=15480
2023-08-15 19:28:53 | INFO | train_inner | epoch 011:   1069 / 1474 loss=2.064, trans_loss=5.007, nll_loss=2.278, w2v_ctc_loss=0.774, task_loss=1.379, contrastive_loss=0.127, total=4141.18, n_correct=2646.29, ppl=4.85, accuracy=63.902, wps=11382.3, ups=1.37, wpb=8282.4, bsz=309.4, num_updates=15800, lr=0.000112509, gnorm=0.534, clip=0, loss_scale=16, train_wall=72, gb_free=16.3, wall=15553
2023-08-15 19:30:06 | INFO | train_inner | epoch 011:   1169 / 1474 loss=2.067, trans_loss=5.015, nll_loss=2.288, w2v_ctc_loss=0.779, task_loss=1.408, contrastive_loss=0.113, total=4173.93, n_correct=2663.28, ppl=4.88, accuracy=63.807, wps=11458.5, ups=1.37, wpb=8347.9, bsz=307.2, num_updates=15900, lr=0.000112154, gnorm=0.523, clip=0, loss_scale=16, train_wall=72, gb_free=16.6, wall=15625
2023-08-15 19:31:19 | INFO | train_inner | epoch 011:   1269 / 1474 loss=2.071, trans_loss=5.005, nll_loss=2.276, w2v_ctc_loss=0.777, task_loss=1.349, contrastive_loss=0.183, total=4174.26, n_correct=2669.25, ppl=4.84, accuracy=63.945, wps=11454.8, ups=1.37, wpb=8348.5, bsz=314.4, num_updates=16000, lr=0.000111803, gnorm=0.533, clip=0, loss_scale=16, train_wall=72, gb_free=17.1, wall=15698
2023-08-15 19:31:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:4')
2023-08-15 19:31:44 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.017 | trans_loss 5.212 | nll_loss 2.494 | w2v_ctc_loss 1.387 | task_loss 4.639 | contrastive_loss 0.322 | total 4003.4 | n_correct 2626.6 | ppl 5.63 | accuracy 65.609 | uer 19.04 | wer 20.764 | raw_wer 20.764 | bleu 21.09 | wps 1938.3 | wpb 4003.4 | bsz 141.8 | num_updates 16000 | best_bleu 21.35
2023-08-15 19:31:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16000 updates
2023-08-15 19:31:44 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_11_16000.pt
2023-08-15 19:31:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_11_16000.pt
2023-08-15 19:32:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_11_16000.pt (epoch 11 @ 16000 updates, score 21.09) (writing took 39.59968184493482 seconds)
2023-08-15 19:33:38 | INFO | train_inner | epoch 011:   1369 / 1474 loss=2.077, trans_loss=5.004, nll_loss=2.275, w2v_ctc_loss=0.758, task_loss=1.297, contrastive_loss=0.344, total=4191.56, n_correct=2682.44, ppl=4.84, accuracy=63.996, wps=6026, ups=0.72, wpb=8383.1, bsz=327.7, num_updates=16100, lr=0.000111456, gnorm=0.537, clip=0, loss_scale=16, train_wall=73, gb_free=17.2, wall=15837
2023-08-15 19:34:51 | INFO | train_inner | epoch 011:   1469 / 1474 loss=2.059, trans_loss=5.009, nll_loss=2.28, w2v_ctc_loss=0.769, task_loss=1.35, contrastive_loss=0.116, total=4161.81, n_correct=2662.35, ppl=4.86, accuracy=63.971, wps=11400.3, ups=1.37, wpb=8323.6, bsz=313.2, num_updates=16200, lr=0.000111111, gnorm=0.536, clip=0, loss_scale=16, train_wall=73, gb_free=16.4, wall=15910
2023-08-15 19:34:55 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 19:35:18 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.012 | trans_loss 5.222 | nll_loss 2.515 | w2v_ctc_loss 1.339 | task_loss 4.611 | contrastive_loss 0.327 | total 4003.4 | n_correct 2619.4 | ppl 5.72 | accuracy 65.429 | uer 19.693 | wer 21.74 | raw_wer 21.74 | bleu 20.99 | wps 2173.2 | wpb 4003.4 | bsz 141.8 | num_updates 16205 | best_bleu 21.35
2023-08-15 19:35:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16205 updates
2023-08-15 19:35:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_20.9902.pt
2023-08-15 19:35:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_20.9902.pt
2023-08-15 19:35:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_20.9902.pt (epoch 11 @ 16205 updates, score 20.99) (writing took 39.059400238096714 seconds)
2023-08-15 19:36:00 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-08-15 19:36:00 | INFO | train | epoch 011 | loss 2.047 | trans_loss 4.595 | nll_loss 2.09 | w2v_ctc_loss 0.834 | task_loss 1.288 | contrastive_loss 0.164 | total 4138.7 | n_correct 2645.46 | ppl 4.26 | accuracy 63.92 | wps 10430.5 | ups 1.16 | wpb 9021.6 | bsz 333.4 | num_updates 16205 | lr 0.000111094 | gnorm 0.514 | clip 0 | loss_scale 16 | train_wall 1128 | gb_free 16.8 | wall 15980
2023-08-15 19:36:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 19:36:01 | INFO | fairseq.trainer | begin training epoch 12
2023-08-15 19:36:01 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 19:37:17 | INFO | train_inner | epoch 012:     95 / 1474 loss=2.042, trans_loss=4.968, nll_loss=2.227, w2v_ctc_loss=0.757, task_loss=1.355, contrastive_loss=0.148, total=4139.2, n_correct=2680.04, ppl=4.68, accuracy=64.748, wps=5688, ups=0.69, wpb=8278.4, bsz=312.5, num_updates=16300, lr=0.00011077, gnorm=0.532, clip=0, loss_scale=16, train_wall=72, gb_free=15.6, wall=16056
2023-08-15 19:38:29 | INFO | train_inner | epoch 012:    195 / 1474 loss=2.042, trans_loss=4.973, nll_loss=2.232, w2v_ctc_loss=0.755, task_loss=1.445, contrastive_loss=0.101, total=4126.87, n_correct=2663.89, ppl=4.7, accuracy=64.55, wps=11431.6, ups=1.39, wpb=8253.7, bsz=295.9, num_updates=16400, lr=0.000110432, gnorm=0.527, clip=0, loss_scale=16, train_wall=72, gb_free=16.1, wall=16128
2023-08-15 19:39:42 | INFO | train_inner | epoch 012:    295 / 1474 loss=2.038, trans_loss=4.974, nll_loss=2.235, w2v_ctc_loss=0.742, task_loss=1.314, contrastive_loss=0.13, total=4203.54, n_correct=2717.59, ppl=4.71, accuracy=64.65, wps=11480.3, ups=1.37, wpb=8407.1, bsz=321.1, num_updates=16500, lr=0.000110096, gnorm=0.523, clip=0, loss_scale=16, train_wall=73, gb_free=14.2, wall=16201
2023-08-15 19:40:55 | INFO | train_inner | epoch 012:    395 / 1474 loss=2.041, trans_loss=4.977, nll_loss=2.238, w2v_ctc_loss=0.753, task_loss=1.375, contrastive_loss=0.114, total=4149.28, n_correct=2680.76, ppl=4.72, accuracy=64.608, wps=11358.6, ups=1.37, wpb=8298.6, bsz=307.1, num_updates=16600, lr=0.000109764, gnorm=0.529, clip=0, loss_scale=16, train_wall=73, gb_free=14.8, wall=16274
2023-08-15 19:42:08 | INFO | train_inner | epoch 012:    495 / 1474 loss=2.055, trans_loss=4.993, nll_loss=2.26, w2v_ctc_loss=0.768, task_loss=1.42, contrastive_loss=0.124, total=4106.46, n_correct=2641.53, ppl=4.79, accuracy=64.326, wps=11329.5, ups=1.38, wpb=8212.9, bsz=301.2, num_updates=16700, lr=0.000109435, gnorm=0.529, clip=0, loss_scale=16, train_wall=72, gb_free=17.5, wall=16347
2023-08-15 19:43:21 | INFO | train_inner | epoch 012:    595 / 1474 loss=2.054, trans_loss=4.981, nll_loss=2.244, w2v_ctc_loss=0.76, task_loss=1.342, contrastive_loss=0.188, total=4190.91, n_correct=2701, ppl=4.74, accuracy=64.449, wps=11416.9, ups=1.36, wpb=8381.8, bsz=316.5, num_updates=16800, lr=0.000109109, gnorm=0.542, clip=0, loss_scale=16, train_wall=73, gb_free=15.5, wall=16420
2023-08-15 19:44:34 | INFO | train_inner | epoch 012:    695 / 1474 loss=2.051, trans_loss=4.978, nll_loss=2.241, w2v_ctc_loss=0.746, task_loss=1.294, contrastive_loss=0.271, total=4203.66, n_correct=2717.56, ppl=4.73, accuracy=64.647, wps=11479.9, ups=1.37, wpb=8407.3, bsz=324.4, num_updates=16900, lr=0.000108786, gnorm=0.534, clip=0, loss_scale=32, train_wall=73, gb_free=17, wall=16494
2023-08-15 19:45:47 | INFO | train_inner | epoch 012:    795 / 1474 loss=2.043, trans_loss=4.974, nll_loss=2.234, w2v_ctc_loss=0.76, task_loss=1.425, contrastive_loss=0.11, total=4095.72, n_correct=2646.27, ppl=4.71, accuracy=64.611, wps=11242.9, ups=1.37, wpb=8191.4, bsz=298.9, num_updates=17000, lr=0.000108465, gnorm=0.537, clip=0, loss_scale=32, train_wall=72, gb_free=16.9, wall=16566
2023-08-15 19:47:00 | INFO | train_inner | epoch 012:    895 / 1474 loss=2.055, trans_loss=4.982, nll_loss=2.246, w2v_ctc_loss=0.764, task_loss=1.443, contrastive_loss=0.162, total=4162.82, n_correct=2681.49, ppl=4.74, accuracy=64.415, wps=11436.5, ups=1.37, wpb=8325.6, bsz=305.4, num_updates=17100, lr=0.000108148, gnorm=0.534, clip=0, loss_scale=32, train_wall=72, gb_free=15.7, wall=16639
2023-08-15 19:48:13 | INFO | train_inner | epoch 012:    995 / 1474 loss=2.052, trans_loss=4.983, nll_loss=2.246, w2v_ctc_loss=0.762, task_loss=1.43, contrastive_loss=0.171, total=4117.63, n_correct=2650.54, ppl=4.74, accuracy=64.371, wps=11325.8, ups=1.38, wpb=8235.3, bsz=301.6, num_updates=17200, lr=0.000107833, gnorm=0.529, clip=0, loss_scale=32, train_wall=72, gb_free=16.5, wall=16712
2023-08-15 19:49:26 | INFO | train_inner | epoch 012:   1095 / 1474 loss=2.065, trans_loss=4.99, nll_loss=2.256, w2v_ctc_loss=0.769, task_loss=1.483, contrastive_loss=0.214, total=4046.48, n_correct=2604.89, ppl=4.78, accuracy=64.374, wps=11032, ups=1.36, wpb=8093, bsz=289.6, num_updates=17300, lr=0.000107521, gnorm=0.539, clip=0, loss_scale=32, train_wall=73, gb_free=15.6, wall=16785
2023-08-15 19:50:39 | INFO | train_inner | epoch 012:   1195 / 1474 loss=2.067, trans_loss=5.005, nll_loss=2.275, w2v_ctc_loss=0.777, task_loss=1.373, contrastive_loss=0.184, total=4201.13, n_correct=2690.14, ppl=4.84, accuracy=64.034, wps=11449.1, ups=1.36, wpb=8402.3, bsz=319.2, num_updates=17400, lr=0.000107211, gnorm=0.524, clip=0, loss_scale=32, train_wall=73, gb_free=16.9, wall=16859
2023-08-15 19:51:53 | INFO | train_inner | epoch 012:   1295 / 1474 loss=2.052, trans_loss=4.985, nll_loss=2.249, w2v_ctc_loss=0.772, task_loss=1.57, contrastive_loss=0.098, total=4070.27, n_correct=2620.36, ppl=4.75, accuracy=64.378, wps=11133.4, ups=1.37, wpb=8140.5, bsz=286.1, num_updates=17500, lr=0.000106904, gnorm=0.533, clip=0, loss_scale=32, train_wall=73, gb_free=15.4, wall=16932
2023-08-15 19:53:05 | INFO | train_inner | epoch 012:   1395 / 1474 loss=2.054, trans_loss=4.99, nll_loss=2.256, w2v_ctc_loss=0.752, task_loss=1.417, contrastive_loss=0.202, total=4139.63, n_correct=2662.32, ppl=4.78, accuracy=64.313, wps=11473.4, ups=1.39, wpb=8279.3, bsz=305.8, num_updates=17600, lr=0.0001066, gnorm=0.537, clip=0, loss_scale=32, train_wall=72, gb_free=16.7, wall=17004
2023-08-15 19:54:03 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 19:54:26 | INFO | dev_st | epoch 012 | valid on 'dev_st' subset | loss 4.013 | trans_loss 5.207 | nll_loss 2.493 | w2v_ctc_loss 1.382 | task_loss 4.626 | contrastive_loss 0.322 | total 4003.4 | n_correct 2629.8 | ppl 5.63 | accuracy 65.689 | uer 19.218 | wer 20.998 | raw_wer 20.998 | bleu 21.58 | wps 2160.9 | wpb 4003.4 | bsz 141.8 | num_updates 17679 | best_bleu 21.58
2023-08-15 19:54:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 17679 updates
2023-08-15 19:54:26 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 19:54:42 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 19:54:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 12 @ 17679 updates, score 21.58) (writing took 32.36259919218719 seconds)
2023-08-15 19:54:59 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-08-15 19:54:59 | INFO | train | epoch 012 | loss 2.051 | trans_loss 4.983 | nll_loss 2.246 | w2v_ctc_loss 0.76 | task_loss 1.406 | contrastive_loss 0.156 | total 4138.65 | n_correct 2667.53 | ppl 4.74 | accuracy 64.454 | wps 10716.8 | ups 1.29 | wpb 8277.3 | bsz 305.7 | num_updates 17679 | lr 0.000106362 | gnorm 0.532 | clip 0 | loss_scale 32 | train_wall 1067 | gb_free 12.4 | wall 17118
2023-08-15 19:54:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 19:54:59 | INFO | fairseq.trainer | begin training epoch 13
2023-08-15 19:54:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 19:55:22 | INFO | train_inner | epoch 013:     21 / 1474 loss=2.054, trans_loss=4.993, nll_loss=2.259, w2v_ctc_loss=0.774, task_loss=1.457, contrastive_loss=0.106, total=4096.49, n_correct=2632.4, ppl=4.79, accuracy=64.26, wps=5972.4, ups=0.73, wpb=8193, bsz=295.4, num_updates=17700, lr=0.000106299, gnorm=0.53, clip=0, loss_scale=32, train_wall=73, gb_free=14.1, wall=17141
2023-08-15 19:56:35 | INFO | train_inner | epoch 013:    121 / 1474 loss=2.032, trans_loss=4.956, nll_loss=2.211, w2v_ctc_loss=0.747, task_loss=1.408, contrastive_loss=0.118, total=4160.97, n_correct=2705.51, ppl=4.63, accuracy=65.021, wps=11443.7, ups=1.38, wpb=8321.9, bsz=302.9, num_updates=17800, lr=0.000106, gnorm=0.526, clip=0, loss_scale=32, train_wall=72, gb_free=15.9, wall=17214
2023-08-15 19:57:48 | INFO | train_inner | epoch 013:    221 / 1474 loss=2.05, trans_loss=4.967, nll_loss=2.226, w2v_ctc_loss=0.741, task_loss=1.295, contrastive_loss=0.33, total=4212.08, n_correct=2733.31, ppl=4.68, accuracy=64.892, wps=11474.5, ups=1.36, wpb=8424.2, bsz=329.7, num_updates=17900, lr=0.000105703, gnorm=0.522, clip=0, loss_scale=32, train_wall=73, gb_free=14.2, wall=17287
2023-08-15 19:59:01 | INFO | train_inner | epoch 013:    321 / 1474 loss=2.027, trans_loss=4.951, nll_loss=2.205, w2v_ctc_loss=0.745, task_loss=1.464, contrastive_loss=0.101, total=4102.3, n_correct=2672.64, ppl=4.61, accuracy=65.15, wps=11186.8, ups=1.36, wpb=8204.6, bsz=294.1, num_updates=18000, lr=0.000105409, gnorm=0.527, clip=0, loss_scale=32, train_wall=73, gb_free=16.9, wall=17361
2023-08-15 19:59:01 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 19:59:27 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 3.987 | trans_loss 5.209 | nll_loss 2.491 | w2v_ctc_loss 1.292 | task_loss 4.61 | contrastive_loss 0.323 | total 4003.4 | n_correct 2631 | ppl 5.62 | accuracy 65.719 | uer 18.838 | wer 20.603 | raw_wer 20.603 | bleu 21.84 | wps 1904.1 | wpb 4003.4 | bsz 141.8 | num_updates 18000 | best_bleu 21.84
2023-08-15 19:59:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 18000 updates
2023-08-15 19:59:27 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_13_18000.pt
2023-08-15 19:59:31 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_13_18000.pt
2023-08-15 20:00:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_13_18000.pt (epoch 13 @ 18000 updates, score 21.84) (writing took 33.24521427601576 seconds)
2023-08-15 20:01:15 | INFO | train_inner | epoch 013:    421 / 1474 loss=2.033, trans_loss=4.958, nll_loss=2.214, w2v_ctc_loss=0.747, task_loss=1.32, contrastive_loss=0.15, total=4177.29, n_correct=2716.13, ppl=4.64, accuracy=65.021, wps=6271.5, ups=0.75, wpb=8354.6, bsz=318.4, num_updates=18100, lr=0.000105118, gnorm=0.552, clip=0, loss_scale=32, train_wall=72, gb_free=17.1, wall=17494
2023-08-15 20:02:28 | INFO | train_inner | epoch 013:    521 / 1474 loss=2.041, trans_loss=4.964, nll_loss=2.222, w2v_ctc_loss=0.755, task_loss=1.366, contrastive_loss=0.186, total=4201.22, n_correct=2721.23, ppl=4.66, accuracy=64.772, wps=11465.6, ups=1.36, wpb=8402.4, bsz=319, num_updates=18200, lr=0.000104828, gnorm=0.526, clip=0, loss_scale=32, train_wall=73, gb_free=12.5, wall=17567
2023-08-15 20:03:40 | INFO | train_inner | epoch 013:    621 / 1474 loss=2.027, trans_loss=4.959, nll_loss=2.216, w2v_ctc_loss=0.745, task_loss=1.369, contrastive_loss=0.098, total=4161.98, n_correct=2706.84, ppl=4.65, accuracy=65.037, wps=11481, ups=1.38, wpb=8324, bsz=308.3, num_updates=18300, lr=0.000104542, gnorm=0.526, clip=0, loss_scale=32, train_wall=72, gb_free=15.4, wall=17640
2023-08-15 20:04:54 | INFO | train_inner | epoch 013:    721 / 1474 loss=2.042, trans_loss=4.965, nll_loss=2.223, w2v_ctc_loss=0.766, task_loss=1.565, contrastive_loss=0.098, total=4096.76, n_correct=2651.83, ppl=4.67, accuracy=64.73, wps=11159.3, ups=1.36, wpb=8193.5, bsz=284.6, num_updates=18400, lr=0.000104257, gnorm=0.539, clip=0, loss_scale=32, train_wall=73, gb_free=16.3, wall=17713
2023-08-15 20:06:08 | INFO | train_inner | epoch 013:    821 / 1474 loss=2.04, trans_loss=4.966, nll_loss=2.225, w2v_ctc_loss=0.753, task_loss=1.423, contrastive_loss=0.145, total=4121.73, n_correct=2667.3, ppl=4.67, accuracy=64.713, wps=11160, ups=1.35, wpb=8243.5, bsz=306.7, num_updates=18500, lr=0.000103975, gnorm=0.54, clip=0, loss_scale=32, train_wall=73, gb_free=14.5, wall=17787
2023-08-15 20:07:20 | INFO | train_inner | epoch 013:    921 / 1474 loss=2.032, trans_loss=4.963, nll_loss=2.222, w2v_ctc_loss=0.747, task_loss=1.435, contrastive_loss=0.108, total=4107.01, n_correct=2666.1, ppl=4.66, accuracy=64.916, wps=11299.7, ups=1.38, wpb=8214, bsz=296.5, num_updates=18600, lr=0.000103695, gnorm=0.527, clip=0, loss_scale=32, train_wall=72, gb_free=15.6, wall=17860
2023-08-15 20:08:33 | INFO | train_inner | epoch 013:   1021 / 1474 loss=2.05, trans_loss=4.971, nll_loss=2.232, w2v_ctc_loss=0.765, task_loss=1.491, contrastive_loss=0.161, total=4081.02, n_correct=2637.13, ppl=4.7, accuracy=64.619, wps=11250.1, ups=1.38, wpb=8162, bsz=293.4, num_updates=18700, lr=0.000103418, gnorm=0.533, clip=0, loss_scale=32, train_wall=72, gb_free=15.9, wall=17932
2023-08-15 20:09:46 | INFO | train_inner | epoch 013:   1121 / 1474 loss=2.028, trans_loss=4.954, nll_loss=2.21, w2v_ctc_loss=0.743, task_loss=1.387, contrastive_loss=0.138, total=4105.62, n_correct=2676.45, ppl=4.63, accuracy=65.19, wps=11205, ups=1.36, wpb=8211.2, bsz=305.9, num_updates=18800, lr=0.000103142, gnorm=0.528, clip=0, loss_scale=32, train_wall=73, gb_free=16.3, wall=18006
2023-08-15 20:10:59 | INFO | train_inner | epoch 013:   1221 / 1474 loss=2.035, trans_loss=4.965, nll_loss=2.224, w2v_ctc_loss=0.755, task_loss=1.496, contrastive_loss=0.1, total=4110.35, n_correct=2667.47, ppl=4.67, accuracy=64.896, wps=11227.5, ups=1.37, wpb=8220.7, bsz=295.1, num_updates=18900, lr=0.000102869, gnorm=0.526, clip=0, loss_scale=64, train_wall=73, gb_free=14.5, wall=18079
2023-08-15 20:11:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-15 20:12:13 | INFO | train_inner | epoch 013:   1322 / 1474 loss=2.03, trans_loss=4.957, nll_loss=2.213, w2v_ctc_loss=0.752, task_loss=1.428, contrastive_loss=0.103, total=4094.47, n_correct=2665.38, ppl=4.64, accuracy=65.097, wps=11158.3, ups=1.36, wpb=8188.9, bsz=300.2, num_updates=19000, lr=0.000102598, gnorm=0.531, clip=0, loss_scale=32, train_wall=73, gb_free=17.6, wall=18152
2023-08-15 20:13:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-15 20:13:27 | INFO | train_inner | epoch 013:   1423 / 1474 loss=2.029, trans_loss=4.961, nll_loss=2.219, w2v_ctc_loss=0.749, task_loss=1.417, contrastive_loss=0.093, total=4152, n_correct=2697.4, ppl=4.66, accuracy=64.966, wps=11247.7, ups=1.35, wpb=8304, bsz=303.4, num_updates=19100, lr=0.000102329, gnorm=0.524, clip=0, loss_scale=16, train_wall=73, gb_free=14.8, wall=18226
2023-08-15 20:14:04 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 20:14:27 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 3.992 | trans_loss 5.196 | nll_loss 2.472 | w2v_ctc_loss 1.338 | task_loss 4.611 | contrastive_loss 0.317 | total 4003.4 | n_correct 2636.5 | ppl 5.55 | accuracy 65.857 | uer 18.953 | wer 20.667 | raw_wer 20.667 | bleu 21.71 | wps 2171.4 | wpb 4003.4 | bsz 141.8 | num_updates 19151 | best_bleu 21.84
2023-08-15 20:14:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 19151 updates
2023-08-15 20:14:27 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_21.7106.pt
2023-08-15 20:14:31 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_21.7106.pt
2023-08-15 20:15:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_21.7106.pt (epoch 13 @ 19151 updates, score 21.71) (writing took 36.452883411198854 seconds)
2023-08-15 20:15:07 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-08-15 20:15:07 | INFO | train | epoch 013 | loss 2.035 | trans_loss 4.961 | nll_loss 2.218 | w2v_ctc_loss 0.751 | task_loss 1.412 | contrastive_loss 0.139 | total 4136.12 | n_correct 2685.98 | ppl 4.65 | accuracy 64.94 | wps 10075.6 | ups 1.22 | wpb 8272.2 | bsz 304.7 | num_updates 19151 | lr 0.000102193 | gnorm 0.53 | clip 0 | loss_scale 16 | train_wall 1070 | gb_free 17.3 | wall 18327
2023-08-15 20:15:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 20:15:08 | INFO | fairseq.trainer | begin training epoch 14
2023-08-15 20:15:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 20:15:51 | INFO | train_inner | epoch 014:     49 / 1474 loss=2.013, trans_loss=4.934, nll_loss=2.184, w2v_ctc_loss=0.734, task_loss=1.293, contrastive_loss=0.112, total=4182.69, n_correct=2742.62, ppl=4.55, accuracy=65.571, wps=5783.6, ups=0.69, wpb=8365.4, bsz=322.3, num_updates=19200, lr=0.000102062, gnorm=0.552, clip=0, loss_scale=16, train_wall=72, gb_free=15.6, wall=18371
2023-08-15 20:17:04 | INFO | train_inner | epoch 014:    149 / 1474 loss=2.011, trans_loss=4.926, nll_loss=2.172, w2v_ctc_loss=0.74, task_loss=1.399, contrastive_loss=0.093, total=4086.4, n_correct=2682.72, ppl=4.51, accuracy=65.65, wps=11279.4, ups=1.38, wpb=8172.8, bsz=301.5, num_updates=19300, lr=0.000101797, gnorm=0.531, clip=0, loss_scale=16, train_wall=72, gb_free=16.6, wall=18443
2023-08-15 20:18:17 | INFO | train_inner | epoch 014:    249 / 1474 loss=2.027, trans_loss=4.941, nll_loss=2.192, w2v_ctc_loss=0.736, task_loss=1.472, contrastive_loss=0.195, total=4103.37, n_correct=2684.13, ppl=4.57, accuracy=65.413, wps=11262.1, ups=1.37, wpb=8206.7, bsz=294, num_updates=19400, lr=0.000101535, gnorm=0.539, clip=0, loss_scale=16, train_wall=72, gb_free=16.9, wall=18516
2023-08-15 20:19:30 | INFO | train_inner | epoch 014:    349 / 1474 loss=2.015, trans_loss=4.94, nll_loss=2.192, w2v_ctc_loss=0.729, task_loss=1.321, contrastive_loss=0.13, total=4168.35, n_correct=2729.23, ppl=4.57, accuracy=65.475, wps=11357, ups=1.36, wpb=8336.7, bsz=318.7, num_updates=19500, lr=0.000101274, gnorm=0.528, clip=0, loss_scale=16, train_wall=73, gb_free=15.9, wall=18589
2023-08-15 20:20:43 | INFO | train_inner | epoch 014:    449 / 1474 loss=2.018, trans_loss=4.944, nll_loss=2.197, w2v_ctc_loss=0.735, task_loss=1.384, contrastive_loss=0.11, total=4155.83, n_correct=2717.77, ppl=4.58, accuracy=65.397, wps=11371.9, ups=1.37, wpb=8311.7, bsz=306.7, num_updates=19600, lr=0.000101015, gnorm=0.531, clip=0, loss_scale=16, train_wall=73, gb_free=15.5, wall=18662
2023-08-15 20:21:56 | INFO | train_inner | epoch 014:    549 / 1474 loss=2.028, trans_loss=4.944, nll_loss=2.196, w2v_ctc_loss=0.752, task_loss=1.527, contrastive_loss=0.107, total=4064.87, n_correct=2647.21, ppl=4.58, accuracy=65.124, wps=11175.7, ups=1.37, wpb=8129.7, bsz=288.5, num_updates=19700, lr=0.000100759, gnorm=0.538, clip=0, loss_scale=16, train_wall=72, gb_free=17.5, wall=18735
2023-08-15 20:23:09 | INFO | train_inner | epoch 014:    649 / 1474 loss=2.028, trans_loss=4.945, nll_loss=2.198, w2v_ctc_loss=0.739, task_loss=1.397, contrastive_loss=0.171, total=4167.34, n_correct=2721.72, ppl=4.59, accuracy=65.311, wps=11471.4, ups=1.38, wpb=8334.7, bsz=307.8, num_updates=19800, lr=0.000100504, gnorm=0.525, clip=0, loss_scale=16, train_wall=72, gb_free=16.8, wall=18808
2023-08-15 20:24:22 | INFO | train_inner | epoch 014:    749 / 1474 loss=2.015, trans_loss=4.932, nll_loss=2.181, w2v_ctc_loss=0.739, task_loss=1.376, contrastive_loss=0.103, total=4142.94, n_correct=2715.45, ppl=4.53, accuracy=65.544, wps=11313.2, ups=1.37, wpb=8285.9, bsz=308.6, num_updates=19900, lr=0.000100251, gnorm=0.523, clip=0, loss_scale=16, train_wall=73, gb_free=16.1, wall=18881
2023-08-15 20:25:35 | INFO | train_inner | epoch 014:    849 / 1474 loss=2.026, trans_loss=4.935, nll_loss=2.185, w2v_ctc_loss=0.733, task_loss=1.339, contrastive_loss=0.215, total=4173.06, n_correct=2728.95, ppl=4.55, accuracy=65.394, wps=11401.3, ups=1.37, wpb=8346.1, bsz=319.1, num_updates=20000, lr=0.0001, gnorm=0.525, clip=0, loss_scale=16, train_wall=73, gb_free=12.3, wall=18954
2023-08-15 20:25:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 20:25:59 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 3.992 | trans_loss 5.185 | nll_loss 2.458 | w2v_ctc_loss 1.374 | task_loss 4.617 | contrastive_loss 0.311 | total 4003.4 | n_correct 2640.2 | ppl 5.5 | accuracy 65.949 | uer 18.313 | wer 19.996 | raw_wer 19.996 | bleu 21.92 | wps 2187.2 | wpb 4003.4 | bsz 141.8 | num_updates 20000 | best_bleu 21.92
2023-08-15 20:25:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20000 updates
2023-08-15 20:25:59 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_14_20000.pt
2023-08-15 20:26:02 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_14_20000.pt
2023-08-15 20:26:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_14_20000.pt (epoch 14 @ 20000 updates, score 21.92) (writing took 47.44644782692194 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:0')
2023-08-15 20:28:01 | INFO | train_inner | epoch 014:    949 / 1474 loss=2.026, trans_loss=4.944, nll_loss=2.197, w2v_ctc_loss=0.74, task_loss=1.4, contrastive_loss=0.15, total=4166.71, n_correct=2717.03, ppl=4.58, accuracy=65.208, wps=5716.8, ups=0.69, wpb=8333.4, bsz=310.6, num_updates=20100, lr=9.97509e-05, gnorm=0.537, clip=0, loss_scale=16, train_wall=73, gb_free=16.1, wall=19100
2023-08-15 20:29:14 | INFO | train_inner | epoch 014:   1049 / 1474 loss=2.025, trans_loss=4.946, nll_loss=2.199, w2v_ctc_loss=0.74, task_loss=1.426, contrastive_loss=0.126, total=4145.57, n_correct=2703.04, ppl=4.59, accuracy=65.203, wps=11294.2, ups=1.36, wpb=8291.1, bsz=301.1, num_updates=20200, lr=9.95037e-05, gnorm=0.531, clip=0, loss_scale=16, train_wall=73, gb_free=17, wall=19174
2023-08-15 20:30:28 | INFO | train_inner | epoch 014:   1149 / 1474 loss=2.051, trans_loss=4.946, nll_loss=2.2, w2v_ctc_loss=0.745, task_loss=1.328, contrastive_loss=0.398, total=4219.9, n_correct=2749.2, ppl=4.59, accuracy=65.148, wps=11410, ups=1.35, wpb=8439.8, bsz=325.2, num_updates=20300, lr=9.92583e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=73, gb_free=16, wall=19248
2023-08-15 20:31:41 | INFO | train_inner | epoch 014:   1249 / 1474 loss=2.028, trans_loss=4.95, nll_loss=2.204, w2v_ctc_loss=0.753, task_loss=1.631, contrastive_loss=0.086, total=4032.06, n_correct=2627.44, ppl=4.61, accuracy=65.164, wps=11114.5, ups=1.38, wpb=8064.1, bsz=274.4, num_updates=20400, lr=9.90148e-05, gnorm=0.546, clip=0, loss_scale=16, train_wall=72, gb_free=17.3, wall=19320
2023-08-15 20:32:53 | INFO | train_inner | epoch 014:   1349 / 1474 loss=2.014, trans_loss=4.942, nll_loss=2.194, w2v_ctc_loss=0.732, task_loss=1.332, contrastive_loss=0.101, total=4205.07, n_correct=2749.08, ppl=4.58, accuracy=65.375, wps=11608.6, ups=1.38, wpb=8410.1, bsz=317.3, num_updates=20500, lr=9.8773e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=72, gb_free=16.2, wall=19393
2023-08-15 20:34:06 | INFO | train_inner | epoch 014:   1449 / 1474 loss=2.024, trans_loss=4.948, nll_loss=2.202, w2v_ctc_loss=0.737, task_loss=1.402, contrastive_loss=0.14, total=4126.44, n_correct=2692.95, ppl=4.6, accuracy=65.261, wps=11387.6, ups=1.38, wpb=8252.9, bsz=303.9, num_updates=20600, lr=9.85329e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=72, gb_free=15.8, wall=19465
2023-08-15 20:34:24 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:2')
2023-08-15 20:34:49 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 3.981 | trans_loss 5.185 | nll_loss 2.46 | w2v_ctc_loss 1.335 | task_loss 4.604 | contrastive_loss 0.309 | total 4003.4 | n_correct 2649.3 | ppl 5.5 | accuracy 66.176 | uer 18.416 | wer 20.234 | raw_wer 20.234 | bleu 22 | wps 2014.5 | wpb 4003.4 | bsz 141.8 | num_updates 20625 | best_bleu 22
2023-08-15 20:34:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20625 updates
2023-08-15 20:34:49 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 20:35:06 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 20:35:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 14 @ 20625 updates, score 22.0) (writing took 32.70984374731779 seconds)
2023-08-15 20:35:22 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-08-15 20:35:22 | INFO | train | epoch 014 | loss 2.024 | trans_loss 4.942 | nll_loss 2.193 | w2v_ctc_loss 0.739 | task_loss 1.405 | contrastive_loss 0.151 | total 4138.65 | n_correct 2704.04 | ppl 4.57 | accuracy 65.336 | wps 10042.5 | ups 1.21 | wpb 8277.3 | bsz 305.7 | num_updates 20625 | lr 9.84732e-05 | gnorm 0.533 | clip 0 | loss_scale 16 | train_wall 1068 | gb_free 15.9 | wall 19542
2023-08-15 20:35:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 20:35:23 | INFO | fairseq.trainer | begin training epoch 15
2023-08-15 20:35:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 20:36:25 | INFO | train_inner | epoch 015:     75 / 1474 loss=2.017, trans_loss=4.93, nll_loss=2.179, w2v_ctc_loss=0.726, task_loss=1.409, contrastive_loss=0.189, total=4090.99, n_correct=2684.09, ppl=4.53, accuracy=65.61, wps=5868.4, ups=0.72, wpb=8182, bsz=300.8, num_updates=20700, lr=9.82946e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=72, gb_free=16.6, wall=19604
2023-08-15 20:37:38 | INFO | train_inner | epoch 015:    175 / 1474 loss=2.005, trans_loss=4.919, nll_loss=2.163, w2v_ctc_loss=0.73, task_loss=1.455, contrastive_loss=0.099, total=4115.56, n_correct=2710.05, ppl=4.48, accuracy=65.849, wps=11329.4, ups=1.38, wpb=8231.1, bsz=298.5, num_updates=20800, lr=9.80581e-05, gnorm=0.523, clip=0, loss_scale=16, train_wall=72, gb_free=16.4, wall=19677
2023-08-15 20:38:51 | INFO | train_inner | epoch 015:    275 / 1474 loss=2.003, trans_loss=4.923, nll_loss=2.169, w2v_ctc_loss=0.725, task_loss=1.369, contrastive_loss=0.091, total=4182.19, n_correct=2752.88, ppl=4.5, accuracy=65.824, wps=11486.9, ups=1.37, wpb=8364.4, bsz=310.5, num_updates=20900, lr=9.78232e-05, gnorm=0.522, clip=0, loss_scale=16, train_wall=72, gb_free=16, wall=19750
2023-08-15 20:40:04 | INFO | train_inner | epoch 015:    375 / 1474 loss=2.004, trans_loss=4.915, nll_loss=2.158, w2v_ctc_loss=0.724, task_loss=1.403, contrastive_loss=0.114, total=4172.52, n_correct=2746.19, ppl=4.46, accuracy=65.816, wps=11390.8, ups=1.36, wpb=8345, bsz=307.7, num_updates=21000, lr=9.759e-05, gnorm=0.543, clip=0, loss_scale=16, train_wall=73, gb_free=16.3, wall=19823
2023-08-15 20:41:17 | INFO | train_inner | epoch 015:    475 / 1474 loss=2.018, trans_loss=4.924, nll_loss=2.169, w2v_ctc_loss=0.725, task_loss=1.472, contrastive_loss=0.205, total=4076.84, n_correct=2674.12, ppl=4.5, accuracy=65.593, wps=11134.6, ups=1.37, wpb=8153.7, bsz=293.4, num_updates=21100, lr=9.73585e-05, gnorm=0.533, clip=0, loss_scale=16, train_wall=73, gb_free=16.4, wall=19896
2023-08-15 20:42:30 | INFO | train_inner | epoch 015:    575 / 1474 loss=2.01, trans_loss=4.922, nll_loss=2.168, w2v_ctc_loss=0.734, task_loss=1.435, contrastive_loss=0.117, total=4156.05, n_correct=2728.85, ppl=4.49, accuracy=65.66, wps=11394.1, ups=1.37, wpb=8312.1, bsz=302.8, num_updates=21200, lr=9.71286e-05, gnorm=0.526, clip=0, loss_scale=32, train_wall=72, gb_free=11.2, wall=19969
2023-08-15 20:43:43 | INFO | train_inner | epoch 015:    675 / 1474 loss=2.016, trans_loss=4.919, nll_loss=2.164, w2v_ctc_loss=0.733, task_loss=1.436, contrastive_loss=0.161, total=4118.87, n_correct=2705.42, ppl=4.48, accuracy=65.684, wps=11259.2, ups=1.37, wpb=8237.7, bsz=303, num_updates=21300, lr=9.69003e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=73, gb_free=16.6, wall=20043
2023-08-15 20:44:56 | INFO | train_inner | epoch 015:    775 / 1474 loss=2.007, trans_loss=4.924, nll_loss=2.171, w2v_ctc_loss=0.73, task_loss=1.418, contrastive_loss=0.1, total=4176.64, n_correct=2741.97, ppl=4.5, accuracy=65.65, wps=11405.2, ups=1.37, wpb=8353.3, bsz=305.3, num_updates=21400, lr=9.66736e-05, gnorm=0.517, clip=0, loss_scale=32, train_wall=73, gb_free=13.6, wall=20116
2023-08-15 20:46:09 | INFO | train_inner | epoch 015:    875 / 1474 loss=2.011, trans_loss=4.927, nll_loss=2.176, w2v_ctc_loss=0.735, task_loss=1.518, contrastive_loss=0.092, total=4056.99, n_correct=2663.3, ppl=4.52, accuracy=65.647, wps=11181, ups=1.38, wpb=8114, bsz=288, num_updates=21500, lr=9.64486e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=72, gb_free=17, wall=20188
2023-08-15 20:46:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-15 20:47:22 | INFO | train_inner | epoch 015:    976 / 1474 loss=2.015, trans_loss=4.927, nll_loss=2.174, w2v_ctc_loss=0.728, task_loss=1.412, contrastive_loss=0.181, total=4126.06, n_correct=2709.76, ppl=4.51, accuracy=65.674, wps=11285.8, ups=1.37, wpb=8252.1, bsz=302, num_updates=21600, lr=9.6225e-05, gnorm=0.539, clip=0, loss_scale=16, train_wall=73, gb_free=16.2, wall=20261
2023-08-15 20:48:36 | INFO | train_inner | epoch 015:   1076 / 1474 loss=2.028, trans_loss=4.929, nll_loss=2.177, w2v_ctc_loss=0.723, task_loss=1.323, contrastive_loss=0.339, total=4187.18, n_correct=2746.36, ppl=4.52, accuracy=65.59, wps=11375.7, ups=1.36, wpb=8374.4, bsz=324.7, num_updates=21700, lr=9.60031e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=73, gb_free=13.5, wall=20335
2023-08-15 20:49:49 | INFO | train_inner | epoch 015:   1176 / 1474 loss=1.999, trans_loss=4.922, nll_loss=2.17, w2v_ctc_loss=0.71, task_loss=1.265, contrastive_loss=0.142, total=4184.18, n_correct=2761.03, ppl=4.5, accuracy=65.987, wps=11438.2, ups=1.37, wpb=8368.4, bsz=328.4, num_updates=21800, lr=9.57826e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=73, gb_free=15.4, wall=20408
2023-08-15 20:51:02 | INFO | train_inner | epoch 015:   1276 / 1474 loss=2.012, trans_loss=4.921, nll_loss=2.166, w2v_ctc_loss=0.742, task_loss=1.439, contrastive_loss=0.099, total=4141.39, n_correct=2720.28, ppl=4.49, accuracy=65.685, wps=11371.7, ups=1.37, wpb=8282.8, bsz=302.1, num_updates=21900, lr=9.55637e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=72, gb_free=16, wall=20481
2023-08-15 20:52:15 | INFO | train_inner | epoch 015:   1376 / 1474 loss=2.001, trans_loss=4.918, nll_loss=2.163, w2v_ctc_loss=0.726, task_loss=1.452, contrastive_loss=0.084, total=4106.11, n_correct=2702.29, ppl=4.48, accuracy=65.811, wps=11257, ups=1.37, wpb=8212.2, bsz=294.2, num_updates=22000, lr=9.53463e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=72, gb_free=16.4, wall=20554
2023-08-15 20:52:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 20:52:40 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 3.957 | trans_loss 5.178 | nll_loss 2.449 | w2v_ctc_loss 1.279 | task_loss 4.627 | contrastive_loss 0.304 | total 4003.4 | n_correct 2655.2 | ppl 5.46 | accuracy 66.324 | uer 18.077 | wer 19.72 | raw_wer 19.72 | bleu 21.95 | wps 2003.4 | wpb 4003.4 | bsz 141.8 | num_updates 22000 | best_bleu 22
2023-08-15 20:52:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22000 updates
2023-08-15 20:52:40 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_15_22000.pt
2023-08-15 20:52:43 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_15_22000.pt
2023-08-15 20:53:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_15_22000.pt (epoch 15 @ 22000 updates, score 21.95) (writing took 33.15908640809357 seconds)
2023-08-15 20:54:26 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 20:54:52 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 3.967 | trans_loss 5.178 | nll_loss 2.449 | w2v_ctc_loss 1.311 | task_loss 4.635 | contrastive_loss 0.3 | total 4003.4 | n_correct 2650 | ppl 5.46 | accuracy 66.194 | uer 17.931 | wer 19.72 | raw_wer 19.72 | bleu 21.82 | wps 1816.9 | wpb 4003.4 | bsz 141.8 | num_updates 22098 | best_bleu 22
2023-08-15 20:54:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22098 updates
2023-08-15 20:54:52 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_21.8206.pt
2023-08-15 20:54:56 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_21.8206.pt
2023-08-15 20:55:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_21.8206.pt (epoch 15 @ 22098 updates, score 21.82) (writing took 23.952523151412606 seconds)
2023-08-15 20:55:17 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-08-15 20:55:17 | INFO | train | epoch 015 | loss 2.01 | trans_loss 4.923 | nll_loss 2.169 | w2v_ctc_loss 0.728 | task_loss 1.406 | contrastive_loss 0.147 | total 4138.26 | n_correct 2719.91 | ppl 4.5 | accuracy 65.726 | wps 10208 | ups 1.23 | wpb 8276.5 | bsz 305.5 | num_updates 22098 | lr 9.51346e-05 | gnorm 0.528 | clip 0 | loss_scale 16 | train_wall 1069 | gb_free 16.6 | wall 20736
2023-08-15 20:55:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 20:55:17 | INFO | fairseq.trainer | begin training epoch 16
2023-08-15 20:55:17 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 20:55:26 | INFO | train_inner | epoch 016:      2 / 1474 loss=2.017, trans_loss=4.931, nll_loss=2.182, w2v_ctc_loss=0.729, task_loss=1.341, contrastive_loss=0.174, total=4152.6, n_correct=2723.03, ppl=4.54, accuracy=65.574, wps=4345.1, ups=0.52, wpb=8305.2, bsz=316.4, num_updates=22100, lr=9.51303e-05, gnorm=0.531, clip=0, loss_scale=16, train_wall=74, gb_free=16.2, wall=20745
2023-08-15 20:56:38 | INFO | train_inner | epoch 016:    102 / 1474 loss=1.99, trans_loss=4.902, nll_loss=2.142, w2v_ctc_loss=0.712, task_loss=1.351, contrastive_loss=0.112, total=4115.14, n_correct=2726.9, ppl=4.41, accuracy=66.265, wps=11375.7, ups=1.38, wpb=8230.3, bsz=313.9, num_updates=22200, lr=9.49158e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=72, gb_free=12.1, wall=20818
2023-08-15 20:57:51 | INFO | train_inner | epoch 016:    202 / 1474 loss=1.983, trans_loss=4.892, nll_loss=2.129, w2v_ctc_loss=0.704, task_loss=1.442, contrastive_loss=0.089, total=4109.58, n_correct=2729.52, ppl=4.37, accuracy=66.418, wps=11297.5, ups=1.37, wpb=8219.2, bsz=297.3, num_updates=22300, lr=9.47027e-05, gnorm=0.523, clip=0, loss_scale=16, train_wall=72, gb_free=14.6, wall=20890
2023-08-15 20:59:04 | INFO | train_inner | epoch 016:    302 / 1474 loss=2, trans_loss=4.903, nll_loss=2.144, w2v_ctc_loss=0.716, task_loss=1.395, contrastive_loss=0.163, total=4164.1, n_correct=2754.92, ppl=4.42, accuracy=66.159, wps=11390.7, ups=1.37, wpb=8328.2, bsz=308.6, num_updates=22400, lr=9.44911e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=73, gb_free=16.9, wall=20963
2023-08-15 21:00:17 | INFO | train_inner | epoch 016:    402 / 1474 loss=2.002, trans_loss=4.901, nll_loss=2.14, w2v_ctc_loss=0.717, task_loss=1.508, contrastive_loss=0.179, total=4065.22, n_correct=2687.23, ppl=4.41, accuracy=66.103, wps=11179.4, ups=1.38, wpb=8130.4, bsz=286.4, num_updates=22500, lr=9.42809e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=72, gb_free=15.8, wall=21036
2023-08-15 21:01:30 | INFO | train_inner | epoch 016:    502 / 1474 loss=1.99, trans_loss=4.905, nll_loss=2.146, w2v_ctc_loss=0.708, task_loss=1.341, contrastive_loss=0.12, total=4181.93, n_correct=2769.28, ppl=4.43, accuracy=66.22, wps=11399.8, ups=1.36, wpb=8363.9, bsz=320.3, num_updates=22600, lr=9.40721e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=73, gb_free=15.4, wall=21110
2023-08-15 21:02:42 | INFO | train_inner | epoch 016:    602 / 1474 loss=1.993, trans_loss=4.906, nll_loss=2.147, w2v_ctc_loss=0.717, task_loss=1.403, contrastive_loss=0.086, total=4122.97, n_correct=2727.73, ppl=4.43, accuracy=66.159, wps=11418, ups=1.38, wpb=8245.9, bsz=299, num_updates=22700, lr=9.38647e-05, gnorm=0.524, clip=0, loss_scale=16, train_wall=72, gb_free=15.8, wall=21182
2023-08-15 21:03:55 | INFO | train_inner | epoch 016:    702 / 1474 loss=1.997, trans_loss=4.908, nll_loss=2.149, w2v_ctc_loss=0.726, task_loss=1.445, contrastive_loss=0.088, total=4093.15, n_correct=2702.67, ppl=4.44, accuracy=66.029, wps=11223.5, ups=1.37, wpb=8186.3, bsz=296.5, num_updates=22800, lr=9.36586e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=72, gb_free=17.5, wall=21255
2023-08-15 21:05:08 | INFO | train_inner | epoch 016:    802 / 1474 loss=1.994, trans_loss=4.902, nll_loss=2.143, w2v_ctc_loss=0.707, task_loss=1.347, contrastive_loss=0.149, total=4183.24, n_correct=2768.88, ppl=4.42, accuracy=66.19, wps=11464.4, ups=1.37, wpb=8366.5, bsz=312.1, num_updates=22900, lr=9.34539e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=72, gb_free=17.5, wall=21328
2023-08-15 21:06:21 | INFO | train_inner | epoch 016:    902 / 1474 loss=1.997, trans_loss=4.905, nll_loss=2.147, w2v_ctc_loss=0.711, task_loss=1.384, contrastive_loss=0.145, total=4150.23, n_correct=2744.64, ppl=4.43, accuracy=66.132, wps=11352.9, ups=1.37, wpb=8300.5, bsz=306.5, num_updates=23000, lr=9.32505e-05, gnorm=0.533, clip=0, loss_scale=16, train_wall=73, gb_free=11.7, wall=21401
2023-08-15 21:07:34 | INFO | train_inner | epoch 016:   1002 / 1474 loss=2.003, trans_loss=4.909, nll_loss=2.152, w2v_ctc_loss=0.725, task_loss=1.449, contrastive_loss=0.14, total=4116.59, n_correct=2716.12, ppl=4.44, accuracy=65.98, wps=11294.8, ups=1.37, wpb=8233.2, bsz=300.6, num_updates=23100, lr=9.30484e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=72, gb_free=16.5, wall=21474
2023-08-15 21:08:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-15 21:08:49 | INFO | train_inner | epoch 016:   1103 / 1474 loss=2.006, trans_loss=4.916, nll_loss=2.161, w2v_ctc_loss=0.728, task_loss=1.49, contrastive_loss=0.117, total=4114.2, n_correct=2708.47, ppl=4.47, accuracy=65.832, wps=11072.8, ups=1.35, wpb=8228.4, bsz=296.2, num_updates=23200, lr=9.28477e-05, gnorm=0.532, clip=0, loss_scale=8, train_wall=74, gb_free=16.7, wall=21548
2023-08-15 21:10:03 | INFO | train_inner | epoch 016:   1203 / 1474 loss=2.005, trans_loss=4.911, nll_loss=2.154, w2v_ctc_loss=0.707, task_loss=1.437, contrastive_loss=0.21, total=4157.18, n_correct=2741.06, ppl=4.45, accuracy=65.936, wps=11242.9, ups=1.35, wpb=8314.4, bsz=306.3, num_updates=23300, lr=9.26482e-05, gnorm=0.53, clip=0, loss_scale=8, train_wall=73, gb_free=16, wall=21622
2023-08-15 21:11:16 | INFO | train_inner | epoch 016:   1303 / 1474 loss=2.01, trans_loss=4.912, nll_loss=2.155, w2v_ctc_loss=0.728, task_loss=1.381, contrastive_loss=0.192, total=4150.54, n_correct=2737.4, ppl=4.46, accuracy=65.953, wps=11292.3, ups=1.36, wpb=8301.1, bsz=312.3, num_updates=23400, lr=9.245e-05, gnorm=0.543, clip=0, loss_scale=8, train_wall=73, gb_free=12.4, wall=21695
2023-08-15 21:12:30 | INFO | train_inner | epoch 016:   1403 / 1474 loss=1.994, trans_loss=4.907, nll_loss=2.15, w2v_ctc_loss=0.716, task_loss=1.333, contrastive_loss=0.116, total=4198.78, n_correct=2774.18, ppl=4.44, accuracy=66.071, wps=11434.6, ups=1.36, wpb=8397.6, bsz=322.4, num_updates=23500, lr=9.22531e-05, gnorm=0.546, clip=0, loss_scale=8, train_wall=73, gb_free=17.4, wall=21769
2023-08-15 21:13:22 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 21:13:46 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 3.969 | trans_loss 5.178 | nll_loss 2.448 | w2v_ctc_loss 1.314 | task_loss 4.661 | contrastive_loss 0.31 | total 4003.4 | n_correct 2649.6 | ppl 5.46 | accuracy 66.184 | uer 18.257 | wer 20.092 | raw_wer 20.092 | bleu 22 | wps 2009.1 | wpb 4003.4 | bsz 141.8 | num_updates 23571 | best_bleu 22
2023-08-15 21:13:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 23571 updates
2023-08-15 21:13:46 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 21:14:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 21:14:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 16 @ 23571 updates, score 22.0) (writing took 31.913441132754087 seconds)
2023-08-15 21:14:19 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-08-15 21:14:19 | INFO | train | epoch 016 | loss 1.998 | trans_loss 4.906 | nll_loss 2.147 | w2v_ctc_loss 0.716 | task_loss 1.406 | contrastive_loss 0.145 | total 4138.64 | n_correct 2735.47 | ppl 4.43 | accuracy 66.096 | wps 10676.2 | ups 1.29 | wpb 8277.3 | bsz 305.6 | num_updates 23571 | lr 9.21141e-05 | gnorm 0.531 | clip 0 | loss_scale 8 | train_wall 1070 | gb_free 15.1 | wall 21878
2023-08-15 21:14:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 21:14:19 | INFO | fairseq.trainer | begin training epoch 17
2023-08-15 21:14:19 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 21:14:48 | INFO | train_inner | epoch 017:     29 / 1474 loss=2.004, trans_loss=4.896, nll_loss=2.135, w2v_ctc_loss=0.713, task_loss=1.452, contrastive_loss=0.255, total=4138.06, n_correct=2738.09, ppl=4.39, accuracy=66.168, wps=5977.4, ups=0.72, wpb=8276.1, bsz=300.4, num_updates=23600, lr=9.20575e-05, gnorm=0.531, clip=0, loss_scale=8, train_wall=73, gb_free=17.4, wall=21907
2023-08-15 21:16:01 | INFO | train_inner | epoch 017:    129 / 1474 loss=1.983, trans_loss=4.881, nll_loss=2.115, w2v_ctc_loss=0.715, task_loss=1.444, contrastive_loss=0.089, total=4110.37, n_correct=2736.31, ppl=4.33, accuracy=66.571, wps=11275.4, ups=1.37, wpb=8220.7, bsz=295.6, num_updates=23700, lr=9.1863e-05, gnorm=0.52, clip=0, loss_scale=8, train_wall=72, gb_free=16.2, wall=21980
2023-08-15 21:17:14 | INFO | train_inner | epoch 017:    229 / 1474 loss=1.996, trans_loss=4.885, nll_loss=2.121, w2v_ctc_loss=0.698, task_loss=1.311, contrastive_loss=0.254, total=4181.59, n_correct=2781.2, ppl=4.35, accuracy=66.511, wps=11493, ups=1.37, wpb=8363.2, bsz=322.2, num_updates=23800, lr=9.16698e-05, gnorm=0.528, clip=0, loss_scale=8, train_wall=72, gb_free=15.4, wall=22053
2023-08-15 21:18:27 | INFO | train_inner | epoch 017:    329 / 1474 loss=1.998, trans_loss=4.888, nll_loss=2.124, w2v_ctc_loss=0.708, task_loss=1.414, contrastive_loss=0.26, total=4157.97, n_correct=2762.72, ppl=4.36, accuracy=66.444, wps=11405.8, ups=1.37, wpb=8315.9, bsz=304, num_updates=23900, lr=9.14779e-05, gnorm=0.528, clip=0, loss_scale=8, train_wall=72, gb_free=15.5, wall=22126
2023-08-15 21:19:41 | INFO | train_inner | epoch 017:    429 / 1474 loss=1.98, trans_loss=4.887, nll_loss=2.122, w2v_ctc_loss=0.706, task_loss=1.405, contrastive_loss=0.088, total=4135.12, n_correct=2750.59, ppl=4.35, accuracy=66.518, wps=11202.9, ups=1.35, wpb=8270.2, bsz=306.1, num_updates=24000, lr=9.12871e-05, gnorm=0.554, clip=0, loss_scale=8, train_wall=73, gb_free=12.2, wall=22200
2023-08-15 21:19:41 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 21:20:04 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 3.98 | trans_loss 5.182 | nll_loss 2.451 | w2v_ctc_loss 1.35 | task_loss 4.669 | contrastive_loss 0.301 | total 4003.4 | n_correct 2651.3 | ppl 5.47 | accuracy 66.226 | uer 18.355 | wer 20.055 | raw_wer 20.055 | bleu 21.65 | wps 2028.9 | wpb 4003.4 | bsz 141.8 | num_updates 24000 | best_bleu 22
2023-08-15 21:20:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 24000 updates
2023-08-15 21:20:04 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_17_24000.pt
2023-08-15 21:20:08 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_17_24000.pt
2023-08-15 21:20:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_17_24000.pt (epoch 17 @ 24000 updates, score 21.65) (writing took 41.16561941988766 seconds)
2023-08-15 21:22:00 | INFO | train_inner | epoch 017:    529 / 1474 loss=1.989, trans_loss=4.892, nll_loss=2.13, w2v_ctc_loss=0.71, task_loss=1.457, contrastive_loss=0.133, total=4185.81, n_correct=2779.05, ppl=4.38, accuracy=66.392, wps=5991.7, ups=0.72, wpb=8371.6, bsz=308.6, num_updates=24100, lr=9.10975e-05, gnorm=0.516, clip=0, loss_scale=8, train_wall=73, gb_free=15.7, wall=22340
2023-08-15 21:23:13 | INFO | train_inner | epoch 017:    629 / 1474 loss=1.981, trans_loss=4.891, nll_loss=2.129, w2v_ctc_loss=0.703, task_loss=1.406, contrastive_loss=0.086, total=4168.62, n_correct=2772.97, ppl=4.37, accuracy=66.52, wps=11451.2, ups=1.37, wpb=8337.2, bsz=303.2, num_updates=24200, lr=9.09091e-05, gnorm=0.511, clip=0, loss_scale=8, train_wall=72, gb_free=13.9, wall=22412
2023-08-15 21:24:26 | INFO | train_inner | epoch 017:    729 / 1474 loss=2, trans_loss=4.898, nll_loss=2.137, w2v_ctc_loss=0.727, task_loss=1.396, contrastive_loss=0.133, total=4167.34, n_correct=2760.97, ppl=4.4, accuracy=66.253, wps=11430.9, ups=1.37, wpb=8334.7, bsz=307.7, num_updates=24300, lr=9.07218e-05, gnorm=0.535, clip=0, loss_scale=8, train_wall=72, gb_free=9.5, wall=22485
2023-08-15 21:25:39 | INFO | train_inner | epoch 017:    829 / 1474 loss=1.988, trans_loss=4.895, nll_loss=2.133, w2v_ctc_loss=0.713, task_loss=1.417, contrastive_loss=0.097, total=4092.64, n_correct=2713.86, ppl=4.39, accuracy=66.311, wps=11249.7, ups=1.37, wpb=8185.3, bsz=296.2, num_updates=24400, lr=9.05357e-05, gnorm=0.529, clip=0, loss_scale=8, train_wall=72, gb_free=15.4, wall=22558
2023-08-15 21:26:51 | INFO | train_inner | epoch 017:    929 / 1474 loss=1.978, trans_loss=4.888, nll_loss=2.125, w2v_ctc_loss=0.699, task_loss=1.379, contrastive_loss=0.094, total=4109.5, n_correct=2733.19, ppl=4.36, accuracy=66.509, wps=11441.7, ups=1.39, wpb=8219, bsz=305.4, num_updates=24500, lr=9.03508e-05, gnorm=0.522, clip=0, loss_scale=8, train_wall=71, gb_free=16.2, wall=22630
2023-08-15 21:28:04 | INFO | train_inner | epoch 017:   1029 / 1474 loss=1.983, trans_loss=4.89, nll_loss=2.128, w2v_ctc_loss=0.709, task_loss=1.417, contrastive_loss=0.098, total=4098.36, n_correct=2722.78, ppl=4.37, accuracy=66.436, wps=11230.5, ups=1.37, wpb=8196.7, bsz=301.7, num_updates=24600, lr=9.0167e-05, gnorm=0.536, clip=0, loss_scale=8, train_wall=72, gb_free=15.4, wall=22703
2023-08-15 21:29:16 | INFO | train_inner | epoch 017:   1129 / 1474 loss=1.982, trans_loss=4.89, nll_loss=2.127, w2v_ctc_loss=0.704, task_loss=1.429, contrastive_loss=0.089, total=4100.14, n_correct=2724.54, ppl=4.37, accuracy=66.45, wps=11342.2, ups=1.38, wpb=8200.3, bsz=299.3, num_updates=24700, lr=8.99843e-05, gnorm=0.524, clip=0, loss_scale=8, train_wall=72, gb_free=16.1, wall=22775
2023-08-15 21:30:30 | INFO | train_inner | epoch 017:   1229 / 1474 loss=2.017, trans_loss=4.903, nll_loss=2.145, w2v_ctc_loss=0.699, task_loss=1.352, contrastive_loss=0.391, total=4173.98, n_correct=2755.62, ppl=4.42, accuracy=66.019, wps=11229.4, ups=1.35, wpb=8348, bsz=325.9, num_updates=24800, lr=8.98027e-05, gnorm=0.531, clip=0, loss_scale=8, train_wall=74, gb_free=15.5, wall=22850
2023-08-15 21:31:43 | INFO | train_inner | epoch 017:   1329 / 1474 loss=1.983, trans_loss=4.892, nll_loss=2.13, w2v_ctc_loss=0.701, task_loss=1.411, contrastive_loss=0.1, total=4146.07, n_correct=2754.25, ppl=4.38, accuracy=66.43, wps=11444.7, ups=1.38, wpb=8292.1, bsz=303.2, num_updates=24900, lr=8.96221e-05, gnorm=0.521, clip=0, loss_scale=8, train_wall=72, gb_free=16.2, wall=22922
2023-08-15 21:32:56 | INFO | train_inner | epoch 017:   1429 / 1474 loss=1.979, trans_loss=4.891, nll_loss=2.128, w2v_ctc_loss=0.702, task_loss=1.421, contrastive_loss=0.09, total=4119.23, n_correct=2737.26, ppl=4.37, accuracy=66.451, wps=11271.1, ups=1.37, wpb=8238.5, bsz=303.4, num_updates=25000, lr=8.94427e-05, gnorm=0.535, clip=0, loss_scale=8, train_wall=73, gb_free=12.8, wall=22995
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:0')
2023-08-15 21:33:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:2')
2023-08-15 21:33:53 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 3.964 | trans_loss 5.174 | nll_loss 2.444 | w2v_ctc_loss 1.307 | task_loss 4.657 | contrastive_loss 0.311 | total 4003.4 | n_correct 2656.4 | ppl 5.44 | accuracy 66.354 | uer 17.939 | wer 19.653 | raw_wer 19.653 | bleu 22.12 | wps 2075.1 | wpb 4003.4 | bsz 141.8 | num_updates 25045 | best_bleu 22.12
2023-08-15 21:33:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 25045 updates
2023-08-15 21:33:53 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 21:34:09 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 21:34:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 17 @ 25045 updates, score 22.12) (writing took 30.640390330925584 seconds)
2023-08-15 21:34:24 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-08-15 21:34:24 | INFO | train | epoch 017 | loss 1.988 | trans_loss 4.891 | nll_loss 2.128 | w2v_ctc_loss 0.707 | task_loss 1.406 | contrastive_loss 0.142 | total 4138.65 | n_correct 2748.89 | ppl 4.37 | accuracy 66.42 | wps 10121.9 | ups 1.22 | wpb 8277.3 | bsz 305.7 | num_updates 25045 | lr 8.93623e-05 | gnorm 0.527 | clip 0 | loss_scale 8 | train_wall 1067 | gb_free 16 | wall 23083
2023-08-15 21:34:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 21:34:24 | INFO | fairseq.trainer | begin training epoch 18
2023-08-15 21:34:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 21:35:14 | INFO | train_inner | epoch 018:     55 / 1474 loss=1.981, trans_loss=4.884, nll_loss=2.12, w2v_ctc_loss=0.709, task_loss=1.437, contrastive_loss=0.098, total=4128.93, n_correct=2750.31, ppl=4.35, accuracy=66.611, wps=5978, ups=0.72, wpb=8257.9, bsz=301.7, num_updates=25100, lr=8.92644e-05, gnorm=0.517, clip=0, loss_scale=8, train_wall=74, gb_free=16.2, wall=23133
2023-08-15 21:36:27 | INFO | train_inner | epoch 018:    155 / 1474 loss=1.978, trans_loss=4.866, nll_loss=2.097, w2v_ctc_loss=0.681, task_loss=1.332, contrastive_loss=0.22, total=4158.38, n_correct=2781.68, ppl=4.28, accuracy=66.893, wps=11433.8, ups=1.37, wpb=8316.8, bsz=313.7, num_updates=25200, lr=8.90871e-05, gnorm=0.517, clip=0, loss_scale=8, train_wall=72, gb_free=16.4, wall=23206
2023-08-15 21:37:40 | INFO | train_inner | epoch 018:    255 / 1474 loss=1.966, trans_loss=4.864, nll_loss=2.094, w2v_ctc_loss=0.693, task_loss=1.363, contrastive_loss=0.091, total=4161.92, n_correct=2791.26, ppl=4.27, accuracy=67.067, wps=11421.8, ups=1.37, wpb=8323.8, bsz=312.8, num_updates=25300, lr=8.89108e-05, gnorm=0.542, clip=0, loss_scale=16, train_wall=72, gb_free=16.2, wall=23279
2023-08-15 21:38:53 | INFO | train_inner | epoch 018:    355 / 1474 loss=1.972, trans_loss=4.874, nll_loss=2.106, w2v_ctc_loss=0.692, task_loss=1.431, contrastive_loss=0.102, total=4167.42, n_correct=2782.09, ppl=4.3, accuracy=66.758, wps=11411.8, ups=1.37, wpb=8334.8, bsz=301.2, num_updates=25400, lr=8.87357e-05, gnorm=0.518, clip=0, loss_scale=16, train_wall=73, gb_free=15, wall=23352
2023-08-15 21:40:06 | INFO | train_inner | epoch 018:    455 / 1474 loss=1.987, trans_loss=4.879, nll_loss=2.112, w2v_ctc_loss=0.703, task_loss=1.509, contrastive_loss=0.194, total=4075.78, n_correct=2716.88, ppl=4.32, accuracy=66.659, wps=11049.8, ups=1.36, wpb=8151.6, bsz=294.2, num_updates=25500, lr=8.85615e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=73, gb_free=17.5, wall=23426
2023-08-15 21:41:19 | INFO | train_inner | epoch 018:    555 / 1474 loss=1.964, trans_loss=4.865, nll_loss=2.095, w2v_ctc_loss=0.689, task_loss=1.255, contrastive_loss=0.101, total=4218.07, n_correct=2825.3, ppl=4.27, accuracy=66.981, wps=11585.9, ups=1.37, wpb=8436.1, bsz=329.6, num_updates=25600, lr=8.83883e-05, gnorm=0.513, clip=0, loss_scale=16, train_wall=72, gb_free=15.5, wall=23498
2023-08-15 21:42:32 | INFO | train_inner | epoch 018:    655 / 1474 loss=1.991, trans_loss=4.887, nll_loss=2.123, w2v_ctc_loss=0.711, task_loss=1.451, contrastive_loss=0.172, total=4093.44, n_correct=2720.56, ppl=4.36, accuracy=66.461, wps=11304.3, ups=1.38, wpb=8186.9, bsz=298.5, num_updates=25700, lr=8.82162e-05, gnorm=0.532, clip=0, loss_scale=16, train_wall=72, gb_free=14.9, wall=23571
2023-08-15 21:43:45 | INFO | train_inner | epoch 018:    755 / 1474 loss=1.994, trans_loss=4.883, nll_loss=2.118, w2v_ctc_loss=0.706, task_loss=1.34, contrastive_loss=0.262, total=4202.99, n_correct=2795.16, ppl=4.34, accuracy=66.504, wps=11460.3, ups=1.36, wpb=8406, bsz=322.5, num_updates=25800, lr=8.80451e-05, gnorm=0.524, clip=0, loss_scale=16, train_wall=73, gb_free=17.3, wall=23644
2023-08-15 21:44:58 | INFO | train_inner | epoch 018:    855 / 1474 loss=1.976, trans_loss=4.879, nll_loss=2.112, w2v_ctc_loss=0.701, task_loss=1.412, contrastive_loss=0.088, total=4177.43, n_correct=2782.13, ppl=4.32, accuracy=66.599, wps=11414.1, ups=1.37, wpb=8354.9, bsz=304.8, num_updates=25900, lr=8.7875e-05, gnorm=0.548, clip=0, loss_scale=16, train_wall=73, gb_free=15.8, wall=23717
2023-08-15 21:46:10 | INFO | train_inner | epoch 018:    955 / 1474 loss=1.962, trans_loss=4.868, nll_loss=2.099, w2v_ctc_loss=0.68, task_loss=1.309, contrastive_loss=0.094, total=4138.23, n_correct=2770.43, ppl=4.29, accuracy=66.947, wps=11485.6, ups=1.39, wpb=8276.5, bsz=314.5, num_updates=26000, lr=8.77058e-05, gnorm=0.522, clip=0, loss_scale=16, train_wall=72, gb_free=16.2, wall=23790
2023-08-15 21:46:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 21:46:35 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 3.972 | trans_loss 5.178 | nll_loss 2.446 | w2v_ctc_loss 1.33 | task_loss 4.618 | contrastive_loss 0.305 | total 4003.4 | n_correct 2663.6 | ppl 5.45 | accuracy 66.533 | uer 18.395 | wer 20.189 | raw_wer 20.189 | bleu 21.87 | wps 1994.4 | wpb 4003.4 | bsz 141.8 | num_updates 26000 | best_bleu 22.12
2023-08-15 21:46:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26000 updates
2023-08-15 21:46:35 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_18_26000.pt
2023-08-15 21:46:38 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_18_26000.pt
2023-08-15 21:47:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_18_26000.pt (epoch 18 @ 26000 updates, score 21.87) (writing took 25.110239991918206 seconds)
2023-08-15 21:48:14 | INFO | train_inner | epoch 018:   1055 / 1474 loss=1.975, trans_loss=4.878, nll_loss=2.111, w2v_ctc_loss=0.697, task_loss=1.479, contrastive_loss=0.09, total=4133.59, n_correct=2755.06, ppl=4.32, accuracy=66.651, wps=6654.5, ups=0.8, wpb=8267.2, bsz=298.7, num_updates=26100, lr=8.75376e-05, gnorm=0.531, clip=0, loss_scale=16, train_wall=73, gb_free=15.5, wall=23914
2023-08-15 21:49:27 | INFO | train_inner | epoch 018:   1155 / 1474 loss=1.981, trans_loss=4.87, nll_loss=2.102, w2v_ctc_loss=0.697, task_loss=1.328, contrastive_loss=0.196, total=4154.22, n_correct=2777.72, ppl=4.29, accuracy=66.865, wps=11430.4, ups=1.38, wpb=8308.4, bsz=315.1, num_updates=26200, lr=8.73704e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=72, gb_free=14.6, wall=23986
2023-08-15 21:50:40 | INFO | train_inner | epoch 018:   1255 / 1474 loss=1.979, trans_loss=4.887, nll_loss=2.124, w2v_ctc_loss=0.7, task_loss=1.507, contrastive_loss=0.085, total=4089.17, n_correct=2721.59, ppl=4.36, accuracy=66.556, wps=11290.3, ups=1.38, wpb=8178.3, bsz=287.6, num_updates=26300, lr=8.72041e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=72, gb_free=16.4, wall=24059
2023-08-15 21:51:52 | INFO | train_inner | epoch 018:   1355 / 1474 loss=1.984, trans_loss=4.883, nll_loss=2.118, w2v_ctc_loss=0.71, task_loss=1.498, contrastive_loss=0.11, total=4068.84, n_correct=2706.06, ppl=4.34, accuracy=66.507, wps=11234.1, ups=1.38, wpb=8137.7, bsz=291.4, num_updates=26400, lr=8.70388e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=72, gb_free=15, wall=24131
2023-08-15 21:53:05 | INFO | train_inner | epoch 018:   1455 / 1474 loss=1.981, trans_loss=4.883, nll_loss=2.119, w2v_ctc_loss=0.707, task_loss=1.492, contrastive_loss=0.096, total=4113.23, n_correct=2737.28, ppl=4.34, accuracy=66.548, wps=11248.4, ups=1.37, wpb=8226.5, bsz=297.2, num_updates=26500, lr=8.68744e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=73, gb_free=16.3, wall=24204
2023-08-15 21:53:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 21:53:43 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 3.967 | trans_loss 5.168 | nll_loss 2.436 | w2v_ctc_loss 1.336 | task_loss 4.655 | contrastive_loss 0.3 | total 4003.4 | n_correct 2665.5 | ppl 5.41 | accuracy 66.581 | uer 17.909 | wer 19.567 | raw_wer 19.567 | bleu 22.12 | wps 2049.3 | wpb 4003.4 | bsz 141.8 | num_updates 26519 | best_bleu 22.12
2023-08-15 21:53:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26519 updates
2023-08-15 21:53:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 21:54:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 21:54:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 18 @ 26519 updates, score 22.12) (writing took 32.69371165148914 seconds)
2023-08-15 21:54:17 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-08-15 21:54:17 | INFO | train | epoch 018 | loss 1.978 | trans_loss 4.876 | nll_loss 2.109 | w2v_ctc_loss 0.698 | task_loss 1.405 | contrastive_loss 0.139 | total 4138.65 | n_correct 2761.05 | ppl 4.32 | accuracy 66.714 | wps 10231.8 | ups 1.24 | wpb 8277.3 | bsz 305.7 | num_updates 26519 | lr 8.68433e-05 | gnorm 0.527 | clip 0 | loss_scale 16 | train_wall 1068 | gb_free 15.6 | wall 24276
2023-08-15 21:54:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 21:54:17 | INFO | fairseq.trainer | begin training epoch 19
2023-08-15 21:54:17 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 21:55:24 | INFO | train_inner | epoch 019:     81 / 1474 loss=1.969, trans_loss=4.858, nll_loss=2.086, w2v_ctc_loss=0.689, task_loss=1.4, contrastive_loss=0.147, total=4107.26, n_correct=2755.1, ppl=4.24, accuracy=67.079, wps=5926.7, ups=0.72, wpb=8214.5, bsz=297.5, num_updates=26600, lr=8.6711e-05, gnorm=0.532, clip=0, loss_scale=16, train_wall=73, gb_free=12.2, wall=24343
2023-08-15 21:56:37 | INFO | train_inner | epoch 019:    181 / 1474 loss=1.967, trans_loss=4.856, nll_loss=2.082, w2v_ctc_loss=0.692, task_loss=1.312, contrastive_loss=0.138, total=4222.18, n_correct=2837.83, ppl=4.24, accuracy=67.212, wps=11512.9, ups=1.36, wpb=8444.4, bsz=324.4, num_updates=26700, lr=8.65485e-05, gnorm=0.519, clip=0, loss_scale=16, train_wall=73, gb_free=11.2, wall=24416
2023-08-15 21:57:50 | INFO | train_inner | epoch 019:    281 / 1474 loss=1.959, trans_loss=4.854, nll_loss=2.08, w2v_ctc_loss=0.686, task_loss=1.38, contrastive_loss=0.082, total=4187.37, n_correct=2814.19, ppl=4.23, accuracy=67.207, wps=11551.3, ups=1.38, wpb=8374.7, bsz=307, num_updates=26800, lr=8.63868e-05, gnorm=0.52, clip=0, loss_scale=16, train_wall=72, gb_free=17.2, wall=24489
2023-08-15 21:59:02 | INFO | train_inner | epoch 019:    381 / 1474 loss=1.967, trans_loss=4.853, nll_loss=2.079, w2v_ctc_loss=0.678, task_loss=1.388, contrastive_loss=0.187, total=4170.67, n_correct=2801.03, ppl=4.22, accuracy=67.16, wps=11508.5, ups=1.38, wpb=8341.3, bsz=310.5, num_updates=26900, lr=8.62261e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=72, gb_free=16.9, wall=24561
2023-08-15 22:00:15 | INFO | train_inner | epoch 019:    481 / 1474 loss=1.96, trans_loss=4.858, nll_loss=2.086, w2v_ctc_loss=0.683, task_loss=1.443, contrastive_loss=0.095, total=4115.22, n_correct=2765.09, ppl=4.24, accuracy=67.192, wps=11246.3, ups=1.37, wpb=8230.4, bsz=301.9, num_updates=27000, lr=8.60663e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=73, gb_free=14.7, wall=24635
2023-08-15 22:01:28 | INFO | train_inner | epoch 019:    581 / 1474 loss=1.963, trans_loss=4.854, nll_loss=2.081, w2v_ctc_loss=0.68, task_loss=1.378, contrastive_loss=0.16, total=4129.22, n_correct=2774.27, ppl=4.23, accuracy=67.186, wps=11378.2, ups=1.38, wpb=8258.4, bsz=306, num_updates=27100, lr=8.59074e-05, gnorm=0.518, clip=0, loss_scale=16, train_wall=72, gb_free=15.5, wall=24707
2023-08-15 22:02:40 | INFO | train_inner | epoch 019:    681 / 1474 loss=1.953, trans_loss=4.862, nll_loss=2.091, w2v_ctc_loss=0.668, task_loss=1.281, contrastive_loss=0.086, total=4197.2, n_correct=2819.97, ppl=4.26, accuracy=67.187, wps=11594.7, ups=1.38, wpb=8394.4, bsz=320.8, num_updates=27200, lr=8.57493e-05, gnorm=0.515, clip=0, loss_scale=16, train_wall=72, gb_free=14.3, wall=24780
2023-08-15 22:03:53 | INFO | train_inner | epoch 019:    781 / 1474 loss=1.968, trans_loss=4.863, nll_loss=2.092, w2v_ctc_loss=0.697, task_loss=1.415, contrastive_loss=0.099, total=4142.6, n_correct=2774.12, ppl=4.26, accuracy=66.966, wps=11364.6, ups=1.37, wpb=8285.2, bsz=305.1, num_updates=27300, lr=8.55921e-05, gnorm=0.554, clip=0, loss_scale=32, train_wall=72, gb_free=15.7, wall=24853
2023-08-15 22:05:06 | INFO | train_inner | epoch 019:    881 / 1474 loss=1.968, trans_loss=4.868, nll_loss=2.098, w2v_ctc_loss=0.695, task_loss=1.436, contrastive_loss=0.085, total=4153.47, n_correct=2779.6, ppl=4.28, accuracy=66.922, wps=11388.8, ups=1.37, wpb=8306.9, bsz=303.1, num_updates=27400, lr=8.54358e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=72, gb_free=15.9, wall=24925
2023-08-15 22:06:20 | INFO | train_inner | epoch 019:    981 / 1474 loss=1.991, trans_loss=4.875, nll_loss=2.108, w2v_ctc_loss=0.69, task_loss=1.404, contrastive_loss=0.32, total=4101.29, n_correct=2739.44, ppl=4.31, accuracy=66.795, wps=11126.7, ups=1.36, wpb=8202.6, bsz=309.9, num_updates=27500, lr=8.52803e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=73, gb_free=16.3, wall=24999
2023-08-15 22:07:32 | INFO | train_inner | epoch 019:   1081 / 1474 loss=1.973, trans_loss=4.873, nll_loss=2.105, w2v_ctc_loss=0.69, task_loss=1.502, contrastive_loss=0.128, total=4036.97, n_correct=2698.44, ppl=4.3, accuracy=66.843, wps=11125.1, ups=1.38, wpb=8073.9, bsz=291, num_updates=27600, lr=8.51257e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=72, gb_free=14.3, wall=25072
2023-08-15 22:08:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-15 22:08:46 | INFO | train_inner | epoch 019:   1182 / 1474 loss=1.972, trans_loss=4.871, nll_loss=2.102, w2v_ctc_loss=0.697, task_loss=1.494, contrastive_loss=0.095, total=4108.86, n_correct=2747.74, ppl=4.29, accuracy=66.874, wps=11099.8, ups=1.35, wpb=8217.7, bsz=296.6, num_updates=27700, lr=8.49719e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=74, gb_free=17.4, wall=25146
2023-08-15 22:09:59 | INFO | train_inner | epoch 019:   1282 / 1474 loss=1.971, trans_loss=4.872, nll_loss=2.104, w2v_ctc_loss=0.689, task_loss=1.413, contrastive_loss=0.108, total=4147.96, n_correct=2773.15, ppl=4.3, accuracy=66.856, wps=11497.1, ups=1.39, wpb=8295.9, bsz=301.4, num_updates=27800, lr=8.48189e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=72, gb_free=15.5, wall=25218
2023-08-15 22:11:11 | INFO | train_inner | epoch 019:   1382 / 1474 loss=1.97, trans_loss=4.867, nll_loss=2.098, w2v_ctc_loss=0.696, task_loss=1.44, contrastive_loss=0.094, total=4125.32, n_correct=2763.62, ppl=4.28, accuracy=66.992, wps=11397.9, ups=1.38, wpb=8250.6, bsz=300.4, num_updates=27900, lr=8.46668e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=72, gb_free=16.1, wall=25290
2023-08-15 22:12:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 22:12:44 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 3.968 | trans_loss 5.167 | nll_loss 2.433 | w2v_ctc_loss 1.341 | task_loss 4.636 | contrastive_loss 0.305 | total 4003.4 | n_correct 2661.5 | ppl 5.4 | accuracy 66.481 | uer 17.67 | wer 19.515 | raw_wer 19.515 | bleu 22.2 | wps 2048.6 | wpb 4003.4 | bsz 141.8 | num_updates 27992 | best_bleu 22.2
2023-08-15 22:12:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 27992 updates
2023-08-15 22:12:44 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 22:13:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-15 22:13:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 19 @ 27992 updates, score 22.2) (writing took 29.888692254200578 seconds)
2023-08-15 22:13:15 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-08-15 22:13:15 | INFO | train | epoch 019 | loss 1.968 | trans_loss 4.862 | nll_loss 2.091 | w2v_ctc_loss 0.688 | task_loss 1.408 | contrastive_loss 0.13 | total 4137.04 | n_correct 2773.63 | ppl 4.26 | accuracy 67.044 | wps 10710.6 | ups 1.29 | wpb 8274.1 | bsz 305 | num_updates 27992 | lr 8.45275e-05 | gnorm 0.527 | clip 0 | loss_scale 16 | train_wall 1067 | gb_free 17 | wall 25414
2023-08-15 22:13:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 22:13:15 | INFO | fairseq.trainer | begin training epoch 20
2023-08-15 22:13:15 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 22:13:28 | INFO | train_inner | epoch 020:      8 / 1474 loss=1.968, trans_loss=4.856, nll_loss=2.084, w2v_ctc_loss=0.687, task_loss=1.411, contrastive_loss=0.176, total=4124.63, n_correct=2767.36, ppl=4.24, accuracy=67.094, wps=6041.4, ups=0.73, wpb=8249.3, bsz=304.8, num_updates=28000, lr=8.45154e-05, gnorm=0.522, clip=0, loss_scale=16, train_wall=73, gb_free=17, wall=25427
2023-08-15 22:13:28 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 22:13:52 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 3.943 | trans_loss 5.166 | nll_loss 2.433 | w2v_ctc_loss 1.265 | task_loss 4.633 | contrastive_loss 0.3 | total 4003.4 | n_correct 2663.3 | ppl 5.4 | accuracy 66.526 | uer 17.434 | wer 19.25 | raw_wer 19.25 | bleu 22.26 | wps 2019.6 | wpb 4003.4 | bsz 141.8 | num_updates 28000 | best_bleu 22.26
2023-08-15 22:13:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 28000 updates
2023-08-15 22:13:52 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_20_28000.pt
2023-08-15 22:13:55 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_20_28000.pt
2023-08-15 22:14:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_20_28000.pt (epoch 20 @ 28000 updates, score 22.26) (writing took 36.423141641542315 seconds)
2023-08-15 22:15:46 | INFO | train_inner | epoch 020:    108 / 1474 loss=1.947, trans_loss=4.838, nll_loss=2.059, w2v_ctc_loss=0.669, task_loss=1.36, contrastive_loss=0.1, total=4199.19, n_correct=2838.29, ppl=4.17, accuracy=67.591, wps=6061.1, ups=0.72, wpb=8398.4, bsz=314, num_updates=28100, lr=8.43649e-05, gnorm=0.517, clip=0, loss_scale=16, train_wall=73, gb_free=14.4, wall=25565
2023-08-15 22:17:00 | INFO | train_inner | epoch 020:    208 / 1474 loss=1.958, trans_loss=4.845, nll_loss=2.068, w2v_ctc_loss=0.677, task_loss=1.464, contrastive_loss=0.154, total=4148.29, n_correct=2797.91, ppl=4.19, accuracy=67.447, wps=11239.5, ups=1.35, wpb=8296.6, bsz=300.5, num_updates=28200, lr=8.42152e-05, gnorm=0.52, clip=0, loss_scale=16, train_wall=73, gb_free=15.3, wall=25639
2023-08-15 22:18:13 | INFO | train_inner | epoch 020:    308 / 1474 loss=1.943, trans_loss=4.838, nll_loss=2.061, w2v_ctc_loss=0.67, task_loss=1.268, contrastive_loss=0.089, total=4191.34, n_correct=2834.74, ppl=4.17, accuracy=67.633, wps=11537.8, ups=1.38, wpb=8382.7, bsz=326.2, num_updates=28300, lr=8.40663e-05, gnorm=0.516, clip=0, loss_scale=16, train_wall=72, gb_free=15.4, wall=25712
2023-08-15 22:19:25 | INFO | train_inner | epoch 020:    408 / 1474 loss=1.952, trans_loss=4.84, nll_loss=2.062, w2v_ctc_loss=0.678, task_loss=1.424, contrastive_loss=0.089, total=4114.19, n_correct=2776.42, ppl=4.17, accuracy=67.484, wps=11299.4, ups=1.37, wpb=8228.4, bsz=297.5, num_updates=28400, lr=8.39181e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=72, gb_free=14.9, wall=25785
2023-08-15 22:20:39 | INFO | train_inner | epoch 020:    508 / 1474 loss=1.962, trans_loss=4.852, nll_loss=2.078, w2v_ctc_loss=0.673, task_loss=1.447, contrastive_loss=0.176, total=4108.2, n_correct=2762.39, ppl=4.22, accuracy=67.241, wps=11220.5, ups=1.37, wpb=8216.4, bsz=299.5, num_updates=28500, lr=8.37708e-05, gnorm=0.519, clip=0, loss_scale=16, train_wall=73, gb_free=15.3, wall=25858
2023-08-15 22:21:51 | INFO | train_inner | epoch 020:    608 / 1474 loss=1.971, trans_loss=4.853, nll_loss=2.078, w2v_ctc_loss=0.687, task_loss=1.477, contrastive_loss=0.177, total=4092.44, n_correct=2747.25, ppl=4.22, accuracy=67.13, wps=11261, ups=1.38, wpb=8184.9, bsz=295.9, num_updates=28600, lr=8.36242e-05, gnorm=0.538, clip=0, loss_scale=16, train_wall=72, gb_free=17.3, wall=25931
2023-08-15 22:23:04 | INFO | train_inner | epoch 020:    708 / 1474 loss=1.958, trans_loss=4.852, nll_loss=2.078, w2v_ctc_loss=0.687, task_loss=1.408, contrastive_loss=0.082, total=4137.06, n_correct=2781, ppl=4.22, accuracy=67.222, wps=11467.7, ups=1.39, wpb=8274.1, bsz=300.5, num_updates=28700, lr=8.34784e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=72, gb_free=16.2, wall=26003
2023-08-15 22:24:16 | INFO | train_inner | epoch 020:    808 / 1474 loss=1.956, trans_loss=4.853, nll_loss=2.079, w2v_ctc_loss=0.682, task_loss=1.389, contrastive_loss=0.085, total=4146.78, n_correct=2790.4, ppl=4.23, accuracy=67.291, wps=11396.7, ups=1.37, wpb=8293.6, bsz=307.3, num_updates=28800, lr=8.33333e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=72, gb_free=14.9, wall=26076
2023-08-15 22:24:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-15 22:25:31 | INFO | train_inner | epoch 020:    909 / 1474 loss=1.988, trans_loss=4.859, nll_loss=2.088, w2v_ctc_loss=0.682, task_loss=1.328, contrastive_loss=0.376, total=4158.85, n_correct=2786.78, ppl=4.25, accuracy=67.008, wps=11154.1, ups=1.34, wpb=8317.7, bsz=324.5, num_updates=28900, lr=8.3189e-05, gnorm=0.539, clip=0, loss_scale=8, train_wall=74, gb_free=16, wall=26150
2023-08-15 22:26:44 | INFO | train_inner | epoch 020:   1009 / 1474 loss=1.953, trans_loss=4.852, nll_loss=2.078, w2v_ctc_loss=0.674, task_loss=1.402, contrastive_loss=0.089, total=4167.85, n_correct=2805.51, ppl=4.22, accuracy=67.313, wps=11348, ups=1.36, wpb=8335.7, bsz=306.3, num_updates=29000, lr=8.30455e-05, gnorm=0.523, clip=0, loss_scale=8, train_wall=73, gb_free=16.5, wall=26224
2023-08-15 22:27:58 | INFO | train_inner | epoch 020:   1109 / 1474 loss=1.974, trans_loss=4.856, nll_loss=2.083, w2v_ctc_loss=0.682, task_loss=1.35, contrastive_loss=0.227, total=4169.06, n_correct=2800.45, ppl=4.24, accuracy=67.172, wps=11348.4, ups=1.36, wpb=8338.1, bsz=315.8, num_updates=29100, lr=8.29027e-05, gnorm=0.529, clip=0, loss_scale=8, train_wall=73, gb_free=16.4, wall=26297
2023-08-15 22:29:11 | INFO | train_inner | epoch 020:   1209 / 1474 loss=1.959, trans_loss=4.845, nll_loss=2.068, w2v_ctc_loss=0.691, task_loss=1.566, contrastive_loss=0.079, total=4023.64, n_correct=2707.77, ppl=4.19, accuracy=67.297, wps=11044.6, ups=1.37, wpb=8047.3, bsz=282.8, num_updates=29200, lr=8.27606e-05, gnorm=0.537, clip=0, loss_scale=8, train_wall=72, gb_free=15.6, wall=26370
2023-08-15 22:30:24 | INFO | train_inner | epoch 020:   1309 / 1474 loss=1.958, trans_loss=4.855, nll_loss=2.082, w2v_ctc_loss=0.685, task_loss=1.469, contrastive_loss=0.084, total=4128.46, n_correct=2775.4, ppl=4.23, accuracy=67.226, wps=11265.4, ups=1.36, wpb=8256.9, bsz=299.2, num_updates=29300, lr=8.26192e-05, gnorm=0.521, clip=0, loss_scale=8, train_wall=73, gb_free=16.2, wall=26443
2023-08-15 22:31:37 | INFO | train_inner | epoch 020:   1409 / 1474 loss=1.961, trans_loss=4.855, nll_loss=2.081, w2v_ctc_loss=0.688, task_loss=1.48, contrastive_loss=0.082, total=4120.53, n_correct=2765.79, ppl=4.23, accuracy=67.122, wps=11246.5, ups=1.36, wpb=8241.1, bsz=294.2, num_updates=29400, lr=8.24786e-05, gnorm=0.531, clip=0, loss_scale=8, train_wall=73, gb_free=15.3, wall=26517
2023-08-15 22:32:24 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 22:32:49 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 3.94 | trans_loss 5.164 | nll_loss 2.426 | w2v_ctc_loss 1.263 | task_loss 4.642 | contrastive_loss 0.297 | total 4003.4 | n_correct 2667.2 | ppl 5.38 | accuracy 66.623 | uer 17.689 | wer 19.369 | raw_wer 19.369 | bleu 22.2 | wps 2082.7 | wpb 4003.4 | bsz 141.8 | num_updates 29465 | best_bleu 22.26
2023-08-15 22:32:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 29465 updates
2023-08-15 22:32:49 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.2000.pt
2023-08-15 22:32:52 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.2000.pt
2023-08-15 22:33:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.2000.pt (epoch 20 @ 29465 updates, score 22.2) (writing took 21.615713141858578 seconds)
2023-08-15 22:33:11 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-08-15 22:33:11 | INFO | train | epoch 020 | loss 1.96 | trans_loss 4.85 | nll_loss 2.075 | w2v_ctc_loss 0.68 | task_loss 1.406 | contrastive_loss 0.136 | total 4138.57 | n_correct 2785.26 | ppl 4.21 | accuracy 67.3 | wps 10192.4 | ups 1.23 | wpb 8277.1 | bsz 305.7 | num_updates 29465 | lr 8.23876e-05 | gnorm 0.526 | clip 0 | loss_scale 8 | train_wall 1070 | gb_free 15.8 | wall 26610
2023-08-15 22:33:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 22:33:11 | INFO | fairseq.trainer | begin training epoch 21
2023-08-15 22:33:11 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 22:33:45 | INFO | train_inner | epoch 021:     35 / 1474 loss=1.964, trans_loss=4.851, nll_loss=2.077, w2v_ctc_loss=0.673, task_loss=1.348, contrastive_loss=0.203, total=4145.63, n_correct=2790.51, ppl=4.22, accuracy=67.312, wps=6494.4, ups=0.78, wpb=8291.3, bsz=315.4, num_updates=29500, lr=8.23387e-05, gnorm=0.522, clip=0, loss_scale=8, train_wall=72, gb_free=16.5, wall=26644
2023-08-15 22:34:58 | INFO | train_inner | epoch 021:    135 / 1474 loss=1.953, trans_loss=4.83, nll_loss=2.05, w2v_ctc_loss=0.667, task_loss=1.315, contrastive_loss=0.195, total=4194.57, n_correct=2840.84, ppl=4.14, accuracy=67.727, wps=11439.1, ups=1.36, wpb=8389.1, bsz=319.6, num_updates=29600, lr=8.21995e-05, gnorm=0.534, clip=0, loss_scale=8, train_wall=73, gb_free=16, wall=26718
2023-08-15 22:36:11 | INFO | train_inner | epoch 021:    235 / 1474 loss=1.943, trans_loss=4.831, nll_loss=2.051, w2v_ctc_loss=0.658, task_loss=1.35, contrastive_loss=0.15, total=4152.42, n_correct=2811.07, ppl=4.14, accuracy=67.697, wps=11393.8, ups=1.37, wpb=8304.8, bsz=312, num_updates=29700, lr=8.2061e-05, gnorm=0.517, clip=0, loss_scale=8, train_wall=72, gb_free=17.1, wall=26790
2023-08-15 22:37:24 | INFO | train_inner | epoch 021:    335 / 1474 loss=1.956, trans_loss=4.839, nll_loss=2.061, w2v_ctc_loss=0.679, task_loss=1.393, contrastive_loss=0.151, total=4157.2, n_correct=2804.68, ppl=4.17, accuracy=67.466, wps=11355.8, ups=1.37, wpb=8314.4, bsz=311.1, num_updates=29800, lr=8.19232e-05, gnorm=0.521, clip=0, loss_scale=8, train_wall=73, gb_free=14.8, wall=26864
2023-08-15 22:38:37 | INFO | train_inner | epoch 021:    435 / 1474 loss=1.934, trans_loss=4.828, nll_loss=2.047, w2v_ctc_loss=0.654, task_loss=1.348, contrastive_loss=0.076, total=4181.07, n_correct=2838.77, ppl=4.13, accuracy=67.896, wps=11556.2, ups=1.38, wpb=8362.1, bsz=308.2, num_updates=29900, lr=8.17861e-05, gnorm=0.518, clip=0, loss_scale=8, train_wall=72, gb_free=14.5, wall=26936
2023-08-15 22:39:49 | INFO | train_inner | epoch 021:    535 / 1474 loss=1.94, trans_loss=4.825, nll_loss=2.042, w2v_ctc_loss=0.671, task_loss=1.461, contrastive_loss=0.076, total=4089.72, n_correct=2774.1, ppl=4.12, accuracy=67.831, wps=11249.1, ups=1.38, wpb=8179.4, bsz=295.7, num_updates=30000, lr=8.16497e-05, gnorm=0.527, clip=0, loss_scale=8, train_wall=72, gb_free=14.1, wall=27009
2023-08-15 22:39:49 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 22:40:13 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 3.965 | trans_loss 5.168 | nll_loss 2.433 | w2v_ctc_loss 1.335 | task_loss 4.651 | contrastive_loss 0.302 | total 4003.4 | n_correct 2661.9 | ppl 5.4 | accuracy 66.491 | uer 17.755 | wer 19.593 | raw_wer 19.593 | bleu 21.95 | wps 2191.1 | wpb 4003.4 | bsz 141.8 | num_updates 30000 | best_bleu 22.26
2023-08-15 22:40:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30000 updates
2023-08-15 22:40:13 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_21_30000.pt
2023-08-15 22:40:16 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_21_30000.pt
2023-08-15 22:40:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_21_30000.pt (epoch 21 @ 30000 updates, score 21.95) (writing took 23.79712088778615 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:0')
2023-08-15 22:41:51 | INFO | train_inner | epoch 021:    635 / 1474 loss=1.959, trans_loss=4.834, nll_loss=2.055, w2v_ctc_loss=0.67, task_loss=1.393, contrastive_loss=0.248, total=4210.28, n_correct=2841.61, ppl=4.16, accuracy=67.492, wps=6910.2, ups=0.82, wpb=8420.6, bsz=315.7, num_updates=30100, lr=8.15139e-05, gnorm=0.519, clip=0, loss_scale=8, train_wall=73, gb_free=16.2, wall=27131
2023-08-15 22:43:04 | INFO | train_inner | epoch 021:    735 / 1474 loss=1.949, trans_loss=4.842, nll_loss=2.065, w2v_ctc_loss=0.667, task_loss=1.407, contrastive_loss=0.107, total=4149.01, n_correct=2802.29, ppl=4.18, accuracy=67.541, wps=11405.8, ups=1.37, wpb=8298, bsz=307.4, num_updates=30200, lr=8.13788e-05, gnorm=0.522, clip=0, loss_scale=8, train_wall=72, gb_free=16.2, wall=27203
2023-08-15 22:44:18 | INFO | train_inner | epoch 021:    835 / 1474 loss=1.955, trans_loss=4.846, nll_loss=2.07, w2v_ctc_loss=0.67, task_loss=1.475, contrastive_loss=0.123, total=4075.99, n_correct=2748.19, ppl=4.2, accuracy=67.424, wps=11099.5, ups=1.36, wpb=8152, bsz=295.7, num_updates=30300, lr=8.12444e-05, gnorm=0.535, clip=0, loss_scale=8, train_wall=73, gb_free=15.9, wall=27277
2023-08-15 22:45:31 | INFO | train_inner | epoch 021:    935 / 1474 loss=1.948, trans_loss=4.838, nll_loss=2.059, w2v_ctc_loss=0.675, task_loss=1.412, contrastive_loss=0.093, total=4091.88, n_correct=2763.81, ppl=4.17, accuracy=67.544, wps=11200.6, ups=1.37, wpb=8183.8, bsz=300, num_updates=30400, lr=8.11107e-05, gnorm=0.531, clip=0, loss_scale=8, train_wall=72, gb_free=16.7, wall=27350
2023-08-15 22:46:43 | INFO | train_inner | epoch 021:   1035 / 1474 loss=1.952, trans_loss=4.846, nll_loss=2.07, w2v_ctc_loss=0.676, task_loss=1.43, contrastive_loss=0.091, total=4107.66, n_correct=2771.05, ppl=4.2, accuracy=67.461, wps=11288.7, ups=1.37, wpb=8215.3, bsz=299.5, num_updates=30500, lr=8.09776e-05, gnorm=0.53, clip=0, loss_scale=8, train_wall=72, gb_free=14.2, wall=27423
2023-08-15 22:47:56 | INFO | train_inner | epoch 021:   1135 / 1474 loss=1.951, trans_loss=4.837, nll_loss=2.058, w2v_ctc_loss=0.674, task_loss=1.506, contrastive_loss=0.097, total=4118.94, n_correct=2781.32, ppl=4.16, accuracy=67.525, wps=11323.1, ups=1.37, wpb=8237.9, bsz=294.9, num_updates=30600, lr=8.08452e-05, gnorm=0.531, clip=0, loss_scale=8, train_wall=72, gb_free=15.5, wall=27495
2023-08-15 22:49:09 | INFO | train_inner | epoch 021:   1235 / 1474 loss=1.956, trans_loss=4.843, nll_loss=2.066, w2v_ctc_loss=0.674, task_loss=1.35, contrastive_loss=0.145, total=4151.84, n_correct=2801.77, ppl=4.19, accuracy=67.483, wps=11385.7, ups=1.37, wpb=8303.7, bsz=309.1, num_updates=30700, lr=8.07134e-05, gnorm=0.526, clip=0, loss_scale=8, train_wall=72, gb_free=14.7, wall=27568
2023-08-15 22:50:22 | INFO | train_inner | epoch 021:   1335 / 1474 loss=1.95, trans_loss=4.841, nll_loss=2.064, w2v_ctc_loss=0.672, task_loss=1.364, contrastive_loss=0.108, total=4145.91, n_correct=2800.71, ppl=4.18, accuracy=67.554, wps=11380.8, ups=1.37, wpb=8291.8, bsz=312.1, num_updates=30800, lr=8.05823e-05, gnorm=0.53, clip=0, loss_scale=8, train_wall=72, gb_free=15.9, wall=27641
2023-08-15 22:51:35 | INFO | train_inner | epoch 021:   1435 / 1474 loss=1.97, trans_loss=4.851, nll_loss=2.076, w2v_ctc_loss=0.693, task_loss=1.472, contrastive_loss=0.157, total=4136.27, n_correct=2777.77, ppl=4.22, accuracy=67.156, wps=11253.1, ups=1.36, wpb=8272.5, bsz=304.5, num_updates=30900, lr=8.04518e-05, gnorm=0.538, clip=0, loss_scale=16, train_wall=73, gb_free=16.2, wall=27715
2023-08-15 22:52:04 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:7')
2023-08-15 22:52:28 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 3.96 | trans_loss 5.171 | nll_loss 2.436 | w2v_ctc_loss 1.303 | task_loss 4.635 | contrastive_loss 0.314 | total 4003.4 | n_correct 2665.5 | ppl 5.41 | accuracy 66.581 | uer 17.846 | wer 19.604 | raw_wer 19.604 | bleu 22.03 | wps 2121.1 | wpb 4003.4 | bsz 141.8 | num_updates 30939 | best_bleu 22.26
2023-08-15 22:52:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30939 updates
2023-08-15 22:52:28 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.0305.pt
2023-08-15 22:52:31 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.0305.pt
2023-08-15 22:53:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.0305.pt (epoch 21 @ 30939 updates, score 22.03) (writing took 37.38369319960475 seconds)
2023-08-15 22:53:08 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-08-15 22:53:08 | INFO | train | epoch 021 | loss 1.951 | trans_loss 4.838 | nll_loss 2.06 | w2v_ctc_loss 0.671 | task_loss 1.407 | contrastive_loss 0.134 | total 4138.65 | n_correct 2795.85 | ppl 4.17 | accuracy 67.555 | wps 10190.3 | ups 1.23 | wpb 8277.3 | bsz 305.7 | num_updates 30939 | lr 8.04011e-05 | gnorm 0.527 | clip 0 | loss_scale 16 | train_wall 1069 | gb_free 15.1 | wall 27807
2023-08-15 22:53:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 22:53:08 | INFO | fairseq.trainer | begin training epoch 22
2023-08-15 22:53:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 22:54:00 | INFO | train_inner | epoch 022:     61 / 1474 loss=1.938, trans_loss=4.823, nll_loss=2.04, w2v_ctc_loss=0.668, task_loss=1.415, contrastive_loss=0.076, total=4133.81, n_correct=2809.41, ppl=4.11, accuracy=67.962, wps=5708.5, ups=0.69, wpb=8267.6, bsz=300.5, num_updates=31000, lr=8.03219e-05, gnorm=0.52, clip=0, loss_scale=16, train_wall=72, gb_free=16, wall=27860
2023-08-15 22:55:14 | INFO | train_inner | epoch 022:    161 / 1474 loss=1.945, trans_loss=4.822, nll_loss=2.038, w2v_ctc_loss=0.666, task_loss=1.441, contrastive_loss=0.156, total=4116.11, n_correct=2792.16, ppl=4.11, accuracy=67.835, wps=11221.6, ups=1.36, wpb=8232.2, bsz=306.9, num_updates=31100, lr=8.01927e-05, gnorm=0.524, clip=0, loss_scale=16, train_wall=73, gb_free=16.4, wall=27933
2023-08-15 22:56:27 | INFO | train_inner | epoch 022:    261 / 1474 loss=1.93, trans_loss=4.817, nll_loss=2.033, w2v_ctc_loss=0.654, task_loss=1.231, contrastive_loss=0.098, total=4272.11, n_correct=2911.71, ppl=4.09, accuracy=68.156, wps=11680, ups=1.37, wpb=8544.2, bsz=331.4, num_updates=31200, lr=8.00641e-05, gnorm=0.514, clip=0, loss_scale=16, train_wall=73, gb_free=17.6, wall=28006
2023-08-15 22:57:41 | INFO | train_inner | epoch 022:    361 / 1474 loss=1.964, trans_loss=4.832, nll_loss=2.052, w2v_ctc_loss=0.67, task_loss=1.429, contrastive_loss=0.256, total=4178.4, n_correct=2823.68, ppl=4.15, accuracy=67.578, wps=11278.8, ups=1.35, wpb=8356.8, bsz=310, num_updates=31300, lr=7.99361e-05, gnorm=0.531, clip=0, loss_scale=16, train_wall=74, gb_free=14.9, wall=28080
2023-08-15 22:58:54 | INFO | train_inner | epoch 022:    461 / 1474 loss=1.949, trans_loss=4.827, nll_loss=2.045, w2v_ctc_loss=0.667, task_loss=1.479, contrastive_loss=0.14, total=4132.96, n_correct=2798.74, ppl=4.13, accuracy=67.718, wps=11278.5, ups=1.36, wpb=8265.9, bsz=297.5, num_updates=31400, lr=7.98087e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=73, gb_free=16.6, wall=28154
2023-08-15 23:00:07 | INFO | train_inner | epoch 022:    561 / 1474 loss=1.939, trans_loss=4.823, nll_loss=2.04, w2v_ctc_loss=0.669, task_loss=1.408, contrastive_loss=0.086, total=4158.17, n_correct=2820.91, ppl=4.11, accuracy=67.84, wps=11350.3, ups=1.36, wpb=8316.3, bsz=307.8, num_updates=31500, lr=7.96819e-05, gnorm=0.522, clip=0, loss_scale=16, train_wall=73, gb_free=16.3, wall=28227
2023-08-15 23:00:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-15 23:01:21 | INFO | train_inner | epoch 022:    662 / 1474 loss=1.93, trans_loss=4.817, nll_loss=2.032, w2v_ctc_loss=0.654, task_loss=1.372, contrastive_loss=0.077, total=4122.59, n_correct=2807.15, ppl=4.09, accuracy=68.092, wps=11284.8, ups=1.37, wpb=8245.2, bsz=304.2, num_updates=31600, lr=7.95557e-05, gnorm=0.519, clip=0, loss_scale=8, train_wall=73, gb_free=12.9, wall=28300
2023-08-15 23:02:34 | INFO | train_inner | epoch 022:    762 / 1474 loss=1.938, trans_loss=4.821, nll_loss=2.037, w2v_ctc_loss=0.665, task_loss=1.446, contrastive_loss=0.088, total=4170.82, n_correct=2831.41, ppl=4.1, accuracy=67.886, wps=11382.8, ups=1.36, wpb=8341.6, bsz=303.9, num_updates=31700, lr=7.94301e-05, gnorm=0.526, clip=0, loss_scale=8, train_wall=73, gb_free=11.4, wall=28373
2023-08-15 23:03:47 | INFO | train_inner | epoch 022:    862 / 1474 loss=1.941, trans_loss=4.83, nll_loss=2.049, w2v_ctc_loss=0.666, task_loss=1.512, contrastive_loss=0.076, total=4077.65, n_correct=2757.4, ppl=4.14, accuracy=67.622, wps=11115.9, ups=1.36, wpb=8155.3, bsz=290.5, num_updates=31800, lr=7.93052e-05, gnorm=0.53, clip=0, loss_scale=8, train_wall=73, gb_free=16.7, wall=28447
2023-08-15 23:05:00 | INFO | train_inner | epoch 022:    962 / 1474 loss=1.934, trans_loss=4.821, nll_loss=2.038, w2v_ctc_loss=0.658, task_loss=1.415, contrastive_loss=0.083, total=4136.66, n_correct=2808.18, ppl=4.11, accuracy=67.885, wps=11305.1, ups=1.37, wpb=8273.3, bsz=304.7, num_updates=31900, lr=7.91808e-05, gnorm=0.528, clip=0, loss_scale=8, train_wall=73, gb_free=14.3, wall=28520
2023-08-15 23:06:13 | INFO | train_inner | epoch 022:   1062 / 1474 loss=1.943, trans_loss=4.819, nll_loss=2.035, w2v_ctc_loss=0.651, task_loss=1.348, contrastive_loss=0.242, total=4152.13, n_correct=2822.43, ppl=4.1, accuracy=67.975, wps=11438, ups=1.38, wpb=8304.3, bsz=313.8, num_updates=32000, lr=7.90569e-05, gnorm=0.515, clip=0, loss_scale=8, train_wall=72, gb_free=15.7, wall=28592
2023-08-15 23:06:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 23:06:37 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 3.974 | trans_loss 5.167 | nll_loss 2.432 | w2v_ctc_loss 1.366 | task_loss 4.677 | contrastive_loss 0.303 | total 4003.4 | n_correct 2660.6 | ppl 5.4 | accuracy 66.459 | uer 17.676 | wer 19.537 | raw_wer 19.537 | bleu 22.18 | wps 2107.2 | wpb 4003.4 | bsz 141.8 | num_updates 32000 | best_bleu 22.26
2023-08-15 23:06:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32000 updates
2023-08-15 23:06:37 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_22_32000.pt
2023-08-15 23:06:40 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_22_32000.pt
2023-08-15 23:07:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_22_32000.pt (epoch 22 @ 32000 updates, score 22.18) (writing took 24.89333563297987 seconds)
2023-08-15 23:08:15 | INFO | train_inner | epoch 022:   1162 / 1474 loss=1.959, trans_loss=4.846, nll_loss=2.07, w2v_ctc_loss=0.682, task_loss=1.457, contrastive_loss=0.129, total=4102.27, n_correct=2762.87, ppl=4.2, accuracy=67.35, wps=6699.8, ups=0.82, wpb=8204.5, bsz=296.1, num_updates=32100, lr=7.89337e-05, gnorm=0.527, clip=0, loss_scale=8, train_wall=72, gb_free=16.4, wall=28715
2023-08-15 23:09:29 | INFO | train_inner | epoch 022:   1262 / 1474 loss=1.944, trans_loss=4.837, nll_loss=2.06, w2v_ctc_loss=0.662, task_loss=1.305, contrastive_loss=0.121, total=4179.1, n_correct=2828.52, ppl=4.17, accuracy=67.683, wps=11411.5, ups=1.37, wpb=8358.2, bsz=321.7, num_updates=32200, lr=7.8811e-05, gnorm=0.52, clip=0, loss_scale=8, train_wall=73, gb_free=16.7, wall=28788
2023-08-15 23:10:41 | INFO | train_inner | epoch 022:   1362 / 1474 loss=1.942, trans_loss=4.825, nll_loss=2.043, w2v_ctc_loss=0.662, task_loss=1.406, contrastive_loss=0.147, total=4061.14, n_correct=2755.88, ppl=4.12, accuracy=67.86, wps=11171.1, ups=1.38, wpb=8122.3, bsz=299.1, num_updates=32300, lr=7.86889e-05, gnorm=0.532, clip=0, loss_scale=8, train_wall=72, gb_free=16.6, wall=28861
2023-08-15 23:11:55 | INFO | train_inner | epoch 022:   1462 / 1474 loss=1.951, trans_loss=4.84, nll_loss=2.062, w2v_ctc_loss=0.678, task_loss=1.506, contrastive_loss=0.091, total=4083.08, n_correct=2756.46, ppl=4.18, accuracy=67.509, wps=11147.4, ups=1.37, wpb=8166.2, bsz=289.3, num_updates=32400, lr=7.85674e-05, gnorm=0.531, clip=0, loss_scale=8, train_wall=73, gb_free=15.8, wall=28934
2023-08-15 23:12:03 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 23:12:28 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 3.95 | trans_loss 5.165 | nll_loss 2.429 | w2v_ctc_loss 1.292 | task_loss 4.609 | contrastive_loss 0.297 | total 4003.4 | n_correct 2667 | ppl 5.39 | accuracy 66.618 | uer 17.641 | wer 19.533 | raw_wer 19.533 | bleu 21.84 | wps 1959.7 | wpb 4003.4 | bsz 141.8 | num_updates 32412 | best_bleu 22.26
2023-08-15 23:12:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32412 updates
2023-08-15 23:12:28 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-15 23:12:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-15 23:12:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt (epoch 22 @ 32412 updates, score 21.84) (writing took 15.792825907468796 seconds)
2023-08-15 23:12:44 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-08-15 23:12:44 | INFO | train | epoch 022 | loss 1.943 | trans_loss 4.826 | nll_loss 2.045 | w2v_ctc_loss 0.665 | task_loss 1.409 | contrastive_loss 0.126 | total 4137.48 | n_correct 2805.25 | ppl 4.13 | accuracy 67.801 | wps 10363.4 | ups 1.25 | wpb 8275 | bsz 305.2 | num_updates 32412 | lr 7.85529e-05 | gnorm 0.525 | clip 0 | loss_scale 8 | train_wall 1071 | gb_free 11.3 | wall 28984
2023-08-15 23:12:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 23:12:45 | INFO | fairseq.trainer | begin training epoch 23
2023-08-15 23:12:45 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 23:13:56 | INFO | train_inner | epoch 023:     88 / 1474 loss=1.93, trans_loss=4.808, nll_loss=2.021, w2v_ctc_loss=0.661, task_loss=1.432, contrastive_loss=0.081, total=4093.3, n_correct=2792.01, ppl=4.06, accuracy=68.209, wps=6729.4, ups=0.82, wpb=8186.6, bsz=301.3, num_updates=32500, lr=7.84465e-05, gnorm=0.52, clip=0, loss_scale=8, train_wall=73, gb_free=15.6, wall=29056
2023-08-15 23:15:10 | INFO | train_inner | epoch 023:    188 / 1474 loss=1.929, trans_loss=4.804, nll_loss=2.015, w2v_ctc_loss=0.659, task_loss=1.494, contrastive_loss=0.079, total=4116.26, n_correct=2807.76, ppl=4.04, accuracy=68.211, wps=11147, ups=1.35, wpb=8232.5, bsz=294.4, num_updates=32600, lr=7.8326e-05, gnorm=0.528, clip=0, loss_scale=8, train_wall=73, gb_free=16.7, wall=29129
2023-08-15 23:16:24 | INFO | train_inner | epoch 023:    288 / 1474 loss=1.939, trans_loss=4.819, nll_loss=2.035, w2v_ctc_loss=0.651, task_loss=1.413, contrastive_loss=0.158, total=4148.03, n_correct=2816.35, ppl=4.1, accuracy=67.896, wps=11206, ups=1.35, wpb=8296.1, bsz=305.7, num_updates=32700, lr=7.82062e-05, gnorm=0.525, clip=0, loss_scale=8, train_wall=74, gb_free=17, wall=29204
2023-08-15 23:17:37 | INFO | train_inner | epoch 023:    388 / 1474 loss=1.924, trans_loss=4.803, nll_loss=2.014, w2v_ctc_loss=0.651, task_loss=1.456, contrastive_loss=0.071, total=4115.99, n_correct=2810.88, ppl=4.04, accuracy=68.292, wps=11335.9, ups=1.38, wpb=8232, bsz=294.1, num_updates=32800, lr=7.80869e-05, gnorm=0.521, clip=0, loss_scale=8, train_wall=72, gb_free=15.6, wall=29276
2023-08-15 23:18:50 | INFO | train_inner | epoch 023:    488 / 1474 loss=1.935, trans_loss=4.816, nll_loss=2.031, w2v_ctc_loss=0.656, task_loss=1.377, contrastive_loss=0.128, total=4156.5, n_correct=2829.04, ppl=4.09, accuracy=68.063, wps=11350.7, ups=1.37, wpb=8313, bsz=312.2, num_updates=32900, lr=7.79681e-05, gnorm=0.524, clip=0, loss_scale=8, train_wall=73, gb_free=17, wall=29349
2023-08-15 23:20:03 | INFO | train_inner | epoch 023:    588 / 1474 loss=1.924, trans_loss=4.807, nll_loss=2.02, w2v_ctc_loss=0.653, task_loss=1.326, contrastive_loss=0.076, total=4174.84, n_correct=2849.8, ppl=4.05, accuracy=68.261, wps=11456.9, ups=1.37, wpb=8349.7, bsz=316.3, num_updates=33000, lr=7.78499e-05, gnorm=0.528, clip=0, loss_scale=8, train_wall=72, gb_free=16.9, wall=29422
2023-08-15 23:21:16 | INFO | train_inner | epoch 023:    688 / 1474 loss=1.938, trans_loss=4.818, nll_loss=2.034, w2v_ctc_loss=0.661, task_loss=1.411, contrastive_loss=0.118, total=4139.68, n_correct=2816.58, ppl=4.09, accuracy=68.039, wps=11388.4, ups=1.38, wpb=8279.4, bsz=302.2, num_updates=33100, lr=7.77322e-05, gnorm=0.527, clip=0, loss_scale=8, train_wall=72, gb_free=16.3, wall=29495
2023-08-15 23:22:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-15 23:22:29 | INFO | train_inner | epoch 023:    789 / 1474 loss=1.938, trans_loss=4.819, nll_loss=2.036, w2v_ctc_loss=0.664, task_loss=1.411, contrastive_loss=0.096, total=4157.1, n_correct=2821.08, ppl=4.1, accuracy=67.862, wps=11367.2, ups=1.37, wpb=8314.2, bsz=306.9, num_updates=33200, lr=7.76151e-05, gnorm=0.531, clip=0, loss_scale=4, train_wall=73, gb_free=16.5, wall=29568
2023-08-15 23:23:42 | INFO | train_inner | epoch 023:    889 / 1474 loss=1.936, trans_loss=4.812, nll_loss=2.027, w2v_ctc_loss=0.653, task_loss=1.281, contrastive_loss=0.177, total=4180.56, n_correct=2845.21, ppl=4.08, accuracy=68.058, wps=11472.2, ups=1.37, wpb=8361.1, bsz=324.5, num_updates=33300, lr=7.74984e-05, gnorm=0.531, clip=0, loss_scale=4, train_wall=72, gb_free=16, wall=29641
2023-08-15 23:23:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-15 23:24:56 | INFO | train_inner | epoch 023:    990 / 1474 loss=1.953, trans_loss=4.815, nll_loss=2.03, w2v_ctc_loss=0.652, task_loss=1.401, contrastive_loss=0.334, total=4164.33, n_correct=2830.02, ppl=4.08, accuracy=67.959, wps=11195.7, ups=1.34, wpb=8328.7, bsz=310.2, num_updates=33400, lr=7.73823e-05, gnorm=0.525, clip=0, loss_scale=2, train_wall=74, gb_free=13.1, wall=29715
2023-08-15 23:26:09 | INFO | train_inner | epoch 023:   1090 / 1474 loss=1.939, trans_loss=4.821, nll_loss=2.037, w2v_ctc_loss=0.667, task_loss=1.492, contrastive_loss=0.084, total=4093.39, n_correct=2781.16, ppl=4.1, accuracy=67.943, wps=11200.6, ups=1.37, wpb=8186.8, bsz=291.2, num_updates=33500, lr=7.72667e-05, gnorm=0.522, clip=0, loss_scale=2, train_wall=73, gb_free=15.9, wall=29789
2023-08-15 23:27:22 | INFO | train_inner | epoch 023:   1190 / 1474 loss=1.937, trans_loss=4.824, nll_loss=2.042, w2v_ctc_loss=0.667, task_loss=1.398, contrastive_loss=0.078, total=4158.33, n_correct=2822.93, ppl=4.12, accuracy=67.886, wps=11365.5, ups=1.37, wpb=8316.7, bsz=308.3, num_updates=33600, lr=7.71517e-05, gnorm=0.525, clip=0, loss_scale=2, train_wall=73, gb_free=16.3, wall=29862
2023-08-15 23:28:36 | INFO | train_inner | epoch 023:   1290 / 1474 loss=1.929, trans_loss=4.816, nll_loss=2.032, w2v_ctc_loss=0.654, task_loss=1.358, contrastive_loss=0.09, total=4142.25, n_correct=2817.83, ppl=4.09, accuracy=68.027, wps=11318.9, ups=1.37, wpb=8284.5, bsz=310.9, num_updates=33700, lr=7.70371e-05, gnorm=0.522, clip=0, loss_scale=2, train_wall=73, gb_free=16.1, wall=29935
2023-08-15 23:29:48 | INFO | train_inner | epoch 023:   1390 / 1474 loss=1.946, trans_loss=4.828, nll_loss=2.046, w2v_ctc_loss=0.659, task_loss=1.394, contrastive_loss=0.167, total=4164.41, n_correct=2824.54, ppl=4.13, accuracy=67.826, wps=11421.6, ups=1.37, wpb=8328.8, bsz=310.7, num_updates=33800, lr=7.69231e-05, gnorm=0.547, clip=0, loss_scale=2, train_wall=72, gb_free=17.1, wall=30008
2023-08-15 23:30:50 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 23:31:14 | INFO | dev_st | epoch 023 | valid on 'dev_st' subset | loss 3.97 | trans_loss 5.166 | nll_loss 2.432 | w2v_ctc_loss 1.354 | task_loss 4.687 | contrastive_loss 0.305 | total 4003.4 | n_correct 2672 | ppl 5.4 | accuracy 66.743 | uer 18.369 | wer 20.253 | raw_wer 20.253 | bleu 22.05 | wps 2031.2 | wpb 4003.4 | bsz 141.8 | num_updates 33884 | best_bleu 22.26
2023-08-15 23:31:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 33884 updates
2023-08-15 23:31:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.0502.pt
2023-08-15 23:31:18 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.0502.pt
2023-08-15 23:31:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.0502.pt (epoch 23 @ 33884 updates, score 22.05) (writing took 22.665151236578822 seconds)
2023-08-15 23:31:37 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-08-15 23:31:37 | INFO | train | epoch 023 | loss 1.937 | trans_loss 4.816 | nll_loss 2.031 | w2v_ctc_loss 0.658 | task_loss 1.405 | contrastive_loss 0.131 | total 4139.28 | n_correct 2815.38 | ppl 4.09 | accuracy 68.016 | wps 10754.3 | ups 1.3 | wpb 8278.6 | bsz 305.8 | num_updates 33884 | lr 7.68277e-05 | gnorm 0.528 | clip 0 | loss_scale 2 | train_wall 1070 | gb_free 13.1 | wall 30117
2023-08-15 23:31:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 23:31:38 | INFO | fairseq.trainer | begin training epoch 24
2023-08-15 23:31:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 23:31:58 | INFO | train_inner | epoch 024:     16 / 1474 loss=1.958, trans_loss=4.828, nll_loss=2.047, w2v_ctc_loss=0.658, task_loss=1.411, contrastive_loss=0.27, total=4094.92, n_correct=2768.14, ppl=4.13, accuracy=67.599, wps=6344.8, ups=0.77, wpb=8189.8, bsz=305.9, num_updates=33900, lr=7.68095e-05, gnorm=0.551, clip=0, loss_scale=2, train_wall=73, gb_free=15.3, wall=30137
2023-08-15 23:33:11 | INFO | train_inner | epoch 024:    116 / 1474 loss=1.928, trans_loss=4.799, nll_loss=2.009, w2v_ctc_loss=0.644, task_loss=1.319, contrastive_loss=0.179, total=4150.31, n_correct=2837.78, ppl=4.02, accuracy=68.375, wps=11369.1, ups=1.37, wpb=8300.6, bsz=319.2, num_updates=34000, lr=7.66965e-05, gnorm=0.586, clip=0, loss_scale=2, train_wall=73, gb_free=17.2, wall=30210
2023-08-15 23:33:11 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 23:33:35 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 3.971 | trans_loss 5.173 | nll_loss 2.434 | w2v_ctc_loss 1.347 | task_loss 4.66 | contrastive_loss 0.295 | total 4003.4 | n_correct 2664.9 | ppl 5.4 | accuracy 66.566 | uer 17.816 | wer 19.802 | raw_wer 19.802 | bleu 22.07 | wps 2168 | wpb 4003.4 | bsz 141.8 | num_updates 34000 | best_bleu 22.26
2023-08-15 23:33:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 34000 updates
2023-08-15 23:33:35 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_24_34000.pt
2023-08-15 23:33:38 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_24_34000.pt
2023-08-15 23:34:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_24_34000.pt (epoch 24 @ 34000 updates, score 22.07) (writing took 39.82358657754958 seconds)
2023-08-15 23:35:33 | INFO | train_inner | epoch 024:    216 / 1474 loss=1.933, trans_loss=4.799, nll_loss=2.009, w2v_ctc_loss=0.63, task_loss=1.237, contrastive_loss=0.293, total=4242.25, n_correct=2903.69, ppl=4.03, accuracy=68.447, wps=5966.6, ups=0.7, wpb=8484.5, bsz=338.9, num_updates=34100, lr=7.6584e-05, gnorm=0.535, clip=0, loss_scale=2, train_wall=73, gb_free=15.7, wall=30352
2023-08-15 23:36:46 | INFO | train_inner | epoch 024:    316 / 1474 loss=1.918, trans_loss=4.799, nll_loss=2.009, w2v_ctc_loss=0.645, task_loss=1.363, contrastive_loss=0.075, total=4140.3, n_correct=2835.4, ppl=4.03, accuracy=68.483, wps=11353.9, ups=1.37, wpb=8280.6, bsz=308.4, num_updates=34200, lr=7.64719e-05, gnorm=0.524, clip=0, loss_scale=2, train_wall=72, gb_free=15.5, wall=30425
2023-08-15 23:37:59 | INFO | train_inner | epoch 024:    416 / 1474 loss=1.946, trans_loss=4.802, nll_loss=2.013, w2v_ctc_loss=0.66, task_loss=1.493, contrastive_loss=0.22, total=4152.16, n_correct=2831.64, ppl=4.04, accuracy=68.197, wps=11367, ups=1.37, wpb=8304.3, bsz=297.4, num_updates=34300, lr=7.63604e-05, gnorm=0.534, clip=0, loss_scale=2, train_wall=73, gb_free=17, wall=30498
2023-08-15 23:39:12 | INFO | train_inner | epoch 024:    516 / 1474 loss=1.929, trans_loss=4.799, nll_loss=2.009, w2v_ctc_loss=0.65, task_loss=1.445, contrastive_loss=0.145, total=4141.4, n_correct=2830.02, ppl=4.02, accuracy=68.335, wps=11268.3, ups=1.36, wpb=8282.8, bsz=301.7, num_updates=34400, lr=7.62493e-05, gnorm=0.523, clip=0, loss_scale=2, train_wall=73, gb_free=16.3, wall=30572
2023-08-15 23:40:25 | INFO | train_inner | epoch 024:    616 / 1474 loss=1.922, trans_loss=4.801, nll_loss=2.013, w2v_ctc_loss=0.641, task_loss=1.4, contrastive_loss=0.106, total=4167.62, n_correct=2845.77, ppl=4.04, accuracy=68.283, wps=11407.8, ups=1.37, wpb=8335.2, bsz=309.3, num_updates=34500, lr=7.61387e-05, gnorm=0.525, clip=0, loss_scale=2, train_wall=73, gb_free=14.4, wall=30645
2023-08-15 23:41:38 | INFO | train_inner | epoch 024:    716 / 1474 loss=1.931, trans_loss=4.809, nll_loss=2.021, w2v_ctc_loss=0.651, task_loss=1.446, contrastive_loss=0.119, total=4097.36, n_correct=2794.17, ppl=4.06, accuracy=68.194, wps=11274, ups=1.38, wpb=8194.7, bsz=295.5, num_updates=34600, lr=7.60286e-05, gnorm=0.522, clip=0, loss_scale=2, train_wall=72, gb_free=15.8, wall=30717
2023-08-15 23:42:51 | INFO | train_inner | epoch 024:    816 / 1474 loss=1.926, trans_loss=4.812, nll_loss=2.026, w2v_ctc_loss=0.648, task_loss=1.413, contrastive_loss=0.094, total=4125.86, n_correct=2814.43, ppl=4.07, accuracy=68.214, wps=11311.5, ups=1.37, wpb=8251.7, bsz=306.7, num_updates=34700, lr=7.5919e-05, gnorm=0.554, clip=0, loss_scale=2, train_wall=72, gb_free=14.3, wall=30790
2023-08-15 23:44:04 | INFO | train_inner | epoch 024:    916 / 1474 loss=1.933, trans_loss=4.809, nll_loss=2.021, w2v_ctc_loss=0.662, task_loss=1.583, contrastive_loss=0.07, total=4034.23, n_correct=2744.96, ppl=4.06, accuracy=68.042, wps=11129.8, ups=1.38, wpb=8068.5, bsz=278.2, num_updates=34800, lr=7.58098e-05, gnorm=0.533, clip=0, loss_scale=2, train_wall=72, gb_free=14.5, wall=30863
2023-08-15 23:45:17 | INFO | train_inner | epoch 024:   1016 / 1474 loss=1.924, trans_loss=4.81, nll_loss=2.023, w2v_ctc_loss=0.646, task_loss=1.463, contrastive_loss=0.074, total=4120.27, n_correct=2811.39, ppl=4.06, accuracy=68.233, wps=11238.7, ups=1.36, wpb=8240.5, bsz=296.9, num_updates=34900, lr=7.57011e-05, gnorm=0.521, clip=0, loss_scale=2, train_wall=73, gb_free=15.6, wall=30936
2023-08-15 23:46:30 | INFO | train_inner | epoch 024:   1116 / 1474 loss=1.924, trans_loss=4.795, nll_loss=2.005, w2v_ctc_loss=0.652, task_loss=1.338, contrastive_loss=0.116, total=4139.93, n_correct=2832.37, ppl=4.01, accuracy=68.416, wps=11391.3, ups=1.38, wpb=8279.9, bsz=311.2, num_updates=35000, lr=7.55929e-05, gnorm=0.529, clip=0, loss_scale=2, train_wall=72, gb_free=17.5, wall=31009
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:0')
2023-08-15 23:47:43 | INFO | train_inner | epoch 024:   1216 / 1474 loss=1.928, trans_loss=4.811, nll_loss=2.024, w2v_ctc_loss=0.648, task_loss=1.403, contrastive_loss=0.104, total=4151.3, n_correct=2827.02, ppl=4.07, accuracy=68.1, wps=11284.4, ups=1.36, wpb=8302.6, bsz=309.6, num_updates=35100, lr=7.54851e-05, gnorm=0.526, clip=0, loss_scale=2, train_wall=73, gb_free=16.3, wall=31082
2023-08-15 23:48:56 | INFO | train_inner | epoch 024:   1316 / 1474 loss=1.935, trans_loss=4.813, nll_loss=2.028, w2v_ctc_loss=0.666, task_loss=1.484, contrastive_loss=0.08, total=4114.24, n_correct=2802.24, ppl=4.08, accuracy=68.111, wps=11264.2, ups=1.37, wpb=8228.5, bsz=296, num_updates=35200, lr=7.53778e-05, gnorm=0.525, clip=0, loss_scale=2, train_wall=72, gb_free=16.1, wall=31155
2023-08-15 23:50:09 | INFO | train_inner | epoch 024:   1416 / 1474 loss=1.934, trans_loss=4.816, nll_loss=2.031, w2v_ctc_loss=0.663, task_loss=1.477, contrastive_loss=0.075, total=4089.93, n_correct=2783.18, ppl=4.09, accuracy=68.05, wps=11224.6, ups=1.37, wpb=8179.9, bsz=291.2, num_updates=35300, lr=7.5271e-05, gnorm=0.527, clip=0, loss_scale=2, train_wall=72, gb_free=16.3, wall=31228
2023-08-15 23:50:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:2')
2023-08-15 23:51:15 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 3.95 | trans_loss 5.163 | nll_loss 2.428 | w2v_ctc_loss 1.298 | task_loss 4.65 | contrastive_loss 0.299 | total 4003.4 | n_correct 2669.2 | ppl 5.38 | accuracy 66.673 | uer 17.862 | wer 19.809 | raw_wer 19.809 | bleu 22.21 | wps 2073 | wpb 4003.4 | bsz 141.8 | num_updates 35358 | best_bleu 22.26
2023-08-15 23:51:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 35358 updates
2023-08-15 23:51:15 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.2107.pt
2023-08-15 23:51:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.2107.pt
2023-08-15 23:51:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.2107.pt (epoch 24 @ 35358 updates, score 22.21) (writing took 20.16241681948304 seconds)
2023-08-15 23:51:36 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-08-15 23:51:36 | INFO | train | epoch 024 | loss 1.929 | trans_loss 4.805 | nll_loss 2.017 | w2v_ctc_loss 0.65 | task_loss 1.407 | contrastive_loss 0.13 | total 4138.65 | n_correct 2824.55 | ppl 4.05 | accuracy 68.248 | wps 10179.3 | ups 1.23 | wpb 8277.3 | bsz 305.7 | num_updates 35358 | lr 7.52092e-05 | gnorm 0.532 | clip 0 | loss_scale 2 | train_wall 1069 | gb_free 15.7 | wall 31315
2023-08-15 23:51:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-15 23:51:36 | INFO | fairseq.trainer | begin training epoch 25
2023-08-15 23:51:36 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-15 23:52:14 | INFO | train_inner | epoch 025:     42 / 1474 loss=1.919, trans_loss=4.797, nll_loss=2.007, w2v_ctc_loss=0.649, task_loss=1.371, contrastive_loss=0.084, total=4161.84, n_correct=2852.08, ppl=4.02, accuracy=68.529, wps=6641.7, ups=0.8, wpb=8323.7, bsz=309.3, num_updates=35400, lr=7.51646e-05, gnorm=0.52, clip=0, loss_scale=4, train_wall=72, gb_free=15.4, wall=31354
2023-08-15 23:53:28 | INFO | train_inner | epoch 025:    142 / 1474 loss=1.907, trans_loss=4.783, nll_loss=1.988, w2v_ctc_loss=0.632, task_loss=1.347, contrastive_loss=0.081, total=4141.46, n_correct=2849.56, ppl=3.97, accuracy=68.806, wps=11324.2, ups=1.37, wpb=8282.9, bsz=311.5, num_updates=35500, lr=7.50587e-05, gnorm=0.529, clip=0, loss_scale=4, train_wall=73, gb_free=16.9, wall=31427
2023-08-15 23:54:41 | INFO | train_inner | epoch 025:    242 / 1474 loss=1.918, trans_loss=4.791, nll_loss=1.998, w2v_ctc_loss=0.645, task_loss=1.442, contrastive_loss=0.085, total=4122.7, n_correct=2822.24, ppl=3.99, accuracy=68.456, wps=11211.5, ups=1.36, wpb=8245.4, bsz=302.9, num_updates=35600, lr=7.49532e-05, gnorm=0.523, clip=0, loss_scale=4, train_wall=73, gb_free=16.4, wall=31500
2023-08-15 23:55:55 | INFO | train_inner | epoch 025:    342 / 1474 loss=1.921, trans_loss=4.79, nll_loss=1.996, w2v_ctc_loss=0.642, task_loss=1.494, contrastive_loss=0.114, total=4132.6, n_correct=2833.28, ppl=3.99, accuracy=68.559, wps=11221.4, ups=1.36, wpb=8265.2, bsz=294.3, num_updates=35700, lr=7.48481e-05, gnorm=0.52, clip=0, loss_scale=4, train_wall=73, gb_free=16.6, wall=31574
2023-08-15 23:57:08 | INFO | train_inner | epoch 025:    442 / 1474 loss=1.937, trans_loss=4.794, nll_loss=2.002, w2v_ctc_loss=0.654, task_loss=1.486, contrastive_loss=0.196, total=4169.31, n_correct=2851.94, ppl=4, accuracy=68.403, wps=11368.2, ups=1.36, wpb=8338.6, bsz=296.4, num_updates=35800, lr=7.47435e-05, gnorm=0.529, clip=0, loss_scale=4, train_wall=73, gb_free=14.9, wall=31647
2023-08-15 23:58:21 | INFO | train_inner | epoch 025:    542 / 1474 loss=1.917, trans_loss=4.799, nll_loss=2.009, w2v_ctc_loss=0.641, task_loss=1.363, contrastive_loss=0.084, total=4157.91, n_correct=2847.45, ppl=4.02, accuracy=68.483, wps=11445, ups=1.38, wpb=8315.8, bsz=314.5, num_updates=35900, lr=7.46393e-05, gnorm=0.531, clip=0, loss_scale=4, train_wall=72, gb_free=15.1, wall=31720
2023-08-15 23:59:34 | INFO | train_inner | epoch 025:    642 / 1474 loss=1.925, trans_loss=4.79, nll_loss=1.998, w2v_ctc_loss=0.648, task_loss=1.388, contrastive_loss=0.153, total=4158.52, n_correct=2851.11, ppl=3.99, accuracy=68.561, wps=11396.4, ups=1.37, wpb=8317, bsz=309.9, num_updates=36000, lr=7.45356e-05, gnorm=0.531, clip=0, loss_scale=4, train_wall=73, gb_free=16, wall=31793
2023-08-15 23:59:34 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-15 23:59:58 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 3.956 | trans_loss 5.164 | nll_loss 2.426 | w2v_ctc_loss 1.317 | task_loss 4.633 | contrastive_loss 0.297 | total 4003.4 | n_correct 2670.2 | ppl 5.38 | accuracy 66.698 | uer 17.453 | wer 19.451 | raw_wer 19.451 | bleu 22.32 | wps 2134 | wpb 4003.4 | bsz 141.8 | num_updates 36000 | best_bleu 22.32
2023-08-15 23:59:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36000 updates
2023-08-15 23:59:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_25_36000.pt
2023-08-16 00:00:01 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_25_36000.pt
2023-08-16 00:00:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_25_36000.pt (epoch 25 @ 36000 updates, score 22.32) (writing took 56.1994785964489 seconds)
2023-08-16 00:02:09 | INFO | train_inner | epoch 025:    742 / 1474 loss=1.925, trans_loss=4.79, nll_loss=1.997, w2v_ctc_loss=0.644, task_loss=1.41, contrastive_loss=0.148, total=4138.4, n_correct=2836.85, ppl=3.99, accuracy=68.549, wps=5334.9, ups=0.64, wpb=8276.8, bsz=304.5, num_updates=36100, lr=7.44323e-05, gnorm=0.529, clip=0, loss_scale=4, train_wall=73, gb_free=16, wall=31948
2023-08-16 00:03:22 | INFO | train_inner | epoch 025:    842 / 1474 loss=1.915, trans_loss=4.795, nll_loss=2.004, w2v_ctc_loss=0.641, task_loss=1.306, contrastive_loss=0.094, total=4171.94, n_correct=2860.4, ppl=4.01, accuracy=68.563, wps=11449.9, ups=1.37, wpb=8343.9, bsz=324, num_updates=36200, lr=7.43294e-05, gnorm=0.526, clip=0, loss_scale=4, train_wall=72, gb_free=15.2, wall=32021
2023-08-16 00:04:35 | INFO | train_inner | epoch 025:    942 / 1474 loss=1.925, trans_loss=4.797, nll_loss=2.007, w2v_ctc_loss=0.648, task_loss=1.354, contrastive_loss=0.15, total=4148.78, n_correct=2841, ppl=4.02, accuracy=68.478, wps=11313.6, ups=1.36, wpb=8297.6, bsz=314.3, num_updates=36300, lr=7.4227e-05, gnorm=0.529, clip=0, loss_scale=4, train_wall=73, gb_free=12, wall=32094
2023-08-16 00:05:49 | INFO | train_inner | epoch 025:   1042 / 1474 loss=1.937, trans_loss=4.804, nll_loss=2.017, w2v_ctc_loss=0.64, task_loss=1.393, contrastive_loss=0.262, total=4176.78, n_correct=2846.56, ppl=4.05, accuracy=68.152, wps=11334.1, ups=1.36, wpb=8353.6, bsz=310.2, num_updates=36400, lr=7.41249e-05, gnorm=0.521, clip=0, loss_scale=4, train_wall=73, gb_free=16.3, wall=32168
2023-08-16 00:07:01 | INFO | train_inner | epoch 025:   1142 / 1474 loss=1.916, trans_loss=4.796, nll_loss=2.005, w2v_ctc_loss=0.641, task_loss=1.504, contrastive_loss=0.069, total=4046.83, n_correct=2768.82, ppl=4.01, accuracy=68.419, wps=11179.5, ups=1.38, wpb=8093.7, bsz=287, num_updates=36500, lr=7.40233e-05, gnorm=0.536, clip=0, loss_scale=4, train_wall=72, gb_free=14.7, wall=32241
2023-08-16 00:08:14 | INFO | train_inner | epoch 025:   1242 / 1474 loss=1.92, trans_loss=4.804, nll_loss=2.016, w2v_ctc_loss=0.641, task_loss=1.432, contrastive_loss=0.077, total=4088.37, n_correct=2791.35, ppl=4.04, accuracy=68.275, wps=11292.7, ups=1.38, wpb=8176.7, bsz=295.1, num_updates=36600, lr=7.39221e-05, gnorm=0.532, clip=0, loss_scale=4, train_wall=72, gb_free=15.8, wall=32313
2023-08-16 00:09:27 | INFO | train_inner | epoch 025:   1342 / 1474 loss=1.931, trans_loss=4.8, nll_loss=2.01, w2v_ctc_loss=0.65, task_loss=1.387, contrastive_loss=0.173, total=4161.94, n_correct=2845.56, ppl=4.03, accuracy=68.371, wps=11352.8, ups=1.36, wpb=8323.9, bsz=307.5, num_updates=36700, lr=7.38213e-05, gnorm=0.53, clip=0, loss_scale=4, train_wall=73, gb_free=12.3, wall=32386
2023-08-16 00:10:41 | INFO | train_inner | epoch 025:   1442 / 1474 loss=1.933, trans_loss=4.811, nll_loss=2.024, w2v_ctc_loss=0.651, task_loss=1.404, contrastive_loss=0.137, total=4121.76, n_correct=2806.68, ppl=4.07, accuracy=68.094, wps=11180.9, ups=1.36, wpb=8243.5, bsz=308.1, num_updates=36800, lr=7.3721e-05, gnorm=0.537, clip=0, loss_scale=4, train_wall=73, gb_free=16.5, wall=32460
2023-08-16 00:11:04 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 00:11:28 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 3.951 | trans_loss 5.16 | nll_loss 2.42 | w2v_ctc_loss 1.312 | task_loss 4.617 | contrastive_loss 0.294 | total 4003.4 | n_correct 2673.6 | ppl 5.35 | accuracy 66.783 | uer 17.28 | wer 19.302 | raw_wer 19.302 | bleu 22.37 | wps 2132.1 | wpb 4003.4 | bsz 141.8 | num_updates 36832 | best_bleu 22.37
2023-08-16 00:11:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36832 updates
2023-08-16 00:11:28 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-16 00:11:43 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-16 00:11:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 25 @ 36832 updates, score 22.37) (writing took 29.644906083121896 seconds)
2023-08-16 00:11:58 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-08-16 00:11:58 | INFO | train | epoch 025 | loss 1.923 | trans_loss 4.796 | nll_loss 2.005 | w2v_ctc_loss 0.644 | task_loss 1.406 | contrastive_loss 0.128 | total 4138.65 | n_correct 2832.69 | ppl 4.01 | accuracy 68.445 | wps 9983.3 | ups 1.21 | wpb 8277.3 | bsz 305.7 | num_updates 36832 | lr 7.36889e-05 | gnorm 0.529 | clip 0 | loss_scale 4 | train_wall 1071 | gb_free 13.9 | wall 32537
2023-08-16 00:11:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 00:11:58 | INFO | fairseq.trainer | begin training epoch 26
2023-08-16 00:11:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 00:12:57 | INFO | train_inner | epoch 026:     68 / 1474 loss=1.906, trans_loss=4.777, nll_loss=1.981, w2v_ctc_loss=0.629, task_loss=1.334, contrastive_loss=0.101, total=4167.28, n_correct=2867.37, ppl=3.95, accuracy=68.807, wps=6130.3, ups=0.74, wpb=8334.6, bsz=315.4, num_updates=36900, lr=7.3621e-05, gnorm=0.521, clip=0, loss_scale=4, train_wall=73, gb_free=16.2, wall=32596
2023-08-16 00:14:10 | INFO | train_inner | epoch 026:    168 / 1474 loss=1.919, trans_loss=4.779, nll_loss=1.984, w2v_ctc_loss=0.62, task_loss=1.244, contrastive_loss=0.279, total=4268.44, n_correct=2945.36, ppl=3.96, accuracy=69.003, wps=11597.1, ups=1.36, wpb=8536.9, bsz=338.3, num_updates=37000, lr=7.35215e-05, gnorm=0.517, clip=0, loss_scale=4, train_wall=73, gb_free=15.8, wall=32670
2023-08-16 00:15:24 | INFO | train_inner | epoch 026:    268 / 1474 loss=1.92, trans_loss=4.78, nll_loss=1.985, w2v_ctc_loss=0.641, task_loss=1.387, contrastive_loss=0.167, total=4127.46, n_correct=2833.94, ppl=3.96, accuracy=68.661, wps=11264.9, ups=1.36, wpb=8254.9, bsz=308, num_updates=37100, lr=7.34223e-05, gnorm=0.531, clip=0, loss_scale=4, train_wall=73, gb_free=15.7, wall=32743
2023-08-16 00:16:37 | INFO | train_inner | epoch 026:    368 / 1474 loss=1.916, trans_loss=4.784, nll_loss=1.989, w2v_ctc_loss=0.642, task_loss=1.351, contrastive_loss=0.123, total=4159.09, n_correct=2858.14, ppl=3.97, accuracy=68.72, wps=11379.7, ups=1.37, wpb=8318.2, bsz=313.5, num_updates=37200, lr=7.33236e-05, gnorm=0.527, clip=0, loss_scale=4, train_wall=73, gb_free=15.7, wall=32816
2023-08-16 00:17:50 | INFO | train_inner | epoch 026:    468 / 1474 loss=1.913, trans_loss=4.773, nll_loss=1.975, w2v_ctc_loss=0.634, task_loss=1.352, contrastive_loss=0.171, total=4167.54, n_correct=2871.75, ppl=3.93, accuracy=68.908, wps=11427.2, ups=1.37, wpb=8335.1, bsz=314.7, num_updates=37300, lr=7.32252e-05, gnorm=0.522, clip=0, loss_scale=4, train_wall=72, gb_free=17.4, wall=32889
2023-08-16 00:19:03 | INFO | train_inner | epoch 026:    568 / 1474 loss=1.919, trans_loss=4.787, nll_loss=1.994, w2v_ctc_loss=0.652, task_loss=1.413, contrastive_loss=0.09, total=4158.38, n_correct=2850.14, ppl=3.98, accuracy=68.54, wps=11326.1, ups=1.36, wpb=8316.8, bsz=303.9, num_updates=37400, lr=7.31272e-05, gnorm=0.522, clip=0, loss_scale=4, train_wall=73, gb_free=16, wall=32962
2023-08-16 00:20:16 | INFO | train_inner | epoch 026:    668 / 1474 loss=1.911, trans_loss=4.785, nll_loss=1.991, w2v_ctc_loss=0.636, task_loss=1.442, contrastive_loss=0.075, total=4132.34, n_correct=2835.55, ppl=3.97, accuracy=68.619, wps=11327.6, ups=1.37, wpb=8264.7, bsz=297.7, num_updates=37500, lr=7.30297e-05, gnorm=0.53, clip=0, loss_scale=8, train_wall=72, gb_free=16, wall=33035
2023-08-16 00:21:29 | INFO | train_inner | epoch 026:    768 / 1474 loss=1.928, trans_loss=4.792, nll_loss=2, w2v_ctc_loss=0.64, task_loss=1.419, contrastive_loss=0.186, total=4092.35, n_correct=2801.3, ppl=4, accuracy=68.452, wps=11220.3, ups=1.37, wpb=8184.7, bsz=300.1, num_updates=37600, lr=7.29325e-05, gnorm=0.54, clip=0, loss_scale=8, train_wall=72, gb_free=16, wall=33108
2023-08-16 00:22:42 | INFO | train_inner | epoch 026:    868 / 1474 loss=1.919, trans_loss=4.787, nll_loss=1.993, w2v_ctc_loss=0.649, task_loss=1.411, contrastive_loss=0.089, total=4176.13, n_correct=2860.97, ppl=3.98, accuracy=68.508, wps=11419.2, ups=1.37, wpb=8352.3, bsz=305.9, num_updates=37700, lr=7.28357e-05, gnorm=0.523, clip=0, loss_scale=8, train_wall=73, gb_free=16.5, wall=33181
2023-08-16 00:23:55 | INFO | train_inner | epoch 026:    968 / 1474 loss=1.919, trans_loss=4.791, nll_loss=1.998, w2v_ctc_loss=0.633, task_loss=1.45, contrastive_loss=0.141, total=4142.72, n_correct=2836.33, ppl=4, accuracy=68.465, wps=11342.7, ups=1.37, wpb=8285.4, bsz=300, num_updates=37800, lr=7.27393e-05, gnorm=0.528, clip=0, loss_scale=8, train_wall=72, gb_free=17.3, wall=33254
2023-08-16 00:25:09 | INFO | train_inner | epoch 026:   1068 / 1474 loss=1.916, trans_loss=4.788, nll_loss=1.995, w2v_ctc_loss=0.646, task_loss=1.475, contrastive_loss=0.075, total=4116.69, n_correct=2824.48, ppl=3.99, accuracy=68.61, wps=11214.6, ups=1.36, wpb=8233.4, bsz=294.1, num_updates=37900, lr=7.26433e-05, gnorm=0.524, clip=0, loss_scale=8, train_wall=73, gb_free=15.9, wall=33328
2023-08-16 00:26:22 | INFO | train_inner | epoch 026:   1168 / 1474 loss=1.923, trans_loss=4.797, nll_loss=2.007, w2v_ctc_loss=0.646, task_loss=1.469, contrastive_loss=0.114, total=4112.79, n_correct=2812.82, ppl=4.02, accuracy=68.392, wps=11256, ups=1.37, wpb=8225.6, bsz=299, num_updates=38000, lr=7.25476e-05, gnorm=0.526, clip=0, loss_scale=8, train_wall=73, gb_free=16.5, wall=33401
2023-08-16 00:26:22 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 00:26:45 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 3.965 | trans_loss 5.168 | nll_loss 2.433 | w2v_ctc_loss 1.34 | task_loss 4.649 | contrastive_loss 0.294 | total 4003.4 | n_correct 2667 | ppl 5.4 | accuracy 66.618 | uer 17.96 | wer 19.779 | raw_wer 19.779 | bleu 22.05 | wps 2138.3 | wpb 4003.4 | bsz 141.8 | num_updates 38000 | best_bleu 22.37
2023-08-16 00:26:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38000 updates
2023-08-16 00:26:45 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_26_38000.pt
2023-08-16 00:26:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_26_38000.pt
2023-08-16 00:27:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_26_38000.pt (epoch 26 @ 38000 updates, score 22.05) (writing took 16.070171924307942 seconds)
2023-08-16 00:28:15 | INFO | train_inner | epoch 026:   1268 / 1474 loss=1.927, trans_loss=4.805, nll_loss=2.016, w2v_ctc_loss=0.656, task_loss=1.56, contrastive_loss=0.077, total=4005.82, n_correct=2735.9, ppl=4.04, accuracy=68.298, wps=7082.4, ups=0.88, wpb=8011.6, bsz=280.5, num_updates=38100, lr=7.24524e-05, gnorm=0.541, clip=0, loss_scale=8, train_wall=72, gb_free=16.5, wall=33514
2023-08-16 00:29:29 | INFO | train_inner | epoch 026:   1368 / 1474 loss=1.912, trans_loss=4.794, nll_loss=2.003, w2v_ctc_loss=0.632, task_loss=1.401, contrastive_loss=0.089, total=4158.84, n_correct=2852.66, ppl=4.01, accuracy=68.593, wps=11259.1, ups=1.35, wpb=8317.7, bsz=311.2, num_updates=38200, lr=7.23575e-05, gnorm=0.526, clip=0, loss_scale=8, train_wall=73, gb_free=13.2, wall=33588
2023-08-16 00:30:41 | INFO | train_inner | epoch 026:   1468 / 1474 loss=1.906, trans_loss=4.786, nll_loss=1.993, w2v_ctc_loss=0.628, task_loss=1.341, contrastive_loss=0.082, total=4151.66, n_correct=2853.96, ppl=3.98, accuracy=68.743, wps=11400, ups=1.37, wpb=8303.3, bsz=315.5, num_updates=38300, lr=7.22629e-05, gnorm=0.525, clip=0, loss_scale=8, train_wall=72, gb_free=17.2, wall=33661
2023-08-16 00:30:46 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 00:31:11 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 3.948 | trans_loss 5.169 | nll_loss 2.434 | w2v_ctc_loss 1.28 | task_loss 4.642 | contrastive_loss 0.299 | total 4003.4 | n_correct 2668 | ppl 5.4 | accuracy 66.643 | uer 17.663 | wer 19.537 | raw_wer 19.537 | bleu 22.07 | wps 1977.8 | wpb 4003.4 | bsz 141.8 | num_updates 38306 | best_bleu 22.37
2023-08-16 00:31:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38306 updates
2023-08-16 00:31:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-16 00:31:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-16 00:31:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt (epoch 26 @ 38306 updates, score 22.07) (writing took 15.259364491328597 seconds)
2023-08-16 00:31:26 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-08-16 00:31:26 | INFO | train | epoch 026 | loss 1.917 | trans_loss 4.787 | nll_loss 1.993 | w2v_ctc_loss 0.639 | task_loss 1.406 | contrastive_loss 0.126 | total 4138.65 | n_correct 2840.35 | ppl 3.98 | accuracy 68.63 | wps 10447.4 | ups 1.26 | wpb 8277.3 | bsz 305.7 | num_updates 38306 | lr 7.22573e-05 | gnorm 0.527 | clip 0 | loss_scale 8 | train_wall 1071 | gb_free 15.6 | wall 33705
2023-08-16 00:31:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 00:31:26 | INFO | fairseq.trainer | begin training epoch 27
2023-08-16 00:31:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 00:32:42 | INFO | train_inner | epoch 027:     94 / 1474 loss=1.898, trans_loss=4.757, nll_loss=1.954, w2v_ctc_loss=0.626, task_loss=1.499, contrastive_loss=0.065, total=4072.63, n_correct=2820.23, ppl=3.88, accuracy=69.248, wps=6750.4, ups=0.83, wpb=8145.3, bsz=284.2, num_updates=38400, lr=7.21688e-05, gnorm=0.531, clip=0, loss_scale=8, train_wall=72, gb_free=15.9, wall=33781
2023-08-16 00:33:56 | INFO | train_inner | epoch 027:    194 / 1474 loss=1.901, trans_loss=4.768, nll_loss=1.968, w2v_ctc_loss=0.63, task_loss=1.35, contrastive_loss=0.092, total=4179.66, n_correct=2885.27, ppl=3.91, accuracy=69.031, wps=11357.3, ups=1.36, wpb=8359.3, bsz=321.7, num_updates=38500, lr=7.2075e-05, gnorm=0.518, clip=0, loss_scale=8, train_wall=73, gb_free=16.2, wall=33855
2023-08-16 00:35:09 | INFO | train_inner | epoch 027:    294 / 1474 loss=1.907, trans_loss=4.776, nll_loss=1.978, w2v_ctc_loss=0.636, task_loss=1.4, contrastive_loss=0.076, total=4173.27, n_correct=2877.57, ppl=3.94, accuracy=68.952, wps=11333.9, ups=1.36, wpb=8346.5, bsz=307, num_updates=38600, lr=7.19816e-05, gnorm=0.53, clip=0, loss_scale=8, train_wall=73, gb_free=16.6, wall=33929
2023-08-16 00:36:24 | INFO | train_inner | epoch 027:    394 / 1474 loss=1.927, trans_loss=4.779, nll_loss=1.983, w2v_ctc_loss=0.634, task_loss=1.467, contrastive_loss=0.259, total=4078.73, n_correct=2806.07, ppl=3.95, accuracy=68.798, wps=10932.7, ups=1.34, wpb=8157.5, bsz=297.5, num_updates=38700, lr=7.18885e-05, gnorm=0.526, clip=0, loss_scale=8, train_wall=74, gb_free=16, wall=34003
2023-08-16 00:37:38 | INFO | train_inner | epoch 027:    494 / 1474 loss=1.918, trans_loss=4.786, nll_loss=1.993, w2v_ctc_loss=0.629, task_loss=1.285, contrastive_loss=0.195, total=4245.37, n_correct=2917.44, ppl=3.98, accuracy=68.721, wps=11503.1, ups=1.35, wpb=8490.7, bsz=331.5, num_updates=38800, lr=7.17958e-05, gnorm=0.523, clip=0, loss_scale=8, train_wall=73, gb_free=16.5, wall=34077
2023-08-16 00:38:51 | INFO | train_inner | epoch 027:    594 / 1474 loss=1.916, trans_loss=4.777, nll_loss=1.981, w2v_ctc_loss=0.643, task_loss=1.379, contrastive_loss=0.134, total=4134.93, n_correct=2845.69, ppl=3.95, accuracy=68.821, wps=11235.6, ups=1.36, wpb=8269.9, bsz=312, num_updates=38900, lr=7.17035e-05, gnorm=0.618, clip=0, loss_scale=8, train_wall=73, gb_free=17.3, wall=34151
2023-08-16 00:40:05 | INFO | train_inner | epoch 027:    694 / 1474 loss=1.915, trans_loss=4.782, nll_loss=1.986, w2v_ctc_loss=0.639, task_loss=1.403, contrastive_loss=0.113, total=4162.17, n_correct=2861.98, ppl=3.96, accuracy=68.762, wps=11255.3, ups=1.35, wpb=8324.3, bsz=305.1, num_updates=39000, lr=7.16115e-05, gnorm=0.525, clip=0, loss_scale=8, train_wall=73, gb_free=15.1, wall=34225
2023-08-16 00:41:18 | INFO | train_inner | epoch 027:    794 / 1474 loss=1.906, trans_loss=4.775, nll_loss=1.977, w2v_ctc_loss=0.634, task_loss=1.474, contrastive_loss=0.075, total=4107.17, n_correct=2829.35, ppl=3.94, accuracy=68.888, wps=11261.8, ups=1.37, wpb=8214.3, bsz=294.3, num_updates=39100, lr=7.15199e-05, gnorm=0.528, clip=0, loss_scale=8, train_wall=72, gb_free=11.1, wall=34298
2023-08-16 00:42:31 | INFO | train_inner | epoch 027:    894 / 1474 loss=1.902, trans_loss=4.781, nll_loss=1.985, w2v_ctc_loss=0.624, task_loss=1.458, contrastive_loss=0.068, total=4101.4, n_correct=2825.95, ppl=3.96, accuracy=68.902, wps=11251.5, ups=1.37, wpb=8202.8, bsz=292.8, num_updates=39200, lr=7.14286e-05, gnorm=0.523, clip=0, loss_scale=8, train_wall=72, gb_free=16, wall=34371
2023-08-16 00:43:45 | INFO | train_inner | epoch 027:    994 / 1474 loss=1.925, trans_loss=4.782, nll_loss=1.988, w2v_ctc_loss=0.636, task_loss=1.365, contrastive_loss=0.255, total=4195.5, n_correct=2881.04, ppl=3.97, accuracy=68.67, wps=11393.6, ups=1.36, wpb=8391, bsz=315.9, num_updates=39300, lr=7.13376e-05, gnorm=0.522, clip=0, loss_scale=8, train_wall=73, gb_free=16.5, wall=34444
2023-08-16 00:44:58 | INFO | train_inner | epoch 027:   1094 / 1474 loss=1.906, trans_loss=4.777, nll_loss=1.981, w2v_ctc_loss=0.633, task_loss=1.415, contrastive_loss=0.085, total=4147.99, n_correct=2855.71, ppl=3.95, accuracy=68.846, wps=11363.7, ups=1.37, wpb=8296, bsz=304.7, num_updates=39400, lr=7.1247e-05, gnorm=0.527, clip=0, loss_scale=8, train_wall=72, gb_free=15.8, wall=34517
2023-08-16 00:46:11 | INFO | train_inner | epoch 027:   1194 / 1474 loss=1.913, trans_loss=4.784, nll_loss=1.989, w2v_ctc_loss=0.639, task_loss=1.472, contrastive_loss=0.089, total=4104.84, n_correct=2820.06, ppl=3.97, accuracy=68.701, wps=11187.5, ups=1.36, wpb=8209.7, bsz=297.2, num_updates=39500, lr=7.11568e-05, gnorm=0.536, clip=0, loss_scale=16, train_wall=73, gb_free=11.8, wall=34591
2023-08-16 00:47:24 | INFO | train_inner | epoch 027:   1294 / 1474 loss=1.918, trans_loss=4.784, nll_loss=1.989, w2v_ctc_loss=0.638, task_loss=1.487, contrastive_loss=0.141, total=4062.86, n_correct=2793.33, ppl=3.97, accuracy=68.753, wps=11185.6, ups=1.38, wpb=8125.7, bsz=293.6, num_updates=39600, lr=7.10669e-05, gnorm=0.551, clip=0, loss_scale=16, train_wall=72, gb_free=16.1, wall=34663
2023-08-16 00:48:36 | INFO | train_inner | epoch 027:   1394 / 1474 loss=1.915, trans_loss=4.786, nll_loss=1.993, w2v_ctc_loss=0.636, task_loss=1.32, contrastive_loss=0.126, total=4157.6, n_correct=2854.04, ppl=3.98, accuracy=68.646, wps=11495.7, ups=1.38, wpb=8315.2, bsz=314, num_updates=39700, lr=7.09773e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=72, gb_free=17.3, wall=34736
2023-08-16 00:49:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 00:49:58 | INFO | dev_st | epoch 027 | valid on 'dev_st' subset | loss 3.945 | trans_loss 5.16 | nll_loss 2.421 | w2v_ctc_loss 1.288 | task_loss 4.616 | contrastive_loss 0.305 | total 4003.4 | n_correct 2681 | ppl 5.36 | accuracy 66.968 | uer 17.442 | wer 19.406 | raw_wer 19.406 | bleu 22.38 | wps 2220.4 | wpb 4003.4 | bsz 141.8 | num_updates 39780 | best_bleu 22.38
2023-08-16 00:49:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 39780 updates
2023-08-16 00:49:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-16 00:50:16 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-16 00:50:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 27 @ 39780 updates, score 22.38) (writing took 33.066271329298615 seconds)
2023-08-16 00:50:32 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-08-16 00:50:32 | INFO | train | epoch 027 | loss 1.911 | trans_loss 4.778 | nll_loss 1.982 | w2v_ctc_loss 0.633 | task_loss 1.405 | contrastive_loss 0.125 | total 4138.65 | n_correct 2849.37 | ppl 3.95 | accuracy 68.848 | wps 10647.5 | ups 1.29 | wpb 8277.3 | bsz 305.7 | num_updates 39780 | lr 7.09059e-05 | gnorm 0.534 | clip 0 | loss_scale 16 | train_wall 1072 | gb_free 17.5 | wall 34851
2023-08-16 00:50:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 00:50:32 | INFO | fairseq.trainer | begin training epoch 28
2023-08-16 00:50:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 00:50:55 | INFO | train_inner | epoch 028:     20 / 1474 loss=1.898, trans_loss=4.773, nll_loss=1.975, w2v_ctc_loss=0.622, task_loss=1.362, contrastive_loss=0.073, total=4107.3, n_correct=2834.76, ppl=3.93, accuracy=69.018, wps=5926.4, ups=0.72, wpb=8214.6, bsz=304.5, num_updates=39800, lr=7.08881e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=73, gb_free=17, wall=34874
2023-08-16 00:52:08 | INFO | train_inner | epoch 028:    120 / 1474 loss=1.894, trans_loss=4.752, nll_loss=1.948, w2v_ctc_loss=0.624, task_loss=1.469, contrastive_loss=0.069, total=4112.44, n_correct=2854.28, ppl=3.86, accuracy=69.406, wps=11244.7, ups=1.37, wpb=8224.9, bsz=292.7, num_updates=39900, lr=7.07992e-05, gnorm=0.524, clip=0, loss_scale=16, train_wall=73, gb_free=13.4, wall=34947
2023-08-16 00:53:21 | INFO | train_inner | epoch 028:    220 / 1474 loss=1.895, trans_loss=4.763, nll_loss=1.962, w2v_ctc_loss=0.622, task_loss=1.326, contrastive_loss=0.08, total=4193.3, n_correct=2901.62, ppl=3.9, accuracy=69.197, wps=11494.4, ups=1.37, wpb=8386.6, bsz=316.4, num_updates=40000, lr=7.07107e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=72, gb_free=16.6, wall=35020
2023-08-16 00:53:21 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 00:53:45 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 3.97 | trans_loss 5.166 | nll_loss 2.425 | w2v_ctc_loss 1.363 | task_loss 4.657 | contrastive_loss 0.295 | total 4003.4 | n_correct 2678.7 | ppl 5.37 | accuracy 66.911 | uer 17.633 | wer 19.447 | raw_wer 19.447 | bleu 22.45 | wps 2104.4 | wpb 4003.4 | bsz 141.8 | num_updates 40000 | best_bleu 22.45
2023-08-16 00:53:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 40000 updates
2023-08-16 00:53:45 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_28_40000.pt
2023-08-16 00:53:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_28_40000.pt
2023-08-16 00:54:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_28_40000.pt (epoch 28 @ 40000 updates, score 22.45) (writing took 31.44790793210268 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:0')
2023-08-16 00:55:32 | INFO | train_inner | epoch 028:    320 / 1474 loss=1.931, trans_loss=4.77, nll_loss=1.972, w2v_ctc_loss=0.619, task_loss=1.407, contrastive_loss=0.415, total=4138.69, n_correct=2851.84, ppl=3.92, accuracy=68.907, wps=6322.9, ups=0.76, wpb=8277.4, bsz=314.4, num_updates=40100, lr=7.06225e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=74, gb_free=12.8, wall=35151
2023-08-16 00:56:45 | INFO | train_inner | epoch 028:    420 / 1474 loss=1.899, trans_loss=4.763, nll_loss=1.962, w2v_ctc_loss=0.631, task_loss=1.451, contrastive_loss=0.067, total=4089.84, n_correct=2827.03, ppl=3.89, accuracy=69.123, wps=11253.3, ups=1.38, wpb=8179.7, bsz=295.7, num_updates=40200, lr=7.05346e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=72, gb_free=16.1, wall=35224
2023-08-16 00:57:57 | INFO | train_inner | epoch 028:    520 / 1474 loss=1.899, trans_loss=4.765, nll_loss=1.965, w2v_ctc_loss=0.625, task_loss=1.463, contrastive_loss=0.079, total=4098.92, n_correct=2830.72, ppl=3.9, accuracy=69.06, wps=11276.7, ups=1.38, wpb=8197.8, bsz=295.7, num_updates=40300, lr=7.0447e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=72, gb_free=16.6, wall=35297
2023-08-16 00:59:10 | INFO | train_inner | epoch 028:    620 / 1474 loss=1.9, trans_loss=4.773, nll_loss=1.975, w2v_ctc_loss=0.624, task_loss=1.417, contrastive_loss=0.078, total=4180.1, n_correct=2887.3, ppl=3.93, accuracy=69.073, wps=11469.6, ups=1.37, wpb=8360.2, bsz=305.3, num_updates=40400, lr=7.03598e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=72, gb_free=16.2, wall=35370
2023-08-16 01:00:24 | INFO | train_inner | epoch 028:    720 / 1474 loss=1.909, trans_loss=4.776, nll_loss=1.98, w2v_ctc_loss=0.622, task_loss=1.267, contrastive_loss=0.191, total=4191.62, n_correct=2893.84, ppl=3.94, accuracy=69.039, wps=11383.2, ups=1.36, wpb=8383.2, bsz=329.2, num_updates=40500, lr=7.02728e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=73, gb_free=16.7, wall=35443
2023-08-16 01:01:37 | INFO | train_inner | epoch 028:    820 / 1474 loss=1.891, trans_loss=4.763, nll_loss=1.962, w2v_ctc_loss=0.618, task_loss=1.389, contrastive_loss=0.07, total=4088.91, n_correct=2834.34, ppl=3.9, accuracy=69.318, wps=11193.3, ups=1.37, wpb=8177.8, bsz=304.3, num_updates=40600, lr=7.01862e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=73, gb_free=16.7, wall=35516
2023-08-16 01:02:50 | INFO | train_inner | epoch 028:    920 / 1474 loss=1.913, trans_loss=4.775, nll_loss=1.978, w2v_ctc_loss=0.632, task_loss=1.454, contrastive_loss=0.134, total=4117.01, n_correct=2831.06, ppl=3.94, accuracy=68.765, wps=11232.9, ups=1.36, wpb=8234, bsz=299.7, num_updates=40700, lr=7.01e-05, gnorm=0.538, clip=0, loss_scale=16, train_wall=73, gb_free=15, wall=35590
2023-08-16 01:04:04 | INFO | train_inner | epoch 028:   1020 / 1474 loss=1.917, trans_loss=4.774, nll_loss=1.976, w2v_ctc_loss=0.633, task_loss=1.369, contrastive_loss=0.185, total=4182.85, n_correct=2880.13, ppl=3.93, accuracy=68.856, wps=11392.5, ups=1.36, wpb=8365.7, bsz=312, num_updates=40800, lr=7.0014e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=73, gb_free=16, wall=35663
2023-08-16 01:05:18 | INFO | train_inner | epoch 028:   1120 / 1474 loss=1.898, trans_loss=4.767, nll_loss=1.968, w2v_ctc_loss=0.625, task_loss=1.352, contrastive_loss=0.09, total=4220.16, n_correct=2914.84, ppl=3.91, accuracy=69.069, wps=11396.8, ups=1.35, wpb=8440.3, bsz=321, num_updates=40900, lr=6.99284e-05, gnorm=0.518, clip=0, loss_scale=16, train_wall=74, gb_free=15.5, wall=35737
2023-08-16 01:06:30 | INFO | train_inner | epoch 028:   1220 / 1474 loss=1.898, trans_loss=4.773, nll_loss=1.975, w2v_ctc_loss=0.621, task_loss=1.4, contrastive_loss=0.077, total=4092.46, n_correct=2822.99, ppl=3.93, accuracy=68.98, wps=11306.4, ups=1.38, wpb=8184.9, bsz=303.1, num_updates=41000, lr=6.9843e-05, gnorm=0.531, clip=0, loss_scale=16, train_wall=72, gb_free=17.3, wall=35809
2023-08-16 01:07:44 | INFO | train_inner | epoch 028:   1320 / 1474 loss=1.91, trans_loss=4.774, nll_loss=1.976, w2v_ctc_loss=0.636, task_loss=1.537, contrastive_loss=0.092, total=4084.55, n_correct=2811.34, ppl=3.93, accuracy=68.829, wps=11048.1, ups=1.35, wpb=8169.1, bsz=285.1, num_updates=41100, lr=6.9758e-05, gnorm=0.535, clip=0, loss_scale=16, train_wall=73, gb_free=15.4, wall=35883
2023-08-16 01:08:58 | INFO | train_inner | epoch 028:   1420 / 1474 loss=1.913, trans_loss=4.776, nll_loss=1.978, w2v_ctc_loss=0.636, task_loss=1.467, contrastive_loss=0.116, total=4154.09, n_correct=2856.55, ppl=3.94, accuracy=68.765, wps=11284.2, ups=1.36, wpb=8308.2, bsz=299.3, num_updates=41200, lr=6.96733e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=73, gb_free=15.6, wall=35957
2023-08-16 01:09:38 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:2')
2023-08-16 01:10:02 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 3.941 | trans_loss 5.162 | nll_loss 2.423 | w2v_ctc_loss 1.28 | task_loss 4.625 | contrastive_loss 0.292 | total 4003.4 | n_correct 2678.1 | ppl 5.36 | accuracy 66.896 | uer 17.174 | wer 18.836 | raw_wer 18.836 | bleu 22.82 | wps 2086.8 | wpb 4003.4 | bsz 141.8 | num_updates 41254 | best_bleu 22.82
2023-08-16 01:10:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 41254 updates
2023-08-16 01:10:02 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-16 01:10:18 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt
2023-08-16 01:10:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_best.pt (epoch 28 @ 41254 updates, score 22.82) (writing took 30.72967378050089 seconds)
2023-08-16 01:10:33 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-08-16 01:10:33 | INFO | train | epoch 028 | loss 1.904 | trans_loss 4.768 | nll_loss 1.969 | w2v_ctc_loss 0.626 | task_loss 1.406 | contrastive_loss 0.124 | total 4138.65 | n_correct 2857.23 | ppl 3.92 | accuracy 69.038 | wps 10157.4 | ups 1.23 | wpb 8277.3 | bsz 305.7 | num_updates 41254 | lr 6.96277e-05 | gnorm 0.527 | clip 0 | loss_scale 16 | train_wall 1073 | gb_free 16.1 | wall 36052
2023-08-16 01:10:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 01:10:33 | INFO | fairseq.trainer | begin training epoch 29
2023-08-16 01:10:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 01:11:15 | INFO | train_inner | epoch 029:     46 / 1474 loss=1.894, trans_loss=4.757, nll_loss=1.955, w2v_ctc_loss=0.624, task_loss=1.354, contrastive_loss=0.089, total=4169.12, n_correct=2890.63, ppl=3.88, accuracy=69.334, wps=6079.7, ups=0.73, wpb=8338.2, bsz=316.4, num_updates=41300, lr=6.95889e-05, gnorm=0.532, clip=0, loss_scale=16, train_wall=73, gb_free=16, wall=36094
2023-08-16 01:12:28 | INFO | train_inner | epoch 029:    146 / 1474 loss=1.897, trans_loss=4.759, nll_loss=1.956, w2v_ctc_loss=0.622, task_loss=1.402, contrastive_loss=0.107, total=4105.72, n_correct=2843.43, ppl=3.88, accuracy=69.255, wps=11157.8, ups=1.36, wpb=8211.4, bsz=304.1, num_updates=41400, lr=6.95048e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=73, gb_free=15.6, wall=36168
2023-08-16 01:13:42 | INFO | train_inner | epoch 029:    246 / 1474 loss=1.898, trans_loss=4.752, nll_loss=1.948, w2v_ctc_loss=0.612, task_loss=1.283, contrastive_loss=0.19, total=4199.67, n_correct=2913.38, ppl=3.86, accuracy=69.372, wps=11381.7, ups=1.36, wpb=8399.3, bsz=330.5, num_updates=41500, lr=6.9421e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=73, gb_free=15.3, wall=36242
2023-08-16 01:14:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-16 01:14:56 | INFO | train_inner | epoch 029:    347 / 1474 loss=1.905, trans_loss=4.77, nll_loss=1.97, w2v_ctc_loss=0.636, task_loss=1.505, contrastive_loss=0.073, total=4098.92, n_correct=2828.22, ppl=3.92, accuracy=68.999, wps=11058.3, ups=1.35, wpb=8197.8, bsz=291.7, num_updates=41600, lr=6.93375e-05, gnorm=0.537, clip=0, loss_scale=16, train_wall=74, gb_free=16, wall=36316
2023-08-16 01:16:10 | INFO | train_inner | epoch 029:    447 / 1474 loss=1.88, trans_loss=4.739, nll_loss=1.931, w2v_ctc_loss=0.609, task_loss=1.347, contrastive_loss=0.066, total=4157.41, n_correct=2896.22, ppl=3.81, accuracy=69.664, wps=11364.2, ups=1.37, wpb=8314.8, bsz=308.5, num_updates=41700, lr=6.92543e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=73, gb_free=14.3, wall=36389
2023-08-16 01:17:23 | INFO | train_inner | epoch 029:    547 / 1474 loss=1.912, trans_loss=4.768, nll_loss=1.968, w2v_ctc_loss=0.625, task_loss=1.51, contrastive_loss=0.163, total=4149.27, n_correct=2861.49, ppl=3.91, accuracy=68.964, wps=11232.3, ups=1.35, wpb=8298.5, bsz=293.3, num_updates=41800, lr=6.91714e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=73, gb_free=15.1, wall=36463
2023-08-16 01:18:37 | INFO | train_inner | epoch 029:    647 / 1474 loss=1.905, trans_loss=4.757, nll_loss=1.955, w2v_ctc_loss=0.62, task_loss=1.329, contrastive_loss=0.233, total=4145.39, n_correct=2872.5, ppl=3.88, accuracy=69.294, wps=11269.3, ups=1.36, wpb=8290.8, bsz=319.3, num_updates=41900, lr=6.90889e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=73, gb_free=17.1, wall=36536
2023-08-16 01:19:52 | INFO | train_inner | epoch 029:    747 / 1474 loss=1.899, trans_loss=4.758, nll_loss=1.956, w2v_ctc_loss=0.62, task_loss=1.297, contrastive_loss=0.15, total=4242.46, n_correct=2937.38, ppl=3.88, accuracy=69.238, wps=11389.1, ups=1.34, wpb=8484.9, bsz=329.9, num_updates=42000, lr=6.90066e-05, gnorm=0.561, clip=0, loss_scale=16, train_wall=74, gb_free=16.2, wall=36611
2023-08-16 01:19:52 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 01:20:16 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 3.949 | trans_loss 5.163 | nll_loss 2.423 | w2v_ctc_loss 1.299 | task_loss 4.641 | contrastive_loss 0.299 | total 4003.4 | n_correct 2677.2 | ppl 5.36 | accuracy 66.873 | uer 17.195 | wer 19.071 | raw_wer 19.071 | bleu 22.19 | wps 2099.1 | wpb 4003.4 | bsz 141.8 | num_updates 42000 | best_bleu 22.82
2023-08-16 01:20:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42000 updates
2023-08-16 01:20:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_29_42000.pt
2023-08-16 01:20:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_29_42000.pt
2023-08-16 01:20:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_29_42000.pt (epoch 29 @ 42000 updates, score 22.19) (writing took 27.4435654040426 seconds)
2023-08-16 01:21:57 | INFO | train_inner | epoch 029:    847 / 1474 loss=1.898, trans_loss=4.769, nll_loss=1.97, w2v_ctc_loss=0.619, task_loss=1.561, contrastive_loss=0.066, total=4027.03, n_correct=2781.74, ppl=3.92, accuracy=69.077, wps=6396.8, ups=0.79, wpb=8054.1, bsz=280.3, num_updates=42100, lr=6.89246e-05, gnorm=0.548, clip=0, loss_scale=16, train_wall=73, gb_free=17.1, wall=36737
2023-08-16 01:23:10 | INFO | train_inner | epoch 029:    947 / 1474 loss=1.899, trans_loss=4.765, nll_loss=1.964, w2v_ctc_loss=0.629, task_loss=1.435, contrastive_loss=0.077, total=4086.72, n_correct=2826.18, ppl=3.9, accuracy=69.155, wps=11276, ups=1.38, wpb=8173.4, bsz=296.3, num_updates=42200, lr=6.88428e-05, gnorm=0.533, clip=0, loss_scale=16, train_wall=72, gb_free=15, wall=36809
2023-08-16 01:24:23 | INFO | train_inner | epoch 029:   1047 / 1474 loss=1.897, trans_loss=4.757, nll_loss=1.955, w2v_ctc_loss=0.614, task_loss=1.407, contrastive_loss=0.153, total=4139.4, n_correct=2869.25, ppl=3.88, accuracy=69.316, wps=11321.4, ups=1.37, wpb=8278.8, bsz=307.4, num_updates=42300, lr=6.87614e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=73, gb_free=15.2, wall=36882
2023-08-16 01:25:36 | INFO | train_inner | epoch 029:   1147 / 1474 loss=1.901, trans_loss=4.77, nll_loss=1.971, w2v_ctc_loss=0.627, task_loss=1.535, contrastive_loss=0.064, total=4072.33, n_correct=2811.27, ppl=3.92, accuracy=69.033, wps=11138.1, ups=1.37, wpb=8144.7, bsz=284.1, num_updates=42400, lr=6.86803e-05, gnorm=0.558, clip=0, loss_scale=16, train_wall=73, gb_free=16.5, wall=36956
2023-08-16 01:26:49 | INFO | train_inner | epoch 029:   1247 / 1474 loss=1.902, trans_loss=4.771, nll_loss=1.973, w2v_ctc_loss=0.63, task_loss=1.422, contrastive_loss=0.072, total=4160.52, n_correct=2870.68, ppl=3.93, accuracy=68.998, wps=11358.4, ups=1.37, wpb=8321, bsz=301.5, num_updates=42500, lr=6.85994e-05, gnorm=0.523, clip=0, loss_scale=16, train_wall=73, gb_free=15.2, wall=37029
2023-08-16 01:28:02 | INFO | train_inner | epoch 029:   1347 / 1474 loss=1.898, trans_loss=4.758, nll_loss=1.955, w2v_ctc_loss=0.617, task_loss=1.388, contrastive_loss=0.133, total=4168.02, n_correct=2884.9, ppl=3.88, accuracy=69.215, wps=11423, ups=1.37, wpb=8336, bsz=310.2, num_updates=42600, lr=6.85189e-05, gnorm=0.556, clip=0, loss_scale=16, train_wall=72, gb_free=16.2, wall=37102
2023-08-16 01:29:16 | INFO | train_inner | epoch 029:   1447 / 1474 loss=1.902, trans_loss=4.759, nll_loss=1.957, w2v_ctc_loss=0.622, task_loss=1.37, contrastive_loss=0.164, total=4166.06, n_correct=2884.9, ppl=3.88, accuracy=69.248, wps=11390, ups=1.37, wpb=8332.1, bsz=313.1, num_updates=42700, lr=6.84386e-05, gnorm=0.552, clip=0, loss_scale=16, train_wall=73, gb_free=16.5, wall=37175
2023-08-16 01:29:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 01:29:59 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 3.94 | trans_loss 5.157 | nll_loss 2.414 | w2v_ctc_loss 1.282 | task_loss 4.658 | contrastive_loss 0.297 | total 4003.4 | n_correct 2676.8 | ppl 5.33 | accuracy 66.863 | uer 17.02 | wer 18.892 | raw_wer 18.892 | bleu 22.39 | wps 2177.5 | wpb 4003.4 | bsz 141.8 | num_updates 42727 | best_bleu 22.82
2023-08-16 01:29:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42727 updates
2023-08-16 01:29:59 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.3902.pt
2023-08-16 01:30:02 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.3902.pt
2023-08-16 01:30:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.3902.pt (epoch 29 @ 42727 updates, score 22.39) (writing took 22.06018311716616 seconds)
2023-08-16 01:30:21 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-08-16 01:30:21 | INFO | train | epoch 029 | loss 1.899 | trans_loss 4.76 | nll_loss 1.959 | w2v_ctc_loss 0.621 | task_loss 1.406 | contrastive_loss 0.122 | total 4138.83 | n_correct 2864.57 | ppl 3.89 | accuracy 69.212 | wps 10260.6 | ups 1.24 | wpb 8277.7 | bsz 305.7 | num_updates 42727 | lr 6.8417e-05 | gnorm 0.538 | clip 0 | loss_scale 16 | train_wall 1074 | gb_free 15.7 | wall 37241
2023-08-16 01:30:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 01:30:22 | INFO | fairseq.trainer | begin training epoch 30
2023-08-16 01:30:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 01:31:23 | INFO | train_inner | epoch 030:     73 / 1474 loss=1.894, trans_loss=4.749, nll_loss=1.945, w2v_ctc_loss=0.607, task_loss=1.341, contrastive_loss=0.183, total=4175.11, n_correct=2896.84, ppl=3.85, accuracy=69.384, wps=6558.2, ups=0.79, wpb=8350.2, bsz=318.6, num_updates=42800, lr=6.83586e-05, gnorm=0.534, clip=0, loss_scale=16, train_wall=73, gb_free=16.8, wall=37302
2023-08-16 01:32:36 | INFO | train_inner | epoch 030:    173 / 1474 loss=1.884, trans_loss=4.733, nll_loss=1.923, w2v_ctc_loss=0.614, task_loss=1.316, contrastive_loss=0.111, total=4202.64, n_correct=2933.33, ppl=3.79, accuracy=69.797, wps=11492.5, ups=1.37, wpb=8405.3, bsz=318.3, num_updates=42900, lr=6.82789e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=73, gb_free=16.3, wall=37375
2023-08-16 01:33:49 | INFO | train_inner | epoch 030:    273 / 1474 loss=1.892, trans_loss=4.752, nll_loss=1.947, w2v_ctc_loss=0.623, task_loss=1.449, contrastive_loss=0.068, total=4120.21, n_correct=2860.4, ppl=3.85, accuracy=69.424, wps=11373.4, ups=1.38, wpb=8240.4, bsz=294.9, num_updates=43000, lr=6.81994e-05, gnorm=0.531, clip=0, loss_scale=16, train_wall=72, gb_free=14.8, wall=37448
2023-08-16 01:35:02 | INFO | train_inner | epoch 030:    373 / 1474 loss=1.882, trans_loss=4.741, nll_loss=1.934, w2v_ctc_loss=0.611, task_loss=1.402, contrastive_loss=0.07, total=4178.23, n_correct=2913.01, ppl=3.82, accuracy=69.719, wps=11302.7, ups=1.35, wpb=8356.5, bsz=307.5, num_updates=43100, lr=6.81203e-05, gnorm=0.516, clip=0, loss_scale=16, train_wall=73, gb_free=9.6, wall=37522
2023-08-16 01:36:15 | INFO | train_inner | epoch 030:    473 / 1474 loss=1.891, trans_loss=4.749, nll_loss=1.944, w2v_ctc_loss=0.61, task_loss=1.35, contrastive_loss=0.133, total=4124.47, n_correct=2867.03, ppl=3.85, accuracy=69.513, wps=11405.7, ups=1.38, wpb=8248.9, bsz=312.6, num_updates=43200, lr=6.80414e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=72, gb_free=17.2, wall=37594
2023-08-16 01:37:27 | INFO | train_inner | epoch 030:    573 / 1474 loss=1.891, trans_loss=4.752, nll_loss=1.948, w2v_ctc_loss=0.619, task_loss=1.366, contrastive_loss=0.094, total=4168.41, n_correct=2893.89, ppl=3.86, accuracy=69.424, wps=11491.2, ups=1.38, wpb=8336.8, bsz=312.4, num_updates=43300, lr=6.79628e-05, gnorm=0.52, clip=0, loss_scale=16, train_wall=72, gb_free=16.9, wall=37667
2023-08-16 01:38:41 | INFO | train_inner | epoch 030:    673 / 1474 loss=1.894, trans_loss=4.752, nll_loss=1.947, w2v_ctc_loss=0.621, task_loss=1.39, contrastive_loss=0.108, total=4187.95, n_correct=2901.1, ppl=3.86, accuracy=69.273, wps=11408.4, ups=1.36, wpb=8375.9, bsz=315, num_updates=43400, lr=6.78844e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=73, gb_free=15.4, wall=37740
2023-08-16 01:39:54 | INFO | train_inner | epoch 030:    773 / 1474 loss=1.912, trans_loss=4.761, nll_loss=1.96, w2v_ctc_loss=0.632, task_loss=1.437, contrastive_loss=0.189, total=4105.32, n_correct=2836.74, ppl=3.89, accuracy=69.099, wps=11171.8, ups=1.36, wpb=8210.6, bsz=302.6, num_updates=43500, lr=6.78064e-05, gnorm=0.541, clip=0, loss_scale=16, train_wall=73, gb_free=12.6, wall=37814
2023-08-16 01:41:07 | INFO | train_inner | epoch 030:    873 / 1474 loss=1.894, trans_loss=4.757, nll_loss=1.954, w2v_ctc_loss=0.618, task_loss=1.451, contrastive_loss=0.081, total=4102.11, n_correct=2844.81, ppl=3.87, accuracy=69.35, wps=11236.5, ups=1.37, wpb=8204.2, bsz=295.6, num_updates=43600, lr=6.77285e-05, gnorm=0.538, clip=0, loss_scale=32, train_wall=73, gb_free=17, wall=37887
2023-08-16 01:42:20 | INFO | train_inner | epoch 030:    973 / 1474 loss=1.897, trans_loss=4.76, nll_loss=1.958, w2v_ctc_loss=0.625, task_loss=1.438, contrastive_loss=0.082, total=4129.98, n_correct=2860.44, ppl=3.89, accuracy=69.26, wps=11324.8, ups=1.37, wpb=8260, bsz=300.4, num_updates=43700, lr=6.7651e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=72, gb_free=16, wall=37960
2023-08-16 01:43:34 | INFO | train_inner | epoch 030:   1073 / 1474 loss=1.903, trans_loss=4.758, nll_loss=1.955, w2v_ctc_loss=0.617, task_loss=1.572, contrastive_loss=0.16, total=4101.17, n_correct=2837.9, ppl=3.88, accuracy=69.197, wps=11143.3, ups=1.36, wpb=8202.3, bsz=282.3, num_updates=43800, lr=6.75737e-05, gnorm=0.526, clip=0, loss_scale=32, train_wall=73, gb_free=15.3, wall=38033
2023-08-16 01:44:48 | INFO | train_inner | epoch 030:   1173 / 1474 loss=1.892, trans_loss=4.753, nll_loss=1.95, w2v_ctc_loss=0.61, task_loss=1.357, contrastive_loss=0.139, total=4168.36, n_correct=2893.56, ppl=3.86, accuracy=69.417, wps=11316.2, ups=1.36, wpb=8336.7, bsz=314, num_updates=43900, lr=6.74967e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=73, gb_free=15.6, wall=38107
2023-08-16 01:46:01 | INFO | train_inner | epoch 030:   1273 / 1474 loss=1.897, trans_loss=4.757, nll_loss=1.954, w2v_ctc_loss=0.626, task_loss=1.552, contrastive_loss=0.074, total=4036.17, n_correct=2793.02, ppl=3.88, accuracy=69.2, wps=11045.1, ups=1.37, wpb=8072.3, bsz=284.3, num_updates=44000, lr=6.742e-05, gnorm=0.537, clip=0, loss_scale=32, train_wall=73, gb_free=15.4, wall=38180
2023-08-16 01:46:01 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 01:46:25 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 3.935 | trans_loss 5.153 | nll_loss 2.408 | w2v_ctc_loss 1.283 | task_loss 4.647 | contrastive_loss 0.285 | total 4003.4 | n_correct 2681.1 | ppl 5.31 | accuracy 66.971 | uer 16.935 | wer 18.821 | raw_wer 18.821 | bleu 22.45 | wps 2066.2 | wpb 4003.4 | bsz 141.8 | num_updates 44000 | best_bleu 22.82
2023-08-16 01:46:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44000 updates
2023-08-16 01:46:25 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_30_44000.pt
2023-08-16 01:46:28 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_30_44000.pt
2023-08-16 01:47:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_30_44000.pt (epoch 30 @ 44000 updates, score 22.45) (writing took 44.47354228235781 seconds)
2023-08-16 01:48:23 | INFO | train_inner | epoch 030:   1373 / 1474 loss=1.885, trans_loss=4.753, nll_loss=1.95, w2v_ctc_loss=0.61, task_loss=1.324, contrastive_loss=0.083, total=4165.07, n_correct=2892.22, ppl=3.86, accuracy=69.44, wps=5846.5, ups=0.7, wpb=8330.1, bsz=321.6, num_updates=44100, lr=6.73435e-05, gnorm=0.521, clip=0, loss_scale=32, train_wall=72, gb_free=16.3, wall=38322
2023-08-16 01:49:36 | INFO | train_inner | epoch 030:   1473 / 1474 loss=1.901, trans_loss=4.759, nll_loss=1.957, w2v_ctc_loss=0.609, task_loss=1.323, contrastive_loss=0.232, total=4141.76, n_correct=2869.35, ppl=3.88, accuracy=69.279, wps=11372.8, ups=1.37, wpb=8283.5, bsz=314.3, num_updates=44200, lr=6.72673e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=72, gb_free=15.9, wall=38395
2023-08-16 01:49:37 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 01:50:01 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 3.938 | trans_loss 5.156 | nll_loss 2.414 | w2v_ctc_loss 1.281 | task_loss 4.643 | contrastive_loss 0.293 | total 4003.4 | n_correct 2684.6 | ppl 5.33 | accuracy 67.058 | uer 17.11 | wer 19.011 | raw_wer 19.011 | bleu 22.67 | wps 2132.9 | wpb 4003.4 | bsz 141.8 | num_updates 44201 | best_bleu 22.82
2023-08-16 01:50:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44201 updates
2023-08-16 01:50:01 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.6700.pt
2023-08-16 01:50:04 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.6700.pt
2023-08-16 01:50:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.6700.pt (epoch 30 @ 44201 updates, score 22.67) (writing took 25.564846048131585 seconds)
2023-08-16 01:50:27 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-08-16 01:50:27 | INFO | train | epoch 030 | loss 1.894 | trans_loss 4.752 | nll_loss 1.948 | w2v_ctc_loss 0.617 | task_loss 1.405 | contrastive_loss 0.121 | total 4138.65 | n_correct 2871.71 | ppl 3.86 | accuracy 69.388 | wps 10122.3 | ups 1.22 | wpb 8277.3 | bsz 305.7 | num_updates 44201 | lr 6.72665e-05 | gnorm 0.529 | clip 0 | loss_scale 32 | train_wall 1071 | gb_free 16.7 | wall 38446
2023-08-16 01:50:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 01:50:27 | INFO | fairseq.trainer | begin training epoch 31
2023-08-16 01:50:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 01:51:47 | INFO | train_inner | epoch 031:     99 / 1474 loss=1.883, trans_loss=4.739, nll_loss=1.93, w2v_ctc_loss=0.612, task_loss=1.504, contrastive_loss=0.067, total=4054.44, n_correct=2827.45, ppl=3.81, accuracy=69.737, wps=6185.7, ups=0.76, wpb=8108.9, bsz=288.2, num_updates=44300, lr=6.71913e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=72, gb_free=16.1, wall=38526
2023-08-16 01:53:01 | INFO | train_inner | epoch 031:    199 / 1474 loss=1.887, trans_loss=4.744, nll_loss=1.937, w2v_ctc_loss=0.612, task_loss=1.441, contrastive_loss=0.095, total=4147.4, n_correct=2885.13, ppl=3.83, accuracy=69.565, wps=11264.3, ups=1.36, wpb=8294.8, bsz=302.2, num_updates=44400, lr=6.71156e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=73, gb_free=16.4, wall=38600
2023-08-16 01:54:14 | INFO | train_inner | epoch 031:    299 / 1474 loss=1.893, trans_loss=4.74, nll_loss=1.932, w2v_ctc_loss=0.616, task_loss=1.435, contrastive_loss=0.135, total=4149.21, n_correct=2885.51, ppl=3.82, accuracy=69.544, wps=11243.1, ups=1.35, wpb=8298.4, bsz=301.6, num_updates=44500, lr=6.70402e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=73, gb_free=15.7, wall=38674
2023-08-16 01:55:28 | INFO | train_inner | epoch 031:    399 / 1474 loss=1.885, trans_loss=4.746, nll_loss=1.94, w2v_ctc_loss=0.608, task_loss=1.535, contrastive_loss=0.074, total=4092.62, n_correct=2845.1, ppl=3.84, accuracy=69.518, wps=11062.2, ups=1.35, wpb=8185.2, bsz=285.6, num_updates=44600, lr=6.6965e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=74, gb_free=16.9, wall=38748
2023-08-16 01:56:42 | INFO | train_inner | epoch 031:    499 / 1474 loss=1.889, trans_loss=4.744, nll_loss=1.937, w2v_ctc_loss=0.622, task_loss=1.473, contrastive_loss=0.08, total=4111.85, n_correct=2861.78, ppl=3.83, accuracy=69.598, wps=11191.4, ups=1.36, wpb=8223.7, bsz=300.3, num_updates=44700, lr=6.689e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=73, gb_free=10.5, wall=38821
2023-08-16 01:57:56 | INFO | train_inner | epoch 031:    599 / 1474 loss=1.884, trans_loss=4.741, nll_loss=1.934, w2v_ctc_loss=0.612, task_loss=1.467, contrastive_loss=0.07, total=4083.44, n_correct=2840.71, ppl=3.82, accuracy=69.567, wps=11084.4, ups=1.36, wpb=8166.9, bsz=294.5, num_updates=44800, lr=6.68153e-05, gnorm=0.538, clip=0, loss_scale=32, train_wall=73, gb_free=16.4, wall=38895
2023-08-16 01:59:09 | INFO | train_inner | epoch 031:    699 / 1474 loss=1.878, trans_loss=4.739, nll_loss=1.931, w2v_ctc_loss=0.604, task_loss=1.339, contrastive_loss=0.072, total=4213.98, n_correct=2936.92, ppl=3.81, accuracy=69.695, wps=11486.1, ups=1.36, wpb=8428, bsz=315.7, num_updates=44900, lr=6.67409e-05, gnorm=0.526, clip=0, loss_scale=32, train_wall=73, gb_free=15.8, wall=38968
2023-08-16 02:00:23 | INFO | train_inner | epoch 031:    799 / 1474 loss=1.897, trans_loss=4.749, nll_loss=1.944, w2v_ctc_loss=0.616, task_loss=1.47, contrastive_loss=0.142, total=4097.37, n_correct=2840.54, ppl=3.85, accuracy=69.326, wps=11009.1, ups=1.34, wpb=8194.7, bsz=295.8, num_updates=45000, lr=6.66667e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=74, gb_free=12.5, wall=39043
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:0')
2023-08-16 02:01:37 | INFO | train_inner | epoch 031:    899 / 1474 loss=1.884, trans_loss=4.736, nll_loss=1.928, w2v_ctc_loss=0.611, task_loss=1.461, contrastive_loss=0.086, total=4096.72, n_correct=2852.61, ppl=3.8, accuracy=69.632, wps=11141.9, ups=1.36, wpb=8193.4, bsz=296.1, num_updates=45100, lr=6.65927e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=73, gb_free=16.9, wall=39116
2023-08-16 02:02:51 | INFO | train_inner | epoch 031:    999 / 1474 loss=1.893, trans_loss=4.752, nll_loss=1.949, w2v_ctc_loss=0.606, task_loss=1.327, contrastive_loss=0.172, total=4187.84, n_correct=2910.93, ppl=3.86, accuracy=69.509, wps=11356.4, ups=1.36, wpb=8375.7, bsz=319.5, num_updates=45200, lr=6.6519e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=73, gb_free=16.9, wall=39190
2023-08-16 02:04:05 | INFO | train_inner | epoch 031:   1099 / 1474 loss=1.888, trans_loss=4.747, nll_loss=1.942, w2v_ctc_loss=0.609, task_loss=1.373, contrastive_loss=0.115, total=4149.44, n_correct=2887.62, ppl=3.84, accuracy=69.591, wps=11219.5, ups=1.35, wpb=8298.9, bsz=315, num_updates=45300, lr=6.64455e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=73, gb_free=17.3, wall=39264
2023-08-16 02:04:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-16 02:05:19 | INFO | train_inner | epoch 031:   1200 / 1474 loss=1.898, trans_loss=4.749, nll_loss=1.945, w2v_ctc_loss=0.608, task_loss=1.32, contrastive_loss=0.235, total=4190.62, n_correct=2911.27, ppl=3.85, accuracy=69.471, wps=11345, ups=1.35, wpb=8381.2, bsz=320.9, num_updates=45400, lr=6.63723e-05, gnorm=0.541, clip=0, loss_scale=16, train_wall=73, gb_free=16.7, wall=39338
2023-08-16 02:06:32 | INFO | train_inner | epoch 031:   1300 / 1474 loss=1.884, trans_loss=4.751, nll_loss=1.947, w2v_ctc_loss=0.611, task_loss=1.258, contrastive_loss=0.077, total=4227.39, n_correct=2942.25, ppl=3.86, accuracy=69.6, wps=11519.8, ups=1.36, wpb=8454.8, bsz=326.7, num_updates=45500, lr=6.62994e-05, gnorm=0.522, clip=0, loss_scale=16, train_wall=73, gb_free=16.6, wall=39411
2023-08-16 02:07:46 | INFO | train_inner | epoch 031:   1400 / 1474 loss=1.907, trans_loss=4.749, nll_loss=1.945, w2v_ctc_loss=0.613, task_loss=1.282, contrastive_loss=0.28, total=4191.1, n_correct=2909.57, ppl=3.85, accuracy=69.423, wps=11354.9, ups=1.35, wpb=8382.2, bsz=327, num_updates=45600, lr=6.62266e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=73, gb_free=16.2, wall=39485
2023-08-16 02:08:40 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2359, device='cuda:2')
2023-08-16 02:09:04 | INFO | dev_st | epoch 031 | valid on 'dev_st' subset | loss 3.953 | trans_loss 5.154 | nll_loss 2.409 | w2v_ctc_loss 1.338 | task_loss 4.647 | contrastive_loss 0.291 | total 4003.4 | n_correct 2683.7 | ppl 5.31 | accuracy 67.036 | uer 16.879 | wer 18.642 | raw_wer 18.642 | bleu 22.41 | wps 2217.2 | wpb 4003.4 | bsz 141.8 | num_updates 45674 | best_bleu 22.82
2023-08-16 02:09:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 45674 updates
2023-08-16 02:09:04 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.4105.pt
2023-08-16 02:09:08 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.4105.pt
2023-08-16 02:09:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.4105.pt (epoch 31 @ 45674 updates, score 22.41) (writing took 21.79709063284099 seconds)
2023-08-16 02:09:26 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-08-16 02:09:26 | INFO | train | epoch 031 | loss 1.889 | trans_loss 4.745 | nll_loss 1.939 | w2v_ctc_loss 0.612 | task_loss 1.405 | contrastive_loss 0.121 | total 4138.75 | n_correct 2878.6 | ppl 3.83 | accuracy 69.552 | wps 10702.6 | ups 1.29 | wpb 8277.5 | bsz 305.7 | num_updates 45674 | lr 6.61729e-05 | gnorm 0.533 | clip 0 | loss_scale 16 | train_wall 1077 | gb_free 11.7 | wall 39585
2023-08-16 02:09:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 02:09:26 | INFO | fairseq.trainer | begin training epoch 32
2023-08-16 02:09:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 02:09:54 | INFO | train_inner | epoch 032:     26 / 1474 loss=1.883, trans_loss=4.74, nll_loss=1.933, w2v_ctc_loss=0.612, task_loss=1.483, contrastive_loss=0.066, total=4040.88, n_correct=2813.96, ppl=3.82, accuracy=69.637, wps=6312.2, ups=0.78, wpb=8081.8, bsz=288.7, num_updates=45700, lr=6.61541e-05, gnorm=0.551, clip=0, loss_scale=16, train_wall=73, gb_free=15.1, wall=39613
2023-08-16 02:11:07 | INFO | train_inner | epoch 032:    126 / 1474 loss=1.865, trans_loss=4.721, nll_loss=1.907, w2v_ctc_loss=0.591, task_loss=1.295, contrastive_loss=0.076, total=4222.14, n_correct=2959.99, ppl=3.75, accuracy=70.106, wps=11506, ups=1.36, wpb=8444.3, bsz=322.6, num_updates=45800, lr=6.60819e-05, gnorm=0.522, clip=0, loss_scale=16, train_wall=73, gb_free=15.1, wall=39687
2023-08-16 02:12:21 | INFO | train_inner | epoch 032:    226 / 1474 loss=1.877, trans_loss=4.738, nll_loss=1.93, w2v_ctc_loss=0.604, task_loss=1.339, contrastive_loss=0.086, total=4159.77, n_correct=2902.17, ppl=3.81, accuracy=69.768, wps=11257, ups=1.35, wpb=8319.5, bsz=320.8, num_updates=45900, lr=6.60098e-05, gnorm=0.532, clip=0, loss_scale=16, train_wall=73, gb_free=15.9, wall=39761
2023-08-16 02:13:35 | INFO | train_inner | epoch 032:    326 / 1474 loss=1.869, trans_loss=4.722, nll_loss=1.909, w2v_ctc_loss=0.595, task_loss=1.331, contrastive_loss=0.079, total=4179.65, n_correct=2930.11, ppl=3.76, accuracy=70.104, wps=11383.5, ups=1.36, wpb=8359.3, bsz=313.8, num_updates=46000, lr=6.5938e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=73, gb_free=15.9, wall=39834
2023-08-16 02:13:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 02:13:59 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 3.967 | trans_loss 5.166 | nll_loss 2.426 | w2v_ctc_loss 1.357 | task_loss 4.676 | contrastive_loss 0.291 | total 4003.4 | n_correct 2681.3 | ppl 5.37 | accuracy 66.976 | uer 17.355 | wer 19.276 | raw_wer 19.276 | bleu 22.6 | wps 1899.7 | wpb 4003.4 | bsz 141.8 | num_updates 46000 | best_bleu 22.82
2023-08-16 02:13:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 46000 updates
2023-08-16 02:13:59 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_32_46000.pt
2023-08-16 02:14:02 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_32_46000.pt
2023-08-16 02:14:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_32_46000.pt (epoch 32 @ 46000 updates, score 22.6) (writing took 27.416279029101133 seconds)
2023-08-16 02:15:41 | INFO | train_inner | epoch 032:    426 / 1474 loss=1.874, trans_loss=4.726, nll_loss=1.914, w2v_ctc_loss=0.604, task_loss=1.372, contrastive_loss=0.078, total=4172.34, n_correct=2918.38, ppl=3.77, accuracy=69.946, wps=6593.4, ups=0.79, wpb=8344.7, bsz=309.9, num_updates=46100, lr=6.58665e-05, gnorm=0.533, clip=0, loss_scale=16, train_wall=73, gb_free=16.8, wall=39961
2023-08-16 02:16:56 | INFO | train_inner | epoch 032:    526 / 1474 loss=1.889, trans_loss=4.739, nll_loss=1.931, w2v_ctc_loss=0.609, task_loss=1.371, contrastive_loss=0.159, total=4191.15, n_correct=2920.77, ppl=3.81, accuracy=69.689, wps=11270.6, ups=1.34, wpb=8382.3, bsz=315.1, num_updates=46200, lr=6.57952e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=74, gb_free=14.6, wall=40035
2023-08-16 02:18:10 | INFO | train_inner | epoch 032:    626 / 1474 loss=1.882, trans_loss=4.739, nll_loss=1.931, w2v_ctc_loss=0.607, task_loss=1.47, contrastive_loss=0.085, total=4138.05, n_correct=2880.25, ppl=3.81, accuracy=69.604, wps=11130.3, ups=1.34, wpb=8276.1, bsz=299.5, num_updates=46300, lr=6.57241e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=74, gb_free=12.9, wall=40109
2023-08-16 02:19:24 | INFO | train_inner | epoch 032:    726 / 1474 loss=1.882, trans_loss=4.739, nll_loss=1.931, w2v_ctc_loss=0.613, task_loss=1.424, contrastive_loss=0.069, total=4156.23, n_correct=2897.83, ppl=3.81, accuracy=69.723, wps=11207.2, ups=1.35, wpb=8312.5, bsz=303, num_updates=46400, lr=6.56532e-05, gnorm=0.535, clip=0, loss_scale=16, train_wall=74, gb_free=16.1, wall=40183
2023-08-16 02:20:37 | INFO | train_inner | epoch 032:    826 / 1474 loss=1.877, trans_loss=4.735, nll_loss=1.925, w2v_ctc_loss=0.604, task_loss=1.454, contrastive_loss=0.065, total=4112.3, n_correct=2870.06, ppl=3.8, accuracy=69.792, wps=11243.1, ups=1.37, wpb=8224.6, bsz=293.9, num_updates=46500, lr=6.55826e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=73, gb_free=15.5, wall=40257
2023-08-16 02:21:51 | INFO | train_inner | epoch 032:    926 / 1474 loss=1.877, trans_loss=4.737, nll_loss=1.928, w2v_ctc_loss=0.603, task_loss=1.455, contrastive_loss=0.064, total=4139.37, n_correct=2885.85, ppl=3.81, accuracy=69.717, wps=11244.3, ups=1.36, wpb=8278.7, bsz=298.6, num_updates=46600, lr=6.55122e-05, gnorm=0.533, clip=0, loss_scale=16, train_wall=73, gb_free=12.3, wall=40330
2023-08-16 02:23:04 | INFO | train_inner | epoch 032:   1026 / 1474 loss=1.888, trans_loss=4.742, nll_loss=1.934, w2v_ctc_loss=0.607, task_loss=1.385, contrastive_loss=0.16, total=4121.85, n_correct=2867.47, ppl=3.82, accuracy=69.568, wps=11278, ups=1.37, wpb=8243.7, bsz=306.1, num_updates=46700, lr=6.5442e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=73, gb_free=16.5, wall=40403
2023-08-16 02:24:18 | INFO | train_inner | epoch 032:   1126 / 1474 loss=1.89, trans_loss=4.741, nll_loss=1.932, w2v_ctc_loss=0.61, task_loss=1.667, contrastive_loss=0.103, total=4015.59, n_correct=2792.3, ppl=3.82, accuracy=69.536, wps=10894.5, ups=1.36, wpb=8031.2, bsz=270.1, num_updates=46800, lr=6.5372e-05, gnorm=0.551, clip=0, loss_scale=16, train_wall=73, gb_free=16.9, wall=40477
2023-08-16 02:25:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-16 02:25:32 | INFO | train_inner | epoch 032:   1227 / 1474 loss=1.888, trans_loss=4.747, nll_loss=1.942, w2v_ctc_loss=0.612, task_loss=1.428, contrastive_loss=0.098, total=4129.97, n_correct=2870.74, ppl=3.84, accuracy=69.51, wps=11133.7, ups=1.35, wpb=8259.9, bsz=302.7, num_updates=46900, lr=6.53023e-05, gnorm=0.531, clip=0, loss_scale=8, train_wall=74, gb_free=15.7, wall=40551
2023-08-16 02:26:44 | INFO | train_inner | epoch 032:   1327 / 1474 loss=1.882, trans_loss=4.739, nll_loss=1.931, w2v_ctc_loss=0.613, task_loss=1.436, contrastive_loss=0.065, total=4079.56, n_correct=2839.11, ppl=3.81, accuracy=69.594, wps=11245.3, ups=1.38, wpb=8159.1, bsz=297.9, num_updates=47000, lr=6.52328e-05, gnorm=0.542, clip=0, loss_scale=8, train_wall=72, gb_free=16.8, wall=40624
2023-08-16 02:27:58 | INFO | train_inner | epoch 032:   1427 / 1474 loss=1.912, trans_loss=4.748, nll_loss=1.943, w2v_ctc_loss=0.622, task_loss=1.425, contrastive_loss=0.303, total=4107.37, n_correct=2847.44, ppl=3.85, accuracy=69.325, wps=11178.5, ups=1.36, wpb=8214.7, bsz=304.2, num_updates=47100, lr=6.51635e-05, gnorm=0.536, clip=0, loss_scale=8, train_wall=73, gb_free=17, wall=40697
2023-08-16 02:28:33 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 02:28:56 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 3.952 | trans_loss 5.16 | nll_loss 2.418 | w2v_ctc_loss 1.324 | task_loss 4.666 | contrastive_loss 0.29 | total 4003.4 | n_correct 2686.2 | ppl 5.35 | accuracy 67.098 | uer 17.15 | wer 19.052 | raw_wer 19.052 | bleu 22.68 | wps 2178.7 | wpb 4003.4 | bsz 141.8 | num_updates 47147 | best_bleu 22.82
2023-08-16 02:28:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 47147 updates
2023-08-16 02:28:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.6808.pt
2023-08-16 02:28:59 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.6808.pt
2023-08-16 02:29:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.6808.pt (epoch 32 @ 47147 updates, score 22.68) (writing took 22.3280616607517 seconds)
2023-08-16 02:29:18 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-08-16 02:29:18 | INFO | train | epoch 032 | loss 1.883 | trans_loss 4.737 | nll_loss 1.928 | w2v_ctc_loss 0.606 | task_loss 1.409 | contrastive_loss 0.112 | total 4137.03 | n_correct 2884 | ppl 3.81 | accuracy 69.712 | wps 10222.5 | ups 1.24 | wpb 8274.1 | bsz 305.1 | num_updates 47147 | lr 6.5131e-05 | gnorm 0.533 | clip 0 | loss_scale 8 | train_wall 1077 | gb_free 16.1 | wall 40778
2023-08-16 02:29:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 02:29:19 | INFO | fairseq.trainer | begin training epoch 33
2023-08-16 02:29:19 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 02:30:06 | INFO | train_inner | epoch 033:     53 / 1474 loss=1.888, trans_loss=4.738, nll_loss=1.931, w2v_ctc_loss=0.604, task_loss=1.337, contrastive_loss=0.169, total=4146.91, n_correct=2889.51, ppl=3.81, accuracy=69.679, wps=6477.4, ups=0.78, wpb=8293.8, bsz=319.7, num_updates=47200, lr=6.50945e-05, gnorm=0.535, clip=0, loss_scale=8, train_wall=73, gb_free=15.7, wall=40825
2023-08-16 02:31:19 | INFO | train_inner | epoch 033:    153 / 1474 loss=1.868, trans_loss=4.72, nll_loss=1.905, w2v_ctc_loss=0.593, task_loss=1.509, contrastive_loss=0.057, total=4073.36, n_correct=2854.69, ppl=3.75, accuracy=70.082, wps=11175.1, ups=1.37, wpb=8146.7, bsz=285.1, num_updates=47300, lr=6.50256e-05, gnorm=0.534, clip=0, loss_scale=8, train_wall=72, gb_free=16.4, wall=40898
2023-08-16 02:32:33 | INFO | train_inner | epoch 033:    253 / 1474 loss=1.886, trans_loss=4.724, nll_loss=1.913, w2v_ctc_loss=0.595, task_loss=1.194, contrastive_loss=0.233, total=4283.64, n_correct=3001.16, ppl=3.76, accuracy=70.061, wps=11560.6, ups=1.35, wpb=8567.3, bsz=347.6, num_updates=47400, lr=6.4957e-05, gnorm=0.528, clip=0, loss_scale=8, train_wall=74, gb_free=16, wall=40972
2023-08-16 02:33:47 | INFO | train_inner | epoch 033:    353 / 1474 loss=1.878, trans_loss=4.728, nll_loss=1.917, w2v_ctc_loss=0.608, task_loss=1.433, contrastive_loss=0.083, total=4131.27, n_correct=2886.03, ppl=3.78, accuracy=69.858, wps=11214.2, ups=1.36, wpb=8262.5, bsz=302.3, num_updates=47500, lr=6.48886e-05, gnorm=0.532, clip=0, loss_scale=8, train_wall=73, gb_free=15.7, wall=41046
2023-08-16 02:35:00 | INFO | train_inner | epoch 033:    453 / 1474 loss=1.865, trans_loss=4.719, nll_loss=1.905, w2v_ctc_loss=0.593, task_loss=1.337, contrastive_loss=0.064, total=4135.1, n_correct=2899.72, ppl=3.75, accuracy=70.125, wps=11328.1, ups=1.37, wpb=8270.2, bsz=309.7, num_updates=47600, lr=6.48204e-05, gnorm=0.531, clip=0, loss_scale=8, train_wall=73, gb_free=15.1, wall=41119
2023-08-16 02:36:14 | INFO | train_inner | epoch 033:    553 / 1474 loss=1.881, trans_loss=4.733, nll_loss=1.922, w2v_ctc_loss=0.607, task_loss=1.463, contrastive_loss=0.086, total=4132.78, n_correct=2882.29, ppl=3.79, accuracy=69.742, wps=11172.4, ups=1.35, wpb=8265.6, bsz=294.2, num_updates=47700, lr=6.47524e-05, gnorm=0.542, clip=0, loss_scale=8, train_wall=73, gb_free=17.4, wall=41193
2023-08-16 02:37:28 | INFO | train_inner | epoch 033:    653 / 1474 loss=1.888, trans_loss=4.744, nll_loss=1.937, w2v_ctc_loss=0.608, task_loss=1.447, contrastive_loss=0.12, total=4156.26, n_correct=2890.77, ppl=3.83, accuracy=69.552, wps=11172.7, ups=1.34, wpb=8312.5, bsz=300.7, num_updates=47800, lr=6.46846e-05, gnorm=0.531, clip=0, loss_scale=8, train_wall=74, gb_free=15.7, wall=41267
2023-08-16 02:38:42 | INFO | train_inner | epoch 033:    753 / 1474 loss=1.886, trans_loss=4.738, nll_loss=1.929, w2v_ctc_loss=0.62, task_loss=1.519, contrastive_loss=0.065, total=4074.99, n_correct=2839.31, ppl=3.81, accuracy=69.676, wps=11094, ups=1.36, wpb=8150, bsz=288.2, num_updates=47900, lr=6.46171e-05, gnorm=0.532, clip=0, loss_scale=8, train_wall=73, gb_free=15.8, wall=41341
2023-08-16 02:39:55 | INFO | train_inner | epoch 033:    853 / 1474 loss=1.872, trans_loss=4.727, nll_loss=1.916, w2v_ctc_loss=0.592, task_loss=1.345, contrastive_loss=0.136, total=4127.6, n_correct=2890.23, ppl=3.77, accuracy=70.022, wps=11277.4, ups=1.37, wpb=8255.2, bsz=315.3, num_updates=48000, lr=6.45497e-05, gnorm=0.526, clip=0, loss_scale=8, train_wall=73, gb_free=15.6, wall=41414
2023-08-16 02:39:55 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 02:40:18 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 3.963 | trans_loss 5.165 | nll_loss 2.422 | w2v_ctc_loss 1.348 | task_loss 4.675 | contrastive_loss 0.294 | total 4003.4 | n_correct 2676.8 | ppl 5.36 | accuracy 66.863 | uer 17.206 | wer 19.019 | raw_wer 19.019 | bleu 22.5 | wps 2137.3 | wpb 4003.4 | bsz 141.8 | num_updates 48000 | best_bleu 22.82
2023-08-16 02:40:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48000 updates
2023-08-16 02:40:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_33_48000.pt
2023-08-16 02:40:22 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_33_48000.pt
2023-08-16 02:40:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_33_48000.pt (epoch 33 @ 48000 updates, score 22.5) (writing took 38.981714284047484 seconds)
2023-08-16 02:42:12 | INFO | train_inner | epoch 033:    953 / 1474 loss=1.875, trans_loss=4.728, nll_loss=1.917, w2v_ctc_loss=0.604, task_loss=1.394, contrastive_loss=0.077, total=4157.37, n_correct=2905.51, ppl=3.78, accuracy=69.888, wps=6073.3, ups=0.73, wpb=8314.7, bsz=310.1, num_updates=48100, lr=6.44826e-05, gnorm=0.525, clip=0, loss_scale=8, train_wall=73, gb_free=17.1, wall=41551
2023-08-16 02:43:26 | INFO | train_inner | epoch 033:   1053 / 1474 loss=1.889, trans_loss=4.732, nll_loss=1.922, w2v_ctc_loss=0.605, task_loss=1.423, contrastive_loss=0.179, total=4134.8, n_correct=2882.83, ppl=3.79, accuracy=69.721, wps=11170, ups=1.35, wpb=8269.6, bsz=306, num_updates=48200, lr=6.44157e-05, gnorm=0.544, clip=0, loss_scale=8, train_wall=74, gb_free=15.3, wall=41625
2023-08-16 02:44:40 | INFO | train_inner | epoch 033:   1153 / 1474 loss=1.889, trans_loss=4.742, nll_loss=1.936, w2v_ctc_loss=0.602, task_loss=1.408, contrastive_loss=0.168, total=4181.58, n_correct=2909.24, ppl=3.83, accuracy=69.573, wps=11317.9, ups=1.35, wpb=8363.2, bsz=310, num_updates=48300, lr=6.43489e-05, gnorm=0.542, clip=0, loss_scale=8, train_wall=73, gb_free=14.8, wall=41699
2023-08-16 02:45:53 | INFO | train_inner | epoch 033:   1253 / 1474 loss=1.878, trans_loss=4.73, nll_loss=1.92, w2v_ctc_loss=0.607, task_loss=1.475, contrastive_loss=0.069, total=4115.76, n_correct=2877.28, ppl=3.78, accuracy=69.909, wps=11191.5, ups=1.36, wpb=8231.5, bsz=294.7, num_updates=48400, lr=6.42824e-05, gnorm=0.538, clip=0, loss_scale=8, train_wall=73, gb_free=16.3, wall=41773
2023-08-16 02:47:07 | INFO | train_inner | epoch 033:   1353 / 1474 loss=1.876, trans_loss=4.733, nll_loss=1.923, w2v_ctc_loss=0.604, task_loss=1.386, contrastive_loss=0.088, total=4120.69, n_correct=2878.62, ppl=3.79, accuracy=69.858, wps=11136.6, ups=1.35, wpb=8241.4, bsz=311.9, num_updates=48500, lr=6.42161e-05, gnorm=0.535, clip=0, loss_scale=8, train_wall=73, gb_free=16.4, wall=41847
2023-08-16 02:48:21 | INFO | train_inner | epoch 033:   1453 / 1474 loss=1.89, trans_loss=4.734, nll_loss=1.925, w2v_ctc_loss=0.601, task_loss=1.403, contrastive_loss=0.236, total=4125.28, n_correct=2876.91, ppl=3.8, accuracy=69.739, wps=11252.5, ups=1.36, wpb=8250.6, bsz=308.7, num_updates=48600, lr=6.415e-05, gnorm=0.525, clip=0, loss_scale=8, train_wall=73, gb_free=16.6, wall=41920
2023-08-16 02:48:36 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 02:48:59 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 3.975 | trans_loss 5.167 | nll_loss 2.425 | w2v_ctc_loss 1.384 | task_loss 4.672 | contrastive_loss 0.293 | total 4003.4 | n_correct 2677.9 | ppl 5.37 | accuracy 66.891 | uer 17.198 | wer 19.026 | raw_wer 19.026 | bleu 22.45 | wps 2245.3 | wpb 4003.4 | bsz 141.8 | num_updates 48621 | best_bleu 22.82
2023-08-16 02:48:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48621 updates
2023-08-16 02:48:59 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.4503.pt
2023-08-16 02:49:02 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.4503.pt
2023-08-16 02:49:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint.best_bleu_22.4503.pt (epoch 33 @ 48621 updates, score 22.45) (writing took 23.301420455798507 seconds)
2023-08-16 02:49:22 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-08-16 02:49:22 | INFO | train | epoch 033 | loss 1.88 | trans_loss 4.731 | nll_loss 1.92 | w2v_ctc_loss 0.603 | task_loss 1.408 | contrastive_loss 0.118 | total 4138.65 | n_correct 2890.67 | ppl 3.78 | accuracy 69.846 | wps 10132.5 | ups 1.22 | wpb 8277.3 | bsz 305.7 | num_updates 48621 | lr 6.41362e-05 | gnorm 0.533 | clip 0 | loss_scale 8 | train_wall 1078 | gb_free 17.5 | wall 41982
2023-08-16 02:49:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 02:49:23 | INFO | fairseq.trainer | begin training epoch 34
2023-08-16 02:49:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 02:50:29 | INFO | train_inner | epoch 034:     79 / 1474 loss=1.868, trans_loss=4.717, nll_loss=1.903, w2v_ctc_loss=0.598, task_loss=1.391, contrastive_loss=0.072, total=4131.47, n_correct=2894.26, ppl=3.74, accuracy=70.054, wps=6436.9, ups=0.78, wpb=8262.9, bsz=301.7, num_updates=48700, lr=6.40841e-05, gnorm=0.531, clip=0, loss_scale=8, train_wall=73, gb_free=16.3, wall=42048
2023-08-16 02:51:42 | INFO | train_inner | epoch 034:    179 / 1474 loss=1.863, trans_loss=4.708, nll_loss=1.89, w2v_ctc_loss=0.593, task_loss=1.469, contrastive_loss=0.072, total=4065.88, n_correct=2862.41, ppl=3.71, accuracy=70.401, wps=11083.7, ups=1.36, wpb=8131.8, bsz=295.1, num_updates=48800, lr=6.40184e-05, gnorm=0.536, clip=0, loss_scale=8, train_wall=73, gb_free=15.6, wall=42122
2023-08-16 02:52:57 | INFO | train_inner | epoch 034:    279 / 1474 loss=1.891, trans_loss=4.727, nll_loss=1.915, w2v_ctc_loss=0.593, task_loss=1.31, contrastive_loss=0.281, total=4246.3, n_correct=2966.34, ppl=3.77, accuracy=69.857, wps=11353, ups=1.34, wpb=8492.6, bsz=328.7, num_updates=48900, lr=6.39529e-05, gnorm=0.538, clip=0, loss_scale=8, train_wall=74, gb_free=17.5, wall=42196
2023-08-16 02:54:11 | INFO | train_inner | epoch 034:    379 / 1474 loss=1.869, trans_loss=4.71, nll_loss=1.894, w2v_ctc_loss=0.585, task_loss=1.334, contrastive_loss=0.171, total=4156.17, n_correct=2920.6, ppl=3.72, accuracy=70.271, wps=11249.8, ups=1.35, wpb=8312.3, bsz=316.7, num_updates=49000, lr=6.38877e-05, gnorm=0.52, clip=0, loss_scale=16, train_wall=73, gb_free=17.4, wall=42270
2023-08-16 02:55:25 | INFO | train_inner | epoch 034:    479 / 1474 loss=1.877, trans_loss=4.723, nll_loss=1.91, w2v_ctc_loss=0.612, task_loss=1.541, contrastive_loss=0.066, total=4070.55, n_correct=2846.43, ppl=3.76, accuracy=69.927, wps=11043.4, ups=1.36, wpb=8141.1, bsz=284.6, num_updates=49100, lr=6.38226e-05, gnorm=0.547, clip=0, loss_scale=16, train_wall=73, gb_free=17.1, wall=42344
2023-08-16 02:56:38 | INFO | train_inner | epoch 034:    579 / 1474 loss=1.865, trans_loss=4.712, nll_loss=1.895, w2v_ctc_loss=0.594, task_loss=1.424, contrastive_loss=0.068, total=4119.38, n_correct=2894.52, ppl=3.72, accuracy=70.266, wps=11227.4, ups=1.36, wpb=8238.8, bsz=300.3, num_updates=49200, lr=6.37577e-05, gnorm=0.537, clip=0, loss_scale=16, train_wall=73, gb_free=12.6, wall=42417
2023-08-16 02:57:52 | INFO | train_inner | epoch 034:    679 / 1474 loss=1.866, trans_loss=4.721, nll_loss=1.907, w2v_ctc_loss=0.594, task_loss=1.424, contrastive_loss=0.062, total=4124.83, n_correct=2893.49, ppl=3.75, accuracy=70.148, wps=11198.4, ups=1.36, wpb=8249.7, bsz=300.2, num_updates=49300, lr=6.3693e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=73, gb_free=13.9, wall=42491
2023-08-16 02:59:06 | INFO | train_inner | epoch 034:    779 / 1474 loss=1.879, trans_loss=4.737, nll_loss=1.929, w2v_ctc_loss=0.591, task_loss=1.475, contrastive_loss=0.132, total=4082.07, n_correct=2850.91, ppl=3.81, accuracy=69.84, wps=11013.8, ups=1.35, wpb=8164.1, bsz=295, num_updates=49400, lr=6.36285e-05, gnorm=0.545, clip=0, loss_scale=16, train_wall=74, gb_free=15.3, wall=42565
2023-08-16 03:00:20 | INFO | train_inner | epoch 034:    879 / 1474 loss=1.875, trans_loss=4.728, nll_loss=1.917, w2v_ctc_loss=0.599, task_loss=1.482, contrastive_loss=0.09, total=4100.9, n_correct=2866.97, ppl=3.78, accuracy=69.911, wps=11124.1, ups=1.36, wpb=8201.8, bsz=296.6, num_updates=49500, lr=6.35642e-05, gnorm=0.551, clip=0, loss_scale=16, train_wall=73, gb_free=11.7, wall=42639
2023-08-16 03:01:33 | INFO | train_inner | epoch 034:    979 / 1474 loss=1.877, trans_loss=4.727, nll_loss=1.916, w2v_ctc_loss=0.608, task_loss=1.383, contrastive_loss=0.085, total=4168.39, n_correct=2911.08, ppl=3.77, accuracy=69.837, wps=11314.6, ups=1.36, wpb=8336.8, bsz=311.9, num_updates=49600, lr=6.35001e-05, gnorm=0.551, clip=0, loss_scale=16, train_wall=73, gb_free=15.5, wall=42713
2023-08-16 03:02:47 | INFO | train_inner | epoch 034:   1079 / 1474 loss=1.869, trans_loss=4.723, nll_loss=1.909, w2v_ctc_loss=0.601, task_loss=1.364, contrastive_loss=0.068, total=4150.57, n_correct=2910.36, ppl=3.76, accuracy=70.12, wps=11234.8, ups=1.35, wpb=8301.1, bsz=308.5, num_updates=49700, lr=6.34361e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=73, gb_free=16.5, wall=42787
2023-08-16 03:04:01 | INFO | train_inner | epoch 034:   1179 / 1474 loss=1.875, trans_loss=4.728, nll_loss=1.917, w2v_ctc_loss=0.601, task_loss=1.455, contrastive_loss=0.081, total=4098.77, n_correct=2863.9, ppl=3.78, accuracy=69.872, wps=11054.4, ups=1.35, wpb=8197.5, bsz=297.1, num_updates=49800, lr=6.33724e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=74, gb_free=16.6, wall=42861
2023-08-16 03:05:15 | INFO | train_inner | epoch 034:   1279 / 1474 loss=1.871, trans_loss=4.722, nll_loss=1.909, w2v_ctc_loss=0.6, task_loss=1.417, contrastive_loss=0.065, total=4150.54, n_correct=2903.73, ppl=3.76, accuracy=69.96, wps=11283.8, ups=1.36, wpb=8301.1, bsz=301, num_updates=49900, lr=6.33089e-05, gnorm=0.532, clip=0, loss_scale=16, train_wall=73, gb_free=16.8, wall=42934
2023-08-16 03:06:29 | INFO | train_inner | epoch 034:   1379 / 1474 loss=1.884, trans_loss=4.733, nll_loss=1.922, w2v_ctc_loss=0.609, task_loss=1.346, contrastive_loss=0.13, total=4196.91, n_correct=2928.41, ppl=3.79, accuracy=69.775, wps=11266.1, ups=1.34, wpb=8393.8, bsz=321.4, num_updates=50000, lr=6.32456e-05, gnorm=0.544, clip=0, loss_scale=16, train_wall=74, gb_free=15.7, wall=43009
2023-08-16 03:06:29 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2023-08-16 03:06:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 03:06:53 | INFO | dev_st | epoch 034 | valid on 'dev_st' subset | loss 3.962 | trans_loss 5.166 | nll_loss 2.424 | w2v_ctc_loss 1.339 | task_loss 4.676 | contrastive_loss 0.298 | total 4003.4 | n_correct 2683.5 | ppl 5.37 | accuracy 67.031 | uer 17.331 | wer 19.328 | raw_wer 19.328 | bleu 22.59 | wps 2154.1 | wpb 4003.4 | bsz 141.8 | num_updates 50000 | best_bleu 22.82
2023-08-16 03:06:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 50000 updates
2023-08-16 03:06:53 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_34_50000.pt
2023-08-16 03:06:57 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_34_50000.pt
2023-08-16 03:07:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0815_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_34_50000.pt (epoch 34 @ 50000 updates, score 22.59) (writing took 42.110527120530605 seconds)
2023-08-16 03:07:36 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2023-08-16 03:07:36 | INFO | train | epoch 034 | loss 1.874 | trans_loss 4.723 | nll_loss 1.909 | w2v_ctc_loss 0.598 | task_loss 1.415 | contrastive_loss 0.105 | total 4133.04 | n_correct 2893.87 | ppl 3.76 | accuracy 70.018 | wps 10423.9 | ups 1.26 | wpb 8266.1 | bsz 304.2 | num_updates 50000 | lr 6.32456e-05 | gnorm 0.537 | clip 0 | loss_scale 16 | train_wall 1011 | gb_free 15.7 | wall 43075
2023-08-16 03:07:36 | INFO | fairseq_cli.train | done training in 43021.9 seconds
Exception in thread Thread-7:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-6:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    raise EOFError
EOFError
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1472 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
