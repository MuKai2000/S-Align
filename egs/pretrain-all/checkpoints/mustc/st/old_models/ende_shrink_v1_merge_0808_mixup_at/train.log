2023-08-08 00:04:44 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:11246
2023-08-08 00:04:44 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:11246
2023-08-08 00:04:44 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:11246
2023-08-08 00:04:44 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:11246
2023-08-08 00:04:44 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:11246
2023-08-08 00:04:44 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:11246
2023-08-08 00:04:44 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:11246
2023-08-08 00:04:44 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:11246
2023-08-08 00:04:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-08 00:04:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-08 00:04:45 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-08 00:04:45 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-08 00:04:45 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-08 00:04:45 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-08 00:04:45 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-08 00:04:45 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-08 00:04:45 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-08 00:04:45 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-08 00:04:45 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-08 00:04:45 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-08 00:04:45 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-08 00:04:45 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-08 00:04:45 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-08 00:04:45 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-08 00:04:45 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-08 00:04:45 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-08 00:04:45 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-08 00:04:45 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-08 00:04:45 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-08 00:04:45 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-08 00:04:45 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-08 00:04:45 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-08 00:04:49 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11246', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_rate=0.5, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_rate=0.5, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_rate=0.5, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-08 00:04:49 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-08-08 00:04:49 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-08-08 00:04:49 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-08 00:04:49 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-08-08 00:04:49 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-08-08 00:04:54 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-08 00:04:54 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-08 00:04:54 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-08 00:04:57 | INFO | root | load pretrained hubert
2023-08-08 00:04:59 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-08-08 00:04:59 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-08-08 00:05:02 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-08-08 00:05:02 | INFO | root | share the sematic adapter and textual encoder
2023-08-08 00:05:02 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-08 00:05:02 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-08-08 00:05:02 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-08 00:05:02 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-08-08 00:05:02 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-08-08 00:05:02 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-08 00:05:02 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-08 00:05:02 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-08 00:05:02 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-08 00:05:02 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-08 00:05:05 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-08 00:05:05 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-08 00:05:05 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-08 00:05:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-08 00:05:05 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-08 00:05:05 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-08 00:05:05 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-08 00:05:05 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-08 00:05:05 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-08 00:05:05 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-08 00:05:05 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-08 00:05:05 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-08 00:05:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-08 00:05:05 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-08 00:05:05 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-08 00:05:05 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_last.pt
2023-08-08 00:05:05 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_last.pt
2023-08-08 00:05:05 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-08 00:05:05 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-08 00:05:05 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-08 00:05:05 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-08 00:05:07 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-08 00:05:08 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-08 00:05:58 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-08 00:05:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 00:05:58 | INFO | fairseq.trainer | begin training epoch 1
2023-08-08 00:05:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 00:07:15 | INFO | train_inner | epoch 001:    100 / 1474 loss=19.136, trans_loss=5.598, nll_loss=4.163, w2v_ctc_loss=22.485, task_loss=1.749, contrastive_loss=3.325, total=4207.04, n_correct=209.24, ppl=17.91, accuracy=4.974, wps=19180, ups=1.53, wpb=12551.1, bsz=471.4, num_updates=100, lr=4.098e-06, gnorm=0.884, clip=0, loss_scale=128, train_wall=68, gb_free=19.5, wall=130
2023-08-08 00:08:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-08 00:08:20 | INFO | train_inner | epoch 001:    201 / 1474 loss=16.984, trans_loss=5.478, nll_loss=4.065, w2v_ctc_loss=19.348, task_loss=1.707, contrastive_loss=3.277, total=4124.14, n_correct=223.48, ppl=16.74, accuracy=5.419, wps=19031.2, ups=1.55, wpb=12313.4, bsz=461, num_updates=200, lr=8.096e-06, gnorm=3.628, clip=0, loss_scale=64, train_wall=64, gb_free=19.2, wall=195
2023-08-08 00:09:23 | INFO | train_inner | epoch 001:    301 / 1474 loss=10.073, trans_loss=5.487, nll_loss=4.13, w2v_ctc_loss=8.756, task_loss=1.706, contrastive_loss=3.202, total=4079.62, n_correct=206.26, ppl=17.51, accuracy=5.056, wps=19099.7, ups=1.57, wpb=12186.7, bsz=438.2, num_updates=300, lr=1.2094e-05, gnorm=4.593, clip=0, loss_scale=64, train_wall=63, gb_free=19.9, wall=258
2023-08-08 00:10:28 | INFO | train_inner | epoch 001:    401 / 1474 loss=8.837, trans_loss=5.516, nll_loss=4.189, w2v_ctc_loss=6.8, task_loss=1.496, contrastive_loss=3.236, total=4174.14, n_correct=194.88, ppl=18.24, accuracy=4.669, wps=19460.9, ups=1.56, wpb=12463.5, bsz=460.4, num_updates=400, lr=1.6092e-05, gnorm=2.932, clip=0, loss_scale=64, train_wall=63, gb_free=18.9, wall=322
2023-08-08 00:11:31 | INFO | train_inner | epoch 001:    501 / 1474 loss=8.406, trans_loss=5.494, nll_loss=4.177, w2v_ctc_loss=6.167, task_loss=1.369, contrastive_loss=3.229, total=4176.18, n_correct=190.05, ppl=18.09, accuracy=4.551, wps=19518.6, ups=1.56, wpb=12479.7, bsz=477.4, num_updates=500, lr=2.009e-05, gnorm=1.406, clip=0, loss_scale=64, train_wall=63, gb_free=19.2, wall=386
2023-08-08 00:12:36 | INFO | train_inner | epoch 001:    601 / 1474 loss=8.154, trans_loss=5.523, nll_loss=4.212, w2v_ctc_loss=5.8, task_loss=1.275, contrastive_loss=3.282, total=4147.79, n_correct=187.87, ppl=18.53, accuracy=4.529, wps=19149.6, ups=1.55, wpb=12371.6, bsz=484.2, num_updates=600, lr=2.4088e-05, gnorm=0.72, clip=0, loss_scale=64, train_wall=64, gb_free=18.9, wall=451
2023-08-08 00:13:39 | INFO | train_inner | epoch 001:    701 / 1474 loss=7.994, trans_loss=5.519, nll_loss=4.213, w2v_ctc_loss=5.682, task_loss=1.325, contrastive_loss=3.031, total=4152.1, n_correct=198.95, ppl=18.54, accuracy=4.792, wps=19638.2, ups=1.58, wpb=12395.5, bsz=456.2, num_updates=700, lr=2.8086e-05, gnorm=0.554, clip=0, loss_scale=64, train_wall=62, gb_free=19.5, wall=514
2023-08-08 00:14:43 | INFO | train_inner | epoch 001:    801 / 1474 loss=7.718, trans_loss=5.453, nll_loss=4.143, w2v_ctc_loss=5.46, task_loss=1.281, contrastive_loss=2.939, total=4123.83, n_correct=245.53, ppl=17.66, accuracy=5.954, wps=19366.1, ups=1.57, wpb=12306.1, bsz=464.1, num_updates=800, lr=3.2084e-05, gnorm=0.828, clip=0, loss_scale=64, train_wall=63, gb_free=19.2, wall=578
2023-08-08 00:15:46 | INFO | train_inner | epoch 001:    901 / 1474 loss=7.463, trans_loss=5.421, nll_loss=4.115, w2v_ctc_loss=5.281, task_loss=1.302, contrastive_loss=2.7, total=4163.61, n_correct=269.28, ppl=17.33, accuracy=6.467, wps=19635.4, ups=1.58, wpb=12433.9, bsz=457.1, num_updates=900, lr=3.6082e-05, gnorm=1.374, clip=0, loss_scale=64, train_wall=63, gb_free=18.9, wall=641
2023-08-08 00:16:51 | INFO | train_inner | epoch 001:   1001 / 1474 loss=7.205, trans_loss=5.398, nll_loss=4.097, w2v_ctc_loss=5.071, task_loss=1.311, contrastive_loss=2.55, total=4135.34, n_correct=289.03, ppl=17.11, accuracy=6.989, wps=19055.1, ups=1.54, wpb=12353.2, bsz=456.8, num_updates=1000, lr=4.008e-05, gnorm=1.435, clip=0, loss_scale=64, train_wall=64, gb_free=19, wall=706
2023-08-08 00:17:54 | INFO | train_inner | epoch 001:   1101 / 1474 loss=6.937, trans_loss=5.387, nll_loss=4.086, w2v_ctc_loss=4.871, task_loss=1.322, contrastive_loss=2.327, total=4147.38, n_correct=312.04, ppl=16.98, accuracy=7.524, wps=19660.3, ups=1.59, wpb=12367, bsz=454.9, num_updates=1100, lr=4.4078e-05, gnorm=1.668, clip=0, loss_scale=64, train_wall=62, gb_free=18.9, wall=769
2023-08-08 00:18:57 | INFO | train_inner | epoch 001:   1201 / 1474 loss=6.714, trans_loss=5.368, nll_loss=4.069, w2v_ctc_loss=4.702, task_loss=1.377, contrastive_loss=2.123, total=4139.9, n_correct=318.3, ppl=16.79, accuracy=7.689, wps=19508.6, ups=1.58, wpb=12366.5, bsz=440.1, num_updates=1200, lr=4.8076e-05, gnorm=1.761, clip=0, loss_scale=64, train_wall=63, gb_free=19, wall=832
2023-08-08 00:20:00 | INFO | train_inner | epoch 001:   1301 / 1474 loss=6.487, trans_loss=5.366, nll_loss=4.068, w2v_ctc_loss=4.501, task_loss=1.324, contrastive_loss=1.929, total=4046.58, n_correct=322.95, ppl=16.78, accuracy=7.981, wps=19218.1, ups=1.59, wpb=12081.6, bsz=439.3, num_updates=1300, lr=5.2074e-05, gnorm=1.725, clip=0, loss_scale=64, train_wall=62, gb_free=19.7, wall=895
2023-08-08 00:21:05 | INFO | train_inner | epoch 001:   1401 / 1474 loss=6.277, trans_loss=5.357, nll_loss=4.061, w2v_ctc_loss=4.294, task_loss=1.308, contrastive_loss=1.996, total=4133.18, n_correct=333.98, ppl=16.69, accuracy=8.08, wps=19038.5, ups=1.54, wpb=12350, bsz=455, num_updates=1400, lr=5.6072e-05, gnorm=1.624, clip=0, loss_scale=64, train_wall=64, gb_free=19.9, wall=960
2023-08-08 00:21:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 00:22:31 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 9.543 | trans_loss 10.908 | nll_loss 9.889 | w2v_ctc_loss 5.57 | task_loss 7.547 | contrastive_loss 2.339 | total 4003.4 | n_correct 386.6 | ppl 948.35 | accuracy 9.657 | uer 71.922 | wer 69.807 | raw_wer 69.807 | bleu 0.02 | wps 1172.8 | wpb 4003.4 | bsz 141.8 | num_updates 1473
2023-08-08 00:22:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1473 updates
2023-08-08 00:22:31 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 00:22:34 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 00:22:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt (epoch 1 @ 1473 updates, score 0.02) (writing took 5.908159300684929 seconds)
2023-08-08 00:22:36 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-08-08 00:22:37 | INFO | train | epoch 001 | loss 9.03 | trans_loss 5.45 | nll_loss 4.124 | w2v_ctc_loss 7.635 | task_loss 1.41 | contrastive_loss 2.755 | total 4138.55 | n_correct 254.474 | ppl 17.44 | accuracy 6.149 | wps 18439.4 | ups 1.49 | wpb 12355.5 | bsz 458.4 | num_updates 1473 | lr 5.89905e-05 | gnorm 1.789 | clip 0 | loss_scale 64 | train_wall 937 | gb_free 19.2 | wall 1051
2023-08-08 00:22:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 00:22:37 | INFO | fairseq.trainer | begin training epoch 2
2023-08-08 00:22:37 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 00:23:02 | INFO | train_inner | epoch 002:     27 / 1474 loss=6.082, trans_loss=5.351, nll_loss=4.05, w2v_ctc_loss=4.094, task_loss=1.246, contrastive_loss=1.837, total=4162.95, n_correct=339.78, ppl=16.56, accuracy=8.162, wps=10585.2, ups=0.85, wpb=12416.8, bsz=470.8, num_updates=1500, lr=6.007e-05, gnorm=1.63, clip=0, loss_scale=64, train_wall=63, gb_free=19.6, wall=1077
2023-08-08 00:24:06 | INFO | train_inner | epoch 002:    127 / 1474 loss=5.916, trans_loss=5.347, nll_loss=4.043, w2v_ctc_loss=3.972, task_loss=1.33, contrastive_loss=1.632, total=4155.98, n_correct=340.82, ppl=16.48, accuracy=8.201, wps=19595.6, ups=1.58, wpb=12394.8, bsz=451.6, num_updates=1600, lr=6.4068e-05, gnorm=1.707, clip=0, loss_scale=64, train_wall=63, gb_free=18.9, wall=1140
2023-08-08 00:25:09 | INFO | train_inner | epoch 002:    227 / 1474 loss=5.745, trans_loss=5.326, nll_loss=4.023, w2v_ctc_loss=3.771, task_loss=1.153, contrastive_loss=1.657, total=4179.21, n_correct=350.03, ppl=16.26, accuracy=8.376, wps=19622.3, ups=1.57, wpb=12484.6, bsz=488.9, num_updates=1700, lr=6.8066e-05, gnorm=1.488, clip=0, loss_scale=64, train_wall=63, gb_free=19, wall=1204
2023-08-08 00:26:13 | INFO | train_inner | epoch 002:    327 / 1474 loss=5.581, trans_loss=5.325, nll_loss=4.017, w2v_ctc_loss=3.677, task_loss=1.325, contrastive_loss=1.364, total=4146.1, n_correct=354.34, ppl=16.19, accuracy=8.546, wps=19365.4, ups=1.56, wpb=12374.1, bsz=447.8, num_updates=1800, lr=7.2064e-05, gnorm=1.374, clip=0, loss_scale=64, train_wall=63, gb_free=18.8, wall=1268
2023-08-08 00:27:16 | INFO | train_inner | epoch 002:    427 / 1474 loss=5.437, trans_loss=5.316, nll_loss=4.011, w2v_ctc_loss=3.575, task_loss=1.456, contrastive_loss=1.185, total=4037.99, n_correct=345.4, ppl=16.12, accuracy=8.554, wps=19113.1, ups=1.58, wpb=12069.2, bsz=415.4, num_updates=1900, lr=7.6062e-05, gnorm=1.412, clip=0, loss_scale=64, train_wall=63, gb_free=19, wall=1331
2023-08-08 00:28:19 | INFO | train_inner | epoch 002:    527 / 1474 loss=5.326, trans_loss=5.306, nll_loss=3.994, w2v_ctc_loss=3.413, task_loss=1.266, contrastive_loss=1.277, total=4176.97, n_correct=363.75, ppl=15.93, accuracy=8.708, wps=19799.1, ups=1.59, wpb=12463.5, bsz=468.6, num_updates=2000, lr=8.006e-05, gnorm=1.255, clip=0, loss_scale=64, train_wall=62, gb_free=19.6, wall=1394
2023-08-08 00:28:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 00:28:58 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 8.918 | trans_loss 10.748 | nll_loss 9.672 | w2v_ctc_loss 4.453 | task_loss 7.546 | contrastive_loss 1.6 | total 4003.4 | n_correct 409.8 | ppl 815.65 | accuracy 10.236 | uer 61.126 | wer 59.118 | raw_wer 59.118 | bleu 0.04 | wps 1140.5 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_bleu 0.04
2023-08-08 00:28:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-08-08 00:28:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_2_2000.pt
2023-08-08 00:29:02 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_2_2000.pt
2023-08-08 00:29:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.04) (writing took 25.46343152411282 seconds)
2023-08-08 00:30:28 | INFO | train_inner | epoch 002:    627 / 1474 loss=5.191, trans_loss=5.298, nll_loss=3.984, w2v_ctc_loss=3.308, task_loss=1.309, contrastive_loss=1.076, total=4126.49, n_correct=367.56, ppl=15.82, accuracy=8.907, wps=9587.6, ups=0.78, wpb=12314.9, bsz=445.5, num_updates=2100, lr=8.4058e-05, gnorm=1.14, clip=0, loss_scale=64, train_wall=63, gb_free=19.2, wall=1522
2023-08-08 00:31:31 | INFO | train_inner | epoch 002:    727 / 1474 loss=5.118, trans_loss=5.28, nll_loss=3.965, w2v_ctc_loss=3.224, task_loss=1.283, contrastive_loss=1.177, total=4149.06, n_correct=379.21, ppl=15.62, accuracy=9.14, wps=19482.4, ups=1.57, wpb=12386.4, bsz=465.4, num_updates=2200, lr=8.8056e-05, gnorm=1.122, clip=0, loss_scale=64, train_wall=63, gb_free=19.2, wall=1586
2023-08-08 00:32:34 | INFO | train_inner | epoch 002:    827 / 1474 loss=5.029, trans_loss=5.266, nll_loss=3.949, w2v_ctc_loss=3.155, task_loss=1.317, contrastive_loss=1.124, total=4175.4, n_correct=386.61, ppl=15.45, accuracy=9.259, wps=19727.8, ups=1.58, wpb=12471.9, bsz=460.9, num_updates=2300, lr=9.2054e-05, gnorm=0.99, clip=0, loss_scale=128, train_wall=63, gb_free=19.8, wall=1649
2023-08-08 00:33:38 | INFO | train_inner | epoch 002:    927 / 1474 loss=4.938, trans_loss=5.253, nll_loss=3.931, w2v_ctc_loss=3.063, task_loss=1.344, contrastive_loss=1.108, total=4104.2, n_correct=383.27, ppl=15.25, accuracy=9.338, wps=19397.2, ups=1.58, wpb=12253.1, bsz=445.9, num_updates=2400, lr=9.6052e-05, gnorm=0.996, clip=0, loss_scale=128, train_wall=63, gb_free=19, wall=1712
2023-08-08 00:34:41 | INFO | train_inner | epoch 002:   1027 / 1474 loss=4.852, trans_loss=5.245, nll_loss=3.924, w2v_ctc_loss=2.994, task_loss=1.305, contrastive_loss=0.957, total=4102.5, n_correct=389.93, ppl=15.17, accuracy=9.505, wps=19434.7, ups=1.59, wpb=12251.2, bsz=456.3, num_updates=2500, lr=0.00010005, gnorm=0.87, clip=0, loss_scale=128, train_wall=63, gb_free=19.2, wall=1776
2023-08-08 00:35:45 | INFO | train_inner | epoch 002:   1127 / 1474 loss=4.809, trans_loss=5.238, nll_loss=3.913, w2v_ctc_loss=2.905, task_loss=1.187, contrastive_loss=1.169, total=4187.61, n_correct=403.15, ppl=15.07, accuracy=9.627, wps=19376.2, ups=1.55, wpb=12495.7, bsz=487.1, num_updates=2600, lr=0.000104048, gnorm=0.882, clip=0, loss_scale=128, train_wall=64, gb_free=19.5, wall=1840
2023-08-08 00:36:49 | INFO | train_inner | epoch 002:   1227 / 1474 loss=4.753, trans_loss=5.227, nll_loss=3.9, w2v_ctc_loss=2.863, task_loss=1.194, contrastive_loss=1.091, total=4221.06, n_correct=419.04, ppl=14.93, accuracy=9.927, wps=19749.4, ups=1.57, wpb=12596.1, bsz=492.8, num_updates=2700, lr=0.000108046, gnorm=0.792, clip=0, loss_scale=128, train_wall=63, gb_free=19.5, wall=1904
2023-08-08 00:37:51 | INFO | train_inner | epoch 002:   1327 / 1474 loss=4.661, trans_loss=5.218, nll_loss=3.893, w2v_ctc_loss=2.829, task_loss=1.259, contrastive_loss=0.798, total=4157.86, n_correct=419.98, ppl=14.86, accuracy=10.101, wps=19862.2, ups=1.6, wpb=12425.5, bsz=460.7, num_updates=2800, lr=0.000112044, gnorm=0.752, clip=0, loss_scale=128, train_wall=62, gb_free=19.5, wall=1966
2023-08-08 00:38:55 | INFO | train_inner | epoch 002:   1427 / 1474 loss=4.626, trans_loss=5.224, nll_loss=3.9, w2v_ctc_loss=2.79, task_loss=1.414, contrastive_loss=0.888, total=4054.34, n_correct=404.05, ppl=14.93, accuracy=9.966, wps=19025.8, ups=1.57, wpb=12107, bsz=438.8, num_updates=2900, lr=0.000116042, gnorm=0.707, clip=0, loss_scale=128, train_wall=63, gb_free=19.4, wall=2030
2023-08-08 00:39:25 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 00:40:04 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 8.15 | trans_loss 10.226 | nll_loss 9.036 | w2v_ctc_loss 3.574 | task_loss 7.547 | contrastive_loss 0.936 | total 4003.4 | n_correct 507.6 | ppl 524.77 | accuracy 12.679 | uer 51.496 | wer 50.375 | raw_wer 50.375 | bleu 0.13 | wps 1163.4 | wpb 4003.4 | bsz 141.8 | num_updates 2947 | best_bleu 0.13
2023-08-08 00:40:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2947 updates
2023-08-08 00:40:04 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 00:40:17 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 00:40:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt (epoch 2 @ 2947 updates, score 0.13) (writing took 23.729611694812775 seconds)
2023-08-08 00:40:28 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-08-08 00:40:28 | INFO | train | epoch 002 | loss 5.14 | trans_loss 5.276 | nll_loss 3.96 | w2v_ctc_loss 3.251 | task_loss 1.292 | contrastive_loss 1.181 | total 4138.65 | n_correct 379.305 | ppl 15.56 | accuracy 9.165 | wps 16997.4 | ups 1.38 | wpb 12355.8 | bsz 458.5 | num_updates 2947 | lr 0.000117921 | gnorm 1.101 | clip 0 | loss_scale 128 | train_wall 928 | gb_free 19.3 | wall 2123
2023-08-08 00:40:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 00:40:28 | INFO | fairseq.trainer | begin training epoch 3
2023-08-08 00:40:28 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 00:41:09 | INFO | train_inner | epoch 003:     53 / 1474 loss=4.551, trans_loss=5.197, nll_loss=3.867, w2v_ctc_loss=2.729, task_loss=1.324, contrastive_loss=0.788, total=4071.2, n_correct=420.26, ppl=14.59, accuracy=10.323, wps=9046.2, ups=0.74, wpb=12154.1, bsz=442.6, num_updates=3000, lr=0.00012004, gnorm=0.689, clip=0, loss_scale=128, train_wall=63, gb_free=19.1, wall=2164
2023-08-08 00:41:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-08 00:41:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-08 00:41:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-08 00:41:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-08 00:41:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-08 00:42:46 | INFO | train_inner | epoch 003:    158 / 1474 loss=3.781, trans_loss=4.461, nll_loss=2.902, w2v_ctc_loss=2.387, task_loss=0.904, contrastive_loss=0.71, total=4144.18, n_correct=1096.7, ppl=7.48, accuracy=26.464, wps=12884.4, ups=1.04, wpb=12374.6, bsz=458.6, num_updates=3100, lr=0.000124038, gnorm=1.681, clip=1, loss_scale=4, train_wall=96, gb_free=16.4, wall=2260
2023-08-08 00:44:19 | INFO | train_inner | epoch 003:    258 / 1474 loss=3.35, trans_loss=4.189, nll_loss=2.545, w2v_ctc_loss=2.118, task_loss=0.915, contrastive_loss=0.607, total=4161.13, n_correct=1400.69, ppl=5.84, accuracy=33.661, wps=13316.2, ups=1.07, wpb=12431.5, bsz=467, num_updates=3200, lr=0.000128036, gnorm=1.137, clip=0, loss_scale=4, train_wall=93, gb_free=17.1, wall=2354
2023-08-08 00:45:51 | INFO | train_inner | epoch 003:    358 / 1474 loss=3.211, trans_loss=4.104, nll_loss=2.429, w2v_ctc_loss=2.015, task_loss=0.92, contrastive_loss=0.645, total=4150.02, n_correct=1519.37, ppl=5.39, accuracy=36.611, wps=13449.5, ups=1.09, wpb=12384.9, bsz=461.6, num_updates=3300, lr=0.000132034, gnorm=1.129, clip=0, loss_scale=4, train_wall=92, gb_free=17.1, wall=2446
2023-08-08 00:47:24 | INFO | train_inner | epoch 003:    458 / 1474 loss=3.074, trans_loss=4.031, nll_loss=2.335, w2v_ctc_loss=1.927, task_loss=0.89, contrastive_loss=0.5, total=4209.57, n_correct=1649.47, ppl=5.04, accuracy=39.184, wps=13523.3, ups=1.08, wpb=12566, bsz=476.8, num_updates=3400, lr=0.000136032, gnorm=1.028, clip=0, loss_scale=4, train_wall=92, gb_free=15.9, wall=2539
2023-08-08 00:48:56 | INFO | train_inner | epoch 003:    558 / 1474 loss=2.971, trans_loss=3.989, nll_loss=2.28, w2v_ctc_loss=1.853, task_loss=0.98, contrastive_loss=0.469, total=4088.48, n_correct=1662.55, ppl=4.86, accuracy=40.664, wps=13281.1, ups=1.09, wpb=12212.5, bsz=439.7, num_updates=3500, lr=0.00014003, gnorm=0.997, clip=0, loss_scale=4, train_wall=91, gb_free=17.6, wall=2631
2023-08-08 00:50:30 | INFO | train_inner | epoch 003:    658 / 1474 loss=2.905, trans_loss=3.947, nll_loss=2.221, w2v_ctc_loss=1.781, task_loss=0.88, contrastive_loss=0.574, total=4221.58, n_correct=1789.53, ppl=4.66, accuracy=42.39, wps=13405.9, ups=1.06, wpb=12587.8, bsz=481.9, num_updates=3600, lr=0.000144028, gnorm=0.952, clip=0, loss_scale=4, train_wall=93, gb_free=16.3, wall=2725
2023-08-08 00:52:02 | INFO | train_inner | epoch 003:    758 / 1474 loss=2.827, trans_loss=3.91, nll_loss=2.178, w2v_ctc_loss=1.754, task_loss=0.877, contrastive_loss=0.343, total=4167.41, n_correct=1821.06, ppl=4.52, accuracy=43.698, wps=13500.2, ups=1.08, wpb=12447.6, bsz=472.6, num_updates=3700, lr=0.000148026, gnorm=0.953, clip=0, loss_scale=4, train_wall=92, gb_free=16.2, wall=2817
2023-08-08 00:53:35 | INFO | train_inner | epoch 003:    858 / 1474 loss=2.77, trans_loss=3.894, nll_loss=2.155, w2v_ctc_loss=1.707, task_loss=0.929, contrastive_loss=0.303, total=4165.53, n_correct=1850.92, ppl=4.45, accuracy=44.434, wps=13417.7, ups=1.08, wpb=12437.8, bsz=456.1, num_updates=3800, lr=0.000152024, gnorm=0.906, clip=0, loss_scale=4, train_wall=92, gb_free=17, wall=2910
2023-08-08 00:55:08 | INFO | train_inner | epoch 003:    958 / 1474 loss=2.743, trans_loss=3.872, nll_loss=2.125, w2v_ctc_loss=1.685, task_loss=0.894, contrastive_loss=0.332, total=4162.3, n_correct=1899.98, ppl=4.36, accuracy=45.647, wps=13337.2, ups=1.07, wpb=12417, bsz=469.2, num_updates=3900, lr=0.000156022, gnorm=0.909, clip=0, loss_scale=4, train_wall=93, gb_free=16.7, wall=3003
2023-08-08 00:56:40 | INFO | train_inner | epoch 003:   1058 / 1474 loss=2.714, trans_loss=3.854, nll_loss=2.103, w2v_ctc_loss=1.675, task_loss=0.981, contrastive_loss=0.291, total=4069.95, n_correct=1869.88, ppl=4.3, accuracy=45.944, wps=13183.2, ups=1.08, wpb=12153.7, bsz=443.6, num_updates=4000, lr=0.00016002, gnorm=0.911, clip=0, loss_scale=4, train_wall=92, gb_free=16.2, wall=3095
2023-08-08 00:56:40 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 00:57:11 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 4.896 | trans_loss 6.277 | nll_loss 3.774 | w2v_ctc_loss 1.973 | task_loss 4.368 | contrastive_loss 0.39 | total 4003.4 | n_correct 2044.1 | ppl 13.68 | accuracy 51.059 | uer 28.7 | wer 29.63 | raw_wer 29.63 | bleu 12.45 | wps 1455.7 | wpb 4003.4 | bsz 141.8 | num_updates 4000 | best_bleu 12.45
2023-08-08 00:57:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-08-08 00:57:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_3_4000.pt
2023-08-08 00:57:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_3_4000.pt
2023-08-08 00:57:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 12.45) (writing took 44.052037293091416 seconds)
2023-08-08 00:59:27 | INFO | train_inner | epoch 003:   1158 / 1474 loss=2.671, trans_loss=3.846, nll_loss=2.093, w2v_ctc_loss=1.634, task_loss=0.999, contrastive_loss=0.271, total=4038.49, n_correct=1875.34, ppl=4.26, accuracy=46.437, wps=7209.5, ups=0.6, wpb=12054.8, bsz=432.5, num_updates=4100, lr=0.000164018, gnorm=0.893, clip=0, loss_scale=4, train_wall=91, gb_free=16.3, wall=3262
2023-08-08 01:00:59 | INFO | train_inner | epoch 003:   1258 / 1474 loss=2.63, trans_loss=3.825, nll_loss=2.066, w2v_ctc_loss=1.602, task_loss=0.977, contrastive_loss=0.253, total=4064.31, n_correct=1917.75, ppl=4.19, accuracy=47.185, wps=13263, ups=1.09, wpb=12136.8, bsz=433.9, num_updates=4200, lr=0.000168016, gnorm=0.86, clip=0, loss_scale=4, train_wall=91, gb_free=17.2, wall=3354
2023-08-08 01:02:33 | INFO | train_inner | epoch 003:   1358 / 1474 loss=2.619, trans_loss=3.808, nll_loss=2.044, w2v_ctc_loss=1.571, task_loss=0.931, contrastive_loss=0.368, total=4134.58, n_correct=1981.96, ppl=4.12, accuracy=47.936, wps=13154, ups=1.07, wpb=12343.8, bsz=460.7, num_updates=4300, lr=0.000172014, gnorm=0.895, clip=0, loss_scale=4, train_wall=93, gb_free=17.7, wall=3447
2023-08-08 01:04:06 | INFO | train_inner | epoch 003:   1458 / 1474 loss=2.594, trans_loss=3.797, nll_loss=2.032, w2v_ctc_loss=1.555, task_loss=0.88, contrastive_loss=0.346, total=4209.94, n_correct=2038.64, ppl=4.09, accuracy=48.424, wps=13519.5, ups=1.08, wpb=12573.5, bsz=477.4, num_updates=4400, lr=0.000176012, gnorm=0.862, clip=0, loss_scale=4, train_wall=92, gb_free=17, wall=3540
2023-08-08 01:04:20 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 01:04:50 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 4.776 | trans_loss 6.178 | nll_loss 3.641 | w2v_ctc_loss 1.802 | task_loss 4.239 | contrastive_loss 0.383 | total 4003.4 | n_correct 2107 | ppl 12.48 | accuracy 52.63 | uer 28.355 | wer 29.1 | raw_wer 29.1 | bleu 14.15 | wps 1508.4 | wpb 4003.4 | bsz 141.8 | num_updates 4416 | best_bleu 14.15
2023-08-08 01:04:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4416 updates
2023-08-08 01:04:50 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 01:05:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 01:05:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt (epoch 3 @ 4416 updates, score 14.15) (writing took 25.807976711541414 seconds)
2023-08-08 01:05:16 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-08-08 01:05:16 | INFO | train | epoch 003 | loss 2.974 | trans_loss 4.009 | nll_loss 2.307 | w2v_ctc_loss 1.835 | task_loss 0.938 | contrastive_loss 0.445 | total 4140.05 | n_correct 1695.74 | ppl 4.95 | accuracy 40.96 | wps 12203.9 | ups 0.99 | wpb 12360.1 | bsz 459 | num_updates 4416 | lr 0.000176652 | gnorm 0.995 | clip 0.1 | loss_scale 4 | train_wall 1341 | gb_free 16.3 | wall 3611
2023-08-08 01:05:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 01:05:16 | INFO | fairseq.trainer | begin training epoch 4
2023-08-08 01:05:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 01:06:42 | INFO | train_inner | epoch 004:     84 / 1474 loss=2.513, trans_loss=3.766, nll_loss=1.988, w2v_ctc_loss=1.502, task_loss=0.955, contrastive_loss=0.198, total=4099.41, n_correct=2022.02, ppl=3.97, accuracy=49.325, wps=7823.1, ups=0.64, wpb=12237, bsz=439.5, num_updates=4500, lr=0.00018001, gnorm=0.839, clip=0, loss_scale=4, train_wall=92, gb_free=16.1, wall=3697
2023-08-08 01:08:14 | INFO | train_inner | epoch 004:    184 / 1474 loss=2.497, trans_loss=3.749, nll_loss=1.966, w2v_ctc_loss=1.484, task_loss=0.883, contrastive_loss=0.225, total=4175.15, n_correct=2086.57, ppl=3.91, accuracy=49.976, wps=13572, ups=1.09, wpb=12464.9, bsz=468.3, num_updates=4600, lr=0.000184008, gnorm=0.833, clip=0, loss_scale=4, train_wall=91, gb_free=16.5, wall=3789
2023-08-08 01:09:47 | INFO | train_inner | epoch 004:    284 / 1474 loss=2.515, trans_loss=3.753, nll_loss=1.973, w2v_ctc_loss=1.487, task_loss=0.926, contrastive_loss=0.351, total=4145.23, n_correct=2067.19, ppl=3.93, accuracy=49.869, wps=13255.3, ups=1.07, wpb=12382.4, bsz=463, num_updates=4700, lr=0.000188006, gnorm=0.828, clip=0, loss_scale=4, train_wall=93, gb_free=15.9, wall=3882
2023-08-08 01:11:19 | INFO | train_inner | epoch 004:    384 / 1474 loss=2.479, trans_loss=3.754, nll_loss=1.972, w2v_ctc_loss=1.471, task_loss=0.965, contrastive_loss=0.195, total=4127.66, n_correct=2069.22, ppl=3.92, accuracy=50.131, wps=13378.6, ups=1.09, wpb=12314.6, bsz=443.5, num_updates=4800, lr=0.000192004, gnorm=0.812, clip=0, loss_scale=4, train_wall=92, gb_free=17.4, wall=3974
2023-08-08 01:12:53 | INFO | train_inner | epoch 004:    484 / 1474 loss=2.503, trans_loss=3.734, nll_loss=1.948, w2v_ctc_loss=1.435, task_loss=0.838, contrastive_loss=0.588, total=4218.78, n_correct=2145.99, ppl=3.86, accuracy=50.868, wps=13445.4, ups=1.07, wpb=12592.4, bsz=497.8, num_updates=4900, lr=0.000196002, gnorm=0.813, clip=0, loss_scale=4, train_wall=93, gb_free=16.4, wall=4068
2023-08-08 01:14:25 | INFO | train_inner | epoch 004:    584 / 1474 loss=2.469, trans_loss=3.731, nll_loss=1.946, w2v_ctc_loss=1.459, task_loss=0.872, contrastive_loss=0.27, total=4217.52, n_correct=2150.8, ppl=3.85, accuracy=50.997, wps=13612.3, ups=1.08, wpb=12591.1, bsz=485.9, num_updates=5000, lr=0.0002, gnorm=0.816, clip=0, loss_scale=4, train_wall=92, gb_free=15.9, wall=4160
Mixup rate:0.5, token after shrink shape:torch.Size([24, 52]), X shape:torch.Size([24, 52, 512])
CTC Tokens:tensor([0, 0, 0, 0, 0], device='cuda:0'), Shrink Mask:tensor([ True, False, False, False, False], device='cuda:0'), New Tokens:tensor([   0,   26,    0, 3666,    0], device='cuda:0')
Mixup Sent Mask:tensor([[ 2],
        [ 6],
        [ 7],
        [ 9],
        [14],
        [17],
        [19],
        [21],
        [23]], device='cuda:0'), 2,  Mixup Mask:tensor([False, False,  True, False,  True], device='cuda:0'), 
                    Org X:tensor([[ 0.7700, -0.3723,  1.4131,  ...,  0.1323, -0.7119,  1.1143],
        [-0.2595,  1.8193,  1.9561,  ...,  0.3308, -0.3582,  0.0591],
        [ 0.4102,  1.0410,  2.3164,  ...,  0.5430,  0.9746,  0.0392],
        [-0.5127,  0.3025,  0.9614,  ...,  0.2717,  0.9092,  0.5928],
        [-0.0276,  0.5049,  1.9639,  ...,  0.6587,  1.1367,  0.8701]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.7700, -0.3723,  1.4131,  ...,  0.1323, -0.7119,  1.1143],
        [-0.2595,  1.8193,  1.9561,  ...,  0.3308, -0.3582,  0.0591],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600],
        [-0.5127,  0.3025,  0.9614,  ...,  0.2717,  0.9092,  0.5928],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600],
        [-0.8052, -0.2089, -0.2773,  ..., -1.5010, -0.7168, -5.1445],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600],
        [-1.6094,  2.3652,  0.5459,  ...,  3.7344, -1.4229,  0.0656],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.4948, device='cuda:0')
2023-08-08 01:16:00 | INFO | train_inner | epoch 004:    684 / 1474 loss=2.439, trans_loss=3.733, nll_loss=1.943, w2v_ctc_loss=1.421, task_loss=0.951, contrastive_loss=0.314, total=4176.39, n_correct=2138.1, ppl=3.85, accuracy=51.195, wps=13207.9, ups=1.06, wpb=12448.9, bsz=455.8, num_updates=5100, lr=0.00019803, gnorm=0.53, clip=0, loss_scale=8, train_wall=94, gb_free=17, wall=4255
2023-08-08 01:17:32 | INFO | train_inner | epoch 004:    784 / 1474 loss=2.428, trans_loss=3.727, nll_loss=1.94, w2v_ctc_loss=1.438, task_loss=1.021, contrastive_loss=0.185, total=4026.63, n_correct=2067.57, ppl=3.84, accuracy=51.347, wps=13027.2, ups=1.08, wpb=12025, bsz=420.6, num_updates=5200, lr=0.000196116, gnorm=0.541, clip=0, loss_scale=8, train_wall=92, gb_free=13, wall=4347
2023-08-08 01:19:05 | INFO | train_inner | epoch 004:    884 / 1474 loss=2.447, trans_loss=3.714, nll_loss=1.925, w2v_ctc_loss=1.429, task_loss=0.926, contrastive_loss=0.364, total=4186.04, n_correct=2167.82, ppl=3.8, accuracy=51.787, wps=13485.2, ups=1.08, wpb=12501.4, bsz=466.3, num_updates=5300, lr=0.000194257, gnorm=0.539, clip=0, loss_scale=8, train_wall=92, gb_free=17.6, wall=4440
2023-08-08 01:20:38 | INFO | train_inner | epoch 004:    984 / 1474 loss=2.404, trans_loss=3.705, nll_loss=1.914, w2v_ctc_loss=1.408, task_loss=0.939, contrastive_loss=0.232, total=4125.02, n_correct=2151.41, ppl=3.77, accuracy=52.155, wps=13281.3, ups=1.08, wpb=12321, bsz=457.1, num_updates=5400, lr=0.00019245, gnorm=0.528, clip=0, loss_scale=8, train_wall=92, gb_free=12.6, wall=4532
2023-08-08 01:22:11 | INFO | train_inner | epoch 004:   1084 / 1474 loss=2.407, trans_loss=3.714, nll_loss=1.924, w2v_ctc_loss=1.414, task_loss=1.003, contrastive_loss=0.209, total=4075.6, n_correct=2121.01, ppl=3.79, accuracy=52.042, wps=13015.8, ups=1.07, wpb=12163.4, bsz=435.7, num_updates=5500, lr=0.000190693, gnorm=0.528, clip=0, loss_scale=8, train_wall=93, gb_free=15.9, wall=4626
2023-08-08 01:23:43 | INFO | train_inner | epoch 004:   1184 / 1474 loss=2.414, trans_loss=3.703, nll_loss=1.913, w2v_ctc_loss=1.405, task_loss=0.872, contrastive_loss=0.323, total=4161.18, n_correct=2178.38, ppl=3.77, accuracy=52.35, wps=13447.8, ups=1.08, wpb=12431.8, bsz=483.4, num_updates=5600, lr=0.000188982, gnorm=0.527, clip=0, loss_scale=8, train_wall=92, gb_free=16.7, wall=4718
2023-08-08 01:25:16 | INFO | train_inner | epoch 004:   1284 / 1474 loss=2.385, trans_loss=3.693, nll_loss=1.899, w2v_ctc_loss=1.386, task_loss=0.887, contrastive_loss=0.28, total=4156.53, n_correct=2194.15, ppl=3.73, accuracy=52.788, wps=13461.9, ups=1.08, wpb=12411.4, bsz=472.7, num_updates=5700, lr=0.000187317, gnorm=0.521, clip=0, loss_scale=8, train_wall=92, gb_free=15.7, wall=4810
2023-08-08 01:26:47 | INFO | train_inner | epoch 004:   1384 / 1474 loss=2.362, trans_loss=3.693, nll_loss=1.9, w2v_ctc_loss=1.385, task_loss=0.953, contrastive_loss=0.16, total=4101.23, n_correct=2167.67, ppl=3.73, accuracy=52.854, wps=13343.3, ups=1.09, wpb=12249, bsz=437.6, num_updates=5800, lr=0.000185695, gnorm=0.515, clip=0, loss_scale=8, train_wall=91, gb_free=15.5, wall=4902
2023-08-08 01:28:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([8, 115]), X shape:torch.Size([8, 115, 512])
CTC Tokens:tensor([ 103, 1917,   56,    0, 2645], device='cuda:7'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:7'), New Tokens:tensor([ 103, 1917,   56,    0, 2645], device='cuda:7')
Mixup Sent Mask:tensor([[2],
        [6],
        [7]], device='cuda:7'), 2,  Mixup Mask:tensor([ True, False, False, False,  True], device='cuda:7'), 
                    Org X:tensor([[ 0.6519, -1.7939,  0.1071,  ...,  0.8481, -0.5503, -0.7109],
        [ 0.2213, -1.0986, -0.3428,  ...,  0.1853, -0.0713, -0.4016],
        [-0.0618,  0.4792,  0.7476,  ..., -0.7383,  0.7363,  0.6870],
        [ 0.3057,  0.5415,  0.8623,  ..., -0.2341,  0.2440,  0.4224],
        [ 0.5195,  0.1885,  0.9424,  ...,  0.1246, -0.7417, -0.5820]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.7788, -1.2783, -0.6206,  ...,  1.6094, -0.1443, -0.8057],
        [ 0.2213, -1.0986, -0.3428,  ...,  0.1853, -0.0713, -0.4016],
        [-0.0618,  0.4792,  0.7476,  ..., -0.7383,  0.7363,  0.6870],
        [ 0.3057,  0.5415,  0.8623,  ..., -0.2341,  0.2440,  0.4224],
        [ 0.5605,  1.0059,  0.8799,  ..., -0.1456, -3.2812, -3.0215]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[ 0.7788, -1.2783, -0.6206,  ...,  1.6094, -0.1443, -0.8057],
        [ 1.1309, -0.1569, -1.0459,  ..., -3.8262, -2.1836,  1.3506],
        [-1.9600, -0.2483, -0.3096,  ..., -2.5156, -0.8384,  2.3691],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600],
        [ 0.5605,  1.0059,  0.8799,  ..., -0.1456, -3.2812, -3.0215]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.4948, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([24, 59]), X shape:torch.Size([24, 59, 512])
CTC Tokens:tensor([101, 641,   0,   0,   0], device='cuda:6'), Shrink Mask:tensor([ True,  True,  True, False, False], device='cuda:6'), New Tokens:tensor([101, 641,   0,   4,   0], device='cuda:6')
Mixup Sent Mask:tensor([[ 2],
        [ 6],
        [ 7],
        [ 9],
        [14],
        [17],
        [19],
        [21],
        [23]], device='cuda:6'), 2,  Mixup Mask:tensor([False,  True,  True,  True, False], device='cuda:6'), 
                    Org X:tensor([[ 0.5840,  1.1006, -0.3994,  ...,  0.2314,  1.0322,  0.6416],
        [ 0.4792,  2.0898, -0.5381,  ..., -0.5728,  0.7183,  0.9048],
        [ 0.4727,  1.0068,  0.2113,  ..., -0.5640,  0.4558,  1.3291],
        [ 0.2468,  0.8872,  1.0537,  ..., -0.3774,  0.0106, -0.7412],
        [ 0.2922,  0.7251,  0.5820,  ..., -0.3452,  0.4958, -0.9038]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.5840,  1.1006, -0.3994,  ...,  0.2314,  1.0322,  0.6416],
        [-1.6406,  1.7061, -0.6729,  ..., -3.7148,  0.1602,  0.6616],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600],
        [-0.1221, -0.5356, -0.5479,  ..., -1.6982, -1.0430, -2.3828],
        [ 0.2922,  0.7251,  0.5820,  ..., -0.3452,  0.4958, -0.9038]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5229, -0.4265,  0.1952,  ...,  0.2175, -2.5195,  1.9346],
        [-1.6406,  1.7061, -0.6729,  ..., -3.7148,  0.1602,  0.6616],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600],
        [-0.1221, -0.5356, -0.5479,  ..., -1.6982, -1.0430, -2.3828],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.4948, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([32, 46]), X shape:torch.Size([32, 46, 512])
CTC Tokens:tensor([   0,  194,    0,   44, 2728], device='cuda:4'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:4'), New Tokens:tensor([   0,  194,    0,   44, 2728], device='cuda:4')
Mixup Sent Mask:tensor([[ 2],
        [ 6],
        [ 7],
        [ 9],
        [14],
        [17],
        [19],
        [21],
        [23]], device='cuda:4'), 2,  Mixup Mask:tensor([False,  True, False, False, False], device='cuda:4'), 
                    Org X:tensor([[ 0.5269, -0.2448,  1.0352,  ..., -0.2798,  0.1063,  1.0420],
        [ 0.6494, -0.5762,  1.6191,  ..., -0.1425,  0.2146,  0.3250],
        [ 0.6992, -0.2656,  1.9209,  ...,  0.0277,  0.2761,  0.3179],
        [ 0.6108, -0.1868,  1.8496,  ..., -0.0025,  0.2468,  0.2026],
        [ 0.6396, -0.1218,  1.9355,  ..., -0.0625,  0.1788,  0.3257]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.5269, -0.2448,  1.0352,  ..., -0.2798,  0.1063,  1.0420],
        [-0.2554, -0.7886, -0.0652,  ...,  0.3201, -0.5464, -0.5205],
        [ 0.6992, -0.2656,  1.9209,  ...,  0.0277,  0.2761,  0.3179],
        [ 0.6108, -0.1868,  1.8496,  ..., -0.0025,  0.2468,  0.2026],
        [ 0.6396, -0.1218,  1.9355,  ..., -0.0625,  0.1788,  0.3257]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600],
        [-0.2554, -0.7886, -0.0652,  ...,  0.3201, -0.5464, -0.5205],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600],
        [ 0.2321, -0.2502,  0.3501,  ...,  1.2871, -2.6680, -6.0156],
        [ 2.1504,  0.5654,  1.2002,  ..., -3.6328, -0.3826,  0.7900]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.4948, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 90]), X shape:torch.Size([8, 90, 512])
CTC Tokens:tensor([67, 67,  0, 70, 24], device='cuda:1'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:1'), New Tokens:tensor([67,  0, 70, 24,  0], device='cuda:1')
Mixup Sent Mask:tensor([[2],
        [6],
        [7]], device='cuda:1'), 2,  Mixup Mask:tensor([ True, False, False,  True,  True], device='cuda:1'), 
                    Org X:tensor([[ 0.3967,  0.3003, -0.5205,  ..., -0.1830, -0.1879, -2.6543],
        [-0.9854, -0.1743,  0.0407,  ...,  0.1569, -0.3672, -2.9531],
        [-1.0850, -0.2300, -0.1646,  ...,  0.3667, -0.3088, -2.5273],
        [-0.9678,  0.1788, -0.3645,  ...,  0.2839,  1.2354, -2.4512],
        [ 0.3176,  0.0701,  0.1279,  ..., -0.1871,  0.5972, -1.0098]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.1009, -0.6143, -1.1465,  ...,  1.4229, -1.5430, -4.5039],
        [-0.9854, -0.1743,  0.0407,  ...,  0.1569, -0.3672, -2.9531],
        [-1.0850, -0.2300, -0.1646,  ...,  0.3667, -0.3088, -2.5273],
        [-0.6191, -0.6353,  0.5107,  ..., -4.6211,  1.7227, -1.3965],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.1009, -0.6143, -1.1465,  ...,  1.4229, -1.5430, -4.5039],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600],
        [-0.0081, -0.8599, -0.1096,  ...,  0.6953, -2.3184, -3.0000],
        [-0.6191, -0.6353,  0.5107,  ..., -4.6211,  1.7227, -1.3965],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.4948, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([40, 35]), X shape:torch.Size([40, 35, 512])
CTC Tokens:tensor([8, 8, 0, 0, 0], device='cuda:3'), Shrink Mask:tensor([ True, False,  True, False, False], device='cuda:3'), New Tokens:tensor([ 8,  0, 19,  0, 34], device='cuda:3')
Mixup Sent Mask:tensor([[ 2],
        [ 6],
        [ 7],
        [ 9],
        [14],
        [17],
        [19],
        [21],
        [23],
        [33],
        [34],
        [35],
        [39]], device='cuda:3'), 2,  Mixup Mask:tensor([False,  True,  True, False, False], device='cuda:3'), 
                    Org X:tensor([[ 0.6421, -0.6948,  0.3623,  ...,  0.4875,  0.2422, -0.5786],
        [-0.2783, -0.3247,  2.2402,  ...,  0.3467,  0.9009, -0.1109],
        [ 0.3784,  1.1729,  1.2207,  ..., -0.1400,  0.9453, -1.0068],
        [ 0.6123,  1.3350,  2.0527,  ..., -0.3538,  1.7295, -0.6763],
        [-0.9658,  0.5029,  2.1680,  ..., -0.8916, -1.6318, -0.7017]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.6421, -0.6948,  0.3623,  ...,  0.4875,  0.2422, -0.5786],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600],
        [-0.0668, -0.3494, -0.1729,  ..., -0.3403, -0.1310, -1.1533],
        [ 0.6123,  1.3350,  2.0527,  ..., -0.3538,  1.7295, -0.6763],
        [-0.9658,  0.5029,  2.1680,  ..., -0.8916, -1.6318, -0.7017]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4492, -0.4829, -0.8066,  ...,  1.2842, -1.9102, -1.4541],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600],
        [-0.0668, -0.3494, -0.1729,  ..., -0.3403, -0.1310, -1.1533],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600],
        [-0.1732, -1.0107, -0.0981,  ..., -1.9014, -3.2012, -3.3457]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.4948, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([64, 28]), X shape:torch.Size([64, 28, 512])
CTC Tokens:tensor([ 0, 29, 29, 29,  0], device='cuda:5'), Shrink Mask:tensor([ True,  True, False, False,  True], device='cuda:5'), New Tokens:tensor([  0,  29,   0, 168,   0], device='cuda:5')
Mixup Sent Mask:tensor([[ 2],
        [ 6],
        [ 7],
        [ 9],
        [14],
        [17],
        [19],
        [21],
        [23],
        [33],
        [34],
        [35],
        [39],
        [40],
        [41],
        [45],
        [48],
        [51],
        [56],
        [57]], device='cuda:5'), 2,  Mixup Mask:tensor([ True,  True, False, False, False], device='cuda:5'), 
                    Org X:tensor([[ 0.3757, -0.1450,  1.0674,  ...,  0.8115, -0.9463,  0.2551],
        [-0.3503,  0.0256,  0.8008,  ...,  0.4431, -2.4551, -0.1028],
        [ 0.5312,  0.8179,  2.0977,  ..., -0.0091,  0.2761,  0.0989],
        [-0.5508,  0.7134, -1.6816,  ..., -0.0612,  0.1293,  0.9546],
        [-1.1055,  0.6924, -0.7866,  ...,  0.0440,  0.4871, -0.3613]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600],
        [-0.9370,  0.1252, -0.6499,  ..., -0.4226, -2.2227,  3.3691],
        [ 0.5312,  0.8179,  2.0977,  ..., -0.0091,  0.2761,  0.0989],
        [-0.5508,  0.7134, -1.6816,  ..., -0.0612,  0.1293,  0.9546],
        [-1.1055,  0.6924, -0.7866,  ...,  0.0440,  0.4871, -0.3613]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600],
        [-0.9370,  0.1252, -0.6499,  ..., -0.4226, -2.2227,  3.3691],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600],
        [ 0.4912,  0.3044, -1.6377,  ..., -2.4766, -2.6406,  1.3760],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.4948, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 62]), X shape:torch.Size([16, 62, 512])
CTC Tokens:tensor([53, 67,  6, 13,  0], device='cuda:2'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:2'), New Tokens:tensor([53, 67,  6, 13,  0], device='cuda:2')
Mixup Sent Mask:tensor([[ 2],
        [ 6],
        [ 7],
        [ 9],
        [14]], device='cuda:2'), 2,  Mixup Mask:tensor([ True,  True, False,  True,  True], device='cuda:2'), 
                    Org X:tensor([[-0.7666,  0.2483,  0.1326,  ...,  0.5732, -0.8359, -1.3125],
        [-0.2413,  0.1888,  0.5249,  ...,  0.2522, -1.3711, -2.0977],
        [-0.0902,  0.4177, -0.7168,  ...,  0.0606, -2.0039, -1.4385],
        [-1.6895,  0.7632, -0.2456,  ...,  0.2346, -1.0693, -0.4468],
        [-0.4802,  1.0693,  0.8311,  ..., -0.0448, -0.0070, -0.5674]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.0114, -0.2771, -0.0053,  ..., -2.8242,  3.7363, -2.6914],
        [-0.1009, -0.6143, -1.1465,  ...,  1.4229, -1.5430, -4.5039],
        [-0.0902,  0.4177, -0.7168,  ...,  0.0606, -2.0039, -1.4385],
        [-0.3918, -0.1106, -0.0084,  ..., -1.8379, -0.5874,  0.7266],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.0114, -0.2771, -0.0053,  ..., -2.8242,  3.7363, -2.6914],
        [-0.1009, -0.6143, -1.1465,  ...,  1.4229, -1.5430, -4.5039],
        [-0.0606, -0.4802, -0.5728,  ..., -3.0430, -3.5137,  1.3682],
        [-0.3918, -0.1106, -0.0084,  ..., -1.8379, -0.5874,  0.7266],
        [-0.6943, -0.9854, -0.3909,  ..., -2.0938, -1.5801, -1.9600]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.4948, device='cuda:2')
2023-08-08 01:28:34 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 4.478 | trans_loss 5.876 | nll_loss 3.238 | w2v_ctc_loss 1.537 | task_loss 4.47 | contrastive_loss 0.307 | total 4003.4 | n_correct 2282.9 | ppl 9.43 | accuracy 57.024 | uer 22.809 | wer 24.347 | raw_wer 24.347 | bleu 17.04 | wps 2132.8 | wpb 4003.4 | bsz 141.8 | num_updates 5890 | best_bleu 17.04
2023-08-08 01:28:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5890 updates
2023-08-08 01:28:34 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 01:28:46 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 01:28:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt (epoch 4 @ 5890 updates, score 17.04) (writing took 24.15144775994122 seconds)
2023-08-08 01:28:58 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-08-08 01:28:58 | INFO | train | epoch 004 | loss 2.441 | trans_loss 3.724 | nll_loss 1.936 | w2v_ctc_loss 1.432 | task_loss 0.927 | contrastive_loss 0.277 | total 4138.65 | n_correct 2126.94 | ppl 3.83 | accuracy 51.392 | wps 12806.4 | ups 1.04 | wpb 12355.8 | bsz 458.5 | num_updates 5890 | lr 0.000184271 | gnorm 0.645 | clip 0 | loss_scale 8 | train_wall 1358 | gb_free 14.7 | wall 5033
2023-08-08 01:28:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 01:28:58 | INFO | fairseq.trainer | begin training epoch 5
2023-08-08 01:28:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 01:29:15 | INFO | train_inner | epoch 005:     10 / 1474 loss=2.346, trans_loss=3.686, nll_loss=1.89, w2v_ctc_loss=1.36, task_loss=0.965, contrastive_loss=0.183, total=4037.7, n_correct=2144.77, ppl=3.71, accuracy=53.119, wps=8190.6, ups=0.68, wpb=12055.9, bsz=439.3, num_updates=5900, lr=0.000184115, gnorm=0.52, clip=0, loss_scale=8, train_wall=91, gb_free=16.8, wall=5049
2023-08-08 01:30:47 | INFO | train_inner | epoch 005:    110 / 1474 loss=2.269, trans_loss=3.628, nll_loss=1.815, w2v_ctc_loss=1.281, task_loss=0.839, contrastive_loss=0.189, total=4247.37, n_correct=2333.56, ppl=3.52, accuracy=54.941, wps=13669.2, ups=1.08, wpb=12683.4, bsz=495.1, num_updates=6000, lr=0.000182574, gnorm=0.496, clip=0, loss_scale=8, train_wall=92, gb_free=16.6, wall=5142
2023-08-08 01:30:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 01:31:11 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.48 | trans_loss 5.882 | nll_loss 3.245 | w2v_ctc_loss 1.53 | task_loss 4.471 | contrastive_loss 0.312 | total 4003.4 | n_correct 2284.7 | ppl 9.48 | accuracy 57.069 | uer 23.08 | wer 24.727 | raw_wer 24.727 | bleu 16.64 | wps 1928.1 | wpb 4003.4 | bsz 141.8 | num_updates 6000 | best_bleu 17.04
2023-08-08 01:31:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6000 updates
2023-08-08 01:31:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_5_6000.pt
2023-08-08 01:31:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_5_6000.pt
2023-08-08 01:31:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_5_6000.pt (epoch 5 @ 6000 updates, score 16.64) (writing took 32.818322755396366 seconds)
2023-08-08 01:33:16 | INFO | train_inner | epoch 005:    210 / 1474 loss=2.308, trans_loss=3.639, nll_loss=1.826, w2v_ctc_loss=1.297, task_loss=0.859, contrastive_loss=0.405, total=4189.85, n_correct=2289.12, ppl=3.55, accuracy=54.635, wps=8399.2, ups=0.67, wpb=12500.5, bsz=488.2, num_updates=6100, lr=0.000181071, gnorm=0.501, clip=0, loss_scale=8, train_wall=92, gb_free=17.7, wall=5291
2023-08-08 01:34:49 | INFO | train_inner | epoch 005:    310 / 1474 loss=2.297, trans_loss=3.637, nll_loss=1.828, w2v_ctc_loss=1.314, task_loss=0.957, contrastive_loss=0.253, total=4090.1, n_correct=2223.02, ppl=3.55, accuracy=54.351, wps=13234.2, ups=1.08, wpb=12228.1, bsz=443.9, num_updates=6200, lr=0.000179605, gnorm=0.508, clip=0, loss_scale=8, train_wall=92, gb_free=16.1, wall=5383
2023-08-08 01:36:21 | INFO | train_inner | epoch 005:    410 / 1474 loss=2.288, trans_loss=3.628, nll_loss=1.818, w2v_ctc_loss=1.283, task_loss=0.897, contrastive_loss=0.338, total=4147.17, n_correct=2274.46, ppl=3.53, accuracy=54.844, wps=13358.6, ups=1.08, wpb=12395.1, bsz=472.5, num_updates=6300, lr=0.000178174, gnorm=0.509, clip=0, loss_scale=8, train_wall=92, gb_free=14.7, wall=5476
2023-08-08 01:37:54 | INFO | train_inner | epoch 005:    510 / 1474 loss=2.262, trans_loss=3.64, nll_loss=1.831, w2v_ctc_loss=1.291, task_loss=1.046, contrastive_loss=0.132, total=4026.81, n_correct=2197.74, ppl=3.56, accuracy=54.578, wps=12943.8, ups=1.08, wpb=12029.7, bsz=416.6, num_updates=6400, lr=0.000176777, gnorm=0.498, clip=0, loss_scale=8, train_wall=92, gb_free=17.3, wall=5569
2023-08-08 01:39:27 | INFO | train_inner | epoch 005:    610 / 1474 loss=2.28, trans_loss=3.643, nll_loss=1.833, w2v_ctc_loss=1.279, task_loss=0.954, contrastive_loss=0.301, total=4107.75, n_correct=2244.95, ppl=3.56, accuracy=54.652, wps=13244.5, ups=1.08, wpb=12253.8, bsz=451.2, num_updates=6500, lr=0.000175412, gnorm=0.512, clip=0, loss_scale=8, train_wall=92, gb_free=16, wall=5662
2023-08-08 01:41:00 | INFO | train_inner | epoch 005:    710 / 1474 loss=2.278, trans_loss=3.638, nll_loss=1.829, w2v_ctc_loss=1.278, task_loss=0.881, contrastive_loss=0.282, total=4178.85, n_correct=2294.95, ppl=3.55, accuracy=54.918, wps=13434.7, ups=1.08, wpb=12473.1, bsz=480.9, num_updates=6600, lr=0.000174078, gnorm=0.5, clip=0, loss_scale=8, train_wall=92, gb_free=17.6, wall=5755
2023-08-08 01:42:33 | INFO | train_inner | epoch 005:    810 / 1474 loss=2.261, trans_loss=3.637, nll_loss=1.826, w2v_ctc_loss=1.274, task_loss=0.957, contrastive_loss=0.209, total=4127.73, n_correct=2268.54, ppl=3.55, accuracy=54.959, wps=13273, ups=1.08, wpb=12320.4, bsz=449.2, num_updates=6700, lr=0.000172774, gnorm=0.498, clip=0, loss_scale=8, train_wall=92, gb_free=15, wall=5847
2023-08-08 01:44:05 | INFO | train_inner | epoch 005:    910 / 1474 loss=2.24, trans_loss=3.628, nll_loss=1.816, w2v_ctc_loss=1.265, task_loss=0.962, contrastive_loss=0.169, total=4095.48, n_correct=2261.47, ppl=3.52, accuracy=55.219, wps=13168.5, ups=1.08, wpb=12229.5, bsz=445.3, num_updates=6800, lr=0.000171499, gnorm=0.5, clip=0, loss_scale=8, train_wall=92, gb_free=15.4, wall=5940
2023-08-08 01:45:37 | INFO | train_inner | epoch 005:   1010 / 1474 loss=2.252, trans_loss=3.63, nll_loss=1.818, w2v_ctc_loss=1.266, task_loss=0.92, contrastive_loss=0.249, total=4165.12, n_correct=2303.49, ppl=3.53, accuracy=55.304, wps=13563.4, ups=1.09, wpb=12433.6, bsz=463.5, num_updates=6900, lr=0.000170251, gnorm=0.489, clip=0, loss_scale=8, train_wall=91, gb_free=15.5, wall=6032
2023-08-08 01:47:11 | INFO | train_inner | epoch 005:   1110 / 1474 loss=2.266, trans_loss=3.631, nll_loss=1.819, w2v_ctc_loss=1.276, task_loss=0.921, contrastive_loss=0.256, total=4176.72, n_correct=2310.92, ppl=3.53, accuracy=55.329, wps=13329.2, ups=1.07, wpb=12459.2, bsz=466.1, num_updates=7000, lr=0.000169031, gnorm=0.496, clip=0, loss_scale=8, train_wall=93, gb_free=16.5, wall=6125
2023-08-08 01:48:43 | INFO | train_inner | epoch 005:   1210 / 1474 loss=2.229, trans_loss=3.628, nll_loss=1.815, w2v_ctc_loss=1.251, task_loss=0.949, contrastive_loss=0.158, total=4164.13, n_correct=2311.34, ppl=3.52, accuracy=55.506, wps=13513.7, ups=1.09, wpb=12420.9, bsz=453.8, num_updates=7100, lr=0.000167836, gnorm=0.494, clip=0, loss_scale=16, train_wall=91, gb_free=16.8, wall=6217
2023-08-08 01:50:15 | INFO | train_inner | epoch 005:   1310 / 1474 loss=2.216, trans_loss=3.627, nll_loss=1.816, w2v_ctc_loss=1.24, task_loss=0.946, contrastive_loss=0.127, total=4134.91, n_correct=2294.56, ppl=3.52, accuracy=55.492, wps=13341, ups=1.08, wpb=12341.4, bsz=445.6, num_updates=7200, lr=0.000166667, gnorm=0.488, clip=0, loss_scale=16, train_wall=92, gb_free=16.1, wall=6310
2023-08-08 01:51:47 | INFO | train_inner | epoch 005:   1410 / 1474 loss=2.225, trans_loss=3.627, nll_loss=1.818, w2v_ctc_loss=1.238, task_loss=0.94, contrastive_loss=0.191, total=4134.37, n_correct=2296.16, ppl=3.53, accuracy=55.538, wps=13441.2, ups=1.09, wpb=12347.5, bsz=458.5, num_updates=7300, lr=0.000165521, gnorm=0.496, clip=0, loss_scale=16, train_wall=91, gb_free=17.7, wall=6402
2023-08-08 01:52:46 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 01:53:11 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.375 | trans_loss 5.79 | nll_loss 3.134 | w2v_ctc_loss 1.384 | task_loss 4.486 | contrastive_loss 0.316 | total 4003.4 | n_correct 2330.5 | ppl 8.78 | accuracy 58.213 | uer 21.323 | wer 22.922 | raw_wer 22.922 | bleu 17.68 | wps 2012.1 | wpb 4003.4 | bsz 141.8 | num_updates 7364 | best_bleu 17.68
2023-08-08 01:53:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7364 updates
2023-08-08 01:53:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 01:53:24 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 01:53:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt (epoch 5 @ 7364 updates, score 17.68) (writing took 23.758691638708115 seconds)
2023-08-08 01:53:35 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-08-08 01:53:35 | INFO | train | epoch 005 | loss 2.262 | trans_loss 3.632 | nll_loss 1.821 | w2v_ctc_loss 1.273 | task_loss 0.93 | contrastive_loss 0.233 | total 4138.65 | n_correct 2277.88 | ppl 3.53 | accuracy 55.039 | wps 12330.9 | ups 1 | wpb 12355.8 | bsz 458.5 | num_updates 7364 | lr 0.0001648 | gnorm 0.5 | clip 0 | loss_scale 16 | train_wall 1357 | gb_free 16.1 | wall 6510
2023-08-08 01:53:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 01:53:35 | INFO | fairseq.trainer | begin training epoch 6
2023-08-08 01:53:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 01:54:17 | INFO | train_inner | epoch 006:     36 / 1474 loss=2.21, trans_loss=3.603, nll_loss=1.785, w2v_ctc_loss=1.234, task_loss=0.954, contrastive_loss=0.187, total=4115.45, n_correct=2310.12, ppl=3.45, accuracy=56.133, wps=8191.7, ups=0.67, wpb=12281.2, bsz=447.9, num_updates=7400, lr=0.000164399, gnorm=0.504, clip=0, loss_scale=16, train_wall=93, gb_free=16.3, wall=6552
2023-08-08 01:55:49 | INFO | train_inner | epoch 006:    136 / 1474 loss=2.166, trans_loss=3.572, nll_loss=1.744, w2v_ctc_loss=1.183, task_loss=0.929, contrastive_loss=0.232, total=4154.25, n_correct=2364.01, ppl=3.35, accuracy=56.906, wps=13497.2, ups=1.09, wpb=12407.4, bsz=456.1, num_updates=7500, lr=0.000163299, gnorm=0.487, clip=0, loss_scale=16, train_wall=91, gb_free=15.4, wall=6644
2023-08-08 01:57:22 | INFO | train_inner | epoch 006:    236 / 1474 loss=2.178, trans_loss=3.583, nll_loss=1.76, w2v_ctc_loss=1.215, task_loss=0.999, contrastive_loss=0.139, total=4112.66, n_correct=2325.41, ppl=3.39, accuracy=56.543, wps=13154.4, ups=1.07, wpb=12287.2, bsz=437.3, num_updates=7600, lr=0.000162221, gnorm=0.486, clip=0, loss_scale=16, train_wall=93, gb_free=16, wall=6737
2023-08-08 01:58:56 | INFO | train_inner | epoch 006:    336 / 1474 loss=2.189, trans_loss=3.571, nll_loss=1.745, w2v_ctc_loss=1.165, task_loss=0.865, contrastive_loss=0.449, total=4177.51, n_correct=2383.99, ppl=3.35, accuracy=57.067, wps=13298.8, ups=1.07, wpb=12473.8, bsz=491.3, num_updates=7700, lr=0.000161165, gnorm=0.492, clip=0, loss_scale=16, train_wall=93, gb_free=15.9, wall=6831
2023-08-08 02:00:28 | INFO | train_inner | epoch 006:    436 / 1474 loss=2.149, trans_loss=3.574, nll_loss=1.748, w2v_ctc_loss=1.175, task_loss=0.894, contrastive_loss=0.155, total=4154.57, n_correct=2375.85, ppl=3.36, accuracy=57.186, wps=13501.2, ups=1.09, wpb=12405.5, bsz=470.2, num_updates=7800, lr=0.000160128, gnorm=0.485, clip=0, loss_scale=16, train_wall=91, gb_free=16, wall=6923
2023-08-08 02:02:01 | INFO | train_inner | epoch 006:    536 / 1474 loss=2.158, trans_loss=3.582, nll_loss=1.757, w2v_ctc_loss=1.19, task_loss=0.936, contrastive_loss=0.143, total=4167.79, n_correct=2375.61, ppl=3.38, accuracy=56.999, wps=13352.6, ups=1.07, wpb=12438.5, bsz=455.2, num_updates=7900, lr=0.000159111, gnorm=0.485, clip=0, loss_scale=16, train_wall=93, gb_free=15.6, wall=7016
2023-08-08 02:03:32 | INFO | train_inner | epoch 006:    636 / 1474 loss=2.157, trans_loss=3.584, nll_loss=1.762, w2v_ctc_loss=1.173, task_loss=0.882, contrastive_loss=0.2, total=4146.17, n_correct=2360.24, ppl=3.39, accuracy=56.926, wps=13552.7, ups=1.1, wpb=12376.6, bsz=471.6, num_updates=8000, lr=0.000158114, gnorm=0.492, clip=0, loss_scale=16, train_wall=91, gb_free=16.2, wall=7107
2023-08-08 02:03:32 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 02:03:56 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.356 | trans_loss 5.756 | nll_loss 3.08 | w2v_ctc_loss 1.418 | task_loss 4.528 | contrastive_loss 0.281 | total 4003.4 | n_correct 2356.2 | ppl 8.46 | accuracy 58.855 | uer 20.84 | wer 22.613 | raw_wer 22.613 | bleu 18 | wps 1935.2 | wpb 4003.4 | bsz 141.8 | num_updates 8000 | best_bleu 18
2023-08-08 02:03:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8000 updates
2023-08-08 02:03:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_6_8000.pt
2023-08-08 02:04:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_6_8000.pt
2023-08-08 02:04:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_6_8000.pt (epoch 6 @ 8000 updates, score 18.0) (writing took 43.63958514109254 seconds)
2023-08-08 02:06:14 | INFO | train_inner | epoch 006:    736 / 1474 loss=2.161, trans_loss=3.586, nll_loss=1.764, w2v_ctc_loss=1.19, task_loss=0.953, contrastive_loss=0.152, total=4148.65, n_correct=2360.08, ppl=3.4, accuracy=56.888, wps=7680.3, ups=0.62, wpb=12388, bsz=453.7, num_updates=8100, lr=0.000157135, gnorm=0.486, clip=0, loss_scale=16, train_wall=93, gb_free=15.4, wall=7268
2023-08-08 02:07:46 | INFO | train_inner | epoch 006:    836 / 1474 loss=2.157, trans_loss=3.595, nll_loss=1.775, w2v_ctc_loss=1.184, task_loss=0.976, contrastive_loss=0.135, total=4114.34, n_correct=2331.29, ppl=3.42, accuracy=56.663, wps=13250.2, ups=1.08, wpb=12282.2, bsz=441.6, num_updates=8200, lr=0.000156174, gnorm=0.487, clip=0, loss_scale=16, train_wall=92, gb_free=14.9, wall=7361
2023-08-08 02:09:19 | INFO | train_inner | epoch 006:    936 / 1474 loss=2.172, trans_loss=3.593, nll_loss=1.773, w2v_ctc_loss=1.184, task_loss=0.97, contrastive_loss=0.232, total=4081.53, n_correct=2317.65, ppl=3.42, accuracy=56.784, wps=13145.8, ups=1.08, wpb=12181.3, bsz=444.5, num_updates=8300, lr=0.00015523, gnorm=0.488, clip=0, loss_scale=16, train_wall=92, gb_free=17.8, wall=7454
2023-08-08 02:10:50 | INFO | train_inner | epoch 006:   1036 / 1474 loss=2.161, trans_loss=3.579, nll_loss=1.756, w2v_ctc_loss=1.163, task_loss=0.882, contrastive_loss=0.306, total=4165.84, n_correct=2381.78, ppl=3.38, accuracy=57.174, wps=13593.6, ups=1.09, wpb=12435.7, bsz=477.2, num_updates=8400, lr=0.000154303, gnorm=0.492, clip=0, loss_scale=16, train_wall=91, gb_free=16.7, wall=7545
2023-08-08 02:12:24 | INFO | train_inner | epoch 006:   1136 / 1474 loss=2.153, trans_loss=3.586, nll_loss=1.765, w2v_ctc_loss=1.183, task_loss=1.028, contrastive_loss=0.137, total=4072.29, n_correct=2316.34, ppl=3.4, accuracy=56.881, wps=13054.8, ups=1.07, wpb=12157.6, bsz=428, num_updates=8500, lr=0.000153393, gnorm=0.493, clip=0, loss_scale=16, train_wall=93, gb_free=16.9, wall=7638
2023-08-08 02:13:56 | INFO | train_inner | epoch 006:   1236 / 1474 loss=2.181, trans_loss=3.576, nll_loss=1.755, w2v_ctc_loss=1.165, task_loss=0.904, contrastive_loss=0.452, total=4141.55, n_correct=2370.54, ppl=3.37, accuracy=57.238, wps=13345.8, ups=1.08, wpb=12370.9, bsz=474.8, num_updates=8600, lr=0.000152499, gnorm=0.483, clip=0, loss_scale=16, train_wall=92, gb_free=13, wall=7731
2023-08-08 02:15:28 | INFO | train_inner | epoch 006:   1336 / 1474 loss=2.132, trans_loss=3.585, nll_loss=1.762, w2v_ctc_loss=1.162, task_loss=0.928, contrastive_loss=0.122, total=4125.31, n_correct=2360.14, ppl=3.39, accuracy=57.211, wps=13424.1, ups=1.09, wpb=12305, bsz=452.6, num_updates=8700, lr=0.00015162, gnorm=0.484, clip=0, loss_scale=16, train_wall=91, gb_free=17.7, wall=7823
2023-08-08 02:17:00 | INFO | train_inner | epoch 006:   1436 / 1474 loss=2.13, trans_loss=3.577, nll_loss=1.754, w2v_ctc_loss=1.162, task_loss=0.93, contrastive_loss=0.13, total=4196.2, n_correct=2409.76, ppl=3.37, accuracy=57.427, wps=13585.9, ups=1.08, wpb=12525.2, bsz=461.5, num_updates=8800, lr=0.000150756, gnorm=0.474, clip=0, loss_scale=16, train_wall=92, gb_free=11.1, wall=7915
2023-08-08 02:17:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 02:17:58 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.32 | trans_loss 5.719 | nll_loss 3.039 | w2v_ctc_loss 1.388 | task_loss 4.566 | contrastive_loss 0.276 | total 4003.4 | n_correct 2377.4 | ppl 8.22 | accuracy 59.385 | uer 19.945 | wer 21.793 | raw_wer 21.793 | bleu 18.06 | wps 2245.8 | wpb 4003.4 | bsz 141.8 | num_updates 8838 | best_bleu 18.06
2023-08-08 02:17:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8838 updates
2023-08-08 02:17:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 02:18:11 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 02:18:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt (epoch 6 @ 8838 updates, score 18.06) (writing took 24.67859536409378 seconds)
2023-08-08 02:18:23 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-08-08 02:18:23 | INFO | train | epoch 006 | loss 2.159 | trans_loss 3.581 | nll_loss 1.758 | w2v_ctc_loss 1.177 | task_loss 0.931 | contrastive_loss 0.212 | total 4138.65 | n_correct 2359.52 | ppl 3.38 | accuracy 57.012 | wps 12244 | ups 0.99 | wpb 12355.8 | bsz 458.5 | num_updates 8838 | lr 0.000150431 | gnorm 0.486 | clip 0 | loss_scale 16 | train_wall 1356 | gb_free 15 | wall 7997
2023-08-08 02:18:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 02:18:23 | INFO | fairseq.trainer | begin training epoch 7
2023-08-08 02:18:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 02:19:29 | INFO | train_inner | epoch 007:     62 / 1474 loss=2.1, trans_loss=3.554, nll_loss=1.725, w2v_ctc_loss=1.132, task_loss=0.907, contrastive_loss=0.145, total=4108.19, n_correct=2382.69, ppl=3.31, accuracy=57.999, wps=8265.7, ups=0.67, wpb=12266.6, bsz=461.9, num_updates=8900, lr=0.000149906, gnorm=0.483, clip=0, loss_scale=16, train_wall=92, gb_free=17, wall=8063
2023-08-08 02:21:00 | INFO | train_inner | epoch 007:    162 / 1474 loss=2.097, trans_loss=3.543, nll_loss=1.708, w2v_ctc_loss=1.117, task_loss=0.944, contrastive_loss=0.217, total=4106.05, n_correct=2395.12, ppl=3.27, accuracy=58.331, wps=13353.2, ups=1.09, wpb=12258.7, bsz=455.4, num_updates=9000, lr=0.000149071, gnorm=0.483, clip=0, loss_scale=16, train_wall=91, gb_free=16.6, wall=8155
2023-08-08 02:22:33 | INFO | train_inner | epoch 007:    262 / 1474 loss=2.084, trans_loss=3.54, nll_loss=1.703, w2v_ctc_loss=1.121, task_loss=0.944, contrastive_loss=0.125, total=4129.3, n_correct=2417.65, ppl=3.26, accuracy=58.549, wps=13337.2, ups=1.08, wpb=12322.8, bsz=451.8, num_updates=9100, lr=0.00014825, gnorm=0.482, clip=0, loss_scale=16, train_wall=92, gb_free=17.2, wall=8248
2023-08-08 02:24:05 | INFO | train_inner | epoch 007:    362 / 1474 loss=2.117, trans_loss=3.549, nll_loss=1.716, w2v_ctc_loss=1.113, task_loss=0.897, contrastive_loss=0.384, total=4201.67, n_correct=2441.64, ppl=3.28, accuracy=58.111, wps=13536.5, ups=1.08, wpb=12539.8, bsz=479.7, num_updates=9200, lr=0.000147442, gnorm=0.477, clip=0, loss_scale=32, train_wall=92, gb_free=15.3, wall=8340
2023-08-08 02:25:38 | INFO | train_inner | epoch 007:    462 / 1474 loss=2.103, trans_loss=3.547, nll_loss=1.716, w2v_ctc_loss=1.108, task_loss=0.914, contrastive_loss=0.306, total=4155.31, n_correct=2411.01, ppl=3.29, accuracy=58.022, wps=13416.4, ups=1.08, wpb=12410.9, bsz=465.5, num_updates=9300, lr=0.000146647, gnorm=0.477, clip=0, loss_scale=32, train_wall=92, gb_free=16.7, wall=8433
2023-08-08 02:27:09 | INFO | train_inner | epoch 007:    562 / 1474 loss=2.084, trans_loss=3.548, nll_loss=1.714, w2v_ctc_loss=1.114, task_loss=0.914, contrastive_loss=0.134, total=4165.88, n_correct=2431.86, ppl=3.28, accuracy=58.376, wps=13593.8, ups=1.09, wpb=12426.4, bsz=459, num_updates=9400, lr=0.000145865, gnorm=0.483, clip=0, loss_scale=32, train_wall=91, gb_free=17.2, wall=8524
2023-08-08 02:28:42 | INFO | train_inner | epoch 007:    662 / 1474 loss=2.075, trans_loss=3.547, nll_loss=1.715, w2v_ctc_loss=1.108, task_loss=0.936, contrastive_loss=0.119, total=4149.29, n_correct=2425.45, ppl=3.28, accuracy=58.455, wps=13349.2, ups=1.08, wpb=12381.3, bsz=451.6, num_updates=9500, lr=0.000145095, gnorm=0.483, clip=0, loss_scale=32, train_wall=92, gb_free=16.9, wall=8617
2023-08-08 02:30:15 | INFO | train_inner | epoch 007:    762 / 1474 loss=2.077, trans_loss=3.542, nll_loss=1.709, w2v_ctc_loss=1.111, task_loss=0.966, contrastive_loss=0.119, total=4134.54, n_correct=2415.38, ppl=3.27, accuracy=58.42, wps=13224.7, ups=1.07, wpb=12345.4, bsz=449.8, num_updates=9600, lr=0.000144338, gnorm=0.482, clip=0, loss_scale=32, train_wall=93, gb_free=13.7, wall=8710
2023-08-08 02:31:49 | INFO | train_inner | epoch 007:    862 / 1474 loss=2.08, trans_loss=3.552, nll_loss=1.722, w2v_ctc_loss=1.11, task_loss=0.933, contrastive_loss=0.138, total=4151.77, n_correct=2416.43, ppl=3.3, accuracy=58.202, wps=13250.9, ups=1.07, wpb=12391.6, bsz=461.9, num_updates=9700, lr=0.000143592, gnorm=0.482, clip=0, loss_scale=32, train_wall=93, gb_free=14.7, wall=8804
2023-08-08 02:33:22 | INFO | train_inner | epoch 007:    962 / 1474 loss=2.083, trans_loss=3.545, nll_loss=1.714, w2v_ctc_loss=1.097, task_loss=0.894, contrastive_loss=0.231, total=4124.8, n_correct=2413.11, ppl=3.28, accuracy=58.502, wps=13232.7, ups=1.07, wpb=12313.3, bsz=471.2, num_updates=9800, lr=0.000142857, gnorm=0.483, clip=0, loss_scale=32, train_wall=93, gb_free=16.4, wall=8897
2023-08-08 02:34:54 | INFO | train_inner | epoch 007:   1062 / 1474 loss=2.077, trans_loss=3.557, nll_loss=1.73, w2v_ctc_loss=1.111, task_loss=0.973, contrastive_loss=0.102, total=4113.08, n_correct=2389.51, ppl=3.32, accuracy=58.095, wps=13323.9, ups=1.09, wpb=12279.6, bsz=439.4, num_updates=9900, lr=0.000142134, gnorm=0.482, clip=0, loss_scale=32, train_wall=92, gb_free=14.6, wall=8989
2023-08-08 02:36:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-08 02:36:28 | INFO | train_inner | epoch 007:   1163 / 1474 loss=2.086, trans_loss=3.543, nll_loss=1.715, w2v_ctc_loss=1.104, task_loss=0.93, contrastive_loss=0.211, total=4113.08, n_correct=2405.82, ppl=3.28, accuracy=58.492, wps=13160.3, ups=1.07, wpb=12290.8, bsz=459.5, num_updates=10000, lr=0.000141421, gnorm=0.484, clip=0, loss_scale=16, train_wall=93, gb_free=15.8, wall=9082
2023-08-08 02:36:28 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 02:36:51 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.269 | trans_loss 5.675 | nll_loss 2.984 | w2v_ctc_loss 1.322 | task_loss 4.577 | contrastive_loss 0.268 | total 4003.4 | n_correct 2408 | ppl 7.91 | accuracy 60.149 | uer 19.006 | wer 20.648 | raw_wer 20.648 | bleu 18.7 | wps 2197.6 | wpb 4003.4 | bsz 141.8 | num_updates 10000 | best_bleu 18.7
2023-08-08 02:36:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10000 updates
2023-08-08 02:36:51 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_7_10000.pt
2023-08-08 02:36:55 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_7_10000.pt
2023-08-08 02:37:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_7_10000.pt (epoch 7 @ 10000 updates, score 18.7) (writing took 42.80821528844535 seconds)
Mixup rate:0.5, token after shrink shape:torch.Size([8, 97]), X shape:torch.Size([8, 97, 512])
CTC Tokens:tensor([  19,   19, 1095, 1095,    0], device='cuda:0'), Shrink Mask:tensor([ True, False,  True, False,  True], device='cuda:0'), New Tokens:tensor([  19, 1095,    0,   91,    0], device='cuda:0')
Mixup Sent Mask:tensor([[4],
        [5],
        [6]], device='cuda:0'), 4,  Mixup Mask:tensor([ True,  True,  True, False, False], device='cuda:0'), 
                    Org X:tensor([[ 0.5234,  0.0756, -0.3865,  ..., -0.8320,  0.6626, -0.7256],
        [ 1.2051,  0.7183,  1.6455,  ...,  0.1371, -1.5479, -1.3750],
        [ 0.0327,  0.5576,  1.2812,  ...,  0.0194, -0.9028,  0.4858],
        [-1.5225, -0.1616, -0.4968,  ..., -0.1198, -0.0641,  1.6309],
        [ 0.1381,  0.6094,  1.7295,  ..., -2.6582,  1.0918,  0.3296]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.0228, -0.5034, -0.3362,  ..., -0.6865, -0.3594, -1.4170],
        [ 0.3267,  0.3818,  0.6890,  ..., -0.2861, -3.0156, -0.6548],
        [-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873],
        [-1.5225, -0.1616, -0.4968,  ..., -0.1198, -0.0641,  1.6309],
        [ 0.1381,  0.6094,  1.7295,  ..., -2.6582,  1.0918,  0.3296]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[ 0.0228, -0.5034, -0.3362,  ..., -0.6865, -0.3594, -1.4170],
        [ 0.3267,  0.3818,  0.6890,  ..., -0.2861, -3.0156, -0.6548],
        [-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873],
        [-0.4434, -0.1443,  0.0908,  ..., -0.5200,  0.4736, -1.9912],
        [-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:0')
2023-08-08 02:39:06 | INFO | train_inner | epoch 007:   1263 / 1474 loss=2.067, trans_loss=3.549, nll_loss=1.721, w2v_ctc_loss=1.097, task_loss=0.942, contrastive_loss=0.128, total=4129.52, n_correct=2410.94, ppl=3.3, accuracy=58.383, wps=7801.6, ups=0.63, wpb=12331.4, bsz=450.2, num_updates=10100, lr=0.00014072, gnorm=0.387, clip=0, loss_scale=16, train_wall=91, gb_free=16.6, wall=9241
2023-08-08 02:40:38 | INFO | train_inner | epoch 007:   1363 / 1474 loss=2.081, trans_loss=3.543, nll_loss=1.713, w2v_ctc_loss=1.107, task_loss=0.877, contrastive_loss=0.167, total=4172.87, n_correct=2444.71, ppl=3.28, accuracy=58.586, wps=13446.9, ups=1.08, wpb=12458.1, bsz=476.2, num_updates=10200, lr=0.000140028, gnorm=0.387, clip=0, loss_scale=16, train_wall=92, gb_free=17.1, wall=9333
2023-08-08 02:42:13 | INFO | train_inner | epoch 007:   1463 / 1474 loss=2.089, trans_loss=3.549, nll_loss=1.722, w2v_ctc_loss=1.105, task_loss=1.005, contrastive_loss=0.231, total=4109.42, n_correct=2396.82, ppl=3.3, accuracy=58.325, wps=13025.5, ups=1.06, wpb=12278.1, bsz=443.8, num_updates=10300, lr=0.000139347, gnorm=0.39, clip=0, loss_scale=16, train_wall=93, gb_free=16.2, wall=9427
2023-08-08 02:42:22 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([32, 49]), X shape:torch.Size([32, 49, 512])
CTC Tokens:tensor([   0,   33,    0, 2353,    0], device='cuda:6'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:6'), New Tokens:tensor([   0,   33,    0, 2353,    0], device='cuda:6')
Mixup Sent Mask:tensor([[ 4],
        [ 5],
        [ 6],
        [17],
        [18],
        [19]], device='cuda:6'), 4,  Mixup Mask:tensor([ True, False, False, False,  True], device='cuda:6'), 
                    Org X:tensor([[-0.4001,  0.4307, -0.8203,  ..., -1.9062, -0.3501,  0.9009],
        [-1.1709,  0.4666, -0.7368,  ..., -0.9658, -0.2019,  0.6997],
        [-0.7778,  0.6074,  0.8037,  ..., -3.0957,  1.3975,  0.5049],
        [-1.1113,  0.1622,  1.3418,  ..., -0.4741,  1.4014, -0.3110],
        [-0.8828,  0.0809,  2.1641,  ..., -0.7744,  0.7632,  0.5308]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873],
        [-1.1709,  0.4666, -0.7368,  ..., -0.9658, -0.2019,  0.6997],
        [-0.7778,  0.6074,  0.8037,  ..., -3.0957,  1.3975,  0.5049],
        [-1.1113,  0.1622,  1.3418,  ..., -0.4741,  1.4014, -0.3110],
        [-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873],
        [-0.2959, -0.3008, -0.1650,  ..., -2.2617, -3.5898, -0.3994],
        [-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873],
        [-0.0219, -1.9600,  1.0117,  ..., -1.0391, -3.2812,  1.3555],
        [-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 92]), X shape:torch.Size([8, 92, 512])
CTC Tokens:tensor([53, 53,  0,  0,  0], device='cuda:5'), Shrink Mask:tensor([ True, False,  True, False, False], device='cuda:5'), New Tokens:tensor([  53,    0, 2318,    0,  388], device='cuda:5')
Mixup Sent Mask:tensor([[4],
        [5],
        [6]], device='cuda:5'), 4,  Mixup Mask:tensor([False, False, False,  True,  True], device='cuda:5'), 
                    Org X:tensor([[ 0.2158, -0.6353, -0.8198,  ..., -1.0879,  0.6216, -1.2412],
        [ 0.7920, -0.0931,  0.8999,  ..., -1.1289,  1.7100, -0.9131],
        [ 0.2463, -0.1429,  0.2152,  ..., -0.4050,  1.2793,  0.5591],
        [ 0.3003,  0.3835,  1.3379,  ...,  0.2024,  1.9580,  1.6484],
        [ 0.3943, -0.2571, -0.8535,  ..., -0.0593,  2.1152, -1.9834]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.2158, -0.6353, -0.8198,  ..., -1.0879,  0.6216, -1.2412],
        [ 0.7920, -0.0931,  0.8999,  ..., -1.1289,  1.7100, -0.9131],
        [ 0.2463, -0.1429,  0.2152,  ..., -0.4050,  1.2793,  0.5591],
        [-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873],
        [-1.6797, -1.2979,  0.4795,  ...,  0.1279,  2.2852, -3.7422]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-8.1116e-02, -1.1957e-01,  8.4543e-04,  ..., -2.4355e+00,
          3.3887e+00, -2.7168e+00],
        [-5.8691e-01, -1.0771e+00, -3.5980e-02,  ..., -2.5371e+00,
         -1.8477e+00, -1.9873e+00],
        [ 7.5317e-02, -4.9316e-02,  5.3467e-01,  ...,  1.4238e+00,
          1.7246e+00, -7.2559e-01],
        [-5.8691e-01, -1.0771e+00, -3.5980e-02,  ..., -2.5371e+00,
         -1.8477e+00, -1.9873e+00],
        [-1.6797e+00, -1.2979e+00,  4.7949e-01,  ...,  1.2793e-01,
          2.2852e+00, -3.7422e+00]], device='cuda:5', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 68]), X shape:torch.Size([16, 68, 512])
CTC Tokens:tensor([ 19,  19,  66,   0, 121], device='cuda:4'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:4'), New Tokens:tensor([  19,   66,    0,  121, 8525], device='cuda:4')
Mixup Sent Mask:tensor([[4],
        [5],
        [6]], device='cuda:4'), 4,  Mixup Mask:tensor([False,  True, False,  True,  True], device='cuda:4'), 
                    Org X:tensor([[ 0.7446,  0.1060,  0.0425,  ..., -0.2998,  0.2815, -0.0086],
        [ 0.6948,  0.7227,  0.2893,  ..., -0.2539,  0.7056,  0.9019],
        [ 0.5811,  1.2910,  0.9644,  ..., -0.5781,  0.8403,  0.1981],
        [ 1.8418,  0.3254,  0.3088,  ..., -0.3845,  1.1533, -1.2285],
        [ 1.2422, -0.4534, -0.0067,  ..., -0.1974,  0.8613, -2.2070]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 7.4463e-01,  1.0596e-01,  4.2480e-02,  ..., -2.9980e-01,
          2.8149e-01, -8.6365e-03],
        [-3.9185e-01,  1.1188e-01, -4.4873e-01,  ...,  1.4209e+00,
          2.2227e+00, -1.3538e-01],
        [ 5.8105e-01,  1.2910e+00,  9.6436e-01,  ..., -5.7812e-01,
          8.4033e-01,  1.9812e-01],
        [ 4.0820e-01,  1.2227e+00, -9.4414e-04,  ..., -3.1567e-01,
          1.3945e+00,  9.8242e-01],
        [ 1.2246e+00, -5.1904e-01, -1.2344e+00,  ...,  4.3408e-01,
         -4.6021e-01, -1.3203e+00]], device='cuda:4', dtype=torch.float16,
       grad_fn=<SliceBackward0>), Emb: tensor([[ 2.2827e-02, -5.0342e-01, -3.3618e-01,  ..., -6.8652e-01,
         -3.5938e-01, -1.4170e+00],
        [-3.9185e-01,  1.1188e-01, -4.4873e-01,  ...,  1.4209e+00,
          2.2227e+00, -1.3538e-01],
        [-5.8691e-01, -1.0771e+00, -3.5980e-02,  ..., -2.5371e+00,
         -1.8477e+00, -1.9873e+00],
        [ 4.0820e-01,  1.2227e+00, -9.4414e-04,  ..., -3.1567e-01,
          1.3945e+00,  9.8242e-01],
        [ 1.2246e+00, -5.1904e-01, -1.2344e+00,  ...,  4.3408e-01,
         -4.6021e-01, -1.3203e+00]], device='cuda:4', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 81]), X shape:torch.Size([8, 81, 512])
CTC Tokens:tensor([67, 67, 24, 24,  0], device='cuda:3'), Shrink Mask:tensor([ True, False,  True, False,  True], device='cuda:3'), New Tokens:tensor([ 67,  24,   0, 135,   0], device='cuda:3')
Mixup Sent Mask:tensor([[4],
        [5],
        [6]], device='cuda:3'), 4,  Mixup Mask:tensor([False, False,  True,  True,  True], device='cuda:3'), 
                    Org X:tensor([[-0.2783,  0.2062, -0.5376,  ..., -0.0234,  0.0512, -3.0625],
        [-2.9395,  0.3457,  0.1877,  ..., -0.0872,  0.7456, -2.2207],
        [ 1.8135,  1.0869,  0.4592,  ..., -0.0400,  0.1251,  0.2271],
        [ 1.2881,  0.3320, -0.5547,  ...,  0.2135, -1.4775,  0.1180],
        [ 1.4453,  0.7334,  0.6821,  ..., -1.5693, -0.8525, -0.0285]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.2783,  0.2062, -0.5376,  ..., -0.0234,  0.0512, -3.0625],
        [-2.9395,  0.3457,  0.1877,  ..., -0.0872,  0.7456, -2.2207],
        [-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873],
        [-0.6348, -0.9541, -1.0957,  ...,  2.2891, -3.3066, -0.3784],
        [-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[ 0.0734, -0.2639, -1.1816,  ...,  1.6914, -1.9238, -4.1641],
        [-0.6514, -0.6768,  0.8423,  ..., -4.6328,  2.0273, -1.5811],
        [-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873],
        [-0.6348, -0.9541, -1.0957,  ...,  2.2891, -3.3066, -0.3784],
        [-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([32, 39]), X shape:torch.Size([32, 39, 512])
CTC Tokens:tensor([   0,   19,   19,   34, 1290], device='cuda:7'), Shrink Mask:tensor([ True,  True, False,  True,  True], device='cuda:7'), New Tokens:tensor([   0,   19,   34, 1290,  712], device='cuda:7')
Mixup Sent Mask:tensor([[ 4],
        [ 5],
        [ 6],
        [17],
        [18],
        [19]], device='cuda:7'), 4,  Mixup Mask:tensor([ True,  True, False,  True,  True], device='cuda:7'), 
                    Org X:tensor([[ 0.8027, -0.8213,  0.7070,  ...,  0.5898,  0.1716, -0.2062],
        [ 0.2727,  0.2886, -0.0764,  ...,  0.1810, -0.1143, -0.6294],
        [ 0.9595, -0.2124,  0.3369,  ..., -0.3960, -0.0488,  1.1191],
        [-0.1208, -0.2517, -0.3501,  ..., -0.7910,  0.1578,  0.5698],
        [-0.0591,  1.2900,  0.1473,  ..., -0.6611, -0.6216,  1.5049]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873],
        [ 0.0228, -0.5034, -0.3362,  ..., -0.6865, -0.3594, -1.4170],
        [ 0.9595, -0.2124,  0.3369,  ..., -0.3960, -0.0488,  1.1191],
        [ 0.0524,  0.1342, -0.8350,  ..., -0.8833,  1.1826, -0.3633],
        [ 1.4971,  0.4617,  0.0336,  ...,  2.6250, -0.9697, -2.5527]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873],
        [ 0.0228, -0.5034, -0.3362,  ..., -0.6865, -0.3594, -1.4170],
        [-0.2717, -1.1387, -0.0605,  ..., -1.7666, -3.1270, -3.5137],
        [ 0.0524,  0.1342, -0.8350,  ..., -0.8833,  1.1826, -0.3633],
        [ 1.4971,  0.4617,  0.0336,  ...,  2.6250, -0.9697, -2.5527]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 160]), X shape:torch.Size([8, 160, 512])
CTC Tokens:tensor([ 101,  246,    4,   38, 1969], device='cuda:2'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:2'), New Tokens:tensor([ 101,  246,    4,   38, 1969], device='cuda:2')
Mixup Sent Mask:tensor([[4],
        [5],
        [6]], device='cuda:2'), 4,  Mixup Mask:tensor([False, False,  True,  True,  True], device='cuda:2'), 
                    Org X:tensor([[ 0.1007, -0.0503, -0.4900,  ..., -0.2106, -0.4387,  0.1324],
        [ 0.2053,  0.3508, -0.4062,  ..., -0.5176, -1.1367, -0.1708],
        [ 1.8184,  0.2394, -0.9292,  ..., -1.8086, -2.5410, -1.6367],
        [ 0.8872,  0.3252, -0.7520,  ..., -2.5273, -0.2510, -2.7168],
        [-0.0876,  0.2152, -0.0729,  ..., -0.7490,  1.0195, -2.1289]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.1007, -0.0503, -0.4900,  ..., -0.2106, -0.4387,  0.1324],
        [ 0.2053,  0.3508, -0.4062,  ..., -0.5176, -1.1367, -0.1708],
        [-0.2333, -0.6294, -0.5088,  ..., -1.7441, -1.3662, -2.5488],
        [-1.2598, -0.5693,  0.3494,  ...,  0.2498, -0.1401, -1.0459],
        [ 0.2280,  0.4250,  0.2961,  ...,  0.1383,  2.0605, -1.5361]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5444, -0.2076,  0.2744,  ...,  0.1293, -1.6875,  1.5215],
        [ 0.3420,  0.1951, -1.5908,  ..., -2.2051, -1.0273, -3.0488],
        [-0.2333, -0.6294, -0.5088,  ..., -1.7441, -1.3662, -2.5488],
        [-1.2598, -0.5693,  0.3494,  ...,  0.2498, -0.1401, -1.0459],
        [ 0.2280,  0.4250,  0.2961,  ...,  0.1383,  2.0605, -1.5361]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([32, 37]), X shape:torch.Size([32, 37, 512])
CTC Tokens:tensor([  9,   9, 409,   0,   0], device='cuda:1'), Shrink Mask:tensor([ True, False,  True,  True, False], device='cuda:1'), New Tokens:tensor([  9, 409,   0,   4,   0], device='cuda:1')
Mixup Sent Mask:tensor([[ 4],
        [ 5],
        [ 6],
        [17],
        [18],
        [19]], device='cuda:1'), 4,  Mixup Mask:tensor([ True,  True, False, False, False], device='cuda:1'), 
                    Org X:tensor([[ 0.3684, -1.8057, -0.7915,  ...,  0.5156, -1.1240, -0.6152],
        [ 0.3542, -0.6440,  0.6055,  ...,  0.2615, -0.5161, -0.0098],
        [ 0.6167, -0.0699,  1.4453,  ...,  0.4028, -0.1652,  0.4858],
        [ 0.8770,  0.3928,  0.6470,  ...,  0.3306,  1.5225, -1.9883],
        [ 0.5889,  0.4402,  0.4478,  ...,  0.4587,  1.3271, -2.0156]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3760, -0.4683, -0.3657,  ...,  0.3640, -1.2803,  1.5215],
        [-0.5312,  0.5166,  0.1173,  ...,  7.5742, -1.7539, -0.1799],
        [ 0.6167, -0.0699,  1.4453,  ...,  0.4028, -0.1652,  0.4858],
        [ 0.8770,  0.3928,  0.6470,  ...,  0.3306,  1.5225, -1.9883],
        [ 0.5889,  0.4402,  0.4478,  ...,  0.4587,  1.3271, -2.0156]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3760, -0.4683, -0.3657,  ...,  0.3640, -1.2803,  1.5215],
        [-0.5312,  0.5166,  0.1173,  ...,  7.5742, -1.7539, -0.1799],
        [-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873],
        [-0.2333, -0.6294, -0.5088,  ..., -1.7441, -1.3662, -2.5488],
        [-0.5869, -1.0771, -0.0360,  ..., -2.5371, -1.8477, -1.9873]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:1')
2023-08-08 02:42:46 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.268 | trans_loss 5.668 | nll_loss 2.973 | w2v_ctc_loss 1.333 | task_loss 4.589 | contrastive_loss 0.264 | total 4003.4 | n_correct 2404.3 | ppl 7.85 | accuracy 60.056 | uer 19.064 | wer 20.846 | raw_wer 20.846 | bleu 18.97 | wps 2193.9 | wpb 4003.4 | bsz 141.8 | num_updates 10311 | best_bleu 18.97
2023-08-08 02:42:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10311 updates
2023-08-08 02:42:46 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 02:42:59 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 02:43:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt (epoch 7 @ 10311 updates, score 18.97) (writing took 25.057775411754847 seconds)
2023-08-08 02:43:11 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-08-08 02:43:11 | INFO | train | epoch 007 | loss 2.086 | trans_loss 3.546 | nll_loss 1.715 | w2v_ctc_loss 1.109 | task_loss 0.933 | contrastive_loss 0.185 | total 4137.25 | n_correct 2413.96 | ppl 3.28 | accuracy 58.347 | wps 12220.3 | ups 0.99 | wpb 12351.7 | bsz 457.8 | num_updates 10311 | lr 0.000139272 | gnorm 0.462 | clip 0 | loss_scale 16 | train_wall 1357 | gb_free 13 | wall 9486
2023-08-08 02:43:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 02:43:12 | INFO | fairseq.trainer | begin training epoch 8
2023-08-08 02:43:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 02:44:42 | INFO | train_inner | epoch 008:     89 / 1474 loss=2.034, trans_loss=3.523, nll_loss=1.681, w2v_ctc_loss=1.068, task_loss=0.984, contrastive_loss=0.125, total=4116.25, n_correct=2440.64, ppl=3.21, accuracy=59.293, wps=8195.8, ups=0.67, wpb=12273, bsz=443.3, num_updates=10400, lr=0.000138675, gnorm=0.388, clip=0, loss_scale=16, train_wall=93, gb_free=16.8, wall=9577
2023-08-08 02:46:14 | INFO | train_inner | epoch 008:    189 / 1474 loss=2.034, trans_loss=3.516, nll_loss=1.672, w2v_ctc_loss=1.064, task_loss=1.011, contrastive_loss=0.145, total=4037.23, n_correct=2400.09, ppl=3.19, accuracy=59.449, wps=13110.4, ups=1.09, wpb=12041.5, bsz=428.6, num_updates=10500, lr=0.000138013, gnorm=0.392, clip=0, loss_scale=16, train_wall=91, gb_free=12.5, wall=9669
2023-08-08 02:47:47 | INFO | train_inner | epoch 008:    289 / 1474 loss=2.032, trans_loss=3.512, nll_loss=1.67, w2v_ctc_loss=1.063, task_loss=0.875, contrastive_loss=0.146, total=4207.78, n_correct=2504.23, ppl=3.18, accuracy=59.514, wps=13534.1, ups=1.08, wpb=12556.5, bsz=488.1, num_updates=10600, lr=0.000137361, gnorm=0.386, clip=0, loss_scale=16, train_wall=92, gb_free=12.7, wall=9762
2023-08-08 02:49:20 | INFO | train_inner | epoch 008:    389 / 1474 loss=2.047, trans_loss=3.52, nll_loss=1.68, w2v_ctc_loss=1.079, task_loss=0.993, contrastive_loss=0.166, total=4127.24, n_correct=2445.55, ppl=3.2, accuracy=59.254, wps=13204.2, ups=1.07, wpb=12316.2, bsz=441.4, num_updates=10700, lr=0.000136717, gnorm=0.391, clip=0, loss_scale=16, train_wall=93, gb_free=11.5, wall=9855
2023-08-08 02:50:54 | INFO | train_inner | epoch 008:    489 / 1474 loss=2.077, trans_loss=3.516, nll_loss=1.677, w2v_ctc_loss=1.057, task_loss=0.834, contrastive_loss=0.428, total=4203.76, n_correct=2495.65, ppl=3.2, accuracy=59.367, wps=13372.1, ups=1.07, wpb=12548.2, bsz=504.5, num_updates=10800, lr=0.000136083, gnorm=0.387, clip=0, loss_scale=16, train_wall=93, gb_free=14.4, wall=9949
2023-08-08 02:52:26 | INFO | train_inner | epoch 008:    589 / 1474 loss=2.038, trans_loss=3.519, nll_loss=1.684, w2v_ctc_loss=1.08, task_loss=1.018, contrastive_loss=0.1, total=4062.5, n_correct=2400.88, ppl=3.21, accuracy=59.099, wps=13176.2, ups=1.08, wpb=12145.4, bsz=427.9, num_updates=10900, lr=0.000135457, gnorm=0.388, clip=0, loss_scale=16, train_wall=92, gb_free=11, wall=10041
2023-08-08 02:53:59 | INFO | train_inner | epoch 008:    689 / 1474 loss=2.03, trans_loss=3.513, nll_loss=1.672, w2v_ctc_loss=1.072, task_loss=0.959, contrastive_loss=0.112, total=4142.78, n_correct=2469.45, ppl=3.19, accuracy=59.609, wps=13295, ups=1.08, wpb=12364.4, bsz=448.6, num_updates=11000, lr=0.00013484, gnorm=0.385, clip=0, loss_scale=16, train_wall=93, gb_free=15.7, wall=10134
2023-08-08 02:55:31 | INFO | train_inner | epoch 008:    789 / 1474 loss=2.04, trans_loss=3.515, nll_loss=1.679, w2v_ctc_loss=1.067, task_loss=0.958, contrastive_loss=0.196, total=4118.9, n_correct=2443.12, ppl=3.2, accuracy=59.315, wps=13380.1, ups=1.09, wpb=12310.9, bsz=447.8, num_updates=11100, lr=0.000134231, gnorm=0.388, clip=0, loss_scale=16, train_wall=91, gb_free=15.1, wall=10226
2023-08-08 02:57:04 | INFO | train_inner | epoch 008:    889 / 1474 loss=2.036, trans_loss=3.515, nll_loss=1.679, w2v_ctc_loss=1.056, task_loss=0.896, contrastive_loss=0.207, total=4169.01, n_correct=2484.42, ppl=3.2, accuracy=59.593, wps=13471.9, ups=1.08, wpb=12452.5, bsz=473.7, num_updates=11200, lr=0.000133631, gnorm=0.389, clip=0, loss_scale=16, train_wall=92, gb_free=15.9, wall=10319
2023-08-08 02:58:35 | INFO | train_inner | epoch 008:    989 / 1474 loss=2.021, trans_loss=3.518, nll_loss=1.681, w2v_ctc_loss=1.057, task_loss=0.891, contrastive_loss=0.111, total=4154.69, n_correct=2472.72, ppl=3.21, accuracy=59.516, wps=13520.3, ups=1.09, wpb=12403.4, bsz=464.9, num_updates=11300, lr=0.000133038, gnorm=0.385, clip=0, loss_scale=16, train_wall=91, gb_free=17.6, wall=10410
2023-08-08 03:00:09 | INFO | train_inner | epoch 008:   1089 / 1474 loss=2.052, trans_loss=3.524, nll_loss=1.688, w2v_ctc_loss=1.057, task_loss=0.926, contrastive_loss=0.33, total=4199.1, n_correct=2488.43, ppl=3.22, accuracy=59.261, wps=13350.4, ups=1.07, wpb=12534.3, bsz=465.3, num_updates=11400, lr=0.000132453, gnorm=0.387, clip=0, loss_scale=16, train_wall=93, gb_free=12.4, wall=10504
2023-08-08 03:01:41 | INFO | train_inner | epoch 008:   1189 / 1474 loss=2.028, trans_loss=3.518, nll_loss=1.682, w2v_ctc_loss=1.062, task_loss=0.882, contrastive_loss=0.119, total=4177.31, n_correct=2484.59, ppl=3.21, accuracy=59.478, wps=13598.4, ups=1.09, wpb=12476.3, bsz=472.6, num_updates=11500, lr=0.000131876, gnorm=0.385, clip=0, loss_scale=16, train_wall=91, gb_free=14.7, wall=10596
2023-08-08 03:03:13 | INFO | train_inner | epoch 008:   1289 / 1474 loss=2.037, trans_loss=3.523, nll_loss=1.69, w2v_ctc_loss=1.072, task_loss=0.973, contrastive_loss=0.142, total=4063.85, n_correct=2404.53, ppl=3.23, accuracy=59.169, wps=13266.2, ups=1.09, wpb=12140.6, bsz=438.4, num_updates=11600, lr=0.000131306, gnorm=0.392, clip=0, loss_scale=16, train_wall=91, gb_free=16.6, wall=10687
2023-08-08 03:04:45 | INFO | train_inner | epoch 008:   1389 / 1474 loss=2.038, trans_loss=3.523, nll_loss=1.69, w2v_ctc_loss=1.06, task_loss=0.922, contrastive_loss=0.192, total=4141.5, n_correct=2462.56, ppl=3.23, accuracy=59.461, wps=13454, ups=1.09, wpb=12367.2, bsz=461.5, num_updates=11700, lr=0.000130744, gnorm=0.384, clip=0, loss_scale=16, train_wall=91, gb_free=16.2, wall=10779
2023-08-08 03:06:03 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 03:06:25 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 4.247 | trans_loss 5.645 | nll_loss 2.939 | w2v_ctc_loss 1.317 | task_loss 4.588 | contrastive_loss 0.266 | total 4003.4 | n_correct 2426.8 | ppl 7.67 | accuracy 60.618 | uer 18.552 | wer 20.413 | raw_wer 20.413 | bleu 19.04 | wps 2258.3 | wpb 4003.4 | bsz 141.8 | num_updates 11785 | best_bleu 19.04
2023-08-08 03:06:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11785 updates
2023-08-08 03:06:25 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 03:06:40 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 03:06:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt (epoch 8 @ 11785 updates, score 19.04) (writing took 26.27494490891695 seconds)
2023-08-08 03:06:52 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-08-08 03:06:52 | INFO | train | epoch 008 | loss 2.039 | trans_loss 3.518 | nll_loss 1.681 | w2v_ctc_loss 1.064 | task_loss 0.933 | contrastive_loss 0.187 | total 4138.65 | n_correct 2458.12 | ppl 3.21 | accuracy 59.394 | wps 12818.4 | ups 1.04 | wpb 12355.8 | bsz 458.5 | num_updates 11785 | lr 0.000130272 | gnorm 0.388 | clip 0 | loss_scale 16 | train_wall 1356 | gb_free 16.7 | wall 10907
2023-08-08 03:06:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 03:06:52 | INFO | fairseq.trainer | begin training epoch 9
2023-08-08 03:06:52 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 03:07:14 | INFO | train_inner | epoch 009:     15 / 1474 loss=2.037, trans_loss=3.517, nll_loss=1.68, w2v_ctc_loss=1.043, task_loss=0.898, contrastive_loss=0.317, total=4139.35, n_correct=2466.34, ppl=3.2, accuracy=59.583, wps=8241.2, ups=0.67, wpb=12350.9, bsz=472.9, num_updates=11800, lr=0.000130189, gnorm=0.392, clip=0, loss_scale=16, train_wall=92, gb_free=15.3, wall=10929
2023-08-08 03:08:47 | INFO | train_inner | epoch 009:    115 / 1474 loss=1.984, trans_loss=3.48, nll_loss=1.632, w2v_ctc_loss=1.018, task_loss=0.887, contrastive_loss=0.141, total=4181.9, n_correct=2535.23, ppl=3.1, accuracy=60.624, wps=13504.2, ups=1.08, wpb=12488.1, bsz=475.9, num_updates=11900, lr=0.000129641, gnorm=0.381, clip=0, loss_scale=16, train_wall=92, gb_free=16.1, wall=11022
2023-08-08 03:10:19 | INFO | train_inner | epoch 009:    215 / 1474 loss=1.983, trans_loss=3.489, nll_loss=1.643, w2v_ctc_loss=1.023, task_loss=1.005, contrastive_loss=0.098, total=4062.07, n_correct=2453.76, ppl=3.12, accuracy=60.407, wps=13146.7, ups=1.08, wpb=12129.1, bsz=431.6, num_updates=12000, lr=0.000129099, gnorm=0.389, clip=0, loss_scale=16, train_wall=92, gb_free=15.4, wall=11114
2023-08-08 03:10:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 03:10:42 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.247 | trans_loss 5.655 | nll_loss 2.952 | w2v_ctc_loss 1.291 | task_loss 4.59 | contrastive_loss 0.272 | total 4003.4 | n_correct 2412 | ppl 7.74 | accuracy 60.249 | uer 18.422 | wer 20.238 | raw_wer 20.238 | bleu 19.12 | wps 2310.4 | wpb 4003.4 | bsz 141.8 | num_updates 12000 | best_bleu 19.12
2023-08-08 03:10:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12000 updates
2023-08-08 03:10:42 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_9_12000.pt
2023-08-08 03:10:45 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_9_12000.pt
2023-08-08 03:11:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_9_12000.pt (epoch 9 @ 12000 updates, score 19.12) (writing took 45.16481155157089 seconds)
2023-08-08 03:13:00 | INFO | train_inner | epoch 009:    315 / 1474 loss=1.978, trans_loss=3.476, nll_loss=1.629, w2v_ctc_loss=1.01, task_loss=0.871, contrastive_loss=0.148, total=4152.1, n_correct=2522.29, ppl=3.09, accuracy=60.747, wps=7696, ups=0.62, wpb=12407.3, bsz=476.6, num_updates=12100, lr=0.000128565, gnorm=0.387, clip=0, loss_scale=32, train_wall=92, gb_free=16.1, wall=11275
2023-08-08 03:14:33 | INFO | train_inner | epoch 009:    415 / 1474 loss=1.983, trans_loss=3.491, nll_loss=1.647, w2v_ctc_loss=1.019, task_loss=0.912, contrastive_loss=0.114, total=4203.78, n_correct=2534.37, ppl=3.13, accuracy=60.288, wps=13518.6, ups=1.08, wpb=12551.8, bsz=469.8, num_updates=12200, lr=0.000128037, gnorm=0.382, clip=0, loss_scale=32, train_wall=92, gb_free=17, wall=11368
2023-08-08 03:16:06 | INFO | train_inner | epoch 009:    515 / 1474 loss=2.012, trans_loss=3.497, nll_loss=1.653, w2v_ctc_loss=1.043, task_loss=0.981, contrastive_loss=0.165, total=4112.78, n_correct=2473.8, ppl=3.15, accuracy=60.149, wps=13261.1, ups=1.08, wpb=12275.6, bsz=437.7, num_updates=12300, lr=0.000127515, gnorm=0.39, clip=0, loss_scale=32, train_wall=92, gb_free=16, wall=11461
2023-08-08 03:17:38 | INFO | train_inner | epoch 009:    615 / 1474 loss=1.979, trans_loss=3.487, nll_loss=1.644, w2v_ctc_loss=1.014, task_loss=0.949, contrastive_loss=0.124, total=4131.32, n_correct=2497.72, ppl=3.13, accuracy=60.458, wps=13418.3, ups=1.09, wpb=12347.4, bsz=455, num_updates=12400, lr=0.000127, gnorm=0.386, clip=0, loss_scale=32, train_wall=92, gb_free=17.7, wall=11553
2023-08-08 03:19:10 | INFO | train_inner | epoch 009:    715 / 1474 loss=2.014, trans_loss=3.501, nll_loss=1.661, w2v_ctc_loss=1.04, task_loss=0.952, contrastive_loss=0.207, total=4082.11, n_correct=2448.6, ppl=3.16, accuracy=59.984, wps=13197.7, ups=1.08, wpb=12194.8, bsz=449.7, num_updates=12500, lr=0.000126491, gnorm=0.392, clip=0, loss_scale=32, train_wall=92, gb_free=16.8, wall=11645
2023-08-08 03:19:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-08 03:20:44 | INFO | train_inner | epoch 009:    816 / 1474 loss=2.019, trans_loss=3.492, nll_loss=1.651, w2v_ctc_loss=1.031, task_loss=0.858, contrastive_loss=0.273, total=4202.99, n_correct=2532.64, ppl=3.14, accuracy=60.258, wps=13371.1, ups=1.06, wpb=12557.2, bsz=492, num_updates=12600, lr=0.000125988, gnorm=0.39, clip=0, loss_scale=16, train_wall=93, gb_free=14.2, wall=11739
2023-08-08 03:22:18 | INFO | train_inner | epoch 009:    916 / 1474 loss=2.018, trans_loss=3.497, nll_loss=1.653, w2v_ctc_loss=1.029, task_loss=0.962, contrastive_loss=0.331, total=4146.05, n_correct=2493.87, ppl=3.15, accuracy=60.151, wps=13150, ups=1.06, wpb=12371.5, bsz=450.3, num_updates=12700, lr=0.000125491, gnorm=0.39, clip=0, loss_scale=16, train_wall=94, gb_free=17.7, wall=11833
2023-08-08 03:23:50 | INFO | train_inner | epoch 009:   1016 / 1474 loss=2.001, trans_loss=3.507, nll_loss=1.666, w2v_ctc_loss=1.037, task_loss=1.04, contrastive_loss=0.112, total=4101.48, n_correct=2455.44, ppl=3.17, accuracy=59.867, wps=13328.4, ups=1.09, wpb=12241.7, bsz=424.4, num_updates=12800, lr=0.000125, gnorm=0.387, clip=0, loss_scale=16, train_wall=91, gb_free=15.7, wall=11925
2023-08-08 03:25:22 | INFO | train_inner | epoch 009:   1116 / 1474 loss=1.994, trans_loss=3.502, nll_loss=1.658, w2v_ctc_loss=1.025, task_loss=0.879, contrastive_loss=0.136, total=4179.09, n_correct=2516.29, ppl=3.16, accuracy=60.211, wps=13484.5, ups=1.08, wpb=12457.7, bsz=474.7, num_updates=12900, lr=0.000124515, gnorm=0.385, clip=0, loss_scale=16, train_wall=92, gb_free=15.1, wall=12017
2023-08-08 03:26:56 | INFO | train_inner | epoch 009:   1216 / 1474 loss=2.002, trans_loss=3.502, nll_loss=1.662, w2v_ctc_loss=1.04, task_loss=0.985, contrastive_loss=0.12, total=4140.66, n_correct=2489.45, ppl=3.17, accuracy=60.122, wps=13282.2, ups=1.07, wpb=12363.4, bsz=448.1, num_updates=13000, lr=0.000124035, gnorm=0.39, clip=0, loss_scale=16, train_wall=93, gb_free=16.9, wall=12110
2023-08-08 03:28:29 | INFO | train_inner | epoch 009:   1316 / 1474 loss=2.013, trans_loss=3.497, nll_loss=1.656, w2v_ctc_loss=1.018, task_loss=0.847, contrastive_loss=0.309, total=4204.43, n_correct=2537.35, ppl=3.15, accuracy=60.349, wps=13429.2, ups=1.07, wpb=12544.9, bsz=492.5, num_updates=13100, lr=0.00012356, gnorm=0.387, clip=0, loss_scale=16, train_wall=93, gb_free=17.6, wall=12204
2023-08-08 03:30:00 | INFO | train_inner | epoch 009:   1416 / 1474 loss=1.999, trans_loss=3.51, nll_loss=1.672, w2v_ctc_loss=1.037, task_loss=1.01, contrastive_loss=0.096, total=4069.19, n_correct=2440.81, ppl=3.19, accuracy=59.983, wps=13328.3, ups=1.1, wpb=12143.2, bsz=427.7, num_updates=13200, lr=0.000123091, gnorm=0.389, clip=0, loss_scale=16, train_wall=91, gb_free=16.4, wall=12295
2023-08-08 03:30:53 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 03:31:17 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.239 | trans_loss 5.626 | nll_loss 2.921 | w2v_ctc_loss 1.336 | task_loss 4.583 | contrastive_loss 0.261 | total 4003.4 | n_correct 2434.3 | ppl 7.57 | accuracy 60.806 | uer 18.374 | wer 20.23 | raw_wer 20.23 | bleu 19.62 | wps 2118 | wpb 4003.4 | bsz 141.8 | num_updates 13258 | best_bleu 19.62
2023-08-08 03:31:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 13258 updates
2023-08-08 03:31:17 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 03:31:30 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 03:31:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt (epoch 9 @ 13258 updates, score 19.62) (writing took 24.477548295632005 seconds)
2023-08-08 03:31:42 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-08-08 03:31:42 | INFO | train | epoch 009 | loss 1.999 | trans_loss 3.495 | nll_loss 1.652 | w2v_ctc_loss 1.027 | task_loss 0.934 | contrastive_loss 0.175 | total 4137.53 | n_correct 2493.33 | ppl 3.14 | accuracy 60.261 | wps 12214.4 | ups 0.99 | wpb 12352.4 | bsz 457.9 | num_updates 13258 | lr 0.000122822 | gnorm 0.388 | clip 0 | loss_scale 16 | train_wall 1357 | gb_free 11.4 | wall 12397
2023-08-08 03:31:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 03:31:42 | INFO | fairseq.trainer | begin training epoch 10
2023-08-08 03:31:42 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 03:32:28 | INFO | train_inner | epoch 010:     42 / 1474 loss=1.984, trans_loss=3.488, nll_loss=1.643, w2v_ctc_loss=1.005, task_loss=0.89, contrastive_loss=0.197, total=4100.8, n_correct=2488.54, ppl=3.12, accuracy=60.684, wps=8273.8, ups=0.68, wpb=12238.2, bsz=469.4, num_updates=13300, lr=0.000122628, gnorm=0.387, clip=0, loss_scale=16, train_wall=91, gb_free=16.2, wall=12443
2023-08-08 03:34:00 | INFO | train_inner | epoch 010:    142 / 1474 loss=1.943, trans_loss=3.463, nll_loss=1.612, w2v_ctc_loss=0.98, task_loss=0.881, contrastive_loss=0.118, total=4247.35, n_correct=2601.61, ppl=3.06, accuracy=61.253, wps=13724.9, ups=1.08, wpb=12684.5, bsz=479.6, num_updates=13400, lr=0.000122169, gnorm=0.382, clip=0, loss_scale=16, train_wall=92, gb_free=11.3, wall=12535
2023-08-08 03:35:33 | INFO | train_inner | epoch 010:    242 / 1474 loss=1.967, trans_loss=3.464, nll_loss=1.611, w2v_ctc_loss=0.99, task_loss=0.921, contrastive_loss=0.241, total=4122.82, n_correct=2526.92, ppl=3.05, accuracy=61.291, wps=13340.6, ups=1.08, wpb=12303.3, bsz=461.4, num_updates=13500, lr=0.000121716, gnorm=0.382, clip=0, loss_scale=16, train_wall=92, gb_free=16.1, wall=12628
2023-08-08 03:37:06 | INFO | train_inner | epoch 010:    342 / 1474 loss=1.953, trans_loss=3.463, nll_loss=1.615, w2v_ctc_loss=0.986, task_loss=0.942, contrastive_loss=0.152, total=4138.27, n_correct=2532.44, ppl=3.06, accuracy=61.196, wps=13322.4, ups=1.08, wpb=12371, bsz=453.8, num_updates=13600, lr=0.000121268, gnorm=0.386, clip=0, loss_scale=16, train_wall=92, gb_free=16.2, wall=12720
2023-08-08 03:38:39 | INFO | train_inner | epoch 010:    442 / 1474 loss=1.969, trans_loss=3.47, nll_loss=1.62, w2v_ctc_loss=0.975, task_loss=0.897, contrastive_loss=0.327, total=4196.37, n_correct=2565.21, ppl=3.07, accuracy=61.129, wps=13400.2, ups=1.07, wpb=12528, bsz=481.1, num_updates=13700, lr=0.000120824, gnorm=0.383, clip=0, loss_scale=16, train_wall=93, gb_free=15.9, wall=12814
2023-08-08 03:40:12 | INFO | train_inner | epoch 010:    542 / 1474 loss=1.968, trans_loss=3.483, nll_loss=1.633, w2v_ctc_loss=1.009, task_loss=1.001, contrastive_loss=0.106, total=4102.8, n_correct=2493.04, ppl=3.1, accuracy=60.764, wps=13210, ups=1.08, wpb=12234.1, bsz=437.8, num_updates=13800, lr=0.000120386, gnorm=0.388, clip=0, loss_scale=16, train_wall=92, gb_free=16.9, wall=12907
2023-08-08 03:41:44 | INFO | train_inner | epoch 010:    642 / 1474 loss=1.981, trans_loss=3.479, nll_loss=1.632, w2v_ctc_loss=1.001, task_loss=0.887, contrastive_loss=0.223, total=4176.56, n_correct=2546.15, ppl=3.1, accuracy=60.963, wps=13497.1, ups=1.08, wpb=12464, bsz=477.2, num_updates=13900, lr=0.000119952, gnorm=0.391, clip=0, loss_scale=16, train_wall=92, gb_free=16, wall=12999
2023-08-08 03:43:16 | INFO | train_inner | epoch 010:    742 / 1474 loss=1.973, trans_loss=3.48, nll_loss=1.633, w2v_ctc_loss=1.016, task_loss=0.935, contrastive_loss=0.105, total=4125.87, n_correct=2506.75, ppl=3.1, accuracy=60.757, wps=13420.9, ups=1.09, wpb=12315.3, bsz=454.4, num_updates=14000, lr=0.000119523, gnorm=0.395, clip=0, loss_scale=16, train_wall=91, gb_free=14.2, wall=13091
2023-08-08 03:43:16 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 03:43:40 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.236 | trans_loss 5.626 | nll_loss 2.916 | w2v_ctc_loss 1.321 | task_loss 4.607 | contrastive_loss 0.272 | total 4003.4 | n_correct 2441.6 | ppl 7.55 | accuracy 60.988 | uer 18.653 | wer 20.421 | raw_wer 20.421 | bleu 19.42 | wps 2058.3 | wpb 4003.4 | bsz 141.8 | num_updates 14000 | best_bleu 19.62
2023-08-08 03:43:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14000 updates
2023-08-08 03:43:40 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_10_14000.pt
2023-08-08 03:43:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_10_14000.pt
2023-08-08 03:44:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_10_14000.pt (epoch 10 @ 14000 updates, score 19.42) (writing took 35.58716846071184 seconds)
2023-08-08 03:45:49 | INFO | train_inner | epoch 010:    842 / 1474 loss=1.949, trans_loss=3.474, nll_loss=1.627, w2v_ctc_loss=0.988, task_loss=0.925, contrastive_loss=0.106, total=4128.44, n_correct=2519.94, ppl=3.09, accuracy=61.039, wps=8064.1, ups=0.65, wpb=12327.8, bsz=456.3, num_updates=14100, lr=0.000119098, gnorm=0.383, clip=0, loss_scale=16, train_wall=92, gb_free=14.6, wall=13244
2023-08-08 03:47:21 | INFO | train_inner | epoch 010:    942 / 1474 loss=1.968, trans_loss=3.477, nll_loss=1.628, w2v_ctc_loss=0.999, task_loss=0.897, contrastive_loss=0.148, total=4160.94, n_correct=2538.67, ppl=3.09, accuracy=61.012, wps=13433.9, ups=1.08, wpb=12411.1, bsz=468.1, num_updates=14200, lr=0.000118678, gnorm=0.389, clip=0, loss_scale=16, train_wall=92, gb_free=15.3, wall=13336
2023-08-08 03:48:53 | INFO | train_inner | epoch 010:   1042 / 1474 loss=1.964, trans_loss=3.481, nll_loss=1.635, w2v_ctc_loss=1.001, task_loss=1.009, contrastive_loss=0.12, total=4067.53, n_correct=2468.18, ppl=3.11, accuracy=60.68, wps=13181.7, ups=1.09, wpb=12145, bsz=434.3, num_updates=14300, lr=0.000118262, gnorm=0.393, clip=0, loss_scale=16, train_wall=92, gb_free=16.8, wall=13428
2023-08-08 03:50:26 | INFO | train_inner | epoch 010:   1142 / 1474 loss=1.973, trans_loss=3.488, nll_loss=1.643, w2v_ctc_loss=1.016, task_loss=1.041, contrastive_loss=0.101, total=4044.03, n_correct=2448.4, ppl=3.12, accuracy=60.544, wps=13053.3, ups=1.08, wpb=12074.4, bsz=422.3, num_updates=14400, lr=0.000117851, gnorm=0.392, clip=0, loss_scale=16, train_wall=92, gb_free=17.2, wall=13521
2023-08-08 03:51:58 | INFO | train_inner | epoch 010:   1242 / 1474 loss=1.961, trans_loss=3.475, nll_loss=1.632, w2v_ctc_loss=1.007, task_loss=0.956, contrastive_loss=0.096, total=4110.41, n_correct=2498.95, ppl=3.1, accuracy=60.796, wps=13368.4, ups=1.09, wpb=12291.6, bsz=445.2, num_updates=14500, lr=0.000117444, gnorm=0.391, clip=0, loss_scale=16, train_wall=91, gb_free=16.3, wall=13612
2023-08-08 03:53:30 | INFO | train_inner | epoch 010:   1342 / 1474 loss=1.962, trans_loss=3.484, nll_loss=1.641, w2v_ctc_loss=1.001, task_loss=0.953, contrastive_loss=0.109, total=4121.38, n_correct=2506.88, ppl=3.12, accuracy=60.826, wps=13374.5, ups=1.09, wpb=12308.4, bsz=451, num_updates=14600, lr=0.000117041, gnorm=0.387, clip=0, loss_scale=32, train_wall=92, gb_free=13.8, wall=13705
2023-08-08 03:55:02 | INFO | train_inner | epoch 010:   1442 / 1474 loss=1.998, trans_loss=3.489, nll_loss=1.646, w2v_ctc_loss=0.986, task_loss=0.88, contrastive_loss=0.36, total=4192.39, n_correct=2543.32, ppl=3.13, accuracy=60.665, wps=13490.3, ups=1.08, wpb=12506.1, bsz=482, num_updates=14700, lr=0.000116642, gnorm=0.39, clip=0, loss_scale=32, train_wall=92, gb_free=16.9, wall=13797
2023-08-08 03:55:31 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 03:55:54 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.224 | trans_loss 5.608 | nll_loss 2.895 | w2v_ctc_loss 1.323 | task_loss 4.604 | contrastive_loss 0.267 | total 4003.4 | n_correct 2447.4 | ppl 7.44 | accuracy 61.133 | uer 17.79 | wer 19.697 | raw_wer 19.697 | bleu 19.49 | wps 2299.3 | wpb 4003.4 | bsz 141.8 | num_updates 14732 | best_bleu 19.62
2023-08-08 03:55:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14732 updates
2023-08-08 03:55:54 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.4906.pt
2023-08-08 03:55:58 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.4906.pt
2023-08-08 03:56:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.4906.pt (epoch 10 @ 14732 updates, score 19.49) (writing took 22.47225516103208 seconds)
2023-08-08 03:56:17 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-08-08 03:56:17 | INFO | train | epoch 010 | loss 1.966 | trans_loss 3.476 | nll_loss 1.629 | w2v_ctc_loss 0.995 | task_loss 0.933 | contrastive_loss 0.176 | total 4138.65 | n_correct 2521.83 | ppl 3.09 | accuracy 60.934 | wps 12346.4 | ups 1 | wpb 12355.8 | bsz 458.5 | num_updates 14732 | lr 0.000116516 | gnorm 0.388 | clip 0 | loss_scale 32 | train_wall 1354 | gb_free 17.2 | wall 13872
2023-08-08 03:56:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 03:56:17 | INFO | fairseq.trainer | begin training epoch 11
2023-08-08 03:56:17 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 03:57:27 | INFO | train_inner | epoch 011:     68 / 1474 loss=1.936, trans_loss=3.452, nll_loss=1.598, w2v_ctc_loss=0.967, task_loss=0.865, contrastive_loss=0.181, total=4175.24, n_correct=2577.46, ppl=3.03, accuracy=61.732, wps=8607.1, ups=0.69, wpb=12463.5, bsz=478.8, num_updates=14800, lr=0.000116248, gnorm=0.38, clip=0, loss_scale=32, train_wall=91, gb_free=16.6, wall=13942
2023-08-08 03:59:00 | INFO | train_inner | epoch 011:    168 / 1474 loss=1.931, trans_loss=3.453, nll_loss=1.601, w2v_ctc_loss=0.974, task_loss=0.961, contrastive_loss=0.103, total=4087.78, n_correct=2520.67, ppl=3.03, accuracy=61.664, wps=13199.8, ups=1.08, wpb=12214.2, bsz=445.9, num_updates=14900, lr=0.000115857, gnorm=0.39, clip=0, loss_scale=32, train_wall=92, gb_free=16.3, wall=14035
2023-08-08 04:00:32 | INFO | train_inner | epoch 011:    268 / 1474 loss=1.92, trans_loss=3.453, nll_loss=1.6, w2v_ctc_loss=0.962, task_loss=0.963, contrastive_loss=0.099, total=4118.77, n_correct=2540.99, ppl=3.03, accuracy=61.693, wps=13316.2, ups=1.08, wpb=12299.1, bsz=446.5, num_updates=15000, lr=0.00011547, gnorm=0.385, clip=0, loss_scale=32, train_wall=92, gb_free=12.1, wall=14127
Mixup rate:0.5, token after shrink shape:torch.Size([8, 95]), X shape:torch.Size([8, 95, 512])
CTC Tokens:tensor([ 0,  0, 86, 86,  0], device='cuda:0'), Shrink Mask:tensor([ True, False,  True, False,  True], device='cuda:0'), New Tokens:tensor([  0,  86,   0, 116,   0], device='cuda:0')
Mixup Sent Mask:tensor([[2],
        [7]], device='cuda:0'), 2,  Mixup Mask:tensor([ True, False, False,  True,  True], device='cuda:0'), 
                    Org X:tensor([[-0.3005, -0.3840,  1.1230,  ..., -1.1211,  0.1271,  1.2803],
        [ 0.3420,  0.3308, -0.1831,  ...,  0.1796, -1.6729,  0.9146],
        [ 0.6167,  1.2646,  0.7026,  ..., -0.1063, -0.2861,  0.9224],
        [-0.1978,  0.5249, -0.4739,  ...,  0.2059, -0.2019,  1.9854],
        [ 0.6626,  0.9683,  1.8838,  ...,  0.1344, -0.0905,  0.7603]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629],
        [ 0.3420,  0.3308, -0.1831,  ...,  0.1796, -1.6729,  0.9146],
        [ 0.6167,  1.2646,  0.7026,  ..., -0.1063, -0.2861,  0.9224],
        [ 0.4619, -0.2957,  0.1578,  ...,  2.4043, -1.2227, -0.8081],
        [-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629],
        [-0.6362,  0.0492, -0.2805,  ...,  0.6445, -1.2656, -0.5469],
        [-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629],
        [ 0.4619, -0.2957,  0.1578,  ...,  2.4043, -1.2227, -0.8081],
        [-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:0')
2023-08-08 04:01:41 | INFO | train_inner | epoch 011:    368 / 1474 loss=2.088, trans_loss=5.132, nll_loss=2.382, w2v_ctc_loss=0.725, task_loss=1.431, contrastive_loss=0.08, total=4097.83, n_correct=2520.52, ppl=5.21, accuracy=61.509, wps=11941.7, ups=1.45, wpb=8240.8, bsz=298, num_updates=15100, lr=0.000115087, gnorm=0.514, clip=0, loss_scale=32, train_wall=69, gb_free=15.4, wall=14196
2023-08-08 04:02:51 | INFO | train_inner | epoch 011:    468 / 1474 loss=2.102, trans_loss=5.17, nll_loss=2.406, w2v_ctc_loss=0.719, task_loss=1.462, contrastive_loss=0.197, total=4110.64, n_correct=2518.59, ppl=5.3, accuracy=61.27, wps=11687.4, ups=1.42, wpb=8221.3, bsz=300.4, num_updates=15200, lr=0.000114708, gnorm=0.513, clip=0, loss_scale=32, train_wall=70, gb_free=16.2, wall=14266
2023-08-08 04:04:01 | INFO | train_inner | epoch 011:    568 / 1474 loss=2.102, trans_loss=5.167, nll_loss=2.404, w2v_ctc_loss=0.728, task_loss=1.498, contrastive_loss=0.197, total=4071.69, n_correct=2501.06, ppl=5.29, accuracy=61.426, wps=11703.5, ups=1.44, wpb=8143.4, bsz=293.7, num_updates=15300, lr=0.000114332, gnorm=0.515, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=14336
2023-08-08 04:05:11 | INFO | train_inner | epoch 011:    668 / 1474 loss=2.106, trans_loss=5.169, nll_loss=2.407, w2v_ctc_loss=0.727, task_loss=1.371, contrastive_loss=0.252, total=4157.2, n_correct=2549.75, ppl=5.3, accuracy=61.333, wps=11957.4, ups=1.44, wpb=8314.4, bsz=309.6, num_updates=15400, lr=0.000113961, gnorm=0.509, clip=0, loss_scale=32, train_wall=69, gb_free=16.6, wall=14405
2023-08-08 04:06:21 | INFO | train_inner | epoch 011:    768 / 1474 loss=2.1, trans_loss=5.179, nll_loss=2.42, w2v_ctc_loss=0.737, task_loss=1.404, contrastive_loss=0.077, total=4174.91, n_correct=2560.96, ppl=5.35, accuracy=61.342, wps=11938.7, ups=1.43, wpb=8349.8, bsz=306.9, num_updates=15500, lr=0.000113592, gnorm=0.511, clip=0, loss_scale=32, train_wall=69, gb_free=16.9, wall=14475
2023-08-08 04:07:29 | INFO | train_inner | epoch 011:    868 / 1474 loss=2.101, trans_loss=5.181, nll_loss=2.422, w2v_ctc_loss=0.733, task_loss=1.464, contrastive_loss=0.066, total=4118.44, n_correct=2516.81, ppl=5.36, accuracy=61.111, wps=11959, ups=1.45, wpb=8236.9, bsz=293.7, num_updates=15600, lr=0.000113228, gnorm=0.511, clip=0, loss_scale=32, train_wall=68, gb_free=10.5, wall=14544
2023-08-08 04:08:39 | INFO | train_inner | epoch 011:    968 / 1474 loss=2.1, trans_loss=5.177, nll_loss=2.418, w2v_ctc_loss=0.737, task_loss=1.426, contrastive_loss=0.08, total=4140.92, n_correct=2534.93, ppl=5.34, accuracy=61.217, wps=11900.9, ups=1.44, wpb=8281.8, bsz=301.9, num_updates=15700, lr=0.000112867, gnorm=0.516, clip=0, loss_scale=32, train_wall=69, gb_free=15.5, wall=14614
2023-08-08 04:09:47 | INFO | train_inner | epoch 011:   1068 / 1474 loss=2.097, trans_loss=5.173, nll_loss=2.414, w2v_ctc_loss=0.734, task_loss=1.371, contrastive_loss=0.099, total=4136.99, n_correct=2540.43, ppl=5.33, accuracy=61.408, wps=12103.3, ups=1.46, wpb=8274, bsz=308.8, num_updates=15800, lr=0.000112509, gnorm=0.512, clip=0, loss_scale=32, train_wall=68, gb_free=17.5, wall=14682
2023-08-08 04:10:57 | INFO | train_inner | epoch 011:   1168 / 1474 loss=2.1, trans_loss=5.181, nll_loss=2.424, w2v_ctc_loss=0.737, task_loss=1.388, contrastive_loss=0.085, total=4185.65, n_correct=2560.52, ppl=5.37, accuracy=61.174, wps=11973.8, ups=1.43, wpb=8371.3, bsz=309.8, num_updates=15900, lr=0.000112154, gnorm=0.51, clip=0, loss_scale=32, train_wall=69, gb_free=13.8, wall=14752
2023-08-08 04:12:06 | INFO | train_inner | epoch 011:   1268 / 1474 loss=2.103, trans_loss=5.174, nll_loss=2.416, w2v_ctc_loss=0.735, task_loss=1.341, contrastive_loss=0.154, total=4171.89, n_correct=2558.89, ppl=5.34, accuracy=61.336, wps=12053.3, ups=1.44, wpb=8343.8, bsz=314.1, num_updates=16000, lr=0.000111803, gnorm=0.513, clip=0, loss_scale=32, train_wall=69, gb_free=15.7, wall=14821
2023-08-08 04:12:06 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([24, 57]), X shape:torch.Size([24, 57, 512])
CTC Tokens:tensor([0, 0, 0, 0, 0], device='cuda:4'), Shrink Mask:tensor([ True, False, False, False, False], device='cuda:4'), New Tokens:tensor([ 0, 46,  0, 11,  0], device='cuda:4')
Mixup Sent Mask:tensor([[ 2],
        [ 7],
        [16],
        [21]], device='cuda:4'), 2,  Mixup Mask:tensor([False,  True, False, False,  True], device='cuda:4'), 
                    Org X:tensor([[-0.1316, -1.2188,  0.7837,  ..., -0.1139, -0.0551, -0.4534],
        [-0.0704,  1.0000,  1.4941,  ..., -0.6650, -0.6782,  0.8345],
        [-0.5020,  1.1562,  1.7412,  ..., -0.3628, -0.8071,  0.9902],
        [-0.2439,  1.2715,  2.3477,  ..., -0.6055, -1.0654,  0.9233],
        [-0.7119,  1.6553,  1.5850,  ..., -0.2444,  0.0529,  0.2866]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.1316, -1.2188,  0.7837,  ..., -0.1139, -0.0551, -0.4534],
        [-0.4207, -0.5166, -0.3027,  ...,  0.6479, -2.4082,  0.2742],
        [-0.5020,  1.1562,  1.7412,  ..., -0.3628, -0.8071,  0.9902],
        [-0.2439,  1.2715,  2.3477,  ..., -0.6055, -1.0654,  0.9233],
        [-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629],
        [-0.4207, -0.5166, -0.3027,  ...,  0.6479, -2.4082,  0.2742],
        [-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629],
        [-0.4683, -0.6260, -0.3152,  ..., -2.3320, -0.1449, -4.2500],
        [-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([6, 187]), X shape:torch.Size([6, 187, 512])
CTC Tokens:tensor([ 8,  8,  0, 70, 24], device='cuda:7'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:7'), New Tokens:tensor([   8,    0,   70,   24, 1631], device='cuda:7')
Mixup Sent Mask:tensor([[2]], device='cuda:7'), 2,  Mixup Mask:tensor([False,  True,  True,  True, False], device='cuda:7'), 
                    Org X:tensor([[ 0.1919, -0.3372, -0.3816,  ...,  0.5537,  1.1084, -0.7627],
        [-1.3760,  0.4355,  0.0877,  ...,  0.0593, -0.3088, -0.7163],
        [-2.7773,  0.2411, -0.1494,  ...,  0.2136, -0.1422, -1.0674],
        [-1.6045,  0.4468,  0.0180,  ..., -0.7783,  0.4417, -1.0693],
        [-0.4124,  0.5918,  0.2180,  ..., -1.3291,  0.3682, -1.0615]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.1919, -0.3372, -0.3816,  ...,  0.5537,  1.1084, -0.7627],
        [-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629],
        [-0.1039, -0.6831, -0.2729,  ...,  0.3398, -2.2090, -3.0605],
        [-0.6021, -0.5596,  0.8032,  ..., -4.7773,  2.0820, -1.6826],
        [-0.4124,  0.5918,  0.2180,  ..., -1.3291,  0.3682, -1.0615]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5845, -0.2427, -0.7832,  ...,  1.1064, -1.7412, -1.7002],
        [-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629],
        [-0.1039, -0.6831, -0.2729,  ...,  0.3398, -2.2090, -3.0605],
        [-0.6021, -0.5596,  0.8032,  ..., -4.7773,  2.0820, -1.6826],
        [ 1.9248,  0.3767,  0.3557,  ..., -0.2325,  4.0117, -1.5361]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 68]), X shape:torch.Size([16, 68, 512])
CTC Tokens:tensor([  55,   56, 9474,    0,    0], device='cuda:5'), Shrink Mask:tensor([ True,  True,  True,  True, False], device='cuda:5'), New Tokens:tensor([  55,   56, 9474,    0,   24], device='cuda:5')
Mixup Sent Mask:tensor([[2],
        [7]], device='cuda:5'), 2,  Mixup Mask:tensor([ True,  True,  True, False, False], device='cuda:5'), 
                    Org X:tensor([[ 0.1142, -0.0568, -0.7734,  ...,  0.3684,  1.0713, -0.0455],
        [-1.3477,  0.3831, -0.1318,  ...,  0.0776,  1.0410, -0.3440],
        [-0.9277,  0.2637,  0.2676,  ...,  0.1317,  0.6655, -0.0814],
        [ 0.1494,  0.2349,  1.1064,  ..., -0.0680,  1.0791, -0.2620],
        [-0.2595, -0.2419, -0.6353,  ..., -1.9170,  0.2969, -0.9907]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-9.2268e-04,  7.4341e-02, -1.4539e-01,  ..., -4.9487e-01,
         -3.4961e+00, -2.5488e+00],
        [-2.1953e+00, -4.2578e-01, -4.7998e-01,  ..., -2.5234e+00,
         -7.3389e-01,  2.8789e+00],
        [-7.6172e-01,  1.7002e+00, -1.2744e+00,  ...,  2.1816e+00,
         -2.3462e-01, -5.5029e-01],
        [ 1.4941e-01,  2.3486e-01,  1.1064e+00,  ..., -6.7993e-02,
          1.0791e+00, -2.6196e-01],
        [-2.5952e-01, -2.4194e-01, -6.3525e-01,  ..., -1.9170e+00,
          2.9688e-01, -9.9072e-01]], device='cuda:5', dtype=torch.float16,
       grad_fn=<SliceBackward0>), Emb: tensor([[-9.2268e-04,  7.4341e-02, -1.4539e-01,  ..., -4.9487e-01,
         -3.4961e+00, -2.5488e+00],
        [-2.1953e+00, -4.2578e-01, -4.7998e-01,  ..., -2.5234e+00,
         -7.3389e-01,  2.8789e+00],
        [-7.6172e-01,  1.7002e+00, -1.2744e+00,  ...,  2.1816e+00,
         -2.3462e-01, -5.5029e-01],
        [-4.4189e-01, -1.0049e+00,  1.5320e-01,  ..., -2.6797e+00,
         -1.8916e+00, -1.9629e+00],
        [-6.0205e-01, -5.5957e-01,  8.0322e-01,  ..., -4.7773e+00,
          2.0820e+00, -1.6826e+00]], device='cuda:5', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([24, 45]), X shape:torch.Size([24, 45, 512])
CTC Tokens:tensor([ 0, 70, 24, 24, 24], device='cuda:6'), Shrink Mask:tensor([ True,  True,  True, False, False], device='cuda:6'), New Tokens:tensor([  0,  70,  24,   0, 150], device='cuda:6')
Mixup Sent Mask:tensor([[ 2],
        [ 7],
        [16],
        [21]], device='cuda:6'), 2,  Mixup Mask:tensor([False, False, False, False, False], device='cuda:6'), 
                    Org X:tensor([[-1.0742, -0.8794,  0.6089,  ..., -0.4473, -0.2573, -0.5093],
        [-1.9658, -0.0245,  0.7124,  ...,  0.0956, -0.1829, -1.0332],
        [-1.7822, -0.0362,  1.5039,  ..., -1.0850,  0.5752, -1.2725],
        [ 1.1025,  1.2578,  2.3945,  ...,  0.0413,  0.7822, -0.5786],
        [ 0.0880,  0.1709,  1.1143,  ...,  0.3118,  0.2126, -1.1104]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-1.0742, -0.8794,  0.6089,  ..., -0.4473, -0.2573, -0.5093],
        [-1.9658, -0.0245,  0.7124,  ...,  0.0956, -0.1829, -1.0332],
        [-1.7822, -0.0362,  1.5039,  ..., -1.0850,  0.5752, -1.2725],
        [ 1.1025,  1.2578,  2.3945,  ...,  0.0413,  0.7822, -0.5786],
        [ 0.0880,  0.1709,  1.1143,  ...,  0.3118,  0.2126, -1.1104]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629],
        [-0.1039, -0.6831, -0.2729,  ...,  0.3398, -2.2090, -3.0605],
        [-0.6021, -0.5596,  0.8032,  ..., -4.7773,  2.0820, -1.6826],
        [-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629],
        [-0.4253, -0.4419, -0.6333,  ...,  0.0442, -1.1904, -2.4668]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 72]), X shape:torch.Size([16, 72, 512])
CTC Tokens:tensor([  0,   7,   0, 211,   0], device='cuda:3'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:3'), New Tokens:tensor([  0,   7,   0, 211,   0], device='cuda:3')
Mixup Sent Mask:tensor([[2],
        [7]], device='cuda:3'), 2,  Mixup Mask:tensor([ True, False, False,  True,  True], device='cuda:3'), 
                    Org X:tensor([[ 5.1367e-01, -2.0728e-01, -3.7793e-01,  ..., -7.6562e-01,
         -1.3092e-02, -1.4902e+00],
        [ 6.3477e-02,  5.4053e-01, -5.9033e-01,  ...,  1.2817e-03,
          3.4668e-02, -5.6543e-01],
        [ 1.7520e+00,  5.6396e-01,  3.0059e+00,  ..., -2.0251e-01,
          8.6121e-02, -7.7286e-03],
        [ 6.9678e-01, -1.5259e-01,  1.8154e+00,  ...,  8.5022e-02,
          8.3008e-01, -9.5117e-01],
        [ 8.1689e-01, -8.8867e-01,  2.5645e+00,  ..., -9.1406e-01,
          9.5337e-02, -1.1172e+00]], device='cuda:3', dtype=torch.float16,
       grad_fn=<SliceBackward0>), New X:tensor([[-4.4189e-01, -1.0049e+00,  1.5320e-01,  ..., -2.6797e+00,
         -1.8916e+00, -1.9629e+00],
        [ 6.3477e-02,  5.4053e-01, -5.9033e-01,  ...,  1.2817e-03,
          3.4668e-02, -5.6543e-01],
        [ 1.7520e+00,  5.6396e-01,  3.0059e+00,  ..., -2.0251e-01,
          8.6121e-02, -7.7286e-03],
        [-4.6478e-02, -1.5283e-01,  2.6538e-01,  ...,  8.9648e-01,
         -1.1885e+00, -2.3984e+00],
        [-4.4189e-01, -1.0049e+00,  1.5320e-01,  ..., -2.6797e+00,
         -1.8916e+00, -1.9629e+00]], device='cuda:3', dtype=torch.float16,
       grad_fn=<SliceBackward0>), Emb: tensor([[-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629],
        [-0.4365, -0.4617, -0.2952,  ..., -0.0485, -1.7676, -1.4775],
        [-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629],
        [-0.0465, -0.1528,  0.2654,  ...,  0.8965, -1.1885, -2.3984],
        [-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([24, 53]), X shape:torch.Size([24, 53, 512])
CTC Tokens:tensor([ 7,  7, 91,  9,  9], device='cuda:2'), Shrink Mask:tensor([ True, False,  True,  True, False], device='cuda:2'), New Tokens:tensor([   7,   91,    9,    7, 1403], device='cuda:2')
Mixup Sent Mask:tensor([[ 2],
        [ 7],
        [16],
        [21]], device='cuda:2'), 2,  Mixup Mask:tensor([False, False,  True,  True,  True], device='cuda:2'), 
                    Org X:tensor([[-0.0620,  0.2488, -1.4004,  ...,  0.5781, -0.0060, -2.0625],
        [-0.4792, -0.6206,  0.4600,  ...,  0.1173,  0.2422, -0.6909],
        [ 1.1045, -0.5083, -1.1445,  ...,  0.0205, -0.2026, -1.4365],
        [ 0.2993,  0.2043, -2.3203,  ...,  0.2062,  1.0283, -0.7168],
        [ 0.8618, -0.1367, -1.1279,  ...,  0.1123,  2.1484,  0.1958]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.0620,  0.2488, -1.4004,  ...,  0.5781, -0.0060, -2.0625],
        [-0.4792, -0.6206,  0.4600,  ...,  0.1173,  0.2422, -0.6909],
        [-0.4524, -0.4910, -0.4312,  ...,  0.2334, -1.3193,  1.5938],
        [-0.4365, -0.4617, -0.2952,  ..., -0.0485, -1.7676, -1.4775],
        [ 0.3608,  0.0734, -0.6841,  ...,  1.2861,  2.0566, -0.8701]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4365, -0.4617, -0.2952,  ..., -0.0485, -1.7676, -1.4775],
        [-0.4751, -0.1846, -0.0640,  ..., -0.6060,  0.5723, -1.8467],
        [-0.4524, -0.4910, -0.4312,  ...,  0.2334, -1.3193,  1.5938],
        [-0.4365, -0.4617, -0.2952,  ..., -0.0485, -1.7676, -1.4775],
        [ 0.3608,  0.0734, -0.6841,  ...,  1.2861,  2.0566, -0.8701]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([72, 23]), X shape:torch.Size([72, 23, 512])
CTC Tokens:tensor([ 21,   0, 203,   0, 587], device='cuda:1'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:1'), New Tokens:tensor([ 21,   0, 203,   0, 587], device='cuda:1')
Mixup Sent Mask:tensor([[ 2],
        [ 7],
        [16],
        [21],
        [24],
        [25],
        [26],
        [30],
        [34],
        [36],
        [39],
        [45],
        [47],
        [52],
        [56],
        [57],
        [59],
        [64],
        [67],
        [69],
        [70],
        [71]], device='cuda:1'), 2,  Mixup Mask:tensor([False,  True,  True,  True,  True], device='cuda:1'), 
                    Org X:tensor([[-0.3931, -1.2500, -3.0645,  ..., -0.4734,  0.6421, -1.8789],
        [-1.0449, -0.5303, -0.7578,  ..., -1.1494,  1.3213, -0.9600],
        [-0.8213, -0.0673,  0.2324,  ...,  0.0550,  1.2012, -1.7266],
        [ 0.2449,  0.1143,  2.0547,  ...,  0.2979, -0.9688, -1.6855],
        [-0.3691,  0.6064,  1.5293,  ...,  0.3345, -1.5742, -0.7432]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3931, -1.2500, -3.0645,  ..., -0.4734,  0.6421, -1.8789],
        [-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629],
        [ 0.0764,  1.0459, -1.2617,  ..., -3.6562,  0.9419, -3.1387],
        [-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629],
        [-0.4629, -0.2496,  1.3682,  ...,  4.6953, -3.2598, -1.4873]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.6621, -0.3752, -0.5947,  ..., -1.0225, -3.6602, -1.9248],
        [-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629],
        [ 0.0764,  1.0459, -1.2617,  ..., -3.6562,  0.9419, -3.1387],
        [-0.4419, -1.0049,  0.1532,  ..., -2.6797, -1.8916, -1.9629],
        [-0.4629, -0.2496,  1.3682,  ...,  4.6953, -3.2598, -1.4873]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:1')
2023-08-08 04:12:31 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.232 | trans_loss 5.608 | nll_loss 2.895 | w2v_ctc_loss 1.354 | task_loss 4.627 | contrastive_loss 0.263 | total 4003.4 | n_correct 2453.9 | ppl 7.44 | accuracy 61.295 | uer 17.957 | wer 19.854 | raw_wer 19.854 | bleu 19.41 | wps 2086 | wpb 4003.4 | bsz 141.8 | num_updates 16000 | best_bleu 19.62
2023-08-08 04:12:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16000 updates
2023-08-08 04:12:31 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_11_16000.pt
2023-08-08 04:12:34 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_11_16000.pt
2023-08-08 04:13:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_11_16000.pt (epoch 11 @ 16000 updates, score 19.41) (writing took 30.90412136539817 seconds)
2023-08-08 04:14:12 | INFO | train_inner | epoch 011:   1368 / 1474 loss=2.108, trans_loss=5.176, nll_loss=2.419, w2v_ctc_loss=0.724, task_loss=1.292, contrastive_loss=0.315, total=4190.34, n_correct=2569.33, ppl=5.35, accuracy=61.316, wps=6669.2, ups=0.8, wpb=8380.7, bsz=327.9, num_updates=16100, lr=0.000111456, gnorm=0.507, clip=0, loss_scale=32, train_wall=69, gb_free=16.9, wall=14947
2023-08-08 04:15:21 | INFO | train_inner | epoch 011:   1468 / 1474 loss=2.095, trans_loss=5.178, nll_loss=2.421, w2v_ctc_loss=0.728, task_loss=1.353, contrastive_loss=0.088, total=4158.39, n_correct=2549.5, ppl=5.35, accuracy=61.31, wps=12074.9, ups=1.45, wpb=8316.8, bsz=312, num_updates=16200, lr=0.000111111, gnorm=0.504, clip=0, loss_scale=32, train_wall=68, gb_free=16.7, wall=15016
2023-08-08 04:15:25 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 04:15:48 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.203 | trans_loss 5.592 | nll_loss 2.879 | w2v_ctc_loss 1.295 | task_loss 4.599 | contrastive_loss 0.252 | total 4003.4 | n_correct 2457.6 | ppl 7.36 | accuracy 61.388 | uer 17.875 | wer 19.649 | raw_wer 19.649 | bleu 19.84 | wps 2274.9 | wpb 4003.4 | bsz 141.8 | num_updates 16206 | best_bleu 19.84
2023-08-08 04:15:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16206 updates
2023-08-08 04:15:48 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 04:16:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 04:16:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt (epoch 11 @ 16206 updates, score 19.84) (writing took 24.127954421564937 seconds)
2023-08-08 04:16:12 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-08-08 04:16:12 | INFO | train | epoch 011 | loss 2.057 | trans_loss 4.744 | nll_loss 2.21 | w2v_ctc_loss 0.789 | task_loss 1.282 | contrastive_loss 0.131 | total 4138.65 | n_correct 2540.83 | ppl 4.63 | accuracy 61.393 | wps 11125.2 | ups 1.23 | wpb 9021 | bsz 333.4 | num_updates 16206 | lr 0.000111091 | gnorm 0.488 | clip 0 | loss_scale 32 | train_wall 1077 | gb_free 17.1 | wall 15067
2023-08-08 04:16:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 04:16:13 | INFO | fairseq.trainer | begin training epoch 12
2023-08-08 04:16:13 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 04:17:25 | INFO | train_inner | epoch 012:     94 / 1474 loss=2.07, trans_loss=5.121, nll_loss=2.345, w2v_ctc_loss=0.714, task_loss=1.338, contrastive_loss=0.121, total=4146.82, n_correct=2585.19, ppl=5.08, accuracy=62.342, wps=6663.9, ups=0.8, wpb=8293.6, bsz=313.9, num_updates=16300, lr=0.00011077, gnorm=0.509, clip=0, loss_scale=32, train_wall=69, gb_free=15.6, wall=15140
2023-08-08 04:18:34 | INFO | train_inner | epoch 012:    194 / 1474 loss=2.077, trans_loss=5.133, nll_loss=2.359, w2v_ctc_loss=0.722, task_loss=1.445, contrastive_loss=0.071, total=4120.68, n_correct=2557.64, ppl=5.13, accuracy=62.068, wps=11970.4, ups=1.45, wpb=8241.4, bsz=294.7, num_updates=16400, lr=0.000110432, gnorm=0.51, clip=0, loss_scale=32, train_wall=68, gb_free=15.5, wall=15209
2023-08-08 04:19:44 | INFO | train_inner | epoch 012:    294 / 1474 loss=2.071, trans_loss=5.132, nll_loss=2.359, w2v_ctc_loss=0.708, task_loss=1.315, contrastive_loss=0.102, total=4199.46, n_correct=2611.96, ppl=5.13, accuracy=62.198, wps=11995.5, ups=1.43, wpb=8398.9, bsz=320.3, num_updates=16500, lr=0.000110096, gnorm=0.506, clip=0, loss_scale=32, train_wall=70, gb_free=16.4, wall=15279
2023-08-08 04:20:54 | INFO | train_inner | epoch 012:    394 / 1474 loss=2.076, trans_loss=5.139, nll_loss=2.368, w2v_ctc_loss=0.718, task_loss=1.37, contrastive_loss=0.088, total=4151.14, n_correct=2576.46, ppl=5.16, accuracy=62.066, wps=11955.9, ups=1.44, wpb=8302.3, bsz=307.8, num_updates=16600, lr=0.000109764, gnorm=0.512, clip=0, loss_scale=32, train_wall=69, gb_free=17, wall=15349
2023-08-08 04:22:03 | INFO | train_inner | epoch 012:    494 / 1474 loss=2.087, trans_loss=5.155, nll_loss=2.389, w2v_ctc_loss=0.727, task_loss=1.404, contrastive_loss=0.094, total=4110.49, n_correct=2541.51, ppl=5.24, accuracy=61.83, wps=11823.4, ups=1.44, wpb=8221, bsz=302.2, num_updates=16700, lr=0.000109435, gnorm=0.513, clip=0, loss_scale=64, train_wall=69, gb_free=13.6, wall=15418
2023-08-08 04:23:13 | INFO | train_inner | epoch 012:    594 / 1474 loss=2.082, trans_loss=5.143, nll_loss=2.375, w2v_ctc_loss=0.719, task_loss=1.341, contrastive_loss=0.157, total=4189.92, n_correct=2595.65, ppl=5.19, accuracy=61.95, wps=12045.8, ups=1.44, wpb=8379.8, bsz=315.1, num_updates=16800, lr=0.000109109, gnorm=0.503, clip=0, loss_scale=64, train_wall=69, gb_free=14.7, wall=15488
2023-08-08 04:24:22 | INFO | train_inner | epoch 012:    694 / 1474 loss=2.078, trans_loss=5.14, nll_loss=2.373, w2v_ctc_loss=0.702, task_loss=1.28, contrastive_loss=0.246, total=4206.3, n_correct=2614.56, ppl=5.18, accuracy=62.158, wps=12154.7, ups=1.44, wpb=8412.6, bsz=325.7, num_updates=16900, lr=0.000108786, gnorm=0.503, clip=0, loss_scale=64, train_wall=69, gb_free=16.1, wall=15557
2023-08-08 04:25:31 | INFO | train_inner | epoch 012:    794 / 1474 loss=2.08, trans_loss=5.142, nll_loss=2.373, w2v_ctc_loss=0.723, task_loss=1.428, contrastive_loss=0.082, total=4085.96, n_correct=2531.61, ppl=5.18, accuracy=61.959, wps=11813.8, ups=1.45, wpb=8171.9, bsz=297.1, num_updates=17000, lr=0.000108465, gnorm=0.518, clip=0, loss_scale=64, train_wall=69, gb_free=16, wall=15626
2023-08-08 04:26:42 | INFO | train_inner | epoch 012:    894 / 1474 loss=2.086, trans_loss=5.151, nll_loss=2.385, w2v_ctc_loss=0.719, task_loss=1.428, contrastive_loss=0.136, total=4169.74, n_correct=2577.61, ppl=5.22, accuracy=61.817, wps=11876.3, ups=1.42, wpb=8339.5, bsz=306.4, num_updates=17100, lr=0.000108148, gnorm=0.511, clip=0, loss_scale=64, train_wall=70, gb_free=15.9, wall=15696
2023-08-08 04:27:51 | INFO | train_inner | epoch 012:    994 / 1474 loss=2.09, trans_loss=5.159, nll_loss=2.396, w2v_ctc_loss=0.724, task_loss=1.428, contrastive_loss=0.142, total=4117.67, n_correct=2540.46, ppl=5.26, accuracy=61.697, wps=11878.6, ups=1.44, wpb=8235.3, bsz=301.4, num_updates=17200, lr=0.000107833, gnorm=0.514, clip=0, loss_scale=64, train_wall=69, gb_free=17.6, wall=15766
2023-08-08 04:29:00 | INFO | train_inner | epoch 012:   1094 / 1474 loss=2.1, trans_loss=5.164, nll_loss=2.403, w2v_ctc_loss=0.73, task_loss=1.467, contrastive_loss=0.187, total=4047.61, n_correct=2491.82, ppl=5.29, accuracy=61.563, wps=11671.2, ups=1.44, wpb=8095.2, bsz=290.4, num_updates=17300, lr=0.000107521, gnorm=0.526, clip=0, loss_scale=64, train_wall=69, gb_free=16.4, wall=15835
2023-08-08 04:30:10 | INFO | train_inner | epoch 012:   1194 / 1474 loss=2.097, trans_loss=5.17, nll_loss=2.411, w2v_ctc_loss=0.734, task_loss=1.387, contrastive_loss=0.143, total=4184.55, n_correct=2569.73, ppl=5.32, accuracy=61.41, wps=12078.2, ups=1.44, wpb=8369.1, bsz=314.3, num_updates=17400, lr=0.000107211, gnorm=0.51, clip=0, loss_scale=64, train_wall=69, gb_free=16.5, wall=15904
2023-08-08 04:31:19 | INFO | train_inner | epoch 012:   1294 / 1474 loss=2.096, trans_loss=5.164, nll_loss=2.403, w2v_ctc_loss=0.742, task_loss=1.522, contrastive_loss=0.088, total=4086.33, n_correct=2513.67, ppl=5.29, accuracy=61.514, wps=11787.1, ups=1.44, wpb=8172.7, bsz=291.4, num_updates=17500, lr=0.000106904, gnorm=0.517, clip=0, loss_scale=64, train_wall=69, gb_free=16.5, wall=15974
2023-08-08 04:32:28 | INFO | train_inner | epoch 012:   1394 / 1474 loss=2.09, trans_loss=5.162, nll_loss=2.402, w2v_ctc_loss=0.717, task_loss=1.418, contrastive_loss=0.174, total=4134.89, n_correct=2546.79, ppl=5.28, accuracy=61.593, wps=11934.3, ups=1.44, wpb=8269.8, bsz=304.4, num_updates=17600, lr=0.0001066, gnorm=0.516, clip=0, loss_scale=64, train_wall=69, gb_free=17.1, wall=16043
2023-08-08 04:33:24 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 04:33:46 | INFO | dev_st | epoch 012 | valid on 'dev_st' subset | loss 4.217 | trans_loss 5.593 | nll_loss 2.879 | w2v_ctc_loss 1.339 | task_loss 4.629 | contrastive_loss 0.255 | total 4003.4 | n_correct 2458 | ppl 7.35 | accuracy 61.398 | uer 18.05 | wer 19.776 | raw_wer 19.776 | bleu 19.81 | wps 2269.8 | wpb 4003.4 | bsz 141.8 | num_updates 17680 | best_bleu 19.84
2023-08-08 04:33:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 17680 updates
2023-08-08 04:33:46 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.8100.pt
2023-08-08 04:33:50 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.8100.pt
2023-08-08 04:34:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.8100.pt (epoch 12 @ 17680 updates, score 19.81) (writing took 18.143228201195598 seconds)
2023-08-08 04:34:05 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-08-08 04:34:05 | INFO | train | epoch 012 | loss 2.084 | trans_loss 5.149 | nll_loss 2.382 | w2v_ctc_loss 0.721 | task_loss 1.399 | contrastive_loss 0.129 | total 4138.65 | n_correct 2560.36 | ppl 5.21 | accuracy 61.865 | wps 11375.3 | ups 1.37 | wpb 8277.3 | bsz 305.7 | num_updates 17680 | lr 0.000106359 | gnorm 0.512 | clip 0 | loss_scale 64 | train_wall 1017 | gb_free 12.6 | wall 16140
2023-08-08 04:34:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 04:34:05 | INFO | fairseq.trainer | begin training epoch 13
2023-08-08 04:34:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 04:34:27 | INFO | train_inner | epoch 013:     20 / 1474 loss=2.088, trans_loss=5.161, nll_loss=2.399, w2v_ctc_loss=0.73, task_loss=1.448, contrastive_loss=0.079, total=4104.86, n_correct=2534.68, ppl=5.27, accuracy=61.748, wps=6930.9, ups=0.84, wpb=8209.7, bsz=296.8, num_updates=17700, lr=0.000106299, gnorm=0.516, clip=0, loss_scale=64, train_wall=69, gb_free=14.5, wall=16161
2023-08-08 04:35:36 | INFO | train_inner | epoch 013:    120 / 1474 loss=2.062, trans_loss=5.114, nll_loss=2.336, w2v_ctc_loss=0.705, task_loss=1.4, contrastive_loss=0.09, total=4161.2, n_correct=2602.93, ppl=5.05, accuracy=62.552, wps=11967.8, ups=1.44, wpb=8322.4, bsz=302.9, num_updates=17800, lr=0.000106, gnorm=0.508, clip=0, loss_scale=64, train_wall=69, gb_free=16, wall=16231
2023-08-08 04:36:45 | INFO | train_inner | epoch 013:    220 / 1474 loss=2.076, trans_loss=5.121, nll_loss=2.348, w2v_ctc_loss=0.703, task_loss=1.295, contrastive_loss=0.307, total=4202.62, n_correct=2623.5, ppl=5.09, accuracy=62.425, wps=12132.3, ups=1.44, wpb=8405.2, bsz=328.3, num_updates=17900, lr=0.000105703, gnorm=0.511, clip=0, loss_scale=64, train_wall=69, gb_free=17.1, wall=16300
2023-08-08 04:37:55 | INFO | train_inner | epoch 013:    320 / 1474 loss=2.059, trans_loss=5.112, nll_loss=2.334, w2v_ctc_loss=0.7, task_loss=1.439, contrastive_loss=0.075, total=4112.8, n_correct=2578.02, ppl=5.04, accuracy=62.683, wps=11870.1, ups=1.44, wpb=8225.6, bsz=296, num_updates=18000, lr=0.000105409, gnorm=0.513, clip=0, loss_scale=64, train_wall=69, gb_free=17.7, wall=16370
2023-08-08 04:37:55 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 04:38:18 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.225 | trans_loss 5.598 | nll_loss 2.882 | w2v_ctc_loss 1.354 | task_loss 4.59 | contrastive_loss 0.255 | total 4003.4 | n_correct 2456.3 | ppl 7.37 | accuracy 61.355 | uer 18.146 | wer 19.872 | raw_wer 19.872 | bleu 19.51 | wps 2227.1 | wpb 4003.4 | bsz 141.8 | num_updates 18000 | best_bleu 19.84
2023-08-08 04:38:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 18000 updates
2023-08-08 04:38:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_13_18000.pt
2023-08-08 04:38:22 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_13_18000.pt
2023-08-08 04:38:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_13_18000.pt (epoch 13 @ 18000 updates, score 19.51) (writing took 39.50479462184012 seconds)
2023-08-08 04:40:07 | INFO | train_inner | epoch 013:    420 / 1474 loss=2.066, trans_loss=5.121, nll_loss=2.347, w2v_ctc_loss=0.712, task_loss=1.317, contrastive_loss=0.124, total=4176.06, n_correct=2612.59, ppl=5.09, accuracy=62.561, wps=6303.9, ups=0.75, wpb=8352.1, bsz=317.5, num_updates=18100, lr=0.000105118, gnorm=0.508, clip=0, loss_scale=64, train_wall=68, gb_free=16.4, wall=16502
2023-08-08 04:41:17 | INFO | train_inner | epoch 013:    520 / 1474 loss=2.072, trans_loss=5.129, nll_loss=2.358, w2v_ctc_loss=0.709, task_loss=1.363, contrastive_loss=0.159, total=4197.57, n_correct=2612.21, ppl=5.12, accuracy=62.231, wps=11994.1, ups=1.43, wpb=8395.1, bsz=318.1, num_updates=18200, lr=0.000104828, gnorm=0.515, clip=0, loss_scale=64, train_wall=70, gb_free=15.1, wall=16572
2023-08-08 04:42:26 | INFO | train_inner | epoch 013:    620 / 1474 loss=2.062, trans_loss=5.123, nll_loss=2.351, w2v_ctc_loss=0.706, task_loss=1.354, contrastive_loss=0.072, total=4160.12, n_correct=2600.96, ppl=5.1, accuracy=62.521, wps=12125, ups=1.46, wpb=8320.2, bsz=308.7, num_updates=18300, lr=0.000104542, gnorm=0.51, clip=0, loss_scale=64, train_wall=68, gb_free=16.2, wall=16641
2023-08-08 04:43:36 | INFO | train_inner | epoch 013:    720 / 1474 loss=2.081, trans_loss=5.138, nll_loss=2.369, w2v_ctc_loss=0.731, task_loss=1.55, contrastive_loss=0.07, total=4101.54, n_correct=2542.42, ppl=5.16, accuracy=61.987, wps=11714.9, ups=1.43, wpb=8203.1, bsz=285.7, num_updates=18400, lr=0.000104257, gnorm=0.513, clip=0, loss_scale=64, train_wall=70, gb_free=15.5, wall=16711
2023-08-08 04:44:45 | INFO | train_inner | epoch 013:    820 / 1474 loss=2.073, trans_loss=5.132, nll_loss=2.362, w2v_ctc_loss=0.713, task_loss=1.413, contrastive_loss=0.121, total=4126.37, n_correct=2565.99, ppl=5.14, accuracy=62.185, wps=11876.4, ups=1.44, wpb=8252.7, bsz=307, num_updates=18500, lr=0.000103975, gnorm=0.511, clip=0, loss_scale=64, train_wall=69, gb_free=17.6, wall=16780
2023-08-08 04:45:55 | INFO | train_inner | epoch 013:    920 / 1474 loss=2.073, trans_loss=5.139, nll_loss=2.37, w2v_ctc_loss=0.714, task_loss=1.434, contrastive_loss=0.081, total=4102.78, n_correct=2551.73, ppl=5.17, accuracy=62.195, wps=11828.8, ups=1.44, wpb=8205.6, bsz=295.9, num_updates=18600, lr=0.000103695, gnorm=0.512, clip=0, loss_scale=64, train_wall=69, gb_free=16.4, wall=16850
2023-08-08 04:47:04 | INFO | train_inner | epoch 013:   1020 / 1474 loss=2.082, trans_loss=5.141, nll_loss=2.374, w2v_ctc_loss=0.721, task_loss=1.492, contrastive_loss=0.131, total=4071.32, n_correct=2523.1, ppl=5.18, accuracy=61.973, wps=11814.1, ups=1.45, wpb=8142.6, bsz=291.3, num_updates=18700, lr=0.000103418, gnorm=0.52, clip=0, loss_scale=128, train_wall=68, gb_free=11.1, wall=16919
2023-08-08 04:48:13 | INFO | train_inner | epoch 013:   1120 / 1474 loss=2.069, trans_loss=5.129, nll_loss=2.359, w2v_ctc_loss=0.711, task_loss=1.372, contrastive_loss=0.115, total=4115.28, n_correct=2567.36, ppl=5.13, accuracy=62.386, wps=11866.7, ups=1.44, wpb=8230.6, bsz=307.5, num_updates=18800, lr=0.000103142, gnorm=0.51, clip=0, loss_scale=128, train_wall=69, gb_free=12, wall=16988
2023-08-08 04:49:22 | INFO | train_inner | epoch 013:   1220 / 1474 loss=2.076, trans_loss=5.143, nll_loss=2.377, w2v_ctc_loss=0.72, task_loss=1.486, contrastive_loss=0.073, total=4105.36, n_correct=2549.95, ppl=5.2, accuracy=62.113, wps=11938.4, ups=1.45, wpb=8210.7, bsz=295.3, num_updates=18900, lr=0.000102869, gnorm=0.514, clip=0, loss_scale=128, train_wall=68, gb_free=16.2, wall=17057
2023-08-08 04:50:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-08 04:50:32 | INFO | train_inner | epoch 013:   1321 / 1474 loss=2.069, trans_loss=5.132, nll_loss=2.363, w2v_ctc_loss=0.715, task_loss=1.42, contrastive_loss=0.075, total=4096.9, n_correct=2554.97, ppl=5.15, accuracy=62.363, wps=11645.1, ups=1.42, wpb=8193.8, bsz=300.4, num_updates=19000, lr=0.000102598, gnorm=0.513, clip=0, loss_scale=64, train_wall=70, gb_free=17.5, wall=17127
2023-08-08 04:51:41 | INFO | train_inner | epoch 013:   1421 / 1474 loss=2.079, trans_loss=5.145, nll_loss=2.38, w2v_ctc_loss=0.708, task_loss=1.372, contrastive_loss=0.183, total=4180.88, n_correct=2594.53, ppl=5.21, accuracy=62.057, wps=12090.1, ups=1.45, wpb=8361.8, bsz=312.2, num_updates=19100, lr=0.000102329, gnorm=0.509, clip=0, loss_scale=64, train_wall=69, gb_free=15.1, wall=17196
2023-08-08 04:52:18 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 04:52:42 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.217 | trans_loss 5.585 | nll_loss 2.867 | w2v_ctc_loss 1.355 | task_loss 4.606 | contrastive_loss 0.264 | total 4003.4 | n_correct 2461.7 | ppl 7.3 | accuracy 61.49 | uer 17.997 | wer 19.783 | raw_wer 19.783 | bleu 19.88 | wps 2154.5 | wpb 4003.4 | bsz 141.8 | num_updates 19153 | best_bleu 19.88
2023-08-08 04:52:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 19153 updates
2023-08-08 04:52:42 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 04:52:55 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 04:53:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt (epoch 13 @ 19153 updates, score 19.88) (writing took 24.41980343312025 seconds)
2023-08-08 04:53:07 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-08-08 04:53:07 | INFO | train | epoch 013 | loss 2.071 | trans_loss 5.13 | nll_loss 2.359 | w2v_ctc_loss 0.712 | task_loss 1.401 | contrastive_loss 0.12 | total 4137.55 | n_correct 2578.04 | ppl 5.13 | accuracy 62.309 | wps 10675 | ups 1.29 | wpb 8275.1 | bsz 305.2 | num_updates 19153 | lr 0.000102187 | gnorm 0.512 | clip 0 | loss_scale 64 | train_wall 1015 | gb_free 17.6 | wall 17282
2023-08-08 04:53:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 04:53:07 | INFO | fairseq.trainer | begin training epoch 14
2023-08-08 04:53:07 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 04:53:47 | INFO | train_inner | epoch 014:     47 / 1474 loss=2.048, trans_loss=5.099, nll_loss=2.321, w2v_ctc_loss=0.699, task_loss=1.28, contrastive_loss=0.088, total=4176.2, n_correct=2631.2, ppl=5, accuracy=63.005, wps=6637.2, ups=0.79, wpb=8352.4, bsz=322.3, num_updates=19200, lr=0.000102062, gnorm=0.506, clip=0, loss_scale=64, train_wall=69, gb_free=10.4, wall=17322
2023-08-08 04:54:56 | INFO | train_inner | epoch 014:    147 / 1474 loss=2.047, trans_loss=5.087, nll_loss=2.303, w2v_ctc_loss=0.703, task_loss=1.413, contrastive_loss=0.07, total=4080.86, n_correct=2579.99, ppl=4.93, accuracy=63.222, wps=11848.5, ups=1.45, wpb=8161.7, bsz=299.6, num_updates=19300, lr=0.000101797, gnorm=0.514, clip=0, loss_scale=64, train_wall=68, gb_free=16.9, wall=17391
2023-08-08 04:56:05 | INFO | train_inner | epoch 014:    247 / 1474 loss=2.064, trans_loss=5.109, nll_loss=2.332, w2v_ctc_loss=0.704, task_loss=1.472, contrastive_loss=0.171, total=4106.97, n_correct=2579.48, ppl=5.03, accuracy=62.807, wps=11870.6, ups=1.45, wpb=8213.9, bsz=293.3, num_updates=19400, lr=0.000101535, gnorm=0.517, clip=0, loss_scale=64, train_wall=69, gb_free=12.2, wall=17460
2023-08-08 04:57:14 | INFO | train_inner | epoch 014:    347 / 1474 loss=2.048, trans_loss=5.097, nll_loss=2.318, w2v_ctc_loss=0.697, task_loss=1.286, contrastive_loss=0.105, total=4179.8, n_correct=2639.55, ppl=4.99, accuracy=63.15, wps=12098.3, ups=1.45, wpb=8359.6, bsz=322.5, num_updates=19500, lr=0.000101274, gnorm=0.512, clip=0, loss_scale=64, train_wall=69, gb_free=17.2, wall=17529
2023-08-08 04:58:24 | INFO | train_inner | epoch 014:    447 / 1474 loss=2.053, trans_loss=5.111, nll_loss=2.335, w2v_ctc_loss=0.698, task_loss=1.445, contrastive_loss=0.064, total=4120.38, n_correct=2584.87, ppl=5.05, accuracy=62.734, wps=11869.6, ups=1.44, wpb=8240.8, bsz=296.3, num_updates=19600, lr=0.000101015, gnorm=0.513, clip=0, loss_scale=64, train_wall=69, gb_free=17.1, wall=17599
2023-08-08 04:59:34 | INFO | train_inner | epoch 014:    547 / 1474 loss=2.068, trans_loss=5.116, nll_loss=2.34, w2v_ctc_loss=0.721, task_loss=1.479, contrastive_loss=0.101, total=4089.86, n_correct=2557.16, ppl=5.06, accuracy=62.524, wps=11608.9, ups=1.42, wpb=8179.7, bsz=295.5, num_updates=19700, lr=0.000100759, gnorm=0.522, clip=0, loss_scale=64, train_wall=70, gb_free=11.8, wall=17669
2023-08-08 05:00:44 | INFO | train_inner | epoch 014:    647 / 1474 loss=2.061, trans_loss=5.112, nll_loss=2.336, w2v_ctc_loss=0.703, task_loss=1.399, contrastive_loss=0.143, total=4158.94, n_correct=2608.98, ppl=5.05, accuracy=62.732, wps=11934.4, ups=1.43, wpb=8317.9, bsz=306.7, num_updates=19800, lr=0.000100504, gnorm=0.503, clip=0, loss_scale=64, train_wall=69, gb_free=15.9, wall=17739
2023-08-08 05:01:54 | INFO | train_inner | epoch 014:    747 / 1474 loss=2.048, trans_loss=5.102, nll_loss=2.324, w2v_ctc_loss=0.695, task_loss=1.355, contrastive_loss=0.076, total=4150.03, n_correct=2610.68, ppl=5.01, accuracy=62.907, wps=11909.4, ups=1.43, wpb=8300.1, bsz=310.4, num_updates=19900, lr=0.000100251, gnorm=0.504, clip=0, loss_scale=64, train_wall=69, gb_free=15.4, wall=17809
2023-08-08 05:03:03 | INFO | train_inner | epoch 014:    847 / 1474 loss=2.059, trans_loss=5.106, nll_loss=2.33, w2v_ctc_loss=0.699, task_loss=1.337, contrastive_loss=0.185, total=4162.8, n_correct=2613.79, ppl=5.03, accuracy=62.789, wps=12091.6, ups=1.45, wpb=8325.6, bsz=317.2, num_updates=20000, lr=0.0001, gnorm=0.508, clip=0, loss_scale=64, train_wall=68, gb_free=17, wall=17877
2023-08-08 05:03:03 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 05:03:25 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.2 | trans_loss 5.577 | nll_loss 2.857 | w2v_ctc_loss 1.323 | task_loss 4.6 | contrastive_loss 0.25 | total 4003.4 | n_correct 2470.9 | ppl 7.25 | accuracy 61.72 | uer 17.57 | wer 19.358 | raw_wer 19.358 | bleu 19.75 | wps 2292.7 | wpb 4003.4 | bsz 141.8 | num_updates 20000 | best_bleu 19.88
2023-08-08 05:03:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20000 updates
2023-08-08 05:03:25 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_14_20000.pt
2023-08-08 05:03:28 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_14_20000.pt
2023-08-08 05:03:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_14_20000.pt (epoch 14 @ 20000 updates, score 19.75) (writing took 15.429453931748867 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:0')
2023-08-08 05:04:51 | INFO | train_inner | epoch 014:    947 / 1474 loss=2.057, trans_loss=5.114, nll_loss=2.34, w2v_ctc_loss=0.705, task_loss=1.422, contrastive_loss=0.077, total=4159.46, n_correct=2606.15, ppl=5.06, accuracy=62.656, wps=7669.7, ups=0.92, wpb=8318.9, bsz=306.7, num_updates=20100, lr=9.97509e-05, gnorm=0.513, clip=0, loss_scale=64, train_wall=69, gb_free=15.1, wall=17986
2023-08-08 05:06:01 | INFO | train_inner | epoch 014:   1047 / 1474 loss=2.062, trans_loss=5.119, nll_loss=2.346, w2v_ctc_loss=0.698, task_loss=1.394, contrastive_loss=0.141, total=4155.93, n_correct=2600.75, ppl=5.09, accuracy=62.579, wps=11924.4, ups=1.43, wpb=8311.9, bsz=305.9, num_updates=20200, lr=9.95037e-05, gnorm=0.508, clip=0, loss_scale=64, train_wall=69, gb_free=16.2, wall=18056
2023-08-08 05:07:11 | INFO | train_inner | epoch 014:   1147 / 1474 loss=2.08, trans_loss=5.118, nll_loss=2.347, w2v_ctc_loss=0.71, task_loss=1.315, contrastive_loss=0.368, total=4228.09, n_correct=2640.48, ppl=5.09, accuracy=62.451, wps=12020.4, ups=1.42, wpb=8456.2, bsz=326.3, num_updates=20300, lr=9.92583e-05, gnorm=0.508, clip=0, loss_scale=64, train_wall=70, gb_free=17.5, wall=18126
2023-08-08 05:08:20 | INFO | train_inner | epoch 014:   1247 / 1474 loss=2.074, trans_loss=5.137, nll_loss=2.368, w2v_ctc_loss=0.721, task_loss=1.63, contrastive_loss=0.059, total=4027.71, n_correct=2507.81, ppl=5.16, accuracy=62.264, wps=11675.5, ups=1.45, wpb=8055.4, bsz=273.6, num_updates=20400, lr=9.90148e-05, gnorm=0.524, clip=0, loss_scale=64, train_wall=68, gb_free=16.4, wall=18195
2023-08-08 05:09:30 | INFO | train_inner | epoch 014:   1347 / 1474 loss=2.057, trans_loss=5.123, nll_loss=2.353, w2v_ctc_loss=0.7, task_loss=1.34, contrastive_loss=0.075, total=4198.71, n_correct=2627.61, ppl=5.11, accuracy=62.581, wps=12095.7, ups=1.44, wpb=8397.4, bsz=315.4, num_updates=20500, lr=9.8773e-05, gnorm=0.513, clip=0, loss_scale=64, train_wall=69, gb_free=16.2, wall=18264
2023-08-08 05:10:38 | INFO | train_inner | epoch 014:   1447 / 1474 loss=2.065, trans_loss=5.132, nll_loss=2.364, w2v_ctc_loss=0.704, task_loss=1.38, contrastive_loss=0.116, total=4140.5, n_correct=2585.23, ppl=5.15, accuracy=62.438, wps=12025.6, ups=1.45, wpb=8281, bsz=307.1, num_updates=20600, lr=9.85329e-05, gnorm=0.519, clip=0, loss_scale=64, train_wall=68, gb_free=17.5, wall=18333
2023-08-08 05:10:57 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:2')
2023-08-08 05:11:20 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.214 | trans_loss 5.581 | nll_loss 2.865 | w2v_ctc_loss 1.355 | task_loss 4.602 | contrastive_loss 0.265 | total 4003.4 | n_correct 2468.8 | ppl 7.28 | accuracy 61.668 | uer 17.893 | wer 19.626 | raw_wer 19.626 | bleu 19.31 | wps 2258.4 | wpb 4003.4 | bsz 141.8 | num_updates 20627 | best_bleu 19.88
2023-08-08 05:11:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20627 updates
2023-08-08 05:11:20 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.3103.pt
2023-08-08 05:11:23 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.3103.pt
2023-08-08 05:11:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.3103.pt (epoch 14 @ 20627 updates, score 19.31) (writing took 13.584760874509811 seconds)
2023-08-08 05:11:34 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-08-08 05:11:34 | INFO | train | epoch 014 | loss 2.06 | trans_loss 5.113 | nll_loss 2.338 | w2v_ctc_loss 0.704 | task_loss 1.399 | contrastive_loss 0.124 | total 4138.65 | n_correct 2595.54 | ppl 5.05 | accuracy 62.715 | wps 11022.5 | ups 1.33 | wpb 8277.3 | bsz 305.7 | num_updates 20627 | lr 9.84684e-05 | gnorm 0.513 | clip 0 | loss_scale 64 | train_wall 1015 | gb_free 16.3 | wall 18389
2023-08-08 05:11:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 05:11:34 | INFO | fairseq.trainer | begin training epoch 15
2023-08-08 05:11:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 05:12:31 | INFO | train_inner | epoch 015:     73 / 1474 loss=2.054, trans_loss=5.099, nll_loss=2.32, w2v_ctc_loss=0.697, task_loss=1.401, contrastive_loss=0.164, total=4083.93, n_correct=2572.93, ppl=4.99, accuracy=63.001, wps=7229.5, ups=0.89, wpb=8167.9, bsz=300.1, num_updates=20700, lr=9.82946e-05, gnorm=0.52, clip=0, loss_scale=64, train_wall=68, gb_free=15.5, wall=18446
2023-08-08 05:13:41 | INFO | train_inner | epoch 015:    173 / 1474 loss=2.046, trans_loss=5.089, nll_loss=2.306, w2v_ctc_loss=0.702, task_loss=1.451, contrastive_loss=0.074, total=4122.67, n_correct=2603.58, ppl=4.94, accuracy=63.153, wps=11884.6, ups=1.44, wpb=8245.3, bsz=299.1, num_updates=20800, lr=9.80581e-05, gnorm=0.513, clip=0, loss_scale=64, train_wall=69, gb_free=17.1, wall=18516
2023-08-08 05:14:50 | INFO | train_inner | epoch 015:    273 / 1474 loss=2.037, trans_loss=5.088, nll_loss=2.305, w2v_ctc_loss=0.684, task_loss=1.348, contrastive_loss=0.065, total=4190.11, n_correct=2653.21, ppl=4.94, accuracy=63.321, wps=12102.2, ups=1.44, wpb=8380.2, bsz=312.6, num_updates=20900, lr=9.78232e-05, gnorm=0.502, clip=0, loss_scale=64, train_wall=69, gb_free=17, wall=18585
2023-08-08 05:15:59 | INFO | train_inner | epoch 015:    373 / 1474 loss=2.042, trans_loss=5.082, nll_loss=2.297, w2v_ctc_loss=0.691, task_loss=1.441, contrastive_loss=0.088, total=4150.33, n_correct=2626.07, ppl=4.91, accuracy=63.274, wps=11977.8, ups=1.44, wpb=8300.7, bsz=301, num_updates=21000, lr=9.759e-05, gnorm=0.514, clip=0, loss_scale=64, train_wall=69, gb_free=15.4, wall=18654
2023-08-08 05:16:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-08 05:17:09 | INFO | train_inner | epoch 015:    474 / 1474 loss=2.04, trans_loss=5.088, nll_loss=2.305, w2v_ctc_loss=0.686, task_loss=1.479, contrastive_loss=0.061, total=4061.99, n_correct=2565.94, ppl=4.94, accuracy=63.17, wps=11674.7, ups=1.44, wpb=8124, bsz=289.8, num_updates=21100, lr=9.73585e-05, gnorm=0.517, clip=0, loss_scale=64, train_wall=69, gb_free=15.6, wall=18724
2023-08-08 05:18:18 | INFO | train_inner | epoch 015:    574 / 1474 loss=2.044, trans_loss=5.089, nll_loss=2.307, w2v_ctc_loss=0.697, task_loss=1.449, contrastive_loss=0.072, total=4140.59, n_correct=2615.56, ppl=4.95, accuracy=63.169, wps=11926.4, ups=1.44, wpb=8281.2, bsz=298.8, num_updates=21200, lr=9.71286e-05, gnorm=0.508, clip=0, loss_scale=64, train_wall=69, gb_free=11.9, wall=18793
2023-08-08 05:19:27 | INFO | train_inner | epoch 015:    674 / 1474 loss=2.052, trans_loss=5.092, nll_loss=2.311, w2v_ctc_loss=0.698, task_loss=1.414, contrastive_loss=0.156, total=4134.99, n_correct=2612.25, ppl=4.96, accuracy=63.174, wps=11970.5, ups=1.45, wpb=8270, bsz=307.1, num_updates=21300, lr=9.69003e-05, gnorm=0.514, clip=0, loss_scale=64, train_wall=69, gb_free=10.4, wall=18862
2023-08-08 05:20:37 | INFO | train_inner | epoch 015:    774 / 1474 loss=2.049, trans_loss=5.1, nll_loss=2.321, w2v_ctc_loss=0.701, task_loss=1.417, contrastive_loss=0.075, total=4173.66, n_correct=2629.18, ppl=5, accuracy=62.995, wps=11953.1, ups=1.43, wpb=8347.3, bsz=305, num_updates=21400, lr=9.66736e-05, gnorm=0.51, clip=0, loss_scale=64, train_wall=69, gb_free=16.6, wall=18932
2023-08-08 05:21:46 | INFO | train_inner | epoch 015:    874 / 1474 loss=2.056, trans_loss=5.108, nll_loss=2.332, w2v_ctc_loss=0.706, task_loss=1.509, contrastive_loss=0.071, total=4059.35, n_correct=2545.81, ppl=5.03, accuracy=62.715, wps=11839.6, ups=1.46, wpb=8118.7, bsz=288.3, num_updates=21500, lr=9.64486e-05, gnorm=0.522, clip=0, loss_scale=64, train_wall=68, gb_free=15.3, wall=19001
2023-08-08 05:22:55 | INFO | train_inner | epoch 015:    974 / 1474 loss=2.051, trans_loss=5.1, nll_loss=2.322, w2v_ctc_loss=0.694, task_loss=1.411, contrastive_loss=0.152, total=4122.87, n_correct=2599.43, ppl=5, accuracy=63.049, wps=11886.9, ups=1.44, wpb=8245.7, bsz=301.7, num_updates=21600, lr=9.6225e-05, gnorm=0.513, clip=0, loss_scale=64, train_wall=69, gb_free=17.5, wall=19070
2023-08-08 05:24:05 | INFO | train_inner | epoch 015:   1074 / 1474 loss=2.064, trans_loss=5.108, nll_loss=2.333, w2v_ctc_loss=0.697, task_loss=1.313, contrastive_loss=0.312, total=4192.24, n_correct=2631.75, ppl=5.04, accuracy=62.777, wps=12045.7, ups=1.44, wpb=8384.5, bsz=325.2, num_updates=21700, lr=9.60031e-05, gnorm=0.514, clip=0, loss_scale=64, train_wall=69, gb_free=17.1, wall=19140
2023-08-08 05:25:14 | INFO | train_inner | epoch 015:   1174 / 1474 loss=2.036, trans_loss=5.094, nll_loss=2.317, w2v_ctc_loss=0.678, task_loss=1.257, contrastive_loss=0.118, total=4185, n_correct=2648.6, ppl=4.98, accuracy=63.288, wps=12057.3, ups=1.44, wpb=8370, bsz=329.3, num_updates=21800, lr=9.57826e-05, gnorm=0.504, clip=0, loss_scale=64, train_wall=69, gb_free=16.1, wall=19209
2023-08-08 05:26:23 | INFO | train_inner | epoch 015:   1274 / 1474 loss=2.051, trans_loss=5.102, nll_loss=2.326, w2v_ctc_loss=0.703, task_loss=1.424, contrastive_loss=0.075, total=4152.04, n_correct=2613.59, ppl=5.01, accuracy=62.947, wps=12042.2, ups=1.45, wpb=8304.1, bsz=303.7, num_updates=21900, lr=9.55637e-05, gnorm=0.514, clip=0, loss_scale=64, train_wall=68, gb_free=16.3, wall=19278
2023-08-08 05:27:33 | INFO | train_inner | epoch 015:   1374 / 1474 loss=2.047, trans_loss=5.104, nll_loss=2.328, w2v_ctc_loss=0.693, task_loss=1.444, contrastive_loss=0.059, total=4100.21, n_correct=2583.89, ppl=5.02, accuracy=63.018, wps=11776.5, ups=1.44, wpb=8200.4, bsz=293.6, num_updates=22000, lr=9.53463e-05, gnorm=0.519, clip=0, loss_scale=64, train_wall=69, gb_free=17.4, wall=19348
2023-08-08 05:27:33 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 05:27:56 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 4.184 | trans_loss 5.574 | nll_loss 2.848 | w2v_ctc_loss 1.272 | task_loss 4.616 | contrastive_loss 0.253 | total 4003.4 | n_correct 2471 | ppl 7.2 | accuracy 61.723 | uer 17.363 | wer 19.216 | raw_wer 19.216 | bleu 19.96 | wps 2237.6 | wpb 4003.4 | bsz 141.8 | num_updates 22000 | best_bleu 19.96
2023-08-08 05:27:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22000 updates
2023-08-08 05:27:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_15_22000.pt
2023-08-08 05:28:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_15_22000.pt
2023-08-08 05:28:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_15_22000.pt (epoch 15 @ 22000 updates, score 19.96) (writing took 44.26842535473406 seconds)
2023-08-08 05:29:51 | INFO | train_inner | epoch 015:   1474 / 1474 loss=2.054, trans_loss=5.109, nll_loss=2.336, w2v_ctc_loss=0.694, task_loss=1.358, contrastive_loss=0.152, total=4141.17, n_correct=2604.94, ppl=5.05, accuracy=62.903, wps=5999.7, ups=0.72, wpb=8282.3, bsz=314.3, num_updates=22100, lr=9.51303e-05, gnorm=0.513, clip=0, loss_scale=64, train_wall=69, gb_free=16.8, wall=19486
2023-08-08 05:29:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 05:30:14 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 4.202 | trans_loss 5.571 | nll_loss 2.85 | w2v_ctc_loss 1.339 | task_loss 4.645 | contrastive_loss 0.255 | total 4003.4 | n_correct 2474.5 | ppl 7.21 | accuracy 61.81 | uer 17.463 | wer 19.336 | raw_wer 19.336 | bleu 19.86 | wps 2286 | wpb 4003.4 | bsz 141.8 | num_updates 22100 | best_bleu 19.96
2023-08-08 05:30:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22100 updates
2023-08-08 05:30:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.8607.pt
2023-08-08 05:30:17 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.8607.pt
2023-08-08 05:30:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.8607.pt (epoch 15 @ 22100 updates, score 19.86) (writing took 13.918615715578198 seconds)
2023-08-08 05:30:28 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-08-08 05:30:28 | INFO | train | epoch 015 | loss 2.048 | trans_loss 5.096 | nll_loss 2.317 | w2v_ctc_loss 0.694 | task_loss 1.403 | contrastive_loss 0.115 | total 4136.86 | n_correct 2609.75 | ppl 4.98 | accuracy 63.085 | wps 10744.9 | ups 1.3 | wpb 8273.7 | bsz 305.1 | num_updates 22100 | lr 9.51303e-05 | gnorm 0.512 | clip 0 | loss_scale 64 | train_wall 1014 | gb_free 16.8 | wall 19523
2023-08-08 05:30:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 05:30:28 | INFO | fairseq.trainer | begin training epoch 16
2023-08-08 05:30:28 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 05:31:44 | INFO | train_inner | epoch 016:    100 / 1474 loss=2.026, trans_loss=5.064, nll_loss=2.275, w2v_ctc_loss=0.681, task_loss=1.334, contrastive_loss=0.092, total=4126.22, n_correct=2632.08, ppl=4.84, accuracy=63.789, wps=7317, ups=0.89, wpb=8252.4, bsz=315.6, num_updates=22200, lr=9.49158e-05, gnorm=0.515, clip=0, loss_scale=64, train_wall=68, gb_free=15.9, wall=19599
2023-08-08 05:32:53 | INFO | train_inner | epoch 016:    200 / 1474 loss=2.027, trans_loss=5.065, nll_loss=2.276, w2v_ctc_loss=0.678, task_loss=1.44, contrastive_loss=0.067, total=4100.6, n_correct=2614.27, ppl=4.84, accuracy=63.753, wps=11877.3, ups=1.45, wpb=8201.2, bsz=296.8, num_updates=22300, lr=9.47027e-05, gnorm=0.516, clip=0, loss_scale=64, train_wall=69, gb_free=12.4, wall=19668
2023-08-08 05:34:02 | INFO | train_inner | epoch 016:    300 / 1474 loss=2.039, trans_loss=5.075, nll_loss=2.29, w2v_ctc_loss=0.688, task_loss=1.391, contrastive_loss=0.139, total=4166.94, n_correct=2647.56, ppl=4.89, accuracy=63.537, wps=12020.7, ups=1.44, wpb=8333.9, bsz=308.9, num_updates=22400, lr=9.44911e-05, gnorm=0.513, clip=0, loss_scale=64, train_wall=69, gb_free=17.1, wall=19737
2023-08-08 05:35:11 | INFO | train_inner | epoch 016:    400 / 1474 loss=2.044, trans_loss=5.078, nll_loss=2.292, w2v_ctc_loss=0.692, task_loss=1.491, contrastive_loss=0.151, total=4073.3, n_correct=2584.69, ppl=4.9, accuracy=63.454, wps=11858.9, ups=1.46, wpb=8146.6, bsz=288.1, num_updates=22500, lr=9.42809e-05, gnorm=0.521, clip=0, loss_scale=64, train_wall=68, gb_free=16.9, wall=19806
2023-08-08 05:36:21 | INFO | train_inner | epoch 016:    500 / 1474 loss=2.032, trans_loss=5.074, nll_loss=2.29, w2v_ctc_loss=0.684, task_loss=1.343, contrastive_loss=0.1, total=4174.67, n_correct=2661.48, ppl=4.89, accuracy=63.753, wps=11929.4, ups=1.43, wpb=8349.3, bsz=319.1, num_updates=22600, lr=9.40721e-05, gnorm=0.514, clip=0, loss_scale=64, train_wall=69, gb_free=15.8, wall=19876
2023-08-08 05:37:30 | INFO | train_inner | epoch 016:    600 / 1474 loss=2.035, trans_loss=5.079, nll_loss=2.294, w2v_ctc_loss=0.685, task_loss=1.413, contrastive_loss=0.061, total=4124.65, n_correct=2617.28, ppl=4.91, accuracy=63.455, wps=11922.2, ups=1.45, wpb=8249.3, bsz=297.6, num_updates=22700, lr=9.38647e-05, gnorm=0.519, clip=0, loss_scale=64, train_wall=69, gb_free=16.1, wall=19945
2023-08-08 05:38:38 | INFO | train_inner | epoch 016:    700 / 1474 loss=2.036, trans_loss=5.083, nll_loss=2.3, w2v_ctc_loss=0.688, task_loss=1.434, contrastive_loss=0.063, total=4095.49, n_correct=2596.04, ppl=4.92, accuracy=63.388, wps=12007, ups=1.47, wpb=8191, bsz=296.3, num_updates=22800, lr=9.36586e-05, gnorm=0.516, clip=0, loss_scale=64, train_wall=68, gb_free=16, wall=20013
2023-08-08 05:39:47 | INFO | train_inner | epoch 016:    800 / 1474 loss=2.035, trans_loss=5.083, nll_loss=2.3, w2v_ctc_loss=0.679, task_loss=1.344, contrastive_loss=0.126, total=4174.94, n_correct=2649.89, ppl=4.92, accuracy=63.471, wps=12116.2, ups=1.45, wpb=8349.9, bsz=310.9, num_updates=22900, lr=9.34539e-05, gnorm=0.512, clip=0, loss_scale=64, train_wall=68, gb_free=16.3, wall=20082
2023-08-08 05:40:56 | INFO | train_inner | epoch 016:    900 / 1474 loss=2.035, trans_loss=5.08, nll_loss=2.297, w2v_ctc_loss=0.682, task_loss=1.354, contrastive_loss=0.118, total=4163.19, n_correct=2647.65, ppl=4.92, accuracy=63.597, wps=12085, ups=1.45, wpb=8326.4, bsz=310.6, num_updates=23000, lr=9.32505e-05, gnorm=0.511, clip=0, loss_scale=64, train_wall=68, gb_free=16.6, wall=20151
2023-08-08 05:42:06 | INFO | train_inner | epoch 016:   1000 / 1474 loss=2.05, trans_loss=5.095, nll_loss=2.316, w2v_ctc_loss=0.7, task_loss=1.462, contrastive_loss=0.116, total=4103.45, n_correct=2587.43, ppl=4.98, accuracy=63.055, wps=11766.2, ups=1.43, wpb=8206.9, bsz=298.1, num_updates=23100, lr=9.30484e-05, gnorm=0.521, clip=0, loss_scale=128, train_wall=69, gb_free=14.7, wall=20221
2023-08-08 05:42:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-08 05:43:16 | INFO | train_inner | epoch 016:   1101 / 1474 loss=2.048, trans_loss=5.098, nll_loss=2.32, w2v_ctc_loss=0.7, task_loss=1.512, contrastive_loss=0.072, total=4100.62, n_correct=2589.17, ppl=4.99, accuracy=63.141, wps=11722, ups=1.43, wpb=8201.2, bsz=291.7, num_updates=23200, lr=9.28477e-05, gnorm=0.515, clip=0, loss_scale=64, train_wall=70, gb_free=14.5, wall=20291
2023-08-08 05:43:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-08 05:44:26 | INFO | train_inner | epoch 016:   1202 / 1474 loss=2.035, trans_loss=5.089, nll_loss=2.309, w2v_ctc_loss=0.681, task_loss=1.464, contrastive_loss=0.071, total=4137.51, n_correct=2620.15, ppl=4.96, accuracy=63.327, wps=11739.4, ups=1.42, wpb=8275, bsz=299.7, num_updates=23300, lr=9.26482e-05, gnorm=0.515, clip=0, loss_scale=32, train_wall=70, gb_free=15.5, wall=20361
2023-08-08 05:45:35 | INFO | train_inner | epoch 016:   1302 / 1474 loss=2.048, trans_loss=5.093, nll_loss=2.314, w2v_ctc_loss=0.696, task_loss=1.375, contrastive_loss=0.165, total=4149.14, n_correct=2625.17, ppl=4.97, accuracy=63.27, wps=12016.1, ups=1.45, wpb=8298.3, bsz=311.9, num_updates=23400, lr=9.245e-05, gnorm=0.511, clip=0, loss_scale=32, train_wall=69, gb_free=11.1, wall=20430
2023-08-08 05:46:45 | INFO | train_inner | epoch 016:   1402 / 1474 loss=2.04, trans_loss=5.093, nll_loss=2.315, w2v_ctc_loss=0.69, task_loss=1.325, contrastive_loss=0.097, total=4200.01, n_correct=2657.22, ppl=4.98, accuracy=63.267, wps=11991.7, ups=1.43, wpb=8400, bsz=322.2, num_updates=23500, lr=9.22531e-05, gnorm=0.513, clip=0, loss_scale=32, train_wall=70, gb_free=16.8, wall=20500
2023-08-08 05:47:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 05:47:58 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 4.196 | trans_loss 5.568 | nll_loss 2.844 | w2v_ctc_loss 1.326 | task_loss 4.623 | contrastive_loss 0.255 | total 4003.4 | n_correct 2481 | ppl 7.18 | accuracy 61.972 | uer 17.535 | wer 19.145 | raw_wer 19.145 | bleu 19.74 | wps 2314 | wpb 4003.4 | bsz 141.8 | num_updates 23572 | best_bleu 19.96
2023-08-08 05:47:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 23572 updates
2023-08-08 05:47:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.7401.pt
2023-08-08 05:48:01 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.7401.pt
2023-08-08 05:48:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.7401.pt (epoch 16 @ 23572 updates, score 19.74) (writing took 21.97718319669366 seconds)
2023-08-08 05:48:20 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-08-08 05:48:20 | INFO | train | epoch 016 | loss 2.038 | trans_loss 5.082 | nll_loss 2.3 | w2v_ctc_loss 0.687 | task_loss 1.404 | contrastive_loss 0.112 | total 4135.81 | n_correct 2623.91 | ppl 4.92 | accuracy 63.444 | wps 11357.8 | ups 1.37 | wpb 8271.6 | bsz 304.8 | num_updates 23572 | lr 9.21121e-05 | gnorm 0.515 | clip 0 | loss_scale 32 | train_wall 1013 | gb_free 15.3 | wall 20595
2023-08-08 05:48:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 05:48:20 | INFO | fairseq.trainer | begin training epoch 17
2023-08-08 05:48:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 05:48:48 | INFO | train_inner | epoch 017:     28 / 1474 loss=2.04, trans_loss=5.076, nll_loss=2.291, w2v_ctc_loss=0.678, task_loss=1.434, contrastive_loss=0.231, total=4141.79, n_correct=2632.85, ppl=4.9, accuracy=63.568, wps=6739, ups=0.81, wpb=8283.6, bsz=301.5, num_updates=23600, lr=9.20575e-05, gnorm=0.517, clip=0, loss_scale=32, train_wall=69, gb_free=16, wall=20623
2023-08-08 05:49:57 | INFO | train_inner | epoch 017:    128 / 1474 loss=2.025, trans_loss=5.054, nll_loss=2.261, w2v_ctc_loss=0.686, task_loss=1.447, contrastive_loss=0.067, total=4110.88, n_correct=2632.81, ppl=4.79, accuracy=64.045, wps=11925.7, ups=1.45, wpb=8221.8, bsz=295.4, num_updates=23700, lr=9.1863e-05, gnorm=0.52, clip=0, loss_scale=32, train_wall=69, gb_free=16.2, wall=20692
2023-08-08 05:51:06 | INFO | train_inner | epoch 017:    228 / 1474 loss=2.028, trans_loss=5.054, nll_loss=2.264, w2v_ctc_loss=0.666, task_loss=1.313, contrastive_loss=0.232, total=4171.95, n_correct=2666.51, ppl=4.8, accuracy=63.915, wps=12048.6, ups=1.44, wpb=8343.9, bsz=320.4, num_updates=23800, lr=9.16698e-05, gnorm=0.51, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=20761
2023-08-08 05:52:15 | INFO | train_inner | epoch 017:    328 / 1474 loss=2.033, trans_loss=5.062, nll_loss=2.274, w2v_ctc_loss=0.673, task_loss=1.401, contrastive_loss=0.234, total=4157.94, n_correct=2652.35, ppl=4.84, accuracy=63.79, wps=12066.8, ups=1.45, wpb=8315.9, bsz=305, num_updates=23900, lr=9.14779e-05, gnorm=0.514, clip=0, loss_scale=32, train_wall=68, gb_free=14.3, wall=20830
2023-08-08 05:53:25 | INFO | train_inner | epoch 017:    428 / 1474 loss=2.02, trans_loss=5.061, nll_loss=2.272, w2v_ctc_loss=0.672, task_loss=1.397, contrastive_loss=0.065, total=4141.8, n_correct=2648.37, ppl=4.83, accuracy=63.942, wps=11925, ups=1.44, wpb=8283.6, bsz=306.7, num_updates=24000, lr=9.12871e-05, gnorm=0.512, clip=0, loss_scale=32, train_wall=69, gb_free=17.2, wall=20900
2023-08-08 05:53:25 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 05:53:48 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 4.202 | trans_loss 5.57 | nll_loss 2.846 | w2v_ctc_loss 1.344 | task_loss 4.621 | contrastive_loss 0.25 | total 4003.4 | n_correct 2474 | ppl 7.19 | accuracy 61.797 | uer 17.418 | wer 19.239 | raw_wer 19.239 | bleu 19.86 | wps 2269.5 | wpb 4003.4 | bsz 141.8 | num_updates 24000 | best_bleu 19.96
2023-08-08 05:53:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 24000 updates
2023-08-08 05:53:48 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_17_24000.pt
2023-08-08 05:53:52 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_17_24000.pt
2023-08-08 05:54:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_17_24000.pt (epoch 17 @ 24000 updates, score 19.86) (writing took 35.58447772823274 seconds)
2023-08-08 05:55:35 | INFO | train_inner | epoch 017:    528 / 1474 loss=2.03, trans_loss=5.067, nll_loss=2.28, w2v_ctc_loss=0.681, task_loss=1.455, contrastive_loss=0.112, total=4180.09, n_correct=2665.06, ppl=4.86, accuracy=63.756, wps=6435.1, ups=0.77, wpb=8360.2, bsz=307.5, num_updates=24100, lr=9.10975e-05, gnorm=0.514, clip=0, loss_scale=32, train_wall=70, gb_free=17, wall=21030
2023-08-08 05:56:43 | INFO | train_inner | epoch 017:    628 / 1474 loss=2.024, trans_loss=5.071, nll_loss=2.285, w2v_ctc_loss=0.675, task_loss=1.409, contrastive_loss=0.06, total=4166.6, n_correct=2659.9, ppl=4.87, accuracy=63.839, wps=12145.7, ups=1.46, wpb=8333.2, bsz=302.3, num_updates=24200, lr=9.09091e-05, gnorm=0.505, clip=0, loss_scale=32, train_wall=68, gb_free=15.5, wall=21098
2023-08-08 05:57:52 | INFO | train_inner | epoch 017:    728 / 1474 loss=2.036, trans_loss=5.075, nll_loss=2.291, w2v_ctc_loss=0.689, task_loss=1.382, contrastive_loss=0.111, total=4168.97, n_correct=2650.88, ppl=4.9, accuracy=63.586, wps=12096.7, ups=1.45, wpb=8337.9, bsz=308.4, num_updates=24300, lr=9.07218e-05, gnorm=0.517, clip=0, loss_scale=32, train_wall=68, gb_free=16.8, wall=21167
2023-08-08 05:59:01 | INFO | train_inner | epoch 017:    828 / 1474 loss=2.029, trans_loss=5.074, nll_loss=2.289, w2v_ctc_loss=0.68, task_loss=1.405, contrastive_loss=0.073, total=4097.38, n_correct=2606.5, ppl=4.89, accuracy=63.614, wps=11934.9, ups=1.46, wpb=8194.8, bsz=297.3, num_updates=24400, lr=9.05357e-05, gnorm=0.516, clip=0, loss_scale=32, train_wall=68, gb_free=10.3, wall=21236
2023-08-08 06:00:09 | INFO | train_inner | epoch 017:    928 / 1474 loss=2.024, trans_loss=5.073, nll_loss=2.288, w2v_ctc_loss=0.673, task_loss=1.383, contrastive_loss=0.071, total=4105.01, n_correct=2614.3, ppl=4.88, accuracy=63.686, wps=12053.2, ups=1.47, wpb=8210, bsz=304.1, num_updates=24500, lr=9.03508e-05, gnorm=0.521, clip=0, loss_scale=32, train_wall=68, gb_free=16.5, wall=21304
2023-08-08 06:01:18 | INFO | train_inner | epoch 017:   1028 / 1474 loss=2.028, trans_loss=5.073, nll_loss=2.289, w2v_ctc_loss=0.681, task_loss=1.398, contrastive_loss=0.076, total=4105.88, n_correct=2619.25, ppl=4.89, accuracy=63.793, wps=11877.9, ups=1.45, wpb=8211.8, bsz=303.4, num_updates=24600, lr=9.0167e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=21373
2023-08-08 06:02:27 | INFO | train_inner | epoch 017:   1128 / 1474 loss=2.027, trans_loss=5.073, nll_loss=2.288, w2v_ctc_loss=0.676, task_loss=1.434, contrastive_loss=0.067, total=4095.58, n_correct=2607.78, ppl=4.88, accuracy=63.673, wps=11908.8, ups=1.45, wpb=8191.2, bsz=298.5, num_updates=24700, lr=8.99843e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=68, gb_free=15.4, wall=21442
2023-08-08 06:03:36 | INFO | train_inner | epoch 017:   1228 / 1474 loss=2.046, trans_loss=5.082, nll_loss=2.301, w2v_ctc_loss=0.674, task_loss=1.37, contrastive_loss=0.306, total=4162.14, n_correct=2636.21, ppl=4.93, accuracy=63.338, wps=11998, ups=1.44, wpb=8324.3, bsz=320.2, num_updates=24800, lr=8.98027e-05, gnorm=0.516, clip=0, loss_scale=32, train_wall=69, gb_free=15.9, wall=21511
2023-08-08 06:04:46 | INFO | train_inner | epoch 017:   1328 / 1474 loss=2.032, trans_loss=5.08, nll_loss=2.298, w2v_ctc_loss=0.67, task_loss=1.394, contrastive_loss=0.145, total=4149.03, n_correct=2636, ppl=4.92, accuracy=63.533, wps=11920.7, ups=1.44, wpb=8298.1, bsz=306.7, num_updates=24900, lr=8.96221e-05, gnorm=0.515, clip=0, loss_scale=32, train_wall=69, gb_free=16.7, wall=21581
2023-08-08 06:05:55 | INFO | train_inner | epoch 017:   1428 / 1474 loss=2.029, trans_loss=5.081, nll_loss=2.3, w2v_ctc_loss=0.676, task_loss=1.407, contrastive_loss=0.067, total=4117.13, n_correct=2611.72, ppl=4.92, accuracy=63.435, wps=11910.2, ups=1.45, wpb=8234.3, bsz=303.7, num_updates=25000, lr=8.94427e-05, gnorm=0.516, clip=0, loss_scale=32, train_wall=69, gb_free=17.4, wall=21650
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:0')
2023-08-08 06:06:27 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:7')
2023-08-08 06:06:50 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 4.19 | trans_loss 5.557 | nll_loss 2.833 | w2v_ctc_loss 1.333 | task_loss 4.617 | contrastive_loss 0.259 | total 4003.4 | n_correct 2477.6 | ppl 7.13 | accuracy 61.887 | uer 17.41 | wer 19.082 | raw_wer 19.082 | bleu 19.89 | wps 2259.7 | wpb 4003.4 | bsz 141.8 | num_updates 25046 | best_bleu 19.96
2023-08-08 06:06:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 25046 updates
2023-08-08 06:06:50 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.8902.pt
2023-08-08 06:06:53 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.8902.pt
2023-08-08 06:07:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.8902.pt (epoch 17 @ 25046 updates, score 19.89) (writing took 13.711019534617662 seconds)
2023-08-08 06:07:04 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-08-08 06:07:04 | INFO | train | epoch 017 | loss 2.029 | trans_loss 5.07 | nll_loss 2.284 | w2v_ctc_loss 0.677 | task_loss 1.4 | contrastive_loss 0.119 | total 4138.65 | n_correct 2636.73 | ppl 4.87 | accuracy 63.71 | wps 10853.4 | ups 1.31 | wpb 8277.3 | bsz 305.7 | num_updates 25046 | lr 8.93605e-05 | gnorm 0.516 | clip 0 | loss_scale 32 | train_wall 1011 | gb_free 16.2 | wall 21719
2023-08-08 06:07:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 06:07:04 | INFO | fairseq.trainer | begin training epoch 18
2023-08-08 06:07:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 06:07:51 | INFO | train_inner | epoch 018:     54 / 1474 loss=2.026, trans_loss=5.066, nll_loss=2.279, w2v_ctc_loss=0.681, task_loss=1.427, contrastive_loss=0.076, total=4138.21, n_correct=2639.64, ppl=4.85, accuracy=63.787, wps=7144.5, ups=0.86, wpb=8276.4, bsz=303.2, num_updates=25100, lr=8.92644e-05, gnorm=0.521, clip=0, loss_scale=32, train_wall=69, gb_free=17.7, wall=21766
2023-08-08 06:09:00 | INFO | train_inner | epoch 018:    154 / 1474 loss=2.013, trans_loss=5.038, nll_loss=2.242, w2v_ctc_loss=0.651, task_loss=1.331, contrastive_loss=0.198, total=4158.88, n_correct=2678.37, ppl=4.73, accuracy=64.401, wps=11994.8, ups=1.44, wpb=8317.8, bsz=314, num_updates=25200, lr=8.90871e-05, gnorm=0.512, clip=0, loss_scale=32, train_wall=69, gb_free=16.6, wall=21835
2023-08-08 06:10:10 | INFO | train_inner | epoch 018:    254 / 1474 loss=2.004, trans_loss=5.036, nll_loss=2.241, w2v_ctc_loss=0.661, task_loss=1.358, contrastive_loss=0.067, total=4164.11, n_correct=2686.35, ppl=4.73, accuracy=64.512, wps=12001.9, ups=1.44, wpb=8328.2, bsz=312.5, num_updates=25300, lr=8.89108e-05, gnorm=0.511, clip=0, loss_scale=64, train_wall=69, gb_free=14.8, wall=21905
2023-08-08 06:11:19 | INFO | train_inner | epoch 018:    354 / 1474 loss=2.016, trans_loss=5.048, nll_loss=2.255, w2v_ctc_loss=0.667, task_loss=1.418, contrastive_loss=0.082, total=4163.13, n_correct=2669.67, ppl=4.77, accuracy=64.127, wps=12002.8, ups=1.44, wpb=8326.3, bsz=301.5, num_updates=25400, lr=8.87357e-05, gnorm=0.511, clip=0, loss_scale=64, train_wall=69, gb_free=17.5, wall=21974
2023-08-08 06:12:29 | INFO | train_inner | epoch 018:    454 / 1474 loss=2.026, trans_loss=5.056, nll_loss=2.265, w2v_ctc_loss=0.669, task_loss=1.491, contrastive_loss=0.172, total=4087.83, n_correct=2612.74, ppl=4.81, accuracy=63.915, wps=11734.8, ups=1.44, wpb=8175.7, bsz=295.3, num_updates=25500, lr=8.85615e-05, gnorm=0.521, clip=0, loss_scale=64, train_wall=69, gb_free=16.2, wall=22044
2023-08-08 06:13:38 | INFO | train_inner | epoch 018:    554 / 1474 loss=2.005, trans_loss=5.042, nll_loss=2.249, w2v_ctc_loss=0.661, task_loss=1.257, contrastive_loss=0.082, total=4204.41, n_correct=2706.16, ppl=4.75, accuracy=64.365, wps=12143.1, ups=1.44, wpb=8408.8, bsz=328, num_updates=25600, lr=8.83883e-05, gnorm=0.507, clip=0, loss_scale=64, train_wall=69, gb_free=17.3, wall=22113
2023-08-08 06:14:47 | INFO | train_inner | epoch 018:    654 / 1474 loss=2.028, trans_loss=5.066, nll_loss=2.279, w2v_ctc_loss=0.675, task_loss=1.444, contrastive_loss=0.149, total=4096.81, n_correct=2615.69, ppl=4.85, accuracy=63.847, wps=11874.8, ups=1.45, wpb=8193.6, bsz=298.9, num_updates=25700, lr=8.82162e-05, gnorm=0.523, clip=0, loss_scale=64, train_wall=68, gb_free=16.5, wall=22182
2023-08-08 06:15:56 | INFO | train_inner | epoch 018:    754 / 1474 loss=2.031, trans_loss=5.062, nll_loss=2.275, w2v_ctc_loss=0.676, task_loss=1.333, contrastive_loss=0.239, total=4208.29, n_correct=2686.71, ppl=4.84, accuracy=63.843, wps=12185.5, ups=1.45, wpb=8416.6, bsz=322.8, num_updates=25800, lr=8.80451e-05, gnorm=0.513, clip=0, loss_scale=64, train_wall=69, gb_free=17.7, wall=22251
2023-08-08 06:17:05 | INFO | train_inner | epoch 018:    854 / 1474 loss=2.019, trans_loss=5.062, nll_loss=2.274, w2v_ctc_loss=0.671, task_loss=1.42, contrastive_loss=0.057, total=4166.81, n_correct=2664.63, ppl=4.84, accuracy=63.949, wps=12021.9, ups=1.44, wpb=8333.6, bsz=301.9, num_updates=25900, lr=8.7875e-05, gnorm=0.518, clip=0, loss_scale=64, train_wall=69, gb_free=12.4, wall=22320
2023-08-08 06:18:14 | INFO | train_inner | epoch 018:    954 / 1474 loss=2.01, trans_loss=5.053, nll_loss=2.264, w2v_ctc_loss=0.66, task_loss=1.302, contrastive_loss=0.08, total=4142.65, n_correct=2659.66, ppl=4.8, accuracy=64.202, wps=12088, ups=1.46, wpb=8285.3, bsz=316.1, num_updates=26000, lr=8.77058e-05, gnorm=0.516, clip=0, loss_scale=64, train_wall=68, gb_free=14.7, wall=22389
2023-08-08 06:18:14 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 06:18:38 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 4.196 | trans_loss 5.565 | nll_loss 2.838 | w2v_ctc_loss 1.337 | task_loss 4.588 | contrastive_loss 0.254 | total 4003.4 | n_correct 2480.3 | ppl 7.15 | accuracy 61.955 | uer 17.405 | wer 19.16 | raw_wer 19.16 | bleu 19.68 | wps 1920 | wpb 4003.4 | bsz 141.8 | num_updates 26000 | best_bleu 19.96
2023-08-08 06:18:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26000 updates
2023-08-08 06:18:38 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_18_26000.pt
2023-08-08 06:18:42 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_18_26000.pt
2023-08-08 06:19:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_18_26000.pt (epoch 18 @ 26000 updates, score 19.68) (writing took 36.679920736700296 seconds)
2023-08-08 06:20:28 | INFO | train_inner | epoch 018:   1054 / 1474 loss=2.015, trans_loss=5.06, nll_loss=2.272, w2v_ctc_loss=0.661, task_loss=1.457, contrastive_loss=0.069, total=4137.77, n_correct=2651.8, ppl=4.83, accuracy=64.088, wps=6186.5, ups=0.75, wpb=8275.5, bsz=300.5, num_updates=26100, lr=8.75376e-05, gnorm=0.512, clip=0, loss_scale=64, train_wall=69, gb_free=17.2, wall=22523
2023-08-08 06:21:37 | INFO | train_inner | epoch 018:   1154 / 1474 loss=2.019, trans_loss=5.049, nll_loss=2.259, w2v_ctc_loss=0.666, task_loss=1.326, contrastive_loss=0.175, total=4153.69, n_correct=2668.41, ppl=4.79, accuracy=64.242, wps=12016.4, ups=1.45, wpb=8307.4, bsz=314.9, num_updates=26200, lr=8.73704e-05, gnorm=0.513, clip=0, loss_scale=64, train_wall=69, gb_free=14.9, wall=22592
2023-08-08 06:22:46 | INFO | train_inner | epoch 018:   1254 / 1474 loss=2.023, trans_loss=5.071, nll_loss=2.286, w2v_ctc_loss=0.669, task_loss=1.501, contrastive_loss=0.063, total=4087.62, n_correct=2610.98, ppl=4.88, accuracy=63.875, wps=11837.8, ups=1.45, wpb=8175.2, bsz=287.1, num_updates=26300, lr=8.72041e-05, gnorm=0.518, clip=0, loss_scale=64, train_wall=69, gb_free=16.5, wall=22661
2023-08-08 06:23:55 | INFO | train_inner | epoch 018:   1354 / 1474 loss=2.034, trans_loss=5.079, nll_loss=2.297, w2v_ctc_loss=0.685, task_loss=1.491, contrastive_loss=0.088, total=4070.69, n_correct=2589.23, ppl=4.91, accuracy=63.607, wps=11865.6, ups=1.46, wpb=8141.4, bsz=291.7, num_updates=26400, lr=8.70388e-05, gnorm=0.524, clip=0, loss_scale=64, train_wall=68, gb_free=17.3, wall=22729
2023-08-08 06:25:04 | INFO | train_inner | epoch 018:   1454 / 1474 loss=2.024, trans_loss=5.071, nll_loss=2.287, w2v_ctc_loss=0.674, task_loss=1.477, contrastive_loss=0.074, total=4113.2, n_correct=2623.52, ppl=4.88, accuracy=63.783, wps=11911.3, ups=1.45, wpb=8226.4, bsz=297.5, num_updates=26500, lr=8.68744e-05, gnorm=0.521, clip=0, loss_scale=64, train_wall=69, gb_free=16.3, wall=22799
2023-08-08 06:25:18 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 06:25:40 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 4.21 | trans_loss 5.56 | nll_loss 2.835 | w2v_ctc_loss 1.393 | task_loss 4.622 | contrastive_loss 0.255 | total 4003.4 | n_correct 2481.5 | ppl 7.14 | accuracy 61.985 | uer 17.347 | wer 19.172 | raw_wer 19.172 | bleu 20.05 | wps 2236.3 | wpb 4003.4 | bsz 141.8 | num_updates 26520 | best_bleu 20.05
2023-08-08 06:25:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26520 updates
2023-08-08 06:25:40 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 06:25:53 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 06:26:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt (epoch 18 @ 26520 updates, score 20.05) (writing took 23.889086415991187 seconds)
2023-08-08 06:26:05 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-08-08 06:26:05 | INFO | train | epoch 018 | loss 2.019 | trans_loss 5.057 | nll_loss 2.267 | w2v_ctc_loss 0.668 | task_loss 1.398 | contrastive_loss 0.117 | total 4138.65 | n_correct 2650.82 | ppl 4.81 | accuracy 64.05 | wps 10697.1 | ups 1.29 | wpb 8277.3 | bsz 305.7 | num_updates 26520 | lr 8.68417e-05 | gnorm 0.516 | clip 0 | loss_scale 64 | train_wall 1012 | gb_free 15.8 | wall 22860
2023-08-08 06:26:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 06:26:05 | INFO | fairseq.trainer | begin training epoch 19
2023-08-08 06:26:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 06:27:08 | INFO | train_inner | epoch 019:     80 / 1474 loss=2.01, trans_loss=5.035, nll_loss=2.239, w2v_ctc_loss=0.662, task_loss=1.403, contrastive_loss=0.123, total=4102.06, n_correct=2643.61, ppl=4.72, accuracy=64.446, wps=6611.4, ups=0.81, wpb=8204.1, bsz=296.9, num_updates=26600, lr=8.6711e-05, gnorm=0.524, clip=0, loss_scale=64, train_wall=69, gb_free=17.4, wall=22923
2023-08-08 06:28:17 | INFO | train_inner | epoch 019:    180 / 1474 loss=2.008, trans_loss=5.028, nll_loss=2.23, w2v_ctc_loss=0.669, task_loss=1.304, contrastive_loss=0.117, total=4227.7, n_correct=2731.15, ppl=4.69, accuracy=64.601, wps=12173, ups=1.44, wpb=8455.4, bsz=324.8, num_updates=26700, lr=8.65485e-05, gnorm=0.514, clip=0, loss_scale=64, train_wall=69, gb_free=16.9, wall=22992
2023-08-08 06:29:27 | INFO | train_inner | epoch 019:    280 / 1474 loss=2.002, trans_loss=5.028, nll_loss=2.23, w2v_ctc_loss=0.663, task_loss=1.382, contrastive_loss=0.058, total=4187.34, n_correct=2707.38, ppl=4.69, accuracy=64.656, wps=12070, ups=1.44, wpb=8374.7, bsz=306.4, num_updates=26800, lr=8.63868e-05, gnorm=0.511, clip=0, loss_scale=64, train_wall=69, gb_free=15.8, wall=23061
2023-08-08 06:30:36 | INFO | train_inner | epoch 019:    380 / 1474 loss=2.01, trans_loss=5.035, nll_loss=2.241, w2v_ctc_loss=0.654, task_loss=1.381, contrastive_loss=0.166, total=4170.52, n_correct=2688.81, ppl=4.73, accuracy=64.472, wps=12019.8, ups=1.44, wpb=8341, bsz=311, num_updates=26900, lr=8.62261e-05, gnorm=0.511, clip=0, loss_scale=64, train_wall=69, gb_free=16.2, wall=23131
2023-08-08 06:31:45 | INFO | train_inner | epoch 019:    480 / 1474 loss=2.011, trans_loss=5.042, nll_loss=2.249, w2v_ctc_loss=0.667, task_loss=1.437, contrastive_loss=0.076, total=4113.89, n_correct=2648.02, ppl=4.75, accuracy=64.368, wps=11970.5, ups=1.45, wpb=8227.8, bsz=301.5, num_updates=27000, lr=8.60663e-05, gnorm=0.52, clip=0, loss_scale=64, train_wall=68, gb_free=17.1, wall=23200
2023-08-08 06:32:54 | INFO | train_inner | epoch 019:    580 / 1474 loss=2.007, trans_loss=5.037, nll_loss=2.243, w2v_ctc_loss=0.659, task_loss=1.373, contrastive_loss=0.136, total=4128.58, n_correct=2665.79, ppl=4.73, accuracy=64.569, wps=11917.9, ups=1.44, wpb=8257.2, bsz=306.2, num_updates=27100, lr=8.59074e-05, gnorm=0.514, clip=0, loss_scale=64, train_wall=69, gb_free=16.5, wall=23269
2023-08-08 06:34:03 | INFO | train_inner | epoch 019:    680 / 1474 loss=1.995, trans_loss=5.038, nll_loss=2.244, w2v_ctc_loss=0.642, task_loss=1.274, contrastive_loss=0.065, total=4201.56, n_correct=2713.63, ppl=4.74, accuracy=64.586, wps=12205.3, ups=1.45, wpb=8403.1, bsz=321.5, num_updates=27200, lr=8.57493e-05, gnorm=0.506, clip=0, loss_scale=64, train_wall=68, gb_free=17.7, wall=23338
2023-08-08 06:35:12 | INFO | train_inner | epoch 019:    780 / 1474 loss=2.011, trans_loss=5.044, nll_loss=2.25, w2v_ctc_loss=0.669, task_loss=1.443, contrastive_loss=0.071, total=4124.03, n_correct=2652.6, ppl=4.76, accuracy=64.321, wps=11880.8, ups=1.44, wpb=8248.1, bsz=299, num_updates=27300, lr=8.55921e-05, gnorm=0.518, clip=0, loss_scale=64, train_wall=69, gb_free=17.4, wall=23407
2023-08-08 06:36:22 | INFO | train_inner | epoch 019:    880 / 1474 loss=2.013, trans_loss=5.053, nll_loss=2.264, w2v_ctc_loss=0.666, task_loss=1.394, contrastive_loss=0.068, total=4177.8, n_correct=2680.86, ppl=4.8, accuracy=64.169, wps=12045.3, ups=1.44, wpb=8355.6, bsz=309.6, num_updates=27400, lr=8.54358e-05, gnorm=0.518, clip=0, loss_scale=128, train_wall=69, gb_free=14.7, wall=23477
2023-08-08 06:36:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-08 06:37:32 | INFO | train_inner | epoch 019:    981 / 1474 loss=2.029, trans_loss=5.062, nll_loss=2.277, w2v_ctc_loss=0.662, task_loss=1.414, contrastive_loss=0.294, total=4081.45, n_correct=2610.59, ppl=4.85, accuracy=63.962, wps=11567.9, ups=1.42, wpb=8162.9, bsz=306.1, num_updates=27500, lr=8.52803e-05, gnorm=0.526, clip=0, loss_scale=64, train_wall=70, gb_free=16.5, wall=23547
2023-08-08 06:38:41 | INFO | train_inner | epoch 019:   1081 / 1474 loss=2.02, trans_loss=5.065, nll_loss=2.279, w2v_ctc_loss=0.665, task_loss=1.497, contrastive_loss=0.105, total=4036.97, n_correct=2581.38, ppl=4.86, accuracy=63.944, wps=11685.7, ups=1.45, wpb=8073.9, bsz=291, num_updates=27600, lr=8.51257e-05, gnorm=0.521, clip=0, loss_scale=64, train_wall=69, gb_free=14.5, wall=23616
2023-08-08 06:39:51 | INFO | train_inner | epoch 019:   1181 / 1474 loss=2.027, trans_loss=5.06, nll_loss=2.273, w2v_ctc_loss=0.671, task_loss=1.421, contrastive_loss=0.186, total=4137.49, n_correct=2645.8, ppl=4.83, accuracy=63.947, wps=11810.2, ups=1.43, wpb=8275, bsz=307.7, num_updates=27700, lr=8.49719e-05, gnorm=0.521, clip=0, loss_scale=64, train_wall=70, gb_free=14.7, wall=23686
2023-08-08 06:41:00 | INFO | train_inner | epoch 019:   1281 / 1474 loss=2.015, trans_loss=5.062, nll_loss=2.275, w2v_ctc_loss=0.658, task_loss=1.417, contrastive_loss=0.085, total=4141.89, n_correct=2650.05, ppl=4.84, accuracy=63.982, wps=12049.2, ups=1.45, wpb=8283.8, bsz=300.1, num_updates=27800, lr=8.48189e-05, gnorm=0.517, clip=0, loss_scale=64, train_wall=68, gb_free=15.4, wall=23755
2023-08-08 06:42:10 | INFO | train_inner | epoch 019:   1381 / 1474 loss=2.013, trans_loss=5.055, nll_loss=2.267, w2v_ctc_loss=0.664, task_loss=1.43, contrastive_loss=0.071, total=4133.26, n_correct=2655.06, ppl=4.81, accuracy=64.236, wps=11812.7, ups=1.43, wpb=8266.5, bsz=301, num_updates=27900, lr=8.46668e-05, gnorm=0.517, clip=0, loss_scale=64, train_wall=69, gb_free=16, wall=23825
2023-08-08 06:43:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 06:43:37 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 4.179 | trans_loss 5.55 | nll_loss 2.824 | w2v_ctc_loss 1.315 | task_loss 4.64 | contrastive_loss 0.25 | total 4003.4 | n_correct 2488.8 | ppl 7.08 | accuracy 62.167 | uer 17.11 | wer 18.87 | raw_wer 18.87 | bleu 19.81 | wps 2285.6 | wpb 4003.4 | bsz 141.8 | num_updates 27993 | best_bleu 20.05
2023-08-08 06:43:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 27993 updates
2023-08-08 06:43:37 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.8105.pt
2023-08-08 06:43:40 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.8105.pt
2023-08-08 06:43:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.8105.pt (epoch 19 @ 27993 updates, score 19.81) (writing took 17.956765696406364 seconds)
2023-08-08 06:43:57 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-08-08 06:43:57 | INFO | train | epoch 019 | loss 2.012 | trans_loss 5.046 | nll_loss 2.254 | w2v_ctc_loss 0.662 | task_loss 1.4 | contrastive_loss 0.115 | total 4137.99 | n_correct 2661.13 | ppl 4.77 | accuracy 64.31 | wps 11374 | ups 1.37 | wpb 8276 | bsz 305.5 | num_updates 27993 | lr 8.4526e-05 | gnorm 0.517 | clip 0 | loss_scale 64 | train_wall 1016 | gb_free 17.3 | wall 23931
2023-08-08 06:43:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 06:43:57 | INFO | fairseq.trainer | begin training epoch 20
2023-08-08 06:43:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 06:44:10 | INFO | train_inner | epoch 020:      7 / 1474 loss=2.013, trans_loss=5.049, nll_loss=2.26, w2v_ctc_loss=0.656, task_loss=1.413, contrastive_loss=0.154, total=4119.08, n_correct=2648.62, ppl=4.79, accuracy=64.301, wps=6897, ups=0.84, wpb=8238.2, bsz=304.1, num_updates=28000, lr=8.45154e-05, gnorm=0.519, clip=0, loss_scale=64, train_wall=69, gb_free=16, wall=23944
2023-08-08 06:44:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 06:44:33 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 4.18 | trans_loss 5.552 | nll_loss 2.823 | w2v_ctc_loss 1.316 | task_loss 4.643 | contrastive_loss 0.248 | total 4003.4 | n_correct 2491.1 | ppl 7.08 | accuracy 62.225 | uer 16.975 | wer 18.698 | raw_wer 18.698 | bleu 19.93 | wps 2251.8 | wpb 4003.4 | bsz 141.8 | num_updates 28000 | best_bleu 20.05
2023-08-08 06:44:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 28000 updates
2023-08-08 06:44:33 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_20_28000.pt
2023-08-08 06:44:36 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_20_28000.pt
2023-08-08 06:45:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_20_28000.pt (epoch 20 @ 28000 updates, score 19.93) (writing took 30.600060537457466 seconds)
2023-08-08 06:46:14 | INFO | train_inner | epoch 020:    107 / 1474 loss=1.988, trans_loss=5.01, nll_loss=2.206, w2v_ctc_loss=0.646, task_loss=1.352, contrastive_loss=0.079, total=4195.03, n_correct=2730.74, ppl=4.61, accuracy=65.095, wps=6753.2, ups=0.8, wpb=8390.1, bsz=313.7, num_updates=28100, lr=8.43649e-05, gnorm=0.512, clip=0, loss_scale=64, train_wall=69, gb_free=14.8, wall=24069
2023-08-08 06:47:23 | INFO | train_inner | epoch 020:    207 / 1474 loss=2, trans_loss=5.022, nll_loss=2.222, w2v_ctc_loss=0.652, task_loss=1.453, contrastive_loss=0.128, total=4154.14, n_correct=2691.17, ppl=4.67, accuracy=64.783, wps=11934.9, ups=1.44, wpb=8308.3, bsz=301, num_updates=28200, lr=8.42152e-05, gnorm=0.512, clip=0, loss_scale=64, train_wall=69, gb_free=17.1, wall=24138
2023-08-08 06:48:33 | INFO | train_inner | epoch 020:    307 / 1474 loss=1.991, trans_loss=5.016, nll_loss=2.216, w2v_ctc_loss=0.653, task_loss=1.264, contrastive_loss=0.069, total=4188.05, n_correct=2719.09, ppl=4.65, accuracy=64.925, wps=12086.5, ups=1.44, wpb=8376.1, bsz=326.2, num_updates=28300, lr=8.40663e-05, gnorm=0.517, clip=0, loss_scale=64, train_wall=69, gb_free=15.4, wall=24208
2023-08-08 06:49:42 | INFO | train_inner | epoch 020:    407 / 1474 loss=1.994, trans_loss=5.022, nll_loss=2.222, w2v_ctc_loss=0.645, task_loss=1.42, contrastive_loss=0.066, total=4115.16, n_correct=2668.86, ppl=4.67, accuracy=64.854, wps=11961.1, ups=1.45, wpb=8230.3, bsz=297, num_updates=28400, lr=8.39181e-05, gnorm=0.511, clip=0, loss_scale=64, train_wall=68, gb_free=15.3, wall=24276
2023-08-08 06:50:51 | INFO | train_inner | epoch 020:    507 / 1474 loss=2.007, trans_loss=5.038, nll_loss=2.243, w2v_ctc_loss=0.65, task_loss=1.434, contrastive_loss=0.154, total=4108.46, n_correct=2648.82, ppl=4.73, accuracy=64.472, wps=11901.2, ups=1.45, wpb=8216.9, bsz=300.4, num_updates=28500, lr=8.37708e-05, gnorm=0.514, clip=0, loss_scale=64, train_wall=69, gb_free=16.3, wall=24346
2023-08-08 06:52:00 | INFO | train_inner | epoch 020:    607 / 1474 loss=2.012, trans_loss=5.037, nll_loss=2.243, w2v_ctc_loss=0.656, task_loss=1.474, contrastive_loss=0.16, total=4094.9, n_correct=2635.68, ppl=4.73, accuracy=64.365, wps=11864.8, ups=1.45, wpb=8189.8, bsz=296.1, num_updates=28600, lr=8.36242e-05, gnorm=0.528, clip=0, loss_scale=64, train_wall=69, gb_free=11.7, wall=24415
2023-08-08 06:53:08 | INFO | train_inner | epoch 020:    707 / 1474 loss=2.002, trans_loss=5.036, nll_loss=2.241, w2v_ctc_loss=0.659, task_loss=1.399, contrastive_loss=0.059, total=4140.23, n_correct=2673.53, ppl=4.73, accuracy=64.574, wps=12060.7, ups=1.46, wpb=8280.5, bsz=300.6, num_updates=28700, lr=8.34784e-05, gnorm=0.515, clip=0, loss_scale=64, train_wall=68, gb_free=16, wall=24483
2023-08-08 06:54:17 | INFO | train_inner | epoch 020:    807 / 1474 loss=2.001, trans_loss=5.034, nll_loss=2.238, w2v_ctc_loss=0.66, task_loss=1.386, contrastive_loss=0.063, total=4140.66, n_correct=2673.55, ppl=4.72, accuracy=64.568, wps=12023.4, ups=1.45, wpb=8281.3, bsz=305.6, num_updates=28800, lr=8.33333e-05, gnorm=0.521, clip=0, loss_scale=64, train_wall=68, gb_free=17.4, wall=24552
2023-08-08 06:55:27 | INFO | train_inner | epoch 020:    907 / 1474 loss=2.027, trans_loss=5.048, nll_loss=2.258, w2v_ctc_loss=0.658, task_loss=1.336, contrastive_loss=0.358, total=4157.15, n_correct=2667.84, ppl=4.78, accuracy=64.175, wps=11881, ups=1.43, wpb=8314.3, bsz=322.6, num_updates=28900, lr=8.3189e-05, gnorm=0.521, clip=0, loss_scale=64, train_wall=70, gb_free=17.6, wall=24622
2023-08-08 06:56:36 | INFO | train_inner | epoch 020:   1007 / 1474 loss=2, trans_loss=5.037, nll_loss=2.243, w2v_ctc_loss=0.65, task_loss=1.381, contrastive_loss=0.07, total=4171.86, n_correct=2694.35, ppl=4.73, accuracy=64.584, wps=12052.4, ups=1.44, wpb=8343.7, bsz=308.6, num_updates=29000, lr=8.30455e-05, gnorm=0.52, clip=0, loss_scale=64, train_wall=69, gb_free=15.5, wall=24691
2023-08-08 06:57:46 | INFO | train_inner | epoch 020:   1107 / 1474 loss=2.014, trans_loss=5.044, nll_loss=2.254, w2v_ctc_loss=0.654, task_loss=1.353, contrastive_loss=0.208, total=4162.96, n_correct=2682.29, ppl=4.77, accuracy=64.432, wps=12049.6, ups=1.45, wpb=8325.9, bsz=314.9, num_updates=29100, lr=8.29027e-05, gnorm=0.516, clip=0, loss_scale=64, train_wall=69, gb_free=16.9, wall=24760
2023-08-08 06:58:54 | INFO | train_inner | epoch 020:   1207 / 1474 loss=2.008, trans_loss=5.037, nll_loss=2.243, w2v_ctc_loss=0.667, task_loss=1.53, contrastive_loss=0.057, total=4033.74, n_correct=2600.11, ppl=4.73, accuracy=64.459, wps=11728.7, ups=1.45, wpb=8067.5, bsz=285.2, num_updates=29200, lr=8.27606e-05, gnorm=0.527, clip=0, loss_scale=64, train_wall=68, gb_free=17.1, wall=24829
2023-08-08 07:00:04 | INFO | train_inner | epoch 020:   1307 / 1474 loss=2.006, trans_loss=5.049, nll_loss=2.259, w2v_ctc_loss=0.656, task_loss=1.478, contrastive_loss=0.063, total=4124.42, n_correct=2653.1, ppl=4.79, accuracy=64.327, wps=11884.9, ups=1.44, wpb=8248.8, bsz=297, num_updates=29300, lr=8.26192e-05, gnorm=0.518, clip=0, loss_scale=64, train_wall=69, gb_free=15.8, wall=24899
2023-08-08 07:01:13 | INFO | train_inner | epoch 020:   1407 / 1474 loss=2.006, trans_loss=5.047, nll_loss=2.256, w2v_ctc_loss=0.655, task_loss=1.482, contrastive_loss=0.061, total=4114.1, n_correct=2648.21, ppl=4.78, accuracy=64.369, wps=11877.5, ups=1.44, wpb=8228.2, bsz=293.7, num_updates=29400, lr=8.24786e-05, gnorm=0.523, clip=0, loss_scale=64, train_wall=69, gb_free=14.3, wall=24968
2023-08-08 07:01:59 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 07:02:22 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 4.185 | trans_loss 5.556 | nll_loss 2.829 | w2v_ctc_loss 1.324 | task_loss 4.616 | contrastive_loss 0.247 | total 4003.4 | n_correct 2483.2 | ppl 7.1 | accuracy 62.027 | uer 17.23 | wer 19.071 | raw_wer 19.071 | bleu 19.87 | wps 2247.7 | wpb 4003.4 | bsz 141.8 | num_updates 29467 | best_bleu 20.05
2023-08-08 07:02:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 29467 updates
2023-08-08 07:02:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.8706.pt
2023-08-08 07:02:25 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.8706.pt
2023-08-08 07:02:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.8706.pt (epoch 20 @ 29467 updates, score 19.87) (writing took 14.305559072643518 seconds)
2023-08-08 07:02:37 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-08-08 07:02:37 | INFO | train | epoch 020 | loss 2.004 | trans_loss 5.035 | nll_loss 2.24 | w2v_ctc_loss 0.654 | task_loss 1.399 | contrastive_loss 0.115 | total 4138.65 | n_correct 2672.01 | ppl 4.72 | accuracy 64.562 | wps 10893.1 | ups 1.32 | wpb 8277.3 | bsz 305.7 | num_updates 29467 | lr 8.23848e-05 | gnorm 0.518 | clip 0 | loss_scale 128 | train_wall 1012 | gb_free 16 | wall 25051
2023-08-08 07:02:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 07:02:37 | INFO | fairseq.trainer | begin training epoch 21
2023-08-08 07:02:37 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 07:03:08 | INFO | train_inner | epoch 021:     33 / 1474 loss=2.009, trans_loss=5.04, nll_loss=2.248, w2v_ctc_loss=0.653, task_loss=1.321, contrastive_loss=0.183, total=4155.01, n_correct=2676.69, ppl=4.75, accuracy=64.421, wps=7248.7, ups=0.87, wpb=8310, bsz=317.6, num_updates=29500, lr=8.23387e-05, gnorm=0.521, clip=0, loss_scale=128, train_wall=68, gb_free=16.2, wall=25083
2023-08-08 07:03:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-08 07:04:18 | INFO | train_inner | epoch 021:    134 / 1474 loss=1.983, trans_loss=5.004, nll_loss=2.199, w2v_ctc_loss=0.644, task_loss=1.353, contrastive_loss=0.063, total=4168.09, n_correct=2717.29, ppl=4.59, accuracy=65.193, wps=11894.1, ups=1.43, wpb=8336.2, bsz=310.2, num_updates=29600, lr=8.21995e-05, gnorm=0.513, clip=0, loss_scale=64, train_wall=70, gb_free=16.9, wall=25153
2023-08-08 07:05:27 | INFO | train_inner | epoch 021:    234 / 1474 loss=1.984, trans_loss=5.009, nll_loss=2.206, w2v_ctc_loss=0.633, task_loss=1.332, contrastive_loss=0.126, total=4155.31, n_correct=2708.33, ppl=4.61, accuracy=65.178, wps=12066.7, ups=1.45, wpb=8310.6, bsz=312.6, num_updates=29700, lr=8.2061e-05, gnorm=0.516, clip=0, loss_scale=64, train_wall=68, gb_free=16.6, wall=25221
2023-08-08 07:06:36 | INFO | train_inner | epoch 021:    334 / 1474 loss=1.993, trans_loss=5.013, nll_loss=2.212, w2v_ctc_loss=0.647, task_loss=1.384, contrastive_loss=0.131, total=4151.51, n_correct=2699.21, ppl=4.63, accuracy=65.018, wps=11911.8, ups=1.43, wpb=8303, bsz=310.8, num_updates=29800, lr=8.19232e-05, gnorm=0.512, clip=0, loss_scale=64, train_wall=69, gb_free=15.4, wall=25291
2023-08-08 07:07:45 | INFO | train_inner | epoch 021:    434 / 1474 loss=1.984, trans_loss=5.013, nll_loss=2.211, w2v_ctc_loss=0.636, task_loss=1.353, contrastive_loss=0.055, total=4180.85, n_correct=2722.74, ppl=4.63, accuracy=65.124, wps=12196.5, ups=1.46, wpb=8361.7, bsz=306.8, num_updates=29900, lr=8.17861e-05, gnorm=0.514, clip=0, loss_scale=64, train_wall=68, gb_free=15.6, wall=25360
2023-08-08 07:08:54 | INFO | train_inner | epoch 021:    534 / 1474 loss=1.989, trans_loss=5.013, nll_loss=2.212, w2v_ctc_loss=0.648, task_loss=1.454, contrastive_loss=0.054, total=4083.98, n_correct=2657.02, ppl=4.63, accuracy=65.06, wps=11790.8, ups=1.44, wpb=8168, bsz=295.1, num_updates=30000, lr=8.16497e-05, gnorm=0.518, clip=0, loss_scale=64, train_wall=69, gb_free=12.8, wall=25429
2023-08-08 07:08:54 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 07:09:18 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 4.192 | trans_loss 5.563 | nll_loss 2.836 | w2v_ctc_loss 1.33 | task_loss 4.621 | contrastive_loss 0.249 | total 4003.4 | n_correct 2479.5 | ppl 7.14 | accuracy 61.935 | uer 17.272 | wer 18.955 | raw_wer 18.955 | bleu 19.67 | wps 2186.2 | wpb 4003.4 | bsz 141.8 | num_updates 30000 | best_bleu 20.05
2023-08-08 07:09:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30000 updates
2023-08-08 07:09:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_21_30000.pt
2023-08-08 07:09:22 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_21_30000.pt
2023-08-08 07:09:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_21_30000.pt (epoch 21 @ 30000 updates, score 19.67) (writing took 18.616062562912703 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:0')
2023-08-08 07:10:46 | INFO | train_inner | epoch 021:    634 / 1474 loss=2, trans_loss=5.021, nll_loss=2.223, w2v_ctc_loss=0.639, task_loss=1.384, contrastive_loss=0.226, total=4215.41, n_correct=2732.89, ppl=4.67, accuracy=64.831, wps=7531.5, ups=0.89, wpb=8430.8, bsz=315.4, num_updates=30100, lr=8.15139e-05, gnorm=0.52, clip=0, loss_scale=64, train_wall=69, gb_free=11.1, wall=25541
2023-08-08 07:11:56 | INFO | train_inner | epoch 021:    734 / 1474 loss=1.997, trans_loss=5.029, nll_loss=2.233, w2v_ctc_loss=0.647, task_loss=1.391, contrastive_loss=0.089, total=4152.97, n_correct=2689.83, ppl=4.7, accuracy=64.769, wps=11952.1, ups=1.44, wpb=8305.9, bsz=309.3, num_updates=30200, lr=8.13788e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=69, gb_free=16.2, wall=25610
2023-08-08 07:13:05 | INFO | train_inner | epoch 021:    834 / 1474 loss=2.001, trans_loss=5.034, nll_loss=2.239, w2v_ctc_loss=0.646, task_loss=1.479, contrastive_loss=0.101, total=4066.93, n_correct=2626.52, ppl=4.72, accuracy=64.582, wps=11771.8, ups=1.45, wpb=8133.9, bsz=294.4, num_updates=30300, lr=8.12444e-05, gnorm=0.526, clip=0, loss_scale=64, train_wall=69, gb_free=16.8, wall=25680
2023-08-08 07:14:14 | INFO | train_inner | epoch 021:    934 / 1474 loss=1.994, trans_loss=5.024, nll_loss=2.226, w2v_ctc_loss=0.647, task_loss=1.4, contrastive_loss=0.074, total=4103.34, n_correct=2655.2, ppl=4.68, accuracy=64.708, wps=11908.9, ups=1.45, wpb=8206.7, bsz=301, num_updates=30400, lr=8.11107e-05, gnorm=0.522, clip=0, loss_scale=64, train_wall=68, gb_free=13.8, wall=25748
2023-08-08 07:15:22 | INFO | train_inner | epoch 021:   1034 / 1474 loss=1.999, trans_loss=5.038, nll_loss=2.245, w2v_ctc_loss=0.649, task_loss=1.427, contrastive_loss=0.071, total=4099.86, n_correct=2646.46, ppl=4.74, accuracy=64.55, wps=11924.2, ups=1.45, wpb=8199.7, bsz=298.8, num_updates=30500, lr=8.09776e-05, gnorm=0.523, clip=0, loss_scale=64, train_wall=68, gb_free=11.1, wall=25817
2023-08-08 07:16:31 | INFO | train_inner | epoch 021:   1134 / 1474 loss=1.999, trans_loss=5.031, nll_loss=2.235, w2v_ctc_loss=0.65, task_loss=1.505, contrastive_loss=0.074, total=4120.75, n_correct=2665.42, ppl=4.71, accuracy=64.683, wps=11928.4, ups=1.45, wpb=8241.5, bsz=293.5, num_updates=30600, lr=8.08452e-05, gnorm=0.518, clip=0, loss_scale=64, train_wall=69, gb_free=15.7, wall=25886
2023-08-08 07:17:40 | INFO | train_inner | epoch 021:   1234 / 1474 loss=1.997, trans_loss=5.028, nll_loss=2.233, w2v_ctc_loss=0.644, task_loss=1.326, contrastive_loss=0.125, total=4154.73, n_correct=2691.38, ppl=4.7, accuracy=64.779, wps=12045.6, ups=1.45, wpb=8309.5, bsz=311.6, num_updates=30700, lr=8.07134e-05, gnorm=0.521, clip=0, loss_scale=64, train_wall=69, gb_free=12.4, wall=25955
2023-08-08 07:18:49 | INFO | train_inner | epoch 021:   1334 / 1474 loss=1.997, trans_loss=5.032, nll_loss=2.238, w2v_ctc_loss=0.649, task_loss=1.353, contrastive_loss=0.087, total=4147.17, n_correct=2685.08, ppl=4.72, accuracy=64.745, wps=12019.8, ups=1.45, wpb=8294.3, bsz=311.9, num_updates=30800, lr=8.05823e-05, gnorm=0.517, clip=0, loss_scale=64, train_wall=69, gb_free=16, wall=26024
2023-08-08 07:19:59 | INFO | train_inner | epoch 021:   1434 / 1474 loss=2.014, trans_loss=5.043, nll_loss=2.251, w2v_ctc_loss=0.665, task_loss=1.462, contrastive_loss=0.138, total=4133.93, n_correct=2659.32, ppl=4.76, accuracy=64.329, wps=11865.7, ups=1.44, wpb=8267.9, bsz=304.5, num_updates=30900, lr=8.04518e-05, gnorm=0.526, clip=0, loss_scale=64, train_wall=69, gb_free=15.4, wall=26094
2023-08-08 07:20:27 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:1')
2023-08-08 07:20:50 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 4.182 | trans_loss 5.558 | nll_loss 2.833 | w2v_ctc_loss 1.301 | task_loss 4.614 | contrastive_loss 0.259 | total 4003.4 | n_correct 2481.7 | ppl 7.13 | accuracy 61.99 | uer 17.19 | wer 19.049 | raw_wer 19.049 | bleu 19.63 | wps 2213 | wpb 4003.4 | bsz 141.8 | num_updates 30940 | best_bleu 20.05
2023-08-08 07:20:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30940 updates
2023-08-08 07:20:50 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_last.pt
2023-08-08 07:21:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_last.pt
2023-08-08 07:21:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_last.pt (epoch 21 @ 30940 updates, score 19.63) (writing took 13.2959611043334 seconds)
2023-08-08 07:21:03 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-08-08 07:21:03 | INFO | train | epoch 021 | loss 1.995 | trans_loss 5.024 | nll_loss 2.226 | w2v_ctc_loss 0.646 | task_loss 1.401 | contrastive_loss 0.106 | total 4137.02 | n_correct 2681.52 | ppl 4.68 | accuracy 64.818 | wps 11011 | ups 1.33 | wpb 8274 | bsz 305.1 | num_updates 30940 | lr 8.03998e-05 | gnorm 0.52 | clip 0 | loss_scale 64 | train_wall 1013 | gb_free 15.3 | wall 26158
2023-08-08 07:21:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 07:21:04 | INFO | fairseq.trainer | begin training epoch 22
2023-08-08 07:21:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 07:21:52 | INFO | train_inner | epoch 022:     60 / 1474 loss=1.987, trans_loss=5.01, nll_loss=2.208, w2v_ctc_loss=0.647, task_loss=1.422, contrastive_loss=0.053, total=4128.84, n_correct=2691.27, ppl=4.62, accuracy=65.182, wps=7317.9, ups=0.89, wpb=8257.7, bsz=297.7, num_updates=31000, lr=8.03219e-05, gnorm=0.521, clip=0, loss_scale=64, train_wall=69, gb_free=14, wall=26207
2023-08-08 07:23:01 | INFO | train_inner | epoch 022:    160 / 1474 loss=1.986, trans_loss=5, nll_loss=2.195, w2v_ctc_loss=0.641, task_loss=1.41, contrastive_loss=0.138, total=4123.35, n_correct=2688.9, ppl=4.58, accuracy=65.212, wps=11885.2, ups=1.44, wpb=8246.7, bsz=310.3, num_updates=31100, lr=8.01927e-05, gnorm=0.522, clip=0, loss_scale=64, train_wall=69, gb_free=14.5, wall=26276
2023-08-08 07:24:11 | INFO | train_inner | epoch 022:    260 / 1474 loss=1.972, trans_loss=4.994, nll_loss=2.188, w2v_ctc_loss=0.626, task_loss=1.233, contrastive_loss=0.078, total=4267.16, n_correct=2793.44, ppl=4.56, accuracy=65.464, wps=12200.6, ups=1.43, wpb=8534.3, bsz=329.9, num_updates=31200, lr=8.00641e-05, gnorm=0.511, clip=0, loss_scale=64, train_wall=69, gb_free=16.5, wall=26346
2023-08-08 07:24:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-08 07:25:22 | INFO | train_inner | epoch 022:    361 / 1474 loss=1.991, trans_loss=5.01, nll_loss=2.207, w2v_ctc_loss=0.645, task_loss=1.457, contrastive_loss=0.122, total=4154.17, n_correct=2705.48, ppl=4.62, accuracy=65.127, wps=11708.6, ups=1.41, wpb=8308.3, bsz=302.4, num_updates=31300, lr=7.99361e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=70, gb_free=15.1, wall=26417
2023-08-08 07:26:32 | INFO | train_inner | epoch 022:    461 / 1474 loss=1.994, trans_loss=5.016, nll_loss=2.214, w2v_ctc_loss=0.644, task_loss=1.468, contrastive_loss=0.119, total=4132.96, n_correct=2687.06, ppl=4.64, accuracy=65.015, wps=11921, ups=1.44, wpb=8265.9, bsz=297.5, num_updates=31400, lr=7.98087e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=69, gb_free=16.8, wall=26487
2023-08-08 07:27:41 | INFO | train_inner | epoch 022:    561 / 1474 loss=1.983, trans_loss=5.007, nll_loss=2.204, w2v_ctc_loss=0.642, task_loss=1.399, contrastive_loss=0.066, total=4158.17, n_correct=2713.08, ppl=4.61, accuracy=65.247, wps=11929.9, ups=1.43, wpb=8316.3, bsz=307.8, num_updates=31500, lr=7.96819e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=69, gb_free=16.6, wall=26556
2023-08-08 07:28:50 | INFO | train_inner | epoch 022:    661 / 1474 loss=1.98, trans_loss=5.003, nll_loss=2.199, w2v_ctc_loss=0.626, task_loss=1.336, contrastive_loss=0.149, total=4139.66, n_correct=2702.7, ppl=4.59, accuracy=65.288, wps=12126.9, ups=1.46, wpb=8279.3, bsz=311.1, num_updates=31600, lr=7.95557e-05, gnorm=0.515, clip=0, loss_scale=32, train_wall=68, gb_free=16, wall=26625
2023-08-08 07:29:59 | INFO | train_inner | epoch 022:    761 / 1474 loss=1.987, trans_loss=5.012, nll_loss=2.21, w2v_ctc_loss=0.643, task_loss=1.439, contrastive_loss=0.069, total=4167.89, n_correct=2716.34, ppl=4.63, accuracy=65.173, wps=11970, ups=1.44, wpb=8335.8, bsz=303.9, num_updates=31700, lr=7.94301e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=69, gb_free=12.6, wall=26694
2023-08-08 07:31:09 | INFO | train_inner | epoch 022:    861 / 1474 loss=1.993, trans_loss=5.023, nll_loss=2.225, w2v_ctc_loss=0.646, task_loss=1.515, contrastive_loss=0.054, total=4075.79, n_correct=2644.45, ppl=4.68, accuracy=64.882, wps=11769.1, ups=1.44, wpb=8151.6, bsz=289, num_updates=31800, lr=7.93052e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=69, gb_free=16.1, wall=26763
2023-08-08 07:32:18 | INFO | train_inner | epoch 022:    961 / 1474 loss=1.983, trans_loss=5.015, nll_loss=2.215, w2v_ctc_loss=0.635, task_loss=1.408, contrastive_loss=0.055, total=4134.72, n_correct=2688.9, ppl=4.64, accuracy=65.032, wps=11892.6, ups=1.44, wpb=8269.4, bsz=303.2, num_updates=31900, lr=7.91808e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=69, gb_free=14, wall=26833
2023-08-08 07:33:27 | INFO | train_inner | epoch 022:   1061 / 1474 loss=1.992, trans_loss=5.013, nll_loss=2.214, w2v_ctc_loss=0.635, task_loss=1.333, contrastive_loss=0.224, total=4160.57, n_correct=2707.64, ppl=4.64, accuracy=65.079, wps=12131.8, ups=1.46, wpb=8321.1, bsz=315.5, num_updates=32000, lr=7.90569e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=68, gb_free=17.2, wall=26902
2023-08-08 07:33:27 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 07:33:49 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 4.183 | trans_loss 5.553 | nll_loss 2.827 | w2v_ctc_loss 1.322 | task_loss 4.645 | contrastive_loss 0.246 | total 4003.4 | n_correct 2490.2 | ppl 7.1 | accuracy 62.202 | uer 17.209 | wer 19.153 | raw_wer 19.153 | bleu 20.22 | wps 2326.8 | wpb 4003.4 | bsz 141.8 | num_updates 32000 | best_bleu 20.22
2023-08-08 07:33:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32000 updates
2023-08-08 07:33:49 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_22_32000.pt
2023-08-08 07:33:52 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_22_32000.pt
2023-08-08 07:34:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_22_32000.pt (epoch 22 @ 32000 updates, score 20.22) (writing took 26.114734783768654 seconds)
2023-08-08 07:35:26 | INFO | train_inner | epoch 022:   1161 / 1474 loss=2.003, trans_loss=5.037, nll_loss=2.244, w2v_ctc_loss=0.652, task_loss=1.442, contrastive_loss=0.106, total=4099.59, n_correct=2652.13, ppl=4.74, accuracy=64.693, wps=6899.4, ups=0.84, wpb=8199.2, bsz=296.2, num_updates=32100, lr=7.89337e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=69, gb_free=14.8, wall=27020
2023-08-08 07:36:34 | INFO | train_inner | epoch 022:   1261 / 1474 loss=1.992, trans_loss=5.028, nll_loss=2.233, w2v_ctc_loss=0.643, task_loss=1.294, contrastive_loss=0.104, total=4182.05, n_correct=2712.02, ppl=4.7, accuracy=64.849, wps=12144.3, ups=1.45, wpb=8364.1, bsz=323, num_updates=32200, lr=7.8811e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=68, gb_free=15.4, wall=27089
2023-08-08 07:37:43 | INFO | train_inner | epoch 022:   1361 / 1474 loss=1.989, trans_loss=5.021, nll_loss=2.223, w2v_ctc_loss=0.634, task_loss=1.404, contrastive_loss=0.122, total=4062.31, n_correct=2638.85, ppl=4.67, accuracy=64.959, wps=11822.9, ups=1.46, wpb=8124.6, bsz=299, num_updates=32300, lr=7.86889e-05, gnorm=0.526, clip=0, loss_scale=32, train_wall=68, gb_free=14.9, wall=27158
2023-08-08 07:38:52 | INFO | train_inner | epoch 022:   1461 / 1474 loss=2.001, trans_loss=5.037, nll_loss=2.244, w2v_ctc_loss=0.654, task_loss=1.499, contrastive_loss=0.07, total=4081.88, n_correct=2638.56, ppl=4.74, accuracy=64.641, wps=11803, ups=1.45, wpb=8163.8, bsz=288.9, num_updates=32400, lr=7.85674e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=69, gb_free=17.1, wall=27227
2023-08-08 07:39:01 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 07:39:24 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 4.182 | trans_loss 5.552 | nll_loss 2.822 | w2v_ctc_loss 1.323 | task_loss 4.618 | contrastive_loss 0.244 | total 4003.4 | n_correct 2490.7 | ppl 7.07 | accuracy 62.215 | uer 17.217 | wer 18.996 | raw_wer 18.996 | bleu 20.07 | wps 2270.5 | wpb 4003.4 | bsz 141.8 | num_updates 32413 | best_bleu 20.22
2023-08-08 07:39:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32413 updates
2023-08-08 07:39:24 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.0700.pt
2023-08-08 07:39:28 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.0700.pt
2023-08-08 07:39:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.0700.pt (epoch 22 @ 32413 updates, score 20.07) (writing took 17.00858381576836 seconds)
2023-08-08 07:39:42 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-08-08 07:39:42 | INFO | train | epoch 022 | loss 1.989 | trans_loss 5.014 | nll_loss 2.214 | w2v_ctc_loss 0.641 | task_loss 1.402 | contrastive_loss 0.104 | total 4136.85 | n_correct 2691.82 | ppl 4.64 | accuracy 65.069 | wps 10898.3 | ups 1.32 | wpb 8273.7 | bsz 305.1 | num_updates 32413 | lr 7.85517e-05 | gnorm 0.522 | clip 0 | loss_scale 32 | train_wall 1013 | gb_free 11.4 | wall 27277
2023-08-08 07:39:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 07:39:42 | INFO | fairseq.trainer | begin training epoch 23
2023-08-08 07:39:42 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 07:40:51 | INFO | train_inner | epoch 023:     87 / 1474 loss=1.976, trans_loss=4.991, nll_loss=2.183, w2v_ctc_loss=0.639, task_loss=1.431, contrastive_loss=0.062, total=4096.09, n_correct=2685.38, ppl=4.54, accuracy=65.56, wps=6906.2, ups=0.84, wpb=8192.2, bsz=301.2, num_updates=32500, lr=7.84465e-05, gnorm=0.521, clip=0, loss_scale=32, train_wall=69, gb_free=16.3, wall=27346
2023-08-08 07:42:00 | INFO | train_inner | epoch 023:    187 / 1474 loss=1.974, trans_loss=4.988, nll_loss=2.179, w2v_ctc_loss=0.629, task_loss=1.489, contrastive_loss=0.06, total=4107.77, n_correct=2691.45, ppl=4.53, accuracy=65.521, wps=11858.6, ups=1.44, wpb=8215.5, bsz=293.4, num_updates=32600, lr=7.8326e-05, gnorm=0.519, clip=0, loss_scale=32, train_wall=69, gb_free=16.2, wall=27415
2023-08-08 07:43:10 | INFO | train_inner | epoch 023:    287 / 1474 loss=1.977, trans_loss=4.997, nll_loss=2.191, w2v_ctc_loss=0.621, task_loss=1.402, contrastive_loss=0.137, total=4153.12, n_correct=2717.14, ppl=4.57, accuracy=65.424, wps=11908.1, ups=1.43, wpb=8306.2, bsz=306.4, num_updates=32700, lr=7.82062e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=69, gb_free=16.9, wall=27485
2023-08-08 07:44:19 | INFO | train_inner | epoch 023:    387 / 1474 loss=1.975, trans_loss=4.996, nll_loss=2.19, w2v_ctc_loss=0.63, task_loss=1.452, contrastive_loss=0.051, total=4116.7, n_correct=2695.06, ppl=4.56, accuracy=65.467, wps=11887.2, ups=1.44, wpb=8233.4, bsz=294, num_updates=32800, lr=7.80869e-05, gnorm=0.52, clip=0, loss_scale=32, train_wall=69, gb_free=15.1, wall=27554
2023-08-08 07:45:28 | INFO | train_inner | epoch 023:    487 / 1474 loss=1.983, trans_loss=5.004, nll_loss=2.2, w2v_ctc_loss=0.636, task_loss=1.358, contrastive_loss=0.112, total=4157.6, n_correct=2711.92, ppl=4.6, accuracy=65.228, wps=12033.6, ups=1.45, wpb=8315.2, bsz=313.5, num_updates=32900, lr=7.79681e-05, gnorm=0.524, clip=0, loss_scale=32, train_wall=69, gb_free=17.2, wall=27623
2023-08-08 07:46:37 | INFO | train_inner | epoch 023:    587 / 1474 loss=1.969, trans_loss=4.991, nll_loss=2.185, w2v_ctc_loss=0.627, task_loss=1.324, contrastive_loss=0.057, total=4173.42, n_correct=2737.7, ppl=4.55, accuracy=65.598, wps=12163.1, ups=1.46, wpb=8346.8, bsz=316, num_updates=33000, lr=7.78499e-05, gnorm=0.521, clip=0, loss_scale=32, train_wall=68, gb_free=12.4, wall=27692
2023-08-08 07:47:46 | INFO | train_inner | epoch 023:    687 / 1474 loss=1.978, trans_loss=4.998, nll_loss=2.193, w2v_ctc_loss=0.63, task_loss=1.401, contrastive_loss=0.097, total=4137.82, n_correct=2709.7, ppl=4.57, accuracy=65.486, wps=12017.9, ups=1.45, wpb=8275.6, bsz=302.5, num_updates=33100, lr=7.77322e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=68, gb_free=17, wall=27761
2023-08-08 07:48:55 | INFO | train_inner | epoch 023:    787 / 1474 loss=1.982, trans_loss=5.006, nll_loss=2.203, w2v_ctc_loss=0.635, task_loss=1.407, contrastive_loss=0.077, total=4150.99, n_correct=2710.28, ppl=4.6, accuracy=65.292, wps=11991.1, ups=1.44, wpb=8302, bsz=305.4, num_updates=33200, lr=7.76151e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=69, gb_free=16.2, wall=27830
2023-08-08 07:50:04 | INFO | train_inner | epoch 023:    887 / 1474 loss=1.982, trans_loss=5.003, nll_loss=2.2, w2v_ctc_loss=0.633, task_loss=1.276, contrastive_loss=0.157, total=4181.99, n_correct=2731.46, ppl=4.6, accuracy=65.315, wps=12187.2, ups=1.46, wpb=8364, bsz=324.7, num_updates=33300, lr=7.74984e-05, gnorm=0.518, clip=0, loss_scale=64, train_wall=68, gb_free=16.3, wall=27899
2023-08-08 07:50:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-08 07:51:14 | INFO | train_inner | epoch 023:    988 / 1474 loss=1.984, trans_loss=5.007, nll_loss=2.205, w2v_ctc_loss=0.628, task_loss=1.45, contrastive_loss=0.149, total=4139.5, n_correct=2701.35, ppl=4.61, accuracy=65.258, wps=11742.3, ups=1.42, wpb=8279, bsz=298.9, num_updates=33400, lr=7.73823e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=70, gb_free=17.6, wall=27969
2023-08-08 07:52:24 | INFO | train_inner | epoch 023:   1088 / 1474 loss=1.989, trans_loss=5.015, nll_loss=2.215, w2v_ctc_loss=0.646, task_loss=1.484, contrastive_loss=0.063, total=4092.37, n_correct=2662.19, ppl=4.64, accuracy=65.053, wps=11742.7, ups=1.43, wpb=8184.7, bsz=290.8, num_updates=33500, lr=7.72667e-05, gnorm=0.52, clip=0, loss_scale=32, train_wall=69, gb_free=17.3, wall=28039
2023-08-08 07:53:34 | INFO | train_inner | epoch 023:   1188 / 1474 loss=1.982, trans_loss=5.016, nll_loss=2.216, w2v_ctc_loss=0.635, task_loss=1.392, contrastive_loss=0.056, total=4164.9, n_correct=2709.41, ppl=4.65, accuracy=65.053, wps=11908.7, ups=1.43, wpb=8329.8, bsz=309.2, num_updates=33600, lr=7.71517e-05, gnorm=0.517, clip=0, loss_scale=32, train_wall=69, gb_free=13.5, wall=28109
2023-08-08 07:54:43 | INFO | train_inner | epoch 023:   1288 / 1474 loss=1.976, trans_loss=5.01, nll_loss=2.21, w2v_ctc_loss=0.626, task_loss=1.36, contrastive_loss=0.068, total=4136.96, n_correct=2699.82, ppl=4.63, accuracy=65.261, wps=11891, ups=1.44, wpb=8273.9, bsz=309.8, num_updates=33700, lr=7.70371e-05, gnorm=0.522, clip=0, loss_scale=32, train_wall=69, gb_free=16.5, wall=28178
2023-08-08 07:55:53 | INFO | train_inner | epoch 023:   1388 / 1474 loss=1.994, trans_loss=5.03, nll_loss=2.235, w2v_ctc_loss=0.638, task_loss=1.412, contrastive_loss=0.126, total=4142.84, n_correct=2685.9, ppl=4.71, accuracy=64.832, wps=11924.4, ups=1.44, wpb=8285.7, bsz=304.8, num_updates=33800, lr=7.69231e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=69, gb_free=16, wall=28248
2023-08-08 07:56:52 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 07:57:15 | INFO | dev_st | epoch 023 | valid on 'dev_st' subset | loss 4.202 | trans_loss 5.549 | nll_loss 2.82 | w2v_ctc_loss 1.395 | task_loss 4.678 | contrastive_loss 0.247 | total 4003.4 | n_correct 2493.2 | ppl 7.06 | accuracy 62.277 | uer 17.121 | wer 18.925 | raw_wer 18.925 | bleu 20.13 | wps 2247.5 | wpb 4003.4 | bsz 141.8 | num_updates 33886 | best_bleu 20.22
2023-08-08 07:57:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 33886 updates
2023-08-08 07:57:15 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.1305.pt
2023-08-08 07:57:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.1305.pt
2023-08-08 07:57:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.1305.pt (epoch 23 @ 33886 updates, score 20.13) (writing took 18.145683398470283 seconds)
2023-08-08 07:57:34 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-08-08 07:57:34 | INFO | train | epoch 023 | loss 1.981 | trans_loss 5.005 | nll_loss 2.202 | w2v_ctc_loss 0.633 | task_loss 1.402 | contrastive_loss 0.099 | total 4137.06 | n_correct 2700.51 | ppl 4.6 | accuracy 65.276 | wps 11368.5 | ups 1.37 | wpb 8274.1 | bsz 305 | num_updates 33886 | lr 7.68254e-05 | gnorm 0.521 | clip 0 | loss_scale 32 | train_wall 1015 | gb_free 13.5 | wall 28349
2023-08-08 07:57:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 07:57:34 | INFO | fairseq.trainer | begin training epoch 24
2023-08-08 07:57:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 07:57:52 | INFO | train_inner | epoch 024:     14 / 1474 loss=1.997, trans_loss=5.023, nll_loss=2.227, w2v_ctc_loss=0.634, task_loss=1.414, contrastive_loss=0.205, total=4084.21, n_correct=2649.61, ppl=4.68, accuracy=64.874, wps=6889.4, ups=0.84, wpb=8168.4, bsz=303.7, num_updates=33900, lr=7.68095e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=69, gb_free=15.2, wall=28366
2023-08-08 07:59:01 | INFO | train_inner | epoch 024:    114 / 1474 loss=1.974, trans_loss=4.978, nll_loss=2.167, w2v_ctc_loss=0.621, task_loss=1.294, contrastive_loss=0.219, total=4168.61, n_correct=2740.55, ppl=4.49, accuracy=65.743, wps=12017.3, ups=1.44, wpb=8337.2, bsz=324.6, num_updates=34000, lr=7.66965e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=69, gb_free=11.2, wall=28436
2023-08-08 07:59:01 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 07:59:26 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 4.197 | trans_loss 5.558 | nll_loss 2.83 | w2v_ctc_loss 1.356 | task_loss 4.642 | contrastive_loss 0.252 | total 4003.4 | n_correct 2486.6 | ppl 7.11 | accuracy 62.112 | uer 17.11 | wer 18.952 | raw_wer 18.952 | bleu 19.95 | wps 1994.8 | wpb 4003.4 | bsz 141.8 | num_updates 34000 | best_bleu 20.22
2023-08-08 07:59:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 34000 updates
2023-08-08 07:59:26 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_24_34000.pt
2023-08-08 07:59:30 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_24_34000.pt
2023-08-08 08:00:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_24_34000.pt (epoch 24 @ 34000 updates, score 19.95) (writing took 41.304311495274305 seconds)
2023-08-08 08:01:18 | INFO | train_inner | epoch 024:    214 / 1474 loss=1.972, trans_loss=4.979, nll_loss=2.17, w2v_ctc_loss=0.61, task_loss=1.219, contrastive_loss=0.274, total=4252.53, n_correct=2797.03, ppl=4.5, accuracy=65.773, wps=6207.4, ups=0.73, wpb=8505.1, bsz=341.3, num_updates=34100, lr=7.6584e-05, gnorm=0.511, clip=0, loss_scale=32, train_wall=70, gb_free=16.6, wall=28573
2023-08-08 08:02:27 | INFO | train_inner | epoch 024:    314 / 1474 loss=1.965, trans_loss=4.985, nll_loss=2.175, w2v_ctc_loss=0.624, task_loss=1.364, contrastive_loss=0.053, total=4138.44, n_correct=2722.22, ppl=4.52, accuracy=65.779, wps=12039.8, ups=1.45, wpb=8276.9, bsz=307.1, num_updates=34200, lr=7.64719e-05, gnorm=0.521, clip=0, loss_scale=32, train_wall=68, gb_free=15.4, wall=28642
2023-08-08 08:03:37 | INFO | train_inner | epoch 024:    414 / 1474 loss=1.989, trans_loss=4.992, nll_loss=2.185, w2v_ctc_loss=0.635, task_loss=1.48, contrastive_loss=0.199, total=4153.83, n_correct=2719.54, ppl=4.55, accuracy=65.471, wps=11850.6, ups=1.43, wpb=8307.7, bsz=298.4, num_updates=34300, lr=7.63604e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=70, gb_free=16, wall=28712
2023-08-08 08:04:46 | INFO | train_inner | epoch 024:    514 / 1474 loss=1.974, trans_loss=4.99, nll_loss=2.182, w2v_ctc_loss=0.627, task_loss=1.427, contrastive_loss=0.121, total=4141.88, n_correct=2716.8, ppl=4.54, accuracy=65.593, wps=11931.9, ups=1.44, wpb=8283.8, bsz=302.6, num_updates=34400, lr=7.62493e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=69, gb_free=15, wall=28781
2023-08-08 08:05:56 | INFO | train_inner | epoch 024:    614 / 1474 loss=1.969, trans_loss=4.991, nll_loss=2.184, w2v_ctc_loss=0.619, task_loss=1.406, contrastive_loss=0.087, total=4162.06, n_correct=2732.91, ppl=4.54, accuracy=65.662, wps=11996.7, ups=1.44, wpb=8324.1, bsz=308.1, num_updates=34500, lr=7.61387e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=69, gb_free=16.7, wall=28850
2023-08-08 08:07:05 | INFO | train_inner | epoch 024:    714 / 1474 loss=1.978, trans_loss=5.002, nll_loss=2.197, w2v_ctc_loss=0.628, task_loss=1.448, contrastive_loss=0.095, total=4097.35, n_correct=2680.04, ppl=4.59, accuracy=65.409, wps=11777.3, ups=1.44, wpb=8194.7, bsz=293.8, num_updates=34600, lr=7.60286e-05, gnorm=0.524, clip=0, loss_scale=32, train_wall=69, gb_free=17.2, wall=28920
2023-08-08 08:08:14 | INFO | train_inner | epoch 024:    814 / 1474 loss=1.975, trans_loss=5.004, nll_loss=2.202, w2v_ctc_loss=0.627, task_loss=1.395, contrastive_loss=0.075, total=4124.25, n_correct=2697.22, ppl=4.6, accuracy=65.399, wps=11962.3, ups=1.45, wpb=8248.5, bsz=308.2, num_updates=34700, lr=7.5919e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=68, gb_free=15.9, wall=28989
2023-08-08 08:09:23 | INFO | train_inner | epoch 024:    914 / 1474 loss=1.983, trans_loss=5.007, nll_loss=2.204, w2v_ctc_loss=0.636, task_loss=1.557, contrastive_loss=0.048, total=4041.44, n_correct=2629.9, ppl=4.61, accuracy=65.073, wps=11740, ups=1.45, wpb=8082.9, bsz=280.5, num_updates=34800, lr=7.58098e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=68, gb_free=16, wall=29058
2023-08-08 08:10:32 | INFO | train_inner | epoch 024:   1014 / 1474 loss=1.976, trans_loss=5.005, nll_loss=2.202, w2v_ctc_loss=0.628, task_loss=1.461, contrastive_loss=0.053, total=4128.8, n_correct=2700.6, ppl=4.6, accuracy=65.409, wps=11925.3, ups=1.44, wpb=8257.6, bsz=296.5, num_updates=34900, lr=7.57011e-05, gnorm=0.52, clip=0, loss_scale=32, train_wall=69, gb_free=14.8, wall=29127
2023-08-08 08:11:41 | INFO | train_inner | epoch 024:   1114 / 1474 loss=1.976, trans_loss=4.995, nll_loss=2.19, w2v_ctc_loss=0.633, task_loss=1.351, contrastive_loss=0.097, total=4130.49, n_correct=2707.23, ppl=4.56, accuracy=65.543, wps=11998.3, ups=1.45, wpb=8261, bsz=308.9, num_updates=35000, lr=7.55929e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=68, gb_free=15.9, wall=29196
Mixup rate:0.5, token after shrink shape:torch.Size([8, 107]), X shape:torch.Size([8, 107, 512])
CTC Tokens:tensor([ 29,  24,  29,  10, 388], device='cuda:0'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:0'), New Tokens:tensor([ 29,  24,  29,  10, 388], device='cuda:0')
Mixup Sent Mask:tensor([[2],
        [7]], device='cuda:0'), 2,  Mixup Mask:tensor([ True,  True,  True,  True, False], device='cuda:0'), 
                    Org X:tensor([[ 0.2788, -0.8979,  0.5205,  ...,  0.5488, -0.9053,  0.5845],
        [-1.5586,  0.0732,  2.0762,  ...,  0.1798, -0.9761,  0.3535],
        [-0.4260,  0.5811,  0.7417,  ...,  0.2671, -1.0020, -0.7476],
        [-0.4941,  0.1754,  1.6455,  ..., -0.1243,  1.1484, -1.8965],
        [ 0.3877, -0.3071,  1.3945,  ..., -0.3337,  2.0898, -2.5449]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-1.0928, -0.0158, -0.2964,  ..., -0.1171, -2.1309,  3.4355],
        [-0.6265, -0.3994,  0.8672,  ..., -4.6836,  2.1582, -1.6367],
        [-1.0928, -0.0158, -0.2964,  ..., -0.1171, -2.1309,  3.4355],
        [-0.2168, -0.0932, -0.2727,  ..., -0.6514, -2.9395, -2.3691],
        [ 0.3877, -0.3071,  1.3945,  ..., -0.3337,  2.0898, -2.5449]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-1.0928, -0.0158, -0.2964,  ..., -0.1171, -2.1309,  3.4355],
        [-0.6265, -0.3994,  0.8672,  ..., -4.6836,  2.1582, -1.6367],
        [-1.0928, -0.0158, -0.2964,  ..., -0.1171, -2.1309,  3.4355],
        [-0.2168, -0.0932, -0.2727,  ..., -0.6514, -2.9395, -2.3691],
        [-1.1377, -1.1504,  0.3374,  ..., -0.3044,  1.8105, -3.1719]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:0')
2023-08-08 08:12:51 | INFO | train_inner | epoch 024:   1214 / 1474 loss=1.975, trans_loss=5.003, nll_loss=2.201, w2v_ctc_loss=0.624, task_loss=1.387, contrastive_loss=0.087, total=4157.47, n_correct=2719.88, ppl=4.6, accuracy=65.422, wps=11959.3, ups=1.44, wpb=8314.9, bsz=311.2, num_updates=35100, lr=7.54851e-05, gnorm=0.519, clip=0, loss_scale=32, train_wall=69, gb_free=12.5, wall=29265
2023-08-08 08:14:00 | INFO | train_inner | epoch 024:   1314 / 1474 loss=1.986, trans_loss=5.015, nll_loss=2.215, w2v_ctc_loss=0.641, task_loss=1.485, contrastive_loss=0.058, total=4107.23, n_correct=2671.86, ppl=4.64, accuracy=65.053, wps=11834.6, ups=1.44, wpb=8214.5, bsz=294.3, num_updates=35200, lr=7.53778e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=69, gb_free=17.4, wall=29335
2023-08-08 08:15:09 | INFO | train_inner | epoch 024:   1414 / 1474 loss=1.984, trans_loss=5.015, nll_loss=2.216, w2v_ctc_loss=0.639, task_loss=1.465, contrastive_loss=0.056, total=4094.39, n_correct=2666.82, ppl=4.65, accuracy=65.134, wps=11829.3, ups=1.44, wpb=8188.8, bsz=292.9, num_updates=35300, lr=7.5271e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=69, gb_free=16, wall=29404
2023-08-08 08:15:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([8, 92]), X shape:torch.Size([8, 92, 512])
CTC Tokens:tensor([8, 0, 4, 4, 4], device='cuda:7'), Shrink Mask:tensor([ True,  True,  True, False, False], device='cuda:7'), New Tokens:tensor([  8,   0,   4, 103,  25], device='cuda:7')
Mixup Sent Mask:tensor([[2],
        [7]], device='cuda:7'), 2,  Mixup Mask:tensor([False, False, False,  True, False], device='cuda:7'), 
                    Org X:tensor([[ 0.3088, -0.3281,  0.3213,  ...,  0.9756,  0.6206, -0.0480],
        [-1.1777,  0.1094,  2.1621,  ...,  0.3347,  1.2002,  0.8281],
        [-0.1212,  0.7217,  1.7490,  ...,  0.2311,  0.7041, -0.0775],
        [-0.2578, -0.7520, -2.4355,  ...,  0.2976,  1.8496,  0.0595],
        [-1.4717, -0.8130, -3.3359,  ..., -0.2722,  3.3750, -0.0480]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.3088, -0.3281,  0.3213,  ...,  0.9756,  0.6206, -0.0480],
        [-1.1777,  0.1094,  2.1621,  ...,  0.3347,  1.2002,  0.8281],
        [-0.1212,  0.7217,  1.7490,  ...,  0.2311,  0.7041, -0.0775],
        [ 0.4878, -0.7271, -0.0892,  ...,  1.5332, -0.8335, -0.9165],
        [-1.4717, -0.8130, -3.3359,  ..., -0.2722,  3.3750, -0.0480]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5308, -0.0484, -0.6299,  ...,  1.2559, -1.7129, -1.6104],
        [-0.4009, -0.8687,  0.2686,  ..., -2.6855, -1.6582, -1.8564],
        [-0.1589, -0.4277, -0.2415,  ..., -1.4082, -1.1035, -2.1680],
        [ 0.4878, -0.7271, -0.0892,  ...,  1.5332, -0.8335, -0.9165],
        [ 0.2639, -0.5273, -0.1326,  ..., -1.0518,  0.3650, -2.3145]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([24, 45]), X shape:torch.Size([24, 45, 512])
CTC Tokens:tensor([ 0, 17, 17, 11,  6], device='cuda:2'), Shrink Mask:tensor([ True,  True, False,  True,  True], device='cuda:2'), New Tokens:tensor([ 0, 17, 11,  6,  7], device='cuda:2')
Mixup Sent Mask:tensor([[ 2],
        [ 7],
        [10],
        [12],
        [17]], device='cuda:2'), 2,  Mixup Mask:tensor([ True, False,  True,  True, False], device='cuda:2'), 
                    Org X:tensor([[-0.2949, -0.1270,  0.1548,  ..., -0.1184,  0.4966, -0.3374],
        [-0.8892,  0.1364, -0.8521,  ..., -0.0931,  0.4099, -0.5771],
        [ 0.2805, -0.7788,  0.5435,  ..., -0.9941,  0.3657, -1.1143],
        [ 1.3408, -0.2050, -0.2888,  ..., -1.1836, -1.0020,  0.3728],
        [-0.2024,  0.4050, -2.0332,  ...,  0.1125, -1.1533, -0.0757]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.4009, -0.8687,  0.2686,  ..., -2.6855, -1.6582, -1.8564],
        [-0.8892,  0.1364, -0.8521,  ..., -0.0931,  0.4099, -0.5771],
        [-0.5205, -0.2576, -0.3877,  ..., -2.3789, -0.2710, -4.2656],
        [ 0.0068, -0.4341, -0.2147,  ..., -3.0430, -3.3203,  1.0430],
        [-0.2024,  0.4050, -2.0332,  ...,  0.1125, -1.1533, -0.0757]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4009, -0.8687,  0.2686,  ..., -2.6855, -1.6582, -1.8564],
        [-0.2405, -0.0767, -0.4751,  ..., -0.0847, -1.3691, -2.3086],
        [-0.5205, -0.2576, -0.3877,  ..., -2.3789, -0.2710, -4.2656],
        [ 0.0068, -0.4341, -0.2147,  ..., -3.0430, -3.3203,  1.0430],
        [-0.2218, -0.3586, -0.2749,  ..., -0.3098, -1.6367, -1.3721]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 97]), X shape:torch.Size([8, 97, 512])
CTC Tokens:tensor([  67,   67,   67, 6481,  133], device='cuda:6'), Shrink Mask:tensor([ True, False, False,  True,  True], device='cuda:6'), New Tokens:tensor([  67, 6481,  133,    0,    4], device='cuda:6')
Mixup Sent Mask:tensor([[2],
        [7]], device='cuda:6'), 2,  Mixup Mask:tensor([ True, False,  True, False,  True], device='cuda:6'), 
                    Org X:tensor([[ 0.4646, -0.1523,  0.5259,  ..., -0.3289,  0.1399, -2.1250],
        [ 0.2257, -0.1638,  0.3386,  ..., -0.0327,  0.8491,  1.2920],
        [ 0.3418,  0.1573,  0.5620,  ...,  0.4045, -0.7114,  0.9233],
        [ 0.3218,  0.2964,  0.6211,  ...,  0.2316, -0.0312, -0.3184],
        [ 0.3101,  0.2352,  0.1919,  ..., -0.5273,  1.2168, -0.3909]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.1267, -0.2054, -0.9438,  ...,  1.3047, -2.0195, -3.6094],
        [ 0.2257, -0.1638,  0.3386,  ..., -0.0327,  0.8491,  1.2920],
        [-0.8135,  0.1216, -0.8384,  ...,  0.7041, -3.3887, -1.7568],
        [ 0.3218,  0.2964,  0.6211,  ...,  0.2316, -0.0312, -0.3184],
        [-0.1589, -0.4277, -0.2415,  ..., -1.4082, -1.1035, -2.1680]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[ 0.1267, -0.2054, -0.9438,  ...,  1.3047, -2.0195, -3.6094],
        [ 0.9756,  0.8164, -0.2112,  ...,  0.4016,  2.5605, -0.5972],
        [-0.8135,  0.1216, -0.8384,  ...,  0.7041, -3.3887, -1.7568],
        [-0.4009, -0.8687,  0.2686,  ..., -2.6855, -1.6582, -1.8564],
        [-0.1589, -0.4277, -0.2415,  ..., -1.4082, -1.1035, -2.1680]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([40, 40]), X shape:torch.Size([40, 40, 512])
CTC Tokens:tensor([17, 17, 11,  6, 77], device='cuda:5'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:5'), New Tokens:tensor([17, 11,  6, 77, 12], device='cuda:5')
Mixup Sent Mask:tensor([[ 2],
        [ 7],
        [10],
        [12],
        [17],
        [24],
        [26],
        [27],
        [30],
        [31],
        [32],
        [37]], device='cuda:5'), 2,  Mixup Mask:tensor([False, False,  True,  True,  True], device='cuda:5'), 
                    Org X:tensor([[-0.5103, -0.3784, -0.2439,  ...,  0.7144,  0.4333,  0.2264],
        [ 0.7134, -0.9058, -0.8682,  ..., -0.5801, -0.3357, -0.0214],
        [ 1.2246,  0.1062,  1.1279,  ..., -2.6250, -1.5020,  0.0847],
        [-0.4780,  0.5488,  1.5283,  ...,  0.1112, -1.3389, -0.5308],
        [-0.2646,  0.4907,  0.0270,  ...,  0.2507, -1.4678, -0.8047]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.5103, -0.3784, -0.2439,  ...,  0.7144,  0.4333,  0.2264],
        [ 0.7134, -0.9058, -0.8682,  ..., -0.5801, -0.3357, -0.0214],
        [ 0.0068, -0.4341, -0.2147,  ..., -3.0430, -3.3203,  1.0430],
        [-0.6392,  0.3076,  0.4458,  ...,  0.8755, -1.8242,  1.0518],
        [-0.2566,  0.0119,  0.0434,  ...,  0.4695, -3.2031, -0.5884]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.2405, -0.0767, -0.4751,  ..., -0.0847, -1.3691, -2.3086],
        [-0.5205, -0.2576, -0.3877,  ..., -2.3789, -0.2710, -4.2656],
        [ 0.0068, -0.4341, -0.2147,  ..., -3.0430, -3.3203,  1.0430],
        [-0.6392,  0.3076,  0.4458,  ...,  0.8755, -1.8242,  1.0518],
        [-0.2566,  0.0119,  0.0434,  ...,  0.4695, -3.2031, -0.5884]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([40, 40]), X shape:torch.Size([40, 40, 512])
CTC Tokens:tensor([  0,   0, 125, 125, 125], device='cuda:3'), Shrink Mask:tensor([ True, False,  True, False, False], device='cuda:3'), New Tokens:tensor([  0, 125,   0,  24,   0], device='cuda:3')
Mixup Sent Mask:tensor([[ 2],
        [ 7],
        [10],
        [12],
        [17],
        [24],
        [26],
        [27],
        [30],
        [31],
        [32],
        [37]], device='cuda:3'), 2,  Mixup Mask:tensor([False, False,  True,  True,  True], device='cuda:3'), 
                    Org X:tensor([[-0.2798, -0.3047,  0.9678,  ..., -0.3923, -0.1533,  0.8003],
        [ 0.1322,  0.4309,  1.1904,  ...,  0.1440, -1.1699,  0.0651],
        [-0.3704,  0.5190,  2.3242,  ..., -0.8359,  1.2363, -0.1744],
        [-2.3848,  0.5752,  0.3811,  ...,  0.1855,  1.4590,  0.3608],
        [-1.9004,  0.5835,  1.8926,  ...,  0.2803,  2.3145, -0.0391]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.2798, -0.3047,  0.9678,  ..., -0.3923, -0.1533,  0.8003],
        [ 0.1322,  0.4309,  1.1904,  ...,  0.1440, -1.1699,  0.0651],
        [-0.4009, -0.8687,  0.2686,  ..., -2.6855, -1.6582, -1.8564],
        [-0.6265, -0.3994,  0.8672,  ..., -4.6836,  2.1582, -1.6367],
        [-0.4009, -0.8687,  0.2686,  ..., -2.6855, -1.6582, -1.8564]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4009, -0.8687,  0.2686,  ..., -2.6855, -1.6582, -1.8564],
        [ 0.0572,  0.4600, -0.1052,  ...,  0.7207, -4.8711, -1.9199],
        [-0.4009, -0.8687,  0.2686,  ..., -2.6855, -1.6582, -1.8564],
        [-0.6265, -0.3994,  0.8672,  ..., -4.6836,  2.1582, -1.6367],
        [-0.4009, -0.8687,  0.2686,  ..., -2.6855, -1.6582, -1.8564]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 107]), X shape:torch.Size([8, 107, 512])
CTC Tokens:tensor([8, 8, 0, 0, 0], device='cuda:4'), Shrink Mask:tensor([ True, False,  True, False, False], device='cuda:4'), New Tokens:tensor([   8,    0, 4005,    0,    4], device='cuda:4')
Mixup Sent Mask:tensor([[2],
        [7]], device='cuda:4'), 2,  Mixup Mask:tensor([ True,  True,  True,  True, False], device='cuda:4'), 
                    Org X:tensor([[ 0.4951, -0.6016,  0.0774,  ...,  0.3696, -0.1692, -0.0654],
        [-0.3774,  0.2400,  2.3594,  ..., -0.2321,  1.1846, -0.3330],
        [-0.6860,  0.1583,  0.0749,  ..., -1.9268,  0.0276,  0.4934],
        [-0.3647,  0.3589,  0.4302,  ..., -2.2129,  0.4214, -0.7456],
        [ 0.8706,  1.3086,  2.1641,  ..., -0.2252,  0.5815, -0.8916]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.5308, -0.0484, -0.6299,  ...,  1.2559, -1.7129, -1.6104],
        [-0.4009, -0.8687,  0.2686,  ..., -2.6855, -1.6582, -1.8564],
        [-0.4036,  1.2529,  0.4600,  ..., -0.1350, -3.4004,  0.1382],
        [-0.4009, -0.8687,  0.2686,  ..., -2.6855, -1.6582, -1.8564],
        [ 0.8706,  1.3086,  2.1641,  ..., -0.2252,  0.5815, -0.8916]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5308, -0.0484, -0.6299,  ...,  1.2559, -1.7129, -1.6104],
        [-0.4009, -0.8687,  0.2686,  ..., -2.6855, -1.6582, -1.8564],
        [-0.4036,  1.2529,  0.4600,  ..., -0.1350, -3.4004,  0.1382],
        [-0.4009, -0.8687,  0.2686,  ..., -2.6855, -1.6582, -1.8564],
        [-0.1589, -0.4277, -0.2415,  ..., -1.4082, -1.1035, -2.1680]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([32, 42]), X shape:torch.Size([32, 42, 512])
CTC Tokens:tensor([ 19,   8,   0, 116,   0], device='cuda:1'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:1'), New Tokens:tensor([ 19,   8,   0, 116,   0], device='cuda:1')
Mixup Sent Mask:tensor([[ 2],
        [ 7],
        [10],
        [12],
        [17],
        [24],
        [26],
        [27],
        [30],
        [31]], device='cuda:1'), 2,  Mixup Mask:tensor([ True,  True, False,  True, False], device='cuda:1'), 
                    Org X:tensor([[ 0.8477, -0.4812,  0.0314,  ...,  0.9717,  0.2383, -0.9209],
        [ 0.7695, -1.1602,  1.3604,  ...,  0.2489,  0.0729,  0.0916],
        [ 1.5381, -0.7593,  2.0977,  ...,  0.2172,  0.2703,  1.4795],
        [ 1.1240, -0.4922,  2.5488,  ..., -0.0545,  0.1665,  0.5879],
        [ 1.0508,  0.5728,  3.0723,  ...,  0.0652, -0.1956, -0.6313]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.2438, -0.3325, -0.1236,  ..., -0.9688,  0.3196, -1.4707],
        [-0.5308, -0.0484, -0.6299,  ...,  1.2559, -1.7129, -1.6104],
        [ 1.5381, -0.7593,  2.0977,  ...,  0.2172,  0.2703,  1.4795],
        [ 0.2900, -0.4558,  0.3208,  ...,  2.1230, -1.5068, -0.7720],
        [ 1.0508,  0.5728,  3.0723,  ...,  0.0652, -0.1956, -0.6313]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[ 0.2438, -0.3325, -0.1236,  ..., -0.9688,  0.3196, -1.4707],
        [-0.5308, -0.0484, -0.6299,  ...,  1.2559, -1.7129, -1.6104],
        [-0.4009, -0.8687,  0.2686,  ..., -2.6855, -1.6582, -1.8564],
        [ 0.2900, -0.4558,  0.3208,  ...,  2.1230, -1.5068, -0.7720],
        [-0.4009, -0.8687,  0.2686,  ..., -2.6855, -1.6582, -1.8564]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:1')
2023-08-08 08:16:14 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 4.186 | trans_loss 5.552 | nll_loss 2.822 | w2v_ctc_loss 1.333 | task_loss 4.664 | contrastive_loss 0.255 | total 4003.4 | n_correct 2487.9 | ppl 7.07 | accuracy 62.145 | uer 16.954 | wer 18.761 | raw_wer 18.761 | bleu 19.79 | wps 2254.1 | wpb 4003.4 | bsz 141.8 | num_updates 35360 | best_bleu 20.22
2023-08-08 08:16:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 35360 updates
2023-08-08 08:16:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_last.pt
2023-08-08 08:16:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_last.pt
2023-08-08 08:16:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_last.pt (epoch 24 @ 35360 updates, score 19.79) (writing took 12.993868624791503 seconds)
2023-08-08 08:16:27 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-08-08 08:16:27 | INFO | train | epoch 024 | loss 1.976 | trans_loss 4.997 | nll_loss 2.192 | w2v_ctc_loss 0.628 | task_loss 1.4 | contrastive_loss 0.109 | total 4138.65 | n_correct 2709.73 | ppl 4.57 | accuracy 65.474 | wps 10771.2 | ups 1.3 | wpb 8277.3 | bsz 305.7 | num_updates 35360 | lr 7.52071e-05 | gnorm 0.523 | clip 0 | loss_scale 32 | train_wall 1015 | gb_free 16 | wall 29481
2023-08-08 08:16:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 08:16:27 | INFO | fairseq.trainer | begin training epoch 25
2023-08-08 08:16:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 08:17:02 | INFO | train_inner | epoch 025:     40 / 1474 loss=1.965, trans_loss=4.988, nll_loss=2.18, w2v_ctc_loss=0.623, task_loss=1.349, contrastive_loss=0.064, total=4165.57, n_correct=2741.48, ppl=4.53, accuracy=65.813, wps=7412.3, ups=0.89, wpb=8331.1, bsz=311.2, num_updates=35400, lr=7.51646e-05, gnorm=0.514, clip=0, loss_scale=32, train_wall=68, gb_free=12.9, wall=29517
2023-08-08 08:18:11 | INFO | train_inner | epoch 025:    140 / 1474 loss=1.955, trans_loss=4.967, nll_loss=2.153, w2v_ctc_loss=0.613, task_loss=1.359, contrastive_loss=0.062, total=4135.43, n_correct=2735.75, ppl=4.45, accuracy=66.154, wps=11916.8, ups=1.44, wpb=8270.9, bsz=308.9, num_updates=35500, lr=7.50587e-05, gnorm=0.519, clip=0, loss_scale=64, train_wall=69, gb_free=17.6, wall=29586
2023-08-08 08:19:21 | INFO | train_inner | epoch 025:    240 / 1474 loss=1.958, trans_loss=4.971, nll_loss=2.158, w2v_ctc_loss=0.618, task_loss=1.435, contrastive_loss=0.066, total=4116.13, n_correct=2718.56, ppl=4.46, accuracy=66.047, wps=11824.5, ups=1.44, wpb=8232.3, bsz=303, num_updates=35600, lr=7.49532e-05, gnorm=0.522, clip=0, loss_scale=64, train_wall=69, gb_free=17.2, wall=29656
2023-08-08 08:20:31 | INFO | train_inner | epoch 025:    340 / 1474 loss=1.97, trans_loss=4.981, nll_loss=2.169, w2v_ctc_loss=0.622, task_loss=1.495, contrastive_loss=0.094, total=4141.49, n_correct=2718.4, ppl=4.5, accuracy=65.638, wps=11859.3, ups=1.43, wpb=8283, bsz=294.2, num_updates=35700, lr=7.48481e-05, gnorm=0.528, clip=0, loss_scale=64, train_wall=69, gb_free=15, wall=29725
2023-08-08 08:21:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-08 08:21:41 | INFO | train_inner | epoch 025:    441 / 1474 loss=1.976, trans_loss=4.985, nll_loss=2.175, w2v_ctc_loss=0.64, task_loss=1.501, contrastive_loss=0.054, total=4149.36, n_correct=2725.85, ppl=4.52, accuracy=65.693, wps=11864.5, ups=1.43, wpb=8298.7, bsz=290.1, num_updates=35800, lr=7.47435e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=69, gb_free=16.7, wall=29795
2023-08-08 08:22:50 | INFO | train_inner | epoch 025:    541 / 1474 loss=1.967, trans_loss=4.99, nll_loss=2.183, w2v_ctc_loss=0.622, task_loss=1.366, contrastive_loss=0.066, total=4154.79, n_correct=2729.63, ppl=4.54, accuracy=65.698, wps=11930, ups=1.44, wpb=8309.6, bsz=313.5, num_updates=35900, lr=7.46393e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=69, gb_free=17.5, wall=29865
2023-08-08 08:23:59 | INFO | train_inner | epoch 025:    641 / 1474 loss=1.971, trans_loss=4.982, nll_loss=2.173, w2v_ctc_loss=0.627, task_loss=1.39, contrastive_loss=0.131, total=4156.33, n_correct=2732.83, ppl=4.51, accuracy=65.751, wps=12004.5, ups=1.44, wpb=8312.7, bsz=309.3, num_updates=36000, lr=7.45356e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=69, gb_free=16.2, wall=29934
2023-08-08 08:23:59 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 08:24:22 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 4.193 | trans_loss 5.552 | nll_loss 2.82 | w2v_ctc_loss 1.359 | task_loss 4.635 | contrastive_loss 0.25 | total 4003.4 | n_correct 2493.3 | ppl 7.06 | accuracy 62.28 | uer 16.755 | wer 18.735 | raw_wer 18.735 | bleu 20.18 | wps 2234.3 | wpb 4003.4 | bsz 141.8 | num_updates 36000 | best_bleu 20.22
2023-08-08 08:24:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36000 updates
2023-08-08 08:24:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_25_36000.pt
2023-08-08 08:24:25 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_25_36000.pt
2023-08-08 08:24:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_25_36000.pt (epoch 25 @ 36000 updates, score 20.18) (writing took 26.050908436998725 seconds)
2023-08-08 08:25:59 | INFO | train_inner | epoch 025:    741 / 1474 loss=1.973, trans_loss=4.987, nll_loss=2.18, w2v_ctc_loss=0.624, task_loss=1.408, contrastive_loss=0.127, total=4133.94, n_correct=2713.91, ppl=4.53, accuracy=65.649, wps=6938.9, ups=0.84, wpb=8267.9, bsz=303.3, num_updates=36100, lr=7.44323e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=69, gb_free=14.9, wall=30053
2023-08-08 08:27:07 | INFO | train_inner | epoch 025:    841 / 1474 loss=1.963, trans_loss=4.987, nll_loss=2.179, w2v_ctc_loss=0.618, task_loss=1.301, contrastive_loss=0.075, total=4174.24, n_correct=2748.64, ppl=4.53, accuracy=65.848, wps=12141.2, ups=1.45, wpb=8348.5, bsz=324, num_updates=36200, lr=7.43294e-05, gnorm=0.517, clip=0, loss_scale=32, train_wall=68, gb_free=16.2, wall=30122
2023-08-08 08:28:17 | INFO | train_inner | epoch 025:    941 / 1474 loss=1.972, trans_loss=4.993, nll_loss=2.189, w2v_ctc_loss=0.624, task_loss=1.334, contrastive_loss=0.131, total=4154.13, n_correct=2725.39, ppl=4.56, accuracy=65.607, wps=11935.3, ups=1.44, wpb=8308.3, bsz=316.7, num_updates=36300, lr=7.4227e-05, gnorm=0.519, clip=0, loss_scale=32, train_wall=69, gb_free=10.6, wall=30192
2023-08-08 08:29:26 | INFO | train_inner | epoch 025:   1041 / 1474 loss=1.98, trans_loss=5, nll_loss=2.196, w2v_ctc_loss=0.615, task_loss=1.391, contrastive_loss=0.242, total=4178.3, n_correct=2736.58, ppl=4.58, accuracy=65.495, wps=12071.1, ups=1.44, wpb=8356.6, bsz=309.7, num_updates=36400, lr=7.41249e-05, gnorm=0.524, clip=0, loss_scale=32, train_wall=69, gb_free=16.8, wall=30261
2023-08-08 08:30:36 | INFO | train_inner | epoch 025:   1141 / 1474 loss=1.966, trans_loss=4.994, nll_loss=2.188, w2v_ctc_loss=0.616, task_loss=1.504, contrastive_loss=0.047, total=4042.33, n_correct=2654.18, ppl=4.56, accuracy=65.66, wps=11659.8, ups=1.44, wpb=8084.7, bsz=286.5, num_updates=36500, lr=7.40233e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=69, gb_free=17.7, wall=30330
2023-08-08 08:31:44 | INFO | train_inner | epoch 025:   1241 / 1474 loss=1.97, trans_loss=4.999, nll_loss=2.195, w2v_ctc_loss=0.621, task_loss=1.424, contrastive_loss=0.057, total=4087.78, n_correct=2677.55, ppl=4.58, accuracy=65.501, wps=11948.9, ups=1.46, wpb=8175.6, bsz=295.1, num_updates=36600, lr=7.39221e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=68, gb_free=17.6, wall=30399
2023-08-08 08:32:53 | INFO | train_inner | epoch 025:   1341 / 1474 loss=1.975, trans_loss=4.995, nll_loss=2.19, w2v_ctc_loss=0.624, task_loss=1.367, contrastive_loss=0.151, total=4166.64, n_correct=2732.06, ppl=4.56, accuracy=65.57, wps=12012.9, ups=1.44, wpb=8333.3, bsz=309.3, num_updates=36700, lr=7.38213e-05, gnorm=0.522, clip=0, loss_scale=32, train_wall=69, gb_free=16.4, wall=30468
2023-08-08 08:34:04 | INFO | train_inner | epoch 025:   1441 / 1474 loss=1.986, trans_loss=5.014, nll_loss=2.215, w2v_ctc_loss=0.632, task_loss=1.426, contrastive_loss=0.121, total=4114.64, n_correct=2679.18, ppl=4.64, accuracy=65.113, wps=11702.5, ups=1.42, wpb=8229.3, bsz=304.3, num_updates=36800, lr=7.3721e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=70, gb_free=16.3, wall=30539
2023-08-08 08:34:26 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 08:34:49 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 4.182 | trans_loss 5.541 | nll_loss 2.811 | w2v_ctc_loss 1.345 | task_loss 4.655 | contrastive_loss 0.25 | total 4003.4 | n_correct 2495.6 | ppl 7.02 | accuracy 62.337 | uer 17.031 | wer 19.082 | raw_wer 19.082 | bleu 19.91 | wps 2170.8 | wpb 4003.4 | bsz 141.8 | num_updates 36833 | best_bleu 20.22
2023-08-08 08:34:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36833 updates
2023-08-08 08:34:49 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.9106.pt
2023-08-08 08:34:53 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.9106.pt
2023-08-08 08:35:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_19.9106.pt (epoch 25 @ 36833 updates, score 19.91) (writing took 20.06949218735099 seconds)
2023-08-08 08:35:10 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-08-08 08:35:10 | INFO | train | epoch 025 | loss 1.97 | trans_loss 4.989 | nll_loss 2.181 | w2v_ctc_loss 0.623 | task_loss 1.402 | contrastive_loss 0.1 | total 4137.25 | n_correct 2717.3 | ppl 4.54 | accuracy 65.679 | wps 10850.7 | ups 1.31 | wpb 8274.5 | bsz 305.1 | num_updates 36833 | lr 7.36879e-05 | gnorm 0.523 | clip 0 | loss_scale 32 | train_wall 1016 | gb_free 14.1 | wall 30605
2023-08-08 08:35:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 08:35:10 | INFO | fairseq.trainer | begin training epoch 26
2023-08-08 08:35:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 08:36:04 | INFO | train_inner | epoch 026:     67 / 1474 loss=1.956, trans_loss=4.968, nll_loss=2.155, w2v_ctc_loss=0.611, task_loss=1.324, contrastive_loss=0.084, total=4172.16, n_correct=2755.11, ppl=4.45, accuracy=66.036, wps=6914.4, ups=0.83, wpb=8344.3, bsz=316.9, num_updates=36900, lr=7.3621e-05, gnorm=0.517, clip=0, loss_scale=32, train_wall=68, gb_free=16.9, wall=30659
2023-08-08 08:37:14 | INFO | train_inner | epoch 026:    167 / 1474 loss=1.959, trans_loss=4.964, nll_loss=2.15, w2v_ctc_loss=0.6, task_loss=1.233, contrastive_loss=0.259, total=4265.22, n_correct=2825.32, ppl=4.44, accuracy=66.241, wps=12290.7, ups=1.44, wpb=8530.4, bsz=338.7, num_updates=37000, lr=7.35215e-05, gnorm=0.514, clip=0, loss_scale=32, train_wall=69, gb_free=15.3, wall=30729
2023-08-08 08:38:23 | INFO | train_inner | epoch 026:    267 / 1474 loss=1.965, trans_loss=4.969, nll_loss=2.155, w2v_ctc_loss=0.621, task_loss=1.386, contrastive_loss=0.144, total=4123.94, n_correct=2723.03, ppl=4.45, accuracy=66.03, wps=11952.7, ups=1.45, wpb=8247.9, bsz=306.6, num_updates=37100, lr=7.34223e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=69, gb_free=16.8, wall=30798
2023-08-08 08:39:31 | INFO | train_inner | epoch 026:    367 / 1474 loss=1.96, trans_loss=4.972, nll_loss=2.159, w2v_ctc_loss=0.617, task_loss=1.336, contrastive_loss=0.106, total=4168.11, n_correct=2752.04, ppl=4.47, accuracy=66.026, wps=12153.3, ups=1.46, wpb=8336.2, bsz=315.2, num_updates=37200, lr=7.33236e-05, gnorm=0.524, clip=0, loss_scale=32, train_wall=68, gb_free=17.2, wall=30866
2023-08-08 08:40:40 | INFO | train_inner | epoch 026:    467 / 1474 loss=1.961, trans_loss=4.963, nll_loss=2.149, w2v_ctc_loss=0.615, task_loss=1.341, contrastive_loss=0.15, total=4167.53, n_correct=2757.36, ppl=4.44, accuracy=66.163, wps=12084.3, ups=1.45, wpb=8335.1, bsz=314.6, num_updates=37300, lr=7.32252e-05, gnorm=0.521, clip=0, loss_scale=32, train_wall=69, gb_free=13.9, wall=30935
2023-08-08 08:41:50 | INFO | train_inner | epoch 026:    567 / 1474 loss=1.966, trans_loss=4.979, nll_loss=2.169, w2v_ctc_loss=0.628, task_loss=1.404, contrastive_loss=0.07, total=4158.48, n_correct=2744.94, ppl=4.5, accuracy=66.008, wps=11958.1, ups=1.44, wpb=8317, bsz=304.4, num_updates=37400, lr=7.31272e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=69, gb_free=12.6, wall=31005
2023-08-08 08:42:59 | INFO | train_inner | epoch 026:    667 / 1474 loss=1.96, trans_loss=4.979, nll_loss=2.169, w2v_ctc_loss=0.613, task_loss=1.435, contrastive_loss=0.054, total=4129.11, n_correct=2720.21, ppl=4.5, accuracy=65.879, wps=11923.9, ups=1.44, wpb=8258.2, bsz=297.7, num_updates=37500, lr=7.30297e-05, gnorm=0.526, clip=0, loss_scale=32, train_wall=69, gb_free=13.7, wall=31074
2023-08-08 08:44:08 | INFO | train_inner | epoch 026:    767 / 1474 loss=1.972, trans_loss=4.986, nll_loss=2.178, w2v_ctc_loss=0.617, task_loss=1.41, contrastive_loss=0.168, total=4096.84, n_correct=2692.72, ppl=4.53, accuracy=65.727, wps=11833, ups=1.44, wpb=8193.7, bsz=300.5, num_updates=37600, lr=7.29325e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=69, gb_free=16.5, wall=31143
2023-08-08 08:45:17 | INFO | train_inner | epoch 026:    867 / 1474 loss=1.964, trans_loss=4.982, nll_loss=2.172, w2v_ctc_loss=0.62, task_loss=1.397, contrastive_loss=0.069, total=4176.27, n_correct=2746.34, ppl=4.51, accuracy=65.761, wps=12150.7, ups=1.45, wpb=8352.5, bsz=306.4, num_updates=37700, lr=7.28357e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=68, gb_free=16.1, wall=31212
2023-08-08 08:46:26 | INFO | train_inner | epoch 026:    967 / 1474 loss=1.969, trans_loss=4.992, nll_loss=2.185, w2v_ctc_loss=0.611, task_loss=1.446, contrastive_loss=0.122, total=4141.01, n_correct=2719.4, ppl=4.55, accuracy=65.67, wps=11994.9, ups=1.45, wpb=8282, bsz=299.7, num_updates=37800, lr=7.27393e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=69, gb_free=15.4, wall=31281
2023-08-08 08:47:35 | INFO | train_inner | epoch 026:   1067 / 1474 loss=1.962, trans_loss=4.985, nll_loss=2.177, w2v_ctc_loss=0.615, task_loss=1.477, contrastive_loss=0.053, total=4113.69, n_correct=2711.26, ppl=4.52, accuracy=65.908, wps=11958, ups=1.45, wpb=8227.4, bsz=293, num_updates=37900, lr=7.26433e-05, gnorm=0.517, clip=0, loss_scale=64, train_wall=68, gb_free=15.4, wall=31350
2023-08-08 08:48:45 | INFO | train_inner | epoch 026:   1167 / 1474 loss=1.969, trans_loss=4.992, nll_loss=2.187, w2v_ctc_loss=0.619, task_loss=1.458, contrastive_loss=0.093, total=4116.78, n_correct=2701.91, ppl=4.55, accuracy=65.632, wps=11835.1, ups=1.44, wpb=8233.6, bsz=299, num_updates=38000, lr=7.25476e-05, gnorm=0.531, clip=0, loss_scale=64, train_wall=69, gb_free=16.5, wall=31419
2023-08-08 08:48:45 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 08:49:08 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 4.195 | trans_loss 5.548 | nll_loss 2.818 | w2v_ctc_loss 1.374 | task_loss 4.638 | contrastive_loss 0.247 | total 4003.4 | n_correct 2501.5 | ppl 7.05 | accuracy 62.484 | uer 17.108 | wer 18.985 | raw_wer 18.985 | bleu 20.2 | wps 2180.9 | wpb 4003.4 | bsz 141.8 | num_updates 38000 | best_bleu 20.22
2023-08-08 08:49:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38000 updates
2023-08-08 08:49:08 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_26_38000.pt
2023-08-08 08:49:12 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_26_38000.pt
2023-08-08 08:49:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_26_38000.pt (epoch 26 @ 38000 updates, score 20.2) (writing took 30.207558315247297 seconds)
2023-08-08 08:50:48 | INFO | train_inner | epoch 026:   1267 / 1474 loss=1.978, trans_loss=5.004, nll_loss=2.202, w2v_ctc_loss=0.633, task_loss=1.55, contrastive_loss=0.056, total=4001.06, n_correct=2614.73, ppl=4.6, accuracy=65.351, wps=6481.8, ups=0.81, wpb=8002.1, bsz=280.6, num_updates=38100, lr=7.24524e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=69, gb_free=15.5, wall=31543
2023-08-08 08:51:58 | INFO | train_inner | epoch 026:   1367 / 1474 loss=1.964, trans_loss=4.995, nll_loss=2.191, w2v_ctc_loss=0.611, task_loss=1.401, contrastive_loss=0.071, total=4157.69, n_correct=2731.12, ppl=4.57, accuracy=65.688, wps=11876.4, ups=1.43, wpb=8315.4, bsz=310.5, num_updates=38200, lr=7.23575e-05, gnorm=0.53, clip=0, loss_scale=64, train_wall=69, gb_free=16, wall=31613
2023-08-08 08:53:07 | INFO | train_inner | epoch 026:   1467 / 1474 loss=1.959, trans_loss=4.988, nll_loss=2.183, w2v_ctc_loss=0.61, task_loss=1.328, contrastive_loss=0.065, total=4158.47, n_correct=2739.91, ppl=4.54, accuracy=65.887, wps=12075.7, ups=1.45, wpb=8316.9, bsz=316.5, num_updates=38300, lr=7.22629e-05, gnorm=0.523, clip=0, loss_scale=64, train_wall=68, gb_free=17.3, wall=31682
2023-08-08 08:53:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 08:53:35 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 4.193 | trans_loss 5.545 | nll_loss 2.817 | w2v_ctc_loss 1.371 | task_loss 4.638 | contrastive_loss 0.25 | total 4003.4 | n_correct 2493.5 | ppl 7.04 | accuracy 62.285 | uer 17.201 | wer 19.164 | raw_wer 19.164 | bleu 20.39 | wps 2194.6 | wpb 4003.4 | bsz 141.8 | num_updates 38307 | best_bleu 20.39
2023-08-08 08:53:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38307 updates
2023-08-08 08:53:35 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 08:53:47 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 08:53:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt (epoch 26 @ 38307 updates, score 20.39) (writing took 23.944660434499383 seconds)
2023-08-08 08:53:59 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-08-08 08:53:59 | INFO | train | epoch 026 | loss 1.964 | trans_loss 4.981 | nll_loss 2.171 | w2v_ctc_loss 0.616 | task_loss 1.398 | contrastive_loss 0.106 | total 4138.65 | n_correct 2726.47 | ppl 4.5 | accuracy 65.878 | wps 10803.4 | ups 1.31 | wpb 8277.3 | bsz 305.7 | num_updates 38307 | lr 7.22563e-05 | gnorm 0.525 | clip 0 | loss_scale 64 | train_wall 1012 | gb_free 15.8 | wall 31734
2023-08-08 08:53:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 08:54:00 | INFO | fairseq.trainer | begin training epoch 27
2023-08-08 08:54:00 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 08:55:12 | INFO | train_inner | epoch 027:     93 / 1474 loss=1.946, trans_loss=4.947, nll_loss=2.126, w2v_ctc_loss=0.604, task_loss=1.492, contrastive_loss=0.044, total=4067.62, n_correct=2704.85, ppl=4.36, accuracy=66.497, wps=6526.9, ups=0.8, wpb=8135.2, bsz=284.4, num_updates=38400, lr=7.21688e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=68, gb_free=14.7, wall=31806
2023-08-08 08:56:21 | INFO | train_inner | epoch 027:    193 / 1474 loss=1.946, trans_loss=4.951, nll_loss=2.134, w2v_ctc_loss=0.609, task_loss=1.339, contrastive_loss=0.074, total=4185.52, n_correct=2781.76, ppl=4.39, accuracy=66.462, wps=12002.1, ups=1.43, wpb=8371, bsz=321.7, num_updates=38500, lr=7.2075e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=69, gb_free=17.5, wall=31876
2023-08-08 08:57:31 | INFO | train_inner | epoch 027:    293 / 1474 loss=1.951, trans_loss=4.962, nll_loss=2.146, w2v_ctc_loss=0.61, task_loss=1.396, contrastive_loss=0.055, total=4167.92, n_correct=2761.21, ppl=4.43, accuracy=66.249, wps=12035.3, ups=1.44, wpb=8335.8, bsz=306.8, num_updates=38600, lr=7.19816e-05, gnorm=0.519, clip=0, loss_scale=64, train_wall=69, gb_free=16.8, wall=31945
2023-08-08 08:58:40 | INFO | train_inner | epoch 027:    393 / 1474 loss=1.968, trans_loss=4.97, nll_loss=2.157, w2v_ctc_loss=0.607, task_loss=1.465, contrastive_loss=0.238, total=4075.21, n_correct=2691.81, ppl=4.46, accuracy=66.053, wps=11677.4, ups=1.43, wpb=8150.4, bsz=296, num_updates=38700, lr=7.18885e-05, gnorm=0.527, clip=0, loss_scale=64, train_wall=69, gb_free=17.6, wall=32015
2023-08-08 08:59:50 | INFO | train_inner | epoch 027:    493 / 1474 loss=1.961, trans_loss=4.974, nll_loss=2.164, w2v_ctc_loss=0.609, task_loss=1.281, contrastive_loss=0.177, total=4249.35, n_correct=2802.16, ppl=4.48, accuracy=65.943, wps=12170.5, ups=1.43, wpb=8498.7, bsz=331.9, num_updates=38800, lr=7.17958e-05, gnorm=0.517, clip=0, loss_scale=64, train_wall=69, gb_free=11.8, wall=32085
2023-08-08 09:00:59 | INFO | train_inner | epoch 027:    593 / 1474 loss=1.96, trans_loss=4.969, nll_loss=2.157, w2v_ctc_loss=0.615, task_loss=1.369, contrastive_loss=0.116, total=4133.39, n_correct=2735.52, ppl=4.46, accuracy=66.181, wps=11954.5, ups=1.45, wpb=8266.8, bsz=312, num_updates=38900, lr=7.17035e-05, gnorm=0.526, clip=0, loss_scale=64, train_wall=69, gb_free=11.9, wall=32154
2023-08-08 09:02:08 | INFO | train_inner | epoch 027:    693 / 1474 loss=1.961, trans_loss=4.975, nll_loss=2.164, w2v_ctc_loss=0.616, task_loss=1.395, contrastive_loss=0.093, total=4162.71, n_correct=2745.04, ppl=4.48, accuracy=65.944, wps=12061.1, ups=1.45, wpb=8325.4, bsz=305.4, num_updates=39000, lr=7.16115e-05, gnorm=0.524, clip=0, loss_scale=64, train_wall=69, gb_free=16.2, wall=32223
2023-08-08 09:03:17 | INFO | train_inner | epoch 027:    793 / 1474 loss=1.96, trans_loss=4.976, nll_loss=2.165, w2v_ctc_loss=0.617, task_loss=1.471, contrastive_loss=0.057, total=4103.81, n_correct=2705.85, ppl=4.48, accuracy=65.935, wps=11883.7, ups=1.45, wpb=8207.6, bsz=294.2, num_updates=39100, lr=7.15199e-05, gnorm=0.528, clip=0, loss_scale=64, train_wall=69, gb_free=16.1, wall=32292
2023-08-08 09:04:26 | INFO | train_inner | epoch 027:    893 / 1474 loss=1.955, trans_loss=4.982, nll_loss=2.173, w2v_ctc_loss=0.603, task_loss=1.458, contrastive_loss=0.046, total=4101.56, n_correct=2705.57, ppl=4.51, accuracy=65.964, wps=11929.5, ups=1.45, wpb=8203.1, bsz=292.1, num_updates=39200, lr=7.14286e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=68, gb_free=17.6, wall=32361
2023-08-08 09:05:36 | INFO | train_inner | epoch 027:    993 / 1474 loss=1.966, trans_loss=4.977, nll_loss=2.167, w2v_ctc_loss=0.608, task_loss=1.354, contrastive_loss=0.234, total=4199.56, n_correct=2769.05, ppl=4.49, accuracy=65.937, wps=12068.9, ups=1.44, wpb=8399.1, bsz=316.8, num_updates=39300, lr=7.13376e-05, gnorm=0.52, clip=0, loss_scale=64, train_wall=69, gb_free=11.4, wall=32431
2023-08-08 09:06:45 | INFO | train_inner | epoch 027:   1093 / 1474 loss=1.953, trans_loss=4.974, nll_loss=2.162, w2v_ctc_loss=0.605, task_loss=1.408, contrastive_loss=0.067, total=4150.97, n_correct=2741.02, ppl=4.48, accuracy=66.033, wps=12035.2, ups=1.45, wpb=8301.9, bsz=305, num_updates=39400, lr=7.1247e-05, gnorm=0.527, clip=0, loss_scale=64, train_wall=69, gb_free=11.8, wall=32500
2023-08-08 09:07:54 | INFO | train_inner | epoch 027:   1193 / 1474 loss=1.966, trans_loss=4.985, nll_loss=2.178, w2v_ctc_loss=0.621, task_loss=1.459, contrastive_loss=0.071, total=4103.06, n_correct=2699.47, ppl=4.52, accuracy=65.792, wps=11850.1, ups=1.44, wpb=8206.1, bsz=297.7, num_updates=39500, lr=7.11568e-05, gnorm=0.531, clip=0, loss_scale=64, train_wall=69, gb_free=16.4, wall=32569
2023-08-08 09:09:03 | INFO | train_inner | epoch 027:   1293 / 1474 loss=1.972, trans_loss=4.991, nll_loss=2.185, w2v_ctc_loss=0.62, task_loss=1.495, contrastive_loss=0.122, total=4062.52, n_correct=2666.73, ppl=4.55, accuracy=65.642, wps=11850.3, ups=1.46, wpb=8125, bsz=292.2, num_updates=39600, lr=7.10669e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=68, gb_free=16.5, wall=32638
2023-08-08 09:10:11 | INFO | train_inner | epoch 027:   1393 / 1474 loss=1.958, trans_loss=4.981, nll_loss=2.173, w2v_ctc_loss=0.605, task_loss=1.32, contrastive_loss=0.105, total=4152, n_correct=2740.65, ppl=4.51, accuracy=66.008, wps=12169.6, ups=1.47, wpb=8304, bsz=312.5, num_updates=39700, lr=7.09773e-05, gnorm=0.519, clip=0, loss_scale=64, train_wall=68, gb_free=16.5, wall=32706
2023-08-08 09:11:07 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 09:11:30 | INFO | dev_st | epoch 027 | valid on 'dev_st' subset | loss 4.177 | trans_loss 5.544 | nll_loss 2.814 | w2v_ctc_loss 1.324 | task_loss 4.602 | contrastive_loss 0.246 | total 4003.4 | n_correct 2494.8 | ppl 7.03 | accuracy 62.317 | uer 16.773 | wer 18.642 | raw_wer 18.642 | bleu 20.25 | wps 2175 | wpb 4003.4 | bsz 141.8 | num_updates 39781 | best_bleu 20.39
2023-08-08 09:11:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 39781 updates
2023-08-08 09:11:30 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.2503.pt
2023-08-08 09:11:33 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.2503.pt
2023-08-08 09:11:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.2503.pt (epoch 27 @ 39781 updates, score 20.25) (writing took 13.525878060609102 seconds)
2023-08-08 09:11:44 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-08-08 09:11:44 | INFO | train | epoch 027 | loss 1.958 | trans_loss 4.973 | nll_loss 2.161 | w2v_ctc_loss 0.61 | task_loss 1.398 | contrastive_loss 0.106 | total 4138.65 | n_correct 2733.72 | ppl 4.47 | accuracy 66.053 | wps 11457.4 | ups 1.38 | wpb 8277.3 | bsz 305.7 | num_updates 39781 | lr 7.0905e-05 | gnorm 0.525 | clip 0 | loss_scale 64 | train_wall 1012 | gb_free 17.7 | wall 32799
2023-08-08 09:11:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 09:11:44 | INFO | fairseq.trainer | begin training epoch 28
2023-08-08 09:11:44 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 09:12:05 | INFO | train_inner | epoch 028:     19 / 1474 loss=1.95, trans_loss=4.973, nll_loss=2.162, w2v_ctc_loss=0.602, task_loss=1.351, contrastive_loss=0.057, total=4108.43, n_correct=2720.25, ppl=4.47, accuracy=66.211, wps=7187.1, ups=0.87, wpb=8216.9, bsz=305.1, num_updates=39800, lr=7.08881e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=68, gb_free=16.1, wall=32820
2023-08-08 09:13:14 | INFO | train_inner | epoch 028:    119 / 1474 loss=1.94, trans_loss=4.942, nll_loss=2.12, w2v_ctc_loss=0.6, task_loss=1.457, contrastive_loss=0.052, total=4113.41, n_correct=2748.89, ppl=4.35, accuracy=66.828, wps=11885.6, ups=1.44, wpb=8226.8, bsz=293.9, num_updates=39900, lr=7.07992e-05, gnorm=0.525, clip=0, loss_scale=128, train_wall=69, gb_free=16.7, wall=32889
2023-08-08 09:14:24 | INFO | train_inner | epoch 028:    219 / 1474 loss=1.943, trans_loss=4.953, nll_loss=2.135, w2v_ctc_loss=0.601, task_loss=1.325, contrastive_loss=0.06, total=4191.56, n_correct=2790.36, ppl=4.39, accuracy=66.571, wps=12125, ups=1.45, wpb=8383.1, bsz=315.2, num_updates=40000, lr=7.07107e-05, gnorm=0.524, clip=0, loss_scale=128, train_wall=69, gb_free=15, wall=32958
2023-08-08 09:14:24 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 09:14:47 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 4.196 | trans_loss 5.554 | nll_loss 2.822 | w2v_ctc_loss 1.367 | task_loss 4.614 | contrastive_loss 0.244 | total 4003.4 | n_correct 2493.6 | ppl 7.07 | accuracy 62.287 | uer 16.784 | wer 18.627 | raw_wer 18.627 | bleu 20.19 | wps 2225 | wpb 4003.4 | bsz 141.8 | num_updates 40000 | best_bleu 20.39
2023-08-08 09:14:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 40000 updates
2023-08-08 09:14:47 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_28_40000.pt
2023-08-08 09:14:51 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_28_40000.pt
2023-08-08 09:15:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_28_40000.pt (epoch 28 @ 40000 updates, score 20.19) (writing took 30.626460982486606 seconds)
Mixup rate:0.5, token after shrink shape:torch.Size([16, 61]), X shape:torch.Size([16, 61, 512])
CTC Tokens:tensor([0, 0, 0, 0, 0], device='cuda:0'), Shrink Mask:tensor([ True, False, False, False, False], device='cuda:0'), New Tokens:tensor([  0, 339,  11,   6, 150], device='cuda:0')
Mixup Sent Mask:tensor([[ 1],
        [ 3],
        [ 9],
        [12],
        [14]], device='cuda:0'), 1,  Mixup Mask:tensor([ True,  True,  True,  True, False], device='cuda:0'), 
                    Org X:tensor([[ 0.0494, -0.9702,  0.9434,  ...,  0.4016, -0.0077,  0.6733],
        [ 0.1862,  0.0064,  0.6729,  ...,  0.1779, -0.2042,  0.3577],
        [ 0.2017, -0.0701,  0.5479,  ...,  0.1277,  0.1586,  0.3979],
        [ 1.5088,  1.1064,  0.4341,  ..., -0.9150, -0.4419,  0.2908],
        [ 0.0919,  1.4307,  0.6543,  ...,  0.1060, -0.2529, -1.1514]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [ 0.8306, -0.7373, -0.1372,  ...,  1.6738, -2.2734, -1.0479],
        [-0.4478, -0.3506, -0.4424,  ..., -2.3691, -0.1564, -4.2539],
        [ 0.0720, -0.4412, -0.1628,  ..., -2.9941, -3.2988,  0.9482],
        [ 0.0919,  1.4307,  0.6543,  ...,  0.1060, -0.2529, -1.1514]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [ 0.8306, -0.7373, -0.1372,  ...,  1.6738, -2.2734, -1.0479],
        [-0.4478, -0.3506, -0.4424,  ..., -2.3691, -0.1564, -4.2539],
        [ 0.0720, -0.4412, -0.1628,  ..., -2.9941, -3.2988,  0.9482],
        [-0.1517, -0.4468, -0.7881,  ...,  0.7549, -1.5293, -2.6367]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:0')
2023-08-08 09:15:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-08 09:16:29 | INFO | train_inner | epoch 028:    320 / 1474 loss=1.959, trans_loss=4.958, nll_loss=2.142, w2v_ctc_loss=0.598, task_loss=1.419, contrastive_loss=0.285, total=4127.08, n_correct=2733.11, ppl=4.41, accuracy=66.224, wps=6582.3, ups=0.8, wpb=8254.2, bsz=308.8, num_updates=40100, lr=7.06225e-05, gnorm=0.527, clip=0, loss_scale=64, train_wall=70, gb_free=13.1, wall=33084
2023-08-08 09:17:38 | INFO | train_inner | epoch 028:    420 / 1474 loss=1.95, trans_loss=4.959, nll_loss=2.142, w2v_ctc_loss=0.609, task_loss=1.442, contrastive_loss=0.048, total=4089.84, n_correct=2715.28, ppl=4.41, accuracy=66.391, wps=11923.4, ups=1.46, wpb=8179.7, bsz=295.7, num_updates=40200, lr=7.05346e-05, gnorm=0.53, clip=0, loss_scale=64, train_wall=68, gb_free=16.4, wall=33152
2023-08-08 09:18:47 | INFO | train_inner | epoch 028:    520 / 1474 loss=1.948, trans_loss=4.96, nll_loss=2.144, w2v_ctc_loss=0.602, task_loss=1.453, contrastive_loss=0.059, total=4098.92, n_correct=2721.5, ppl=4.42, accuracy=66.396, wps=11857, ups=1.45, wpb=8197.8, bsz=295.7, num_updates=40300, lr=7.0447e-05, gnorm=0.527, clip=0, loss_scale=64, train_wall=69, gb_free=16.8, wall=33222
2023-08-08 09:19:56 | INFO | train_inner | epoch 028:    620 / 1474 loss=1.954, trans_loss=4.972, nll_loss=2.161, w2v_ctc_loss=0.608, task_loss=1.409, contrastive_loss=0.06, total=4180.1, n_correct=2764.69, ppl=4.47, accuracy=66.139, wps=12018.5, ups=1.44, wpb=8360.2, bsz=305.3, num_updates=40400, lr=7.03598e-05, gnorm=0.527, clip=0, loss_scale=64, train_wall=69, gb_free=16.4, wall=33291
2023-08-08 09:21:06 | INFO | train_inner | epoch 028:    720 / 1474 loss=1.953, trans_loss=4.967, nll_loss=2.155, w2v_ctc_loss=0.601, task_loss=1.262, contrastive_loss=0.173, total=4191.62, n_correct=2776.76, ppl=4.45, accuracy=66.246, wps=12099.5, ups=1.44, wpb=8383.2, bsz=329.2, num_updates=40500, lr=7.02728e-05, gnorm=0.519, clip=0, loss_scale=64, train_wall=69, gb_free=17, wall=33360
2023-08-08 09:22:14 | INFO | train_inner | epoch 028:    820 / 1474 loss=1.946, trans_loss=4.964, nll_loss=2.151, w2v_ctc_loss=0.602, task_loss=1.383, contrastive_loss=0.05, total=4088.91, n_correct=2714.06, ppl=4.44, accuracy=66.376, wps=11947.3, ups=1.46, wpb=8177.8, bsz=304.3, num_updates=40600, lr=7.01862e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=68, gb_free=16.9, wall=33429
2023-08-08 09:23:24 | INFO | train_inner | epoch 028:    920 / 1474 loss=1.962, trans_loss=4.976, nll_loss=2.166, w2v_ctc_loss=0.61, task_loss=1.448, contrastive_loss=0.115, total=4117.01, n_correct=2716.6, ppl=4.49, accuracy=65.985, wps=11776.8, ups=1.43, wpb=8234, bsz=299.7, num_updates=40700, lr=7.01e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=69, gb_free=15.1, wall=33499
2023-08-08 09:24:33 | INFO | train_inner | epoch 028:   1020 / 1474 loss=1.965, trans_loss=4.974, nll_loss=2.164, w2v_ctc_loss=0.614, task_loss=1.364, contrastive_loss=0.167, total=4182.85, n_correct=2761.99, ppl=4.48, accuracy=66.031, wps=12071.8, ups=1.44, wpb=8365.7, bsz=312, num_updates=40800, lr=7.0014e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=16.2, wall=33568
2023-08-08 09:25:43 | INFO | train_inner | epoch 028:   1120 / 1474 loss=1.947, trans_loss=4.964, nll_loss=2.152, w2v_ctc_loss=0.602, task_loss=1.344, contrastive_loss=0.074, total=4220.16, n_correct=2794.54, ppl=4.44, accuracy=66.219, wps=12152.2, ups=1.44, wpb=8440.3, bsz=321, num_updates=40900, lr=6.99284e-05, gnorm=0.524, clip=0, loss_scale=64, train_wall=69, gb_free=15.8, wall=33638
2023-08-08 09:26:52 | INFO | train_inner | epoch 028:   1220 / 1474 loss=1.95, trans_loss=4.972, nll_loss=2.162, w2v_ctc_loss=0.601, task_loss=1.392, contrastive_loss=0.059, total=4092.46, n_correct=2708.68, ppl=4.47, accuracy=66.187, wps=11867.7, ups=1.45, wpb=8184.9, bsz=303.1, num_updates=41000, lr=6.9843e-05, gnorm=0.526, clip=0, loss_scale=64, train_wall=68, gb_free=17.5, wall=33707
2023-08-08 09:28:02 | INFO | train_inner | epoch 028:   1320 / 1474 loss=1.966, trans_loss=4.981, nll_loss=2.172, w2v_ctc_loss=0.622, task_loss=1.533, contrastive_loss=0.073, total=4084.55, n_correct=2689.9, ppl=4.51, accuracy=65.855, wps=11667.8, ups=1.43, wpb=8169.1, bsz=285.1, num_updates=41100, lr=6.9758e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=69, gb_free=15.6, wall=33777
2023-08-08 09:29:11 | INFO | train_inner | epoch 028:   1420 / 1474 loss=1.958, trans_loss=4.976, nll_loss=2.166, w2v_ctc_loss=0.606, task_loss=1.461, contrastive_loss=0.095, total=4154.09, n_correct=2740.74, ppl=4.49, accuracy=65.977, wps=11973.7, ups=1.44, wpb=8308.2, bsz=299.3, num_updates=41200, lr=6.96733e-05, gnorm=0.521, clip=0, loss_scale=64, train_wall=69, gb_free=15.9, wall=33846
2023-08-08 09:29:48 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([24, 55]), X shape:torch.Size([24, 55, 512])
CTC Tokens:tensor([ 0, 29,  0,  4, 71], device='cuda:7'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:7'), New Tokens:tensor([ 0, 29,  0,  4, 71], device='cuda:7')
Mixup Sent Mask:tensor([[ 1],
        [ 3],
        [ 9],
        [12],
        [14],
        [19],
        [21],
        [23]], device='cuda:7'), 1,  Mixup Mask:tensor([ True,  True, False,  True,  True], device='cuda:7'), 
                    Org X:tensor([[ 0.4436, -1.0781,  0.4626,  ..., -0.1870, -1.1973,  0.1748],
        [-0.3828,  0.4165,  0.8174,  ...,  0.1920, -1.2383, -0.0642],
        [-0.3972,  0.8560,  2.3379,  ...,  0.0604, -1.4111, -0.2239],
        [ 0.0716,  0.9629,  2.5527,  ..., -0.0542,  0.7124,  0.2063],
        [-1.0059,  0.3389, -0.7036,  ...,  0.2646,  1.0537,  0.3359]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [-1.0527, -0.0542, -0.2788,  ..., -0.0934, -2.1348,  3.4629],
        [-0.3972,  0.8560,  2.3379,  ...,  0.0604, -1.4111, -0.2239],
        [-0.2052, -0.4143, -0.2272,  ..., -1.3877, -1.0830, -2.1016],
        [-0.0109,  0.2683, -0.2380,  ..., -0.7847, -0.3530, -1.5430]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [-1.0527, -0.0542, -0.2788,  ..., -0.0934, -2.1348,  3.4629],
        [-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [-0.2052, -0.4143, -0.2272,  ..., -1.3877, -1.0830, -2.1016],
        [-0.0109,  0.2683, -0.2380,  ..., -0.7847, -0.3530, -1.5430]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 100]), X shape:torch.Size([8, 100, 512])
CTC Tokens:tensor([  29,   29,    0,    0, 1042], device='cuda:5'), Shrink Mask:tensor([ True, False,  True, False,  True], device='cuda:5'), New Tokens:tensor([  29,    0, 1042,    0,   25], device='cuda:5')
Mixup Sent Mask:tensor([[1],
        [3]], device='cuda:5'), 1,  Mixup Mask:tensor([False, False,  True,  True, False], device='cuda:5'), 
                    Org X:tensor([[-0.2222, -0.5303,  0.4412,  ...,  0.0393, -0.9463, -0.0995],
        [-0.6753,  0.4321,  1.8203,  ..., -0.0047, -0.5127, -0.3901],
        [-0.6235, -0.8784,  0.4714,  ...,  0.1597,  0.2634,  0.7339],
        [ 0.2837, -0.0905,  2.1934,  ..., -0.0525,  1.0840,  0.3665],
        [ 0.1392,  0.2646, -1.4932,  ..., -0.2571,  0.7056, -0.5239]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.2222, -0.5303,  0.4412,  ...,  0.0393, -0.9463, -0.0995],
        [-0.6753,  0.4321,  1.8203,  ..., -0.0047, -0.5127, -0.3901],
        [-0.2200, -0.1830,  0.6152,  ...,  2.6953, -1.7041, -1.6260],
        [-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [ 0.1392,  0.2646, -1.4932,  ..., -0.2571,  0.7056, -0.5239]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-1.0527, -0.0542, -0.2788,  ..., -0.0934, -2.1348,  3.4629],
        [-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [-0.2200, -0.1830,  0.6152,  ...,  2.6953, -1.7041, -1.6260],
        [-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [ 0.2783, -0.5039, -0.1055,  ..., -1.0166,  0.3069, -2.3359]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 78]), X shape:torch.Size([16, 78, 512])
CTC Tokens:tensor([ 21, 169, 169,  51,  51], device='cuda:1'), Shrink Mask:tensor([ True,  True, False,  True, False], device='cuda:1'), New Tokens:tensor([ 21, 169,  51,  13, 324], device='cuda:1')
Mixup Sent Mask:tensor([[ 1],
        [ 3],
        [ 9],
        [12],
        [14]], device='cuda:1'), 1,  Mixup Mask:tensor([ True,  True, False,  True, False], device='cuda:1'), 
                    Org X:tensor([[-0.4182, -1.8877, -2.2188,  ..., -0.5566,  1.1729, -0.1938],
        [ 0.5356, -0.2189,  0.2729,  ..., -0.3167,  2.4824,  0.0836],
        [-0.6108,  0.2803, -0.1117,  ..., -0.0885,  0.6973, -0.9229],
        [-0.9062,  0.7905, -1.7285,  ..., -0.1085,  1.6279,  0.4355],
        [-0.2764,  0.3962, -2.2891,  ...,  0.2668,  1.9473,  0.5625]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.5581, -0.2708, -0.4561,  ..., -1.0645, -3.1406, -1.7930],
        [ 0.2371, -0.2932,  0.4409,  ..., -0.8638,  2.3008, -1.4199],
        [-0.6108,  0.2803, -0.1117,  ..., -0.0885,  0.6973, -0.9229],
        [-0.4844, -0.2786,  0.0204,  ..., -1.7480, -0.7046,  0.9458],
        [-0.2764,  0.3962, -2.2891,  ...,  0.2668,  1.9473,  0.5625]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5581, -0.2708, -0.4561,  ..., -1.0645, -3.1406, -1.7930],
        [ 0.2371, -0.2932,  0.4409,  ..., -0.8638,  2.3008, -1.4199],
        [-1.5479,  0.6270,  0.3743,  ..., -1.5264,  2.0605,  0.2462],
        [-0.4844, -0.2786,  0.0204,  ..., -1.7480, -0.7046,  0.9458],
        [ 0.0402, -0.2013, -1.0361,  ...,  0.9688,  0.2515, -0.7397]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([6, 178]), X shape:torch.Size([6, 178, 512])
CTC Tokens:tensor([ 0,  0, 70,  0,  7], device='cuda:3'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:3'), New Tokens:tensor([ 0, 70,  0,  7,  0], device='cuda:3')
Mixup Sent Mask:tensor([[1],
        [3]], device='cuda:3'), 1,  Mixup Mask:tensor([False, False,  True, False, False], device='cuda:3'), 
                    Org X:tensor([[ 0.1019, -1.9727,  0.3118,  ...,  0.2512, -0.4763,  0.4277],
        [-0.3845,  0.1921,  2.0488,  ...,  0.2089, -3.3008,  0.1140],
        [-0.8223,  0.7705,  1.7305,  ...,  0.3591, -2.2773, -0.5435],
        [-0.5918,  0.3716, -0.7847,  ...,  0.2798, -0.8579, -0.7329],
        [-1.3545,  1.3311,  2.9902,  ...,  0.0581,  0.2632,  0.6123]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.1019, -1.9727,  0.3118,  ...,  0.2512, -0.4763,  0.4277],
        [-0.3845,  0.1921,  2.0488,  ...,  0.2089, -3.3008,  0.1140],
        [-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [-0.5918,  0.3716, -0.7847,  ...,  0.2798, -0.8579, -0.7329],
        [-1.3545,  1.3311,  2.9902,  ...,  0.0581,  0.2632,  0.6123]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [-0.1057, -0.6138, -0.1802,  ...,  0.5205, -1.6641, -2.7324],
        [-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [-0.2289, -0.3240, -0.2512,  ..., -0.2766, -1.6797, -1.4209],
        [-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([24, 49]), X shape:torch.Size([24, 49, 512])
CTC Tokens:tensor([225, 225,  11,   6,   6], device='cuda:6'), Shrink Mask:tensor([ True, False,  True,  True, False], device='cuda:6'), New Tokens:tensor([225,  11,   6,   0,  73], device='cuda:6')
Mixup Sent Mask:tensor([[ 1],
        [ 3],
        [ 9],
        [12],
        [14],
        [19],
        [21],
        [23]], device='cuda:6'), 1,  Mixup Mask:tensor([False, False,  True, False,  True], device='cuda:6'), 
                    Org X:tensor([[-0.3223, -1.2402, -1.1943,  ..., -0.5273, -0.0339,  2.2500],
        [ 0.6084, -0.0278, -1.9590,  ..., -1.0303,  0.3506,  0.1232],
        [ 1.3457,  0.6050, -2.0410,  ..., -1.8066, -0.2854,  0.3523],
        [-1.4961,  1.8252, -0.0922,  ..., -1.1113, -0.2805,  0.1987],
        [ 0.2986,  0.4333,  3.5332,  ...,  0.3596, -1.2598,  1.5547]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3223, -1.2402, -1.1943,  ..., -0.5273, -0.0339,  2.2500],
        [ 0.6084, -0.0278, -1.9590,  ..., -1.0303,  0.3506,  0.1232],
        [ 0.0720, -0.4412, -0.1628,  ..., -2.9941, -3.2988,  0.9482],
        [-1.4961,  1.8252, -0.0922,  ..., -1.1113, -0.2805,  0.1987],
        [-0.2644, -0.3083, -0.6782,  ...,  3.9297, -1.0146, -1.7109]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.1029, -0.1379, -0.4910,  ..., -0.6606,  1.0586,  0.4771],
        [-0.4478, -0.3506, -0.4424,  ..., -2.3691, -0.1564, -4.2539],
        [ 0.0720, -0.4412, -0.1628,  ..., -2.9941, -3.2988,  0.9482],
        [-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [-0.2644, -0.3083, -0.6782,  ...,  3.9297, -1.0146, -1.7109]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 80]), X shape:torch.Size([8, 80, 512])
CTC Tokens:tensor([  0,   0, 419,   0,   0], device='cuda:2'), Shrink Mask:tensor([ True, False,  True,  True, False], device='cuda:2'), New Tokens:tensor([   0,  419,    0, 5186,    0], device='cuda:2')
Mixup Sent Mask:tensor([[1],
        [3]], device='cuda:2'), 1,  Mixup Mask:tensor([False, False, False,  True, False], device='cuda:2'), 
                    Org X:tensor([[-1.4636e-01,  2.1387e-01,  1.1768e+00,  ..., -5.0781e-01,
          1.4717e+00,  5.0049e-01],
        [-6.5137e-01,  1.8232e+00, -1.3525e+00,  ...,  1.0425e-01,
          9.5020e-01,  1.1602e+00],
        [-9.0527e-01,  1.4639e+00,  6.3916e-01,  ..., -6.4087e-02,
          1.4902e+00,  1.2119e+00],
        [ 9.1797e-01,  2.9004e-01,  2.8203e+00,  ..., -5.9619e-01,
          5.5127e-01,  5.6738e-01],
        [ 2.3262e+00,  3.6987e-01,  3.5078e+00,  ..., -3.2363e+00,
          3.7280e-01,  3.1738e-03]], device='cuda:2', dtype=torch.float16,
       grad_fn=<SliceBackward0>), New X:tensor([[-1.4636e-01,  2.1387e-01,  1.1768e+00,  ..., -5.0781e-01,
          1.4717e+00,  5.0049e-01],
        [-6.5137e-01,  1.8232e+00, -1.3525e+00,  ...,  1.0425e-01,
          9.5020e-01,  1.1602e+00],
        [-9.0527e-01,  1.4639e+00,  6.3916e-01,  ..., -6.4087e-02,
          1.4902e+00,  1.2119e+00],
        [ 1.6729e+00,  4.6021e-01,  6.1230e-01,  ..., -3.1689e-01,
         -5.3369e-01, -1.3447e+00],
        [ 2.3262e+00,  3.6987e-01,  3.5078e+00,  ..., -3.2363e+00,
          3.7280e-01,  3.1738e-03]], device='cuda:2', dtype=torch.float16,
       grad_fn=<SliceBackward0>), Emb: tensor([[-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [-0.6963,  0.4395, -0.1163,  ...,  0.8433,  0.6738, -1.9971],
        [-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [ 1.6729,  0.4602,  0.6123,  ..., -0.3169, -0.5337, -1.3447],
        [-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([128, 14]), X shape:torch.Size([128, 14, 512])
CTC Tokens:tensor([  8,   0, 568,   0,  17], device='cuda:4'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:4'), New Tokens:tensor([  8,   0, 568,   0,  17], device='cuda:4')
Mixup Sent Mask:tensor([[  1],
        [  3],
        [  9],
        [ 12],
        [ 14],
        [ 19],
        [ 21],
        [ 23],
        [ 24],
        [ 29],
        [ 35],
        [ 36],
        [ 37],
        [ 46],
        [ 47],
        [ 49],
        [ 52],
        [ 55],
        [ 57],
        [ 60],
        [ 62],
        [ 65],
        [ 70],
        [ 75],
        [ 77],
        [ 80],
        [ 83],
        [ 84],
        [ 91],
        [ 94],
        [ 97],
        [ 99],
        [101],
        [104],
        [105],
        [106],
        [111],
        [114],
        [117],
        [124],
        [125]], device='cuda:4'), 1,  Mixup Mask:tensor([ True,  True,  True,  True, False], device='cuda:4'), 
                    Org X:tensor([[ 0.2158, -1.1230,  0.2876,  ...,  0.0403,  0.3591, -0.2520],
        [-1.3359, -0.3914,  2.1133,  ..., -0.4404, -0.8857,  0.5479],
        [-1.9492,  0.1547,  0.6401,  ..., -0.0991, -0.8374, -0.0472],
        [-1.8428,  0.4250,  1.2461,  ..., -1.5586,  0.0323, -0.0527],
        [-1.9551,  0.4094, -1.5791,  ..., -0.8701,  1.0957, -1.1240]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.5615, -0.0246, -0.5820,  ...,  1.2412, -1.6475, -1.5938],
        [-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [-0.0707,  0.2803,  0.4829,  ..., -0.0886, -0.6406, -1.9775],
        [-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [-1.9551,  0.4094, -1.5791,  ..., -0.8701,  1.0957, -1.1240]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5615, -0.0246, -0.5820,  ...,  1.2412, -1.6475, -1.5938],
        [-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [-0.0707,  0.2803,  0.4829,  ..., -0.0886, -0.6406, -1.9775],
        [-0.3745, -0.8701,  0.3088,  ..., -2.6660, -1.6035, -1.7881],
        [-0.2264, -0.0274, -0.4475,  ..., -0.0574, -1.2861, -2.1738]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:4')
2023-08-08 09:30:12 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 4.177 | trans_loss 5.546 | nll_loss 2.816 | w2v_ctc_loss 1.32 | task_loss 4.605 | contrastive_loss 0.245 | total 4003.4 | n_correct 2498.9 | ppl 7.04 | accuracy 62.419 | uer 16.715 | wer 18.564 | raw_wer 18.564 | bleu 20.32 | wps 2163.2 | wpb 4003.4 | bsz 141.8 | num_updates 41254 | best_bleu 20.39
2023-08-08 09:30:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 41254 updates
2023-08-08 09:30:12 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.3200.pt
2023-08-08 09:30:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.3200.pt
2023-08-08 09:30:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.3200.pt (epoch 28 @ 41254 updates, score 20.32) (writing took 14.23803093098104 seconds)
2023-08-08 09:30:26 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-08-08 09:30:26 | INFO | train | epoch 028 | loss 1.953 | trans_loss 4.965 | nll_loss 2.152 | w2v_ctc_loss 0.606 | task_loss 1.401 | contrastive_loss 0.097 | total 4137.55 | n_correct 2741.15 | ppl 4.44 | accuracy 66.251 | wps 10861.7 | ups 1.31 | wpb 8275.1 | bsz 305.2 | num_updates 41254 | lr 6.96277e-05 | gnorm 0.528 | clip 0 | loss_scale 64 | train_wall 1013 | gb_free 16.3 | wall 33921
2023-08-08 09:30:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 09:30:27 | INFO | fairseq.trainer | begin training epoch 29
2023-08-08 09:30:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 09:31:06 | INFO | train_inner | epoch 029:     46 / 1474 loss=1.944, trans_loss=4.95, nll_loss=2.133, w2v_ctc_loss=0.607, task_loss=1.344, contrastive_loss=0.072, total=4169.12, n_correct=2776.07, ppl=4.39, accuracy=66.586, wps=7259.5, ups=0.87, wpb=8338.2, bsz=316.4, num_updates=41300, lr=6.95889e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=68, gb_free=16.2, wall=33961
2023-08-08 09:32:15 | INFO | train_inner | epoch 029:    146 / 1474 loss=1.945, trans_loss=4.949, nll_loss=2.13, w2v_ctc_loss=0.603, task_loss=1.394, contrastive_loss=0.09, total=4105.72, n_correct=2733.43, ppl=4.38, accuracy=66.576, wps=11842.4, ups=1.44, wpb=8211.4, bsz=304.1, num_updates=41400, lr=6.95048e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=69, gb_free=15.8, wall=34030
2023-08-08 09:33:25 | INFO | train_inner | epoch 029:    246 / 1474 loss=1.938, trans_loss=4.939, nll_loss=2.118, w2v_ctc_loss=0.586, task_loss=1.276, contrastive_loss=0.174, total=4199.67, n_correct=2805.67, ppl=4.34, accuracy=66.807, wps=12045, ups=1.43, wpb=8399.3, bsz=330.5, num_updates=41500, lr=6.9421e-05, gnorm=0.52, clip=0, loss_scale=64, train_wall=69, gb_free=15.5, wall=34100
2023-08-08 09:34:35 | INFO | train_inner | epoch 029:    346 / 1474 loss=1.951, trans_loss=4.962, nll_loss=2.146, w2v_ctc_loss=0.61, task_loss=1.498, contrastive_loss=0.055, total=4095.17, n_correct=2718.67, ppl=4.43, accuracy=66.387, wps=11761.6, ups=1.44, wpb=8190.3, bsz=291.2, num_updates=41600, lr=6.93375e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=69, gb_free=16.6, wall=34170
2023-08-08 09:35:44 | INFO | train_inner | epoch 029:    446 / 1474 loss=1.932, trans_loss=4.937, nll_loss=2.113, w2v_ctc_loss=0.593, task_loss=1.349, contrastive_loss=0.047, total=4157.44, n_correct=2780.18, ppl=4.33, accuracy=66.872, wps=12072.6, ups=1.45, wpb=8314.9, bsz=307.7, num_updates=41700, lr=6.92543e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=68, gb_free=16.3, wall=34238
2023-08-08 09:36:53 | INFO | train_inner | epoch 029:    546 / 1474 loss=1.956, trans_loss=4.962, nll_loss=2.147, w2v_ctc_loss=0.6, task_loss=1.489, contrastive_loss=0.143, total=4150.87, n_correct=2751.35, ppl=4.43, accuracy=66.284, wps=11939.8, ups=1.44, wpb=8301.7, bsz=294.9, num_updates=41800, lr=6.91714e-05, gnorm=0.526, clip=0, loss_scale=64, train_wall=69, gb_free=15.6, wall=34308
2023-08-08 09:38:02 | INFO | train_inner | epoch 029:    646 / 1474 loss=1.947, trans_loss=4.951, nll_loss=2.135, w2v_ctc_loss=0.594, task_loss=1.324, contrastive_loss=0.215, total=4143.02, n_correct=2756.94, ppl=4.39, accuracy=66.544, wps=12003.9, ups=1.45, wpb=8286, bsz=318.6, num_updates=41900, lr=6.90889e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=68, gb_free=16.9, wall=34377
2023-08-08 09:39:13 | INFO | train_inner | epoch 029:    746 / 1474 loss=1.943, trans_loss=4.951, nll_loss=2.133, w2v_ctc_loss=0.593, task_loss=1.291, contrastive_loss=0.133, total=4249.79, n_correct=2827.16, ppl=4.39, accuracy=66.525, wps=12069.7, ups=1.42, wpb=8499.6, bsz=330, num_updates=42000, lr=6.90066e-05, gnorm=0.523, clip=0, loss_scale=64, train_wall=70, gb_free=16.5, wall=34447
2023-08-08 09:39:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 09:39:37 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 4.191 | trans_loss 5.545 | nll_loss 2.811 | w2v_ctc_loss 1.368 | task_loss 4.633 | contrastive_loss 0.248 | total 4003.4 | n_correct 2500.4 | ppl 7.02 | accuracy 62.457 | uer 16.853 | wer 18.653 | raw_wer 18.653 | bleu 20.33 | wps 2099.1 | wpb 4003.4 | bsz 141.8 | num_updates 42000 | best_bleu 20.39
2023-08-08 09:39:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42000 updates
2023-08-08 09:39:37 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_29_42000.pt
2023-08-08 09:39:40 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_29_42000.pt
2023-08-08 09:40:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_29_42000.pt (epoch 29 @ 42000 updates, score 20.33) (writing took 32.8806355278939 seconds)
2023-08-08 09:41:19 | INFO | train_inner | epoch 029:    846 / 1474 loss=1.956, trans_loss=4.975, nll_loss=2.163, w2v_ctc_loss=0.606, task_loss=1.554, contrastive_loss=0.048, total=4027.19, n_correct=2660.19, ppl=4.48, accuracy=66.056, wps=6362.3, ups=0.79, wpb=8054.4, bsz=280.6, num_updates=42100, lr=6.89246e-05, gnorm=0.538, clip=0, loss_scale=128, train_wall=68, gb_free=17.1, wall=34574
2023-08-08 09:42:28 | INFO | train_inner | epoch 029:    946 / 1474 loss=1.953, trans_loss=4.972, nll_loss=2.16, w2v_ctc_loss=0.608, task_loss=1.423, contrastive_loss=0.059, total=4082.14, n_correct=2702.65, ppl=4.47, accuracy=66.207, wps=11883.4, ups=1.46, wpb=8164.3, bsz=296.3, num_updates=42200, lr=6.88428e-05, gnorm=0.532, clip=0, loss_scale=128, train_wall=68, gb_free=15.2, wall=34643
2023-08-08 09:43:37 | INFO | train_inner | epoch 029:   1046 / 1474 loss=1.946, trans_loss=4.957, nll_loss=2.142, w2v_ctc_loss=0.594, task_loss=1.392, contrastive_loss=0.131, total=4148.18, n_correct=2758, ppl=4.41, accuracy=66.487, wps=11971.3, ups=1.44, wpb=8296.4, bsz=308.2, num_updates=42300, lr=6.87614e-05, gnorm=0.523, clip=0, loss_scale=128, train_wall=69, gb_free=15.8, wall=34712
2023-08-08 09:44:47 | INFO | train_inner | epoch 029:   1146 / 1474 loss=1.954, trans_loss=4.975, nll_loss=2.164, w2v_ctc_loss=0.609, task_loss=1.529, contrastive_loss=0.043, total=4063.95, n_correct=2689.51, ppl=4.48, accuracy=66.18, wps=11715.4, ups=1.44, wpb=8127.9, bsz=283, num_updates=42400, lr=6.86803e-05, gnorm=0.528, clip=0, loss_scale=128, train_wall=69, gb_free=13.1, wall=34781
2023-08-08 09:45:56 | INFO | train_inner | epoch 029:   1246 / 1474 loss=1.952, trans_loss=4.973, nll_loss=2.162, w2v_ctc_loss=0.607, task_loss=1.417, contrastive_loss=0.052, total=4158.81, n_correct=2752.42, ppl=4.47, accuracy=66.183, wps=12022.4, ups=1.45, wpb=8317.6, bsz=301.2, num_updates=42500, lr=6.85994e-05, gnorm=0.525, clip=0, loss_scale=128, train_wall=69, gb_free=15.5, wall=34851
2023-08-08 09:47:05 | INFO | train_inner | epoch 029:   1346 / 1474 loss=1.951, trans_loss=4.965, nll_loss=2.153, w2v_ctc_loss=0.598, task_loss=1.375, contrastive_loss=0.117, total=4166.34, n_correct=2758.65, ppl=4.45, accuracy=66.213, wps=11962.4, ups=1.44, wpb=8332.7, bsz=310.7, num_updates=42600, lr=6.85189e-05, gnorm=0.523, clip=0, loss_scale=128, train_wall=69, gb_free=17.7, wall=34920
2023-08-08 09:48:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-08 09:48:15 | INFO | train_inner | epoch 029:   1447 / 1474 loss=1.95, trans_loss=4.965, nll_loss=2.153, w2v_ctc_loss=0.608, task_loss=1.401, contrastive_loss=0.054, total=4145.9, n_correct=2744.97, ppl=4.45, accuracy=66.209, wps=11914.3, ups=1.44, wpb=8291.8, bsz=305.1, num_updates=42700, lr=6.84386e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=69, gb_free=16.7, wall=34990
2023-08-08 09:48:33 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 09:48:56 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 4.183 | trans_loss 5.544 | nll_loss 2.811 | w2v_ctc_loss 1.343 | task_loss 4.628 | contrastive_loss 0.246 | total 4003.4 | n_correct 2503.6 | ppl 7.02 | accuracy 62.537 | uer 16.771 | wer 18.683 | raw_wer 18.683 | bleu 20.23 | wps 2278.3 | wpb 4003.4 | bsz 141.8 | num_updates 42727 | best_bleu 20.39
2023-08-08 09:48:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42727 updates
2023-08-08 09:48:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.2308.pt
2023-08-08 09:48:59 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.2308.pt
2023-08-08 09:49:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.2308.pt (epoch 29 @ 42727 updates, score 20.23) (writing took 14.117687873542309 seconds)
2023-08-08 09:49:11 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-08-08 09:49:11 | INFO | train | epoch 029 | loss 1.947 | trans_loss 4.958 | nll_loss 2.143 | w2v_ctc_loss 0.6 | task_loss 1.401 | contrastive_loss 0.098 | total 4137.15 | n_correct 2747.63 | ppl 4.42 | accuracy 66.414 | wps 10837.9 | ups 1.31 | wpb 8274.3 | bsz 305.1 | num_updates 42727 | lr 6.8417e-05 | gnorm 0.529 | clip 0 | loss_scale 64 | train_wall 1013 | gb_free 15.9 | wall 35046
2023-08-08 09:49:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 09:49:11 | INFO | fairseq.trainer | begin training epoch 30
2023-08-08 09:49:11 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 09:50:09 | INFO | train_inner | epoch 030:     73 / 1474 loss=1.94, trans_loss=4.943, nll_loss=2.122, w2v_ctc_loss=0.589, task_loss=1.332, contrastive_loss=0.16, total=4175.11, n_correct=2783.94, ppl=4.35, accuracy=66.679, wps=7325.9, ups=0.88, wpb=8350.2, bsz=318.6, num_updates=42800, lr=6.83586e-05, gnorm=0.526, clip=0, loss_scale=64, train_wall=69, gb_free=17, wall=35104
2023-08-08 09:51:18 | INFO | train_inner | epoch 030:    173 / 1474 loss=1.933, trans_loss=4.926, nll_loss=2.102, w2v_ctc_loss=0.596, task_loss=1.31, contrastive_loss=0.095, total=4202.64, n_correct=2817.26, ppl=4.29, accuracy=67.035, wps=12146.4, ups=1.45, wpb=8405.3, bsz=318.3, num_updates=42900, lr=6.82789e-05, gnorm=0.524, clip=0, loss_scale=64, train_wall=69, gb_free=16.5, wall=35173
2023-08-08 09:52:27 | INFO | train_inner | epoch 030:    273 / 1474 loss=1.94, trans_loss=4.942, nll_loss=2.121, w2v_ctc_loss=0.604, task_loss=1.444, contrastive_loss=0.046, total=4120.21, n_correct=2747.43, ppl=4.35, accuracy=66.682, wps=12017, ups=1.46, wpb=8240.4, bsz=294.9, num_updates=43000, lr=6.81994e-05, gnorm=0.53, clip=0, loss_scale=64, train_wall=68, gb_free=15.2, wall=35242
2023-08-08 09:53:36 | INFO | train_inner | epoch 030:    373 / 1474 loss=1.933, trans_loss=4.937, nll_loss=2.115, w2v_ctc_loss=0.592, task_loss=1.396, contrastive_loss=0.051, total=4178.23, n_correct=2795.62, ppl=4.33, accuracy=66.909, wps=11986.3, ups=1.43, wpb=8356.5, bsz=307.5, num_updates=43100, lr=6.81203e-05, gnorm=0.527, clip=0, loss_scale=64, train_wall=69, gb_free=9.8, wall=35311
2023-08-08 09:54:45 | INFO | train_inner | epoch 030:    473 / 1474 loss=1.937, trans_loss=4.942, nll_loss=2.122, w2v_ctc_loss=0.589, task_loss=1.34, contrastive_loss=0.115, total=4124.47, n_correct=2753.35, ppl=4.35, accuracy=66.756, wps=12061.5, ups=1.46, wpb=8248.9, bsz=312.6, num_updates=43200, lr=6.80414e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=68, gb_free=17.5, wall=35380
2023-08-08 09:55:54 | INFO | train_inner | epoch 030:    573 / 1474 loss=1.939, trans_loss=4.948, nll_loss=2.13, w2v_ctc_loss=0.594, task_loss=1.36, contrastive_loss=0.077, total=4168.41, n_correct=2777.81, ppl=4.38, accuracy=66.64, wps=12023.2, ups=1.44, wpb=8336.8, bsz=312.4, num_updates=43300, lr=6.79628e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=69, gb_free=17.1, wall=35449
2023-08-08 09:57:03 | INFO | train_inner | epoch 030:    673 / 1474 loss=1.945, trans_loss=4.951, nll_loss=2.134, w2v_ctc_loss=0.602, task_loss=1.381, contrastive_loss=0.092, total=4187.95, n_correct=2783.85, ppl=4.39, accuracy=66.473, wps=12110.2, ups=1.45, wpb=8375.9, bsz=315, num_updates=43400, lr=6.78844e-05, gnorm=0.531, clip=0, loss_scale=64, train_wall=69, gb_free=15.7, wall=35518
2023-08-08 09:58:13 | INFO | train_inner | epoch 030:    773 / 1474 loss=1.959, trans_loss=4.964, nll_loss=2.15, w2v_ctc_loss=0.611, task_loss=1.431, contrastive_loss=0.172, total=4105.32, n_correct=2718.38, ppl=4.44, accuracy=66.216, wps=11810.8, ups=1.44, wpb=8210.6, bsz=302.6, num_updates=43500, lr=6.78064e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=69, gb_free=12.8, wall=35588
2023-08-08 09:59:22 | INFO | train_inner | epoch 030:    873 / 1474 loss=1.945, trans_loss=4.959, nll_loss=2.143, w2v_ctc_loss=0.596, task_loss=1.443, contrastive_loss=0.061, total=4102.11, n_correct=2723.31, ppl=4.42, accuracy=66.388, wps=11883.6, ups=1.45, wpb=8204.2, bsz=295.6, num_updates=43600, lr=6.77285e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=68, gb_free=17.3, wall=35657
2023-08-08 10:00:31 | INFO | train_inner | epoch 030:    973 / 1474 loss=1.947, trans_loss=4.962, nll_loss=2.148, w2v_ctc_loss=0.601, task_loss=1.433, contrastive_loss=0.063, total=4129.98, n_correct=2740.16, ppl=4.43, accuracy=66.348, wps=11955.2, ups=1.45, wpb=8260, bsz=300.4, num_updates=43700, lr=6.7651e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=69, gb_free=16.2, wall=35726
2023-08-08 10:01:41 | INFO | train_inner | epoch 030:   1073 / 1474 loss=1.96, trans_loss=4.971, nll_loss=2.157, w2v_ctc_loss=0.607, task_loss=1.566, contrastive_loss=0.139, total=4101.17, n_correct=2709.08, ppl=4.46, accuracy=66.056, wps=11676.2, ups=1.42, wpb=8202.3, bsz=282.3, num_updates=43800, lr=6.75737e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=70, gb_free=15.5, wall=35796
2023-08-08 10:02:51 | INFO | train_inner | epoch 030:   1173 / 1474 loss=1.94, trans_loss=4.955, nll_loss=2.14, w2v_ctc_loss=0.584, task_loss=1.347, contrastive_loss=0.122, total=4168.36, n_correct=2772.86, ppl=4.41, accuracy=66.522, wps=12029.2, ups=1.44, wpb=8336.7, bsz=314, num_updates=43900, lr=6.74967e-05, gnorm=0.526, clip=0, loss_scale=64, train_wall=69, gb_free=15.8, wall=35865
2023-08-08 10:04:00 | INFO | train_inner | epoch 030:   1273 / 1474 loss=1.951, trans_loss=4.967, nll_loss=2.153, w2v_ctc_loss=0.605, task_loss=1.544, contrastive_loss=0.056, total=4036.17, n_correct=2673.53, ppl=4.45, accuracy=66.239, wps=11699.7, ups=1.45, wpb=8072.3, bsz=284.3, num_updates=44000, lr=6.742e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=68, gb_free=15.6, wall=35934
2023-08-08 10:04:00 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 10:04:23 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 4.191 | trans_loss 5.544 | nll_loss 2.811 | w2v_ctc_loss 1.372 | task_loss 4.623 | contrastive_loss 0.248 | total 4003.4 | n_correct 2503.4 | ppl 7.02 | accuracy 62.532 | uer 16.741 | wer 18.527 | raw_wer 18.527 | bleu 20.18 | wps 2170.4 | wpb 4003.4 | bsz 141.8 | num_updates 44000 | best_bleu 20.39
2023-08-08 10:04:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44000 updates
2023-08-08 10:04:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_30_44000.pt
2023-08-08 10:04:27 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_30_44000.pt
2023-08-08 10:04:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_30_44000.pt (epoch 30 @ 44000 updates, score 20.18) (writing took 32.25491043739021 seconds)
2023-08-08 10:06:05 | INFO | train_inner | epoch 030:   1373 / 1474 loss=1.938, trans_loss=4.956, nll_loss=2.142, w2v_ctc_loss=0.594, task_loss=1.32, contrastive_loss=0.068, total=4165.07, n_correct=2773.78, ppl=4.41, accuracy=66.596, wps=6652, ups=0.8, wpb=8330.1, bsz=321.6, num_updates=44100, lr=6.73435e-05, gnorm=0.528, clip=0, loss_scale=64, train_wall=68, gb_free=16.5, wall=36060
2023-08-08 10:07:14 | INFO | train_inner | epoch 030:   1473 / 1474 loss=1.947, trans_loss=4.96, nll_loss=2.147, w2v_ctc_loss=0.586, task_loss=1.318, contrastive_loss=0.211, total=4141.76, n_correct=2753.15, ppl=4.43, accuracy=66.473, wps=11990, ups=1.45, wpb=8283.5, bsz=314.3, num_updates=44200, lr=6.72673e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=69, gb_free=16.2, wall=36129
2023-08-08 10:07:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 10:07:38 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 4.185 | trans_loss 5.545 | nll_loss 2.81 | w2v_ctc_loss 1.35 | task_loss 4.645 | contrastive_loss 0.243 | total 4003.4 | n_correct 2504.7 | ppl 7.01 | accuracy 62.564 | uer 16.967 | wer 18.896 | raw_wer 18.896 | bleu 20.49 | wps 2258.8 | wpb 4003.4 | bsz 141.8 | num_updates 44201 | best_bleu 20.49
2023-08-08 10:07:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44201 updates
2023-08-08 10:07:38 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 10:07:50 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt
2023-08-08 10:08:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_best.pt (epoch 30 @ 44201 updates, score 20.49) (writing took 23.914292665198445 seconds)
2023-08-08 10:08:02 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-08-08 10:08:02 | INFO | train | epoch 030 | loss 1.944 | trans_loss 4.952 | nll_loss 2.135 | w2v_ctc_loss 0.597 | task_loss 1.398 | contrastive_loss 0.103 | total 4138.65 | n_correct 2753.76 | ppl 4.39 | accuracy 66.538 | wps 10784.9 | ups 1.3 | wpb 8277.3 | bsz 305.7 | num_updates 44201 | lr 6.72665e-05 | gnorm 0.53 | clip 0 | loss_scale 64 | train_wall 1012 | gb_free 17 | wall 36177
2023-08-08 10:08:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 10:08:02 | INFO | fairseq.trainer | begin training epoch 31
2023-08-08 10:08:02 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 10:09:18 | INFO | train_inner | epoch 031:     99 / 1474 loss=1.935, trans_loss=4.933, nll_loss=2.109, w2v_ctc_loss=0.598, task_loss=1.492, contrastive_loss=0.049, total=4054.44, n_correct=2714.15, ppl=4.31, accuracy=66.943, wps=6529.6, ups=0.81, wpb=8108.9, bsz=288.2, num_updates=44300, lr=6.71913e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=68, gb_free=16.3, wall=36253
2023-08-08 10:10:27 | INFO | train_inner | epoch 031:    199 / 1474 loss=1.936, trans_loss=4.937, nll_loss=2.114, w2v_ctc_loss=0.596, task_loss=1.429, contrastive_loss=0.078, total=4147.4, n_correct=2774.28, ppl=4.33, accuracy=66.892, wps=11960.4, ups=1.44, wpb=8294.8, bsz=302.2, num_updates=44400, lr=6.71156e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=69, gb_free=16.7, wall=36322
2023-08-08 10:11:37 | INFO | train_inner | epoch 031:    299 / 1474 loss=1.937, trans_loss=4.936, nll_loss=2.113, w2v_ctc_loss=0.589, task_loss=1.428, contrastive_loss=0.115, total=4149.21, n_correct=2777.55, ppl=4.33, accuracy=66.942, wps=11948.2, ups=1.44, wpb=8298.4, bsz=301.6, num_updates=44500, lr=6.70402e-05, gnorm=0.528, clip=0, loss_scale=64, train_wall=69, gb_free=16, wall=36392
2023-08-08 10:12:46 | INFO | train_inner | epoch 031:    399 / 1474 loss=1.938, trans_loss=4.944, nll_loss=2.123, w2v_ctc_loss=0.594, task_loss=1.526, contrastive_loss=0.052, total=4092.62, n_correct=2734.23, ppl=4.36, accuracy=66.809, wps=11826.3, ups=1.44, wpb=8185.2, bsz=285.6, num_updates=44600, lr=6.6965e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=17.1, wall=36461
2023-08-08 10:13:56 | INFO | train_inner | epoch 031:    499 / 1474 loss=1.941, trans_loss=4.943, nll_loss=2.122, w2v_ctc_loss=0.605, task_loss=1.462, contrastive_loss=0.063, total=4111.85, n_correct=2740.09, ppl=4.35, accuracy=66.639, wps=11845.9, ups=1.44, wpb=8223.7, bsz=300.3, num_updates=44700, lr=6.689e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=69, gb_free=10.7, wall=36530
2023-08-08 10:15:05 | INFO | train_inner | epoch 031:    599 / 1474 loss=1.934, trans_loss=4.941, nll_loss=2.12, w2v_ctc_loss=0.588, task_loss=1.46, contrastive_loss=0.053, total=4083.44, n_correct=2727.96, ppl=4.35, accuracy=66.805, wps=11724.9, ups=1.44, wpb=8166.9, bsz=294.5, num_updates=44800, lr=6.68153e-05, gnorm=0.53, clip=0, loss_scale=128, train_wall=69, gb_free=16.7, wall=36600
2023-08-08 10:16:14 | INFO | train_inner | epoch 031:    699 / 1474 loss=1.927, trans_loss=4.937, nll_loss=2.115, w2v_ctc_loss=0.583, task_loss=1.332, contrastive_loss=0.053, total=4213.98, n_correct=2820.89, ppl=4.33, accuracy=66.941, wps=12211.8, ups=1.45, wpb=8428, bsz=315.7, num_updates=44900, lr=6.67409e-05, gnorm=0.518, clip=0, loss_scale=128, train_wall=69, gb_free=16.1, wall=36669
2023-08-08 10:16:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-08 10:17:25 | INFO | train_inner | epoch 031:    800 / 1474 loss=1.944, trans_loss=4.952, nll_loss=2.134, w2v_ctc_loss=0.592, task_loss=1.479, contrastive_loss=0.106, total=4083.52, n_correct=2715.1, ppl=4.39, accuracy=66.489, wps=11624.1, ups=1.42, wpb=8167, bsz=292.5, num_updates=45000, lr=6.66667e-05, gnorm=0.531, clip=0, loss_scale=64, train_wall=70, gb_free=15.2, wall=36739
Mixup rate:0.5, token after shrink shape:torch.Size([24, 52]), X shape:torch.Size([24, 52, 512])
CTC Tokens:tensor([ 21,  21,  11,   6, 115], device='cuda:0'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:0'), New Tokens:tensor([ 21,  11,   6, 115,  39], device='cuda:0')
Mixup Sent Mask:tensor([[ 2],
        [ 3],
        [ 8],
        [11],
        [12],
        [13],
        [15],
        [16],
        [20]], device='cuda:0'), 2,  Mixup Mask:tensor([False, False,  True,  True,  True], device='cuda:0'), 
                    Org X:tensor([[-0.0186, -1.5332, -0.9761,  ...,  0.6484, -1.0293, -0.3994],
        [ 0.6558, -0.6812, -0.9897,  ..., -0.5635, -0.2976,  0.0766],
        [ 1.3086, -0.1322, -0.4275,  ..., -2.9902, -0.3108,  0.1802],
        [ 0.6880,  0.0975,  0.3320,  ..., -0.8540, -0.8164,  0.4888],
        [ 0.5757, -0.0130, -1.5664,  ..., -0.4966, -0.8105,  0.7222]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.0186, -1.5332, -0.9761,  ...,  0.6484, -1.0293, -0.3994],
        [ 0.6558, -0.6812, -0.9897,  ..., -0.5635, -0.2976,  0.0766],
        [ 0.0704, -0.4270, -0.1013,  ..., -3.0410, -3.1797,  0.9214],
        [-0.6289, -0.2246,  0.0788,  ...,  1.3916, -1.5371, -0.8540],
        [-1.2070, -0.5698,  1.2598,  ..., -1.2598, -2.2754,  0.5610]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5488, -0.1854, -0.4153,  ..., -1.0908, -3.0547, -1.7295],
        [-0.3535, -0.2502, -0.3584,  ..., -2.3906, -0.1929, -4.2031],
        [ 0.0704, -0.4270, -0.1013,  ..., -3.0410, -3.1797,  0.9214],
        [-0.6289, -0.2246,  0.0788,  ...,  1.3916, -1.5371, -0.8540],
        [-1.2070, -0.5698,  1.2598,  ..., -1.2598, -2.2754,  0.5610]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:0')
2023-08-08 10:18:34 | INFO | train_inner | epoch 031:    900 / 1474 loss=1.94, trans_loss=4.944, nll_loss=2.125, w2v_ctc_loss=0.596, task_loss=1.474, contrastive_loss=0.068, total=4099.13, n_correct=2732.11, ppl=4.36, accuracy=66.651, wps=11856, ups=1.45, wpb=8198.3, bsz=294.3, num_updates=45100, lr=6.65927e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=69, gb_free=15, wall=36809
2023-08-08 10:19:43 | INFO | train_inner | epoch 031:   1000 / 1474 loss=1.942, trans_loss=4.953, nll_loss=2.137, w2v_ctc_loss=0.589, task_loss=1.304, contrastive_loss=0.152, total=4186.81, n_correct=2789.29, ppl=4.4, accuracy=66.621, wps=12057.3, ups=1.44, wpb=8373.6, bsz=320.9, num_updates=45200, lr=6.6519e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=69, gb_free=13.2, wall=36878
2023-08-08 10:20:52 | INFO | train_inner | epoch 031:   1100 / 1474 loss=1.939, trans_loss=4.949, nll_loss=2.132, w2v_ctc_loss=0.59, task_loss=1.364, contrastive_loss=0.1, total=4149.25, n_correct=2764.68, ppl=4.38, accuracy=66.631, wps=12045.3, ups=1.45, wpb=8298.5, bsz=315.2, num_updates=45300, lr=6.64455e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=68, gb_free=15.8, wall=36947
2023-08-08 10:22:01 | INFO | train_inner | epoch 031:   1200 / 1474 loss=1.945, trans_loss=4.954, nll_loss=2.139, w2v_ctc_loss=0.586, task_loss=1.317, contrastive_loss=0.215, total=4187.45, n_correct=2789.3, ppl=4.41, accuracy=66.611, wps=12214.5, ups=1.46, wpb=8374.9, bsz=320.3, num_updates=45400, lr=6.63723e-05, gnorm=0.53, clip=0, loss_scale=64, train_wall=68, gb_free=16.9, wall=37015
2023-08-08 10:23:10 | INFO | train_inner | epoch 031:   1300 / 1474 loss=1.936, trans_loss=4.954, nll_loss=2.139, w2v_ctc_loss=0.591, task_loss=1.252, contrastive_loss=0.059, total=4227.39, n_correct=2817.06, ppl=4.4, accuracy=66.638, wps=12254.5, ups=1.45, wpb=8454.8, bsz=326.7, num_updates=45500, lr=6.62994e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=69, gb_free=16.8, wall=37084
2023-08-08 10:24:19 | INFO | train_inner | epoch 031:   1400 / 1474 loss=1.952, trans_loss=4.953, nll_loss=2.138, w2v_ctc_loss=0.592, task_loss=1.278, contrastive_loss=0.262, total=4191.1, n_correct=2789.39, ppl=4.4, accuracy=66.555, wps=12008, ups=1.43, wpb=8382.2, bsz=327, num_updates=45600, lr=6.62266e-05, gnorm=0.53, clip=0, loss_scale=64, train_wall=69, gb_free=16.4, wall=37154
2023-08-08 10:25:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([8, 102]), X shape:torch.Size([8, 102, 512])
CTC Tokens:tensor([ 21,   0,   6,   6, 226], device='cuda:7'), Shrink Mask:tensor([ True,  True,  True, False,  True], device='cuda:7'), New Tokens:tensor([ 21,   0,   6, 226,  39], device='cuda:7')
Mixup Sent Mask:tensor([[2],
        [3]], device='cuda:7'), 2,  Mixup Mask:tensor([ True, False,  True, False, False], device='cuda:7'), 
                    Org X:tensor([[-8.2886e-02, -1.8008e+00, -2.0898e+00,  ..., -2.6855e-01,
         -1.1729e+00,  8.7158e-01],
        [ 2.5781e-01, -1.0312e+00, -9.3604e-01,  ..., -1.0664e+00,
          3.7012e-01,  1.7822e-01],
        [ 4.4971e-01, -3.6621e-04, -2.0430e+00,  ..., -6.7285e-01,
          3.4595e-01,  3.0713e-01],
        [-3.8672e-01,  5.4688e-02, -8.3643e-01,  ...,  4.3640e-02,
          6.9031e-02,  1.5161e-01],
        [-1.6562e+00,  9.7559e-01,  3.3008e-01,  ..., -1.5430e+00,
          6.0693e-01,  4.4800e-01]], device='cuda:7', dtype=torch.float16,
       grad_fn=<SliceBackward0>), New X:tensor([[-0.5488, -0.1854, -0.4153,  ..., -1.0908, -3.0547, -1.7295],
        [ 0.2578, -1.0312, -0.9360,  ..., -1.0664,  0.3701,  0.1782],
        [ 0.0704, -0.4270, -0.1013,  ..., -3.0410, -3.1797,  0.9214],
        [-0.3867,  0.0547, -0.8364,  ...,  0.0436,  0.0690,  0.1516],
        [-1.6562,  0.9756,  0.3301,  ..., -1.5430,  0.6069,  0.4480]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5488, -0.1854, -0.4153,  ..., -1.0908, -3.0547, -1.7295],
        [-0.4336, -0.8516,  0.2930,  ..., -2.6387, -1.5713, -1.8164],
        [ 0.0704, -0.4270, -0.1013,  ..., -3.0410, -3.1797,  0.9214],
        [ 1.4297, -0.8130,  0.2056,  ..., -0.8589,  0.1289,  0.5815],
        [-1.2070, -0.5698,  1.2598,  ..., -1.2598, -2.2754,  0.5610]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 72]), X shape:torch.Size([16, 72, 512])
CTC Tokens:tensor([ 19, 246,   4,  38, 597], device='cuda:1'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:1'), New Tokens:tensor([ 19, 246,   4,  38, 597], device='cuda:1')
Mixup Sent Mask:tensor([[ 2],
        [ 3],
        [ 8],
        [11],
        [12],
        [13],
        [15]], device='cuda:1'), 2,  Mixup Mask:tensor([False, False, False, False, False], device='cuda:1'), 
                    Org X:tensor([[ 0.5239, -0.6216,  0.1752,  ...,  1.0723,  1.0869, -0.6978],
        [ 0.6177,  0.0366, -0.3967,  ..., -0.2849,  0.0735, -1.7979],
        [ 0.3936,  0.2561, -1.0049,  ..., -0.0114, -0.3933, -3.0332],
        [-3.5117,  0.5254, -0.9253,  ..., -0.0208,  1.4600, -2.0273],
        [-3.1465,  0.0135, -2.0996,  ...,  0.6401,  1.0957, -0.6748]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.5239, -0.6216,  0.1752,  ...,  1.0723,  1.0869, -0.6978],
        [ 0.6177,  0.0366, -0.3967,  ..., -0.2849,  0.0735, -1.7979],
        [ 0.3936,  0.2561, -1.0049,  ..., -0.0114, -0.3933, -3.0332],
        [-3.5117,  0.5254, -0.9253,  ..., -0.0208,  1.4600, -2.0273],
        [-3.1465,  0.0135, -2.0996,  ...,  0.6401,  1.0957, -0.6748]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[ 0.3491, -0.2788, -0.1675,  ..., -0.9473,  0.2698, -1.4893],
        [ 0.2134,  0.3445, -1.4004,  ..., -1.6768, -1.2314, -2.9141],
        [-0.1603, -0.3984, -0.1809,  ..., -1.3877, -1.0146, -2.1426],
        [-1.2861, -0.5605,  0.5391,  ...,  0.5713,  0.1234, -0.6763],
        [-0.2729, -0.7104, -0.1299,  ...,  3.5215,  2.7031,  3.8535]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([56, 28]), X shape:torch.Size([56, 28, 512])
CTC Tokens:tensor([  8,   8,  25, 150, 150], device='cuda:5'), Shrink Mask:tensor([ True, False,  True,  True, False], device='cuda:5'), New Tokens:tensor([  8,  25, 150, 103,  25], device='cuda:5')
Mixup Sent Mask:tensor([[ 2],
        [ 3],
        [ 8],
        [11],
        [12],
        [13],
        [15],
        [16],
        [20],
        [31],
        [32],
        [34],
        [40],
        [43],
        [45],
        [51]], device='cuda:5'), 2,  Mixup Mask:tensor([ True,  True, False,  True, False], device='cuda:5'), 
                    Org X:tensor([[ 0.5947, -0.9600,  0.0041,  ...,  0.8320,  1.1123, -0.0995],
        [-0.0553, -0.8672, -1.2334,  ...,  0.1833,  1.5693,  0.3486],
        [ 1.3174, -0.6602,  0.2747,  ...,  0.3574,  0.0314,  0.0380],
        [-0.5625, -0.4941, -2.3594,  ...,  0.3320, -0.5479,  0.9692],
        [-0.6943, -1.4736, -2.2109,  ...,  0.2695,  0.7983,  0.1541]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.5547, -0.0454, -0.5747,  ...,  1.1621, -1.7275, -1.5801],
        [ 0.1691, -0.4885, -0.0961,  ..., -0.9966,  0.2668, -2.2773],
        [ 1.3174, -0.6602,  0.2747,  ...,  0.3574,  0.0314,  0.0380],
        [ 0.4141, -0.6699, -0.0523,  ...,  1.6016, -0.9766, -1.0000],
        [-0.6943, -1.4736, -2.2109,  ...,  0.2695,  0.7983,  0.1541]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.5547, -0.0454, -0.5747,  ...,  1.1621, -1.7275, -1.5801],
        [ 0.1691, -0.4885, -0.0961,  ..., -0.9966,  0.2668, -2.2773],
        [-0.1686, -0.4607, -0.7251,  ...,  0.9199, -1.6445, -2.7910],
        [ 0.4141, -0.6699, -0.0523,  ...,  1.6016, -0.9766, -1.0000],
        [ 0.1691, -0.4885, -0.0961,  ..., -0.9966,  0.2668, -2.2773]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 73]), X shape:torch.Size([16, 73, 512])
CTC Tokens:tensor([  0, 101,   0,  34,   0], device='cuda:2'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:2'), New Tokens:tensor([  0, 101,   0,  34,   0], device='cuda:2')
Mixup Sent Mask:tensor([[ 2],
        [ 3],
        [ 8],
        [11],
        [12],
        [13],
        [15]], device='cuda:2'), 2,  Mixup Mask:tensor([False, False, False, False, False], device='cuda:2'), 
                    Org X:tensor([[ 0.0301, -0.5576, -0.6802,  ..., -0.3833, -0.6694,  0.8418],
        [-1.8096,  0.3083, -1.4678,  ..., -1.2422,  0.5352,  0.8735],
        [ 0.7773, -0.0218,  1.0908,  ..., -2.2539, -0.7129,  0.7261],
        [-0.5591, -0.3027,  0.3918,  ..., -0.8916, -1.7256,  0.1633],
        [ 0.9941,  1.3838,  0.9067,  ..., -3.7168, -1.7451,  0.0432]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.0301, -0.5576, -0.6802,  ..., -0.3833, -0.6694,  0.8418],
        [-1.8096,  0.3083, -1.4678,  ..., -1.2422,  0.5352,  0.8735],
        [ 0.7773, -0.0218,  1.0908,  ..., -2.2539, -0.7129,  0.7261],
        [-0.5591, -0.3027,  0.3918,  ..., -0.8916, -1.7256,  0.1633],
        [ 0.9941,  1.3838,  0.9067,  ..., -3.7168, -1.7451,  0.0432]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[-0.4336, -0.8516,  0.2930,  ..., -2.6387, -1.5713, -1.8164],
        [-0.4792, -0.0234,  0.3240,  ...,  0.2639, -1.5693,  1.4941],
        [-0.4336, -0.8516,  0.2930,  ..., -2.6387, -1.5713, -1.8164],
        [-0.0479, -0.9590, -0.0155,  ..., -1.6465, -3.3340, -3.4023],
        [-0.4336, -0.8516,  0.2930,  ..., -2.6387, -1.5713, -1.8164]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 65]), X shape:torch.Size([16, 65, 512])
CTC Tokens:tensor([  19,   19,    0, 2937,    0], device='cuda:4'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:4'), New Tokens:tensor([  19,    0, 2937,    0,   62], device='cuda:4')
Mixup Sent Mask:tensor([[ 2],
        [ 3],
        [ 8],
        [11],
        [12],
        [13],
        [15]], device='cuda:4'), 2,  Mixup Mask:tensor([ True,  True, False,  True,  True], device='cuda:4'), 
                    Org X:tensor([[ 0.0720, -0.6084, -0.2095,  ...,  1.1836, -0.0442, -0.3235],
        [-0.0465,  1.2666,  1.5078,  ...,  0.2289,  1.2969, -0.6289],
        [ 0.0204,  0.6201,  1.2012,  ...,  0.0555,  1.7852, -1.7021],
        [ 2.7207, -0.1915,  3.3086,  ..., -0.9434, -0.4441, -1.7002],
        [ 3.3848, -0.2524,  2.0254,  ..., -0.2720, -2.8477, -0.7808]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.3491, -0.2788, -0.1675,  ..., -0.9473,  0.2698, -1.4893],
        [-0.4336, -0.8516,  0.2930,  ..., -2.6387, -1.5713, -1.8164],
        [ 0.0204,  0.6201,  1.2012,  ...,  0.0555,  1.7852, -1.7021],
        [-0.4336, -0.8516,  0.2930,  ..., -2.6387, -1.5713, -1.8164],
        [-0.2979, -0.1742,  0.5522,  ..., -0.9551, -3.6289, -1.5146]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[ 0.3491, -0.2788, -0.1675,  ..., -0.9473,  0.2698, -1.4893],
        [-0.4336, -0.8516,  0.2930,  ..., -2.6387, -1.5713, -1.8164],
        [ 0.2427,  0.1444,  0.1923,  ...,  2.2461,  0.4526, -2.8594],
        [-0.4336, -0.8516,  0.2930,  ..., -2.6387, -1.5713, -1.8164],
        [-0.2979, -0.1742,  0.5522,  ..., -0.9551, -3.6289, -1.5146]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 63]), X shape:torch.Size([16, 63, 512])
CTC Tokens:tensor([ 19,   0,   0, 213,   0], device='cuda:3'), Shrink Mask:tensor([ True,  True, False,  True,  True], device='cuda:3'), New Tokens:tensor([  19,    0,  213,    0, 3374], device='cuda:3')
Mixup Sent Mask:tensor([[ 2],
        [ 3],
        [ 8],
        [11],
        [12],
        [13],
        [15]], device='cuda:3'), 2,  Mixup Mask:tensor([False,  True,  True, False,  True], device='cuda:3'), 
                    Org X:tensor([[-0.3535, -0.4136, -0.6973,  ...,  0.7334,  1.0537, -1.2998],
        [ 0.1395,  0.7437,  1.0156,  ..., -0.5024,  1.3496, -1.0156],
        [ 0.2205,  0.9585,  0.8184,  ..., -2.0332, -0.2031, -1.3037],
        [ 1.4062,  2.0957,  0.2935,  ..., -3.0625, -0.3835, -0.5703],
        [ 0.5527,  0.3320, -0.4524,  ..., -1.2852,  0.3230,  0.9897]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[-0.3535, -0.4136, -0.6973,  ...,  0.7334,  1.0537, -1.2998],
        [-0.4336, -0.8516,  0.2930,  ..., -2.6387, -1.5713, -1.8164],
        [ 0.0779,  0.0610,  0.1781,  ..., -1.4463, -0.3828, -0.6411],
        [ 1.4062,  2.0957,  0.2935,  ..., -3.0625, -0.3835, -0.5703],
        [ 0.1670,  0.4343,  0.1066,  ...,  1.0293,  0.2109, -0.8735]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[ 0.3491, -0.2788, -0.1675,  ..., -0.9473,  0.2698, -1.4893],
        [-0.4336, -0.8516,  0.2930,  ..., -2.6387, -1.5713, -1.8164],
        [ 0.0779,  0.0610,  0.1781,  ..., -1.4463, -0.3828, -0.6411],
        [-0.4336, -0.8516,  0.2930,  ..., -2.6387, -1.5713, -1.8164],
        [ 0.1670,  0.4343,  0.1066,  ...,  1.0293,  0.2109, -0.8735]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 72]), X shape:torch.Size([8, 72, 512])
CTC Tokens:tensor([1436, 1436,   59,  658,  658], device='cuda:6'), Shrink Mask:tensor([ True, False,  True,  True, False], device='cuda:6'), New Tokens:tensor([1436,   59,  658,    0, 5727], device='cuda:6')
Mixup Sent Mask:tensor([[2],
        [3]], device='cuda:6'), 2,  Mixup Mask:tensor([ True,  True, False, False,  True], device='cuda:6'), 
                    Org X:tensor([[-1.6680, -0.0958,  0.9614,  ..., -0.4985, -0.5986,  0.6743],
        [ 0.2034, -1.0762,  0.9805,  ...,  0.1305, -0.2312,  0.4805],
        [ 0.5962, -1.4248,  1.4121,  ...,  0.1143,  0.3679, -0.6284],
        [ 0.4424, -0.2654, -0.9219,  ..., -0.6270,  0.1833, -2.7012],
        [-0.7158, -0.4189, -1.0410,  ..., -0.0085,  0.5283, -0.8296]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), New X:tensor([[ 0.1885,  0.6646, -0.0721,  ..., -1.2881, -1.5361, -2.9473],
        [ 0.3093, -0.4570,  0.2771,  ...,  0.2654,  0.7690,  1.0977],
        [ 0.5962, -1.4248,  1.4121,  ...,  0.1143,  0.3679, -0.6284],
        [ 0.4424, -0.2654, -0.9219,  ..., -0.6270,  0.1833, -2.7012],
        [ 1.9746,  0.6265, -1.0967,  ...,  2.0859,  0.0410, -3.5547]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>), Emb: tensor([[ 0.1885,  0.6646, -0.0721,  ..., -1.2881, -1.5361, -2.9473],
        [ 0.3093, -0.4570,  0.2771,  ...,  0.2654,  0.7690,  1.0977],
        [ 2.7422, -0.9780,  2.4316,  ...,  0.1063,  1.6660, -3.1133],
        [-0.4336, -0.8516,  0.2930,  ..., -2.6387, -1.5713, -1.8164],
        [ 1.9746,  0.6265, -1.0967,  ...,  2.0859,  0.0410, -3.5547]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:6')
2023-08-08 10:25:33 | INFO | dev_st | epoch 031 | valid on 'dev_st' subset | loss 4.183 | trans_loss 5.546 | nll_loss 2.815 | w2v_ctc_loss 1.341 | task_loss 4.626 | contrastive_loss 0.247 | total 4003.4 | n_correct 2497.4 | ppl 7.04 | accuracy 62.382 | uer 16.545 | wer 18.486 | raw_wer 18.486 | bleu 20.41 | wps 2104.3 | wpb 4003.4 | bsz 141.8 | num_updates 45674 | best_bleu 20.49
2023-08-08 10:25:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 45674 updates
2023-08-08 10:25:33 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.4107.pt
2023-08-08 10:25:36 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.4107.pt
2023-08-08 10:26:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.4107.pt (epoch 31 @ 45674 updates, score 20.41) (writing took 26.772966507822275 seconds)
2023-08-08 10:26:01 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-08-08 10:26:01 | INFO | train | epoch 031 | loss 1.939 | trans_loss 4.946 | nll_loss 2.127 | w2v_ctc_loss 0.592 | task_loss 1.399 | contrastive_loss 0.101 | total 4137.67 | n_correct 2760.45 | ppl 4.37 | accuracy 66.715 | wps 11296.9 | ups 1.37 | wpb 8275.3 | bsz 305.4 | num_updates 45674 | lr 6.61729e-05 | gnorm 0.531 | clip 0 | loss_scale 64 | train_wall 1013 | gb_free 11.9 | wall 37256
2023-08-08 10:26:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 10:26:01 | INFO | fairseq.trainer | begin training epoch 32
2023-08-08 10:26:01 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 10:26:27 | INFO | train_inner | epoch 032:     26 / 1474 loss=1.938, trans_loss=4.948, nll_loss=2.13, w2v_ctc_loss=0.595, task_loss=1.474, contrastive_loss=0.049, total=4040.88, n_correct=2695.72, ppl=4.38, accuracy=66.711, wps=6318.4, ups=0.78, wpb=8081.8, bsz=288.7, num_updates=45700, lr=6.61541e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=68, gb_free=15.4, wall=37282
2023-08-08 10:27:36 | INFO | train_inner | epoch 032:    126 / 1474 loss=1.913, trans_loss=4.911, nll_loss=2.082, w2v_ctc_loss=0.57, task_loss=1.289, contrastive_loss=0.06, total=4222.14, n_correct=2845.5, ppl=4.23, accuracy=67.395, wps=12218.9, ups=1.45, wpb=8444.3, bsz=322.6, num_updates=45800, lr=6.60819e-05, gnorm=0.522, clip=0, loss_scale=64, train_wall=69, gb_free=15.3, wall=37351
2023-08-08 10:28:46 | INFO | train_inner | epoch 032:    226 / 1474 loss=1.925, trans_loss=4.928, nll_loss=2.104, w2v_ctc_loss=0.584, task_loss=1.33, contrastive_loss=0.069, total=4159.77, n_correct=2789.01, ppl=4.3, accuracy=67.047, wps=11926.4, ups=1.43, wpb=8319.5, bsz=320.8, num_updates=45900, lr=6.60098e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=69, gb_free=16.1, wall=37421
2023-08-08 10:29:55 | INFO | train_inner | epoch 032:    326 / 1474 loss=1.918, trans_loss=4.917, nll_loss=2.09, w2v_ctc_loss=0.575, task_loss=1.322, contrastive_loss=0.062, total=4179.65, n_correct=2815.48, ppl=4.26, accuracy=67.362, wps=12090.2, ups=1.45, wpb=8359.3, bsz=313.8, num_updates=46000, lr=6.5938e-05, gnorm=0.526, clip=0, loss_scale=64, train_wall=69, gb_free=16.2, wall=37490
2023-08-08 10:29:55 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 10:30:19 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 4.186 | trans_loss 5.551 | nll_loss 2.821 | w2v_ctc_loss 1.339 | task_loss 4.616 | contrastive_loss 0.25 | total 4003.4 | n_correct 2497.6 | ppl 7.07 | accuracy 62.387 | uer 16.556 | wer 18.411 | raw_wer 18.411 | bleu 20.19 | wps 2219 | wpb 4003.4 | bsz 141.8 | num_updates 46000 | best_bleu 20.49
2023-08-08 10:30:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 46000 updates
2023-08-08 10:30:19 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_32_46000.pt
2023-08-08 10:30:23 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_32_46000.pt
2023-08-08 10:30:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_32_46000.pt (epoch 32 @ 46000 updates, score 20.19) (writing took 16.503850804641843 seconds)
2023-08-08 10:30:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-08 10:31:45 | INFO | train_inner | epoch 032:    427 / 1474 loss=1.924, trans_loss=4.926, nll_loss=2.101, w2v_ctc_loss=0.583, task_loss=1.355, contrastive_loss=0.061, total=4173.31, n_correct=2804.07, ppl=4.29, accuracy=67.191, wps=7590.1, ups=0.91, wpb=8346.6, bsz=311.4, num_updates=46100, lr=6.58665e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=69, gb_free=17.7, wall=37600
2023-08-08 10:32:55 | INFO | train_inner | epoch 032:    527 / 1474 loss=1.937, trans_loss=4.936, nll_loss=2.115, w2v_ctc_loss=0.591, task_loss=1.376, contrastive_loss=0.143, total=4188.83, n_correct=2805.17, ppl=4.33, accuracy=66.968, wps=12026.9, ups=1.44, wpb=8377.7, bsz=313.9, num_updates=46200, lr=6.57952e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=69, gb_free=12.6, wall=37670
2023-08-08 10:34:05 | INFO | train_inner | epoch 032:    627 / 1474 loss=1.936, trans_loss=4.943, nll_loss=2.123, w2v_ctc_loss=0.592, task_loss=1.469, contrastive_loss=0.066, total=4133.19, n_correct=2758.45, ppl=4.36, accuracy=66.739, wps=11772.4, ups=1.42, wpb=8266.4, bsz=298.7, num_updates=46300, lr=6.57241e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=70, gb_free=17.2, wall=37740
2023-08-08 10:35:15 | INFO | train_inner | epoch 032:    727 / 1474 loss=1.935, trans_loss=4.942, nll_loss=2.122, w2v_ctc_loss=0.597, task_loss=1.407, contrastive_loss=0.05, total=4162.1, n_correct=2783.34, ppl=4.35, accuracy=66.873, wps=11952.2, ups=1.44, wpb=8324.2, bsz=304.3, num_updates=46400, lr=6.56532e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=69, gb_free=15.7, wall=37810
2023-08-08 10:36:24 | INFO | train_inner | epoch 032:    827 / 1474 loss=1.929, trans_loss=4.937, nll_loss=2.115, w2v_ctc_loss=0.583, task_loss=1.458, contrastive_loss=0.047, total=4107.86, n_correct=2751.25, ppl=4.33, accuracy=66.975, wps=11934.1, ups=1.45, wpb=8215.7, bsz=292.6, num_updates=46500, lr=6.55826e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=68, gb_free=15.6, wall=37879
2023-08-08 10:37:33 | INFO | train_inner | epoch 032:    927 / 1474 loss=1.931, trans_loss=4.942, nll_loss=2.122, w2v_ctc_loss=0.583, task_loss=1.435, contrastive_loss=0.045, total=4146.9, n_correct=2773.77, ppl=4.35, accuracy=66.888, wps=12022.4, ups=1.45, wpb=8293.8, bsz=300.4, num_updates=46600, lr=6.55122e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=69, gb_free=16.7, wall=37948
2023-08-08 10:38:42 | INFO | train_inner | epoch 032:   1027 / 1474 loss=1.944, trans_loss=4.953, nll_loss=2.135, w2v_ctc_loss=0.595, task_loss=1.387, contrastive_loss=0.14, total=4112.45, n_correct=2740.27, ppl=4.39, accuracy=66.634, wps=11936, ups=1.45, wpb=8224.9, bsz=304.3, num_updates=46700, lr=6.5442e-05, gnorm=0.537, clip=0, loss_scale=32, train_wall=68, gb_free=17, wall=38016
2023-08-08 10:39:51 | INFO | train_inner | epoch 032:   1127 / 1474 loss=1.948, trans_loss=4.956, nll_loss=2.139, w2v_ctc_loss=0.595, task_loss=1.664, contrastive_loss=0.083, total=4015.2, n_correct=2670.35, ppl=4.4, accuracy=66.506, wps=11557.7, ups=1.44, wpb=8030.4, bsz=269.7, num_updates=46800, lr=6.5372e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=69, gb_free=14.2, wall=38086
2023-08-08 10:41:00 | INFO | train_inner | epoch 032:   1227 / 1474 loss=1.949, trans_loss=4.958, nll_loss=2.144, w2v_ctc_loss=0.589, task_loss=1.372, contrastive_loss=0.189, total=4158.99, n_correct=2764.81, ppl=4.42, accuracy=66.478, wps=12017, ups=1.44, wpb=8318, bsz=312.1, num_updates=46900, lr=6.53023e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=69, gb_free=16, wall=38155
2023-08-08 10:42:09 | INFO | train_inner | epoch 032:   1327 / 1474 loss=1.937, trans_loss=4.949, nll_loss=2.131, w2v_ctc_loss=0.593, task_loss=1.428, contrastive_loss=0.047, total=4079.56, n_correct=2718, ppl=4.38, accuracy=66.625, wps=11811.2, ups=1.45, wpb=8159.1, bsz=297.9, num_updates=47000, lr=6.52328e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=69, gb_free=17, wall=38224
2023-08-08 10:43:19 | INFO | train_inner | epoch 032:   1427 / 1474 loss=1.955, trans_loss=4.955, nll_loss=2.14, w2v_ctc_loss=0.597, task_loss=1.414, contrastive_loss=0.281, total=4107.37, n_correct=2733.13, ppl=4.41, accuracy=66.542, wps=11870.9, ups=1.45, wpb=8214.7, bsz=304.2, num_updates=47100, lr=6.51635e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=69, gb_free=17.2, wall=38293
2023-08-08 10:43:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 10:44:14 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 4.188 | trans_loss 5.542 | nll_loss 2.812 | w2v_ctc_loss 1.365 | task_loss 4.653 | contrastive_loss 0.248 | total 4003.4 | n_correct 2501.8 | ppl 7.02 | accuracy 62.492 | uer 16.582 | wer 18.389 | raw_wer 18.389 | bleu 20.14 | wps 2189.6 | wpb 4003.4 | bsz 141.8 | num_updates 47147 | best_bleu 20.49
2023-08-08 10:44:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 47147 updates
2023-08-08 10:44:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_last.pt
2023-08-08 10:44:27 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_last.pt
2023-08-08 10:44:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_last.pt (epoch 32 @ 47147 updates, score 20.14) (writing took 12.856764292344451 seconds)
2023-08-08 10:44:27 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-08-08 10:44:27 | INFO | train | epoch 032 | loss 1.934 | trans_loss 4.939 | nll_loss 2.118 | w2v_ctc_loss 0.587 | task_loss 1.398 | contrastive_loss 0.101 | total 4138.52 | n_correct 2767.83 | ppl 4.34 | accuracy 66.88 | wps 11025.3 | ups 1.33 | wpb 8277 | bsz 305.7 | num_updates 47147 | lr 6.5131e-05 | gnorm 0.532 | clip 0 | loss_scale 32 | train_wall 1014 | gb_free 16.3 | wall 38362
2023-08-08 10:44:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 10:44:27 | INFO | fairseq.trainer | begin training epoch 33
2023-08-08 10:44:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 10:45:12 | INFO | train_inner | epoch 033:     53 / 1474 loss=1.932, trans_loss=4.933, nll_loss=2.111, w2v_ctc_loss=0.579, task_loss=1.331, contrastive_loss=0.154, total=4146.91, n_correct=2778.42, ppl=4.32, accuracy=67, wps=7316.5, ups=0.88, wpb=8293.8, bsz=319.7, num_updates=47200, lr=6.50945e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=69, gb_free=16, wall=38407
2023-08-08 10:46:21 | INFO | train_inner | epoch 033:    153 / 1474 loss=1.918, trans_loss=4.915, nll_loss=2.086, w2v_ctc_loss=0.572, task_loss=1.501, contrastive_loss=0.037, total=4073.36, n_correct=2744.45, ppl=4.24, accuracy=67.376, wps=11736, ups=1.44, wpb=8146.7, bsz=285.1, num_updates=47300, lr=6.50256e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=69, gb_free=16.6, wall=38476
2023-08-08 10:47:30 | INFO | train_inner | epoch 033:    253 / 1474 loss=1.929, trans_loss=4.917, nll_loss=2.092, w2v_ctc_loss=0.577, task_loss=1.188, contrastive_loss=0.219, total=4283.64, n_correct=2883.86, ppl=4.26, accuracy=67.323, wps=12393.5, ups=1.45, wpb=8567.3, bsz=347.6, num_updates=47400, lr=6.4957e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=69, gb_free=16.2, wall=38545
2023-08-08 10:48:40 | INFO | train_inner | epoch 033:    353 / 1474 loss=1.926, trans_loss=4.927, nll_loss=2.102, w2v_ctc_loss=0.584, task_loss=1.42, contrastive_loss=0.069, total=4131.27, n_correct=2772.84, ppl=4.29, accuracy=67.118, wps=11910.7, ups=1.44, wpb=8262.5, bsz=302.3, num_updates=47500, lr=6.48886e-05, gnorm=0.537, clip=0, loss_scale=32, train_wall=69, gb_free=16, wall=38615
2023-08-08 10:49:48 | INFO | train_inner | epoch 033:    453 / 1474 loss=1.913, trans_loss=4.915, nll_loss=2.086, w2v_ctc_loss=0.575, task_loss=1.326, contrastive_loss=0.045, total=4135.1, n_correct=2788.55, ppl=4.24, accuracy=67.436, wps=12108.1, ups=1.46, wpb=8270.2, bsz=309.7, num_updates=47600, lr=6.48204e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=68, gb_free=15.4, wall=38683
2023-08-08 10:50:58 | INFO | train_inner | epoch 033:    553 / 1474 loss=1.932, trans_loss=4.935, nll_loss=2.111, w2v_ctc_loss=0.588, task_loss=1.453, contrastive_loss=0.069, total=4132.78, n_correct=2766.46, ppl=4.32, accuracy=66.939, wps=11871.1, ups=1.44, wpb=8265.6, bsz=294.2, num_updates=47700, lr=6.47524e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=69, gb_free=17.7, wall=38753
2023-08-08 10:52:07 | INFO | train_inner | epoch 033:    653 / 1474 loss=1.935, trans_loss=4.944, nll_loss=2.124, w2v_ctc_loss=0.585, task_loss=1.438, contrastive_loss=0.1, total=4156.26, n_correct=2774.89, ppl=4.36, accuracy=66.764, wps=11986.4, ups=1.44, wpb=8312.5, bsz=300.7, num_updates=47800, lr=6.46846e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=69, gb_free=15.9, wall=38822
2023-08-08 10:53:16 | INFO | train_inner | epoch 033:    753 / 1474 loss=1.939, trans_loss=4.942, nll_loss=2.121, w2v_ctc_loss=0.601, task_loss=1.511, contrastive_loss=0.047, total=4074.99, n_correct=2721.7, ppl=4.35, accuracy=66.79, wps=11827.5, ups=1.45, wpb=8150, bsz=288.2, num_updates=47900, lr=6.46171e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=68, gb_free=16.1, wall=38891
2023-08-08 10:54:25 | INFO | train_inner | epoch 033:    853 / 1474 loss=1.919, trans_loss=4.929, nll_loss=2.105, w2v_ctc_loss=0.567, task_loss=1.336, contrastive_loss=0.116, total=4127.6, n_correct=2773.14, ppl=4.3, accuracy=67.185, wps=11968.2, ups=1.45, wpb=8255.2, bsz=315.3, num_updates=48000, lr=6.45497e-05, gnorm=0.524, clip=0, loss_scale=32, train_wall=69, gb_free=15.9, wall=38960
2023-08-08 10:54:25 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 10:54:49 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 4.175 | trans_loss 5.545 | nll_loss 2.814 | w2v_ctc_loss 1.317 | task_loss 4.626 | contrastive_loss 0.242 | total 4003.4 | n_correct 2500.5 | ppl 7.03 | accuracy 62.459 | uer 16.553 | wer 18.333 | raw_wer 18.333 | bleu 20.39 | wps 2166.8 | wpb 4003.4 | bsz 141.8 | num_updates 48000 | best_bleu 20.49
2023-08-08 10:54:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48000 updates
2023-08-08 10:54:49 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_33_48000.pt
2023-08-08 10:54:52 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_33_48000.pt
2023-08-08 10:55:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_33_48000.pt (epoch 33 @ 48000 updates, score 20.39) (writing took 28.232014739885926 seconds)
2023-08-08 10:56:26 | INFO | train_inner | epoch 033:    953 / 1474 loss=1.932, trans_loss=4.936, nll_loss=2.115, w2v_ctc_loss=0.593, task_loss=1.384, contrastive_loss=0.06, total=4157.37, n_correct=2786.82, ppl=4.33, accuracy=67.033, wps=6865.3, ups=0.83, wpb=8314.7, bsz=310.1, num_updates=48100, lr=6.44826e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=68, gb_free=17.3, wall=39081
2023-08-08 10:57:36 | INFO | train_inner | epoch 033:   1053 / 1474 loss=1.938, trans_loss=4.937, nll_loss=2.116, w2v_ctc_loss=0.584, task_loss=1.415, contrastive_loss=0.165, total=4134.8, n_correct=2764.86, ppl=4.34, accuracy=66.868, wps=11833.1, ups=1.43, wpb=8269.6, bsz=306, num_updates=48200, lr=6.44157e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=15.6, wall=39151
2023-08-08 10:58:46 | INFO | train_inner | epoch 033:   1153 / 1474 loss=1.937, trans_loss=4.944, nll_loss=2.126, w2v_ctc_loss=0.581, task_loss=1.396, contrastive_loss=0.153, total=4181.58, n_correct=2792.48, ppl=4.36, accuracy=66.78, wps=12009.3, ups=1.44, wpb=8363.2, bsz=310, num_updates=48300, lr=6.43489e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=15, wall=39221
2023-08-08 10:59:55 | INFO | train_inner | epoch 033:   1253 / 1474 loss=1.933, trans_loss=4.939, nll_loss=2.119, w2v_ctc_loss=0.589, task_loss=1.47, contrastive_loss=0.052, total=4115.76, n_correct=2754.51, ppl=4.34, accuracy=66.926, wps=11852.1, ups=1.44, wpb=8231.5, bsz=294.7, num_updates=48400, lr=6.42824e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=69, gb_free=16.6, wall=39290
2023-08-08 11:01:04 | INFO | train_inner | epoch 033:   1353 / 1474 loss=1.928, trans_loss=4.939, nll_loss=2.12, w2v_ctc_loss=0.584, task_loss=1.374, contrastive_loss=0.072, total=4120.69, n_correct=2761.04, ppl=4.35, accuracy=67.004, wps=11892.4, ups=1.44, wpb=8241.4, bsz=311.9, num_updates=48500, lr=6.42161e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=69, gb_free=16.7, wall=39359
2023-08-08 11:02:14 | INFO | train_inner | epoch 033:   1453 / 1474 loss=1.939, trans_loss=4.941, nll_loss=2.122, w2v_ctc_loss=0.583, task_loss=1.396, contrastive_loss=0.217, total=4125.28, n_correct=2754.41, ppl=4.35, accuracy=66.769, wps=11859.3, ups=1.44, wpb=8250.6, bsz=308.7, num_updates=48600, lr=6.415e-05, gnorm=0.528, clip=0, loss_scale=64, train_wall=69, gb_free=16.8, wall=39429
2023-08-08 11:02:28 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 11:02:51 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 4.193 | trans_loss 5.547 | nll_loss 2.812 | w2v_ctc_loss 1.372 | task_loss 4.604 | contrastive_loss 0.249 | total 4003.4 | n_correct 2506.7 | ppl 7.02 | accuracy 62.614 | uer 16.633 | wer 18.411 | raw_wer 18.411 | bleu 20.25 | wps 2235.4 | wpb 4003.4 | bsz 141.8 | num_updates 48621 | best_bleu 20.49
2023-08-08 11:02:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48621 updates
2023-08-08 11:02:51 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.2505.pt
2023-08-08 11:02:55 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.2505.pt
2023-08-08 11:03:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint.best_bleu_20.2505.pt (epoch 33 @ 48621 updates, score 20.25) (writing took 16.794354986399412 seconds)
2023-08-08 11:03:09 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-08-08 11:03:09 | INFO | train | epoch 033 | loss 1.93 | trans_loss 4.933 | nll_loss 2.11 | w2v_ctc_loss 0.583 | task_loss 1.398 | contrastive_loss 0.101 | total 4138.65 | n_correct 2774.06 | ppl 4.32 | accuracy 67.028 | wps 10879.1 | ups 1.31 | wpb 8277.3 | bsz 305.7 | num_updates 48621 | lr 6.41362e-05 | gnorm 0.533 | clip 0 | loss_scale 64 | train_wall 1014 | gb_free 17.8 | wall 39483
2023-08-08 11:03:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-08 11:03:09 | INFO | fairseq.trainer | begin training epoch 34
2023-08-08 11:03:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-08 11:04:12 | INFO | train_inner | epoch 034:     79 / 1474 loss=1.919, trans_loss=4.913, nll_loss=2.085, w2v_ctc_loss=0.582, task_loss=1.38, contrastive_loss=0.054, total=4131.47, n_correct=2786.5, ppl=4.24, accuracy=67.446, wps=7017.5, ups=0.85, wpb=8262.9, bsz=301.7, num_updates=48700, lr=6.40841e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=68, gb_free=16.5, wall=39547
2023-08-08 11:05:21 | INFO | train_inner | epoch 034:    179 / 1474 loss=1.915, trans_loss=4.909, nll_loss=2.078, w2v_ctc_loss=0.574, task_loss=1.461, contrastive_loss=0.055, total=4065.88, n_correct=2745.98, ppl=4.22, accuracy=67.537, wps=11685.3, ups=1.44, wpb=8131.8, bsz=295.1, num_updates=48800, lr=6.40184e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=15.8, wall=39616
2023-08-08 11:06:31 | INFO | train_inner | epoch 034:    279 / 1474 loss=1.936, trans_loss=4.927, nll_loss=2.103, w2v_ctc_loss=0.575, task_loss=1.304, contrastive_loss=0.263, total=4246.3, n_correct=2849.96, ppl=4.3, accuracy=67.116, wps=12216.8, ups=1.44, wpb=8492.6, bsz=328.7, num_updates=48900, lr=6.39529e-05, gnorm=0.528, clip=0, loss_scale=64, train_wall=69, gb_free=17.7, wall=39686
2023-08-08 11:07:40 | INFO | train_inner | epoch 034:    379 / 1474 loss=1.918, trans_loss=4.91, nll_loss=2.082, w2v_ctc_loss=0.569, task_loss=1.33, contrastive_loss=0.152, total=4156.17, n_correct=2806.31, ppl=4.23, accuracy=67.522, wps=11974.5, ups=1.44, wpb=8312.3, bsz=316.7, num_updates=49000, lr=6.38877e-05, gnorm=0.527, clip=0, loss_scale=64, train_wall=69, gb_free=17.7, wall=39755
2023-08-08 11:08:49 | INFO | train_inner | epoch 034:    479 / 1474 loss=1.928, trans_loss=4.928, nll_loss=2.103, w2v_ctc_loss=0.588, task_loss=1.534, contrastive_loss=0.048, total=4070.55, n_correct=2730.95, ppl=4.3, accuracy=67.09, wps=11878.7, ups=1.46, wpb=8141.1, bsz=284.6, num_updates=49100, lr=6.38226e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=68, gb_free=17.3, wall=39824
2023-08-08 11:09:58 | INFO | train_inner | epoch 034:    579 / 1474 loss=1.92, trans_loss=4.917, nll_loss=2.089, w2v_ctc_loss=0.579, task_loss=1.419, contrastive_loss=0.05, total=4119.38, n_correct=2775.01, ppl=4.25, accuracy=67.365, wps=11884.8, ups=1.44, wpb=8238.8, bsz=300.3, num_updates=49200, lr=6.37577e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=12.9, wall=39893
2023-08-08 11:11:07 | INFO | train_inner | epoch 034:    679 / 1474 loss=1.919, trans_loss=4.922, nll_loss=2.097, w2v_ctc_loss=0.576, task_loss=1.417, contrastive_loss=0.044, total=4124.83, n_correct=2775.52, ppl=4.28, accuracy=67.288, wps=11923.4, ups=1.45, wpb=8249.7, bsz=300.2, num_updates=49300, lr=6.3693e-05, gnorm=0.531, clip=0, loss_scale=64, train_wall=69, gb_free=14.1, wall=39962
2023-08-08 11:12:17 | INFO | train_inner | epoch 034:    779 / 1474 loss=1.931, trans_loss=4.943, nll_loss=2.123, w2v_ctc_loss=0.571, task_loss=1.469, contrastive_loss=0.114, total=4082.07, n_correct=2731.84, ppl=4.36, accuracy=66.923, wps=11710, ups=1.43, wpb=8164.1, bsz=295, num_updates=49400, lr=6.36285e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=15.4, wall=40032
2023-08-08 11:13:27 | INFO | train_inner | epoch 034:    879 / 1474 loss=1.929, trans_loss=4.934, nll_loss=2.112, w2v_ctc_loss=0.582, task_loss=1.475, contrastive_loss=0.074, total=4100.9, n_correct=2747.8, ppl=4.32, accuracy=67.005, wps=11794.9, ups=1.44, wpb=8201.8, bsz=296.6, num_updates=49500, lr=6.35642e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=69, gb_free=11.9, wall=40102
2023-08-08 11:14:36 | INFO | train_inner | epoch 034:    979 / 1474 loss=1.929, trans_loss=4.934, nll_loss=2.112, w2v_ctc_loss=0.59, task_loss=1.374, contrastive_loss=0.069, total=4168.39, n_correct=2793.49, ppl=4.32, accuracy=67.016, wps=12067.6, ups=1.45, wpb=8336.8, bsz=311.9, num_updates=49600, lr=6.35001e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=15.7, wall=40171
2023-08-08 11:15:44 | INFO | train_inner | epoch 034:   1079 / 1474 loss=1.928, trans_loss=4.936, nll_loss=2.115, w2v_ctc_loss=0.588, task_loss=1.358, contrastive_loss=0.051, total=4150.57, n_correct=2778.2, ppl=4.33, accuracy=66.935, wps=12089, ups=1.46, wpb=8301.1, bsz=308.5, num_updates=49700, lr=6.34361e-05, gnorm=0.53, clip=0, loss_scale=64, train_wall=68, gb_free=16.7, wall=40239
2023-08-08 11:16:54 | INFO | train_inner | epoch 034:   1179 / 1474 loss=1.927, trans_loss=4.934, nll_loss=2.112, w2v_ctc_loss=0.581, task_loss=1.445, contrastive_loss=0.062, total=4098.77, n_correct=2749.12, ppl=4.32, accuracy=67.072, wps=11827.7, ups=1.44, wpb=8197.5, bsz=297.1, num_updates=49800, lr=6.33724e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=69, gb_free=16.8, wall=40309
2023-08-08 11:18:03 | INFO | train_inner | epoch 034:   1279 / 1474 loss=1.923, trans_loss=4.931, nll_loss=2.108, w2v_ctc_loss=0.579, task_loss=1.408, contrastive_loss=0.047, total=4150.54, n_correct=2782.96, ppl=4.31, accuracy=67.051, wps=12001.6, ups=1.45, wpb=8301.1, bsz=301, num_updates=49900, lr=6.33089e-05, gnorm=0.528, clip=0, loss_scale=64, train_wall=69, gb_free=17.1, wall=40378
2023-08-08 11:19:13 | INFO | train_inner | epoch 034:   1379 / 1474 loss=1.936, trans_loss=4.941, nll_loss=2.121, w2v_ctc_loss=0.592, task_loss=1.34, contrastive_loss=0.113, total=4196.91, n_correct=2804.98, ppl=4.35, accuracy=66.834, wps=12006.7, ups=1.43, wpb=8393.8, bsz=321.4, num_updates=50000, lr=6.32456e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=69, gb_free=15.9, wall=40448
2023-08-08 11:19:13 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2023-08-08 11:19:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-08 11:19:36 | INFO | dev_st | epoch 034 | valid on 'dev_st' subset | loss 4.182 | trans_loss 5.546 | nll_loss 2.815 | w2v_ctc_loss 1.336 | task_loss 4.64 | contrastive_loss 0.248 | total 4003.4 | n_correct 2502.8 | ppl 7.04 | accuracy 62.517 | uer 16.492 | wer 18.232 | raw_wer 18.232 | bleu 20.25 | wps 2286.8 | wpb 4003.4 | bsz 141.8 | num_updates 50000 | best_bleu 20.49
2023-08-08 11:19:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 50000 updates
2023-08-08 11:19:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_34_50000.pt
2023-08-08 11:19:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_34_50000.pt
2023-08-08 11:20:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_0808_mixup_at/checkpoint_34_50000.pt (epoch 34 @ 50000 updates, score 20.25) (writing took 51.62013531476259 seconds)
2023-08-08 11:20:28 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2023-08-08 11:20:28 | INFO | train | epoch 034 | loss 1.926 | trans_loss 4.927 | nll_loss 2.103 | w2v_ctc_loss 0.58 | task_loss 1.408 | contrastive_loss 0.088 | total 4133.04 | n_correct 2775.78 | ppl 4.29 | accuracy 67.161 | wps 10969 | ups 1.33 | wpb 8266.1 | bsz 304.2 | num_updates 50000 | lr 6.32456e-05 | gnorm 0.533 | clip 0 | loss_scale 64 | train_wall 949 | gb_free 15.9 | wall 40523
2023-08-08 11:20:28 | INFO | fairseq_cli.train | done training in 40469.7 seconds
Exception in thread Thread-6:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-7:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
    raise EOFError
EOFError
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1472 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
